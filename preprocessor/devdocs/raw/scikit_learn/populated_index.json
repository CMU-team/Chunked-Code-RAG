[{"name": "1.1. Linear Models", "path": "modules/linear_model", "type": "Guide", "text": "\nThe following are a set of methods intended for regression in which the target\nvalue is expected to be a linear combination of the features. In mathematical\nnotation, if \\\\(\\hat{y}\\\\) is the predicted value.\n\nAcross the module, we designate the vector \\\\(w = (w_1, ..., w_p)\\\\) as\n`coef_` and \\\\(w_0\\\\) as `intercept_`.\n\nTo perform classification with generalized linear models, see Logistic\nregression.\n\n`LinearRegression` fits a linear model with coefficients \\\\(w = (w_1, ...,\nw_p)\\\\) to minimize the residual sum of squares between the observed targets\nin the dataset, and the targets predicted by the linear approximation.\nMathematically it solves a problem of the form:\n\n`LinearRegression` will take in its `fit` method arrays X, y and will store\nthe coefficients \\\\(w\\\\) of the linear model in its `coef_` member:\n\nThe coefficient estimates for Ordinary Least Squares rely on the independence\nof the features. When features are correlated and the columns of the design\nmatrix \\\\(X\\\\) have an approximate linear dependence, the design matrix\nbecomes close to singular and as a result, the least-squares estimate becomes\nhighly sensitive to random errors in the observed target, producing a large\nvariance. This situation of multicollinearity can arise, for example, when\ndata are collected without an experimental design.\n\nExamples:\n\nIt is possible to constrain all the coefficients to be non-negative, which may\nbe useful when they represent some physical or naturally non-negative\nquantities (e.g., frequency counts or prices of goods). `LinearRegression`\naccepts a boolean `positive` parameter: when set to `True` Non Negative Least\nSquares are then applied.\n\nExamples:\n\nThe least squares solution is computed using the singular value decomposition\nof X. If X is a matrix of shape `(n_samples, n_features)` this method has a\ncost of \\\\(O(n_{\\text{samples}} n_{\\text{features}}^2)\\\\), assuming that\n\\\\(n_{\\text{samples}} \\geq n_{\\text{features}}\\\\).\n\n`Ridge` regression addresses some of the problems of Ordinary Least Squares by\nimposing a penalty on the size of the coefficients. The ridge coefficients\nminimize a penalized residual sum of squares:\n\nThe complexity parameter \\\\(\\alpha \\geq 0\\\\) controls the amount of shrinkage:\nthe larger the value of \\\\(\\alpha\\\\), the greater the amount of shrinkage and\nthus the coefficients become more robust to collinearity.\n\nAs with other linear models, `Ridge` will take in its `fit` method arrays X, y\nand will store the coefficients \\\\(w\\\\) of the linear model in its `coef_`\nmember:\n\nThe `Ridge` regressor has a classifier variant: `RidgeClassifier`. This\nclassifier first converts binary targets to `{-1, 1}` and then treats the\nproblem as a regression task, optimizing the same objective as above. The\npredicted class corresponds to the sign of the regressor\u2019s prediction. For\nmulticlass classification, the problem is treated as multi-output regression,\nand the predicted class corresponds to the output with the highest value.\n\nIt might seem questionable to use a (penalized) Least Squares loss to fit a\nclassification model instead of the more traditional logistic or hinge losses.\nHowever in practice all those models can lead to similar cross-validation\nscores in terms of accuracy or precision/recall, while the penalized least\nsquares loss used by the `RidgeClassifier` allows for a very different choice\nof the numerical solvers with distinct computational performance profiles.\n\nThe `RidgeClassifier` can be significantly faster than e.g.\n`LogisticRegression` with a high number of classes, because it is able to\ncompute the projection matrix \\\\((X^T X)^{-1} X^T\\\\) only once.\n\nThis classifier is sometimes referred to as a Least Squares Support Vector\nMachines with a linear kernel.\n\nExamples:\n\nThis method has the same order of complexity as Ordinary Least Squares.\n\n`RidgeCV` implements ridge regression with built-in cross-validation of the\nalpha parameter. The object works in the same way as GridSearchCV except that\nit defaults to Leave-One-Out Cross-Validation:\n\nSpecifying the value of the cv attribute will trigger the use of cross-\nvalidation with `GridSearchCV`, for example `cv=10` for 10-fold cross-\nvalidation, rather than Leave-One-Out Cross-Validation.\n\nReferences\n\nThe `Lasso` is a linear model that estimates sparse coefficients. It is useful\nin some contexts due to its tendency to prefer solutions with fewer non-zero\ncoefficients, effectively reducing the number of features upon which the given\nsolution is dependent. For this reason Lasso and its variants are fundamental\nto the field of compressed sensing. Under certain conditions, it can recover\nthe exact set of non-zero coefficients (see Compressive sensing: tomography\nreconstruction with L1 prior (Lasso)).\n\nMathematically, it consists of a linear model with an added regularization\nterm. The objective function to minimize is:\n\nThe lasso estimate thus solves the minimization of the least-squares penalty\nwith \\\\(\\alpha ||w||_1\\\\) added, where \\\\(\\alpha\\\\) is a constant and\n\\\\(||w||_1\\\\) is the \\\\(\\ell_1\\\\)-norm of the coefficient vector.\n\nThe implementation in the class `Lasso` uses coordinate descent as the\nalgorithm to fit the coefficients. See Least Angle Regression for another\nimplementation:\n\nThe function `lasso_path` is useful for lower-level tasks, as it computes the\ncoefficients along the full path of possible values.\n\nExamples:\n\nNote\n\nFeature selection with Lasso\n\nAs the Lasso regression yields sparse models, it can thus be used to perform\nfeature selection, as detailed in L1-based feature selection.\n\nThe following two references explain the iterations used in the coordinate\ndescent solver of scikit-learn, as well as the duality gap computation used\nfor convergence control.\n\nReferences\n\nThe `alpha` parameter controls the degree of sparsity of the estimated\ncoefficients.\n\nscikit-learn exposes objects that set the Lasso `alpha` parameter by cross-\nvalidation: `LassoCV` and `LassoLarsCV`. `LassoLarsCV` is based on the Least\nAngle Regression algorithm explained below.\n\nFor high-dimensional datasets with many collinear features, `LassoCV` is most\noften preferable. However, `LassoLarsCV` has the advantage of exploring more\nrelevant values of `alpha` parameter, and if the number of samples is very\nsmall compared to the number of features, it is often faster than `LassoCV`.\n\nAlternatively, the estimator `LassoLarsIC` proposes to use the Akaike\ninformation criterion (AIC) and the Bayes Information criterion (BIC). It is a\ncomputationally cheaper alternative to find the optimal value of alpha as the\nregularization path is computed only once instead of k+1 times when using\nk-fold cross-validation. However, such criteria needs a proper estimation of\nthe degrees of freedom of the solution, are derived for large samples\n(asymptotic results) and assume the model is correct, i.e. that the data are\nactually generated by this model. They also tend to break when the problem is\nbadly conditioned (more features than samples).\n\nExamples:\n\nThe equivalence between `alpha` and the regularization parameter of SVM, `C`\nis given by `alpha = 1 / C` or `alpha = 1 / (n_samples * C)`, depending on the\nestimator and the exact objective function optimized by the model.\n\nThe `MultiTaskLasso` is a linear model that estimates sparse coefficients for\nmultiple regression problems jointly: `y` is a 2D array, of shape `(n_samples,\nn_tasks)`. The constraint is that the selected features are the same for all\nthe regression problems, also called tasks.\n\nThe following figure compares the location of the non-zero entries in the\ncoefficient matrix W obtained with a simple Lasso or a MultiTaskLasso. The\nLasso estimates yield scattered non-zeros while the non-zeros of the\nMultiTaskLasso are full columns.\n\nFitting a time-series model, imposing that any active feature be active at all\ntimes.\n\nExamples:\n\nMathematically, it consists of a linear model trained with a mixed\n\\\\(\\ell_1\\\\) \\\\(\\ell_2\\\\)-norm for regularization. The objective function to\nminimize is:\n\nwhere \\\\(\\text{Fro}\\\\) indicates the Frobenius norm\n\nand \\\\(\\ell_1\\\\) \\\\(\\ell_2\\\\) reads\n\nThe implementation in the class `MultiTaskLasso` uses coordinate descent as\nthe algorithm to fit the coefficients.\n\n`ElasticNet` is a linear regression model trained with both \\\\(\\ell_1\\\\) and\n\\\\(\\ell_2\\\\)-norm regularization of the coefficients. This combination allows\nfor learning a sparse model where few of the weights are non-zero like\n`Lasso`, while still maintaining the regularization properties of `Ridge`. We\ncontrol the convex combination of \\\\(\\ell_1\\\\) and \\\\(\\ell_2\\\\) using the\n`l1_ratio` parameter.\n\nElastic-net is useful when there are multiple features which are correlated\nwith one another. Lasso is likely to pick one of these at random, while\nelastic-net is likely to pick both.\n\nA practical advantage of trading-off between Lasso and Ridge is that it allows\nElastic-Net to inherit some of Ridge\u2019s stability under rotation.\n\nThe objective function to minimize is in this case\n\nThe class `ElasticNetCV` can be used to set the parameters `alpha`\n(\\\\(\\alpha\\\\)) and `l1_ratio` (\\\\(\\rho\\\\)) by cross-validation.\n\nExamples:\n\nThe following two references explain the iterations used in the coordinate\ndescent solver of scikit-learn, as well as the duality gap computation used\nfor convergence control.\n\nReferences\n\nThe `MultiTaskElasticNet` is an elastic-net model that estimates sparse\ncoefficients for multiple regression problems jointly: `Y` is a 2D array of\nshape `(n_samples, n_tasks)`. The constraint is that the selected features are\nthe same for all the regression problems, also called tasks.\n\nMathematically, it consists of a linear model trained with a mixed\n\\\\(\\ell_1\\\\) \\\\(\\ell_2\\\\)-norm and \\\\(\\ell_2\\\\)-norm for regularization. The\nobjective function to minimize is:\n\nThe implementation in the class `MultiTaskElasticNet` uses coordinate descent\nas the algorithm to fit the coefficients.\n\nThe class `MultiTaskElasticNetCV` can be used to set the parameters `alpha`\n(\\\\(\\alpha\\\\)) and `l1_ratio` (\\\\(\\rho\\\\)) by cross-validation.\n\nLeast-angle regression (LARS) is a regression algorithm for high-dimensional\ndata, developed by Bradley Efron, Trevor Hastie, Iain Johnstone and Robert\nTibshirani. LARS is similar to forward stepwise regression. At each step, it\nfinds the feature most correlated with the target. When there are multiple\nfeatures having equal correlation, instead of continuing along the same\nfeature, it proceeds in a direction equiangular between the features.\n\nThe advantages of LARS are:\n\nThe disadvantages of the LARS method include:\n\nThe LARS model can be used using estimator `Lars`, or its low-level\nimplementation `lars_path` or `lars_path_gram`.\n\n`LassoLars` is a lasso model implemented using the LARS algorithm, and unlike\nthe implementation based on coordinate descent, this yields the exact\nsolution, which is piecewise linear as a function of the norm of its\ncoefficients.\n\nExamples:\n\nThe Lars algorithm provides the full path of the coefficients along the\nregularization parameter almost for free, thus a common operation is to\nretrieve the path with one of the functions `lars_path` or `lars_path_gram`.\n\nThe algorithm is similar to forward stepwise regression, but instead of\nincluding features at each step, the estimated coefficients are increased in a\ndirection equiangular to each one\u2019s correlations with the residual.\n\nInstead of giving a vector result, the LARS solution consists of a curve\ndenoting the solution for each value of the \\\\(\\ell_1\\\\) norm of the parameter\nvector. The full coefficients path is stored in the array `coef_path_`, which\nhas size (n_features, max_features+1). The first column is always zero.\n\nReferences:\n\n`OrthogonalMatchingPursuit` and `orthogonal_mp` implements the OMP algorithm\nfor approximating the fit of a linear model with constraints imposed on the\nnumber of non-zero coefficients (ie. the \\\\(\\ell_0\\\\) pseudo-norm).\n\nBeing a forward feature selection method like Least Angle Regression,\northogonal matching pursuit can approximate the optimum solution vector with a\nfixed number of non-zero elements:\n\nAlternatively, orthogonal matching pursuit can target a specific error instead\nof a specific number of non-zero coefficients. This can be expressed as:\n\nOMP is based on a greedy algorithm that includes at each step the atom most\nhighly correlated with the current residual. It is similar to the simpler\nmatching pursuit (MP) method, but better in that at each iteration, the\nresidual is recomputed using an orthogonal projection on the space of the\npreviously chosen dictionary elements.\n\nExamples:\n\nReferences:\n\nBayesian regression techniques can be used to include regularization\nparameters in the estimation procedure: the regularization parameter is not\nset in a hard sense but tuned to the data at hand.\n\nThis can be done by introducing uninformative priors over the hyper parameters\nof the model. The \\\\(\\ell_{2}\\\\) regularization used in Ridge regression and\nclassification is equivalent to finding a maximum a posteriori estimation\nunder a Gaussian prior over the coefficients \\\\(w\\\\) with precision\n\\\\(\\lambda^{-1}\\\\). Instead of setting `lambda` manually, it is possible to\ntreat it as a random variable to be estimated from the data.\n\nTo obtain a fully probabilistic model, the output \\\\(y\\\\) is assumed to be\nGaussian distributed around \\\\(X w\\\\):\n\nwhere \\\\(\\alpha\\\\) is again treated as a random variable that is to be\nestimated from the data.\n\nThe advantages of Bayesian Regression are:\n\nThe disadvantages of Bayesian regression include:\n\nReferences\n\n`BayesianRidge` estimates a probabilistic model of the regression problem as\ndescribed above. The prior for the coefficient \\\\(w\\\\) is given by a spherical\nGaussian:\n\nThe priors over \\\\(\\alpha\\\\) and \\\\(\\lambda\\\\) are chosen to be gamma\ndistributions, the conjugate prior for the precision of the Gaussian. The\nresulting model is called Bayesian Ridge Regression, and is similar to the\nclassical `Ridge`.\n\nThe parameters \\\\(w\\\\), \\\\(\\alpha\\\\) and \\\\(\\lambda\\\\) are estimated jointly\nduring the fit of the model, the regularization parameters \\\\(\\alpha\\\\) and\n\\\\(\\lambda\\\\) being estimated by maximizing the log marginal likelihood. The\nscikit-learn implementation is based on the algorithm described in Appendix A\nof (Tipping, 2001) where the update of the parameters \\\\(\\alpha\\\\) and\n\\\\(\\lambda\\\\) is done as suggested in (MacKay, 1992). The initial value of the\nmaximization procedure can be set with the hyperparameters `alpha_init` and\n`lambda_init`.\n\nThere are four more hyperparameters, \\\\(\\alpha_1\\\\), \\\\(\\alpha_2\\\\),\n\\\\(\\lambda_1\\\\) and \\\\(\\lambda_2\\\\) of the gamma prior distributions over\n\\\\(\\alpha\\\\) and \\\\(\\lambda\\\\). These are usually chosen to be non-\ninformative. By default \\\\(\\alpha_1 = \\alpha_2 = \\lambda_1 = \\lambda_2 =\n10^{-6}\\\\).\n\nBayesian Ridge Regression is used for regression:\n\nAfter being fitted, the model can then be used to predict new values:\n\nThe coefficients \\\\(w\\\\) of the model can be accessed:\n\nDue to the Bayesian framework, the weights found are slightly different to the\nones found by Ordinary Least Squares. However, Bayesian Ridge Regression is\nmore robust to ill-posed problems.\n\nExamples:\n\nReferences:\n\n`ARDRegression` is very similar to Bayesian Ridge Regression, but can lead to\nsparser coefficients \\\\(w\\\\) 1 2. `ARDRegression` poses a different prior over\n\\\\(w\\\\), by dropping the assumption of the Gaussian being spherical.\n\nInstead, the distribution over \\\\(w\\\\) is assumed to be an axis-parallel,\nelliptical Gaussian distribution.\n\nThis means each coefficient \\\\(w_{i}\\\\) is drawn from a Gaussian distribution,\ncentered on zero and with a precision \\\\(\\lambda_{i}\\\\):\n\nwith \\\\(\\text{diag}(A) = \\lambda = \\\\{\\lambda_{1},...,\\lambda_{p}\\\\}\\\\).\n\nIn contrast to Bayesian Ridge Regression, each coordinate of \\\\(w_{i}\\\\) has\nits own standard deviation \\\\(\\lambda_i\\\\). The prior over all \\\\(\\lambda_i\\\\)\nis chosen to be the same gamma distribution given by hyperparameters\n\\\\(\\lambda_1\\\\) and \\\\(\\lambda_2\\\\).\n\nARD is also known in the literature as Sparse Bayesian Learning and Relevance\nVector Machine 3 4.\n\nExamples:\n\nReferences:\n\nChristopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 7.2.1\n\nDavid Wipf and Srikantan Nagarajan: A new view of automatic relevance\ndetermination\n\nMichael E. Tipping: Sparse Bayesian Learning and the Relevance Vector Machine\n\nTristan Fletcher: Relevance Vector Machines explained\n\nLogistic regression, despite its name, is a linear model for classification\nrather than regression. Logistic regression is also known in the literature as\nlogit regression, maximum-entropy classification (MaxEnt) or the log-linear\nclassifier. In this model, the probabilities describing the possible outcomes\nof a single trial are modeled using a logistic function.\n\nLogistic regression is implemented in `LogisticRegression`. This\nimplementation can fit binary, One-vs-Rest, or multinomial logistic regression\nwith optional \\\\(\\ell_1\\\\), \\\\(\\ell_2\\\\) or Elastic-Net regularization.\n\nNote\n\nRegularization is applied by default, which is common in machine learning but\nnot in statistics. Another advantage of regularization is that it improves\nnumerical stability. No regularization amounts to setting C to a very high\nvalue.\n\nAs an optimization problem, binary class \\\\(\\ell_2\\\\) penalized logistic\nregression minimizes the following cost function:\n\nSimilarly, \\\\(\\ell_1\\\\) regularized logistic regression solves the following\noptimization problem:\n\nElastic-Net regularization is a combination of \\\\(\\ell_1\\\\) and \\\\(\\ell_2\\\\),\nand minimizes the following cost function:\n\nwhere \\\\(\\rho\\\\) controls the strength of \\\\(\\ell_1\\\\) regularization vs.\n\\\\(\\ell_2\\\\) regularization (it corresponds to the `l1_ratio` parameter).\n\nNote that, in this notation, it\u2019s assumed that the target \\\\(y_i\\\\) takes\nvalues in the set \\\\({-1, 1}\\\\) at trial \\\\(i\\\\). We can also see that\nElastic-Net is equivalent to \\\\(\\ell_1\\\\) when \\\\(\\rho = 1\\\\) and equivalent\nto \\\\(\\ell_2\\\\) when \\\\(\\rho=0\\\\).\n\nThe solvers implemented in the class `LogisticRegression` are \u201cliblinear\u201d,\n\u201cnewton-cg\u201d, \u201clbfgs\u201d, \u201csag\u201d and \u201csaga\u201d:\n\nThe solver \u201cliblinear\u201d uses a coordinate descent (CD) algorithm, and relies on\nthe excellent C++ LIBLINEAR library, which is shipped with scikit-learn.\nHowever, the CD algorithm implemented in liblinear cannot learn a true\nmultinomial (multiclass) model; instead, the optimization problem is\ndecomposed in a \u201cone-vs-rest\u201d fashion so separate binary classifiers are\ntrained for all classes. This happens under the hood, so `LogisticRegression`\ninstances using this solver behave as multiclass classifiers. For \\\\(\\ell_1\\\\)\nregularization `sklearn.svm.l1_min_c` allows to calculate the lower bound for\nC in order to get a non \u201cnull\u201d (all feature weights to zero) model.\n\nThe \u201clbfgs\u201d, \u201csag\u201d and \u201cnewton-cg\u201d solvers only support \\\\(\\ell_2\\\\)\nregularization or no regularization, and are found to converge faster for some\nhigh-dimensional data. Setting `multi_class` to \u201cmultinomial\u201d with these\nsolvers learns a true multinomial logistic regression model 5, which means\nthat its probability estimates should be better calibrated than the default\n\u201cone-vs-rest\u201d setting.\n\nThe \u201csag\u201d solver uses Stochastic Average Gradient descent 6. It is faster than\nother solvers for large datasets, when both the number of samples and the\nnumber of features are large.\n\nThe \u201csaga\u201d solver 7 is a variant of \u201csag\u201d that also supports the non-smooth\n`penalty=\"l1\"`. This is therefore the solver of choice for sparse multinomial\nlogistic regression. It is also the only solver that supports\n`penalty=\"elasticnet\"`.\n\nThe \u201clbfgs\u201d is an optimization algorithm that approximates the\nBroyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm 8, which belongs to quasi-Newton\nmethods. The \u201clbfgs\u201d solver is recommended for use for small data-sets but for\nlarger datasets its performance suffers. 9\n\nThe following table summarizes the penalties supported by each solver:\n\nSolvers\n\nPenalties\n\n\u2018liblinear\u2019\n\n\u2018lbfgs\u2019\n\n\u2018newton-cg\u2019\n\n\u2018sag\u2019\n\n\u2018saga\u2019\n\nMultinomial + L2 penalty\n\nno\n\nyes\n\nyes\n\nyes\n\nyes\n\nOVR + L2 penalty\n\nyes\n\nyes\n\nyes\n\nyes\n\nyes\n\nMultinomial + L1 penalty\n\nno\n\nno\n\nno\n\nno\n\nyes\n\nOVR + L1 penalty\n\nyes\n\nno\n\nno\n\nno\n\nyes\n\nElastic-Net\n\nno\n\nno\n\nno\n\nno\n\nyes\n\nNo penalty (\u2018none\u2019)\n\nno\n\nyes\n\nyes\n\nyes\n\nyes\n\nBehaviors\n\nPenalize the intercept (bad)\n\nyes\n\nno\n\nno\n\nno\n\nno\n\nFaster for large datasets\n\nno\n\nno\n\nno\n\nyes\n\nyes\n\nRobust to unscaled datasets\n\nyes\n\nyes\n\nyes\n\nno\n\nno\n\nThe \u201clbfgs\u201d solver is used by default for its robustness. For large datasets\nthe \u201csaga\u201d solver is usually faster. For large dataset, you may also consider\nusing `SGDClassifier` with \u2018log\u2019 loss, which might be even faster but requires\nmore tuning.\n\nExamples:\n\nDifferences from liblinear:\n\nThere might be a difference in the scores obtained between\n`LogisticRegression` with `solver=liblinear` or `LinearSVC` and the external\nliblinear library directly, when `fit_intercept=False` and the fit `coef_`\n(or) the data to be predicted are zeroes. This is because for the sample(s)\nwith `decision_function` zero, `LogisticRegression` and `LinearSVC` predict\nthe negative class, while liblinear predicts the positive class. Note that a\nmodel with `fit_intercept=False` and having many samples with\n`decision_function` zero, is likely to be a underfit, bad model and you are\nadvised to set `fit_intercept=True` and increase the intercept_scaling.\n\nNote\n\nFeature selection with sparse logistic regression\n\nA logistic regression with \\\\(\\ell_1\\\\) penalty yields sparse models, and can\nthus be used to perform feature selection, as detailed in L1-based feature\nselection.\n\nNote\n\nP-value estimation\n\nIt is possible to obtain the p-values and confidence intervals for\ncoefficients in cases of regression without penalization. The `statsmodels\npackage <https://pypi.org/project/statsmodels/>` natively supports this.\nWithin sklearn, one could use bootstrapping instead as well.\n\n`LogisticRegressionCV` implements Logistic Regression with built-in cross-\nvalidation support, to find the optimal `C` and `l1_ratio` parameters\naccording to the `scoring` attribute. The \u201cnewton-cg\u201d, \u201csag\u201d, \u201csaga\u201d and\n\u201clbfgs\u201d solvers are found to be faster for high-dimensional dense data, due to\nwarm-starting (see Glossary).\n\nReferences:\n\nChristopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4\n\nMark Schmidt, Nicolas Le Roux, and Francis Bach: Minimizing Finite Sums with\nthe Stochastic Average Gradient.\n\nAaron Defazio, Francis Bach, Simon Lacoste-Julien: SAGA: A Fast Incremental\nGradient Method With Support for Non-Strongly Convex Composite Objectives.\n\nhttps://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm\n\n\u201cPerformance Evaluation of Lbfgs vs other solvers\u201d\n\nGeneralized Linear Models (GLM) extend linear models in two ways 10. First,\nthe predicted values \\\\(\\hat{y}\\\\) are linked to a linear combination of the\ninput variables \\\\(X\\\\) via an inverse link function \\\\(h\\\\) as\n\nSecondly, the squared loss function is replaced by the unit deviance \\\\(d\\\\)\nof a distribution in the exponential family (or more precisely, a reproductive\nexponential dispersion model (EDM) 11).\n\nThe minimization problem becomes:\n\nwhere \\\\(\\alpha\\\\) is the L2 regularization penalty. When sample weights are\nprovided, the average becomes a weighted average.\n\nThe following table lists some specific EDMs and their unit deviance (all of\nthese are instances of the Tweedie family):\n\nDistribution\n\nTarget Domain\n\nUnit Deviance \\\\(d(y, \\hat{y})\\\\)\n\nNormal\n\n\\\\(y \\in (-\\infty, \\infty)\\\\)\n\n\\\\((y-\\hat{y})^2\\\\)\n\nPoisson\n\n\\\\(y \\in [0, \\infty)\\\\)\n\n\\\\(2(y\\log\\frac{y}{\\hat{y}}-y+\\hat{y})\\\\)\n\nGamma\n\n\\\\(y \\in (0, \\infty)\\\\)\n\n\\\\(2(\\log\\frac{\\hat{y}}{y}+\\frac{y}{\\hat{y}}-1)\\\\)\n\nInverse Gaussian\n\n\\\\(y \\in (0, \\infty)\\\\)\n\n\\\\(\\frac{(y-\\hat{y})^2}{y\\hat{y}^2}\\\\)\n\nThe Probability Density Functions (PDF) of these distributions are illustrated\nin the following figure,\n\nPDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma\ndistributions with different mean values (\\\\(\\mu\\\\)). Observe the point mass\nat \\\\(Y=0\\\\) for the Poisson distribution and the Tweedie (power=1.5)\ndistribution, but not for the Gamma distribution which has a strictly positive\ntarget domain.\n\nThe choice of the distribution depends on the problem at hand:\n\nExamples of use cases include:\n\nReferences:\n\nMcCullagh, Peter; Nelder, John (1989). Generalized Linear Models, Second\nEdition. Boca Raton: Chapman and Hall/CRC. ISBN 0-412-31760-5.\n\nJ\u00f8rgensen, B. (1992). The theory of exponential dispersion models and analysis\nof deviance. Monografias de matem\u00e1tica, no. 51. See also Exponential\ndispersion model.\n\n`TweedieRegressor` implements a generalized linear model for the Tweedie\ndistribution, that allows to model any of the above mentioned distributions\nusing the appropriate `power` parameter. In particular:\n\nThe link function is determined by the `link` parameter.\n\nUsage example:\n\nExamples:\n\nThe feature matrix `X` should be standardized before fitting. This ensures\nthat the penalty treats features equally.\n\nSince the linear predictor \\\\(Xw\\\\) can be negative and Poisson, Gamma and\nInverse Gaussian distributions don\u2019t support negative values, it is necessary\nto apply an inverse link function that guarantees the non-negativeness. For\nexample with `link='log'`, the inverse link function becomes\n\\\\(h(Xw)=\\exp(Xw)\\\\).\n\nIf you want to model a relative frequency, i.e. counts per exposure (time,\nvolume, \u2026) you can do so by using a Poisson distribution and passing\n\\\\(y=\\frac{\\mathrm{counts}}{\\mathrm{exposure}}\\\\) as target values together\nwith \\\\(\\mathrm{exposure}\\\\) as sample weights. For a concrete example see\ne.g. Tweedie regression on insurance claims.\n\nWhen performing cross-validation for the `power` parameter of\n`TweedieRegressor`, it is advisable to specify an explicit `scoring` function,\nbecause the default scorer `TweedieRegressor.score` is a function of `power`\nitself.\n\nStochastic gradient descent is a simple yet very efficient approach to fit\nlinear models. It is particularly useful when the number of samples (and the\nnumber of features) is very large. The `partial_fit` method allows online/out-\nof-core learning.\n\nThe classes `SGDClassifier` and `SGDRegressor` provide functionality to fit\nlinear models for classification and regression using different (convex) loss\nfunctions and different penalties. E.g., with `loss=\"log\"`, `SGDClassifier`\nfits a logistic regression model, while with `loss=\"hinge\"` it fits a linear\nsupport vector machine (SVM).\n\nReferences\n\nThe `Perceptron` is another simple classification algorithm suitable for large\nscale learning. By default:\n\nThe last characteristic implies that the Perceptron is slightly faster to\ntrain than SGD with the hinge loss and that the resulting models are sparser.\n\nThe passive-aggressive algorithms are a family of algorithms for large-scale\nlearning. They are similar to the Perceptron in that they do not require a\nlearning rate. However, contrary to the Perceptron, they include a\nregularization parameter `C`.\n\nFor classification, `PassiveAggressiveClassifier` can be used with\n`loss='hinge'` (PA-I) or `loss='squared_hinge'` (PA-II). For regression,\n`PassiveAggressiveRegressor` can be used with `loss='epsilon_insensitive'`\n(PA-I) or `loss='squared_epsilon_insensitive'` (PA-II).\n\nReferences:\n\nRobust regression aims to fit a regression model in the presence of corrupt\ndata: either outliers, or error in the model.\n\nThere are different things to keep in mind when dealing with data corrupted by\noutliers:\n\nOutliers in X or in y?\n\nOutliers in the y direction\n\nOutliers in the X direction\n\nFraction of outliers versus amplitude of error\n\nThe number of outlying points matters, but also how much they are outliers.\n\nSmall outliers\n\nLarge outliers\n\nAn important notion of robust fitting is that of breakdown point: the fraction\nof data that can be outlying for the fit to start missing the inlying data.\n\nNote that in general, robust fitting in high-dimensional setting (large\n`n_features`) is very hard. The robust models here will probably not work in\nthese settings.\n\nTrade-offs: which estimator?\n\nScikit-learn provides 3 robust regression estimators: RANSAC, Theil Sen and\nHuberRegressor.\n\nWhen in doubt, use RANSAC.\n\nRANSAC (RANdom SAmple Consensus) fits a model from random subsets of inliers\nfrom the complete data set.\n\nRANSAC is a non-deterministic algorithm producing only a reasonable result\nwith a certain probability, which is dependent on the number of iterations\n(see `max_trials` parameter). It is typically used for linear and non-linear\nregression problems and is especially popular in the field of photogrammetric\ncomputer vision.\n\nThe algorithm splits the complete input sample data into a set of inliers,\nwhich may be subject to noise, and outliers, which are e.g. caused by\nerroneous measurements or invalid hypotheses about the data. The resulting\nmodel is then estimated only from the determined inliers.\n\nEach iteration performs the following steps:\n\nThese steps are performed either a maximum number of times (`max_trials`) or\nuntil one of the special stop criteria are met (see `stop_n_inliers` and\n`stop_score`). The final model is estimated using all inlier samples\n(consensus set) of the previously determined best model.\n\nThe `is_data_valid` and `is_model_valid` functions allow to identify and\nreject degenerate combinations of random sub-samples. If the estimated model\nis not needed for identifying degenerate cases, `is_data_valid` should be used\nas it is called prior to fitting the model and thus leading to better\ncomputational performance.\n\nExamples:\n\nReferences:\n\nThe `TheilSenRegressor` estimator uses a generalization of the median in\nmultiple dimensions. It is thus robust to multivariate outliers. Note however\nthat the robustness of the estimator decreases quickly with the dimensionality\nof the problem. It loses its robustness properties and becomes no better than\nan ordinary least squares in high dimension.\n\nExamples:\n\nReferences:\n\n`TheilSenRegressor` is comparable to the Ordinary Least Squares (OLS) in terms\nof asymptotic efficiency and as an unbiased estimator. In contrast to OLS,\nTheil-Sen is a non-parametric method which means it makes no assumption about\nthe underlying distribution of the data. Since Theil-Sen is a median-based\nestimator, it is more robust against corrupted data aka outliers. In\nunivariate setting, Theil-Sen has a breakdown point of about 29.3% in case of\na simple linear regression which means that it can tolerate arbitrary\ncorrupted data of up to 29.3%.\n\nThe implementation of `TheilSenRegressor` in scikit-learn follows a\ngeneralization to a multivariate linear regression model 12 using the spatial\nmedian which is a generalization of the median to multiple dimensions 13.\n\nIn terms of time and space complexity, Theil-Sen scales according to\n\nwhich makes it infeasible to be applied exhaustively to problems with a large\nnumber of samples and features. Therefore, the magnitude of a subpopulation\ncan be chosen to limit the time and space complexity by considering only a\nrandom subset of all possible combinations.\n\nExamples:\n\nReferences:\n\nXin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: Theil-Sen Estimators in\na Multiple Linear Regression Model.\n\nThe `HuberRegressor` is different to `Ridge` because it applies a linear loss\nto samples that are classified as outliers. A sample is classified as an\ninlier if the absolute error of that sample is lesser than a certain\nthreshold. It differs from `TheilSenRegressor` and `RANSACRegressor` because\nit does not ignore the effect of the outliers but gives a lesser weight to\nthem.\n\nThe loss function that `HuberRegressor` minimizes is given by\n\nwhere\n\nIt is advised to set the parameter `epsilon` to 1.35 to achieve 95%\nstatistical efficiency.\n\nThe `HuberRegressor` differs from using `SGDRegressor` with loss set to\n`huber` in the following ways.\n\nExamples:\n\nReferences:\n\nNote that this estimator is different from the R implementation of Robust\nRegression (http://www.ats.ucla.edu/stat/r/dae/rreg.htm) because the R\nimplementation does a weighted least squares implementation with weights given\nto each sample on the basis of how much the residual is greater than a certain\nthreshold.\n\nOne common pattern within machine learning is to use linear models trained on\nnonlinear functions of the data. This approach maintains the generally fast\nperformance of linear methods, while allowing them to fit a much wider range\nof data.\n\nFor example, a simple linear regression can be extended by constructing\npolynomial features from the coefficients. In the standard linear regression\ncase, you might have a model that looks like this for two-dimensional data:\n\nIf we want to fit a paraboloid to the data instead of a plane, we can combine\nthe features in second-order polynomials, so that the model looks like this:\n\nThe (sometimes surprising) observation is that this is still a linear model:\nto see this, imagine creating a new set of features\n\nWith this re-labeling of the data, our problem can be written\n\nWe see that the resulting polynomial regression is in the same class of linear\nmodels we considered above (i.e. the model is linear in \\\\(w\\\\)) and can be\nsolved by the same techniques. By considering linear fits within a higher-\ndimensional space built with these basis functions, the model has the\nflexibility to fit a much broader range of data.\n\nHere is an example of applying this idea to one-dimensional data, using\npolynomial features of varying degrees:\n\nThis figure is created using the `PolynomialFeatures` transformer, which\ntransforms an input data matrix into a new data matrix of a given degree. It\ncan be used as follows:\n\nThe features of `X` have been transformed from \\\\([x_1, x_2]\\\\) to \\\\([1, x_1,\nx_2, x_1^2, x_1 x_2, x_2^2]\\\\), and can now be used within any linear model.\n\nThis sort of preprocessing can be streamlined with the Pipeline tools. A\nsingle object representing a simple polynomial regression can be created and\nused as follows:\n\nThe linear model trained on polynomial features is able to exactly recover the\ninput polynomial coefficients.\n\nIn some cases it\u2019s not necessary to include higher powers of any single\nfeature, but only the so-called interaction features that multiply together at\nmost \\\\(d\\\\) distinct features. These can be gotten from `PolynomialFeatures`\nwith the setting `interaction_only=True`.\n\nFor example, when dealing with boolean features, \\\\(x_i^n = x_i\\\\) for all\n\\\\(n\\\\) and is therefore useless; but \\\\(x_i x_j\\\\) represents the conjunction\nof two booleans. This way, we can solve the XOR problem with a linear\nclassifier:\n\nAnd the classifier \u201cpredictions\u201d are perfect:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.2. Linear and Quadratic Discriminant Analysis", "path": "modules/lda_qda", "type": "Guide", "text": "\nLinear Discriminant Analysis (`LinearDiscriminantAnalysis`) and Quadratic\nDiscriminant Analysis (`QuadraticDiscriminantAnalysis`) are two classic\nclassifiers, with, as their names suggest, a linear and a quadratic decision\nsurface, respectively.\n\nThese classifiers are attractive because they have closed-form solutions that\ncan be easily computed, are inherently multiclass, have proven to work well in\npractice, and have no hyperparameters to tune.\n\nThe plot shows decision boundaries for Linear Discriminant Analysis and\nQuadratic Discriminant Analysis. The bottom row demonstrates that Linear\nDiscriminant Analysis can only learn linear boundaries, while Quadratic\nDiscriminant Analysis can learn quadratic boundaries and is therefore more\nflexible.\n\nExamples:\n\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid:\nComparison of LDA and QDA on synthetic data.\n\n`LinearDiscriminantAnalysis` can be used to perform supervised dimensionality\nreduction, by projecting the input data to a linear subspace consisting of the\ndirections which maximize the separation between classes (in a precise sense\ndiscussed in the mathematics section below). The dimension of the output is\nnecessarily less than the number of classes, so this is in general a rather\nstrong dimensionality reduction, and only makes sense in a multiclass setting.\n\nThis is implemented in the `transform` method. The desired dimensionality can\nbe set using the `n_components` parameter. This parameter has no influence on\nthe `fit` and `predict` methods.\n\nExamples:\n\nComparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and\nPCA for dimensionality reduction of the Iris dataset\n\nBoth LDA and QDA can be derived from simple probabilistic models which model\nthe class conditional distribution of the data \\\\(P(X|y=k)\\\\) for each class\n\\\\(k\\\\). Predictions can then be obtained by using Bayes\u2019 rule, for each\ntraining sample \\\\(x \\in \\mathcal{R}^d\\\\):\n\nand we select the class \\\\(k\\\\) which maximizes this posterior probability.\n\nMore specifically, for linear and quadratic discriminant analysis,\n\\\\(P(x|y)\\\\) is modeled as a multivariate Gaussian distribution with density:\n\nwhere \\\\(d\\\\) is the number of features.\n\nAccording to the model above, the log of the posterior is:\n\nwhere the constant term \\\\(Cst\\\\) corresponds to the denominator \\\\(P(x)\\\\),\nin addition to other constant terms from the Gaussian. The predicted class is\nthe one that maximises this log-posterior.\n\nNote\n\nRelation with Gaussian Naive Bayes\n\nIf in the QDA model one assumes that the covariance matrices are diagonal,\nthen the inputs are assumed to be conditionally independent in each class, and\nthe resulting classifier is equivalent to the Gaussian Naive Bayes classifier\n`naive_bayes.GaussianNB`.\n\nLDA is a special case of QDA, where the Gaussians for each class are assumed\nto share the same covariance matrix: \\\\(\\Sigma_k = \\Sigma\\\\) for all \\\\(k\\\\).\nThis reduces the log posterior to:\n\nThe term \\\\((x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)\\\\) corresponds to the\nMahalanobis Distance between the sample \\\\(x\\\\) and the mean \\\\(\\mu_k\\\\). The\nMahalanobis distance tells how close \\\\(x\\\\) is from \\\\(\\mu_k\\\\), while also\naccounting for the variance of each feature. We can thus interpret LDA as\nassigning \\\\(x\\\\) to the class whose mean is the closest in terms of\nMahalanobis distance, while also accounting for the class prior probabilities.\n\nThe log-posterior of LDA can also be written 3 as:\n\nwhere \\\\(\\omega_k = \\Sigma^{-1} \\mu_k\\\\) and \\\\(\\omega_{k0} = -\\frac{1}{2}\n\\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)\\\\). These quantities correspond to\nthe `coef_` and `intercept_` attributes, respectively.\n\nFrom the above formula, it is clear that LDA has a linear decision surface. In\nthe case of QDA, there are no assumptions on the covariance matrices\n\\\\(\\Sigma_k\\\\) of the Gaussians, leading to quadratic decision surfaces. See 1\nfor more details.\n\nFirst note that the K means \\\\(\\mu_k\\\\) are vectors in \\\\(\\mathcal{R}^d\\\\),\nand they lie in an affine subspace \\\\(H\\\\) of dimension at least \\\\(K - 1\\\\)\n(2 points lie on a line, 3 points lie on a plane, etc).\n\nAs mentioned above, we can interpret LDA as assigning \\\\(x\\\\) to the class\nwhose mean \\\\(\\mu_k\\\\) is the closest in terms of Mahalanobis distance, while\nalso accounting for the class prior probabilities. Alternatively, LDA is\nequivalent to first sphering the data so that the covariance matrix is the\nidentity, and then assigning \\\\(x\\\\) to the closest mean in terms of Euclidean\ndistance (still accounting for the class priors).\n\nComputing Euclidean distances in this d-dimensional space is equivalent to\nfirst projecting the data points into \\\\(H\\\\), and computing the distances\nthere (since the other dimensions will contribute equally to each class in\nterms of distance). In other words, if \\\\(x\\\\) is closest to \\\\(\\mu_k\\\\) in\nthe original space, it will also be the case in \\\\(H\\\\). This shows that,\nimplicit in the LDA classifier, there is a dimensionality reduction by linear\nprojection onto a \\\\(K-1\\\\) dimensional space.\n\nWe can reduce the dimension even more, to a chosen \\\\(L\\\\), by projecting onto\nthe linear subspace \\\\(H_L\\\\) which maximizes the variance of the\n\\\\(\\mu^*_k\\\\) after projection (in effect, we are doing a form of PCA for the\ntransformed class means \\\\(\\mu^*_k\\\\)). This \\\\(L\\\\) corresponds to the\n`n_components` parameter used in the `transform` method. See 1 for more\ndetails.\n\nShrinkage is a form of regularization used to improve the estimation of\ncovariance matrices in situations where the number of training samples is\nsmall compared to the number of features. In this scenario, the empirical\nsample covariance is a poor estimator, and shrinkage helps improving the\ngeneralization performance of the classifier. Shrinkage LDA can be used by\nsetting the `shrinkage` parameter of the `LinearDiscriminantAnalysis` class to\n\u2018auto\u2019. This automatically determines the optimal shrinkage parameter in an\nanalytic way following the lemma introduced by Ledoit and Wolf 2. Note that\ncurrently shrinkage only works when setting the `solver` parameter to \u2018lsqr\u2019\nor \u2018eigen\u2019.\n\nThe `shrinkage` parameter can also be manually set between 0 and 1. In\nparticular, a value of 0 corresponds to no shrinkage (which means the\nempirical covariance matrix will be used) and a value of 1 corresponds to\ncomplete shrinkage (which means that the diagonal matrix of variances will be\nused as an estimate for the covariance matrix). Setting this parameter to a\nvalue between these two extrema will estimate a shrunk version of the\ncovariance matrix.\n\nThe shrinked Ledoit and Wolf estimator of covariance may not always be the\nbest choice. For example if the distribution of the data is normally\ndistributed, the Oracle Shrinkage Approximating estimator\n`sklearn.covariance.OAS` yields a smaller Mean Squared Error than the one\ngiven by Ledoit and Wolf\u2019s formula used with shrinkage=\u201dauto\u201d. In LDA, the\ndata are assumed to be gaussian conditionally to the class. If these\nassumptions hold, using LDA with the OAS estimator of covariance will yield a\nbetter classification accuracy than if Ledoit and Wolf or the empirical\ncovariance estimator is used.\n\nThe covariance estimator can be chosen using with the `covariance_estimator`\nparameter of the `discriminant_analysis.LinearDiscriminantAnalysis` class. A\ncovariance estimator should have a fit method and a `covariance_` attribute\nlike all covariance estimators in the `sklearn.covariance` module.\n\nExamples:\n\nNormal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification:\nComparison of LDA classifiers with Empirical, Ledoit Wolf and OAS covariance\nestimator.\n\nUsing LDA and QDA requires computing the log-posterior which depends on the\nclass priors \\\\(P(y=k)\\\\), the class means \\\\(\\mu_k\\\\), and the covariance\nmatrices.\n\nThe \u2018svd\u2019 solver is the default solver used for `LinearDiscriminantAnalysis`,\nand it is the only available solver for `QuadraticDiscriminantAnalysis`. It\ncan perform both classification and transform (for LDA). As it does not rely\non the calculation of the covariance matrix, the \u2018svd\u2019 solver may be\npreferable in situations where the number of features is large. The \u2018svd\u2019\nsolver cannot be used with shrinkage. For QDA, the use of the SVD solver\nrelies on the fact that the covariance matrix \\\\(\\Sigma_k\\\\) is, by\ndefinition, equal to \\\\(\\frac{1}{n - 1} X_k^tX_k = V S^2 V^t\\\\) where \\\\(V\\\\)\ncomes from the SVD of the (centered) matrix: \\\\(X_k = U S V^t\\\\). It turns out\nthat we can compute the log-posterior above without having to explictly\ncompute \\\\(\\Sigma\\\\): computing \\\\(S\\\\) and \\\\(V\\\\) via the SVD of \\\\(X\\\\) is\nenough. For LDA, two SVDs are computed: the SVD of the centered input matrix\n\\\\(X\\\\) and the SVD of the class-wise mean vectors.\n\nThe \u2018lsqr\u2019 solver is an efficient algorithm that only works for\nclassification. It needs to explicitly compute the covariance matrix\n\\\\(\\Sigma\\\\), and supports shrinkage and custom covariance estimators. This\nsolver computes the coefficients \\\\(\\omega_k = \\Sigma^{-1}\\mu_k\\\\) by solving\nfor \\\\(\\Sigma \\omega = \\mu_k\\\\), thus avoiding the explicit computation of the\ninverse \\\\(\\Sigma^{-1}\\\\).\n\nThe \u2018eigen\u2019 solver is based on the optimization of the between class scatter\nto within class scatter ratio. It can be used for both classification and\ntransform, and it supports shrinkage. However, the \u2018eigen\u2019 solver needs to\ncompute the covariance matrix, so it might not be suitable for situations with\na high number of features.\n\nReferences:\n\n\u201cThe Elements of Statistical Learning\u201d, Hastie T., Tibshirani R., Friedman J.,\nSection 4.3, p.106-119, 2008.\n\nLedoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of\nPortfolio Management 30(4), 110-119, 2004.\n\nR. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition),\nsection 2.6.2.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.3. Kernel ridge regression", "path": "modules/kernel_ridge", "type": "Guide", "text": "\nKernel ridge regression (KRR) [M2012] combines Ridge regression and\nclassification (linear least squares with l2-norm regularization) with the\nkernel trick. It thus learns a linear function in the space induced by the\nrespective kernel and the data. For non-linear kernels, this corresponds to a\nnon-linear function in the original space.\n\nThe form of the model learned by `KernelRidge` is identical to support vector\nregression (`SVR`). However, different loss functions are used: KRR uses\nsquared error loss while support vector regression uses\n\\\\(\\epsilon\\\\)-insensitive loss, both combined with l2 regularization. In\ncontrast to `SVR`, fitting `KernelRidge` can be done in closed-form and is\ntypically faster for medium-sized datasets. On the other hand, the learned\nmodel is non-sparse and thus slower than `SVR`, which learns a sparse model\nfor \\\\(\\epsilon > 0\\\\), at prediction-time.\n\nThe following figure compares `KernelRidge` and `SVR` on an artificial\ndataset, which consists of a sinusoidal target function and strong noise added\nto every fifth datapoint. The learned model of `KernelRidge` and `SVR` is\nplotted, where both complexity/regularization and bandwidth of the RBF kernel\nhave been optimized using grid-search. The learned functions are very similar;\nhowever, fitting `KernelRidge` is approximately seven times faster than\nfitting `SVR` (both with grid-search). However, prediction of 100000 target\nvalues is more than three times faster with `SVR` since it has learned a\nsparse model using only approximately 1/3 of the 100 training datapoints as\nsupport vectors.\n\nThe next figure compares the time for fitting and prediction of `KernelRidge`\nand `SVR` for different sizes of the training set. Fitting `KernelRidge` is\nfaster than `SVR` for medium-sized training sets (less than 1000 samples);\nhowever, for larger training sets `SVR` scales better. With regard to\nprediction time, `SVR` is faster than `KernelRidge` for all sizes of the\ntraining set because of the learned sparse solution. Note that the degree of\nsparsity and thus the prediction time depends on the parameters \\\\(\\epsilon\\\\)\nand \\\\(C\\\\) of the `SVR`; \\\\(\\epsilon = 0\\\\) would correspond to a dense\nmodel.\n\nReferences:\n\n\u201cMachine Learning: A Probabilistic Perspective\u201d Murphy, K. P. - chapter\n14.4.3, pp. 492-493, The MIT Press, 2012\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.4. Support Vector Machines", "path": "modules/svm", "type": "Guide", "text": "\nSupport vector machines (SVMs) are a set of supervised learning methods used\nfor classification, regression and outliers detection.\n\nThe advantages of support vector machines are:\n\nThe disadvantages of support vector machines include:\n\nThe support vector machines in scikit-learn support both dense\n(`numpy.ndarray` and convertible to that by `numpy.asarray`) and sparse (any\n`scipy.sparse`) sample vectors as input. However, to use an SVM to make\npredictions for sparse data, it must have been fit on such data. For optimal\nperformance, use C-ordered `numpy.ndarray` (dense) or\n`scipy.sparse.csr_matrix` (sparse) with `dtype=float64`.\n\n`SVC`, `NuSVC` and `LinearSVC` are classes capable of performing binary and\nmulti-class classification on a dataset.\n\n`SVC` and `NuSVC` are similar methods, but accept slightly different sets of\nparameters and have different mathematical formulations (see section\nMathematical formulation). On the other hand, `LinearSVC` is another (faster)\nimplementation of Support Vector Classification for the case of a linear\nkernel. Note that `LinearSVC` does not accept parameter `kernel`, as this is\nassumed to be linear. It also lacks some of the attributes of `SVC` and\n`NuSVC`, like `support_`.\n\nAs other classifiers, `SVC`, `NuSVC` and `LinearSVC` take as input two arrays:\nan array `X` of shape `(n_samples, n_features)` holding the training samples,\nand an array `y` of class labels (strings or integers), of shape\n`(n_samples)`:\n\nAfter being fitted, the model can then be used to predict new values:\n\nSVMs decision function (detailed in the Mathematical formulation) depends on\nsome subset of the training data, called the support vectors. Some properties\nof these support vectors can be found in attributes `support_vectors_`,\n`support_` and `n_support_`:\n\nExamples:\n\n`SVC` and `NuSVC` implement the \u201cone-versus-one\u201d approach for multi-class\nclassification. In total, `n_classes * (n_classes - 1) / 2` classifiers are\nconstructed and each one trains data from two classes. To provide a consistent\ninterface with other classifiers, the `decision_function_shape` option allows\nto monotonically transform the results of the \u201cone-versus-one\u201d classifiers to\na \u201cone-vs-rest\u201d decision function of shape `(n_samples, n_classes)`.\n\nOn the other hand, `LinearSVC` implements \u201cone-vs-the-rest\u201d multi-class\nstrategy, thus training `n_classes` models.\n\nSee Mathematical formulation for a complete description of the decision\nfunction.\n\nNote that the `LinearSVC` also implements an alternative multi-class strategy,\nthe so-called multi-class SVM formulated by Crammer and Singer 16, by using\nthe option `multi_class='crammer_singer'`. In practice, one-vs-rest\nclassification is usually preferred, since the results are mostly similar, but\nthe runtime is significantly less.\n\nFor \u201cone-vs-rest\u201d `LinearSVC` the attributes `coef_` and `intercept_` have the\nshape `(n_classes, n_features)` and `(n_classes,)` respectively. Each row of\nthe coefficients corresponds to one of the `n_classes` \u201cone-vs-rest\u201d\nclassifiers and similar for the intercepts, in the order of the \u201cone\u201d class.\n\nIn the case of \u201cone-vs-one\u201d `SVC` and `NuSVC`, the layout of the attributes is\na little more involved. In the case of a linear kernel, the attributes `coef_`\nand `intercept_` have the shape `(n_classes * (n_classes - 1) / 2,\nn_features)` and `(n_classes * (n_classes - 1) / 2)` respectively. This is\nsimilar to the layout for `LinearSVC` described above, with each row now\ncorresponding to a binary classifier. The order for classes 0 to n is \u201c0 vs\n1\u201d, \u201c0 vs 2\u201d , \u2026 \u201c0 vs n\u201d, \u201c1 vs 2\u201d, \u201c1 vs 3\u201d, \u201c1 vs n\u201d, . . . \u201cn-1 vs n\u201d.\n\nThe shape of `dual_coef_` is `(n_classes-1, n_SV)` with a somewhat hard to\ngrasp layout. The columns correspond to the support vectors involved in any of\nthe `n_classes * (n_classes - 1) / 2` \u201cone-vs-one\u201d classifiers. Each of the\nsupport vectors is used in `n_classes - 1` classifiers. The `n_classes - 1`\nentries in each row correspond to the dual coefficients for these classifiers.\n\nThis might be clearer with an example: consider a three class problem with\nclass 0 having three support vectors \\\\(v^{0}_0, v^{1}_0, v^{2}_0\\\\) and class\n1 and 2 having two support vectors \\\\(v^{0}_1, v^{1}_1\\\\) and \\\\(v^{0}_2,\nv^{1}_2\\\\) respectively. For each support vector \\\\(v^{j}_i\\\\), there are two\ndual coefficients. Let\u2019s call the coefficient of support vector \\\\(v^{j}_i\\\\)\nin the classifier between classes \\\\(i\\\\) and \\\\(k\\\\) \\\\(\\alpha^{j}_{i,k}\\\\).\nThen `dual_coef_` looks like this:\n\n\\\\(\\alpha^{0}_{0,1}\\\\)\n\n\\\\(\\alpha^{0}_{0,2}\\\\)\n\nCoefficients for SVs of class 0\n\n\\\\(\\alpha^{1}_{0,1}\\\\)\n\n\\\\(\\alpha^{1}_{0,2}\\\\)\n\n\\\\(\\alpha^{2}_{0,1}\\\\)\n\n\\\\(\\alpha^{2}_{0,2}\\\\)\n\n\\\\(\\alpha^{0}_{1,0}\\\\)\n\n\\\\(\\alpha^{0}_{1,2}\\\\)\n\nCoefficients for SVs of class 1\n\n\\\\(\\alpha^{1}_{1,0}\\\\)\n\n\\\\(\\alpha^{1}_{1,2}\\\\)\n\n\\\\(\\alpha^{0}_{2,0}\\\\)\n\n\\\\(\\alpha^{0}_{2,1}\\\\)\n\nCoefficients for SVs of class 2\n\n\\\\(\\alpha^{1}_{2,0}\\\\)\n\n\\\\(\\alpha^{1}_{2,1}\\\\)\n\nExamples:\n\nThe `decision_function` method of `SVC` and `NuSVC` gives per-class scores for\neach sample (or a single score per sample in the binary case). When the\nconstructor option `probability` is set to `True`, class membership\nprobability estimates (from the methods `predict_proba` and\n`predict_log_proba`) are enabled. In the binary case, the probabilities are\ncalibrated using Platt scaling 9: logistic regression on the SVM\u2019s scores, fit\nby an additional cross-validation on the training data. In the multiclass\ncase, this is extended as per 10.\n\nNote\n\nThe same probability calibration procedure is available for all estimators via\nthe `CalibratedClassifierCV` (see Probability calibration). In the case of\n`SVC` and `NuSVC`, this procedure is builtin in libsvm which is used under the\nhood, so it does not rely on scikit-learn\u2019s `CalibratedClassifierCV`.\n\nThe cross-validation involved in Platt scaling is an expensive operation for\nlarge datasets. In addition, the probability estimates may be inconsistent\nwith the scores:\n\nPlatt\u2019s method is also known to have theoretical issues. If confidence scores\nare required, but these do not have to be probabilities, then it is advisable\nto set `probability=False` and use `decision_function` instead of\n`predict_proba`.\n\nPlease note that when `decision_function_shape='ovr'` and `n_classes > 2`,\nunlike `decision_function`, the `predict` method does not try to break ties by\ndefault. You can set `break_ties=True` for the output of `predict` to be the\nsame as `np.argmax(clf.decision_function(...), axis=1)`, otherwise the first\nclass among the tied classes will always be returned; but have in mind that it\ncomes with a computational cost. See SVM Tie Breaking Example for an example\non tie breaking.\n\nIn problems where it is desired to give more importance to certain classes or\ncertain individual samples, the parameters `class_weight` and `sample_weight`\ncan be used.\n\n`SVC` (but not `NuSVC`) implements the parameter `class_weight` in the `fit`\nmethod. It\u2019s a dictionary of the form `{class_label : value}`, where value is\na floating point number > 0 that sets the parameter `C` of class `class_label`\nto `C * value`. The figure below illustrates the decision boundary of an\nunbalanced problem, with and without weight correction.\n\n`SVC`, `NuSVC`, `SVR`, `NuSVR`, `LinearSVC`, `LinearSVR` and `OneClassSVM`\nimplement also weights for individual samples in the `fit` method through the\n`sample_weight` parameter. Similar to `class_weight`, this sets the parameter\n`C` for the i-th example to `C * sample_weight[i]`, which will encourage the\nclassifier to get these samples right. The figure below illustrates the effect\nof sample weighting on the decision boundary. The size of the circles is\nproportional to the sample weights:\n\nExamples:\n\nThe method of Support Vector Classification can be extended to solve\nregression problems. This method is called Support Vector Regression.\n\nThe model produced by support vector classification (as described above)\ndepends only on a subset of the training data, because the cost function for\nbuilding the model does not care about training points that lie beyond the\nmargin. Analogously, the model produced by Support Vector Regression depends\nonly on a subset of the training data, because the cost function ignores\nsamples whose prediction is close to their target.\n\nThere are three different implementations of Support Vector Regression: `SVR`,\n`NuSVR` and `LinearSVR`. `LinearSVR` provides a faster implementation than\n`SVR` but only considers the linear kernel, while `NuSVR` implements a\nslightly different formulation than `SVR` and `LinearSVR`. See Implementation\ndetails for further details.\n\nAs with classification classes, the fit method will take as argument vectors\nX, y, only that in this case y is expected to have floating point values\ninstead of integer values:\n\nExamples:\n\nThe class `OneClassSVM` implements a One-Class SVM which is used in outlier\ndetection.\n\nSee Novelty and Outlier Detection for the description and usage of\nOneClassSVM.\n\nSupport Vector Machines are powerful tools, but their compute and storage\nrequirements increase rapidly with the number of training vectors. The core of\nan SVM is a quadratic programming problem (QP), separating support vectors\nfrom the rest of the training data. The QP solver used by the libsvm-based\nimplementation scales between \\\\(O(n_{features} \\times n_{samples}^2)\\\\) and\n\\\\(O(n_{features} \\times n_{samples}^3)\\\\) depending on how efficiently the\nlibsvm cache is used in practice (dataset dependent). If the data is very\nsparse \\\\(n_{features}\\\\) should be replaced by the average number of non-zero\nfeatures in a sample vector.\n\nFor the linear case, the algorithm used in `LinearSVC` by the liblinear\nimplementation is much more efficient than its libsvm-based `SVC` counterpart\nand can scale almost linearly to millions of samples and/or features.\n\nAvoiding data copy: For `SVC`, `SVR`, `NuSVC` and `NuSVR`, if the data passed\nto certain methods is not C-ordered contiguous and double precision, it will\nbe copied before calling the underlying C implementation. You can check\nwhether a given numpy array is C-contiguous by inspecting its `flags`\nattribute.\n\nFor `LinearSVC` (and `LogisticRegression`) any input passed as a numpy array\nwill be copied and converted to the liblinear internal sparse data\nrepresentation (double precision floats and int32 indices of non-zero\ncomponents). If you want to fit a large-scale linear classifier without\ncopying a dense numpy C-contiguous double precision array as input, we suggest\nto use the `SGDClassifier` class instead. The objective function can be\nconfigured to be almost the same as the `LinearSVC` model.\n\nSetting C: `C` is `1` by default and it\u2019s a reasonable default choice. If you\nhave a lot of noisy observations you should decrease it: decreasing C\ncorresponds to more regularization.\n\n`LinearSVC` and `LinearSVR` are less sensitive to `C` when it becomes large,\nand prediction results stop improving after a certain threshold. Meanwhile,\nlarger `C` values will take more time to train, sometimes up to 10 times\nlonger, as shown in 11.\n\nSupport Vector Machine algorithms are not scale invariant, so it is highly\nrecommended to scale your data. For example, scale each attribute on the input\nvector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1.\nNote that the same scaling must be applied to the test vector to obtain\nmeaningful results. This can be done easily by using a `Pipeline`:\n\nSee section Preprocessing data for more details on scaling and normalization.\n\nRandomness of the underlying implementations: The underlying implementations\nof `SVC` and `NuSVC` use a random number generator only to shuffle the data\nfor probability estimation (when `probability` is set to `True`). This\nrandomness can be controlled with the `random_state` parameter. If\n`probability` is set to `False` these estimators are not random and\n`random_state` has no effect on the results. The underlying `OneClassSVM`\nimplementation is similar to the ones of `SVC` and `NuSVC`. As no probability\nestimation is provided for `OneClassSVM`, it is not random.\n\nThe underlying `LinearSVC` implementation uses a random number generator to\nselect features when fitting the model with a dual coordinate descent (i.e\nwhen `dual` is set to `True`). It is thus not uncommon to have slightly\ndifferent results for the same input data. If that happens, try with a smaller\n`tol` parameter. This randomness can also be controlled with the\n`random_state` parameter. When `dual` is set to `False` the underlying\nimplementation of `LinearSVC` is not random and `random_state` has no effect\non the results.\n\nThe kernel function can be any of the following:\n\nDifferent kernels are specified by the `kernel` parameter:\n\nWhen training an SVM with the Radial Basis Function (RBF) kernel, two\nparameters must be considered: `C` and `gamma`. The parameter `C`, common to\nall SVM kernels, trades off misclassification of training examples against\nsimplicity of the decision surface. A low `C` makes the decision surface\nsmooth, while a high `C` aims at classifying all training examples correctly.\n`gamma` defines how much influence a single training example has. The larger\n`gamma` is, the closer other examples must be to be affected.\n\nProper choice of `C` and `gamma` is critical to the SVM\u2019s performance. One is\nadvised to use `GridSearchCV` with `C` and `gamma` spaced exponentially far\napart to choose good values.\n\nExamples:\n\nYou can define your own kernels by either giving the kernel as a python\nfunction or by precomputing the Gram matrix.\n\nClassifiers with custom kernels behave the same way as any other classifiers,\nexcept that:\n\nYou can use your own defined kernels by passing a function to the `kernel`\nparameter.\n\nYour kernel must take as arguments two matrices of shape `(n_samples_1,\nn_features)`, `(n_samples_2, n_features)` and return a kernel matrix of shape\n`(n_samples_1, n_samples_2)`.\n\nThe following code defines a linear kernel and creates a classifier instance\nthat will use that kernel:\n\nExamples:\n\nYou can pass pre-computed kernels by using the `kernel='precomputed'` option.\nYou should then pass Gram matrix instead of X to the `fit` and `predict`\nmethods. The kernel values between all training vectors and the test vectors\nmust be provided:\n\nA support vector machine constructs a hyper-plane or set of hyper-planes in a\nhigh or infinite dimensional space, which can be used for classification,\nregression or other tasks. Intuitively, a good separation is achieved by the\nhyper-plane that has the largest distance to the nearest training data points\nof any class (so-called functional margin), since in general the larger the\nmargin the lower the generalization error of the classifier. The figure below\nshows the decision function for a linearly separable problem, with three\nsamples on the margin boundaries, called \u201csupport vectors\u201d:\n\nIn general, when the problem isn\u2019t linearly separable, the support vectors are\nthe samples within the margin boundaries.\n\nWe recommend 13 and 14 as good references for the theory and practicalities of\nSVMs.\n\nGiven training vectors \\\\(x_i \\in \\mathbb{R}^p\\\\), i=1,\u2026, n, in two classes,\nand a vector \\\\(y \\in \\\\{1, -1\\\\}^n\\\\), our goal is to find \\\\(w \\in\n\\mathbb{R}^p\\\\) and \\\\(b \\in \\mathbb{R}\\\\) such that the prediction given by\n\\\\(\\text{sign} (w^T\\phi(x) + b)\\\\) is correct for most samples.\n\nSVC solves the following primal problem:\n\nIntuitively, we\u2019re trying to maximize the margin (by minimizing \\\\(||w||^2 =\nw^Tw\\\\)), while incurring a penalty when a sample is misclassified or within\nthe margin boundary. Ideally, the value \\\\(y_i (w^T \\phi (x_i) + b)\\\\) would\nbe \\\\(\\geq 1\\\\) for all samples, which indicates a perfect prediction. But\nproblems are usually not always perfectly separable with a hyperplane, so we\nallow some samples to be at a distance \\\\(\\zeta_i\\\\) from their correct margin\nboundary. The penalty term `C` controls the strengh of this penalty, and as a\nresult, acts as an inverse regularization parameter (see note below).\n\nThe dual problem to the primal is\n\nwhere \\\\(e\\\\) is the vector of all ones, and \\\\(Q\\\\) is an \\\\(n\\\\) by \\\\(n\\\\)\npositive semidefinite matrix, \\\\(Q_{ij} \\equiv y_i y_j K(x_i, x_j)\\\\), where\n\\\\(K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)\\\\) is the kernel. The terms\n\\\\(\\alpha_i\\\\) are called the dual coefficients, and they are upper-bounded by\n\\\\(C\\\\). This dual representation highlights the fact that training vectors\nare implicitly mapped into a higher (maybe infinite) dimensional space by the\nfunction \\\\(\\phi\\\\): see kernel trick.\n\nOnce the optimization problem is solved, the output of decision_function for a\ngiven sample \\\\(x\\\\) becomes:\n\nand the predicted class correspond to its sign. We only need to sum over the\nsupport vectors (i.e. the samples that lie within the margin) because the dual\ncoefficients \\\\(\\alpha_i\\\\) are zero for the other samples.\n\nThese parameters can be accessed through the attributes `dual_coef_` which\nholds the product \\\\(y_i \\alpha_i\\\\), `support_vectors_` which holds the\nsupport vectors, and `intercept_` which holds the independent term \\\\(b\\\\)\n\nNote\n\nWhile SVM models derived from libsvm and liblinear use `C` as regularization\nparameter, most other estimators use `alpha`. The exact equivalence between\nthe amount of regularization of two models depends on the exact objective\nfunction optimized by the model. For example, when the estimator used is\n`Ridge` regression, the relation between them is given as \\\\(C =\n\\frac{1}{alpha}\\\\).\n\nThe primal problem can be equivalently formulated as\n\nwhere we make use of the hinge loss. This is the form that is directly\noptimized by `LinearSVC`, but unlike the dual form, this one does not involve\ninner products between samples, so the famous kernel trick cannot be applied.\nThis is why only the linear kernel is supported by `LinearSVC` (\\\\(\\phi\\\\) is\nthe identity function).\n\nThe \\\\(\\nu\\\\)-SVC formulation 15 is a reparameterization of the \\\\(C\\\\)-SVC\nand therefore mathematically equivalent.\n\nWe introduce a new parameter \\\\(\\nu\\\\) (instead of \\\\(C\\\\)) which controls the\nnumber of support vectors and margin errors: \\\\(\\nu \\in (0, 1]\\\\) is an upper\nbound on the fraction of margin errors and a lower bound of the fraction of\nsupport vectors. A margin error corresponds to a sample that lies on the wrong\nside of its margin boundary: it is either misclassified, or it is correctly\nclassified but does not lie beyond the margin.\n\nGiven training vectors \\\\(x_i \\in \\mathbb{R}^p\\\\), i=1,\u2026, n, and a vector \\\\(y\n\\in \\mathbb{R}^n\\\\) \\\\(\\varepsilon\\\\)-SVR solves the following primal problem:\n\nHere, we are penalizing samples whose prediction is at least \\\\(\\varepsilon\\\\)\naway from their true target. These samples penalize the objective by\n\\\\(\\zeta_i\\\\) or \\\\(\\zeta_i^*\\\\), depending on whether their predictions lie\nabove or below the \\\\(\\varepsilon\\\\) tube.\n\nThe dual problem is\n\nwhere \\\\(e\\\\) is the vector of all ones, \\\\(Q\\\\) is an \\\\(n\\\\) by \\\\(n\\\\)\npositive semidefinite matrix, \\\\(Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^T \\phi\n(x_j)\\\\) is the kernel. Here training vectors are implicitly mapped into a\nhigher (maybe infinite) dimensional space by the function \\\\(\\phi\\\\).\n\nThe prediction is:\n\nThese parameters can be accessed through the attributes `dual_coef_` which\nholds the difference \\\\(\\alpha_i - \\alpha_i^*\\\\), `support_vectors_` which\nholds the support vectors, and `intercept_` which holds the independent term\n\\\\(b\\\\)\n\nThe primal problem can be equivalently formulated as\n\nwhere we make use of the epsilon-insensitive loss, i.e. errors of less than\n\\\\(\\varepsilon\\\\) are ignored. This is the form that is directly optimized by\n`LinearSVR`.\n\nInternally, we use libsvm 12 and liblinear 11 to handle all computations.\nThese libraries are wrapped using C and Cython. For a description of the\nimplementation and details of the algorithms used, please refer to their\nrespective papers.\n\nReferences:\n\nPlatt \u201cProbabilistic outputs for SVMs and comparisons to regularized\nlikelihood methods\u201d.\n\nWu, Lin and Weng, \u201cProbability estimates for multi-class classification by\npairwise coupling\u201d, JMLR 5:975-1005, 2004.\n\nFan, Rong-En, et al., \u201cLIBLINEAR: A library for large linear classification.\u201d,\nJournal of machine learning research 9.Aug (2008): 1871-1874.\n\nChang and Lin, LIBSVM: A Library for Support Vector Machines.\n\nBishop, Pattern recognition and machine learning, chapter 7 Sparse Kernel\nMachines\n\n\u201cA Tutorial on Support Vector Regression\u201d, Alex J. Smola, Bernhard Sch\u00f6lkopf -\nStatistics and Computing archive Volume 14 Issue 3, August 2004, p. 199-222.\n\nSch\u00f6lkopf et. al New Support Vector Algorithms\n\nCrammer and Singer On the Algorithmic Implementation ofMulticlass Kernel-based\nVector Machines, JMLR 2001.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.5. Stochastic Gradient Descent", "path": "modules/sgd", "type": "Guide", "text": "\nStochastic Gradient Descent (SGD) is a simple yet very efficient approach to\nfitting linear classifiers and regressors under convex loss functions such as\n(linear) Support Vector Machines and Logistic Regression. Even though SGD has\nbeen around in the machine learning community for a long time, it has received\na considerable amount of attention just recently in the context of large-scale\nlearning.\n\nSGD has been successfully applied to large-scale and sparse machine learning\nproblems often encountered in text classification and natural language\nprocessing. Given that the data is sparse, the classifiers in this module\neasily scale to problems with more than 10^5 training examples and more than\n10^5 features.\n\nStrictly speaking, SGD is merely an optimization technique and does not\ncorrespond to a specific family of machine learning models. It is only a way\nto train a model. Often, an instance of `SGDClassifier` or `SGDRegressor` will\nhave an equivalent estimator in the scikit-learn API, potentially using a\ndifferent optimization technique. For example, using\n`SGDClassifier(loss='log')` results in logistic regression, i.e. a model\nequivalent to `LogisticRegression` which is fitted via SGD instead of being\nfitted by one of the other solvers in `LogisticRegression`. Similarly,\n`SGDRegressor(loss='squared_loss', penalty='l2')` and `Ridge` solve the same\noptimization problem, via different means.\n\nThe advantages of Stochastic Gradient Descent are:\n\nThe disadvantages of Stochastic Gradient Descent include:\n\nWarning\n\nMake sure you permute (shuffle) your training data before fitting the model or\nuse `shuffle=True` to shuffle after each iteration (used by default). Also,\nideally, features should be standardized using e.g.\n`make_pipeline(StandardScaler(), SGDClassifier())` (see Pipelines).\n\nThe class `SGDClassifier` implements a plain stochastic gradient descent\nlearning routine which supports different loss functions and penalties for\nclassification. Below is the decision boundary of a `SGDClassifier` trained\nwith the hinge loss, equivalent to a linear SVM.\n\nAs other classifiers, SGD has to be fitted with two arrays: an array `X` of\nshape (n_samples, n_features) holding the training samples, and an array y of\nshape (n_samples,) holding the target values (class labels) for the training\nsamples:\n\nAfter being fitted, the model can then be used to predict new values:\n\nSGD fits a linear model to the training data. The `coef_` attribute holds the\nmodel parameters:\n\nThe `intercept_` attribute holds the intercept (aka offset or bias):\n\nWhether or not the model should use an intercept, i.e. a biased hyperplane, is\ncontrolled by the parameter `fit_intercept`.\n\nThe signed distance to the hyperplane (computed as the dot product between the\ncoefficients and the input sample, plus the intercept) is given by\n`SGDClassifier.decision_function`:\n\nThe concrete loss function can be set via the `loss` parameter.\n`SGDClassifier` supports the following loss functions:\n\nPlease refer to the mathematical section below for formulas. The first two\nloss functions are lazy, they only update the model parameters if an example\nviolates the margin constraint, which makes training very efficient and may\nresult in sparser models (i.e. with more zero coefficents), even when L2\npenalty is used.\n\nUsing `loss=\"log\"` or `loss=\"modified_huber\"` enables the `predict_proba`\nmethod, which gives a vector of probability estimates \\\\(P(y|x)\\\\) per sample\n\\\\(x\\\\):\n\nThe concrete penalty can be set via the `penalty` parameter. SGD supports the\nfollowing penalties:\n\nThe default setting is `penalty=\"l2\"`. The L1 penalty leads to sparse\nsolutions, driving most coefficients to zero. The Elastic Net 11 solves some\ndeficiencies of the L1 penalty in the presence of highly correlated\nattributes. The parameter `l1_ratio` controls the convex combination of L1 and\nL2 penalty.\n\n`SGDClassifier` supports multi-class classification by combining multiple\nbinary classifiers in a \u201cone versus all\u201d (OVA) scheme. For each of the \\\\(K\\\\)\nclasses, a binary classifier is learned that discriminates between that and\nall other \\\\(K-1\\\\) classes. At testing time, we compute the confidence score\n(i.e. the signed distances to the hyperplane) for each classifier and choose\nthe class with the highest confidence. The Figure below illustrates the OVA\napproach on the iris dataset. The dashed lines represent the three OVA\nclassifiers; the background colors show the decision surface induced by the\nthree classifiers.\n\nIn the case of multi-class classification `coef_` is a two-dimensional array\nof shape (n_classes, n_features) and `intercept_` is a one-dimensional array\nof shape (n_classes,). The i-th row of `coef_` holds the weight vector of the\nOVA classifier for the i-th class; classes are indexed in ascending order (see\nattribute `classes_`). Note that, in principle, since they allow to create a\nprobability model, `loss=\"log\"` and `loss=\"modified_huber\"` are more suitable\nfor one-vs-all classification.\n\n`SGDClassifier` supports both weighted classes and weighted instances via the\nfit parameters `class_weight` and `sample_weight`. See the examples below and\nthe docstring of `SGDClassifier.fit` for further information.\n\n`SGDClassifier` supports averaged SGD (ASGD) 10. Averaging can be enabled by\nsetting `average=True`. ASGD performs the same updates as the regular SGD (see\nMathematical formulation), but instead of using the last value of the\ncoefficients as the `coef_` attribute (i.e. the values of the last update),\n`coef_` is set instead to the average value of the coefficients across all\nupdates. The same is done for the `intercept_` attribute. When using ASGD the\nlearning rate can be larger and even constant, leading on some datasets to a\nspeed up in training time.\n\nFor classification with a logistic loss, another variant of SGD with an\naveraging strategy is available with Stochastic Average Gradient (SAG)\nalgorithm, available as a solver in `LogisticRegression`.\n\nExamples:\n\nThe class `SGDRegressor` implements a plain stochastic gradient descent\nlearning routine which supports different loss functions and penalties to fit\nlinear regression models. `SGDRegressor` is well suited for regression\nproblems with a large number of training samples (> 10.000), for other\nproblems we recommend `Ridge`, `Lasso`, or `ElasticNet`.\n\nThe concrete loss function can be set via the `loss` parameter. `SGDRegressor`\nsupports the following loss functions:\n\nPlease refer to the mathematical section below for formulas. The Huber and\nepsilon-insensitive loss functions can be used for robust regression. The\nwidth of the insensitive region has to be specified via the parameter\n`epsilon`. This parameter depends on the scale of the target variables.\n\nThe `penalty` parameter determines the regularization to be used (see\ndescription above in the classification section).\n\n`SGDRegressor` also supports averaged SGD 10 (here again, see description\nabove in the classification section).\n\nFor regression with a squared loss and a l2 penalty, another variant of SGD\nwith an averaging strategy is available with Stochastic Average Gradient (SAG)\nalgorithm, available as a solver in `Ridge`.\n\nNote\n\nThe sparse implementation produces slightly different results from the dense\nimplementation, due to a shrunk learning rate for the intercept. See\nImplementation details.\n\nThere is built-in support for sparse data given in any matrix in a format\nsupported by scipy.sparse. For maximum efficiency, however, use the CSR matrix\nformat as defined in scipy.sparse.csr_matrix.\n\nExamples:\n\nThe major advantage of SGD is its efficiency, which is basically linear in the\nnumber of training examples. If X is a matrix of size (n, p) training has a\ncost of \\\\(O(k n \\bar p)\\\\), where k is the number of iterations (epochs) and\n\\\\(\\bar p\\\\) is the average number of non-zero attributes per sample.\n\nRecent theoretical results, however, show that the runtime to get some desired\noptimization accuracy does not increase as the training set size increases.\n\nThe classes `SGDClassifier` and `SGDRegressor` provide two criteria to stop\nthe algorithm when a given level of convergence is reached:\n\nIn both cases, the criterion is evaluated once by epoch, and the algorithm\nstops when the criterion does not improve `n_iter_no_change` times in a row.\nThe improvement is evaluated with absolute tolerance `tol`, and the algorithm\nstops in any case after a maximum number of iteration `max_iter`.\n\nStochastic Gradient Descent is sensitive to feature scaling, so it is highly\nrecommended to scale your data. For example, scale each attribute on the input\nvector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1.\nNote that the same scaling must be applied to the test vector to obtain\nmeaningful results. This can be easily done using `StandardScaler`:\n\nIf your attributes have an intrinsic scale (e.g. word frequencies or indicator\nfeatures) scaling is not needed.\n\nReferences:\n\nWe describe here the mathematical details of the SGD procedure. A good\noverview with convergence rates can be found in 12.\n\nGiven a set of training examples \\\\((x_1, y_1), \\ldots, (x_n, y_n)\\\\) where\n\\\\(x_i \\in \\mathbf{R}^m\\\\) and \\\\(y_i \\in \\mathcal{R}\\\\) (\\\\(y_i \\in {-1,\n1}\\\\) for classification), our goal is to learn a linear scoring function\n\\\\(f(x) = w^T x + b\\\\) with model parameters \\\\(w \\in \\mathbf{R}^m\\\\) and\nintercept \\\\(b \\in \\mathbf{R}\\\\). In order to make predictions for binary\nclassification, we simply look at the sign of \\\\(f(x)\\\\). To find the model\nparameters, we minimize the regularized training error given by\n\nwhere \\\\(L\\\\) is a loss function that measures model (mis)fit and \\\\(R\\\\) is a\nregularization term (aka penalty) that penalizes model complexity; \\\\(\\alpha >\n0\\\\) is a non-negative hyperparameter that controls the regularization\nstength.\n\nDifferent choices for \\\\(L\\\\) entail different classifiers or regressors:\n\nAll of the above loss functions can be regarded as an upper bound on the\nmisclassification error (Zero-one loss) as shown in the Figure below.\n\nPopular choices for the regularization term \\\\(R\\\\) (the `penalty` parameter)\ninclude:\n\nThe Figure below shows the contours of the different regularization terms in a\n2-dimensional parameter space (\\\\(m=2\\\\)) when \\\\(R(w) = 1\\\\).\n\nStochastic gradient descent is an optimization method for unconstrained\noptimization problems. In contrast to (batch) gradient descent, SGD\napproximates the true gradient of \\\\(E(w,b)\\\\) by considering a single\ntraining example at a time.\n\nThe class `SGDClassifier` implements a first-order SGD learning routine. The\nalgorithm iterates over the training examples and for each example updates the\nmodel parameters according to the update rule given by\n\nwhere \\\\(\\eta\\\\) is the learning rate which controls the step-size in the\nparameter space. The intercept \\\\(b\\\\) is updated similarly but without\nregularization (and with additional decay for sparse matrices, as detailed in\nImplementation details).\n\nThe learning rate \\\\(\\eta\\\\) can be either constant or gradually decaying. For\nclassification, the default learning rate schedule (`learning_rate='optimal'`)\nis given by\n\nwhere \\\\(t\\\\) is the time step (there are a total of `n_samples * n_iter` time\nsteps), \\\\(t_0\\\\) is determined based on a heuristic proposed by L\u00e9on Bottou\nsuch that the expected initial updates are comparable with the expected size\nof the weights (this assuming that the norm of the training samples is approx.\n1). The exact definition can be found in `_init_t` in `BaseSGD`.\n\nFor regression the default learning rate schedule is inverse scaling\n(`learning_rate='invscaling'`), given by\n\nwhere \\\\(eta_0\\\\) and \\\\(power\\\\_t\\\\) are hyperparameters chosen by the user\nvia `eta0` and `power_t`, resp.\n\nFor a constant learning rate use `learning_rate='constant'` and use `eta0` to\nspecify the learning rate.\n\nFor an adaptively decreasing learning rate, use `learning_rate='adaptive'` and\nuse `eta0` to specify the starting learning rate. When the stopping criterion\nis reached, the learning rate is divided by 5, and the algorithm does not\nstop. The algorithm stops when the learning rate goes below 1e-6.\n\nThe model parameters can be accessed through the `coef_` and `intercept_`\nattributes: `coef_` holds the weights \\\\(w\\\\) and `intercept_` holds \\\\(b\\\\).\n\nWhen using Averaged SGD (with the `average` parameter), `coef_` is set to the\naverage weight across all updates: `coef_` \\\\(= \\frac{1}{T} \\sum_{t=0}^{T-1}\nw^{(t)}\\\\), where \\\\(T\\\\) is the total number of updates, found in the `t_`\nattribute.\n\nThe implementation of SGD is influenced by the `Stochastic Gradient SVM` of 7.\nSimilar to SvmSGD, the weight vector is represented as the product of a scalar\nand a vector which allows an efficient weight update in the case of L2\nregularization. In the case of sparse input `X`, the intercept is updated with\na smaller learning rate (multiplied by 0.01) to account for the fact that it\nis updated more frequently. Training examples are picked up sequentially and\nthe learning rate is lowered after each observed example. We adopted the\nlearning rate schedule from 8. For multi-class classification, a \u201cone versus\nall\u201d approach is used. We use the truncated gradient algorithm proposed in 9\nfor L1 regularization (and the Elastic Net). The code is written in Cython.\n\nReferences:\n\n\u201cStochastic Gradient Descent\u201d L. Bottou - Website, 2010.\n\n\u201cPegasos: Primal estimated sub-gradient solver for svm\u201d S. Shalev-Shwartz, Y.\nSinger, N. Srebro - In Proceedings of ICML \u201807.\n\n\u201cStochastic gradient descent training for l1-regularized log-linear models\nwith cumulative penalty\u201d Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings\nof the AFNLP/ACL \u201809.\n\n\u201cTowards Optimal One Pass Large Scale Learning with Averaged Stochastic\nGradient Descent\u201d Xu, Wei\n\n\u201cRegularization and variable selection via the elastic net\u201d H. Zou, T. Hastie\n- Journal of the Royal Statistical Society Series B, 67 (2), 301-320.\n\n\u201cSolving large scale linear prediction problems using stochastic gradient\ndescent algorithms\u201d T. Zhang - In Proceedings of ICML \u201804.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.6. Nearest Neighbors", "path": "modules/neighbors", "type": "Guide", "text": "\n`sklearn.neighbors` provides functionality for unsupervised and supervised\nneighbors-based learning methods. Unsupervised nearest neighbors is the\nfoundation of many other learning methods, notably manifold learning and\nspectral clustering. Supervised neighbors-based learning comes in two flavors:\nclassification for data with discrete labels, and regression for data with\ncontinuous labels.\n\nThe principle behind nearest neighbor methods is to find a predefined number\nof training samples closest in distance to the new point, and predict the\nlabel from these. The number of samples can be a user-defined constant\n(k-nearest neighbor learning), or vary based on the local density of points\n(radius-based neighbor learning). The distance can, in general, be any metric\nmeasure: standard Euclidean distance is the most common choice. Neighbors-\nbased methods are known as non-generalizing machine learning methods, since\nthey simply \u201cremember\u201d all of its training data (possibly transformed into a\nfast indexing structure such as a Ball Tree or KD Tree).\n\nDespite its simplicity, nearest neighbors has been successful in a large\nnumber of classification and regression problems, including handwritten digits\nand satellite image scenes. Being a non-parametric method, it is often\nsuccessful in classification situations where the decision boundary is very\nirregular.\n\nThe classes in `sklearn.neighbors` can handle either NumPy arrays or\n`scipy.sparse` matrices as input. For dense matrices, a large number of\npossible distance metrics are supported. For sparse matrices, arbitrary\nMinkowski metrics are supported for searches.\n\nThere are many learning routines which rely on nearest neighbors at their\ncore. One example is kernel density estimation, discussed in the density\nestimation section.\n\n`NearestNeighbors` implements unsupervised nearest neighbors learning. It acts\nas a uniform interface to three different nearest neighbors algorithms:\n`BallTree`, `KDTree`, and a brute-force algorithm based on routines in\n`sklearn.metrics.pairwise`. The choice of neighbors search algorithm is\ncontrolled through the keyword `'algorithm'`, which must be one of `['auto',\n'ball_tree', 'kd_tree', 'brute']`. When the default value `'auto'` is passed,\nthe algorithm attempts to determine the best approach from the training data.\nFor a discussion of the strengths and weaknesses of each option, see Nearest\nNeighbor Algorithms.\n\nWarning\n\nRegarding the Nearest Neighbors algorithms, if two neighbors \\\\(k+1\\\\) and\n\\\\(k\\\\) have identical distances but different labels, the result will depend\non the ordering of the training data.\n\nFor the simple task of finding the nearest neighbors between two sets of data,\nthe unsupervised algorithms within `sklearn.neighbors` can be used:\n\nBecause the query set matches the training set, the nearest neighbor of each\npoint is the point itself, at a distance of zero.\n\nIt is also possible to efficiently produce a sparse graph showing the\nconnections between neighboring points:\n\nThe dataset is structured such that points nearby in index order are nearby in\nparameter space, leading to an approximately block-diagonal matrix of\nK-nearest neighbors. Such a sparse graph is useful in a variety of\ncircumstances which make use of spatial relationships between points for\nunsupervised learning: in particular, see `Isomap`, `LocallyLinearEmbedding`,\nand `SpectralClustering`.\n\nAlternatively, one can use the `KDTree` or `BallTree` classes directly to find\nnearest neighbors. This is the functionality wrapped by the `NearestNeighbors`\nclass used above. The Ball Tree and KD Tree have the same interface; we\u2019ll\nshow an example of using the KD Tree here:\n\nRefer to the `KDTree` and `BallTree` class documentation for more information\non the options available for nearest neighbors searches, including\nspecification of query strategies, distance metrics, etc. For a list of\navailable metrics, see the documentation of the `DistanceMetric` class.\n\nNeighbors-based classification is a type of instance-based learning or non-\ngeneralizing learning: it does not attempt to construct a general internal\nmodel, but simply stores instances of the training data. Classification is\ncomputed from a simple majority vote of the nearest neighbors of each point: a\nquery point is assigned the data class which has the most representatives\nwithin the nearest neighbors of the point.\n\nscikit-learn implements two different nearest neighbors classifiers:\n`KNeighborsClassifier` implements learning based on the \\\\(k\\\\) nearest\nneighbors of each query point, where \\\\(k\\\\) is an integer value specified by\nthe user. `RadiusNeighborsClassifier` implements learning based on the number\nof neighbors within a fixed radius \\\\(r\\\\) of each training point, where\n\\\\(r\\\\) is a floating-point value specified by the user.\n\nThe \\\\(k\\\\)-neighbors classification in `KNeighborsClassifier` is the most\ncommonly used technique. The optimal choice of the value \\\\(k\\\\) is highly\ndata-dependent: in general a larger \\\\(k\\\\) suppresses the effects of noise,\nbut makes the classification boundaries less distinct.\n\nIn cases where the data is not uniformly sampled, radius-based neighbors\nclassification in `RadiusNeighborsClassifier` can be a better choice. The user\nspecifies a fixed radius \\\\(r\\\\), such that points in sparser neighborhoods\nuse fewer nearest neighbors for the classification. For high-dimensional\nparameter spaces, this method becomes less effective due to the so-called\n\u201ccurse of dimensionality\u201d.\n\nThe basic nearest neighbors classification uses uniform weights: that is, the\nvalue assigned to a query point is computed from a simple majority vote of the\nnearest neighbors. Under some circumstances, it is better to weight the\nneighbors such that nearer neighbors contribute more to the fit. This can be\naccomplished through the `weights` keyword. The default value, `weights =\n'uniform'`, assigns uniform weights to each neighbor. `weights = 'distance'`\nassigns weights proportional to the inverse of the distance from the query\npoint. Alternatively, a user-defined function of the distance can be supplied\nto compute the weights.\n\nExamples:\n\nNeighbors-based regression can be used in cases where the data labels are\ncontinuous rather than discrete variables. The label assigned to a query point\nis computed based on the mean of the labels of its nearest neighbors.\n\nscikit-learn implements two different neighbors regressors:\n`KNeighborsRegressor` implements learning based on the \\\\(k\\\\) nearest\nneighbors of each query point, where \\\\(k\\\\) is an integer value specified by\nthe user. `RadiusNeighborsRegressor` implements learning based on the\nneighbors within a fixed radius \\\\(r\\\\) of the query point, where \\\\(r\\\\) is a\nfloating-point value specified by the user.\n\nThe basic nearest neighbors regression uses uniform weights: that is, each\npoint in the local neighborhood contributes uniformly to the classification of\na query point. Under some circumstances, it can be advantageous to weight\npoints such that nearby points contribute more to the regression than faraway\npoints. This can be accomplished through the `weights` keyword. The default\nvalue, `weights = 'uniform'`, assigns equal weights to all points. `weights =\n'distance'` assigns weights proportional to the inverse of the distance from\nthe query point. Alternatively, a user-defined function of the distance can be\nsupplied, which will be used to compute the weights.\n\nThe use of multi-output nearest neighbors for regression is demonstrated in\nFace completion with a multi-output estimators. In this example, the inputs X\nare the pixels of the upper half of faces and the outputs Y are the pixels of\nthe lower half of those faces.\n\nExamples:\n\nFast computation of nearest neighbors is an active area of research in machine\nlearning. The most naive neighbor search implementation involves the brute-\nforce computation of distances between all pairs of points in the dataset: for\n\\\\(N\\\\) samples in \\\\(D\\\\) dimensions, this approach scales as \\\\(O[D N^2]\\\\).\nEfficient brute-force neighbors searches can be very competitive for small\ndata samples. However, as the number of samples \\\\(N\\\\) grows, the brute-force\napproach quickly becomes infeasible. In the classes within\n`sklearn.neighbors`, brute-force neighbors searches are specified using the\nkeyword `algorithm = 'brute'`, and are computed using the routines available\nin `sklearn.metrics.pairwise`.\n\nTo address the computational inefficiencies of the brute-force approach, a\nvariety of tree-based data structures have been invented. In general, these\nstructures attempt to reduce the required number of distance calculations by\nefficiently encoding aggregate distance information for the sample. The basic\nidea is that if point \\\\(A\\\\) is very distant from point \\\\(B\\\\), and point\n\\\\(B\\\\) is very close to point \\\\(C\\\\), then we know that points \\\\(A\\\\) and\n\\\\(C\\\\) are very distant, without having to explicitly calculate their\ndistance. In this way, the computational cost of a nearest neighbors search\ncan be reduced to \\\\(O[D N \\log(N)]\\\\) or better. This is a significant\nimprovement over brute-force for large \\\\(N\\\\).\n\nAn early approach to taking advantage of this aggregate information was the KD\ntree data structure (short for K-dimensional tree), which generalizes two-\ndimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of\ndimensions. The KD tree is a binary tree structure which recursively\npartitions the parameter space along the data axes, dividing it into nested\northotropic regions into which data points are filed. The construction of a KD\ntree is very fast: because partitioning is performed only along the data axes,\nno \\\\(D\\\\)-dimensional distances need to be computed. Once constructed, the\nnearest neighbor of a query point can be determined with only \\\\(O[\\log(N)]\\\\)\ndistance computations. Though the KD tree approach is very fast for low-\ndimensional (\\\\(D < 20\\\\)) neighbors searches, it becomes inefficient as\n\\\\(D\\\\) grows very large: this is one manifestation of the so-called \u201ccurse of\ndimensionality\u201d. In scikit-learn, KD tree neighbors searches are specified\nusing the keyword `algorithm = 'kd_tree'`, and are computed using the class\n`KDTree`.\n\nReferences:\n\nTo address the inefficiencies of KD Trees in higher dimensions, the ball tree\ndata structure was developed. Where KD trees partition data along Cartesian\naxes, ball trees partition data in a series of nesting hyper-spheres. This\nmakes tree construction more costly than that of the KD tree, but results in a\ndata structure which can be very efficient on highly structured data, even in\nvery high dimensions.\n\nA ball tree recursively divides the data into nodes defined by a centroid\n\\\\(C\\\\) and radius \\\\(r\\\\), such that each point in the node lies within the\nhyper-sphere defined by \\\\(r\\\\) and \\\\(C\\\\). The number of candidate points\nfor a neighbor search is reduced through use of the triangle inequality:\n\nWith this setup, a single distance calculation between a test point and the\ncentroid is sufficient to determine a lower and upper bound on the distance to\nall points within the node. Because of the spherical geometry of the ball tree\nnodes, it can out-perform a KD-tree in high dimensions, though the actual\nperformance is highly dependent on the structure of the training data. In\nscikit-learn, ball-tree-based neighbors searches are specified using the\nkeyword `algorithm = 'ball_tree'`, and are computed using the class\n`BallTree`. Alternatively, the user can work with the `BallTree` class\ndirectly.\n\nReferences:\n\nThe optimal algorithm for a given dataset is a complicated choice, and depends\non a number of factors:\n\nnumber of samples \\\\(N\\\\) (i.e. `n_samples`) and dimensionality \\\\(D\\\\) (i.e.\n`n_features`).\n\nFor small data sets (\\\\(N\\\\) less than 30 or so), \\\\(\\log(N)\\\\) is comparable\nto \\\\(N\\\\), and brute force algorithms can be more efficient than a tree-based\napproach. Both `KDTree` and `BallTree` address this through providing a leaf\nsize parameter: this controls the number of samples at which a query switches\nto brute-force. This allows both algorithms to approach the efficiency of a\nbrute-force computation for small \\\\(N\\\\).\n\ndata structure: intrinsic dimensionality of the data and/or sparsity of the\ndata. Intrinsic dimensionality refers to the dimension \\\\(d \\le D\\\\) of a\nmanifold on which the data lies, which can be linearly or non-linearly\nembedded in the parameter space. Sparsity refers to the degree to which the\ndata fills the parameter space (this is to be distinguished from the concept\nas used in \u201csparse\u201d matrices. The data matrix may have no zero entries, but\nthe structure can still be \u201csparse\u201d in this sense).\n\nDatasets used in machine learning tend to be very structured, and are very\nwell-suited for tree-based queries.\n\nnumber of neighbors \\\\(k\\\\) requested for a query point.\n\nAs \\\\(k\\\\) becomes large compared to \\\\(N\\\\), the ability to prune branches in\na tree-based query is reduced. In this situation, Brute force queries can be\nmore efficient.\n\nCurrently, `algorithm = 'auto'` selects `'brute'` if any of the following\nconditions are verified:\n\nOtherwise, it selects the first out of `'kd_tree'` and `'ball_tree'` that has\n`effective_metric_` in its `VALID_METRICS` list. This heuristic is based on\nthe following assumptions:\n\nAs noted above, for small sample sizes a brute force search can be more\nefficient than a tree-based query. This fact is accounted for in the ball tree\nand KD tree by internally switching to brute force searches within leaf nodes.\nThe level of this switch can be specified with the parameter `leaf_size`. This\nparameter choice has many effects:\n\nA larger `leaf_size` leads to a faster tree construction time, because fewer\nnodes need to be created\n\nBoth a large or small `leaf_size` can lead to suboptimal query cost. For\n`leaf_size` approaching 1, the overhead involved in traversing nodes can\nsignificantly slow query times. For `leaf_size` approaching the size of the\ntraining set, queries become essentially brute force. A good compromise\nbetween these is `leaf_size = 30`, the default value of the parameter.\n\nAs `leaf_size` increases, the memory required to store a tree structure\ndecreases. This is especially important in the case of ball tree, which stores\na \\\\(D\\\\)-dimensional centroid for each node. The required storage space for\n`BallTree` is approximately `1 / leaf_size` times the size of the training\nset.\n\n`leaf_size` is not referenced for brute force queries.\n\nThe `NearestCentroid` classifier is a simple algorithm that represents each\nclass by the centroid of its members. In effect, this makes it similar to the\nlabel updating phase of the `KMeans` algorithm. It also has no parameters to\nchoose, making it a good baseline classifier. It does, however, suffer on non-\nconvex classes, as well as when classes have drastically different variances,\nas equal variance in all dimensions is assumed. See Linear Discriminant\nAnalysis (`LinearDiscriminantAnalysis`) and Quadratic Discriminant Analysis\n(`QuadraticDiscriminantAnalysis`) for more complex methods that do not make\nthis assumption. Usage of the default `NearestCentroid` is simple:\n\nThe `NearestCentroid` classifier has a `shrink_threshold` parameter, which\nimplements the nearest shrunken centroid classifier. In effect, the value of\neach feature for each centroid is divided by the within-class variance of that\nfeature. The feature values are then reduced by `shrink_threshold`. Most\nnotably, if a particular feature value crosses zero, it is set to zero. In\neffect, this removes the feature from affecting the classification. This is\nuseful, for example, for removing noisy features.\n\nIn the example below, using a small shrink threshold increases the accuracy of\nthe model from 0.81 to 0.82.\n\nExamples:\n\nMany scikit-learn estimators rely on nearest neighbors: Several classifiers\nand regressors such as `KNeighborsClassifier` and `KNeighborsRegressor`, but\nalso some clustering methods such as `DBSCAN` and `SpectralClustering`, and\nsome manifold embeddings such as `TSNE` and `Isomap`.\n\nAll these estimators can compute internally the nearest neighbors, but most of\nthem also accept precomputed nearest neighbors sparse graph, as given by\n`kneighbors_graph` and `radius_neighbors_graph`. With mode\n`mode='connectivity'`, these functions return a binary adjacency sparse graph\nas required, for instance, in `SpectralClustering`. Whereas with\n`mode='distance'`, they return a distance sparse graph as required, for\ninstance, in `DBSCAN`. To include these functions in a scikit-learn pipeline,\none can also use the corresponding classes `KNeighborsTransformer` and\n`RadiusNeighborsTransformer`. The benefits of this sparse graph API are\nmultiple.\n\nFirst, the precomputed graph can be re-used multiple times, for instance while\nvarying a parameter of the estimator. This can be done manually by the user,\nor using the caching properties of the scikit-learn pipeline:\n\nSecond, precomputing the graph can give finer control on the nearest neighbors\nestimation, for instance enabling multiprocessing though the parameter\n`n_jobs`, which might not be available in all estimators.\n\nFinally, the precomputation can be performed by custom estimators to use\ndifferent implementations, such as approximate nearest neighbors methods, or\nimplementation with special data types. The precomputed neighbors sparse graph\nneeds to be formatted as in `radius_neighbors_graph` output:\n\nNote\n\nWhen a specific number of neighbors is queried (using\n`KNeighborsTransformer`), the definition of `n_neighbors` is ambiguous since\nit can either include each training point as its own neighbor, or exclude\nthem. Neither choice is perfect, since including them leads to a different\nnumber of non-self neighbors during training and testing, while excluding them\nleads to a difference between `fit(X).transform(X)` and `fit_transform(X)`,\nwhich is against scikit-learn API. In `KNeighborsTransformer` we use the\ndefinition which includes each training point as its own neighbor in the count\nof `n_neighbors`. However, for compatibility reasons with other estimators\nwhich use the other definition, one extra neighbor will be computed when `mode\n== 'distance'`. To maximise compatibility with all estimators, a safe choice\nis to always include one extra neighbor in a custom nearest neighbors\nestimator, since unnecessary neighbors will be filtered by following\nestimators.\n\nExamples:\n\nNeighborhood Components Analysis (NCA, `NeighborhoodComponentsAnalysis`) is a\ndistance metric learning algorithm which aims to improve the accuracy of\nnearest neighbors classification compared to the standard Euclidean distance.\nThe algorithm directly maximizes a stochastic variant of the leave-one-out\nk-nearest neighbors (KNN) score on the training set. It can also learn a low-\ndimensional linear projection of data that can be used for data visualization\nand fast classification.\n\nIn the above illustrating figure, we consider some points from a randomly\ngenerated dataset. We focus on the stochastic KNN classification of point no.\n3. The thickness of a link between sample 3 and another point is proportional\nto their distance, and can be seen as the relative weight (or probability)\nthat a stochastic nearest neighbor prediction rule would assign to this point.\nIn the original space, sample 3 has many stochastic neighbors from various\nclasses, so the right class is not very likely. However, in the projected\nspace learned by NCA, the only stochastic neighbors with non-negligible weight\nare from the same class as sample 3, guaranteeing that the latter will be well\nclassified. See the mathematical formulation for more details.\n\nCombined with a nearest neighbors classifier (`KNeighborsClassifier`), NCA is\nattractive for classification because it can naturally handle multi-class\nproblems without any increase in the model size, and does not introduce\nadditional parameters that require fine-tuning by the user.\n\nNCA classification has been shown to work well in practice for data sets of\nvarying size and difficulty. In contrast to related methods such as Linear\nDiscriminant Analysis, NCA does not make any assumptions about the class\ndistributions. The nearest neighbor classification can naturally produce\nhighly irregular decision boundaries.\n\nTo use this model for classification, one needs to combine a\n`NeighborhoodComponentsAnalysis` instance that learns the optimal\ntransformation with a `KNeighborsClassifier` instance that performs the\nclassification in the projected space. Here is an example using the two\nclasses:\n\nThe plot shows decision boundaries for Nearest Neighbor Classification and\nNeighborhood Components Analysis classification on the iris dataset, when\ntraining and scoring on only two features, for visualisation purposes.\n\nNCA can be used to perform supervised dimensionality reduction. The input data\nare projected onto a linear subspace consisting of the directions which\nminimize the NCA objective. The desired dimensionality can be set using the\nparameter `n_components`. For instance, the following figure shows a\ncomparison of dimensionality reduction with Principal Component Analysis\n(`PCA`), Linear Discriminant Analysis (`LinearDiscriminantAnalysis`) and\nNeighborhood Component Analysis (`NeighborhoodComponentsAnalysis`) on the\nDigits dataset, a dataset with size \\\\(n_{samples} = 1797\\\\) and\n\\\\(n_{features} = 64\\\\). The data set is split into a training and a test set\nof equal size, then standardized. For evaluation the 3-nearest neighbor\nclassification accuracy is computed on the 2-dimensional projected points\nfound by each method. Each data sample belongs to one of 10 classes.\n\nExamples:\n\nThe goal of NCA is to learn an optimal linear transformation matrix of size\n`(n_components, n_features)`, which maximises the sum over all samples \\\\(i\\\\)\nof the probability \\\\(p_i\\\\) that \\\\(i\\\\) is correctly classified, i.e.:\n\nwith \\\\(N\\\\) = `n_samples` and \\\\(p_i\\\\) the probability of sample \\\\(i\\\\)\nbeing correctly classified according to a stochastic nearest neighbors rule in\nthe learned embedded space:\n\nwhere \\\\(C_i\\\\) is the set of points in the same class as sample \\\\(i\\\\), and\n\\\\(p_{i j}\\\\) is the softmax over Euclidean distances in the embedded space:\n\nNCA can be seen as learning a (squared) Mahalanobis distance metric:\n\nwhere \\\\(M = L^T L\\\\) is a symmetric positive semi-definite matrix of size\n`(n_features, n_features)`.\n\nThis implementation follows what is explained in the original paper 1. For the\noptimisation method, it currently uses scipy\u2019s L-BFGS-B with a full gradient\ncomputation at each iteration, to avoid to tune the learning rate and provide\nstable learning.\n\nSee the examples below and the docstring of\n`NeighborhoodComponentsAnalysis.fit` for further information.\n\nNCA stores a matrix of pairwise distances, taking `n_samples ** 2` memory.\nTime complexity depends on the number of iterations done by the optimisation\nalgorithm. However, one can set the maximum number of iterations with the\nargument `max_iter`. For each iteration, time complexity is `O(n_components x\nn_samples x min(n_samples, n_features))`.\n\nHere the `transform` operation returns \\\\(LX^T\\\\), therefore its time\ncomplexity equals `n_components * n_features * n_samples_test`. There is no\nadded space complexity in the operation.\n\nReferences:\n\n\u201cNeighbourhood Components Analysis\u201d, J. Goldberger, S. Roweis, G. Hinton, R.\nSalakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, May\n2005, pp. 513-520.\n\nWikipedia entry on Neighborhood Components Analysis\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.7. Gaussian Processes", "path": "modules/gaussian_process", "type": "Guide", "text": "\nGaussian Processes (GP) are a generic supervised learning method designed to\nsolve regression and probabilistic classification problems.\n\nThe advantages of Gaussian processes are:\n\nThe disadvantages of Gaussian processes include:\n\nThe `GaussianProcessRegressor` implements Gaussian processes (GP) for\nregression purposes. For this, the prior of the GP needs to be specified. The\nprior mean is assumed to be constant and zero (for `normalize_y=False`) or the\ntraining data\u2019s mean (for `normalize_y=True`). The prior\u2019s covariance is\nspecified by passing a kernel object. The hyperparameters of the kernel are\noptimized during fitting of GaussianProcessRegressor by maximizing the log-\nmarginal-likelihood (LML) based on the passed `optimizer`. As the LML may have\nmultiple local optima, the optimizer can be started repeatedly by specifying\n`n_restarts_optimizer`. The first run is always conducted starting from the\ninitial hyperparameter values of the kernel; subsequent runs are conducted\nfrom hyperparameter values that have been chosen randomly from the range of\nallowed values. If the initial hyperparameters should be kept fixed, `None`\ncan be passed as optimizer.\n\nThe noise level in the targets can be specified by passing it via the\nparameter `alpha`, either globally as a scalar or per datapoint. Note that a\nmoderate noise level can also be helpful for dealing with numeric issues\nduring fitting as it is effectively implemented as Tikhonov regularization,\ni.e., by adding it to the diagonal of the kernel matrix. An alternative to\nspecifying the noise level explicitly is to include a WhiteKernel component\ninto the kernel, which can estimate the global noise level from the data (see\nexample below).\n\nThe implementation is based on Algorithm 2.1 of [RW2006]. In addition to the\nAPI of standard scikit-learn estimators, GaussianProcessRegressor:\n\nThis example illustrates that GPR with a sum-kernel including a WhiteKernel\ncan estimate the noise level of data. An illustration of the log-marginal-\nlikelihood (LML) landscape shows that there exist two local maxima of LML.\n\nThe first corresponds to a model with a high noise level and a large length\nscale, which explains all variations in the data by noise.\n\nThe second one has a smaller noise level and shorter length scale, which\nexplains most of the variation by the noise-free functional relationship. The\nsecond model has a higher likelihood; however, depending on the initial value\nfor the hyperparameters, the gradient-based optimization might also converge\nto the high-noise solution. It is thus important to repeat the optimization\nseveral times for different initializations.\n\nBoth kernel ridge regression (KRR) and GPR learn a target function by\nemploying internally the \u201ckernel trick\u201d. KRR learns a linear function in the\nspace induced by the respective kernel which corresponds to a non-linear\nfunction in the original space. The linear function in the kernel space is\nchosen based on the mean-squared error loss with ridge regularization. GPR\nuses the kernel to define the covariance of a prior distribution over the\ntarget functions and uses the observed training data to define a likelihood\nfunction. Based on Bayes theorem, a (Gaussian) posterior distribution over\ntarget functions is defined, whose mean is used for prediction.\n\nA major difference is that GPR can choose the kernel\u2019s hyperparameters based\non gradient-ascent on the marginal likelihood function while KRR needs to\nperform a grid search on a cross-validated loss function (mean-squared error\nloss). A further difference is that GPR learns a generative, probabilistic\nmodel of the target function and can thus provide meaningful confidence\nintervals and posterior samples along with the predictions while KRR only\nprovides predictions.\n\nThe following figure illustrates both methods on an artificial dataset, which\nconsists of a sinusoidal target function and strong noise. The figure compares\nthe learned model of KRR and GPR based on a ExpSineSquared kernel, which is\nsuited for learning periodic functions. The kernel\u2019s hyperparameters control\nthe smoothness (length_scale) and periodicity of the kernel (periodicity).\nMoreover, the noise level of the data is learned explicitly by GPR by an\nadditional WhiteKernel component in the kernel and by the regularization\nparameter alpha of KRR.\n\nThe figure shows that both methods learn reasonable models of the target\nfunction. GPR correctly identifies the periodicity of the function to be\nroughly \\\\(2*\\pi\\\\) (6.28), while KRR chooses the doubled periodicity\n\\\\(4*\\pi\\\\) . Besides that, GPR provides reasonable confidence bounds on the\nprediction which are not available for KRR. A major difference between the two\nmethods is the time required for fitting and predicting: while fitting KRR is\nfast in principle, the grid-search for hyperparameter optimization scales\nexponentially with the number of hyperparameters (\u201ccurse of dimensionality\u201d).\nThe gradient-based optimization of the parameters in GPR does not suffer from\nthis exponential scaling and is thus considerably faster on this example with\n3-dimensional hyperparameter space. The time for predicting is similar;\nhowever, generating the variance of the predictive distribution of GPR takes\nconsiderably longer than just predicting the mean.\n\nThis example is based on Section 5.4.3 of [RW2006]. It illustrates an example\nof complex kernel engineering and hyperparameter optimization using gradient\nascent on the log-marginal-likelihood. The data consists of the monthly\naverage atmospheric CO2 concentrations (in parts per million by volume (ppmv))\ncollected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The\nobjective is to model the CO2 concentration as a function of the time t.\n\nThe kernel is composed of several terms that are responsible for explaining\ndifferent properties of the signal:\n\nMaximizing the log-marginal-likelihood after subtracting the target\u2019s mean\nyields the following kernel with an LML of -83.214:\n\nThus, most of the target signal (34.4ppm) is explained by a long-term rising\ntrend (length-scale 41.8 years). The periodic component has an amplitude of\n3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay\ntime indicates that we have a locally very close to periodic seasonal\ncomponent. The correlated noise has an amplitude of 0.197ppm with a length\nscale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the\noverall noise level is very small, indicating that the data can be very well\nexplained by the model. The figure shows also that the model makes very\nconfident predictions until around 2015\n\nThe `GaussianProcessClassifier` implements Gaussian processes (GP) for\nclassification purposes, more specifically for probabilistic classification,\nwhere test predictions take the form of class probabilities.\nGaussianProcessClassifier places a GP prior on a latent function \\\\(f\\\\),\nwhich is then squashed through a link function to obtain the probabilistic\nclassification. The latent function \\\\(f\\\\) is a so-called nuisance function,\nwhose values are not observed and are not relevant by themselves. Its purpose\nis to allow a convenient formulation of the model, and \\\\(f\\\\) is removed\n(integrated out) during prediction. GaussianProcessClassifier implements the\nlogistic link function, for which the integral cannot be computed analytically\nbut is easily approximated in the binary case.\n\nIn contrast to the regression setting, the posterior of the latent function\n\\\\(f\\\\) is not Gaussian even for a GP prior since a Gaussian likelihood is\ninappropriate for discrete class labels. Rather, a non-Gaussian likelihood\ncorresponding to the logistic link function (logit) is used.\nGaussianProcessClassifier approximates the non-Gaussian posterior with a\nGaussian based on the Laplace approximation. More details can be found in\nChapter 3 of [RW2006].\n\nThe GP prior mean is assumed to be zero. The prior\u2019s covariance is specified\nby passing a kernel object. The hyperparameters of the kernel are optimized\nduring fitting of GaussianProcessRegressor by maximizing the log-marginal-\nlikelihood (LML) based on the passed `optimizer`. As the LML may have multiple\nlocal optima, the optimizer can be started repeatedly by specifying\n`n_restarts_optimizer`. The first run is always conducted starting from the\ninitial hyperparameter values of the kernel; subsequent runs are conducted\nfrom hyperparameter values that have been chosen randomly from the range of\nallowed values. If the initial hyperparameters should be kept fixed, `None`\ncan be passed as optimizer.\n\n`GaussianProcessClassifier` supports multi-class classification by performing\neither one-versus-rest or one-versus-one based training and prediction. In\none-versus-rest, one binary Gaussian process classifier is fitted for each\nclass, which is trained to separate this class from the rest. In \u201cone_vs_one\u201d,\none binary Gaussian process classifier is fitted for each pair of classes,\nwhich is trained to separate these two classes. The predictions of these\nbinary predictors are combined into multi-class predictions. See the section\non multi-class classification for more details.\n\nIn the case of Gaussian process classification, \u201cone_vs_one\u201d might be\ncomputationally cheaper since it has to solve many problems involving only a\nsubset of the whole training set rather than fewer problems on the whole\ndataset. Since Gaussian process classification scales cubically with the size\nof the dataset, this might be considerably faster. However, note that\n\u201cone_vs_one\u201d does not support predicting probability estimates but only plain\npredictions. Moreover, note that `GaussianProcessClassifier` does not (yet)\nimplement a true multi-class Laplace approximation internally, but as\ndiscussed above is based on solving several binary classification tasks\ninternally, which are combined using one-versus-rest or one-versus-one.\n\nThis example illustrates the predicted probability of GPC for an RBF kernel\nwith different choices of the hyperparameters. The first figure shows the\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\nthe hyperparameters corresponding to the maximum log-marginal-likelihood\n(LML).\n\nWhile the hyperparameters chosen by optimizing LML have a considerably larger\nLML, they perform slightly worse according to the log-loss on test data. The\nfigure shows that this is because they exhibit a steep change of the class\nprobabilities at the class boundaries (which is good) but have predicted\nprobabilities close to 0.5 far away from the class boundaries (which is bad)\nThis undesirable effect is caused by the Laplace approximation used internally\nby GPC.\n\nThe second figure shows the log-marginal-likelihood for different choices of\nthe kernel\u2019s hyperparameters, highlighting the two choices of the\nhyperparameters used in the first figure by black dots.\n\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic\nkernel (`RBF`) and a non-stationary kernel (`DotProduct`). On this particular\ndataset, the `DotProduct` kernel obtains considerably better results because\nthe class-boundaries are linear and coincide with the coordinate axes. In\npractice, however, stationary kernels such as `RBF` often obtain better\nresults.\n\nThis example illustrates the predicted probability of GPC for an isotropic and\nanisotropic RBF kernel on a two-dimensional version for the iris-dataset. This\nillustrates the applicability of GPC to non-binary classification. The\nanisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\nassigning different length-scales to the two feature dimensions.\n\nKernels (also called \u201ccovariance functions\u201d in the context of GPs) are a\ncrucial ingredient of GPs which determine the shape of prior and posterior of\nthe GP. They encode the assumptions on the function being learned by defining\nthe \u201csimilarity\u201d of two datapoints combined with the assumption that similar\ndatapoints should have similar target values. Two categories of kernels can be\ndistinguished: stationary kernels depend only on the distance of two\ndatapoints and not on their absolute values \\\\(k(x_i, x_j)= k(d(x_i, x_j))\\\\)\nand are thus invariant to translations in the input space, while non-\nstationary kernels depend also on the specific values of the datapoints.\nStationary kernels can further be subdivided into isotropic and anisotropic\nkernels, where isotropic kernels are also invariant to rotations in the input\nspace. For more details, we refer to Chapter 4 of [RW2006]. For guidance on\nhow to best combine different kernels, we refer to [Duv2014].\n\nThe main usage of a `Kernel` is to compute the GP\u2019s covariance between\ndatapoints. For this, the method `__call__` of the kernel can be called. This\nmethod can either be used to compute the \u201cauto-covariance\u201d of all pairs of\ndatapoints in a 2d array X, or the \u201ccross-covariance\u201d of all combinations of\ndatapoints of a 2d array X with datapoints in a 2d array Y. The following\nidentity holds true for all kernels k (except for the `WhiteKernel`): `k(X) ==\nK(X, Y=X)`\n\nIf only the diagonal of the auto-covariance is being used, the method `diag()`\nof a kernel can be called, which is more computationally efficient than the\nequivalent call to `__call__`: `np.diag(k(X, X)) == k.diag(X)`\n\nKernels are parameterized by a vector \\\\(\\theta\\\\) of hyperparameters. These\nhyperparameters can for instance control length-scales or periodicity of a\nkernel (see below). All kernels support computing analytic gradients of the\nkernel\u2019s auto-covariance with respect to \\\\(log(\\theta)\\\\) via setting\n`eval_gradient=True` in the `__call__` method. That is, a `(len(X), len(X),\nlen(theta))` array is returned where the entry `[i, j, l]` contains\n\\\\(\\frac{\\partial k_\\theta(x_i, x_j)}{\\partial log(\\theta_l)}\\\\). This\ngradient is used by the Gaussian process (both regressor and classifier) in\ncomputing the gradient of the log-marginal-likelihood, which in turn is used\nto determine the value of \\\\(\\theta\\\\), which maximizes the log-marginal-\nlikelihood, via gradient ascent. For each hyperparameter, the initial value\nand the bounds need to be specified when creating an instance of the kernel.\nThe current value of \\\\(\\theta\\\\) can be get and set via the property `theta`\nof the kernel object. Moreover, the bounds of the hyperparameters can be\naccessed by the property `bounds` of the kernel. Note that both properties\n(theta and bounds) return log-transformed values of the internally used values\nsince those are typically more amenable to gradient-based optimization. The\nspecification of each hyperparameter is stored in the form of an instance of\n`Hyperparameter` in the respective kernel. Note that a kernel using a\nhyperparameter with name \u201cx\u201d must have the attributes self.x and\nself.x_bounds.\n\nThe abstract base class for all kernels is `Kernel`. Kernel implements a\nsimilar interface as `Estimator`, providing the methods `get_params()`,\n`set_params()`, and `clone()`. This allows setting kernel values also via\nmeta-estimators such as `Pipeline` or `GridSearch`. Note that due to the\nnested structure of kernels (by applying kernel operators, see below), the\nnames of kernel parameters might become relatively complicated. In general,\nfor a binary kernel operator, parameters of the left operand are prefixed with\n`k1__` and parameters of the right operand with `k2__`. An additional\nconvenience method is `clone_with_theta(theta)`, which returns a cloned\nversion of the kernel but with the hyperparameters set to `theta`. An\nillustrative example:\n\nAll Gaussian process kernels are interoperable with `sklearn.metrics.pairwise`\nand vice versa: instances of subclasses of `Kernel` can be passed as `metric`\nto `pairwise_kernels` from `sklearn.metrics.pairwise`. Moreover, kernel\nfunctions from pairwise can be used as GP kernels by using the wrapper class\n`PairwiseKernel`. The only caveat is that the gradient of the hyperparameters\nis not analytic but numeric and all those kernels support only isotropic\ndistances. The parameter `gamma` is considered to be a hyperparameter and may\nbe optimized. The other kernel parameters are set directly at initialization\nand are kept fixed.\n\nThe `ConstantKernel` kernel can be used as part of a `Product` kernel where it\nscales the magnitude of the other factor (kernel) or as part of a `Sum`\nkernel, where it modifies the mean of the Gaussian process. It depends on a\nparameter \\\\(constant\\\\_value\\\\). It is defined as:\n\nThe main use-case of the `WhiteKernel` kernel is as part of a sum-kernel where\nit explains the noise-component of the signal. Tuning its parameter\n\\\\(noise\\\\_level\\\\) corresponds to estimating the noise-level. It is defined\nas:\n\nKernel operators take one or two base kernels and combine them into a new\nkernel. The `Sum` kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and\ncombines them via \\\\(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\\\\). The `Product`\nkernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and combines them via\n\\\\(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\\\\). The `Exponentiation` kernel\ntakes one base kernel and a scalar parameter \\\\(p\\\\) and combines them via\n\\\\(k_{exp}(X, Y) = k(X, Y)^p\\\\). Note that magic methods `__add__`, `__mul___`\nand `__pow__` are overridden on the Kernel objects, so one can use e.g. `RBF()\n+ RBF()` as a shortcut for `Sum(RBF(), RBF())`.\n\nThe `RBF` kernel is a stationary kernel. It is also known as the \u201csquared\nexponential\u201d kernel. It is parameterized by a length-scale parameter\n\\\\(l>0\\\\), which can either be a scalar (isotropic variant of the kernel) or a\nvector with the same number of dimensions as the inputs \\\\(x\\\\) (anisotropic\nvariant of the kernel). The kernel is given by:\n\nwhere \\\\(d(\\cdot, \\cdot)\\\\) is the Euclidean distance. This kernel is\ninfinitely differentiable, which implies that GPs with this kernel as\ncovariance function have mean square derivatives of all orders, and are thus\nvery smooth. The prior and posterior of a GP resulting from an RBF kernel are\nshown in the following figure:\n\nThe `Matern` kernel is a stationary kernel and a generalization of the `RBF`\nkernel. It has an additional parameter \\\\(\\nu\\\\) which controls the smoothness\nof the resulting function. It is parameterized by a length-scale parameter\n\\\\(l>0\\\\), which can either be a scalar (isotropic variant of the kernel) or a\nvector with the same number of dimensions as the inputs \\\\(x\\\\) (anisotropic\nvariant of the kernel). The kernel is given by:\n\nwhere \\\\(d(\\cdot,\\cdot)\\\\) is the Euclidean distance, \\\\(K_\\nu(\\cdot)\\\\) is a\nmodified Bessel function and \\\\(\\Gamma(\\cdot)\\\\) is the gamma function. As\n\\\\(\\nu\\rightarrow\\infty\\\\), the Mat\u00e9rn kernel converges to the RBF kernel.\nWhen \\\\(\\nu = 1/2\\\\), the Mat\u00e9rn kernel becomes identical to the absolute\nexponential kernel, i.e.,\n\nIn particular, \\\\(\\nu = 3/2\\\\):\n\nand \\\\(\\nu = 5/2\\\\):\n\nare popular choices for learning functions that are not infinitely\ndifferentiable (as assumed by the RBF kernel) but at least once (\\\\(\\nu =\n3/2\\\\)) or twice differentiable (\\\\(\\nu = 5/2\\\\)).\n\nThe flexibility of controlling the smoothness of the learned function via\n\\\\(\\nu\\\\) allows adapting to the properties of the true underlying functional\nrelation. The prior and posterior of a GP resulting from a Mat\u00e9rn kernel are\nshown in the following figure:\n\nSee [RW2006], pp84 for further details regarding the different variants of the\nMat\u00e9rn kernel.\n\nThe `RationalQuadratic` kernel can be seen as a scale mixture (an infinite\nsum) of `RBF` kernels with different characteristic length-scales. It is\nparameterized by a length-scale parameter \\\\(l>0\\\\) and a scale mixture\nparameter \\\\(\\alpha>0\\\\) Only the isotropic variant where \\\\(l\\\\) is a scalar\nis supported at the moment. The kernel is given by:\n\nThe prior and posterior of a GP resulting from a `RationalQuadratic` kernel\nare shown in the following figure:\n\nThe `ExpSineSquared` kernel allows modeling periodic functions. It is\nparameterized by a length-scale parameter \\\\(l>0\\\\) and a periodicity\nparameter \\\\(p>0\\\\). Only the isotropic variant where \\\\(l\\\\) is a scalar is\nsupported at the moment. The kernel is given by:\n\nThe prior and posterior of a GP resulting from an ExpSineSquared kernel are\nshown in the following figure:\n\nThe `DotProduct` kernel is non-stationary and can be obtained from linear\nregression by putting \\\\(N(0, 1)\\\\) priors on the coefficients of \\\\(x_d (d =\n1, . . . , D)\\\\) and a prior of \\\\(N(0, \\sigma_0^2)\\\\) on the bias. The\n`DotProduct` kernel is invariant to a rotation of the coordinates about the\norigin, but not translations. It is parameterized by a parameter\n\\\\(\\sigma_0^2\\\\). For \\\\(\\sigma_0^2 = 0\\\\), the kernel is called the\nhomogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given\nby\n\nThe `DotProduct` kernel is commonly combined with exponentiation. An example\nwith exponent 2 is shown in the following figure:\n\nCarl Eduard Rasmussen and Christopher K.I. Williams, \u201cGaussian Processes for\nMachine Learning\u201d, MIT Press 2006, Link to an official complete PDF version of\nthe book here .\n\nDavid Duvenaud, \u201cThe Kernel Cookbook: Advice on Covariance functions\u201d, 2014,\nLink .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.8. Cross decomposition", "path": "modules/cross_decomposition", "type": "Guide", "text": "\nThe cross decomposition module contains supervised estimators for\ndimensionality reduction and regression, belonging to the \u201cPartial Least\nSquares\u201d family.\n\nCross decomposition algorithms find the fundamental relations between two\nmatrices (X and Y). They are latent variable approaches to modeling the\ncovariance structures in these two spaces. They will try to find the\nmultidimensional direction in the X space that explains the maximum\nmultidimensional variance direction in the Y space. In other words, PLS\nprojects both `X` and `Y` into a lower-dimensional subspace such that the\ncovariance between `transformed(X)` and `transformed(Y)` is maximal.\n\nPLS draws similarities with Principal Component Regression (PCR), where the\nsamples are first projected into a lower-dimensional subspace, and the targets\n`y` are predicted using `transformed(X)`. One issue with PCR is that the\ndimensionality reduction is unsupervized, and may lose some important\nvariables: PCR would keep the features with the most variance, but it\u2019s\npossible that features with a small variances are relevant from predicting the\ntarget. In a way, PLS allows for the same kind of dimensionality reduction,\nbut by taking into account the targets `y`. An illustration of this fact is\ngiven in the following example: * Principal Component Regression vs Partial\nLeast Squares Regression.\n\nApart from CCA, the PLS estimators are particularly suited when the matrix of\npredictors has more variables than observations, and when there is\nmulticollinearity among the features. By contrast, standard linear regression\nwould fail in these cases unless it is regularized.\n\nClasses included in this module are `PLSRegression`, `PLSCanonical`, `CCA` and\n`PLSSVD`\n\nWe here describe the algorithm used in `PLSCanonical`. The other estimators\nuse variants of this algorithm, and are detailed below. We recommend section 1\nfor more details and comparisons between these algorithms. In 1,\n`PLSCanonical` corresponds to \u201cPLSW2A\u201d.\n\nGiven two centered matrices \\\\(X \\in \\mathbb{R}^{n \\times d}\\\\) and \\\\(Y \\in\n\\mathbb{R}^{n \\times t}\\\\), and a number of components \\\\(K\\\\), `PLSCanonical`\nproceeds as follows:\n\nSet \\\\(X_1\\\\) to \\\\(X\\\\) and \\\\(Y_1\\\\) to \\\\(Y\\\\). Then, for each \\\\(k \\in [1,\nK]\\\\):\n\nAt the end, we have approximated \\\\(X\\\\) as a sum of rank-1 matrices: \\\\(X =\n\\Xi \\Gamma^T\\\\) where \\\\(\\Xi \\in \\mathbb{R}^{n \\times K}\\\\) contains the\nscores in its columns, and \\\\(\\Gamma^T \\in \\mathbb{R}^{K \\times d}\\\\) contains\nthe loadings in its rows. Similarly for \\\\(Y\\\\), we have \\\\(Y = \\Omega\n\\Delta^T\\\\).\n\nNote that the scores matrices \\\\(\\Xi\\\\) and \\\\(\\Omega\\\\) correspond to the\nprojections of the training data \\\\(X\\\\) and \\\\(Y\\\\), respectively.\n\nStep a) may be performed in two ways: either by computing the whole SVD of\n\\\\(C\\\\) and only retain the singular vectors with the biggest singular values,\nor by directly computing the singular vectors using the power method (cf\nsection 11.3 in 1), which corresponds to the `'nipals'` option of the\n`algorithm` parameter.\n\nTo transform \\\\(X\\\\) into \\\\(\\bar{X}\\\\), we need to find a projection matrix\n\\\\(P\\\\) such that \\\\(\\bar{X} = XP\\\\). We know that for the training data,\n\\\\(\\Xi = XP\\\\), and \\\\(X = \\Xi \\Gamma^T\\\\). Setting \\\\(P = U(\\Gamma^T\nU)^{-1}\\\\) where \\\\(U\\\\) is the matrix with the \\\\(u_k\\\\) in the columns, we\nhave \\\\(XP = X U(\\Gamma^T U)^{-1} = \\Xi (\\Gamma^T U) (\\Gamma^T U)^{-1} =\n\\Xi\\\\) as desired. The rotation matrix \\\\(P\\\\) can be accessed from the\n`x_rotations_` attribute.\n\nSimilarly, \\\\(Y\\\\) can be transformed using the rotation matrix \\\\(V(\\Delta^T\nV)^{-1}\\\\), accessed via the `y_rotations_` attribute.\n\nTo predict the targets of some data \\\\(X\\\\), we are looking for a coefficient\nmatrix \\\\(\\beta \\in R^{d \\times t}\\\\) such that \\\\(Y = X\\beta\\\\).\n\nThe idea is to try to predict the transformed targets \\\\(\\Omega\\\\) as a\nfunction of the transformed samples \\\\(\\Xi\\\\), by computing \\\\(\\alpha \\in\n\\mathbb{R}\\\\) such that \\\\(\\Omega = \\alpha \\Xi\\\\).\n\nThen, we have \\\\(Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T\\\\), and since\n\\\\(\\Xi\\\\) is the transformed training data we have that \\\\(Y = X \\alpha P\n\\Delta^T\\\\), and as a result the coefficient matrix \\\\(\\beta = \\alpha P\n\\Delta^T\\\\).\n\n\\\\(\\beta\\\\) can be accessed through the `coef_` attribute.\n\n`PLSSVD` is a simplified version of `PLSCanonical` described earlier: instead\nof iteratively deflating the matrices \\\\(X_k\\\\) and \\\\(Y_k\\\\), `PLSSVD`\ncomputes the SVD of \\\\(C = X^TY\\\\) only once, and stores the `n_components`\nsingular vectors corresponding to the biggest singular values in the matrices\n`U` and `V`, corresponding to the `x_weights_` and `y_weights_` attributes.\nHere, the transformed data is simply `transformed(X) = XU` and `transformed(Y)\n= YV`.\n\nIf `n_components == 1`, `PLSSVD` and `PLSCanonical` are strictly equivalent.\n\nThe `PLSRegression` estimator is similar to `PLSCanonical` with\n`algorithm='nipals'`, with 2 significant differences:\n\nThese two modifications affect the output of `predict` and `transform`, which\nare not the same as for `PLSCanonical`. Also, while the number of components\nis limited by `min(n_samples, n_features, n_targets)` in `PLSCanonical`, here\nthe limit is the rank of \\\\(X^TX\\\\), i.e. `min(n_samples, n_features)`.\n\n`PLSRegression` is also known as PLS1 (single targets) and PLS2 (multiple\ntargets). Much like `Lasso`, `PLSRegression` is a form of regularized linear\nregression where the number of components controls the strength of the\nregularization.\n\nCanonical Correlation Analysis was developed prior and independently to PLS.\nBut it turns out that `CCA` is a special case of PLS, and corresponds to PLS\nin \u201cMode B\u201d in the literature.\n\n`CCA` differs from `PLSCanonical` in the way the weights \\\\(u_k\\\\) and\n\\\\(v_k\\\\) are computed in the power method of step a). Details can be found in\nsection 10 of 1.\n\nSince `CCA` involves the inversion of \\\\(X_k^TX_k\\\\) and \\\\(Y_k^TY_k\\\\), this\nestimator can be unstable if the number of features or targets is greater than\nthe number of samples.\n\nReference:\n\nA survey of Partial Least Squares (PLS) methods, with emphasis on the two-\nblock case JA Wegelin\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.9. Naive Bayes", "path": "modules/naive_bayes", "type": "Guide", "text": "\nNaive Bayes methods are a set of supervised learning algorithms based on\napplying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional\nindependence between every pair of features given the value of the class\nvariable. Bayes\u2019 theorem states the following relationship, given class\nvariable \\\\(y\\\\) and dependent feature vector \\\\(x_1\\\\) through \\\\(x_n\\\\), :\n\nUsing the naive conditional independence assumption that\n\nfor all \\\\(i\\\\), this relationship is simplified to\n\nSince \\\\(P(x_1, \\dots, x_n)\\\\) is constant given the input, we can use the\nfollowing classification rule:\n\nand we can use Maximum A Posteriori (MAP) estimation to estimate \\\\(P(y)\\\\)\nand \\\\(P(x_i \\mid y)\\\\); the former is then the relative frequency of class\n\\\\(y\\\\) in the training set.\n\nThe different naive Bayes classifiers differ mainly by the assumptions they\nmake regarding the distribution of \\\\(P(x_i \\mid y)\\\\).\n\nIn spite of their apparently over-simplified assumptions, naive Bayes\nclassifiers have worked quite well in many real-world situations, famously\ndocument classification and spam filtering. They require a small amount of\ntraining data to estimate the necessary parameters. (For theoretical reasons\nwhy naive Bayes works well, and on which types of data it does, see the\nreferences below.)\n\nNaive Bayes learners and classifiers can be extremely fast compared to more\nsophisticated methods. The decoupling of the class conditional feature\ndistributions means that each distribution can be independently estimated as a\none dimensional distribution. This in turn helps to alleviate problems\nstemming from the curse of dimensionality.\n\nOn the flip side, although naive Bayes is known as a decent classifier, it is\nknown to be a bad estimator, so the probability outputs from `predict_proba`\nare not to be taken too seriously.\n\nReferences:\n\n`GaussianNB` implements the Gaussian Naive Bayes algorithm for classification.\nThe likelihood of the features is assumed to be Gaussian:\n\nThe parameters \\\\(\\sigma_y\\\\) and \\\\(\\mu_y\\\\) are estimated using maximum\nlikelihood.\n\n`MultinomialNB` implements the naive Bayes algorithm for multinomially\ndistributed data, and is one of the two classic naive Bayes variants used in\ntext classification (where the data are typically represented as word vector\ncounts, although tf-idf vectors are also known to work well in practice). The\ndistribution is parametrized by vectors \\\\(\\theta_y =\n(\\theta_{y1},\\ldots,\\theta_{yn})\\\\) for each class \\\\(y\\\\), where \\\\(n\\\\) is\nthe number of features (in text classification, the size of the vocabulary)\nand \\\\(\\theta_{yi}\\\\) is the probability \\\\(P(x_i \\mid y)\\\\) of feature\n\\\\(i\\\\) appearing in a sample belonging to class \\\\(y\\\\).\n\nThe parameters \\\\(\\theta_y\\\\) is estimated by a smoothed version of maximum\nlikelihood, i.e. relative frequency counting:\n\nwhere \\\\(N_{yi} = \\sum_{x \\in T} x_i\\\\) is the number of times feature \\\\(i\\\\)\nappears in a sample of class \\\\(y\\\\) in the training set \\\\(T\\\\), and \\\\(N_{y}\n= \\sum_{i=1}^{n} N_{yi}\\\\) is the total count of all features for class\n\\\\(y\\\\).\n\nThe smoothing priors \\\\(\\alpha \\ge 0\\\\) accounts for features not present in\nthe learning samples and prevents zero probabilities in further computations.\nSetting \\\\(\\alpha = 1\\\\) is called Laplace smoothing, while \\\\(\\alpha < 1\\\\)\nis called Lidstone smoothing.\n\n`ComplementNB` implements the complement naive Bayes (CNB) algorithm. CNB is\nan adaptation of the standard multinomial naive Bayes (MNB) algorithm that is\nparticularly suited for imbalanced data sets. Specifically, CNB uses\nstatistics from the complement of each class to compute the model\u2019s weights.\nThe inventors of CNB show empirically that the parameter estimates for CNB are\nmore stable than those for MNB. Further, CNB regularly outperforms MNB (often\nby a considerable margin) on text classification tasks. The procedure for\ncalculating the weights is as follows:\n\nwhere the summations are over all documents \\\\(j\\\\) not in class \\\\(c\\\\),\n\\\\(d_{ij}\\\\) is either the count or tf-idf value of term \\\\(i\\\\) in document\n\\\\(j\\\\), \\\\(\\alpha_i\\\\) is a smoothing hyperparameter like that found in MNB,\nand \\\\(\\alpha = \\sum_{i} \\alpha_i\\\\). The second normalization addresses the\ntendency for longer documents to dominate parameter estimates in MNB. The\nclassification rule is:\n\ni.e., a document is assigned to the class that is the poorest complement\nmatch.\n\nReferences:\n\n`BernoulliNB` implements the naive Bayes training and classification\nalgorithms for data that is distributed according to multivariate Bernoulli\ndistributions; i.e., there may be multiple features but each one is assumed to\nbe a binary-valued (Bernoulli, boolean) variable. Therefore, this class\nrequires samples to be represented as binary-valued feature vectors; if handed\nany other kind of data, a `BernoulliNB` instance may binarize its input\n(depending on the `binarize` parameter).\n\nThe decision rule for Bernoulli naive Bayes is based on\n\nwhich differs from multinomial NB\u2019s rule in that it explicitly penalizes the\nnon-occurrence of a feature \\\\(i\\\\) that is an indicator for class \\\\(y\\\\),\nwhere the multinomial variant would simply ignore a non-occurring feature.\n\nIn the case of text classification, word occurrence vectors (rather than word\ncount vectors) may be used to train and use this classifier. `BernoulliNB`\nmight perform better on some datasets, especially those with shorter\ndocuments. It is advisable to evaluate both models, if time permits.\n\nReferences:\n\n`CategoricalNB` implements the categorical naive Bayes algorithm for\ncategorically distributed data. It assumes that each feature, which is\ndescribed by the index \\\\(i\\\\), has its own categorical distribution.\n\nFor each feature \\\\(i\\\\) in the training set \\\\(X\\\\), `CategoricalNB`\nestimates a categorical distribution for each feature i of X conditioned on\nthe class y. The index set of the samples is defined as \\\\(J = \\\\{ 1, \\dots, m\n\\\\}\\\\), with \\\\(m\\\\) as the number of samples.\n\nThe probability of category \\\\(t\\\\) in feature \\\\(i\\\\) given class \\\\(c\\\\) is\nestimated as:\n\nwhere \\\\(N_{tic} = |\\\\{j \\in J \\mid x_{ij} = t, y_j = c\\\\}|\\\\) is the number\nof times category \\\\(t\\\\) appears in the samples \\\\(x_{i}\\\\), which belong to\nclass \\\\(c\\\\), \\\\(N_{c} = |\\\\{ j \\in J\\mid y_j = c\\\\}|\\\\) is the number of\nsamples with class c, \\\\(\\alpha\\\\) is a smoothing parameter and \\\\(n_i\\\\) is\nthe number of available categories of feature \\\\(i\\\\).\n\n`CategoricalNB` assumes that the sample matrix \\\\(X\\\\) is encoded (for\ninstance with the help of `OrdinalEncoder`) such that all categories for each\nfeature \\\\(i\\\\) are represented with numbers \\\\(0, ..., n_i - 1\\\\) where\n\\\\(n_i\\\\) is the number of available categories of feature \\\\(i\\\\).\n\nNaive Bayes models can be used to tackle large scale classification problems\nfor which the full training set might not fit in memory. To handle this case,\n`MultinomialNB`, `BernoulliNB`, and `GaussianNB` expose a `partial_fit` method\nthat can be used incrementally as done with other classifiers as demonstrated\nin Out-of-core classification of text documents. All naive Bayes classifiers\nsupport sample weighting.\n\nContrary to the `fit` method, the first call to `partial_fit` needs to be\npassed the list of all the expected class labels.\n\nFor an overview of available strategies in scikit-learn, see also the out-of-\ncore learning documentation.\n\nNote\n\nThe `partial_fit` method call of naive Bayes models introduces some\ncomputational overhead. It is recommended to use data chunk sizes that are as\nlarge as possible, that is as the available RAM allows.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.10. Decision Trees", "path": "modules/tree", "type": "Guide", "text": "\nDecision Trees (DTs) are a non-parametric supervised learning method used for\nclassification and regression. The goal is to create a model that predicts the\nvalue of a target variable by learning simple decision rules inferred from the\ndata features. A tree can be seen as a piecewise constant approximation.\n\nFor instance, in the example below, decision trees learn from data to\napproximate a sine curve with a set of if-then-else decision rules. The deeper\nthe tree, the more complex the decision rules and the fitter the model.\n\nSome advantages of decision trees are:\n\nThe disadvantages of decision trees include:\n\n`DecisionTreeClassifier` is a class capable of performing multi-class\nclassification on a dataset.\n\nAs with other classifiers, `DecisionTreeClassifier` takes as input two arrays:\nan array X, sparse or dense, of shape `(n_samples, n_features)` holding the\ntraining samples, and an array Y of integer values, shape `(n_samples,)`,\nholding the class labels for the training samples:\n\nAfter being fitted, the model can then be used to predict the class of\nsamples:\n\nIn case that there are multiple classes with the same and highest probability,\nthe classifier will predict the class with the lowest index amongst those\nclasses.\n\nAs an alternative to outputting a specific class, the probability of each\nclass can be predicted, which is the fraction of training samples of the class\nin a leaf:\n\n`DecisionTreeClassifier` is capable of both binary (where the labels are [-1,\n1]) classification and multiclass (where the labels are [0, \u2026, K-1])\nclassification.\n\nUsing the Iris dataset, we can construct a tree as follows:\n\nOnce trained, you can plot the tree with the `plot_tree` function:\n\nWe can also export the tree in Graphviz format using the `export_graphviz`\nexporter. If you use the conda package manager, the graphviz binaries and the\npython package can be installed with `conda install python-graphviz`.\n\nAlternatively binaries for graphviz can be downloaded from the graphviz\nproject homepage, and the Python wrapper installed from pypi with `pip install\ngraphviz`.\n\nBelow is an example graphviz export of the above tree trained on the entire\niris dataset; the results are saved in an output file `iris.pdf`:\n\nThe `export_graphviz` exporter also supports a variety of aesthetic options,\nincluding coloring nodes by their class (or value for regression) and using\nexplicit variable and class names if desired. Jupyter notebooks also render\nthese plots inline automatically:\n\nAlternatively, the tree can also be exported in textual format with the\nfunction `export_text`. This method doesn\u2019t require the installation of\nexternal libraries and is more compact:\n\nExamples:\n\nDecision trees can also be applied to regression problems, using the\n`DecisionTreeRegressor` class.\n\nAs in the classification setting, the fit method will take as argument arrays\nX and y, only that in this case y is expected to have floating point values\ninstead of integer values:\n\nExamples:\n\nA multi-output problem is a supervised learning problem with several outputs\nto predict, that is when Y is a 2d array of shape `(n_samples, n_outputs)`.\n\nWhen there is no correlation between the outputs, a very simple way to solve\nthis kind of problem is to build n independent models, i.e. one for each\noutput, and then to use those models to independently predict each one of the\nn outputs. However, because it is likely that the output values related to the\nsame input are themselves correlated, an often better way is to build a single\nmodel capable of predicting simultaneously all n outputs. First, it requires\nlower training time since only a single estimator is built. Second, the\ngeneralization accuracy of the resulting estimator may often be increased.\n\nWith regard to decision trees, this strategy can readily be used to support\nmulti-output problems. This requires the following changes:\n\nThis module offers support for multi-output problems by implementing this\nstrategy in both `DecisionTreeClassifier` and `DecisionTreeRegressor`. If a\ndecision tree is fit on an output array Y of shape `(n_samples, n_outputs)`\nthen the resulting estimator will:\n\nThe use of multi-output trees for regression is demonstrated in Multi-output\nDecision Tree Regression. In this example, the input X is a single real value\nand the outputs Y are the sine and cosine of X.\n\nThe use of multi-output trees for classification is demonstrated in Face\ncompletion with a multi-output estimators. In this example, the inputs X are\nthe pixels of the upper half of faces and the outputs Y are the pixels of the\nlower half of those faces.\n\nExamples:\n\nReferences:\n\nIn general, the run time cost to construct a balanced binary tree is\n\\\\(O(n_{samples}n_{features}\\log(n_{samples}))\\\\) and query time\n\\\\(O(\\log(n_{samples}))\\\\). Although the tree construction algorithm attempts\nto generate balanced trees, they will not always be balanced. Assuming that\nthe subtrees remain approximately balanced, the cost at each node consists of\nsearching through \\\\(O(n_{features})\\\\) to find the feature that offers the\nlargest reduction in entropy. This has a cost of\n\\\\(O(n_{features}n_{samples}\\log(n_{samples}))\\\\) at each node, leading to a\ntotal cost over the entire trees (by summing the cost at each node) of\n\\\\(O(n_{features}n_{samples}^{2}\\log(n_{samples}))\\\\).\n\nUse `min_samples_split` or `min_samples_leaf` to ensure that multiple samples\ninform every decision in the tree, by controlling which splits will be\nconsidered. A very small number will usually mean the tree will overfit,\nwhereas a large number will prevent the tree from learning the data. Try\n`min_samples_leaf=5` as an initial value. If the sample size varies greatly, a\nfloat number can be used as percentage in these two parameters. While\n`min_samples_split` can create arbitrarily small leaves, `min_samples_leaf`\nguarantees that each leaf has a minimum size, avoiding low-variance, over-fit\nleaf nodes in regression problems. For classification with few classes,\n`min_samples_leaf=1` is often the best choice.\n\nNote that `min_samples_split` considers samples directly and independent of\n`sample_weight`, if provided (e.g. a node with m weighted samples is still\ntreated as having exactly m samples). Consider `min_weight_fraction_leaf` or\n`min_impurity_decrease` if accounting for sample weights is required at\nsplits.\n\nWhat are all the various decision tree algorithms and how do they differ from\neach other? Which one is implemented in scikit-learn?\n\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The\nalgorithm creates a multiway tree, finding for each node (i.e. in a greedy\nmanner) the categorical feature that will yield the largest information gain\nfor categorical targets. Trees are grown to their maximum size and then a\npruning step is usually applied to improve the ability of the tree to\ngeneralise to unseen data.\n\nC4.5 is the successor to ID3 and removed the restriction that features must be\ncategorical by dynamically defining a discrete attribute (based on numerical\nvariables) that partitions the continuous attribute value into a discrete set\nof intervals. C4.5 converts the trained trees (i.e. the output of the ID3\nalgorithm) into sets of if-then rules. These accuracy of each rule is then\nevaluated to determine the order in which they should be applied. Pruning is\ndone by removing a rule\u2019s precondition if the accuracy of the rule improves\nwithout it.\n\nC5.0 is Quinlan\u2019s latest version release under a proprietary license. It uses\nless memory and builds smaller rulesets than C4.5 while being more accurate.\n\nCART (Classification and Regression Trees) is very similar to C4.5, but it\ndiffers in that it supports numerical target variables (regression) and does\nnot compute rule sets. CART constructs binary trees using the feature and\nthreshold that yield the largest information gain at each node.\n\nscikit-learn uses an optimised version of the CART algorithm; however, scikit-\nlearn implementation does not support categorical variables for now.\n\nGiven training vectors \\\\(x_i \\in R^n\\\\), i=1,\u2026, l and a label vector \\\\(y \\in\nR^l\\\\), a decision tree recursively partitions the feature space such that the\nsamples with the same labels or similar target values are grouped together.\n\nLet the data at node \\\\(m\\\\) be represented by \\\\(Q_m\\\\) with \\\\(N_m\\\\)\nsamples. For each candidate split \\\\(\\theta = (j, t_m)\\\\) consisting of a\nfeature \\\\(j\\\\) and threshold \\\\(t_m\\\\), partition the data into\n\\\\(Q_m^{left}(\\theta)\\\\) and \\\\(Q_m^{right}(\\theta)\\\\) subsets\n\nThe quality of a candidate split of node \\\\(m\\\\) is then computed using an\nimpurity function or loss function \\\\(H()\\\\), the choice of which depends on\nthe task being solved (classification or regression)\n\nSelect the parameters that minimises the impurity\n\nRecurse for subsets \\\\(Q_m^{left}(\\theta^*)\\\\) and \\\\(Q_m^{right}(\\theta^*)\\\\)\nuntil the maximum allowable depth is reached, \\\\(N_m < \\min_{samples}\\\\) or\n\\\\(N_m = 1\\\\).\n\nIf a target is a classification outcome taking on values 0,1,\u2026,K-1, for node\n\\\\(m\\\\), let\n\nbe the proportion of class k observations in node \\\\(m\\\\). If \\\\(m\\\\) is a\nterminal node, `predict_proba` for this region is set to \\\\(p_{mk}\\\\). Common\nmeasures of impurity are the following.\n\nGini:\n\nEntropy:\n\nMisclassification:\n\nIf the target is a continuous value, then for node \\\\(m\\\\), common criteria to\nminimize as for determining locations for future splits are Mean Squared Error\n(MSE or L2 error), Poisson deviance as well as Mean Absolute Error (MAE or L1\nerror). MSE and Poisson deviance both set the predicted value of terminal\nnodes to the learned mean value \\\\(\\bar{y}_m\\\\) of the node whereas the MAE\nsets the predicted value of terminal nodes to the median \\\\(median(y)_m\\\\).\n\nMean Squared Error:\n\nHalf Poisson deviance:\n\nSetting `criterion=\"poisson\"` might be a good choice if your target is a count\nor a frequency (count per some unit). In any case, \\\\(y >= 0\\\\) is a necessary\ncondition to use this criterion. Note that it fits much slower than the MSE\ncriterion.\n\nMean Absolute Error:\n\nNote that it fits much slower than the MSE criterion.\n\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\nover-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized\nby \\\\(\\alpha\\ge0\\\\) known as the complexity parameter. The complexity\nparameter is used to define the cost-complexity measure, \\\\(R_\\alpha(T)\\\\) of\na given tree \\\\(T\\\\):\n\nwhere \\\\(|\\widetilde{T}|\\\\) is the number of terminal nodes in \\\\(T\\\\) and\n\\\\(R(T)\\\\) is traditionally defined as the total misclassification rate of the\nterminal nodes. Alternatively, scikit-learn uses the total sample weighted\nimpurity of the terminal nodes for \\\\(R(T)\\\\). As shown above, the impurity of\na node depends on the criterion. Minimal cost-complexity pruning finds the\nsubtree of \\\\(T\\\\) that minimizes \\\\(R_\\alpha(T)\\\\).\n\nThe cost complexity measure of a single node is \\\\(R_\\alpha(t)=R(t)+\\alpha\\\\).\nThe branch, \\\\(T_t\\\\), is defined to be a tree where node \\\\(t\\\\) is its root.\nIn general, the impurity of a node is greater than the sum of impurities of\nits terminal nodes, \\\\(R(T_t)<R(t)\\\\). However, the cost complexity measure of\na node, \\\\(t\\\\), and its branch, \\\\(T_t\\\\), can be equal depending on\n\\\\(\\alpha\\\\). We define the effective \\\\(\\alpha\\\\) of a node to be the value\nwhere they are equal, \\\\(R_\\alpha(T_t)=R_\\alpha(t)\\\\) or\n\\\\(\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}\\\\). A non-terminal node with the\nsmallest value of \\\\(\\alpha_{eff}\\\\) is the weakest link and will be pruned.\nThis process stops when the pruned tree\u2019s minimal \\\\(\\alpha_{eff}\\\\) is\ngreater than the `ccp_alpha` parameter.\n\nExamples:\n\nReferences:\n\nL. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and\nRegression Trees. Wadsworth, Belmont, CA, 1984.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.11. Ensemble methods", "path": "modules/ensemble", "type": "Guide", "text": "\nThe goal of ensemble methods is to combine the predictions of several base\nestimators built with a given learning algorithm in order to improve\ngeneralizability / robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\nIn averaging methods, the driving principle is to build several estimators\nindependently and then to average their predictions. On average, the combined\nestimator is usually better than any of the single base estimator because its\nvariance is reduced.\n\nExamples: Bagging methods, Forests of randomized trees, \u2026\n\nBy contrast, in boosting methods, base estimators are built sequentially and\none tries to reduce the bias of the combined estimator. The motivation is to\ncombine several weak models to produce a powerful ensemble.\n\nExamples: AdaBoost, Gradient Tree Boosting, \u2026\n\nIn ensemble algorithms, bagging methods form a class of algorithms which build\nseveral instances of a black-box estimator on random subsets of the original\ntraining set and then aggregate their individual predictions to form a final\nprediction. These methods are used as a way to reduce the variance of a base\nestimator (e.g., a decision tree), by introducing randomization into its\nconstruction procedure and then making an ensemble out of it. In many cases,\nbagging methods constitute a very simple way to improve with respect to a\nsingle model, without making it necessary to adapt the underlying base\nalgorithm. As they provide a way to reduce overfitting, bagging methods work\nbest with strong and complex models (e.g., fully developed decision trees), in\ncontrast with boosting methods which usually work best with weak models (e.g.,\nshallow decision trees).\n\nBagging methods come in many flavours but mostly differ from each other by the\nway they draw random subsets of the training set:\n\nIn scikit-learn, bagging methods are offered as a unified `BaggingClassifier`\nmeta-estimator (resp. `BaggingRegressor`), taking as input a user-specified\nbase estimator along with parameters specifying the strategy to draw random\nsubsets. In particular, `max_samples` and `max_features` control the size of\nthe subsets (in terms of samples and features), while `bootstrap` and\n`bootstrap_features` control whether samples and features are drawn with or\nwithout replacement. When using a subset of the available samples the\ngeneralization accuracy can be estimated with the out-of-bag samples by\nsetting `oob_score=True`. As an example, the snippet below illustrates how to\ninstantiate a bagging ensemble of `KNeighborsClassifier` base estimators, each\nbuilt on random subsets of 50% of the samples and 50% of the features.\n\nExamples:\n\nReferences\n\nL. Breiman, \u201cPasting small votes for classification in large databases and on-\nline\u201d, Machine Learning, 36(1), 85-103, 1999.\n\nL. Breiman, \u201cBagging predictors\u201d, Machine Learning, 24(2), 123-140, 1996.\n\nT. Ho, \u201cThe random subspace method for constructing decision forests\u201d, Pattern\nAnalysis and Machine Intelligence, 20(8), 832-844, 1998.\n\nG. Louppe and P. Geurts, \u201cEnsembles on Random Patches\u201d, Machine Learning and\nKnowledge Discovery in Databases, 346-361, 2012.\n\nThe `sklearn.ensemble` module includes two averaging algorithms based on\nrandomized decision trees: the RandomForest algorithm and the Extra-Trees\nmethod. Both algorithms are perturb-and-combine techniques [B1998]\nspecifically designed for trees. This means a diverse set of classifiers is\ncreated by introducing randomness in the classifier construction. The\nprediction of the ensemble is given as the averaged prediction of the\nindividual classifiers.\n\nAs other classifiers, forest classifiers have to be fitted with two arrays: a\nsparse or dense array X of shape `(n_samples, n_features)` holding the\ntraining samples, and an array Y of shape `(n_samples,)` holding the target\nvalues (class labels) for the training samples:\n\nLike decision trees, forests of trees also extend to multi-output problems (if\nY is an array of shape `(n_samples, n_outputs)`).\n\nIn random forests (see `RandomForestClassifier` and `RandomForestRegressor`\nclasses), each tree in the ensemble is built from a sample drawn with\nreplacement (i.e., a bootstrap sample) from the training set.\n\nFurthermore, when splitting each node during the construction of a tree, the\nbest split is found either from all input features or a random subset of size\n`max_features`. (See the parameter tuning guidelines for more details).\n\nThe purpose of these two sources of randomness is to decrease the variance of\nthe forest estimator. Indeed, individual decision trees typically exhibit high\nvariance and tend to overfit. The injected randomness in forests yield\ndecision trees with somewhat decoupled prediction errors. By taking an average\nof those predictions, some errors can cancel out. Random forests achieve a\nreduced variance by combining diverse trees, sometimes at the cost of a slight\nincrease in bias. In practice the variance reduction is often significant\nhence yielding an overall better model.\n\nIn contrast to the original publication [B2001], the scikit-learn\nimplementation combines classifiers by averaging their probabilistic\nprediction, instead of letting each classifier vote for a single class.\n\nIn extremely randomized trees (see `ExtraTreesClassifier` and\n`ExtraTreesRegressor` classes), randomness goes one step further in the way\nsplits are computed. As in random forests, a random subset of candidate\nfeatures is used, but instead of looking for the most discriminative\nthresholds, thresholds are drawn at random for each candidate feature and the\nbest of these randomly-generated thresholds is picked as the splitting rule.\nThis usually allows to reduce the variance of the model a bit more, at the\nexpense of a slightly greater increase in bias:\n\nThe main parameters to adjust when using these methods is `n_estimators` and\n`max_features`. The former is the number of trees in the forest. The larger\nthe better, but also the longer it will take to compute. In addition, note\nthat results will stop getting significantly better beyond a critical number\nof trees. The latter is the size of the random subsets of features to consider\nwhen splitting a node. The lower the greater the reduction of variance, but\nalso the greater the increase in bias. Empirical good default values are\n`max_features=None` (always considering all features instead of a random\nsubset) for regression problems, and `max_features=\"sqrt\"` (using a random\nsubset of size `sqrt(n_features)`) for classification tasks (where\n`n_features` is the number of features in the data). Good results are often\nachieved when setting `max_depth=None` in combination with\n`min_samples_split=2` (i.e., when fully developing the trees). Bear in mind\nthough that these values are usually not optimal, and might result in models\nthat consume a lot of RAM. The best parameter values should always be cross-\nvalidated. In addition, note that in random forests, bootstrap samples are\nused by default (`bootstrap=True`) while the default strategy for extra-trees\nis to use the whole dataset (`bootstrap=False`). When using bootstrap sampling\nthe generalization accuracy can be estimated on the left out or out-of-bag\nsamples. This can be enabled by setting `oob_score=True`.\n\nNote\n\nThe size of the model with the default parameters is \\\\(O( M * N * log (N)\n)\\\\), where \\\\(M\\\\) is the number of trees and \\\\(N\\\\) is the number of\nsamples. In order to reduce the size of the model, you can change these\nparameters: `min_samples_split`, `max_leaf_nodes`, `max_depth` and\n`min_samples_leaf`.\n\nFinally, this module also features the parallel construction of the trees and\nthe parallel computation of the predictions through the `n_jobs` parameter. If\n`n_jobs=k` then computations are partitioned into `k` jobs, and run on `k`\ncores of the machine. If `n_jobs=-1` then all cores available on the machine\nare used. Note that because of inter-process communication overhead, the\nspeedup might not be linear (i.e., using `k` jobs will unfortunately not be\n`k` times as fast). Significant speedup can still be achieved though when\nbuilding a large number of trees, or when building a single tree requires a\nfair amount of time (e.g., on large datasets).\n\nExamples:\n\nReferences\n\nThe relative rank (i.e. depth) of a feature used as a decision node in a tree\ncan be used to assess the relative importance of that feature with respect to\nthe predictability of the target variable. Features used at the top of the\ntree contribute to the final prediction decision of a larger fraction of the\ninput samples. The expected fraction of the samples they contribute to can\nthus be used as an estimate of the relative importance of the features. In\nscikit-learn, the fraction of samples a feature contributes to is combined\nwith the decrease in impurity from splitting them to create a normalized\nestimate of the predictive power of that feature.\n\nBy averaging the estimates of predictive ability over several randomized trees\none can reduce the variance of such an estimate and use it for feature\nselection. This is known as the mean decrease in impurity, or MDI. Refer to\n[L2014] for more information on MDI and feature importance evaluation with\nRandom Forests.\n\nWarning\n\nThe impurity-based feature importances computed on tree-based models suffer\nfrom two flaws that can lead to misleading conclusions. First they are\ncomputed on statistics derived from the training dataset and therefore do not\nnecessarily inform us on which features are most important to make good\npredictions on held-out dataset. Secondly, they favor high cardinality\nfeatures, that is features with many unique values. Permutation feature\nimportance is an alternative to impurity-based feature importance that does\nnot suffer from these flaws. These two methods of obtaining feature importance\nare explored in: Permutation Importance vs Random Forest Feature Importance\n(MDI).\n\nThe following example shows a color-coded representation of the relative\nimportances of each individual pixel for a face recognition task using a\n`ExtraTreesClassifier` model.\n\nIn practice those estimates are stored as an attribute named\n`feature_importances_` on the fitted model. This is an array with shape\n`(n_features,)` whose values are positive and sum to 1.0. The higher the\nvalue, the more important is the contribution of the matching feature to the\nprediction function.\n\nExamples:\n\nReferences\n\nG. Louppe, \u201cUnderstanding Random Forests: From Theory to Practice\u201d, PhD\nThesis, U. of Liege, 2014.\n\n`RandomTreesEmbedding` implements an unsupervised transformation of the data.\nUsing a forest of completely random trees, `RandomTreesEmbedding` encodes the\ndata by the indices of the leaves a data point ends up in. This index is then\nencoded in a one-of-K manner, leading to a high dimensional, sparse binary\ncoding. This coding can be computed very efficiently and can then be used as a\nbasis for other learning tasks. The size and sparsity of the code can be\ninfluenced by choosing the number of trees and the maximum depth per tree. For\neach tree in the ensemble, the coding contains one entry of one. The size of\nthe coding is at most `n_estimators * 2 ** max_depth`, the maximum number of\nleaves in the forest.\n\nAs neighboring data points are more likely to lie within the same leaf of a\ntree, the transformation performs an implicit, non-parametric density\nestimation.\n\nExamples:\n\nSee also\n\nManifold learning techniques can also be useful to derive non-linear\nrepresentations of feature space, also these approaches focus also on\ndimensionality reduction.\n\nThe module `sklearn.ensemble` includes the popular boosting algorithm\nAdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\n\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e.,\nmodels that are only slightly better than random guessing, such as small\ndecision trees) on repeatedly modified versions of the data. The predictions\nfrom all of them are then combined through a weighted majority vote (or sum)\nto produce the final prediction. The data modifications at each so-called\nboosting iteration consist of applying weights \\\\(w_1\\\\), \\\\(w_2\\\\), \u2026,\n\\\\(w_N\\\\) to each of the training samples. Initially, those weights are all\nset to \\\\(w_i = 1/N\\\\), so that the first step simply trains a weak learner on\nthe original data. For each successive iteration, the sample weights are\nindividually modified and the learning algorithm is reapplied to the\nreweighted data. At a given step, those training examples that were\nincorrectly predicted by the boosted model induced at the previous step have\ntheir weights increased, whereas the weights are decreased for those that were\npredicted correctly. As iterations proceed, examples that are difficult to\npredict receive ever-increasing influence. Each subsequent weak learner is\nthereby forced to concentrate on the examples that are missed by the previous\nones in the sequence [HTF].\n\nAdaBoost can be used both for classification and regression problems:\n\nThe following example shows how to fit an AdaBoost classifier with 100 weak\nlearners:\n\nThe number of weak learners is controlled by the parameter `n_estimators`. The\n`learning_rate` parameter controls the contribution of the weak learners in\nthe final combination. By default, weak learners are decision stumps.\nDifferent weak learners can be specified through the `base_estimator`\nparameter. The main parameters to tune to obtain good results are\n`n_estimators` and the complexity of the base estimators (e.g., its depth\n`max_depth` or minimum required number of samples to consider a split\n`min_samples_split`).\n\nExamples:\n\nReferences\n\nY. Freund, and R. Schapire, \u201cA Decision-Theoretic Generalization of On-Line\nLearning and an Application to Boosting\u201d, 1997.\n\nJ. Zhu, H. Zou, S. Rosset, T. Hastie. \u201cMulti-class AdaBoost\u201d, 2009.\n\nT. Hastie, R. Tibshirani and J. Friedman, \u201cElements of Statistical Learning\nEd. 2\u201d, Springer, 2009.\n\nGradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a\ngeneralization of boosting to arbitrary differentiable loss functions. GBDT is\nan accurate and effective off-the-shelf procedure that can be used for both\nregression and classification problems in a variety of areas including Web\nsearch ranking and ecology.\n\nThe module `sklearn.ensemble` provides methods for both classification and\nregression via gradient boosted decision trees.\n\nNote\n\nScikit-learn 0.21 introduces two new experimental implementations of gradient\nboosting trees, namely `HistGradientBoostingClassifier` and\n`HistGradientBoostingRegressor`, inspired by LightGBM (See [LightGBM]).\n\nThese histogram-based estimators can be orders of magnitude faster than\n`GradientBoostingClassifier` and `GradientBoostingRegressor` when the number\nof samples is larger than tens of thousands of samples.\n\nThey also have built-in support for missing values, which avoids the need for\nan imputer.\n\nThese estimators are described in more detail below in Histogram-Based\nGradient Boosting.\n\nThe following guide focuses on `GradientBoostingClassifier` and\n`GradientBoostingRegressor`, which might be preferred for small sample sizes\nsince binning may lead to split points that are too approximate in this\nsetting.\n\nThe usage and the parameters of `GradientBoostingClassifier` and\n`GradientBoostingRegressor` are described below. The 2 most important\nparameters of these estimators are `n_estimators` and `learning_rate`.\n\n`GradientBoostingClassifier` supports both binary and multi-class\nclassification. The following example shows how to fit a gradient boosting\nclassifier with 100 decision stumps as weak learners:\n\nThe number of weak learners (i.e. regression trees) is controlled by the\nparameter `n_estimators`; The size of each tree can be controlled either by\nsetting the tree depth via `max_depth` or by setting the number of leaf nodes\nvia `max_leaf_nodes`. The `learning_rate` is a hyper-parameter in the range\n(0.0, 1.0] that controls overfitting via shrinkage .\n\nNote\n\nClassification with more than 2 classes requires the induction of `n_classes`\nregression trees at each iteration, thus, the total number of induced trees\nequals `n_classes * n_estimators`. For datasets with a large number of classes\nwe strongly recommend to use `HistGradientBoostingClassifier` as an\nalternative to `GradientBoostingClassifier` .\n\n`GradientBoostingRegressor` supports a number of different loss functions for\nregression which can be specified via the argument `loss`; the default loss\nfunction for regression is least squares (`'ls'`).\n\nThe figure below shows the results of applying `GradientBoostingRegressor`\nwith least squares loss and 500 base learners to the diabetes dataset\n(`sklearn.datasets.load_diabetes`). The plot on the left shows the train and\ntest error at each iteration. The train error at each iteration is stored in\nthe `train_score_` attribute of the gradient boosting model. The test error at\neach iterations can be obtained via the `staged_predict` method which returns\na generator that yields the predictions at each stage. Plots like these can be\nused to determine the optimal number of trees (i.e. `n_estimators`) by early\nstopping.\n\nExamples:\n\nBoth `GradientBoostingRegressor` and `GradientBoostingClassifier` support\n`warm_start=True` which allows you to add more estimators to an already fitted\nmodel.\n\nThe size of the regression tree base learners defines the level of variable\ninteractions that can be captured by the gradient boosting model. In general,\na tree of depth `h` can capture interactions of order `h` . There are two ways\nin which the size of the individual regression trees can be controlled.\n\nIf you specify `max_depth=h` then complete binary trees of depth `h` will be\ngrown. Such trees will have (at most) `2**h` leaf nodes and `2**h - 1` split\nnodes.\n\nAlternatively, you can control the tree size by specifying the number of leaf\nnodes via the parameter `max_leaf_nodes`. In this case, trees will be grown\nusing best-first search where nodes with the highest improvement in impurity\nwill be expanded first. A tree with `max_leaf_nodes=k` has `k - 1` split nodes\nand thus can model interactions of up to order `max_leaf_nodes - 1` .\n\nWe found that `max_leaf_nodes=k` gives comparable results to `max_depth=k-1`\nbut is significantly faster to train at the expense of a slightly higher\ntraining error. The parameter `max_leaf_nodes` corresponds to the variable `J`\nin the chapter on gradient boosting in [F2001] and is related to the parameter\n`interaction.depth` in R\u2019s gbm package where `max_leaf_nodes ==\ninteraction.depth + 1` .\n\nWe first present GBRT for regression, and then detail the classification case.\n\nGBRT regressors are additive models whose prediction \\\\(y_i\\\\) for a given\ninput \\\\(x_i\\\\) is of the following form:\n\nwhere the \\\\(h_m\\\\) are estimators called weak learners in the context of\nboosting. Gradient Tree Boosting uses decision tree regressors of fixed size\nas weak learners. The constant M corresponds to the `n_estimators` parameter.\n\nSimilar to other boosting algorithms, a GBRT is built in a greedy fashion:\n\nwhere the newly added tree \\\\(h_m\\\\) is fitted in order to minimize a sum of\nlosses \\\\(L_m\\\\), given the previous ensemble \\\\(F_{m-1}\\\\):\n\nwhere \\\\(l(y_i, F(x_i))\\\\) is defined by the `loss` parameter, detailed in the\nnext section.\n\nBy default, the initial model \\\\(F_{0}\\\\) is chosen as the constant that\nminimizes the loss: for a least-squares loss, this is the empirical mean of\nthe target values. The initial model can also be specified via the `init`\nargument.\n\nUsing a first-order Taylor approximation, the value of \\\\(l\\\\) can be\napproximated as follows:\n\nNote\n\nBriefly, a first-order Taylor approximation says that \\\\(l(z) \\approx l(a) +\n(z - a) \\frac{\\partial l(a)}{\\partial a}\\\\). Here, \\\\(z\\\\) corresponds to\n\\\\(F_{m - 1}(x_i) + h_m(x_i)\\\\), and \\\\(a\\\\) corresponds to \\\\(F_{m-1}(x_i)\\\\)\n\nThe quantity \\\\(\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)}\n\\right]_{F=F_{m - 1}}\\\\) is the derivative of the loss with respect to its\nsecond parameter, evaluated at \\\\(F_{m-1}(x)\\\\). It is easy to compute for any\ngiven \\\\(F_{m - 1}(x_i)\\\\) in a closed form since the loss is differentiable.\nWe will denote it by \\\\(g_i\\\\).\n\nRemoving the constant terms, we have:\n\nThis is minimized if \\\\(h(x_i)\\\\) is fitted to predict a value that is\nproportional to the negative gradient \\\\(-g_i\\\\). Therefore, at each\niteration, the estimator \\\\(h_m\\\\) is fitted to predict the negative gradients\nof the samples. The gradients are updated at each iteration. This can be\nconsidered as some kind of gradient descent in a functional space.\n\nNote\n\nFor some losses, e.g. the least absolute deviation (LAD) where the gradients\nare \\\\(\\pm 1\\\\), the values predicted by a fitted \\\\(h_m\\\\) are not accurate\nenough: the tree can only output integer values. As a result, the leaves\nvalues of the tree \\\\(h_m\\\\) are modified once the tree is fitted, such that\nthe leaves values minimize the loss \\\\(L_m\\\\). The update is loss-dependent:\nfor the LAD loss, the value of a leaf is updated to the median of the samples\nin that leaf.\n\nGradient boosting for classification is very similar to the regression case.\nHowever, the sum of the trees \\\\(F_M(x_i) = \\sum_m h_m(x_i)\\\\) is not\nhomogeneous to a prediction: it cannot be a class, since the trees predict\ncontinuous values.\n\nThe mapping from the value \\\\(F_M(x_i)\\\\) to a class or a probability is loss-\ndependent. For the deviance (or log-loss), the probability that \\\\(x_i\\\\)\nbelongs to the positive class is modeled as \\\\(p(y_i = 1 | x_i) =\n\\sigma(F_M(x_i))\\\\) where \\\\(\\sigma\\\\) is the sigmoid function.\n\nFor multiclass classification, K trees (for K classes) are built at each of\nthe \\\\(M\\\\) iterations. The probability that \\\\(x_i\\\\) belongs to class k is\nmodeled as a softmax of the \\\\(F_{M,k}(x_i)\\\\) values.\n\nNote that even for a classification task, the \\\\(h_m\\\\) sub-estimator is still\na regressor, not a classifier. This is because the sub-estimators are trained\nto predict (negative) gradients, which are always continuous quantities.\n\nThe following loss functions are supported and can be specified using the\nparameter `loss`:\n\nRegression\n\nClassification\n\n[F2001] proposed a simple regularization strategy that scales the contribution\nof each weak learner by a constant factor \\\\(\\nu\\\\):\n\nThe parameter \\\\(\\nu\\\\) is also called the learning rate because it scales the\nstep length the gradient descent procedure; it can be set via the\n`learning_rate` parameter.\n\nThe parameter `learning_rate` strongly interacts with the parameter\n`n_estimators`, the number of weak learners to fit. Smaller values of\n`learning_rate` require larger numbers of weak learners to maintain a constant\ntraining error. Empirical evidence suggests that small values of\n`learning_rate` favor better test error. [HTF] recommend to set the learning\nrate to a small constant (e.g. `learning_rate <= 0.1`) and choose\n`n_estimators` by early stopping. For a more detailed discussion of the\ninteraction between `learning_rate` and `n_estimators` see [R2007].\n\n[F1999] proposed stochastic gradient boosting, which combines gradient\nboosting with bootstrap averaging (bagging). At each iteration the base\nclassifier is trained on a fraction `subsample` of the available training\ndata. The subsample is drawn without replacement. A typical value of\n`subsample` is 0.5.\n\nThe figure below illustrates the effect of shrinkage and subsampling on the\ngoodness-of-fit of the model. We can clearly see that shrinkage outperforms\nno-shrinkage. Subsampling with shrinkage can further increase the accuracy of\nthe model. Subsampling without shrinkage, on the other hand, does poorly.\n\nAnother strategy to reduce the variance is by subsampling the features\nanalogous to the random splits in `RandomForestClassifier` . The number of\nsubsampled features can be controlled via the `max_features` parameter.\n\nNote\n\nUsing a small `max_features` value can significantly decrease the runtime.\n\nStochastic gradient boosting allows to compute out-of-bag estimates of the\ntest deviance by computing the improvement in deviance on the examples that\nare not included in the bootstrap sample (i.e. the out-of-bag examples). The\nimprovements are stored in the attribute `oob_improvement_`.\n`oob_improvement_[i]` holds the improvement in terms of the loss on the OOB\nsamples if you add the i-th stage to the current predictions. Out-of-bag\nestimates can be used for model selection, for example to determine the\noptimal number of iterations. OOB estimates are usually very pessimistic thus\nwe recommend to use cross-validation instead and only use OOB if cross-\nvalidation is too time consuming.\n\nExamples:\n\nIndividual decision trees can be interpreted easily by simply visualizing the\ntree structure. Gradient boosting models, however, comprise hundreds of\nregression trees thus they cannot be easily interpreted by visual inspection\nof the individual trees. Fortunately, a number of techniques have been\nproposed to summarize and interpret gradient boosting models.\n\nOften features do not contribute equally to predict the target response; in\nmany situations the majority of the features are in fact irrelevant. When\ninterpreting a model, the first question usually is: what are those important\nfeatures and how do they contributing in predicting the target response?\n\nIndividual decision trees intrinsically perform feature selection by selecting\nappropriate split points. This information can be used to measure the\nimportance of each feature; the basic idea is: the more often a feature is\nused in the split points of a tree the more important that feature is. This\nnotion of importance can be extended to decision tree ensembles by simply\naveraging the impurity-based feature importance of each tree (see Feature\nimportance evaluation for more details).\n\nThe feature importance scores of a fit gradient boosting model can be accessed\nvia the `feature_importances_` property:\n\nNote that this computation of feature importance is based on entropy, and it\nis distinct from `sklearn.inspection.permutation_importance` which is based on\npermutation of the features.\n\nExamples:\n\nScikit-learn 0.21 introduced two new experimental implementations of gradient\nboosting trees, namely `HistGradientBoostingClassifier` and\n`HistGradientBoostingRegressor`, inspired by LightGBM (See [LightGBM]).\n\nThese histogram-based estimators can be orders of magnitude faster than\n`GradientBoostingClassifier` and `GradientBoostingRegressor` when the number\nof samples is larger than tens of thousands of samples.\n\nThey also have built-in support for missing values, which avoids the need for\nan imputer.\n\nThese fast estimators first bin the input samples `X` into integer-valued bins\n(typically 256 bins) which tremendously reduces the number of splitting points\nto consider, and allows the algorithm to leverage integer-based data\nstructures (histograms) instead of relying on sorted continuous values when\nbuilding the trees. The API of these estimators is slightly different, and\nsome of the features from `GradientBoostingClassifier` and\n`GradientBoostingRegressor` are not yet supported, for instance some loss\nfunctions.\n\nThese estimators are still experimental: their predictions and their API might\nchange without any deprecation cycle. To use them, you need to explicitly\nimport `enable_hist_gradient_boosting`:\n\nExamples:\n\nMost of the parameters are unchanged from `GradientBoostingClassifier` and\n`GradientBoostingRegressor`. One exception is the `max_iter` parameter that\nreplaces `n_estimators`, and controls the number of iterations of the boosting\nprocess:\n\nAvailable losses for regression are \u2018least_squares\u2019,\n\u2018least_absolute_deviation\u2019, which is less sensitive to outliers, and\n\u2018poisson\u2019, which is well suited to model counts and frequencies. For\nclassification, \u2018binary_crossentropy\u2019 is used for binary classification and\n\u2018categorical_crossentropy\u2019 is used for multiclass classification. By default\nthe loss is \u2018auto\u2019 and will select the appropriate loss depending on y passed\nto fit.\n\nThe size of the trees can be controlled through the `max_leaf_nodes`,\n`max_depth`, and `min_samples_leaf` parameters.\n\nThe number of bins used to bin the data is controlled with the `max_bins`\nparameter. Using less bins acts as a form of regularization. It is generally\nrecommended to use as many bins as possible, which is the default.\n\nThe `l2_regularization` parameter is a regularizer on the loss function and\ncorresponds to \\\\(\\lambda\\\\) in equation (2) of [XGBoost].\n\nNote that early-stopping is enabled by default if the number of samples is\nlarger than 10,000. The early-stopping behaviour is controlled via the `early-\nstopping`, `scoring`, `validation_fraction`, `n_iter_no_change`, and `tol`\nparameters. It is possible to early-stop using an arbitrary scorer, or just\nthe training or validation loss. Note that for technical reasons, using a\nscorer is significantly slower than using the loss. By default, early-stopping\nis performed if there are at least 10,000 samples in the training set, using\nthe validation loss.\n\n`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` have\nbuilt-in support for missing values (NaNs).\n\nDuring training, the tree grower learns at each split point whether samples\nwith missing values should go to the left or right child, based on the\npotential gain. When predicting, samples with missing values are assigned to\nthe left or right child consequently:\n\nWhen the missingness pattern is predictive, the splits can be done on whether\nthe feature value is missing or not:\n\nIf no missing values were encountered for a given feature during training,\nthen samples with missing values are mapped to whichever child has the most\nsamples.\n\n`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` sample\nsupport weights during fit.\n\nThe following toy example demonstrates how the model ignores the samples with\nzero sample weights:\n\nAs you can see, the `[1, 0]` is comfortably classified as `1` since the first\ntwo samples are ignored due to their sample weights.\n\nImplementation detail: taking sample weights into account amounts to\nmultiplying the gradients (and the hessians) by the sample weights. Note that\nthe binning stage (specifically the quantiles computation) does not take the\nweights into account.\n\n`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` have\nnative support for categorical features: they can consider splits on non-\nordered, categorical data.\n\nFor datasets with categorical features, using the native categorical support\nis often better than relying on one-hot encoding (`OneHotEncoder`), because\none-hot encoding requires more tree depth to achieve equivalent splits. It is\nalso usually better to rely on the native categorical support rather than to\ntreat categorical features as continuous (ordinal), which happens for ordinal-\nencoded categorical data, since categories are nominal quantities where order\ndoes not matter.\n\nTo enable categorical support, a boolean mask can be passed to the\n`categorical_features` parameter, indicating which feature is categorical. In\nthe following, the first feature will be treated as categorical and the second\nfeature as numerical:\n\nEquivalently, one can pass a list of integers indicating the indices of the\ncategorical features:\n\nThe cardinality of each categorical feature should be less than the `max_bins`\nparameter, and each categorical feature is expected to be encoded in `[0,\nmax_bins - 1]`. To that end, it might be useful to pre-process the data with\nan `OrdinalEncoder` as done in Categorical Feature Support in Gradient\nBoosting.\n\nIf there are missing values during training, the missing values will be\ntreated as a proper category. If there are no missing values during training,\nthen at prediction time, missing values are mapped to the child node that has\nthe most samples (just like for continuous features). When predicting,\ncategories that were not seen during fit time will be treated as missing\nvalues.\n\nSplit finding with categorical features: The canonical way of considering\ncategorical splits in a tree is to consider all of the \\\\(2^{K - 1} - 1\\\\)\npartitions, where \\\\(K\\\\) is the number of categories. This can quickly become\nprohibitive when \\\\(K\\\\) is large. Fortunately, since gradient boosting trees\nare always regression trees (even for classification problems), there exist a\nfaster strategy that can yield equivalent splits. First, the categories of a\nfeature are sorted according to the variance of the target, for each category\n`k`. Once the categories are sorted, one can consider continuous partitions,\ni.e. treat the categories as if they were ordered continuous values (see\nFisher [Fisher1958] for a formal proof). As a result, only \\\\(K - 1\\\\) splits\nneed to be considered instead of \\\\(2^{K - 1} - 1\\\\). The initial sorting is a\n\\\\(\\mathcal{O}(K \\log(K))\\\\) operation, leading to a total complexity of\n\\\\(\\mathcal{O}(K \\log(K) + K)\\\\), instead of \\\\(\\mathcal{O}(2^K)\\\\).\n\nExamples:\n\nDepending on the problem at hand, you may have prior knowledge indicating that\na given feature should in general have a positive (or negative) effect on the\ntarget value. For example, all else being equal, a higher credit score should\nincrease the probability of getting approved for a loan. Monotonic constraints\nallow you to incorporate such prior knowledge into the model.\n\nA positive monotonic constraint is a constraint of the form:\n\n\\\\(x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)\\\\), where \\\\(F\\\\) is\nthe predictor with two features.\n\nSimilarly, a negative monotonic constraint is of the form:\n\n\\\\(x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)\\\\).\n\nNote that monotonic constraints only constraint the output \u201call else being\nequal\u201d. Indeed, the following relation is not enforced by a positive\nconstraint: \\\\(x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2')\\\\).\n\nYou can specify a monotonic constraint on each feature using the\n`monotonic_cst` parameter. For each feature, a value of 0 indicates no\nconstraint, while -1 and 1 indicate a negative and positive constraint,\nrespectively:\n\nIn a binary classification context, imposing a monotonic constraint means that\nthe feature is supposed to have a positive / negative effect on the\nprobability to belong to the positive class. Monotonic constraints are not\nsupported for multiclass context.\n\nNote\n\nSince categories are unordered quantities, it is not possible to enforce\nmonotonic constraints on categorical features.\n\nExamples:\n\n`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` have\nimplementations that use OpenMP for parallelization through Cython. For more\ndetails on how to control the number of threads, please refer to our\nParallelism notes.\n\nThe following parts are parallelized:\n\nThe bottleneck of a gradient boosting procedure is building the decision\ntrees. Building a traditional decision tree (as in the other GBDTs\n`GradientBoostingClassifier` and `GradientBoostingRegressor`) requires sorting\nthe samples at each node (for each feature). Sorting is needed so that the\npotential gain of a split point can be computed efficiently. Splitting a\nsingle node has thus a complexity of \\\\(\\mathcal{O}(n_\\text{features} \\times n\n\\log(n))\\\\) where \\\\(n\\\\) is the number of samples at the node.\n\n`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor`, in\ncontrast, do not require sorting the feature values and instead use a data-\nstructure called a histogram, where the samples are implicitly ordered.\nBuilding a histogram has a \\\\(\\mathcal{O}(n)\\\\) complexity, so the node\nsplitting procedure has a \\\\(\\mathcal{O}(n_\\text{features} \\times n)\\\\)\ncomplexity, much smaller than the previous one. In addition, instead of\nconsidering \\\\(n\\\\) split points, we here consider only `max_bins` split\npoints, which is much smaller.\n\nIn order to build histograms, the input data `X` needs to be binned into\ninteger-valued bins. This binning procedure does require sorting the feature\nvalues, but it only happens once at the very beginning of the boosting process\n(not at each node, like in `GradientBoostingClassifier` and\n`GradientBoostingRegressor`).\n\nFinally, many parts of the implementation of `HistGradientBoostingClassifier`\nand `HistGradientBoostingRegressor` are parallelized.\n\nReferences\n\nFriedmann, Jerome H., 2007, \u201cStochastic Gradient Boosting\u201d\n\nG. Ridgeway, \u201cGeneralized Boosted Models: A guide to the gbm package\u201d, 2007\n\nTianqi Chen, Carlos Guestrin, \u201cXGBoost: A Scalable Tree Boosting System\u201d\n\nKe et. al. \u201cLightGBM: A Highly Efficient Gradient BoostingDecision Tree\u201d\n\nWalter D. Fisher. \u201cOn Grouping for Maximum Homogeneity\u201d\n\nThe idea behind the `VotingClassifier` is to combine conceptually different\nmachine learning classifiers and use a majority vote or the average predicted\nprobabilities (soft vote) to predict the class labels. Such a classifier can\nbe useful for a set of equally well performing model in order to balance out\ntheir individual weaknesses.\n\nIn majority voting, the predicted class label for a particular sample is the\nclass label that represents the majority (mode) of the class labels predicted\nby each individual classifier.\n\nE.g., if the prediction for a given sample is\n\nthe VotingClassifier (with `voting='hard'`) would classify the sample as\n\u201cclass 1\u201d based on the majority class label.\n\nIn the cases of a tie, the `VotingClassifier` will select the class based on\nthe ascending sort order. E.g., in the following scenario\n\nthe class label 1 will be assigned to the sample.\n\nThe following example shows how to fit the majority rule classifier:\n\nIn contrast to majority voting (hard voting), soft voting returns the class\nlabel as argmax of the sum of predicted probabilities.\n\nSpecific weights can be assigned to each classifier via the `weights`\nparameter. When weights are provided, the predicted class probabilities for\neach classifier are collected, multiplied by the classifier weight, and\naveraged. The final class label is then derived from the class label with the\nhighest average probability.\n\nTo illustrate this with a simple example, let\u2019s assume we have 3 classifiers\nand a 3-class classification problems where we assign equal weights to all\nclassifiers: w1=1, w2=1, w3=1.\n\nThe weighted average probabilities for a sample would then be calculated as\nfollows:\n\nclassifier\n\nclass 1\n\nclass 2\n\nclass 3\n\nclassifier 1\n\nw1 * 0.2\n\nw1 * 0.5\n\nw1 * 0.3\n\nclassifier 2\n\nw2 * 0.6\n\nw2 * 0.3\n\nw2 * 0.1\n\nclassifier 3\n\nw3 * 0.3\n\nw3 * 0.4\n\nw3 * 0.3\n\nweighted average\n\n0.37\n\n0.4\n\n0.23\n\nHere, the predicted class label is 2, since it has the highest average\nprobability.\n\nThe following example illustrates how the decision regions may change when a\nsoft `VotingClassifier` is used based on an linear Support Vector Machine, a\nDecision Tree, and a K-nearest neighbor classifier:\n\nThe `VotingClassifier` can also be used together with `GridSearchCV` in order\nto tune the hyperparameters of the individual estimators:\n\nIn order to predict the class labels based on the predicted class-\nprobabilities (scikit-learn estimators in the VotingClassifier must support\n`predict_proba` method):\n\nOptionally, weights can be provided for the individual classifiers:\n\nThe idea behind the `VotingRegressor` is to combine conceptually different\nmachine learning regressors and return the average predicted values. Such a\nregressor can be useful for a set of equally well performing models in order\nto balance out their individual weaknesses.\n\nThe following example shows how to fit the VotingRegressor:\n\nExamples:\n\nStacked generalization is a method for combining estimators to reduce their\nbiases [W1992] [HTF]. More precisely, the predictions of each individual\nestimator are stacked together and used as input to a final estimator to\ncompute the prediction. This final estimator is trained through cross-\nvalidation.\n\nThe `StackingClassifier` and `StackingRegressor` provide such strategies which\ncan be applied to classification and regression problems.\n\nThe `estimators` parameter corresponds to the list of the estimators which are\nstacked together in parallel on the input data. It should be given as a list\nof names and estimators:\n\nThe `final_estimator` will use the predictions of the `estimators` as input.\nIt needs to be a classifier or a regressor when using `StackingClassifier` or\n`StackingRegressor`, respectively:\n\nTo train the `estimators` and `final_estimator`, the `fit` method needs to be\ncalled on the training data:\n\nDuring training, the `estimators` are fitted on the whole training data\n`X_train`. They will be used when calling `predict` or `predict_proba`. To\ngeneralize and avoid over-fitting, the `final_estimator` is trained on out-\nsamples using `sklearn.model_selection.cross_val_predict` internally.\n\nFor `StackingClassifier`, note that the output of the `estimators` is\ncontrolled by the parameter `stack_method` and it is called by each estimator.\nThis parameter is either a string, being estimator method names, or `'auto'`\nwhich will automatically identify an available method depending on the\navailability, tested in the order of preference: `predict_proba`,\n`decision_function` and `predict`.\n\nA `StackingRegressor` and `StackingClassifier` can be used as any other\nregressor or classifier, exposing a `predict`, `predict_proba`, and\n`decision_function` methods, e.g.:\n\nNote that it is also possible to get the output of the stacked `estimators`\nusing the `transform` method:\n\nIn practice, a stacking predictor predicts as good as the best predictor of\nthe base layer and even sometimes outperforms it by combining the different\nstrengths of the these predictors. However, training a stacking predictor is\ncomputationally expensive.\n\nNote\n\nFor `StackingClassifier`, when using `stack_method_='predict_proba'`, the\nfirst column is dropped when the problem is a binary classification problem.\nIndeed, both probability columns predicted by each estimator are perfectly\ncollinear.\n\nNote\n\nMultiple stacking layers can be achieved by assigning `final_estimator` to a\n`StackingClassifier` or `StackingRegressor`:\n\nReferences\n\nWolpert, David H. \u201cStacked generalization.\u201d Neural networks 5.2 (1992):\n241-259.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.12. Multiclass and multioutput algorithms", "path": "modules/multiclass", "type": "Guide", "text": "\nThis section of the user guide covers functionality related to multi-learning\nproblems, including multiclass, multilabel, and multioutput classification and\nregression.\n\nThe modules in this section implement meta-estimators, which require a base\nestimator to be provided in their constructor. Meta-estimators extend the\nfunctionality of the base estimator to support multi-learning problems, which\nis accomplished by transforming the multi-learning problem into a set of\nsimpler problems, then fitting one estimator per problem.\n\nThis section covers two modules: `sklearn.multiclass` and\n`sklearn.multioutput`. The chart below demonstrates the problem types that\neach module is responsible for, and the corresponding meta-estimators that\neach module provides.\n\nThe table below provides a quick reference on the differences between problem\ntypes. More detailed explanations can be found in subsequent sections of this\nguide.\n\nNumber of targets\n\nTarget cardinality\n\nValid `type_of_target`\n\nMulticlass classification\n\n1\n\n>2\n\n\u2018multiclass\u2019\n\nMultilabel classification\n\n>1\n\n2 (0 or 1)\n\n\u2018multilabel-indicator\u2019\n\nMulticlass-multioutput classification\n\n>1\n\n>2\n\n\u2018multiclass-multioutput\u2019\n\nMultioutput regression\n\n>1\n\nContinuous\n\n\u2018continuous-multioutput\u2019\n\nBelow is a summary of scikit-learn estimators that have multi-learning support\nbuilt-in, grouped by strategy. You don\u2019t need the meta-estimators provided by\nthis section if you\u2019re using one of these estimators. However, meta-estimators\ncan provide additional strategies beyond what is built-in:\n\nInherently multiclass:\n\nMulticlass as One-Vs-One:\n\nMulticlass as One-Vs-The-Rest:\n\nSupport multilabel:\n\nSupport multiclass-multioutput:\n\nWarning\n\nAll classifiers in scikit-learn do multiclass classification out-of-the-box.\nYou don\u2019t need to use the `sklearn.multiclass` module unless you want to\nexperiment with different multiclass strategies.\n\nMulticlass classification is a classification task with more than two classes.\nEach sample can only be labeled as one class.\n\nFor example, classification using features extracted from a set of images of\nfruit, where each image may either be of an orange, an apple, or a pear. Each\nimage is one sample and is labeled as one of the 3 possible classes.\nMulticlass classification makes the assumption that each sample is assigned to\none and only one label - one sample cannot, for example, be both a pear and an\napple.\n\nWhile all scikit-learn classifiers are capable of multiclass classification,\nthe meta-estimators offered by `sklearn.multiclass` permit changing the way\nthey handle more than two classes because this may have an effect on\nclassifier performance (either in terms of generalization error or required\ncomputational resources).\n\nValid multiclass representations for `type_of_target` (`y`) are:\n\n1d or column vector containing more than two discrete values. An example of a\nvector `y` for 4 samples:\n\nDense or sparse binary matrix of shape `(n_samples, n_classes)` with a single\nsample per row, where each column represents one class. An example of both a\ndense and sparse binary matrix `y` for 4 samples, where the columns, in order,\nare apple, orange, and pear:\n\nFor more information about `LabelBinarizer`, refer to Transforming the\nprediction target (y).\n\nThe one-vs-rest strategy, also known as one-vs-all, is implemented in\n`OneVsRestClassifier`. The strategy consists in fitting one classifier per\nclass. For each classifier, the class is fitted against all the other classes.\nIn addition to its computational efficiency (only `n_classes` classifiers are\nneeded), one advantage of this approach is its interpretability. Since each\nclass is represented by one and only one classifier, it is possible to gain\nknowledge about the class by inspecting its corresponding classifier. This is\nthe most commonly used strategy and is a fair default choice.\n\nBelow is an example of multiclass learning using OvR:\n\n`OneVsRestClassifier` also supports multilabel classification. To use this\nfeature, feed the classifier an indicator matrix, in which cell [i, j]\nindicates the presence of label j in sample i.\n\nExamples:\n\n`OneVsOneClassifier` constructs one classifier per pair of classes. At\nprediction time, the class which received the most votes is selected. In the\nevent of a tie (among two classes with an equal number of votes), it selects\nthe class with the highest aggregate classification confidence by summing over\nthe pair-wise classification confidence levels computed by the underlying\nbinary classifiers.\n\nSince it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this\nmethod is usually slower than one-vs-the-rest, due to its O(n_classes^2)\ncomplexity. However, this method may be advantageous for algorithms such as\nkernel algorithms which don\u2019t scale well with `n_samples`. This is because\neach individual learning problem only involves a small subset of the data\nwhereas, with one-vs-the-rest, the complete dataset is used `n_classes` times.\nThe decision function is the result of a monotonic transformation of the one-\nversus-one classification.\n\nBelow is an example of multiclass learning using OvO:\n\nReferences:\n\nError-Correcting Output Code-based strategies are fairly different from one-\nvs-the-rest and one-vs-one. With these strategies, each class is represented\nin a Euclidean space, where each dimension can only be 0 or 1. Another way to\nput it is that each class is represented by a binary code (an array of 0 and\n1). The matrix which keeps track of the location/code of each class is called\nthe code book. The code size is the dimensionality of the aforementioned\nspace. Intuitively, each class should be represented by a code as unique as\npossible and a good code book should be designed to optimize classification\naccuracy. In this implementation, we simply use a randomly-generated code book\nas advocated in 3 although more elaborate methods may be added in the future.\n\nAt fitting time, one binary classifier per bit in the code book is fitted. At\nprediction time, the classifiers are used to project new points in the class\nspace and the class closest to the points is chosen.\n\nIn `OutputCodeClassifier`, the `code_size` attribute allows the user to\ncontrol the number of classifiers which will be used. It is a percentage of\nthe total number of classes.\n\nA number between 0 and 1 will require fewer classifiers than one-vs-the-rest.\nIn theory, `log2(n_classes) / n_classes` is sufficient to represent each class\nunambiguously. However, in practice, it may not lead to good accuracy since\n`log2(n_classes)` is much smaller than n_classes.\n\nA number greater than 1 will require more classifiers than one-vs-the-rest. In\nthis case, some classifiers will in theory correct for the mistakes made by\nother classifiers, hence the name \u201cerror-correcting\u201d. In practice, however,\nthis may not happen as classifier mistakes will typically be correlated. The\nerror-correcting output codes have a similar effect to bagging.\n\nBelow is an example of multiclass learning using Output-Codes:\n\nReferences:\n\n\u201cThe error coding method and PICTs\u201d, James G., Hastie T., Journal of\nComputational and Graphical statistics 7, 1998.\n\nMultilabel classification (closely related to multioutput classification) is a\nclassification task labeling each sample with `m` labels from `n_classes`\npossible classes, where `m` can be 0 to `n_classes` inclusive. This can be\nthought of as predicting properties of a sample that are not mutually\nexclusive. Formally, a binary output is assigned to each class, for every\nsample. Positive classes are indicated with 1 and negative classes with 0 or\n-1. It is thus comparable to running `n_classes` binary classification tasks,\nfor example with `MultiOutputClassifier`. This approach treats each label\nindependently whereas multilabel classifiers may treat the multiple classes\nsimultaneously, accounting for correlated behavior among them.\n\nFor example, prediction of the topics relevant to a text document or video.\nThe document or video may be about one of \u2018religion\u2019, \u2018politics\u2019, \u2018finance\u2019 or\n\u2018education\u2019, several of the topic classes or all of the topic classes.\n\nA valid representation of multilabel `y` is an either dense or sparse binary\nmatrix of shape `(n_samples, n_classes)`. Each column represents a class. The\n`1`\u2019s in each row denote the positive classes a sample has been labeled with.\nAn example of a dense matrix `y` for 3 samples:\n\nDense binary matrices can also be created using `MultiLabelBinarizer`. For\nmore information, refer to Transforming the prediction target (y).\n\nAn example of the same `y` in sparse matrix form:\n\nMultilabel classification support can be added to any classifier with\n`MultiOutputClassifier`. This strategy consists of fitting one classifier per\ntarget. This allows multiple target variable classifications. The purpose of\nthis class is to extend estimators to be able to estimate a series of target\nfunctions (f1,f2,f3\u2026,fn) that are trained on a single X predictor matrix to\npredict a series of responses (y1,y2,y3\u2026,yn).\n\nBelow is an example of multilabel classification:\n\nClassifier chains (see `ClassifierChain`) are a way of combining a number of\nbinary classifiers into a single multi-label model that is capable of\nexploiting correlations among targets.\n\nFor a multi-label classification problem with N classes, N binary classifiers\nare assigned an integer between 0 and N-1. These integers define the order of\nmodels in the chain. Each classifier is then fit on the available training\ndata plus the true labels of the classes whose models were assigned a lower\nnumber.\n\nWhen predicting, the true labels will not be available. Instead the\npredictions of each model are passed on to the subsequent models in the chain\nto be used as features.\n\nClearly the order of the chain is important. The first model in the chain has\nno information about the other labels while the last model in the chain has\nfeatures indicating the presence of all of the other labels. In general one\ndoes not know the optimal ordering of the models in the chain so typically\nmany randomly ordered chains are fit and their predictions are averaged\ntogether.\n\nReferences:\n\n\u201cClassifier Chains for Multi-label Classification\u201d, 2009.\n\nMulticlass-multioutput classification (also known as multitask classification)\nis a classification task which labels each sample with a set of non-binary\nproperties. Both the number of properties and the number of classes per\nproperty is greater than 2. A single estimator thus handles several joint\nclassification tasks. This is both a generalization of the multilabel\nclassification task, which only considers binary attributes, as well as a\ngeneralization of the multiclass classification task, where only one property\nis considered.\n\nFor example, classification of the properties \u201ctype of fruit\u201d and \u201ccolour\u201d for\na set of images of fruit. The property \u201ctype of fruit\u201d has the possible\nclasses: \u201capple\u201d, \u201cpear\u201d and \u201corange\u201d. The property \u201ccolour\u201d has the possible\nclasses: \u201cgreen\u201d, \u201cred\u201d, \u201cyellow\u201d and \u201corange\u201d. Each sample is an image of a\nfruit, a label is output for both properties and each label is one of the\npossible classes of the corresponding property.\n\nNote that all classifiers handling multiclass-multioutput (also known as\nmultitask classification) tasks, support the multilabel classification task as\na special case. Multitask classification is similar to the multioutput\nclassification task with different model formulations. For more information,\nsee the relevant estimator documentation.\n\nWarning\n\nAt present, no metric in `sklearn.metrics` supports the multiclass-multioutput\nclassification task.\n\nA valid representation of multioutput `y` is a dense matrix of shape\n`(n_samples, n_classes)` of class labels. A column wise concatenation of 1d\nmulticlass variables. An example of `y` for 3 samples:\n\nMultioutput regression predicts multiple numerical properties for each sample.\nEach property is a numerical variable and the number of properties to be\npredicted for each sample is greater than or equal to 2. Some estimators that\nsupport multioutput regression are faster than just running `n_output`\nestimators.\n\nFor example, prediction of both wind speed and wind direction, in degrees,\nusing data obtained at a certain location. Each sample would be data obtained\nat one location and both wind speed and direction would be output for each\nsample.\n\nA valid representation of multioutput `y` is a dense matrix of shape\n`(n_samples, n_classes)` of floats. A column wise concatenation of continuous\nvariables. An example of `y` for 3 samples:\n\nMultioutput regression support can be added to any regressor with\n`MultiOutputRegressor`. This strategy consists of fitting one regressor per\ntarget. Since each target is represented by exactly one regressor it is\npossible to gain knowledge about the target by inspecting its corresponding\nregressor. As `MultiOutputRegressor` fits one regressor per target it can not\ntake advantage of correlations between targets.\n\nBelow is an example of multioutput regression:\n\nRegressor chains (see `RegressorChain`) is analogous to `ClassifierChain` as a\nway of combining a number of regressions into a single multi-target model that\nis capable of exploiting correlations among targets.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.13. Feature selection", "path": "modules/feature_selection", "type": "Guide", "text": "\nThe classes in the `sklearn.feature_selection` module can be used for feature\nselection/dimensionality reduction on sample sets, either to improve\nestimators\u2019 accuracy scores or to boost their performance on very high-\ndimensional datasets.\n\n`VarianceThreshold` is a simple baseline approach to feature selection. It\nremoves all features whose variance doesn\u2019t meet some threshold. By default,\nit removes all zero-variance features, i.e. features that have the same value\nin all samples.\n\nAs an example, suppose that we have a dataset with boolean features, and we\nwant to remove all features that are either one or zero (on or off) in more\nthan 80% of the samples. Boolean features are Bernoulli random variables, and\nthe variance of such variables is given by\n\nso we can select using the threshold `.8 * (1 - .8)`:\n\nAs expected, `VarianceThreshold` has removed the first column, which has a\nprobability \\\\(p = 5/6 > .8\\\\) of containing a zero.\n\nUnivariate feature selection works by selecting the best features based on\nunivariate statistical tests. It can be seen as a preprocessing step to an\nestimator. Scikit-learn exposes feature selection routines as objects that\nimplement the `transform` method:\n\nFor instance, we can perform a \\\\(\\chi^2\\\\) test to the samples to retrieve\nonly the two best features as follows:\n\nThese objects take as input a scoring function that returns univariate scores\nand p-values (or only scores for `SelectKBest` and `SelectPercentile`):\n\nThe methods based on F-test estimate the degree of linear dependency between\ntwo random variables. On the other hand, mutual information methods can\ncapture any kind of statistical dependency, but being nonparametric, they\nrequire more samples for accurate estimation.\n\nFeature selection with sparse data\n\nIf you use sparse data (i.e. data represented as sparse matrices), `chi2`,\n`mutual_info_regression`, `mutual_info_classif` will deal with the data\nwithout making it dense.\n\nWarning\n\nBeware not to use a regression scoring function with a classification problem,\nyou will get useless results.\n\nExamples:\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(`RFE`) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and the importance of each feature is obtained either through any\nspecific attribute (such as `coef_`, `feature_importances_`) or callable.\nThen, the least important features are pruned from current set of features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\n`RFECV` performs RFE in a cross-validation loop to find the optimal number of\nfeatures.\n\nExamples:\n\n`SelectFromModel` is a meta-transformer that can be used along with any\nestimator that importance of each feature through a specific attribute (such\nas `coef_`, `feature_importances_`) or callable after fitting. The features\nare considered unimportant and removed, if the corresponding importance of the\nfeature values are below the provided `threshold` parameter. Apart from\nspecifying the threshold numerically, there are built-in heuristics for\nfinding a threshold using a string argument. Available heuristics are \u201cmean\u201d,\n\u201cmedian\u201d and float multiples of these like \u201c0.1*mean\u201d. In combination with the\n`threshold` criteria, one can use the `max_features` parameter to set a limit\non the number of features to select.\n\nFor examples on how it is to be used refer to the sections below.\n\nExamples\n\nLinear models penalized with the L1 norm have sparse solutions: many of their\nestimated coefficients are zero. When the goal is to reduce the dimensionality\nof the data to use with another classifier, they can be used along with\n`SelectFromModel` to select the non-zero coefficients. In particular, sparse\nestimators useful for this purpose are the `Lasso` for regression, and of\n`LogisticRegression` and `LinearSVC` for classification:\n\nWith SVMs and logistic-regression, the parameter C controls the sparsity: the\nsmaller C the fewer features selected. With Lasso, the higher the alpha\nparameter, the fewer features selected.\n\nExamples:\n\nL1-recovery and compressive sensing\n\nFor a good choice of alpha, the Lasso can fully recover the exact set of non-\nzero variables using only few observations, provided certain specific\nconditions are met. In particular, the number of samples should be\n\u201csufficiently large\u201d, or L1 models will perform at random, where \u201csufficiently\nlarge\u201d depends on the number of non-zero coefficients, the logarithm of the\nnumber of features, the amount of noise, the smallest absolute value of non-\nzero coefficients, and the structure of the design matrix X. In addition, the\ndesign matrix must display certain specific properties, such as not being too\ncorrelated.\n\nThere is no general rule to select an alpha parameter for recovery of non-zero\ncoefficients. It can by set by cross-validation (`LassoCV` or `LassoLarsCV`),\nthough this may lead to under-penalized models: including a small number of\nnon-relevant variables is not detrimental to prediction score. BIC\n(`LassoLarsIC`) tends, on the opposite, to set high values of alpha.\n\nReference Richard G. Baraniuk \u201cCompressive Sensing\u201d, IEEE Signal Processing\nMagazine [120] July 2007 http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf\n\nTree-based estimators (see the `sklearn.tree` module and forest of trees in\nthe `sklearn.ensemble` module) can be used to compute impurity-based feature\nimportances, which in turn can be used to discard irrelevant features (when\ncoupled with the `SelectFromModel` meta-transformer):\n\nExamples:\n\nSequential Feature Selection [sfs] (SFS) is available in the\n`SequentialFeatureSelector` transformer. SFS can be either forward or\nbackward:\n\nForward-SFS is a greedy procedure that iteratively finds the best new feature\nto add to the set of selected features. Concretely, we initially start with\nzero feature and find the one feature that maximizes a cross-validated score\nwhen an estimator is trained on this single feature. Once that first feature\nis selected, we repeat the procedure by adding a new feature to the set of\nselected features. The procedure stops when the desired number of selected\nfeatures is reached, as determined by the `n_features_to_select` parameter.\n\nBackward-SFS follows the same idea but works in the opposite direction:\ninstead of starting with no feature and greedily adding features, we start\nwith all the features and greedily remove features from the set. The\n`direction` parameter controls whether forward or backward SFS is used.\n\nIn general, forward and backward selection do not yield equivalent results.\nAlso, one may be much faster than the other depending on the requested number\nof selected features: if we have 10 features and ask for 7 selected features,\nforward selection would need to perform 7 iterations while backward selection\nwould only need to perform 3.\n\nSFS differs from `RFE` and `SelectFromModel` in that it does not require the\nunderlying model to expose a `coef_` or `feature_importances_` attribute. It\nmay however be slower considering that more models need to be evaluated,\ncompared to the other approaches. For example in backward selection, the\niteration going from `m` features to `m - 1` features using k-fold cross-\nvalidation requires fitting `m * k` models, while `RFE` would require only a\nsingle fit, and `SelectFromModel` always just does a single fit and requires\nno iterations.\n\nExamples\n\nReferences:\n\nFerri et al, Comparative study of techniques for large-scale feature\nselection.\n\nFeature selection is usually used as a pre-processing step before doing the\nactual learning. The recommended way to do this in scikit-learn is to use a\n`Pipeline`:\n\nIn this snippet we make use of a `LinearSVC` coupled with `SelectFromModel` to\nevaluate feature importances and select the most relevant features. Then, a\n`RandomForestClassifier` is trained on the transformed output, i.e. using only\nrelevant features. You can perform similar operations with the other feature\nselection methods and also classifiers that provide a way to evaluate feature\nimportances of course. See the `Pipeline` examples for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.14. Semi-supervised learning", "path": "modules/semi_supervised", "type": "Guide", "text": "\nSemi-supervised learning is a situation in which in your training data some of\nthe samples are not labeled. The semi-supervised estimators in\n`sklearn.semi_supervised` are able to make use of this additional unlabeled\ndata to better capture the shape of the underlying data distribution and\ngeneralize better to new samples. These algorithms can perform well when we\nhave a very small amount of labeled points and a large amount of unlabeled\npoints.\n\nUnlabeled entries in `y`\n\nIt is important to assign an identifier to unlabeled points along with the\nlabeled data when training the model with the `fit` method. The identifier\nthat this implementation uses is the integer value \\\\(-1\\\\). Note that for\nstring labels, the dtype of `y` should be object so that it can contain both\nstrings and integers.\n\nNote\n\nSemi-supervised algorithms need to make assumptions about the distribution of\nthe dataset in order to achieve performance gains. See here for more details.\n\nThis self-training implementation is based on Yarowsky\u2019s 1 algorithm. Using\nthis algorithm, a given supervised classifier can function as a semi-\nsupervised classifier, allowing it to learn from unlabeled data.\n\n`SelfTrainingClassifier` can be called with any classifier that implements\n`predict_proba`, passed as the parameter `base_classifier`. In each iteration,\nthe `base_classifier` predicts labels for the unlabeled samples and adds a\nsubset of these labels to the labeled dataset.\n\nThe choice of this subset is determined by the selection criterion. This\nselection can be done using a `threshold` on the prediction probabilities, or\nby choosing the `k_best` samples according to the prediction probabilities.\n\nThe labels used for the final fit as well as the iteration in which each\nsample was labeled are available as attributes. The optional `max_iter`\nparameter specifies how many times the loop is executed at most.\n\nThe `max_iter` parameter may be set to `None`, causing the algorithm to\niterate until all samples have labels or no new samples are selected in that\niteration.\n\nNote\n\nWhen using the self-training classifier, the calibration of the classifier is\nimportant.\n\nExamples\n\nReferences\n\nDavid Yarowsky. 1995. Unsupervised word sense disambiguation rivaling\nsupervised methods. In Proceedings of the 33rd annual meeting on Association\nfor Computational Linguistics (ACL \u201895). Association for Computational\nLinguistics, Stroudsburg, PA, USA, 189-196. DOI:\nhttps://doi.org/10.3115/981658.981684\n\nLabel propagation denotes a few variations of semi-supervised graph inference\nalgorithms.\n\n`scikit-learn` provides two label propagation models: `LabelPropagation` and\n`LabelSpreading`. Both work by constructing a similarity graph over all items\nin the input dataset.\n\nAn illustration of label-propagation: the structure of unlabeled observations\nis consistent with the class structure, and thus the class label can be\npropagated to the unlabeled observations of the training set.\n\n`LabelPropagation` and `LabelSpreading` differ in modifications to the\nsimilarity matrix that graph and the clamping effect on the label\ndistributions. Clamping allows the algorithm to change the weight of the true\nground labeled data to some degree. The `LabelPropagation` algorithm performs\nhard clamping of input labels, which means \\\\(\\alpha=0\\\\). This clamping\nfactor can be relaxed, to say \\\\(\\alpha=0.2\\\\), which means that we will\nalways retain 80 percent of our original label distribution, but the algorithm\ngets to change its confidence of the distribution within 20 percent.\n\n`LabelPropagation` uses the raw similarity matrix constructed from the data\nwith no modifications. In contrast, `LabelSpreading` minimizes a loss function\nthat has regularization properties, as such it is often more robust to noise.\nThe algorithm iterates on a modified version of the original graph and\nnormalizes the edge weights by computing the normalized graph Laplacian\nmatrix. This procedure is also used in Spectral clustering.\n\nLabel propagation models have two built-in kernel methods. Choice of kernel\neffects both scalability and performance of the algorithms. The following are\navailable:\n\nThe RBF kernel will produce a fully connected graph which is represented in\nmemory by a dense matrix. This matrix may be very large and combined with the\ncost of performing a full matrix multiplication calculation for each iteration\nof the algorithm can lead to prohibitively long running times. On the other\nhand, the KNN kernel will produce a much more memory-friendly sparse matrix\nwhich can drastically reduce running times.\n\nExamples\n\nReferences\n\n[2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised\nLearning (2006), pp. 193-216\n\n[3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient Non-\nParametric Function Induction in Semi-Supervised Learning. AISTAT 2005\nhttps://research.microsoft.com/en-us/people/nicolasl/efficient_ssl.pdf\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.15. Isotonic regression", "path": "modules/isotonic", "type": "Guide", "text": "\nThe class `IsotonicRegression` fits a non-decreasing real function to\n1-dimensional data. It solves the following problem:\n\nminimize \\\\(\\sum_i w_i (y_i - \\hat{y}_i)^2\\\\)\n\nsubject to \\\\(\\hat{y}_i \\le \\hat{y}_j\\\\) whenever \\\\(X_i \\le X_j\\\\),\n\nwhere the weights \\\\(w_i\\\\) are strictly positive, and both `X` and `y` are\narbitrary real quantities.\n\nThe `increasing` parameter changes the constraint to \\\\(\\hat{y}_i \\ge\n\\hat{y}_j\\\\) whenever \\\\(X_i \\le X_j\\\\). Setting it to \u2018auto\u2019 will\nautomatically choose the constraint based on Spearman\u2019s rank correlation\ncoefficient.\n\n`IsotonicRegression` produces a series of predictions \\\\(\\hat{y}_i\\\\) for the\ntraining data which are the closest to the targets \\\\(y\\\\) in terms of mean\nsquared error. These predictions are interpolated for predicting to unseen\ndata. The predictions of `IsotonicRegression` thus form a function that is\npiecewise linear:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.16. Probability calibration", "path": "modules/calibration", "type": "Guide", "text": "\nWhen performing classification you often want not only to predict the class\nlabel, but also obtain a probability of the respective label. This probability\ngives you some kind of confidence on the prediction. Some models can give you\npoor estimates of the class probabilities and some even do not support\nprobability prediction (e.g., some instances of `SGDClassifier`). The\ncalibration module allows you to better calibrate the probabilities of a given\nmodel, or to add support for probability prediction.\n\nWell calibrated classifiers are probabilistic classifiers for which the output\nof the predict_proba method can be directly interpreted as a confidence level.\nFor instance, a well calibrated (binary) classifier should classify the\nsamples such that among the samples to which it gave a predict_proba value\nclose to 0.8, approximately 80% actually belong to the positive class.\n\nThe following plot compares how well the probabilistic predictions of\ndifferent classifiers are calibrated, using `calibration_curve`. The x axis\nrepresents the average predicted probability in each bin. The y axis is the\nfraction of positives, i.e. the proportion of samples whose class is the\npositive class (in each bin).\n\n`LogisticRegression` returns well calibrated predictions by default as it\ndirectly optimizes Log loss. In contrast, the other methods return biased\nprobabilities; with different biases per method:\n\n`GaussianNB` tends to push probabilities to 0 or 1 (note the counts in the\nhistograms). This is mainly because it makes the assumption that features are\nconditionally independent given the class, which is not the case in this\ndataset which contains 2 redundant features.\n\n`RandomForestClassifier` shows the opposite behavior: the histograms show\npeaks at approximately 0.2 and 0.9 probability, while probabilities close to 0\nor 1 are very rare. An explanation for this is given by Niculescu-Mizil and\nCaruana 1: \u201cMethods such as bagging and random forests that average\npredictions from a base set of models can have difficulty making predictions\nnear 0 and 1 because variance in the underlying base models will bias\npredictions that should be near zero or one away from these values. Because\npredictions are restricted to the interval [0,1], errors caused by variance\ntend to be one-sided near zero and one. For example, if a model should predict\np = 0 for a case, the only way bagging can achieve this is if all bagged trees\npredict zero. If we add noise to the trees that bagging is averaging over,\nthis noise will cause some trees to predict values larger than 0 for this\ncase, thus moving the average prediction of the bagged ensemble away from 0.\nWe observe this effect most strongly with random forests because the base-\nlevel trees trained with random forests have relatively high variance due to\nfeature subsetting.\u201d As a result, the calibration curve also referred to as\nthe reliability diagram (Wilks 1995 2) shows a characteristic sigmoid shape,\nindicating that the classifier could trust its \u201cintuition\u201d more and return\nprobabilities closer to 0 or 1 typically.\n\nLinear Support Vector Classification (`LinearSVC`) shows an even more sigmoid\ncurve than `RandomForestClassifier`, which is typical for maximum-margin\nmethods (compare Niculescu-Mizil and Caruana 1), which focus on difficult to\nclassify samples that are close to the decision boundary (the support\nvectors).\n\nCalibrating a classifier consists of fitting a regressor (called a calibrator)\nthat maps the output of the classifier (as given by decision_function or\npredict_proba) to a calibrated probability in [0, 1]. Denoting the output of\nthe classifier for a given sample by \\\\(f_i\\\\), the calibrator tries to\npredict \\\\(p(y_i = 1 | f_i)\\\\).\n\nThe samples that are used to fit the calibrator should not be the same samples\nused to fit the classifier, as this would introduce bias. This is because\nperformance of the classifier on its training data would be better than for\nnovel data. Using the classifier output of training data to fit the calibrator\nwould thus result in a biased calibrator that maps to probabilities closer to\n0 and 1 than it should.\n\nThe `CalibratedClassifierCV` class is used to calibrate a classifier.\n\n`CalibratedClassifierCV` uses a cross-validation approach to ensure unbiased\ndata is always used to fit the calibrator. The data is split into k\n`(train_set, test_set)` couples (as determined by `cv`). When `ensemble=True`\n(default), the following procedure is repeated independently for each cross-\nvalidation split: a clone of `base_estimator` is first trained on the train\nsubset. Then its predictions on the test subset are used to fit a calibrator\n(either a sigmoid or isotonic regressor). This results in an ensemble of k\n`(classifier, calibrator)` couples where each calibrator maps the output of\nits corresponding classifier into [0, 1]. Each couple is exposed in the\n`calibrated_classifiers_` attribute, where each entry is a calibrated\nclassifier with a predict_proba method that outputs calibrated probabilities.\nThe output of predict_proba for the main `CalibratedClassifierCV` instance\ncorresponds to the average of the predicted probabilities of the `k`\nestimators in the `calibrated_classifiers_` list. The output of predict is the\nclass that has the highest probability.\n\nWhen `ensemble=False`, cross-validation is used to obtain \u2018unbiased\u2019\npredictions for all the data, via `cross_val_predict`. These unbiased\npredictions are then used to train the calibrator. The attribute\n`calibrated_classifiers_` consists of only one `(classifier, calibrator)`\ncouple where the classifier is the `base_estimator` trained on all the data.\nIn this case the output of predict_proba for `CalibratedClassifierCV` is the\npredicted probabilities obtained from the single `(classifier, calibrator)`\ncouple.\n\nThe main advantage of `ensemble=True` is to benefit from the traditional\nensembling effect (similar to Bagging meta-estimator). The resulting ensemble\nshould both be well calibrated and slightly more accurate than with\n`ensemble=False`. The main advantage of using `ensemble=False` is\ncomputational: it reduces the overall fit time by training only a single base\nclassifier and calibrator pair, decreases the final model size and increases\nprediction speed.\n\nAlternatively an already fitted classifier can be calibrated by setting\n`cv=\"prefit\"`. In this case, the data is not split and all of it is used to\nfit the regressor. It is up to the user to make sure that the data used for\nfitting the classifier is disjoint from the data used for fitting the\nregressor.\n\n`sklearn.metrics.brier_score_loss` may be used to assess how well a classifier\nis calibrated. However, this metric should be used with care because a lower\nBrier score does not always mean a better calibrated model. This is because\nthe Brier score metric is a combination of calibration loss and refinement\nloss. Calibration loss is defined as the mean squared deviation from empirical\nprobabilities derived from the slope of ROC segments. Refinement loss can be\ndefined as the expected optimal loss as measured by the area under the optimal\ncost curve. As refinement loss can change independently from calibration loss,\na lower Brier score does not necessarily mean a better calibrated model.\n\n`CalibratedClassifierCV` supports the use of two \u2018calibration\u2019 regressors:\n\u2018sigmoid\u2019 and \u2018isotonic\u2019.\n\nThe sigmoid regressor is based on Platt\u2019s logistic model 3:\n\nwhere \\\\(y_i\\\\) is the true label of sample \\\\(i\\\\) and \\\\(f_i\\\\) is the\noutput of the un-calibrated classifier for sample \\\\(i\\\\). \\\\(A\\\\) and \\\\(B\\\\)\nare real numbers to be determined when fitting the regressor via maximum\nlikelihood.\n\nThe sigmoid method assumes the calibration curve can be corrected by applying\na sigmoid function to the raw predictions. This assumption has been\nempirically justified in the case of Support Vector Machines with common\nkernel functions on various benchmark datasets in section 2.1 of Platt 1999 3\nbut does not necessarily hold in general. Additionally, the logistic model\nworks best if the calibration error is symmetrical, meaning the classifier\noutput for each binary class is normally distributed with the same variance 6.\nThis is can be a problem for highly imbalanced classification problems, where\noutputs do not have equal variance.\n\nIn general this method is most effective when the un-calibrated model is\nunder-confident and has similar calibration errors for both high and low\noutputs.\n\nThe \u2018isotonic\u2019 method fits a non-parametric isotonic regressor, which outputs\na step-wise non-decreasing function (see `sklearn.isotonic`). It minimizes:\n\nsubject to \\\\(\\hat{f}_i >= \\hat{f}_j\\\\) whenever \\\\(f_i >= f_j\\\\). \\\\(y_i\\\\)\nis the true label of sample \\\\(i\\\\) and \\\\(\\hat{f}_i\\\\) is the output of the\ncalibrated classifier for sample \\\\(i\\\\) (i.e., the calibrated probability).\nThis method is more general when compared to \u2018sigmoid\u2019 as the only restriction\nis that the mapping function is monotonically increasing. It is thus more\npowerful as it can correct any monotonic distortion of the un-calibrated\nmodel. However, it is more prone to overfitting, especially on small datasets\n5.\n\nOverall, \u2018isotonic\u2019 will perform as well as or better than \u2018sigmoid\u2019 when\nthere is enough data (greater than ~ 1000 samples) to avoid overfitting 1.\n\nBoth isotonic and sigmoid regressors only support 1-dimensional data (e.g.,\nbinary classification output) but are extended for multiclass classification\nif the `base_estimator` supports multiclass predictions. For multiclass\npredictions, `CalibratedClassifierCV` calibrates for each class separately in\na OneVsRestClassifier fashion 4. When predicting probabilities, the calibrated\nprobabilities for each class are predicted separately. As those probabilities\ndo not necessarily sum to one, a postprocessing is performed to normalize\nthem.\n\nExamples:\n\nReferences:\n\nPredicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil &\nR. Caruana, ICML 2005\n\nOn the combination of forecast probabilities for consecutive precipitation\nperiods. Wea. Forecasting, 5, 640\u2013650., Wilks, D. S., 1990a\n\nProbabilistic Outputs for Support Vector Machines and Comparisons to\nRegularized Likelihood Methods. J. Platt, (1999)\n\nTransforming Classifier Scores into Accurate Multiclass Probability Estimates.\nB. Zadrozny & C. Elkan, (KDD 2002)\n\nPredicting accurate probabilities with a ranking loss. Menon AK, Jiang XJ,\nVembu S, Elkan C, Ohno-Machado L. Proc Int Conf Mach Learn. 2012;2012:703-710\n\nBeyond sigmoids: How to obtain well-calibrated probabilities from binary\nclassifiers with beta calibration Kull, M., Silva Filho, T. M., & Flach, P.\n(2017).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "1.17. Neural network models", "path": "modules/neural_networks_supervised", "type": "Guide", "text": "\nWarning\n\nThis implementation is not intended for large-scale applications. In\nparticular, scikit-learn offers no GPU support. For much faster, GPU-based\nimplementations, as well as frameworks offering much more flexibility to build\ndeep learning architectures, see Related Projects.\n\nMulti-layer Perceptron (MLP) is a supervised learning algorithm that learns a\nfunction \\\\(f(\\cdot): R^m \\rightarrow R^o\\\\) by training on a dataset, where\n\\\\(m\\\\) is the number of dimensions for input and \\\\(o\\\\) is the number of\ndimensions for output. Given a set of features \\\\(X = {x_1, x_2, ..., x_m}\\\\)\nand a target \\\\(y\\\\), it can learn a non-linear function approximator for\neither classification or regression. It is different from logistic regression,\nin that between the input and the output layer, there can be one or more non-\nlinear layers, called hidden layers. Figure 1 shows a one hidden layer MLP\nwith scalar output.\n\nFigure 1 : One hidden layer MLP.\n\nThe leftmost layer, known as the input layer, consists of a set of neurons\n\\\\(\\\\{x_i | x_1, x_2, ..., x_m\\\\}\\\\) representing the input features. Each\nneuron in the hidden layer transforms the values from the previous layer with\na weighted linear summation \\\\(w_1x_1 + w_2x_2 + ... + w_mx_m\\\\), followed by\na non-linear activation function \\\\(g(\\cdot):R \\rightarrow R\\\\) \\- like the\nhyperbolic tan function. The output layer receives the values from the last\nhidden layer and transforms them into output values.\n\nThe module contains the public attributes `coefs_` and `intercepts_`. `coefs_`\nis a list of weight matrices, where weight matrix at index \\\\(i\\\\) represents\nthe weights between layer \\\\(i\\\\) and layer \\\\(i+1\\\\). `intercepts_` is a list\nof bias vectors, where the vector at index \\\\(i\\\\) represents the bias values\nadded to layer \\\\(i+1\\\\).\n\nThe advantages of Multi-layer Perceptron are:\n\nThe disadvantages of Multi-layer Perceptron (MLP) include:\n\nPlease see Tips on Practical Use section that addresses some of these\ndisadvantages.\n\nClass `MLPClassifier` implements a multi-layer perceptron (MLP) algorithm that\ntrains using Backpropagation.\n\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\nthe training samples represented as floating point feature vectors; and array\ny of size (n_samples,), which holds the target values (class labels) for the\ntraining samples:\n\nAfter fitting (training), the model can predict labels for new samples:\n\nMLP can fit a non-linear model to the training data. `clf.coefs_` contains the\nweight matrices that constitute the model parameters:\n\nCurrently, `MLPClassifier` supports only the Cross-Entropy loss function,\nwhich allows probability estimates by running the `predict_proba` method.\n\nMLP trains using Backpropagation. More precisely, it trains using some form of\ngradient descent and the gradients are calculated using Backpropagation. For\nclassification, it minimizes the Cross-Entropy loss function, giving a vector\nof probability estimates \\\\(P(y|x)\\\\) per sample \\\\(x\\\\):\n\n`MLPClassifier` supports multi-class classification by applying Softmax as the\noutput function.\n\nFurther, the model supports multi-label classification in which a sample can\nbelong to more than one class. For each class, the raw output passes through\nthe logistic function. Values larger or equal to `0.5` are rounded to `1`,\notherwise to `0`. For a predicted output of a sample, the indices where the\nvalue is `1` represents the assigned classes of that sample:\n\nSee the examples below and the docstring of `MLPClassifier.fit` for further\ninformation.\n\nExamples:\n\nClass `MLPRegressor` implements a multi-layer perceptron (MLP) that trains\nusing backpropagation with no activation function in the output layer, which\ncan also be seen as using the identity function as activation function.\nTherefore, it uses the square error as the loss function, and the output is a\nset of continuous values.\n\n`MLPRegressor` also supports multi-output regression, in which a sample can\nhave more than one target.\n\nBoth `MLPRegressor` and `MLPClassifier` use parameter `alpha` for\nregularization (L2 regularization) term which helps in avoiding overfitting by\npenalizing weights with large magnitudes. Following plot displays varying\ndecision function with value of alpha.\n\nSee the examples below for further information.\n\nExamples:\n\nMLP trains using Stochastic Gradient Descent, Adam, or L-BFGS. Stochastic\nGradient Descent (SGD) updates parameters using the gradient of the loss\nfunction with respect to a parameter that needs adaptation, i.e.\n\nwhere \\\\(\\eta\\\\) is the learning rate which controls the step-size in the\nparameter space search. \\\\(Loss\\\\) is the loss function used for the network.\n\nMore details can be found in the documentation of SGD\n\nAdam is similar to SGD in a sense that it is a stochastic optimizer, but it\ncan automatically adjust the amount to update parameters based on adaptive\nestimates of lower-order moments.\n\nWith SGD or Adam, training supports online and mini-batch learning.\n\nL-BFGS is a solver that approximates the Hessian matrix which represents the\nsecond-order partial derivative of a function. Further it approximates the\ninverse of the Hessian matrix to perform parameter updates. The implementation\nuses the Scipy version of L-BFGS.\n\nIf the selected solver is \u2018L-BFGS\u2019, training does not support online nor mini-\nbatch learning.\n\nSuppose there are \\\\(n\\\\) training samples, \\\\(m\\\\) features, \\\\(k\\\\) hidden\nlayers, each containing \\\\(h\\\\) neurons - for simplicity, and \\\\(o\\\\) output\nneurons. The time complexity of backpropagation is \\\\(O(n\\cdot m \\cdot h^k\n\\cdot o \\cdot i)\\\\), where \\\\(i\\\\) is the number of iterations. Since\nbackpropagation has a high time complexity, it is advisable to start with\nsmaller number of hidden neurons and few hidden layers for training.\n\nGiven a set of training examples \\\\((x_1, y_1), (x_2, y_2), \\ldots, (x_n,\ny_n)\\\\) where \\\\(x_i \\in \\mathbf{R}^n\\\\) and \\\\(y_i \\in \\\\{0, 1\\\\}\\\\), a one\nhidden layer one hidden neuron MLP learns the function \\\\(f(x) = W_2 g(W_1^T x\n+ b_1) + b_2\\\\) where \\\\(W_1 \\in \\mathbf{R}^m\\\\) and \\\\(W_2, b_1, b_2 \\in\n\\mathbf{R}\\\\) are model parameters. \\\\(W_1, W_2\\\\) represent the weights of\nthe input layer and hidden layer, respectively; and \\\\(b_1, b_2\\\\) represent\nthe bias added to the hidden layer and the output layer, respectively.\n\\\\(g(\\cdot) : R \\rightarrow R\\\\) is the activation function, set by default as\nthe hyperbolic tan. It is given as,\n\nFor binary classification, \\\\(f(x)\\\\) passes through the logistic function\n\\\\(g(z)=1/(1+e^{-z})\\\\) to obtain output values between zero and one. A\nthreshold, set to 0.5, would assign samples of outputs larger or equal 0.5 to\nthe positive class, and the rest to the negative class.\n\nIf there are more than two classes, \\\\(f(x)\\\\) itself would be a vector of\nsize (n_classes,). Instead of passing through logistic function, it passes\nthrough the softmax function, which is written as,\n\nwhere \\\\(z_i\\\\) represents the \\\\(i\\\\) th element of the input to softmax,\nwhich corresponds to class \\\\(i\\\\), and \\\\(K\\\\) is the number of classes. The\nresult is a vector containing the probabilities that sample \\\\(x\\\\) belong to\neach class. The output is the class with the highest probability.\n\nIn regression, the output remains as \\\\(f(x)\\\\); therefore, output activation\nfunction is just the identity function.\n\nMLP uses different loss functions depending on the problem type. The loss\nfunction for classification is Cross-Entropy, which in binary case is given\nas,\n\nwhere \\\\(\\alpha ||W||_2^2\\\\) is an L2-regularization term (aka penalty) that\npenalizes complex models; and \\\\(\\alpha > 0\\\\) is a non-negative\nhyperparameter that controls the magnitude of the penalty.\n\nFor regression, MLP uses the Square Error loss function; written as,\n\nStarting from initial random weights, multi-layer perceptron (MLP) minimizes\nthe loss function by repeatedly updating these weights. After computing the\nloss, a backward pass propagates it from the output layer to the previous\nlayers, providing each weight parameter with an update value meant to decrease\nthe loss.\n\nIn gradient descent, the gradient \\\\(\\nabla Loss_{W}\\\\) of the loss with\nrespect to the weights is computed and deducted from \\\\(W\\\\). More formally,\nthis is expressed as,\n\nwhere \\\\(i\\\\) is the iteration step, and \\\\(\\epsilon\\\\) is the learning rate\nwith a value larger than 0.\n\nThe algorithm stops when it reaches a preset maximum number of iterations; or\nwhen the improvement in loss is below a certain, small number.\n\nMulti-layer Perceptron is sensitive to feature scaling, so it is highly\nrecommended to scale your data. For example, scale each attribute on the input\nvector X to [0, 1] or [-1, +1], or standardize it to have mean 0 and variance\n1. Note that you must apply the same scaling to the test set for meaningful\nresults. You can use `StandardScaler` for standardization.\n\nAn alternative and recommended approach is to use `StandardScaler` in a\n`Pipeline`\n\nIf you want more control over stopping criteria or learning rate in SGD, or\nwant to do additional monitoring, using `warm_start=True` and `max_iter=1` and\niterating yourself can be helpful:\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.1. Gaussian mixture models", "path": "modules/mixture", "type": "Guide", "text": "\n`sklearn.mixture` is a package which enables one to learn Gaussian Mixture\nModels (diagonal, spherical, tied and full covariance matrices supported),\nsample them, and estimate them from data. Facilities to help determine the\nappropriate number of components are also provided.\n\nTwo-component Gaussian mixture model: data points, and equi-probability\nsurfaces of the model.\n\nA Gaussian mixture model is a probabilistic model that assumes all the data\npoints are generated from a mixture of a finite number of Gaussian\ndistributions with unknown parameters. One can think of mixture models as\ngeneralizing k-means clustering to incorporate information about the\ncovariance structure of the data as well as the centers of the latent\nGaussians.\n\nScikit-learn implements different classes to estimate Gaussian mixture models,\nthat correspond to different estimation strategies, detailed below.\n\nThe `GaussianMixture` object implements the expectation-maximization (EM)\nalgorithm for fitting mixture-of-Gaussian models. It can also draw confidence\nellipsoids for multivariate models, and compute the Bayesian Information\nCriterion to assess the number of clusters in the data. A\n`GaussianMixture.fit` method is provided that learns a Gaussian Mixture Model\nfrom train data. Given test data, it can assign to each sample the Gaussian it\nmostly probably belong to using the `GaussianMixture.predict` method.\n\nThe `GaussianMixture` comes with different options to constrain the covariance\nof the difference classes estimated: spherical, diagonal, tied or full\ncovariance.\n\nExamples:\n\nIt is the fastest algorithm for learning mixture models\n\nAs this algorithm maximizes only the likelihood, it will not bias the means\ntowards zero, or bias the cluster sizes to have specific structures that might\nor might not apply.\n\nWhen one has insufficiently many points per mixture, estimating the covariance\nmatrices becomes difficult, and the algorithm is known to diverge and find\nsolutions with infinite likelihood unless one regularizes the covariances\nartificially.\n\nThis algorithm will always use all the components it has access to, needing\nheld-out data or information theoretical criteria to decide how many\ncomponents to use in the absence of external cues.\n\nThe BIC criterion can be used to select the number of components in a Gaussian\nMixture in an efficient way. In theory, it recovers the true number of\ncomponents only in the asymptotic regime (i.e. if much data is available and\nassuming that the data was actually generated i.i.d. from a mixture of\nGaussian distribution). Note that using a Variational Bayesian Gaussian\nmixture avoids the specification of the number of components for a Gaussian\nmixture model.\n\nExamples:\n\nThe main difficulty in learning Gaussian mixture models from unlabeled data is\nthat it is one usually doesn\u2019t know which points came from which latent\ncomponent (if one has access to this information it gets very easy to fit a\nseparate Gaussian distribution to each set of points). Expectation-\nmaximization is a well-founded statistical algorithm to get around this\nproblem by an iterative process. First one assumes random components (randomly\ncentered on data points, learned from k-means, or even just normally\ndistributed around the origin) and computes for each point a probability of\nbeing generated by each component of the model. Then, one tweaks the\nparameters to maximize the likelihood of the data given those assignments.\nRepeating this process is guaranteed to always converge to a local optimum.\n\nThe `BayesianGaussianMixture` object implements a variant of the Gaussian\nmixture model with variational inference algorithms. The API is similar as the\none defined by `GaussianMixture`.\n\nVariational inference is an extension of expectation-maximization that\nmaximizes a lower bound on model evidence (including priors) instead of data\nlikelihood. The principle behind variational methods is the same as\nexpectation-maximization (that is both are iterative algorithms that alternate\nbetween finding the probabilities for each point to be generated by each\nmixture and fitting the mixture to these assigned points), but variational\nmethods add regularization by integrating information from prior\ndistributions. This avoids the singularities often found in expectation-\nmaximization solutions but introduces some subtle biases to the model.\nInference is often notably slower, but not usually as much so as to render\nusage unpractical.\n\nDue to its Bayesian nature, the variational algorithm needs more hyper-\nparameters than expectation-maximization, the most important of these being\nthe concentration parameter `weight_concentration_prior`. Specifying a low\nvalue for the concentration prior will make the model put most of the weight\non few components set the remaining components weights very close to zero.\nHigh values of the concentration prior will allow a larger number of\ncomponents to be active in the mixture.\n\nThe parameters implementation of the `BayesianGaussianMixture` class proposes\ntwo types of prior for the weights distribution: a finite mixture model with\nDirichlet distribution and an infinite mixture model with the Dirichlet\nProcess. In practice Dirichlet Process inference algorithm is approximated and\nuses a truncated distribution with a fixed maximum number of components\n(called the Stick-breaking representation). The number of components actually\nused almost always depends on the data.\n\nThe next figure compares the results obtained for the different type of the\nweight concentration prior (parameter `weight_concentration_prior_type`) for\ndifferent values of `weight_concentration_prior`. Here, we can see the value\nof the `weight_concentration_prior` parameter has a strong impact on the\neffective number of active components obtained. We can also notice that large\nvalues for the concentration weight prior lead to more uniform weights when\nthe type of prior is \u2018dirichlet_distribution\u2019 while this is not necessarily\nthe case for the \u2018dirichlet_process\u2019 type (used by default).\n\nThe examples below compare Gaussian mixture models with a fixed number of\ncomponents, to the variational Gaussian mixture models with a Dirichlet\nprocess prior. Here, a classical Gaussian mixture is fitted with 5 components\non a dataset composed of 2 clusters. We can see that the variational Gaussian\nmixture with a Dirichlet process prior is able to limit itself to only 2\ncomponents whereas the Gaussian mixture fits the data with a fixed number of\ncomponents that has to be set a priori by the user. In this case the user has\nselected `n_components=5` which does not match the true generative\ndistribution of this toy dataset. Note that with very little observations, the\nvariational Gaussian mixture models with a Dirichlet process prior can take a\nconservative stand, and fit only one component.\n\nOn the following figure we are fitting a dataset not well-depicted by a\nGaussian mixture. Adjusting the `weight_concentration_prior`, parameter of the\n`BayesianGaussianMixture` controls the number of components used to fit this\ndata. We also present on the last two plots a random sampling generated from\nthe two resulting mixtures.\n\nExamples:\n\nwhen `weight_concentration_prior` is small enough and `n_components` is larger\nthan what is found necessary by the model, the Variational Bayesian mixture\nmodel has a natural tendency to set some mixture weights values close to zero.\nThis makes it possible to let the model choose a suitable number of effective\ncomponents automatically. Only an upper bound of this number needs to be\nprovided. Note however that the \u201cideal\u201d number of active components is very\napplication specific and is typically ill-defined in a data exploration\nsetting.\n\nunlike finite models, which will almost always use all components as much as\nthey can, and hence will produce wildly different solutions for different\nnumbers of components, the variational inference with a Dirichlet process\nprior (`weight_concentration_prior_type='dirichlet_process'`) won\u2019t change\nmuch with changes to the parameters, leading to more stability and less\ntuning.\n\ndue to the incorporation of prior information, variational solutions have less\npathological special cases than expectation-maximization solutions.\n\nthe extra parametrization necessary for variational inference make inference\nslower, although not by much.\n\nthis algorithm needs an extra hyperparameter that might need experimental\ntuning via cross-validation.\n\nthere are many implicit biases in the inference algorithms (and also in the\nDirichlet process if used), and whenever there is a mismatch between these\nbiases and the data it might be possible to fit better models using a finite\nmixture.\n\nHere we describe variational inference algorithms on Dirichlet process\nmixture. The Dirichlet process is a prior probability distribution on\nclusterings with an infinite, unbounded, number of partitions. Variational\ntechniques let us incorporate this prior structure on Gaussian mixture models\nat almost no penalty in inference time, comparing with a finite Gaussian\nmixture model.\n\nAn important question is how can the Dirichlet process use an infinite,\nunbounded number of clusters and still be consistent. While a full explanation\ndoesn\u2019t fit this manual, one can think of its stick breaking process analogy\nto help understanding it. The stick breaking process is a generative story for\nthe Dirichlet process. We start with a unit-length stick and in each step we\nbreak off a portion of the remaining stick. Each time, we associate the length\nof the piece of the stick to the proportion of points that falls into a group\nof the mixture. At the end, to represent the infinite mixture, we associate\nthe last remaining piece of the stick to the proportion of points that don\u2019t\nfall into all the other groups. The length of each piece is a random variable\nwith probability proportional to the concentration parameter. Smaller value of\nthe concentration will divide the unit-length into larger pieces of the stick\n(defining more concentrated distribution). Larger concentration values will\ncreate smaller pieces of the stick (increasing the number of components with\nnon zero weights).\n\nVariational inference techniques for the Dirichlet process still work with a\nfinite approximation to this infinite mixture model, but instead of having to\nspecify a priori how many components one wants to use, one just specifies the\nconcentration parameter and an upper bound on the number of mixture components\n(this upper bound, assuming it is higher than the \u201ctrue\u201d number of components,\naffects only algorithmic complexity, not the actual number of components\nused).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.2. Manifold learning", "path": "modules/manifold", "type": "Guide", "text": "\nManifold learning is an approach to non-linear dimensionality reduction.\nAlgorithms for this task are based on the idea that the dimensionality of many\ndata sets is only artificially high.\n\nHigh-dimensional datasets can be very difficult to visualize. While data in\ntwo or three dimensions can be plotted to show the inherent structure of the\ndata, equivalent high-dimensional plots are much less intuitive. To aid\nvisualization of the structure of a dataset, the dimension must be reduced in\nsome way.\n\nThe simplest way to accomplish this dimensionality reduction is by taking a\nrandom projection of the data. Though this allows some degree of visualization\nof the data structure, the randomness of the choice leaves much to be desired.\nIn a random projection, it is likely that the more interesting structure\nwithin the data will be lost.\n\nTo address this concern, a number of supervised and unsupervised linear\ndimensionality reduction frameworks have been designed, such as Principal\nComponent Analysis (PCA), Independent Component Analysis, Linear Discriminant\nAnalysis, and others. These algorithms define specific rubrics to choose an\n\u201cinteresting\u201d linear projection of the data. These methods can be powerful,\nbut often miss important non-linear structure in the data.\n\nManifold Learning can be thought of as an attempt to generalize linear\nframeworks like PCA to be sensitive to non-linear structure in data. Though\nsupervised variants exist, the typical manifold learning problem is\nunsupervised: it learns the high-dimensional structure of the data from the\ndata itself, without the use of predetermined classifications.\n\nExamples:\n\nThe manifold learning implementations available in scikit-learn are summarized\nbelow\n\nOne of the earliest approaches to manifold learning is the Isomap algorithm,\nshort for Isometric Mapping. Isomap can be viewed as an extension of Multi-\ndimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional\nembedding which maintains geodesic distances between all points. Isomap can be\nperformed with the object `Isomap`.\n\nThe Isomap algorithm comprises three stages:\n\nThe overall complexity of Isomap is \\\\(O[D \\log(k) N \\log(N)] + O[N^2(k +\n\\log(N))] + O[d N^2]\\\\).\n\nReferences:\n\nLocally linear embedding (LLE) seeks a lower-dimensional projection of the\ndata which preserves distances within local neighborhoods. It can be thought\nof as a series of local Principal Component Analyses which are globally\ncompared to find the best non-linear embedding.\n\nLocally linear embedding can be performed with function\n`locally_linear_embedding` or its object-oriented counterpart\n`LocallyLinearEmbedding`.\n\nThe standard LLE algorithm comprises three stages:\n\nThe overall complexity of standard LLE is \\\\(O[D \\log(k) N \\log(N)] + O[D N\nk^3] + O[d N^2]\\\\).\n\nReferences:\n\nOne well-known issue with LLE is the regularization problem. When the number\nof neighbors is greater than the number of input dimensions, the matrix\ndefining each local neighborhood is rank-deficient. To address this, standard\nLLE applies an arbitrary regularization parameter \\\\(r\\\\), which is chosen\nrelative to the trace of the local weight matrix. Though it can be shown\nformally that as \\\\(r \\to 0\\\\), the solution converges to the desired\nembedding, there is no guarantee that the optimal solution will be found for\n\\\\(r > 0\\\\). This problem manifests itself in embeddings which distort the\nunderlying geometry of the manifold.\n\nOne method to address the regularization problem is to use multiple weight\nvectors in each neighborhood. This is the essence of modified locally linear\nembedding (MLLE). MLLE can be performed with function\n`locally_linear_embedding` or its object-oriented counterpart\n`LocallyLinearEmbedding`, with the keyword `method = 'modified'`. It requires\n`n_neighbors > n_components`.\n\nThe MLLE algorithm comprises three stages:\n\nThe overall complexity of MLLE is \\\\(O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N\n(k-D) k^2] + O[d N^2]\\\\).\n\nReferences:\n\nHessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method\nof solving the regularization problem of LLE. It revolves around a hessian-\nbased quadratic form at each neighborhood which is used to recover the locally\nlinear structure. Though other implementations note its poor scaling with data\nsize, `sklearn` implements some algorithmic improvements which make its cost\ncomparable to that of other LLE variants for small output dimension. HLLE can\nbe performed with function `locally_linear_embedding` or its object-oriented\ncounterpart `LocallyLinearEmbedding`, with the keyword `method = 'hessian'`.\nIt requires `n_neighbors > n_components * (n_components + 3) / 2`.\n\nThe HLLE algorithm comprises three stages:\n\nThe overall complexity of standard HLLE is \\\\(O[D \\log(k) N \\log(N)] + O[D N\nk^3] + O[N d^6] + O[d N^2]\\\\).\n\nReferences:\n\nSpectral Embedding is an approach to calculating a non-linear embedding.\nScikit-learn implements Laplacian Eigenmaps, which finds a low dimensional\nrepresentation of the data using a spectral decomposition of the graph\nLaplacian. The graph generated can be considered as a discrete approximation\nof the low dimensional manifold in the high dimensional space. Minimization of\na cost function based on the graph ensures that points close to each other on\nthe manifold are mapped close to each other in the low dimensional space,\npreserving local distances. Spectral embedding can be performed with the\nfunction `spectral_embedding` or its object-oriented counterpart\n`SpectralEmbedding`.\n\nThe Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:\n\nThe overall complexity of spectral embedding is \\\\(O[D \\log(k) N \\log(N)] +\nO[D N k^3] + O[d N^2]\\\\).\n\nReferences:\n\nThough not technically a variant of LLE, Local tangent space alignment (LTSA)\nis algorithmically similar enough to LLE that it can be put in this category.\nRather than focusing on preserving neighborhood distances as in LLE, LTSA\nseeks to characterize the local geometry at each neighborhood via its tangent\nspace, and performs a global optimization to align these local tangent spaces\nto learn the embedding. LTSA can be performed with function\n`locally_linear_embedding` or its object-oriented counterpart\n`LocallyLinearEmbedding`, with the keyword `method = 'ltsa'`.\n\nThe LTSA algorithm comprises three stages:\n\nThe overall complexity of standard LTSA is \\\\(O[D \\log(k) N \\log(N)] + O[D N\nk^3] + O[k^2 d] + O[d N^2]\\\\).\n\nReferences:\n\nMultidimensional scaling (`MDS`) seeks a low-dimensional representation of the\ndata in which the distances respect well the distances in the original high-\ndimensional space.\n\nIn general, `MDS` is a technique used for analyzing similarity or\ndissimilarity data. It attempts to model similarity or dissimilarity data as\ndistances in a geometric spaces. The data can be ratings of similarity between\nobjects, interaction frequencies of molecules, or trade indices between\ncountries.\n\nThere exists two types of MDS algorithm: metric and non metric. In the scikit-\nlearn, the class `MDS` implements both. In Metric MDS, the input similarity\nmatrix arises from a metric (and thus respects the triangular inequality), the\ndistances between output two points are then set to be as close as possible to\nthe similarity or dissimilarity data. In the non-metric version, the\nalgorithms will try to preserve the order of the distances, and hence seek for\na monotonic relationship between the distances in the embedded space and the\nsimilarities/dissimilarities.\n\nLet \\\\(S\\\\) be the similarity matrix, and \\\\(X\\\\) the coordinates of the\n\\\\(n\\\\) input points. Disparities \\\\(\\hat{d}_{ij}\\\\) are transformation of the\nsimilarities chosen in some optimal ways. The objective, called the stress, is\nthen defined by \\\\(\\sum_{i < j} d_{ij}(X) - \\hat{d}_{ij}(X)\\\\)\n\nThe simplest metric `MDS` model, called absolute MDS, disparities are defined\nby \\\\(\\hat{d}_{ij} = S_{ij}\\\\). With absolute MDS, the value \\\\(S_{ij}\\\\)\nshould then correspond exactly to the distance between point \\\\(i\\\\) and\n\\\\(j\\\\) in the embedding point.\n\nMost commonly, disparities are set to \\\\(\\hat{d}_{ij} = b S_{ij}\\\\).\n\nNon metric `MDS` focuses on the ordination of the data. If \\\\(S_{ij} <\nS_{jk}\\\\), then the embedding should enforce \\\\(d_{ij} < d_{jk}\\\\). A simple\nalgorithm to enforce that is to use a monotonic regression of \\\\(d_{ij}\\\\) on\n\\\\(S_{ij}\\\\), yielding disparities \\\\(\\hat{d}_{ij}\\\\) in the same order as\n\\\\(S_{ij}\\\\).\n\nA trivial solution to this problem is to set all the points on the origin. In\norder to avoid that, the disparities \\\\(\\hat{d}_{ij}\\\\) are normalized.\n\nReferences:\n\nt-SNE (`TSNE`) converts affinities of data points to probabilities. The\naffinities in the original space are represented by Gaussian joint\nprobabilities and the affinities in the embedded space are represented by\nStudent\u2019s t-distributions. This allows t-SNE to be particularly sensitive to\nlocal structure and has a few other advantages over existing techniques:\n\nWhile Isomap, LLE and variants are best suited to unfold a single continuous\nlow dimensional manifold, t-SNE will focus on the local structure of the data\nand will tend to extract clustered local groups of samples as highlighted on\nthe S-curve example. This ability to group samples based on the local\nstructure might be beneficial to visually disentangle a dataset that comprises\nseveral manifolds at once as is the case in the digits dataset.\n\nThe Kullback-Leibler (KL) divergence of the joint probabilities in the\noriginal space and the embedded space will be minimized by gradient descent.\nNote that the KL divergence is not convex, i.e. multiple restarts with\ndifferent initializations will end up in local minima of the KL divergence.\nHence, it is sometimes useful to try different seeds and select the embedding\nwith the lowest KL divergence.\n\nThe disadvantages to using t-SNE are roughly:\n\nThe main purpose of t-SNE is visualization of high-dimensional data. Hence, it\nworks best when the data will be embedded on two or three dimensions.\n\nOptimizing the KL divergence can be a little bit tricky sometimes. There are\nfive parameters that control the optimization of t-SNE and therefore possibly\nthe quality of the resulting embedding:\n\nThe perplexity is defined as \\\\(k=2^{(S)}\\\\) where \\\\(S\\\\) is the Shannon\nentropy of the conditional probability distribution. The perplexity of a\n\\\\(k\\\\)-sided die is \\\\(k\\\\), so that \\\\(k\\\\) is effectively the number of\nnearest neighbors t-SNE considers when generating the conditional\nprobabilities. Larger perplexities lead to more nearest neighbors and less\nsensitive to small structure. Conversely a lower perplexity considers a\nsmaller number of neighbors, and thus ignores more global information in\nfavour of the local neighborhood. As dataset sizes get larger more points will\nbe required to get a reasonable sample of the local neighborhood, and hence\nlarger perplexities may be required. Similarly noisier datasets will require\nlarger perplexity values to encompass enough local neighbors to see beyond the\nbackground noise.\n\nThe maximum number of iterations is usually high enough and does not need any\ntuning. The optimization consists of two phases: the early exaggeration phase\nand the final optimization. During early exaggeration the joint probabilities\nin the original space will be artificially increased by multiplication with a\ngiven factor. Larger factors result in larger gaps between natural clusters in\nthe data. If the factor is too high, the KL divergence could increase during\nthis phase. Usually it does not have to be tuned. A critical parameter is the\nlearning rate. If it is too low gradient descent will get stuck in a bad local\nminimum. If it is too high the KL divergence will increase during\noptimization. More tips can be found in Laurens van der Maaten\u2019s FAQ (see\nreferences). The last parameter, angle, is a tradeoff between performance and\naccuracy. Larger angles imply that we can approximate larger regions by a\nsingle point, leading to better speed but less accurate results.\n\n\u201cHow to Use t-SNE Effectively\u201d provides a good discussion of the effects of\nthe various parameters, as well as interactive plots to explore the effects of\ndifferent parameters.\n\nThe Barnes-Hut t-SNE that has been implemented here is usually much slower\nthan other manifold learning algorithms. The optimization is quite difficult\nand the computation of the gradient is \\\\(O[d N log(N)]\\\\), where \\\\(d\\\\) is\nthe number of output dimensions and \\\\(N\\\\) is the number of samples. The\nBarnes-Hut method improves on the exact method where t-SNE complexity is\n\\\\(O[d N^2]\\\\), but has several other notable differences:\n\nFor visualization purpose (which is the main use case of t-SNE), using the\nBarnes-Hut method is strongly recommended. The exact t-SNE method is useful\nfor checking the theoretically properties of the embedding possibly in higher\ndimensional space but limit to small datasets due to computational\nconstraints.\n\nAlso note that the digits labels roughly match the natural grouping found by\nt-SNE while the linear 2D projection of the PCA model yields a representation\nwhere label regions largely overlap. This is a strong clue that this data can\nbe well separated by non linear methods that focus on the local structure\n(e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well\nseparated homogeneously labeled groups with t-SNE in 2D does not necessarily\nimply that the data cannot be correctly classified by a supervised model. It\nmight be the case that 2 dimensions are not low enough to accurately\nrepresents the internal structure of the data.\n\nReferences:\n\nSee also\n\nTotally Random Trees Embedding can also be useful to derive non-linear\nrepresentations of feature space, also it does not perform dimensionality\nreduction.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.3. Clustering", "path": "modules/clustering", "type": "Guide", "text": "\nClustering of unlabeled data can be performed with the module\n`sklearn.cluster`.\n\nEach clustering algorithm comes in two variants: a class, that implements the\n`fit` method to learn the clusters on train data, and a function, that, given\ntrain data, returns an array of integer labels corresponding to the different\nclusters. For the class, the labels over the training data can be found in the\n`labels_` attribute.\n\nInput data\n\nOne important thing to note is that the algorithms implemented in this module\ncan take different kinds of matrix as input. All the methods accept standard\ndata matrices of shape `(n_samples, n_features)`. These can be obtained from\nthe classes in the `sklearn.feature_extraction` module. For\n`AffinityPropagation`, `SpectralClustering` and `DBSCAN` one can also input\nsimilarity matrices of shape `(n_samples, n_samples)`. These can be obtained\nfrom the functions in the `sklearn.metrics.pairwise` module.\n\nA comparison of the clustering algorithms in scikit-learn\n\nMethod name\n\nParameters\n\nScalability\n\nUsecase\n\nGeometry (metric used)\n\nK-Means\n\nnumber of clusters\n\nVery large `n_samples`, medium `n_clusters` with MiniBatch code\n\nGeneral-purpose, even cluster size, flat geometry, not too many clusters\n\nDistances between points\n\nAffinity propagation\n\ndamping, sample preference\n\nNot scalable with n_samples\n\nMany clusters, uneven cluster size, non-flat geometry\n\nGraph distance (e.g. nearest-neighbor graph)\n\nMean-shift\n\nbandwidth\n\nNot scalable with `n_samples`\n\nMany clusters, uneven cluster size, non-flat geometry\n\nDistances between points\n\nSpectral clustering\n\nnumber of clusters\n\nMedium `n_samples`, small `n_clusters`\n\nFew clusters, even cluster size, non-flat geometry\n\nGraph distance (e.g. nearest-neighbor graph)\n\nWard hierarchical clustering\n\nnumber of clusters or distance threshold\n\nLarge `n_samples` and `n_clusters`\n\nMany clusters, possibly connectivity constraints\n\nDistances between points\n\nAgglomerative clustering\n\nnumber of clusters or distance threshold, linkage type, distance\n\nLarge `n_samples` and `n_clusters`\n\nMany clusters, possibly connectivity constraints, non Euclidean distances\n\nAny pairwise distance\n\nDBSCAN\n\nneighborhood size\n\nVery large `n_samples`, medium `n_clusters`\n\nNon-flat geometry, uneven cluster sizes\n\nDistances between nearest points\n\nOPTICS\n\nminimum cluster membership\n\nVery large `n_samples`, large `n_clusters`\n\nNon-flat geometry, uneven cluster sizes, variable cluster density\n\nDistances between points\n\nGaussian mixtures\n\nmany\n\nNot scalable\n\nFlat geometry, good for density estimation\n\nMahalanobis distances to centers\n\nBirch\n\nbranching factor, threshold, optional global clusterer.\n\nLarge `n_clusters` and `n_samples`\n\nLarge dataset, outlier removal, data reduction.\n\nEuclidean distance between points\n\nNon-flat geometry clustering is useful when the clusters have a specific\nshape, i.e. a non-flat manifold, and the standard euclidean distance is not\nthe right metric. This case arises in the two top rows of the figure above.\n\nGaussian mixture models, useful for clustering, are described in another\nchapter of the documentation dedicated to mixture models. KMeans can be seen\nas a special case of Gaussian mixture model with equal covariance per\ncomponent.\n\nThe `KMeans` algorithm clusters data by trying to separate samples in n groups\nof equal variance, minimizing a criterion known as the inertia or within-\ncluster sum-of-squares (see below). This algorithm requires the number of\nclusters to be specified. It scales well to large number of samples and has\nbeen used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of \\\\(N\\\\) samples \\\\(X\\\\) into \\\\(K\\\\)\ndisjoint clusters \\\\(C\\\\), each described by the mean \\\\(\\mu_j\\\\) of the\nsamples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d;\nnote that they are not, in general, points from \\\\(X\\\\), although they live in\nthe same space.\n\nThe K-means algorithm aims to choose centroids that minimise the inertia, or\nwithin-cluster sum-of-squares criterion:\n\nInertia can be recognized as a measure of how internally coherent clusters\nare. It suffers from various drawbacks:\n\nK-means is often referred to as Lloyd\u2019s algorithm. In basic terms, the\nalgorithm has three steps. The first step chooses the initial centroids, with\nthe most basic method being to choose \\\\(k\\\\) samples from the dataset\n\\\\(X\\\\). After initialization, K-means consists of looping between the two\nother steps. The first step assigns each sample to its nearest centroid. The\nsecond step creates new centroids by taking the mean value of all of the\nsamples assigned to each previous centroid. The difference between the old and\nthe new centroids are computed and the algorithm repeats these last two steps\nuntil this value is less than a threshold. In other words, it repeats until\nthe centroids do not move significantly.\n\nK-means is equivalent to the expectation-maximization algorithm with a small,\nall-equal, diagonal covariance matrix.\n\nThe algorithm can also be understood through the concept of Voronoi diagrams.\nFirst the Voronoi diagram of the points is calculated using the current\ncentroids. Each segment in the Voronoi diagram becomes a separate cluster.\nSecondly, the centroids are updated to the mean of each segment. The algorithm\nthen repeats this until a stopping criterion is fulfilled. Usually, the\nalgorithm stops when the relative decrease in the objective function between\niterations is less than the given tolerance value. This is not the case in\nthis implementation: iteration stops when centroids move less than the\ntolerance.\n\nGiven enough time, K-means will always converge, however this may be to a\nlocal minimum. This is highly dependent on the initialization of the\ncentroids. As a result, the computation is often done several times, with\ndifferent initializations of the centroids. One method to help address this\nissue is the k-means++ initialization scheme, which has been implemented in\nscikit-learn (use the `init='k-means++'` parameter). This initializes the\ncentroids to be (generally) distant from each other, leading to provably\nbetter results than random initialization, as shown in the reference.\n\nK-means++ can also be called independently to select seeds for other\nclustering algorithms, see `sklearn.cluster.kmeans_plusplus` for details and\nexample usage.\n\nThe algorithm supports sample weights, which can be given by a parameter\n`sample_weight`. This allows to assign more weight to some samples when\ncomputing cluster centers and values of inertia. For example, assigning a\nweight of 2 to a sample is equivalent to adding a duplicate of that sample to\nthe dataset \\\\(X\\\\).\n\nK-means can be used for vector quantization. This is achieved using the\ntransform method of a trained model of `KMeans`.\n\n`KMeans` benefits from OpenMP based parallelism through Cython. Small chunks\nof data (256 samples) are processed in parallel, which in addition yields a\nlow memory footprint. For more details on how to control the number of\nthreads, please refer to our Parallelism notes.\n\nExamples:\n\nReferences:\n\nThe `MiniBatchKMeans` is a variant of the `KMeans` algorithm which uses mini-\nbatches to reduce the computation time, while still attempting to optimise the\nsame objective function. Mini-batches are subsets of the input data, randomly\nsampled in each training iteration. These mini-batches drastically reduce the\namount of computation required to converge to a local solution. In contrast to\nother algorithms that reduce the convergence time of k-means, mini-batch\nk-means produces results that are generally only slightly worse than the\nstandard algorithm.\n\nThe algorithm iterates between two major steps, similar to vanilla k-means. In\nthe first step, \\\\(b\\\\) samples are drawn randomly from the dataset, to form a\nmini-batch. These are then assigned to the nearest centroid. In the second\nstep, the centroids are updated. In contrast to k-means, this is done on a\nper-sample basis. For each sample in the mini-batch, the assigned centroid is\nupdated by taking the streaming average of the sample and all previous samples\nassigned to that centroid. This has the effect of decreasing the rate of\nchange for a centroid over time. These steps are performed until convergence\nor a predetermined number of iterations is reached.\n\n`MiniBatchKMeans` converges faster than `KMeans`, but the quality of the\nresults is reduced. In practice this difference in quality can be quite small,\nas shown in the example and cited reference.\n\nExamples:\n\nReferences:\n\n`AffinityPropagation` creates clusters by sending messages between pairs of\nsamples until convergence. A dataset is then described using a small number of\nexemplars, which are identified as those most representative of other samples.\nThe messages sent between pairs represent the suitability for one sample to be\nthe exemplar of the other, which is updated in response to the values from\nother pairs. This updating happens iteratively until convergence, at which\npoint the final exemplars are chosen, and hence the final clustering is given.\n\nAffinity Propagation can be interesting as it chooses the number of clusters\nbased on the data provided. For this purpose, the two important parameters are\nthe preference, which controls how many exemplars are used, and the damping\nfactor which damps the responsibility and availability messages to avoid\nnumerical oscillations when updating these messages.\n\nThe main drawback of Affinity Propagation is its complexity. The algorithm has\na time complexity of the order \\\\(O(N^2 T)\\\\), where \\\\(N\\\\) is the number of\nsamples and \\\\(T\\\\) is the number of iterations until convergence. Further,\nthe memory complexity is of the order \\\\(O(N^2)\\\\) if a dense similarity\nmatrix is used, but reducible if a sparse similarity matrix is used. This\nmakes Affinity Propagation most appropriate for small to medium sized\ndatasets.\n\nExamples:\n\nAlgorithm description: The messages sent between points belong to one of two\ncategories. The first is the responsibility \\\\(r(i, k)\\\\), which is the\naccumulated evidence that sample \\\\(k\\\\) should be the exemplar for sample\n\\\\(i\\\\). The second is the availability \\\\(a(i, k)\\\\) which is the accumulated\nevidence that sample \\\\(i\\\\) should choose sample \\\\(k\\\\) to be its exemplar,\nand considers the values for all other samples that \\\\(k\\\\) should be an\nexemplar. In this way, exemplars are chosen by samples if they are (1) similar\nenough to many samples and (2) chosen by many samples to be representative of\nthemselves.\n\nMore formally, the responsibility of a sample \\\\(k\\\\) to be the exemplar of\nsample \\\\(i\\\\) is given by:\n\nWhere \\\\(s(i, k)\\\\) is the similarity between samples \\\\(i\\\\) and \\\\(k\\\\). The\navailability of sample \\\\(k\\\\) to be the exemplar of sample \\\\(i\\\\) is given\nby:\n\nTo begin with, all values for \\\\(r\\\\) and \\\\(a\\\\) are set to zero, and the\ncalculation of each iterates until convergence. As discussed above, in order\nto avoid numerical oscillations when updating the messages, the damping factor\n\\\\(\\lambda\\\\) is introduced to iteration process:\n\nwhere \\\\(t\\\\) indicates the iteration times.\n\n`MeanShift` clustering aims to discover blobs in a smooth density of samples.\nIt is a centroid based algorithm, which works by updating candidates for\ncentroids to be the mean of the points within a given region. These candidates\nare then filtered in a post-processing stage to eliminate near-duplicates to\nform the final set of centroids.\n\nGiven a candidate centroid \\\\(x_i\\\\) for iteration \\\\(t\\\\), the candidate is\nupdated according to the following equation:\n\nWhere \\\\(N(x_i)\\\\) is the neighborhood of samples within a given distance\naround \\\\(x_i\\\\) and \\\\(m\\\\) is the mean shift vector that is computed for\neach centroid that points towards a region of the maximum increase in the\ndensity of points. This is computed using the following equation, effectively\nupdating a centroid to be the mean of the samples within its neighborhood:\n\nThe algorithm automatically sets the number of clusters, instead of relying on\na parameter `bandwidth`, which dictates the size of the region to search\nthrough. This parameter can be set manually, but can be estimated using the\nprovided `estimate_bandwidth` function, which is called if the bandwidth is\nnot set.\n\nThe algorithm is not highly scalable, as it requires multiple nearest neighbor\nsearches during the execution of the algorithm. The algorithm is guaranteed to\nconverge, however the algorithm will stop iterating when the change in\ncentroids is small.\n\nLabelling a new sample is performed by finding the nearest centroid for a\ngiven sample.\n\nExamples:\n\nReferences:\n\n`SpectralClustering` performs a low-dimension embedding of the affinity matrix\nbetween samples, followed by clustering, e.g., by KMeans, of the components of\nthe eigenvectors in the low dimensional space. It is especially\ncomputationally efficient if the affinity matrix is sparse and the `amg`\nsolver is used for the eigenvalue problem (Note, the `amg` solver requires\nthat the pyamg module is installed.)\n\nThe present version of SpectralClustering requires the number of clusters to\nbe specified in advance. It works well for a small number of clusters, but is\nnot advised for many clusters.\n\nFor two clusters, SpectralClustering solves a convex relaxation of the\nnormalised cuts problem on the similarity graph: cutting the graph in two so\nthat the weight of the edges cut is small compared to the weights of the edges\ninside each cluster. This criteria is especially interesting when working on\nimages, where graph vertices are pixels, and weights of the edges of the\nsimilarity graph are computed using a function of a gradient of the image.\n\nWarning\n\nTransforming distance to well-behaved similarities\n\nNote that if the values of your similarity matrix are not well distributed,\ne.g. with negative values or with a distance matrix rather than a similarity,\nthe spectral problem will be singular and the problem not solvable. In which\ncase it is advised to apply a transformation to the entries of the matrix. For\ninstance, in the case of a signed distance matrix, is common to apply a heat\nkernel:\n\nSee the examples for such an application.\n\nExamples:\n\nDifferent label assignment strategies can be used, corresponding to the\n`assign_labels` parameter of `SpectralClustering`. `\"kmeans\"` strategy can\nmatch finer details, but can be unstable. In particular, unless you control\nthe `random_state`, it may not be reproducible from run-to-run, as it depends\non random initialization. The alternative `\"discretize\"` strategy is 100%\nreproducible, but tends to create parcels of fairly even and geometrical\nshape.\n\n`assign_labels=\"kmeans\"`\n\n`assign_labels=\"discretize\"`\n\nSpectral Clustering can also be used to partition graphs via their spectral\nembeddings. In this case, the affinity matrix is the adjacency matrix of the\ngraph, and SpectralClustering is initialized with `affinity='precomputed'`:\n\nReferences:\n\nHierarchical clustering is a general family of clustering algorithms that\nbuild nested clusters by merging or splitting them successively. This\nhierarchy of clusters is represented as a tree (or dendrogram). The root of\nthe tree is the unique cluster that gathers all the samples, the leaves being\nthe clusters with only one sample. See the Wikipedia page for more details.\n\nThe `AgglomerativeClustering` object performs a hierarchical clustering using\na bottom up approach: each observation starts in its own cluster, and clusters\nare successively merged together. The linkage criteria determines the metric\nused for the merge strategy:\n\n`AgglomerativeClustering` can also scale to large number of samples when it is\nused jointly with a connectivity matrix, but is computationally expensive when\nno connectivity constraints are added between samples: it considers at each\nstep all the possible merges.\n\n`FeatureAgglomeration`\n\nThe `FeatureAgglomeration` uses agglomerative clustering to group together\nfeatures that look very similar, thus decreasing the number of features. It is\na dimensionality reduction tool, see Unsupervised dimensionality reduction.\n\n`AgglomerativeClustering` supports Ward, single, average, and complete linkage\nstrategies.\n\nAgglomerative cluster has a \u201crich get richer\u201d behavior that leads to uneven\ncluster sizes. In this regard, single linkage is the worst strategy, and Ward\ngives the most regular sizes. However, the affinity (or distance used in\nclustering) cannot be varied with Ward, thus for non Euclidean metrics,\naverage linkage is a good alternative. Single linkage, while not robust to\nnoisy data, can be computed very efficiently and can therefore be useful to\nprovide hierarchical clustering of larger datasets. Single linkage can also\nperform well on non-globular data.\n\nExamples:\n\nIt\u2019s possible to visualize the tree representing the hierarchical merging of\nclusters as a dendrogram. Visual inspection can often be useful for\nunderstanding the structure of the data, though more so in the case of small\nsample sizes.\n\nAn interesting aspect of `AgglomerativeClustering` is that connectivity\nconstraints can be added to this algorithm (only adjacent clusters can be\nmerged together), through a connectivity matrix that defines for each sample\nthe neighboring samples following a given structure of the data. For instance,\nin the swiss-roll example below, the connectivity constraints forbid the\nmerging of points that are not adjacent on the swiss roll, and thus avoid\nforming clusters that extend across overlapping folds of the roll.\n\nThese constraint are useful to impose a certain local structure, but they also\nmake the algorithm faster, especially when the number of the samples is high.\n\nThe connectivity constraints are imposed via an connectivity matrix: a scipy\nsparse matrix that has elements only at the intersection of a row and a column\nwith indices of the dataset that should be connected. This matrix can be\nconstructed from a-priori information: for instance, you may wish to cluster\nweb pages by only merging pages with a link pointing from one to another. It\ncan also be learned from the data, for instance using\n`sklearn.neighbors.kneighbors_graph` to restrict merging to nearest neighbors\nas in this example, or using `sklearn.feature_extraction.image.grid_to_graph`\nto enable only merging of neighboring pixels on an image, as in the coin\nexample.\n\nExamples:\n\nWarning\n\nConnectivity constraints with single, average and complete linkage\n\nConnectivity constraints and single, complete or average linkage can enhance\nthe \u2018rich getting richer\u2019 aspect of agglomerative clustering, particularly so\nif they are built with `sklearn.neighbors.kneighbors_graph`. In the limit of a\nsmall number of clusters, they tend to give a few macroscopically occupied\nclusters and almost empty ones. (see the discussion in Agglomerative\nclustering with and without structure). Single linkage is the most brittle\nlinkage option with regard to this issue.\n\nSingle, average and complete linkage can be used with a variety of distances\n(or affinities), in particular Euclidean distance (l2), Manhattan distance (or\nCityblock, or l1), cosine distance, or any precomputed affinity matrix.\n\nThe guidelines for choosing a metric is to use one that maximizes the distance\nbetween samples in different classes, and minimizes that within each class.\n\nExamples:\n\nThe `DBSCAN` algorithm views clusters as areas of high density separated by\nareas of low density. Due to this rather generic view, clusters found by\nDBSCAN can be any shape, as opposed to k-means which assumes that clusters are\nconvex shaped. The central component to the DBSCAN is the concept of core\nsamples, which are samples that are in areas of high density. A cluster is\ntherefore a set of core samples, each close to each other (measured by some\ndistance measure) and a set of non-core samples that are close to a core\nsample (but are not themselves core samples). There are two parameters to the\nalgorithm, `min_samples` and `eps`, which define formally what we mean when we\nsay dense. Higher `min_samples` or lower `eps` indicate higher density\nnecessary to form a cluster.\n\nMore formally, we define a core sample as being a sample in the dataset such\nthat there exist `min_samples` other samples within a distance of `eps`, which\nare defined as neighbors of the core sample. This tells us that the core\nsample is in a dense area of the vector space. A cluster is a set of core\nsamples that can be built by recursively taking a core sample, finding all of\nits neighbors that are core samples, finding all of their neighbors that are\ncore samples, and so on. A cluster also has a set of non-core samples, which\nare samples that are neighbors of a core sample in the cluster but are not\nthemselves core samples. Intuitively, these samples are on the fringes of a\ncluster.\n\nAny core sample is part of a cluster, by definition. Any sample that is not a\ncore sample, and is at least `eps` in distance from any core sample, is\nconsidered an outlier by the algorithm.\n\nWhile the parameter `min_samples` primarily controls how tolerant the\nalgorithm is towards noise (on noisy and large data sets it may be desirable\nto increase this parameter), the parameter `eps` is crucial to choose\nappropriately for the data set and distance function and usually cannot be\nleft at the default value. It controls the local neighborhood of the points.\nWhen chosen too small, most data will not be clustered at all (and labeled as\n`-1` for \u201cnoise\u201d). When chosen too large, it causes close clusters to be\nmerged into one cluster, and eventually the entire data set to be returned as\na single cluster. Some heuristics for choosing this parameter have been\ndiscussed in the literature, for example based on a knee in the nearest\nneighbor distances plot (as discussed in the references below).\n\nIn the figure below, the color indicates cluster membership, with large\ncircles indicating core samples found by the algorithm. Smaller circles are\nnon-core samples that are still part of a cluster. Moreover, the outliers are\nindicated by black points below.\n\nExamples:\n\nImplementation\n\nThe DBSCAN algorithm is deterministic, always generating the same clusters\nwhen given the same data in the same order. However, the results can differ\nwhen data is provided in a different order. First, even though the core\nsamples will always be assigned to the same clusters, the labels of those\nclusters will depend on the order in which those samples are encountered in\nthe data. Second and more importantly, the clusters to which non-core samples\nare assigned can differ depending on the data order. This would happen when a\nnon-core sample has a distance lower than `eps` to two core samples in\ndifferent clusters. By the triangular inequality, those two core samples must\nbe more distant than `eps` from each other, or they would be in the same\ncluster. The non-core sample is assigned to whichever cluster is generated\nfirst in a pass through the data, and so the results will depend on the data\nordering.\n\nThe current implementation uses ball trees and kd-trees to determine the\nneighborhood of points, which avoids calculating the full distance matrix (as\nwas done in scikit-learn versions before 0.14). The possibility to use custom\nmetrics is retained; for details, see `NearestNeighbors`.\n\nMemory consumption for large sample sizes\n\nThis implementation is by default not memory efficient because it constructs a\nfull pairwise similarity matrix in the case where kd-trees or ball-trees\ncannot be used (e.g., with sparse matrices). This matrix will consume\n\\\\(n^2\\\\) floats. A couple of mechanisms for getting around this are:\n\nReferences:\n\nThe `OPTICS` algorithm shares many similarities with the `DBSCAN` algorithm,\nand can be considered a generalization of DBSCAN that relaxes the `eps`\nrequirement from a single value to a value range. The key difference between\nDBSCAN and OPTICS is that the OPTICS algorithm builds a reachability graph,\nwhich assigns each sample both a `reachability_` distance, and a spot within\nthe cluster `ordering_` attribute; these two attributes are assigned when the\nmodel is fitted, and are used to determine cluster membership. If OPTICS is\nrun with the default value of inf set for `max_eps`, then DBSCAN style cluster\nextraction can be performed repeatedly in linear time for any given `eps`\nvalue using the `cluster_optics_dbscan` method. Setting `max_eps` to a lower\nvalue will result in shorter run times, and can be thought of as the maximum\nneighborhood radius from each point to find other potential reachable points.\n\nThe reachability distances generated by OPTICS allow for variable density\nextraction of clusters within a single data set. As shown in the above plot,\ncombining reachability distances and data set `ordering_` produces a\nreachability plot, where point density is represented on the Y-axis, and\npoints are ordered such that nearby points are adjacent. \u2018Cutting\u2019 the\nreachability plot at a single value produces DBSCAN like results; all points\nabove the \u2018cut\u2019 are classified as noise, and each time that there is a break\nwhen reading from left to right signifies a new cluster. The default cluster\nextraction with OPTICS looks at the steep slopes within the graph to find\nclusters, and the user can define what counts as a steep slope using the\nparameter `xi`. There are also other possibilities for analysis on the graph\nitself, such as generating hierarchical representations of the data through\nreachability-plot dendrograms, and the hierarchy of clusters detected by the\nalgorithm can be accessed through the `cluster_hierarchy_` parameter. The plot\nabove has been color-coded so that cluster colors in planar space match the\nlinear segment clusters of the reachability plot. Note that the blue and red\nclusters are adjacent in the reachability plot, and can be hierarchically\nrepresented as children of a larger parent cluster.\n\nExamples:\n\nComparison with DBSCAN\n\nThe results from OPTICS `cluster_optics_dbscan` method and DBSCAN are very\nsimilar, but not always identical; specifically, labeling of periphery and\nnoise points. This is in part because the first samples of each dense area\nprocessed by OPTICS have a large reachability value while being close to other\npoints in their area, and will thus sometimes be marked as noise rather than\nperiphery. This affects adjacent points when they are considered as candidates\nfor being marked as either periphery or noise.\n\nNote that for any single value of `eps`, DBSCAN will tend to have a shorter\nrun time than OPTICS; however, for repeated runs at varying `eps` values, a\nsingle run of OPTICS may require less cumulative runtime than DBSCAN. It is\nalso important to note that OPTICS\u2019 output is close to DBSCAN\u2019s only if `eps`\nand `max_eps` are close.\n\nComputational Complexity\n\nSpatial indexing trees are used to avoid calculating the full distance matrix,\nand allow for efficient memory usage on large sets of samples. Different\ndistance metrics can be supplied via the `metric` keyword.\n\nFor large datasets, similar (but not identical) results can be obtained via\nHDBSCAN. The HDBSCAN implementation is multithreaded, and has better\nalgorithmic runtime complexity than OPTICS, at the cost of worse memory\nscaling. For extremely large datasets that exhaust system memory using\nHDBSCAN, OPTICS will maintain \\\\(n\\\\) (as opposed to \\\\(n^2\\\\)) memory\nscaling; however, tuning of the `max_eps` parameter will likely need to be\nused to give a solution in a reasonable amount of wall time.\n\nReferences:\n\nThe `Birch` builds a tree called the Clustering Feature Tree (CFT) for the\ngiven data. The data is essentially lossy compressed to a set of Clustering\nFeature nodes (CF Nodes). The CF Nodes have a number of subclusters called\nClustering Feature subclusters (CF Subclusters) and these CF Subclusters\nlocated in the non-terminal CF Nodes can have CF Nodes as children.\n\nThe CF Subclusters hold the necessary information for clustering which\nprevents the need to hold the entire input data in memory. This information\nincludes:\n\nThe Birch algorithm has two parameters, the threshold and the branching\nfactor. The branching factor limits the number of subclusters in a node and\nthe threshold limits the distance between the entering sample and the existing\nsubclusters.\n\nThis algorithm can be viewed as an instance or data reduction method, since it\nreduces the input data to a set of subclusters which are obtained directly\nfrom the leaves of the CFT. This reduced data can be further processed by\nfeeding it into a global clusterer. This global clusterer can be set by\n`n_clusters`. If `n_clusters` is set to None, the subclusters from the leaves\nare directly read off, otherwise a global clustering step labels these\nsubclusters into global clusters (labels) and the samples are mapped to the\nglobal label of the nearest subcluster.\n\nAlgorithm description:\n\nBirch or MiniBatchKMeans?\n\nHow to use partial_fit?\n\nTo avoid the computation of global clustering, for every call of `partial_fit`\nthe user is advised\n\nReferences:\n\nEvaluating the performance of a clustering algorithm is not as trivial as\ncounting the number of errors or the precision and recall of a supervised\nclassification algorithm. In particular any evaluation metric should not take\nthe absolute values of the cluster labels into account but rather if this\nclustering define separations of the data similar to some ground truth set of\nclasses or satisfying some assumption such that members belong to the same\nclass are more similar than members of different classes according to some\nsimilarity metric.\n\nGiven the knowledge of the ground truth class assignments `labels_true` and\nour clustering algorithm assignments of the same samples `labels_pred`, the\n(adjusted or unadjusted) Rand index is a function that measures the similarity\nof the two assignments, ignoring permutations:\n\nThe Rand index does not ensure to obtain a value close to 0.0 for a random\nlabelling. The adjusted Rand index corrects for chance and will give such a\nbaseline.\n\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\nlabels, rename 2 to 3, and get the same score:\n\nFurthermore, both `rand_score` `adjusted_rand_score` are symmetric: swapping\nthe argument does not change the scores. They can thus be used as consensus\nmeasures:\n\nPerfect labeling is scored 1.0:\n\nPoorly agreeing labels (e.g. independent labelings) have lower scores, and for\nthe adjusted Rand index the score will be negative or close to zero. However,\nfor the unadjusted Rand index the score, while lower, will not necessarily be\nclose to zero.:\n\nContrary to inertia, the (adjusted or unadjusted) Rand index requires\nknowledge of the ground truth classes which is almost never available in\npractice or requires manual assignment by human annotators (as in the\nsupervised learning setting).\n\nHowever (adjusted or unadjusted) Rand index can also be useful in a purely\nunsupervised setting as a building block for a Consensus Index that can be\nused for clustering model selection (TODO).\n\nExamples:\n\nIf C is a ground truth class assignment and K the clustering, let us define\n\\\\(a\\\\) and \\\\(b\\\\) as:\n\nThe unadjusted Rand index is then given by:\n\nwhere \\\\(C_2^{n_{samples}}\\\\) is the total number of possible pairs in the\ndataset. It does not matter if the calculation is performed on ordered pairs\nor unordered pairs as long as the calculation is performed consistently.\n\nHowever, the Rand index does not guarantee that random label assignments will\nget a value close to zero (esp. if the number of clusters is in the same order\nof magnitude as the number of samples).\n\nTo counter this effect we can discount the expected RI \\\\(E[\\text{RI}]\\\\) of\nrandom labelings by defining the adjusted Rand index as follows:\n\nReferences\n\nGiven the knowledge of the ground truth class assignments `labels_true` and\nour clustering algorithm assignments of the same samples `labels_pred`, the\nMutual Information is a function that measures the agreement of the two\nassignments, ignoring permutations. Two different normalized versions of this\nmeasure are available, Normalized Mutual Information (NMI) and Adjusted Mutual\nInformation (AMI). NMI is often used in the literature, while AMI was proposed\nmore recently and is normalized against chance:\n\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the\nsame score:\n\nAll, `mutual_info_score`, `adjusted_mutual_info_score` and\n`normalized_mutual_info_score` are symmetric: swapping the argument does not\nchange the score. Thus they can be used as a consensus measure:\n\nPerfect labeling is scored 1.0:\n\nThis is not true for `mutual_info_score`, which is therefore harder to judge:\n\nBad (e.g. independent labelings) have non-positive scores:\n\nContrary to inertia, MI-based measures require the knowledge of the ground\ntruth classes while almost never available in practice or requires manual\nassignment by human annotators (as in the supervised learning setting).\n\nHowever MI-based measures can also be useful in purely unsupervised setting as\na building block for a Consensus Index that can be used for clustering model\nselection.\n\nExamples:\n\nAssume two label assignments (of the same N objects), \\\\(U\\\\) and \\\\(V\\\\).\nTheir entropy is the amount of uncertainty for a partition set, defined by:\n\nwhere \\\\(P(i) = |U_i| / N\\\\) is the probability that an object picked at\nrandom from \\\\(U\\\\) falls into class \\\\(U_i\\\\). Likewise for \\\\(V\\\\):\n\nWith \\\\(P'(j) = |V_j| / N\\\\). The mutual information (MI) between \\\\(U\\\\) and\n\\\\(V\\\\) is calculated by:\n\nwhere \\\\(P(i, j) = |U_i \\cap V_j| / N\\\\) is the probability that an object\npicked at random falls into both classes \\\\(U_i\\\\) and \\\\(V_j\\\\).\n\nIt also can be expressed in set cardinality formulation:\n\nThe normalized mutual information is defined as\n\nThis value of the mutual information and also the normalized variant is not\nadjusted for chance and will tend to increase as the number of different\nlabels (clusters) increases, regardless of the actual amount of \u201cmutual\ninformation\u201d between the label assignments.\n\nThe expected value for the mutual information can be calculated using the\nfollowing equation [VEB2009]. In this equation, \\\\(a_i = |U_i|\\\\) (the number\nof elements in \\\\(U_i\\\\)) and \\\\(b_j = |V_j|\\\\) (the number of elements in\n\\\\(V_j\\\\)).\n\nUsing the expected value, the adjusted mutual information can then be\ncalculated using a similar form to that of the adjusted Rand index:\n\nFor normalized mutual information and adjusted mutual information, the\nnormalizing value is typically some generalized mean of the entropies of each\nclustering. Various generalized means exist, and no firm rules exist for\npreferring one over the others. The decision is largely a field-by-field\nbasis; for instance, in community detection, the arithmetic mean is most\ncommon. Each normalizing method provides \u201cqualitatively similar behaviours\u201d\n[YAT2016]. In our implementation, this is controlled by the `average_method`\nparameter.\n\nVinh et al. (2010) named variants of NMI and AMI by their averaging method\n[VEB2010]. Their \u2018sqrt\u2019 and \u2018sum\u2019 averages are the geometric and arithmetic\nmeans; we use these more broadly common names.\n\nReferences\n\nVinh, Epps, and Bailey, (2009). \u201cInformation theoretic measures for\nclusterings comparison\u201d. Proceedings of the 26th Annual International\nConference on Machine Learning - ICML \u201809. doi:10.1145/1553374.1553511. ISBN\n9781605585161.\n\nVinh, Epps, and Bailey, (2010). \u201cInformation Theoretic Measures for\nClusterings Comparison: Variants, Properties, Normalization and Correction for\nChance\u201d. JMLR <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>\n\nYang, Algesheimer, and Tessone, (2016). \u201cA comparative analysis of community\ndetection algorithms on artificial networks\u201d. Scientific Reports 6: 30750.\ndoi:10.1038/srep30750.\n\nGiven the knowledge of the ground truth class assignments of the samples, it\nis possible to define some intuitive metric using conditional entropy\nanalysis.\n\nIn particular Rosenberg and Hirschberg (2007) define the following two\ndesirable objectives for any cluster assignment:\n\nWe can turn those concept as scores `homogeneity_score` and\n`completeness_score`. Both are bounded below by 0.0 and above by 1.0 (higher\nis better):\n\nTheir harmonic mean called V-measure is computed by `v_measure_score`:\n\nThis function\u2019s formula is as follows:\n\n`beta` defaults to a value of 1.0, but for using a value less than 1 for beta:\n\nmore weight will be attributed to homogeneity, and using a value greater than\n1:\n\nmore weight will be attributed to completeness.\n\nThe V-measure is actually equivalent to the mutual information (NMI) discussed\nabove, with the aggregation function being the arithmetic mean [B2011].\n\nHomogeneity, completeness and V-measure can be computed at once using\n`homogeneity_completeness_v_measure` as follows:\n\nThe following clustering assignment is slightly better, since it is\nhomogeneous but not complete:\n\nNote\n\n`v_measure_score` is symmetric: it can be used to evaluate the agreement of\ntwo independent assignments on the same dataset.\n\nThis is not the case for `completeness_score` and `homogeneity_score`: both\nare bound by the relationship:\n\nThe previously introduced metrics are not normalized with regards to random\nlabeling: this means that depending on the number of samples, clusters and\nground truth classes, a completely random labeling will not always yield the\nsame values for homogeneity, completeness and hence v-measure. In particular\nrandom labeling won\u2019t yield zero scores especially when the number of clusters\nis large.\n\nThis problem can safely be ignored when the number of samples is more than a\nthousand and the number of clusters is less than 10. For smaller sample sizes\nor larger number of clusters it is safer to use an adjusted index such as the\nAdjusted Rand Index (ARI).\n\nExamples:\n\nHomogeneity and completeness scores are formally given by:\n\nwhere \\\\(H(C|K)\\\\) is the conditional entropy of the classes given the cluster\nassignments and is given by:\n\nand \\\\(H(C)\\\\) is the entropy of the classes and is given by:\n\nwith \\\\(n\\\\) the total number of samples, \\\\(n_c\\\\) and \\\\(n_k\\\\) the number\nof samples respectively belonging to class \\\\(c\\\\) and cluster \\\\(k\\\\), and\nfinally \\\\(n_{c,k}\\\\) the number of samples from class \\\\(c\\\\) assigned to\ncluster \\\\(k\\\\).\n\nThe conditional entropy of clusters given class \\\\(H(K|C)\\\\) and the entropy\nof clusters \\\\(H(K)\\\\) are defined in a symmetric manner.\n\nRosenberg and Hirschberg further define V-measure as the harmonic mean of\nhomogeneity and completeness:\n\nReferences\n\nIdentication and Characterization of Events in Social Media, Hila Becker, PhD\nThesis.\n\nThe Fowlkes-Mallows index (`sklearn.metrics.fowlkes_mallows_score`) can be\nused when the ground truth class assignments of the samples is known. The\nFowlkes-Mallows score FMI is defined as the geometric mean of the pairwise\nprecision and recall:\n\nWhere `TP` is the number of True Positive (i.e. the number of pair of points\nthat belong to the same clusters in both the true labels and the predicted\nlabels), `FP` is the number of False Positive (i.e. the number of pair of\npoints that belong to the same clusters in the true labels and not in the\npredicted labels) and `FN` is the number of False Negative (i.e the number of\npair of points that belongs in the same clusters in the predicted labels and\nnot in the true labels).\n\nThe score ranges from 0 to 1. A high value indicates a good similarity between\ntwo clusters.\n\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the\nsame score:\n\nPerfect labeling is scored 1.0:\n\nBad (e.g. independent labelings) have zero scores:\n\nReferences\n\nIf the ground truth labels are not known, evaluation must be performed using\nthe model itself. The Silhouette Coefficient\n(`sklearn.metrics.silhouette_score`) is an example of such an evaluation,\nwhere a higher Silhouette Coefficient score relates to a model with better\ndefined clusters. The Silhouette Coefficient is defined for each sample and is\ncomposed of two scores:\n\nThe Silhouette Coefficient s for a single sample is then given as:\n\nThe Silhouette Coefficient for a set of samples is given as the mean of the\nSilhouette Coefficient for each sample.\n\nIn normal usage, the Silhouette Coefficient is applied to the results of a\ncluster analysis.\n\nReferences\n\nExamples:\n\nIf the ground truth labels are not known, the Calinski-Harabasz index\n(`sklearn.metrics.calinski_harabasz_score`) - also known as the Variance Ratio\nCriterion - can be used to evaluate the model, where a higher Calinski-\nHarabasz score relates to a model with better defined clusters.\n\nThe index is the ratio of the sum of between-clusters dispersion and of\nwithin-cluster dispersion for all clusters (where dispersion is defined as the\nsum of distances squared):\n\nIn normal usage, the Calinski-Harabasz index is applied to the results of a\ncluster analysis:\n\nFor a set of data \\\\(E\\\\) of size \\\\(n_E\\\\) which has been clustered into\n\\\\(k\\\\) clusters, the Calinski-Harabasz score \\\\(s\\\\) is defined as the ratio\nof the between-clusters dispersion mean and the within-cluster dispersion:\n\nwhere \\\\(\\mathrm{tr}(B_k)\\\\) is trace of the between group dispersion matrix\nand \\\\(\\mathrm{tr}(W_k)\\\\) is the trace of the within-cluster dispersion\nmatrix defined by:\n\nwith \\\\(C_q\\\\) the set of points in cluster \\\\(q\\\\), \\\\(c_q\\\\) the center of\ncluster \\\\(q\\\\), \\\\(c_E\\\\) the center of \\\\(E\\\\), and \\\\(n_q\\\\) the number of\npoints in cluster \\\\(q\\\\).\n\nReferences\n\nIf the ground truth labels are not known, the Davies-Bouldin index\n(`sklearn.metrics.davies_bouldin_score`) can be used to evaluate the model,\nwhere a lower Davies-Bouldin index relates to a model with better separation\nbetween the clusters.\n\nThis index signifies the average \u2018similarity\u2019 between clusters, where the\nsimilarity is a measure that compares the distance between clusters with the\nsize of the clusters themselves.\n\nZero is the lowest possible score. Values closer to zero indicate a better\npartition.\n\nIn normal usage, the Davies-Bouldin index is applied to the results of a\ncluster analysis as follows:\n\nThe index is defined as the average similarity between each cluster \\\\(C_i\\\\)\nfor \\\\(i=1, ..., k\\\\) and its most similar one \\\\(C_j\\\\). In the context of\nthis index, similarity is defined as a measure \\\\(R_{ij}\\\\) that trades off:\n\nA simple choice to construct \\\\(R_{ij}\\\\) so that it is nonnegative and\nsymmetric is:\n\nThen the Davies-Bouldin index is defined as:\n\nReferences\n\nContingency matrix (`sklearn.metrics.cluster.contingency_matrix`) reports the\nintersection cardinality for every true/predicted cluster pair. The\ncontingency matrix provides sufficient statistics for all clustering metrics\nwhere the samples are independent and identically distributed and one doesn\u2019t\nneed to account for some instances not being clustered.\n\nHere is an example:\n\nThe first row of output array indicates that there are three samples whose\ntrue cluster is \u201ca\u201d. Of them, two are in predicted cluster 0, one is in 1, and\nnone is in 2. And the second row indicates that there are three samples whose\ntrue cluster is \u201cb\u201d. Of them, none is in predicted cluster 0, one is in 1 and\ntwo are in 2.\n\nA confusion matrix for classification is a square contingency matrix where the\norder of rows and columns correspond to a list of classes.\n\nReferences\n\nThe pair confusion matrix (`sklearn.metrics.cluster.pair_confusion_matrix`) is\na 2x2 similarity matrix\n\nbetween two clusterings computed by considering all pairs of samples and\ncounting pairs that are assigned into the same or into different clusters\nunder the true and predicted clusterings.\n\nIt has the following entries:\n\n\\\\(C_{00}\\\\) : number of pairs with both clusterings having the samples not\nclustered together\n\n\\\\(C_{10}\\\\) : number of pairs with the true label clustering having the\nsamples clustered together but the other clustering not having the samples\nclustered together\n\n\\\\(C_{01}\\\\) : number of pairs with the true label clustering not having the\nsamples clustered together but the other clustering having the samples\nclustered together\n\n\\\\(C_{11}\\\\) : number of pairs with both clusterings having the samples\nclustered together\n\nConsidering a pair of samples that is clustered together a positive pair, then\nas in binary classification the count of true negatives is \\\\(C_{00}\\\\), false\nnegatives is \\\\(C_{10}\\\\), true positives is \\\\(C_{11}\\\\) and false positives\nis \\\\(C_{01}\\\\).\n\nPerfectly matching labelings have all non-zero entries on the diagonal\nregardless of actual label values:\n\nLabelings that assign all classes members to the same clusters are complete\nbut may not always be pure, hence penalized, and have some off-diagonal non-\nzero entries:\n\nThe matrix is not symmetric:\n\nIf classes members are completely split across different clusters, the\nassignment is totally incomplete, hence the matrix has all zero diagonal\nentries:\n\nReferences\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.4. Biclustering", "path": "modules/biclustering", "type": "Guide", "text": "\nBiclustering can be performed with the module `sklearn.cluster.bicluster`.\nBiclustering algorithms simultaneously cluster rows and columns of a data\nmatrix. These clusters of rows and columns are known as biclusters. Each\ndetermines a submatrix of the original data matrix with some desired\nproperties.\n\nFor instance, given a matrix of shape `(10, 10)`, one possible bicluster with\nthree rows and two columns induces a submatrix of shape `(3, 2)`:\n\nFor visualization purposes, given a bicluster, the rows and columns of the\ndata matrix may be rearranged to make the bicluster contiguous.\n\nAlgorithms differ in how they define biclusters. Some of the common types\ninclude:\n\nAlgorithms also differ in how rows and columns may be assigned to biclusters,\nwhich leads to different bicluster structures. Block diagonal or checkerboard\nstructures occur when rows and columns are divided into partitions.\n\nIf each row and each column belongs to exactly one bicluster, then rearranging\nthe rows and columns of the data matrix reveals the biclusters on the\ndiagonal. Here is an example of this structure where biclusters have higher\naverage values than the other rows and columns:\n\nAn example of biclusters formed by partitioning rows and columns.\n\nIn the checkerboard case, each row belongs to all column clusters, and each\ncolumn belongs to all row clusters. Here is an example of this structure where\nthe variance of the values within each bicluster is small:\n\nAn example of checkerboard biclusters.\n\nAfter fitting a model, row and column cluster membership can be found in the\n`rows_` and `columns_` attributes. `rows_[i]` is a binary vector with nonzero\nentries corresponding to rows that belong to bicluster `i`. Similarly,\n`columns_[i]` indicates which columns belong to bicluster `i`.\n\nSome models also have `row_labels_` and `column_labels_` attributes. These\nmodels partition the rows and columns, such as in the block diagonal and\ncheckerboard bicluster structures.\n\nNote\n\nBiclustering has many other names in different fields including co-clustering,\ntwo-mode clustering, two-way clustering, block clustering, coupled two-way\nclustering, etc. The names of some algorithms, such as the Spectral Co-\nClustering algorithm, reflect these alternate names.\n\nThe `SpectralCoclustering` algorithm finds biclusters with values higher than\nthose in the corresponding other rows and columns. Each row and each column\nbelongs to exactly one bicluster, so rearranging the rows and columns to make\npartitions contiguous reveals these high values along the diagonal:\n\nNote\n\nThe algorithm treats the input data matrix as a bipartite graph: the rows and\ncolumns of the matrix correspond to the two sets of vertices, and each entry\ncorresponds to an edge between a row and a column. The algorithm approximates\nthe normalized cut of this graph to find heavy subgraphs.\n\nAn approximate solution to the optimal normalized cut may be found via the\ngeneralized eigenvalue decomposition of the Laplacian of the graph. Usually\nthis would mean working directly with the Laplacian matrix. If the original\ndata matrix \\\\(A\\\\) has shape \\\\(m \\times n\\\\), the Laplacian matrix for the\ncorresponding bipartite graph has shape \\\\((m + n) \\times (m + n)\\\\). However,\nin this case it is possible to work directly with \\\\(A\\\\), which is smaller\nand more efficient.\n\nThe input matrix \\\\(A\\\\) is preprocessed as follows:\n\nWhere \\\\(R\\\\) is the diagonal matrix with entry \\\\(i\\\\) equal to \\\\(\\sum_{j}\nA_{ij}\\\\) and \\\\(C\\\\) is the diagonal matrix with entry \\\\(j\\\\) equal to\n\\\\(\\sum_{i} A_{ij}\\\\).\n\nThe singular value decomposition, \\\\(A_n = U \\Sigma V^\\top\\\\), provides the\npartitions of the rows and columns of \\\\(A\\\\). A subset of the left singular\nvectors gives the row partitions, and a subset of the right singular vectors\ngives the column partitions.\n\nThe \\\\(\\ell = \\lceil \\log_2 k \\rceil\\\\) singular vectors, starting from the\nsecond, provide the desired partitioning information. They are used to form\nthe matrix \\\\(Z\\\\):\n\nwhere the columns of \\\\(U\\\\) are \\\\(u_2, \\dots, u_{\\ell + 1}\\\\), and similarly\nfor \\\\(V\\\\).\n\nThen the rows of \\\\(Z\\\\) are clustered using k-means. The first `n_rows`\nlabels provide the row partitioning, and the remaining `n_columns` labels\nprovide the column partitioning.\n\nExamples:\n\nReferences:\n\nThe `SpectralBiclustering` algorithm assumes that the input data matrix has a\nhidden checkerboard structure. The rows and columns of a matrix with this\nstructure may be partitioned so that the entries of any bicluster in the\nCartesian product of row clusters and column clusters are approximately\nconstant. For instance, if there are two row partitions and three column\npartitions, each row will belong to three biclusters, and each column will\nbelong to two biclusters.\n\nThe algorithm partitions the rows and columns of a matrix so that a\ncorresponding blockwise-constant checkerboard matrix provides a good\napproximation to the original matrix.\n\nThe input matrix \\\\(A\\\\) is first normalized to make the checkerboard pattern\nmore obvious. There are three possible methods:\n\nAfter normalizing, the first few singular vectors are computed, just as in the\nSpectral Co-Clustering algorithm.\n\nIf log normalization was used, all the singular vectors are meaningful.\nHowever, if independent normalization or bistochastization were used, the\nfirst singular vectors, \\\\(u_1\\\\) and \\\\(v_1\\\\). are discarded. From now on,\nthe \u201cfirst\u201d singular vectors refers to \\\\(u_2 \\dots u_{p+1}\\\\) and \\\\(v_2\n\\dots v_{p+1}\\\\) except in the case of log normalization.\n\nGiven these singular vectors, they are ranked according to which can be best\napproximated by a piecewise-constant vector. The approximations for each\nvector are found using one-dimensional k-means and scored using the Euclidean\ndistance. Some subset of the best left and right singular vector are selected.\nNext, the data is projected to this best subset of singular vectors and\nclustered.\n\nFor instance, if \\\\(p\\\\) singular vectors were calculated, the \\\\(q\\\\) best\nare found as described, where \\\\(q<p\\\\). Let \\\\(U\\\\) be the matrix with\ncolumns the \\\\(q\\\\) best left singular vectors, and similarly \\\\(V\\\\) for the\nright. To partition the rows, the rows of \\\\(A\\\\) are projected to a \\\\(q\\\\)\ndimensional space: \\\\(A * V\\\\). Treating the \\\\(m\\\\) rows of this \\\\(m \\times\nq\\\\) matrix as samples and clustering using k-means yields the row labels.\nSimilarly, projecting the columns to \\\\(A^{\\top} * U\\\\) and clustering this\n\\\\(n \\times q\\\\) matrix yields the column labels.\n\nExamples:\n\nReferences:\n\nThere are two ways of evaluating a biclustering result: internal and external.\nInternal measures, such as cluster stability, rely only on the data and the\nresult themselves. Currently there are no internal bicluster measures in\nscikit-learn. External measures refer to an external source of information,\nsuch as the true solution. When working with real data the true solution is\nusually unknown, but biclustering artificial data may be useful for evaluating\nalgorithms precisely because the true solution is known.\n\nTo compare a set of found biclusters to the set of true biclusters, two\nsimilarity measures are needed: a similarity measure for individual\nbiclusters, and a way to combine these individual similarities into an overall\nscore.\n\nTo compare individual biclusters, several measures have been used. For now,\nonly the Jaccard index is implemented:\n\nwhere \\\\(A\\\\) and \\\\(B\\\\) are biclusters, \\\\(|A \\cap B|\\\\) is the number of\nelements in their intersection. The Jaccard index achieves its minimum of 0\nwhen the biclusters to not overlap at all and its maximum of 1 when they are\nidentical.\n\nSeveral methods have been developed to compare two sets of biclusters. For\nnow, only `consensus_score` (Hochreiter et. al., 2010) is available:\n\nThe minimum consensus score, 0, occurs when all pairs of biclusters are\ntotally dissimilar. The maximum score, 1, occurs when both sets are identical.\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.5. Decomposing signals in components", "path": "modules/decomposition", "type": "Guide", "text": "\nPCA is used to decompose a multivariate dataset in a set of successive\northogonal components that explain a maximum amount of the variance. In\nscikit-learn, `PCA` is implemented as a transformer object that learns \\\\(n\\\\)\ncomponents in its `fit` method, and can be used on new data to project it on\nthese components.\n\nPCA centers but does not scale the input data for each feature before applying\nthe SVD. The optional parameter `whiten=True` makes it possible to project the\ndata onto the singular space while scaling each component to unit variance.\nThis is often useful if the models down-stream make strong assumptions on the\nisotropy of the signal: this is for example the case for Support Vector\nMachines with the RBF kernel and the K-Means clustering algorithm.\n\nBelow is an example of the iris dataset, which is comprised of 4 features,\nprojected on the 2 dimensions that explain most variance:\n\nThe `PCA` object also provides a probabilistic interpretation of the PCA that\ncan give a likelihood of data based on the amount of variance it explains. As\nsuch it implements a score method that can be used in cross-validation:\n\nExamples:\n\nThe `PCA` object is very useful, but has certain limitations for large\ndatasets. The biggest limitation is that `PCA` only supports batch processing,\nwhich means all of the data to be processed must fit in main memory. The\n`IncrementalPCA` object uses a different form of processing and allows for\npartial computations which almost exactly match the results of `PCA` while\nprocessing the data in a minibatch fashion. `IncrementalPCA` makes it possible\nto implement out-of-core Principal Component Analysis either by:\n\n`IncrementalPCA` only stores estimates of component and noise variances, in\norder update `explained_variance_ratio_` incrementally. This is why memory\nusage depends on the number of samples per batch, rather than the number of\nsamples to be processed in the dataset.\n\nAs in `PCA`, `IncrementalPCA` centers but does not scale the input data for\neach feature before applying the SVD.\n\nExamples:\n\nIt is often interesting to project data to a lower-dimensional space that\npreserves most of the variance, by dropping the singular vector of components\nassociated with lower singular values.\n\nFor instance, if we work with 64x64 pixel gray-level pictures for face\nrecognition, the dimensionality of the data is 4096 and it is slow to train an\nRBF support vector machine on such wide data. Furthermore we know that the\nintrinsic dimensionality of the data is much lower than 4096 since all\npictures of human faces look somewhat alike. The samples lie on a manifold of\nmuch lower dimension (say around 200 for instance). The PCA algorithm can be\nused to linearly transform the data while both reducing the dimensionality and\npreserve most of the explained variance at the same time.\n\nThe class `PCA` used with the optional parameter `svd_solver='randomized'` is\nvery useful in that case: since we are going to drop most of the singular\nvectors it is much more efficient to limit the computation to an approximated\nestimate of the singular vectors we will keep to actually perform the\ntransform.\n\nFor instance, the following shows 16 sample portraits (centered around 0.0)\nfrom the Olivetti dataset. On the right hand side are the first 16 singular\nvectors reshaped as portraits. Since we only require the top 16 singular\nvectors of a dataset with size \\\\(n_{samples} = 400\\\\) and \\\\(n_{features} =\n64 \\times 64 = 4096\\\\), the computation time is less than 1s:\n\nIf we note \\\\(n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})\\\\)\nand \\\\(n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})\\\\), the\ntime complexity of the randomized `PCA` is \\\\(O(n_{\\max}^2 \\cdot\nn_{\\mathrm{components}})\\\\) instead of \\\\(O(n_{\\max}^2 \\cdot n_{\\min})\\\\) for\nthe exact method implemented in `PCA`.\n\nThe memory footprint of randomized `PCA` is also proportional to \\\\(2 \\cdot\nn_{\\max} \\cdot n_{\\mathrm{components}}\\\\) instead of \\\\(n_{\\max} \\cdot\nn_{\\min}\\\\) for the exact method.\n\nNote: the implementation of `inverse_transform` in `PCA` with\n`svd_solver='randomized'` is not the exact inverse transform of `transform`\neven when `whiten=False` (default).\n\nExamples:\n\nReferences:\n\n`KernelPCA` is an extension of PCA which achieves non-linear dimensionality\nreduction through the use of kernels (see Pairwise metrics, Affinities and\nKernels). It has many applications including denoising, compression and\nstructured prediction (kernel dependency estimation). `KernelPCA` supports\nboth `transform` and `inverse_transform`.\n\nExamples:\n\n`SparsePCA` is a variant of PCA, with the goal of extracting the set of sparse\ncomponents that best reconstruct the data.\n\nMini-batch sparse PCA (`MiniBatchSparsePCA`) is a variant of `SparsePCA` that\nis faster but less accurate. The increased speed is reached by iterating over\nsmall chunks of the set of features, for a given number of iterations.\n\nPrincipal component analysis (`PCA`) has the disadvantage that the components\nextracted by this method have exclusively dense expressions, i.e. they have\nnon-zero coefficients when expressed as linear combinations of the original\nvariables. This can make interpretation difficult. In many cases, the real\nunderlying components can be more naturally imagined as sparse vectors; for\nexample in face recognition, components might naturally map to parts of faces.\n\nSparse principal components yields a more parsimonious, interpretable\nrepresentation, clearly emphasizing which of the original features contribute\nto the differences between samples.\n\nThe following example illustrates 16 components extracted using sparse PCA\nfrom the Olivetti faces dataset. It can be seen how the regularization term\ninduces many zeros. Furthermore, the natural structure of the data causes the\nnon-zero coefficients to be vertically adjacent. The model does not enforce\nthis mathematically: each component is a vector \\\\(h \\in \\mathbf{R}^{4096}\\\\),\nand there is no notion of vertical adjacency except during the human-friendly\nvisualization as 64x64 pixel images. The fact that the components shown below\nappear local is the effect of the inherent structure of the data, which makes\nsuch local patterns minimize reconstruction error. There exist sparsity-\ninducing norms that take into account adjacency and different kinds of\nstructure; see [Jen09] for a review of such methods. For more details on how\nto use Sparse PCA, see the Examples section, below.\n\nNote that there are many different formulations for the Sparse PCA problem.\nThe one implemented here is based on [Mrl09] . The optimization problem solved\nis a PCA problem (dictionary learning) with an \\\\(\\ell_1\\\\) penalty on the\ncomponents:\n\nThe sparsity-inducing \\\\(\\ell_1\\\\) norm also prevents learning components from\nnoise when few training samples are available. The degree of penalization (and\nthus sparsity) can be adjusted through the hyperparameter `alpha`. Small\nvalues lead to a gently regularized factorization, while larger values shrink\nmany coefficients to zero.\n\nNote\n\nWhile in the spirit of an online algorithm, the class `MiniBatchSparsePCA`\ndoes not implement `partial_fit` because the algorithm is online along the\nfeatures direction, not the samples direction.\n\nExamples:\n\nReferences:\n\n\u201cOnline Dictionary Learning for Sparse Coding\u201d J. Mairal, F. Bach, J. Ponce,\nG. Sapiro, 2009\n\n\u201cStructured Sparse Principal Component Analysis\u201d R. Jenatton, G. Obozinski, F.\nBach, 2009\n\n`TruncatedSVD` implements a variant of singular value decomposition (SVD) that\nonly computes the \\\\(k\\\\) largest singular values, where \\\\(k\\\\) is a user-\nspecified parameter.\n\nWhen truncated SVD is applied to term-document matrices (as returned by\n`CountVectorizer` or `TfidfVectorizer`), this transformation is known as\nlatent semantic analysis (LSA), because it transforms such matrices to a\n\u201csemantic\u201d space of low dimensionality. In particular, LSA is known to combat\nthe effects of synonymy and polysemy (both of which roughly mean there are\nmultiple meanings per word), which cause term-document matrices to be overly\nsparse and exhibit poor similarity under measures such as cosine similarity.\n\nNote\n\nLSA is also known as latent semantic indexing, LSI, though strictly that\nrefers to its use in persistent indexes for information retrieval purposes.\n\nMathematically, truncated SVD applied to training samples \\\\(X\\\\) produces a\nlow-rank approximation \\\\(X\\\\):\n\nAfter this operation, \\\\(U_k \\Sigma_k^\\top\\\\) is the transformed training set\nwith \\\\(k\\\\) features (called `n_components` in the API).\n\nTo also transform a test set \\\\(X\\\\), we multiply it with \\\\(V_k\\\\):\n\nNote\n\nMost treatments of LSA in the natural language processing (NLP) and\ninformation retrieval (IR) literature swap the axes of the matrix \\\\(X\\\\) so\nthat it has shape `n_features` \u00d7 `n_samples`. We present LSA in a different\nway that matches the scikit-learn API better, but the singular values found\nare the same.\n\n`TruncatedSVD` is very similar to `PCA`, but differs in that the matrix\n\\\\(X\\\\) does not need to be centered. When the columnwise (per-feature) means\nof \\\\(X\\\\) are subtracted from the feature values, truncated SVD on the\nresulting matrix is equivalent to PCA. In practical terms, this means that the\n`TruncatedSVD` transformer accepts `scipy.sparse` matrices without the need to\ndensify them, as densifying may fill up memory even for medium-sized document\ncollections.\n\nWhile the `TruncatedSVD` transformer works with any feature matrix, using it\non tf\u2013idf matrices is recommended over raw frequency counts in an LSA/document\nprocessing setting. In particular, sublinear scaling and inverse document\nfrequency should be turned on (`sublinear_tf=True, use_idf=True`) to bring the\nfeature values closer to a Gaussian distribution, compensating for LSA\u2019s\nerroneous assumptions about textual data.\n\nExamples:\n\nReferences:\n\nThe `SparseCoder` object is an estimator that can be used to transform signals\ninto sparse linear combination of atoms from a fixed, precomputed dictionary\nsuch as a discrete wavelet basis. This object therefore does not implement a\n`fit` method. The transformation amounts to a sparse coding problem: finding a\nrepresentation of the data as a linear combination of as few dictionary atoms\nas possible. All variations of dictionary learning implement the following\ntransform methods, controllable via the `transform_method` initialization\nparameter:\n\nThresholding is very fast but it does not yield accurate reconstructions. They\nhave been shown useful in literature for classification tasks. For image\nreconstruction tasks, orthogonal matching pursuit yields the most accurate,\nunbiased reconstruction.\n\nThe dictionary learning objects offer, via the `split_code` parameter, the\npossibility to separate the positive and negative values in the results of\nsparse coding. This is useful when dictionary learning is used for extracting\nfeatures that will be used for supervised learning, because it allows the\nlearning algorithm to assign different weights to negative loadings of a\nparticular atom, from to the corresponding positive loading.\n\nThe split code for a single sample has length `2 * n_components` and is\nconstructed using the following rule: First, the regular code of length\n`n_components` is computed. Then, the first `n_components` entries of the\n`split_code` are filled with the positive part of the regular code vector. The\nsecond half of the split code is filled with the negative part of the code\nvector, only with a positive sign. Therefore, the split_code is non-negative.\n\nExamples:\n\nDictionary learning (`DictionaryLearning`) is a matrix factorization problem\nthat amounts to finding a (usually overcomplete) dictionary that will perform\nwell at sparsely encoding the fitted data.\n\nRepresenting data as sparse combinations of atoms from an overcomplete\ndictionary is suggested to be the way the mammalian primary visual cortex\nworks. Consequently, dictionary learning applied on image patches has been\nshown to give good results in image processing tasks such as image completion,\ninpainting and denoising, as well as for supervised recognition tasks.\n\nDictionary learning is an optimization problem solved by alternatively\nupdating the sparse code, as a solution to multiple Lasso problems,\nconsidering the dictionary fixed, and then updating the dictionary to best fit\nthe sparse code.\n\nAfter using such a procedure to fit the dictionary, the transform is simply a\nsparse coding step that shares the same implementation with all dictionary\nlearning objects (see Sparse coding with a precomputed dictionary).\n\nIt is also possible to constrain the dictionary and/or code to be positive to\nmatch constraints that may be present in the data. Below are the faces with\ndifferent positivity constraints applied. Red indicates negative values, blue\nindicates positive values, and white represents zeros.\n\nThe following image shows how a dictionary learned from 4x4 pixel image\npatches extracted from part of the image of a raccoon face looks like.\n\nExamples:\n\nReferences:\n\n`MiniBatchDictionaryLearning` implements a faster, but less accurate version\nof the dictionary learning algorithm that is better suited for large datasets.\n\nBy default, `MiniBatchDictionaryLearning` divides the data into mini-batches\nand optimizes in an online manner by cycling over the mini-batches for the\nspecified number of iterations. However, at the moment it does not implement a\nstopping condition.\n\nThe estimator also implements `partial_fit`, which updates the dictionary by\niterating only once over a mini-batch. This can be used for online learning\nwhen the data is not readily available from the start, or for when the data\ndoes not fit into the memory.\n\nClustering for dictionary learning\n\nNote that when using dictionary learning to extract a representation (e.g. for\nsparse coding) clustering can be a good proxy to learn the dictionary. For\ninstance the `MiniBatchKMeans` estimator is computationally efficient and\nimplements on-line learning with a `partial_fit` method.\n\nExample: Online learning of a dictionary of parts of faces\n\nIn unsupervised learning we only have a dataset \\\\(X = \\\\{x_1, x_2, \\dots, x_n\n\\\\}\\\\). How can this dataset be described mathematically? A very simple\n`continuous latent variable` model for \\\\(X\\\\) is\n\nThe vector \\\\(h_i\\\\) is called \u201clatent\u201d because it is unobserved.\n\\\\(\\epsilon\\\\) is considered a noise term distributed according to a Gaussian\nwith mean 0 and covariance \\\\(\\Psi\\\\) (i.e. \\\\(\\epsilon \\sim \\mathcal{N}(0,\n\\Psi)\\\\)), \\\\(\\mu\\\\) is some arbitrary offset vector. Such a model is called\n\u201cgenerative\u201d as it describes how \\\\(x_i\\\\) is generated from \\\\(h_i\\\\). If we\nuse all the \\\\(x_i\\\\)\u2019s as columns to form a matrix \\\\(\\mathbf{X}\\\\) and all\nthe \\\\(h_i\\\\)\u2019s as columns of a matrix \\\\(\\mathbf{H}\\\\) then we can write\n(with suitably defined \\\\(\\mathbf{M}\\\\) and \\\\(\\mathbf{E}\\\\)):\n\nIn other words, we decomposed matrix \\\\(\\mathbf{X}\\\\).\n\nIf \\\\(h_i\\\\) is given, the above equation automatically implies the following\nprobabilistic interpretation:\n\nFor a complete probabilistic model we also need a prior distribution for the\nlatent variable \\\\(h\\\\). The most straightforward assumption (based on the\nnice properties of the Gaussian distribution) is \\\\(h \\sim \\mathcal{N}(0,\n\\mathbf{I})\\\\). This yields a Gaussian as the marginal distribution of\n\\\\(x\\\\):\n\nNow, without any further assumptions the idea of having a latent variable\n\\\\(h\\\\) would be superfluous \u2013 \\\\(x\\\\) can be completely modelled with a mean\nand a covariance. We need to impose some more specific structure on one of\nthese two parameters. A simple additional assumption regards the structure of\nthe error covariance \\\\(\\Psi\\\\):\n\nBoth models essentially estimate a Gaussian with a low-rank covariance matrix.\nBecause both models are probabilistic they can be integrated in more complex\nmodels, e.g. Mixture of Factor Analysers. One gets very different models (e.g.\n`FastICA`) if non-Gaussian priors on the latent variables are assumed.\n\nFactor analysis can produce similar components (the columns of its loading\nmatrix) to `PCA`. However, one can not make any general statements about these\ncomponents (e.g. whether they are orthogonal):\n\nThe main advantage for Factor Analysis over `PCA` is that it can model the\nvariance in every direction of the input space independently (heteroscedastic\nnoise):\n\nThis allows better model selection than probabilistic PCA in the presence of\nheteroscedastic noise:\n\nFactor Analysis is often followed by a rotation of the factors (with the\nparameter `rotation`), usually to improve interpretability. For example,\nVarimax rotation maximizes the sum of the variances of the squared loadings,\ni.e., it tends to produce sparser factors, which are influenced by only a few\nfeatures each (the \u201csimple structure\u201d). See e.g., the first example below.\n\nExamples:\n\nIndependent component analysis separates a multivariate signal into additive\nsubcomponents that are maximally independent. It is implemented in scikit-\nlearn using the `Fast ICA` algorithm. Typically, ICA is not used for reducing\ndimensionality but for separating superimposed signals. Since the ICA model\ndoes not include a noise term, for the model to be correct, whitening must be\napplied. This can be done internally using the whiten argument or manually\nusing one of the PCA variants.\n\nIt is classically used to separate mixed signals (a problem known as blind\nsource separation), as in the example below:\n\nICA can also be used as yet another non linear decomposition that finds\ncomponents with some sparsity:\n\nExamples:\n\n`NMF` 1 is an alternative approach to decomposition that assumes that the data\nand the components are non-negative. `NMF` can be plugged in instead of `PCA`\nor its variants, in the cases where the data matrix does not contain negative\nvalues. It finds a decomposition of samples \\\\(X\\\\) into two matrices \\\\(W\\\\)\nand \\\\(H\\\\) of non-negative elements, by optimizing the distance \\\\(d\\\\)\nbetween \\\\(X\\\\) and the matrix product \\\\(WH\\\\). The most widely used distance\nfunction is the squared Frobenius norm, which is an obvious extension of the\nEuclidean norm to matrices:\n\nUnlike `PCA`, the representation of a vector is obtained in an additive\nfashion, by superimposing the components, without subtracting. Such additive\nmodels are efficient for representing images and text.\n\nIt has been observed in [Hoyer, 2004] 2 that, when carefully constrained,\n`NMF` can produce a parts-based representation of the dataset, resulting in\ninterpretable models. The following example displays 16 sparse components\nfound by `NMF` from the images in the Olivetti faces dataset, in comparison\nwith the PCA eigenfaces.\n\nThe `init` attribute determines the initialization method applied, which has a\ngreat impact on the performance of the method. `NMF` implements the method\nNonnegative Double Singular Value Decomposition. NNDSVD 4 is based on two SVD\nprocesses, one approximating the data matrix, the other approximating positive\nsections of the resulting partial SVD factors utilizing an algebraic property\nof unit rank matrices. The basic NNDSVD algorithm is better fit for sparse\nfactorization. Its variants NNDSVDa (in which all zeros are set equal to the\nmean of all elements of the data), and NNDSVDar (in which the zeros are set to\nrandom perturbations less than the mean of the data divided by 100) are\nrecommended in the dense case.\n\nNote that the Multiplicative Update (\u2018mu\u2019) solver cannot update zeros present\nin the initialization, so it leads to poorer results when used jointly with\nthe basic NNDSVD algorithm which introduces a lot of zeros; in this case,\nNNDSVDa or NNDSVDar should be preferred.\n\n`NMF` can also be initialized with correctly scaled random non-negative\nmatrices by setting `init=\"random\"`. An integer seed or a `RandomState` can\nalso be passed to `random_state` to control reproducibility.\n\nIn `NMF`, L1 and L2 priors can be added to the loss function in order to\nregularize the model. The L2 prior uses the Frobenius norm, while the L1 prior\nuses an elementwise L1 norm. As in `ElasticNet`, we control the combination of\nL1 and L2 with the `l1_ratio` (\\\\(\\rho\\\\)) parameter, and the intensity of the\nregularization with the `alpha` (\\\\(\\alpha\\\\)) parameter. Then the priors\nterms are:\n\nand the regularized objective function is:\n\n`NMF` regularizes both W and H by default. The `regularization` parameter\nallows for finer control, with which only W, only H, or both can be\nregularized.\n\nAs described previously, the most widely used distance function is the squared\nFrobenius norm, which is an obvious extension of the Euclidean norm to\nmatrices:\n\nOther distance functions can be used in NMF as, for example, the (generalized)\nKullback-Leibler (KL) divergence, also referred as I-divergence:\n\nOr, the Itakura-Saito (IS) divergence:\n\nThese three distances are special cases of the beta-divergence family, with\n\\\\(\\beta = 2, 1, 0\\\\) respectively 6. The beta-divergence are defined by :\n\nNote that this definition is not valid if \\\\(\\beta \\in (0; 1)\\\\), yet it can\nbe continuously extended to the definitions of \\\\(d_{KL}\\\\) and \\\\(d_{IS}\\\\)\nrespectively.\n\n`NMF` implements two solvers, using Coordinate Descent (\u2018cd\u2019) 5, and\nMultiplicative Update (\u2018mu\u2019) 6. The \u2018mu\u2019 solver can optimize every beta-\ndivergence, including of course the Frobenius norm (\\\\(\\beta=2\\\\)), the\n(generalized) Kullback-Leibler divergence (\\\\(\\beta=1\\\\)) and the Itakura-\nSaito divergence (\\\\(\\beta=0\\\\)). Note that for \\\\(\\beta \\in (1; 2)\\\\), the\n\u2018mu\u2019 solver is significantly faster than for other values of \\\\(\\beta\\\\). Note\nalso that with a negative (or 0, i.e. \u2018itakura-saito\u2019) \\\\(\\beta\\\\), the input\nmatrix cannot contain zero values.\n\nThe \u2018cd\u2019 solver can only optimize the Frobenius norm. Due to the underlying\nnon-convexity of NMF, the different solvers may converge to different minima,\neven when optimizing the same distance function.\n\nNMF is best used with the `fit_transform` method, which returns the matrix W.\nThe matrix H is stored into the fitted model in the `components_` attribute;\nthe method `transform` will decompose a new matrix X_new based on these stored\ncomponents:\n\nExamples:\n\nReferences:\n\n\u201cLearning the parts of objects by non-negative matrix factorization\u201d D. Lee,\nS. Seung, 1999\n\n\u201cNon-negative Matrix Factorization with Sparseness Constraints\u201d P. Hoyer, 2004\n\n\u201cSVD based initialization: A head start for nonnegative matrix factorization\u201d\nC. Boutsidis, E. Gallopoulos, 2008\n\n\u201cFast local algorithms for large scale nonnegative matrix and tensor\nfactorizations.\u201d A. Cichocki, A. Phan, 2009\n\n\u201cAlgorithms for nonnegative matrix factorization with the beta-divergence\u201d C.\nFevotte, J. Idier, 2011\n\nLatent Dirichlet Allocation is a generative probabilistic model for\ncollections of discrete dataset such as text corpora. It is also a topic model\nthat is used for discovering abstract topics from a collection of documents.\n\nThe graphical model of LDA is a three-level generative model:\n\nNote on notations presented in the graphical model above, which can be found\nin Hoffman et al. (2013):\n\nIn the graphical model, each node is a random variable and has a role in the\ngenerative process. A shaded node indicates an observed variable and an\nunshaded node indicates a hidden (latent) variable. In this case, words in the\ncorpus are the only data that we observe. The latent variables determine the\nrandom mixture of topics in the corpus and the distribution of words in the\ndocuments. The goal of LDA is to use the observed words to infer the hidden\ntopic structure.\n\nWhen modeling text corpora, the model assumes the following generative process\nfor a corpus with \\\\(D\\\\) documents and \\\\(K\\\\) topics, with \\\\(K\\\\)\ncorresponding to `n_components` in the API:\n\nFor parameter estimation, the posterior distribution is:\n\nSince the posterior is intractable, variational Bayesian method uses a simpler\ndistribution \\\\(q(z,\\theta,\\beta | \\lambda, \\phi, \\gamma)\\\\) to approximate\nit, and those variational parameters \\\\(\\lambda\\\\), \\\\(\\phi\\\\), \\\\(\\gamma\\\\)\nare optimized to maximize the Evidence Lower Bound (ELBO):\n\nMaximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL)\ndivergence between \\\\(q(z,\\theta,\\beta)\\\\) and the true posterior \\\\(p(z,\n\\theta, \\beta |w, \\alpha, \\eta)\\\\).\n\n`LatentDirichletAllocation` implements the online variational Bayes algorithm\nand supports both online and batch update methods. While the batch method\nupdates variational variables after each full pass through the data, the\nonline method updates variational variables from mini-batch data points.\n\nNote\n\nAlthough the online method is guaranteed to converge to a local optimum point,\nthe quality of the optimum point and the speed of convergence may depend on\nmini-batch size and attributes related to learning rate setting.\n\nWhen `LatentDirichletAllocation` is applied on a \u201cdocument-term\u201d matrix, the\nmatrix will be decomposed into a \u201ctopic-term\u201d matrix and a \u201cdocument-topic\u201d\nmatrix. While \u201ctopic-term\u201d matrix is stored as `components_` in the model,\n\u201cdocument-topic\u201d matrix can be calculated from `transform` method.\n\n`LatentDirichletAllocation` also implements `partial_fit` method. This is used\nwhen data can be fetched sequentially.\n\nExamples:\n\nReferences:\n\nSee also Dimensionality reduction for dimensionality reduction with\nNeighborhood Components Analysis.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.6. Covariance estimation", "path": "modules/covariance", "type": "Guide", "text": "\nMany statistical problems require the estimation of a population\u2019s covariance\nmatrix, which can be seen as an estimation of data set scatter plot shape.\nMost of the time, such an estimation has to be done on a sample whose\nproperties (size, structure, homogeneity) have a large influence on the\nestimation\u2019s quality. The `sklearn.covariance` package provides tools for\naccurately estimating a population\u2019s covariance matrix under various settings.\n\nWe assume that the observations are independent and identically distributed\n(i.i.d.).\n\nThe covariance matrix of a data set is known to be well approximated by the\nclassical maximum likelihood estimator (or \u201cempirical covariance\u201d), provided\nthe number of observations is large enough compared to the number of features\n(the variables describing the observations). More precisely, the Maximum\nLikelihood Estimator of a sample is an asymptotically unbiased estimator of\nthe corresponding population\u2019s covariance matrix.\n\nThe empirical covariance matrix of a sample can be computed using the\n`empirical_covariance` function of the package, or by fitting an\n`EmpiricalCovariance` object to the data sample with the\n`EmpiricalCovariance.fit` method. Be careful that results depend on whether\nthe data are centered, so one may want to use the `assume_centered` parameter\naccurately. More precisely, if `assume_centered=False`, then the test set is\nsupposed to have the same mean vector as the training set. If not, both should\nbe centered by the user, and `assume_centered=True` should be used.\n\nExamples:\n\nDespite being an asymptotically unbiased estimator of the covariance matrix,\nthe Maximum Likelihood Estimator is not a good estimator of the eigenvalues of\nthe covariance matrix, so the precision matrix obtained from its inversion is\nnot accurate. Sometimes, it even occurs that the empirical covariance matrix\ncannot be inverted for numerical reasons. To avoid such an inversion problem,\na transformation of the empirical covariance matrix has been introduced: the\n`shrinkage`.\n\nIn scikit-learn, this transformation (with a user-defined shrinkage\ncoefficient) can be directly applied to a pre-computed covariance with the\n`shrunk_covariance` method. Also, a shrunk estimator of the covariance can be\nfitted to data with a `ShrunkCovariance` object and its `ShrunkCovariance.fit`\nmethod. Again, results depend on whether the data are centered, so one may\nwant to use the `assume_centered` parameter accurately.\n\nMathematically, this shrinkage consists in reducing the ratio between the\nsmallest and the largest eigenvalues of the empirical covariance matrix. It\ncan be done by simply shifting every eigenvalue according to a given offset,\nwhich is equivalent of finding the l2-penalized Maximum Likelihood Estimator\nof the covariance matrix. In practice, shrinkage boils down to a simple a\nconvex transformation : \\\\(\\Sigma_{\\rm shrunk} = (1-\\alpha)\\hat{\\Sigma} +\n\\alpha\\frac{{\\rm Tr}\\hat{\\Sigma}}{p}\\rm Id\\\\).\n\nChoosing the amount of shrinkage, \\\\(\\alpha\\\\) amounts to setting a\nbias/variance trade-off, and is discussed below.\n\nExamples:\n\nIn their 2004 paper 1, O. Ledoit and M. Wolf propose a formula to compute the\noptimal shrinkage coefficient \\\\(\\alpha\\\\) that minimizes the Mean Squared\nError between the estimated and the real covariance matrix.\n\nThe Ledoit-Wolf estimator of the covariance matrix can be computed on a sample\nwith the `ledoit_wolf` function of the `sklearn.covariance` package, or it can\nbe otherwise obtained by fitting a `LedoitWolf` object to the same sample.\n\nNote\n\nCase when population covariance matrix is isotropic\n\nIt is important to note that when the number of samples is much larger than\nthe number of features, one would expect that no shrinkage would be necessary.\nThe intuition behind this is that if the population covariance is full rank,\nwhen the number of sample grows, the sample covariance will also become\npositive definite. As a result, no shrinkage would necessary and the method\nshould automatically do this.\n\nThis, however, is not the case in the Ledoit-Wolf procedure when the\npopulation covariance happens to be a multiple of the identity matrix. In this\ncase, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples\nincreases. This indicates that the optimal estimate of the covariance matrix\nin the Ledoit-Wolf sense is multiple of the identity. Since the population\ncovariance is already a multiple of the identity matrix, the Ledoit-Wolf\nsolution is indeed a reasonable estimate.\n\nExamples:\n\nReferences:\n\nO. Ledoit and M. Wolf, \u201cA Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\u201d, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411.\n\nUnder the assumption that the data are Gaussian distributed, Chen et al. 2\nderived a formula aimed at choosing a shrinkage coefficient that yields a\nsmaller Mean Squared Error than the one given by Ledoit and Wolf\u2019s formula.\nThe resulting estimator is known as the Oracle Shrinkage Approximating\nestimator of the covariance.\n\nThe OAS estimator of the covariance matrix can be computed on a sample with\nthe `oas` function of the `sklearn.covariance` package, or it can be otherwise\nobtained by fitting an `OAS` object to the same sample.\n\nBias-variance trade-off when setting the shrinkage: comparing the choices of\nLedoit-Wolf and OAS estimators\n\nReferences:\n\nChen et al., \u201cShrinkage Algorithms for MMSE Covariance Estimation\u201d, IEEE\nTrans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nExamples:\n\nThe matrix inverse of the covariance matrix, often called the precision\nmatrix, is proportional to the partial correlation matrix. It gives the\npartial independence relationship. In other words, if two features are\nindependent conditionally on the others, the corresponding coefficient in the\nprecision matrix will be zero. This is why it makes sense to estimate a sparse\nprecision matrix: the estimation of the covariance matrix is better\nconditioned by learning independence relations from the data. This is known as\ncovariance selection.\n\nIn the small-samples situation, in which `n_samples` is on the order of\n`n_features` or smaller, sparse inverse covariance estimators tend to work\nbetter than shrunk covariance estimators. However, in the opposite situation,\nor for very correlated data, they can be numerically unstable. In addition,\nunlike shrinkage estimators, sparse estimators are able to recover off-\ndiagonal structure.\n\nThe `GraphicalLasso` estimator uses an l1 penalty to enforce sparsity on the\nprecision matrix: the higher its `alpha` parameter, the more sparse the\nprecision matrix. The corresponding `GraphicalLassoCV` object uses cross-\nvalidation to automatically set the `alpha` parameter.\n\nA comparison of maximum likelihood, shrinkage and sparse estimates of the\ncovariance and precision matrix in the very small samples settings.\n\nNote\n\nStructure recovery\n\nRecovering a graphical structure from correlations in the data is a\nchallenging thing. If you are interested in such recovery keep in mind that:\n\nThe mathematical formulation is the following:\n\nWhere \\\\(K\\\\) is the precision matrix to be estimated, and \\\\(S\\\\) is the\nsample covariance matrix. \\\\(\\|K\\|_1\\\\) is the sum of the absolute values of\noff-diagonal coefficients of \\\\(K\\\\). The algorithm employed to solve this\nproblem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper.\nIt is the same algorithm as in the R `glasso` package.\n\nExamples:\n\nReferences:\n\nReal data sets are often subject to measurement or recording errors. Regular\nbut uncommon observations may also appear for a variety of reasons.\nObservations which are very uncommon are called outliers. The empirical\ncovariance estimator and the shrunk covariance estimators presented above are\nvery sensitive to the presence of outliers in the data. Therefore, one should\nuse robust covariance estimators to estimate the covariance of its real data\nsets. Alternatively, robust covariance estimators can be used to perform\noutlier detection and discard/downweight some observations according to\nfurther processing of the data.\n\nThe `sklearn.covariance` package implements a robust estimator of covariance,\nthe Minimum Covariance Determinant 3.\n\nThe Minimum Covariance Determinant estimator is a robust estimator of a data\nset\u2019s covariance introduced by P.J. Rousseeuw in 3. The idea is to find a\ngiven proportion (h) of \u201cgood\u201d observations which are not outliers and compute\ntheir empirical covariance matrix. This empirical covariance matrix is then\nrescaled to compensate the performed selection of observations (\u201cconsistency\nstep\u201d). Having computed the Minimum Covariance Determinant estimator, one can\ngive weights to observations according to their Mahalanobis distance, leading\nto a reweighted estimate of the covariance matrix of the data set\n(\u201creweighting step\u201d).\n\nRousseeuw and Van Driessen 4 developed the FastMCD algorithm in order to\ncompute the Minimum Covariance Determinant. This algorithm is used in scikit-\nlearn when fitting an MCD object to data. The FastMCD algorithm also computes\na robust estimate of the data set location at the same time.\n\nRaw estimates can be accessed as `raw_location_` and `raw_covariance_`\nattributes of a `MinCovDet` robust covariance estimator object.\n\nReferences:\n\nP. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871,\n1984.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS.\n\nExamples:\n\nInfluence of outliers on location and covariance estimates\n\nSeparating inliers from outliers using a Mahalanobis distance\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.7. Novelty and Outlier Detection", "path": "modules/outlier_detection", "type": "Guide", "text": "\nMany applications require being able to decide whether a new observation\nbelongs to the same distribution as existing observations (it is an inlier),\nor should be considered as different (it is an outlier). Often, this ability\nis used to clean real data sets. Two important distinctions must be made:\n\nThe training data contains outliers which are defined as observations that are\nfar from the others. Outlier detection estimators thus try to fit the regions\nwhere the training data is the most concentrated, ignoring the deviant\nobservations.\n\nThe training data is not polluted by outliers and we are interested in\ndetecting whether a new observation is an outlier. In this context an outlier\nis also called a novelty.\n\nOutlier detection and novelty detection are both used for anomaly detection,\nwhere one is interested in detecting abnormal or unusual observations. Outlier\ndetection is then also known as unsupervised anomaly detection and novelty\ndetection as semi-supervised anomaly detection. In the context of outlier\ndetection, the outliers/anomalies cannot form a dense cluster as available\nestimators assume that the outliers/anomalies are located in low density\nregions. On the contrary, in the context of novelty detection,\nnovelties/anomalies can form a dense cluster as long as they are in a low\ndensity region of the training data, considered as normal in this context.\n\nThe scikit-learn project provides a set of machine learning tools that can be\nused both for novelty or outlier detection. This strategy is implemented with\nobjects learning in an unsupervised way from the data:\n\nnew observations can then be sorted as inliers or outliers with a `predict`\nmethod:\n\nInliers are labeled 1, while outliers are labeled -1. The predict method makes\nuse of a threshold on the raw scoring function computed by the estimator. This\nscoring function is accessible through the `score_samples` method, while the\nthreshold can be controlled by the `contamination` parameter.\n\nThe `decision_function` method is also defined from the scoring function, in\nsuch a way that negative values are outliers and non-negative ones are\ninliers:\n\nNote that `neighbors.LocalOutlierFactor` does not support `predict`,\n`decision_function` and `score_samples` methods by default but only a\n`fit_predict` method, as this estimator was originally meant to be applied for\noutlier detection. The scores of abnormality of the training samples are\naccessible through the `negative_outlier_factor_` attribute.\n\nIf you really want to use `neighbors.LocalOutlierFactor` for novelty\ndetection, i.e. predict labels or compute the score of abnormality of new\nunseen data, you can instantiate the estimator with the `novelty` parameter\nset to `True` before fitting the estimator. In this case, `fit_predict` is not\navailable.\n\nWarning\n\nNovelty detection with Local Outlier Factor\n\nWhen `novelty` is set to `True` be aware that you must only use `predict`,\n`decision_function` and `score_samples` on new unseen data and not on the\ntraining samples as this would lead to wrong results. The scores of\nabnormality of the training samples are always accessible through the\n`negative_outlier_factor_` attribute.\n\nThe behavior of `neighbors.LocalOutlierFactor` is summarized in the following\ntable.\n\nMethod\n\nOutlier detection\n\nNovelty detection\n\n`fit_predict`\n\nOK\n\nNot available\n\n`predict`\n\nNot available\n\nUse only on new data\n\n`decision_function`\n\nNot available\n\nUse only on new data\n\n`score_samples`\n\nUse `negative_outlier_factor_`\n\nUse only on new data\n\nA comparison of the outlier detection algorithms in scikit-learn. Local\nOutlier Factor (LOF) does not show a decision boundary in black as it has no\npredict method to be applied on new data when it is used for outlier\ndetection.\n\n`ensemble.IsolationForest` and `neighbors.LocalOutlierFactor` perform\nreasonably well on the data sets considered here. The `svm.OneClassSVM` is\nknown to be sensitive to outliers and thus does not perform very well for\noutlier detection. That being said, outlier detection in high-dimension, or\nwithout any assumptions on the distribution of the inlying data is very\nchallenging. `svm.OneClassSVM` may still be used with outlier detection but\nrequires fine-tuning of its hyperparameter `nu` to handle outliers and prevent\noverfitting. Finally, `covariance.EllipticEnvelope` assumes the data is\nGaussian and learns an ellipse. For more details on the different estimators\nrefer to the example Comparing anomaly detection algorithms for outlier\ndetection on toy datasets and the sections hereunder.\n\nExamples:\n\nConsider a data set of \\\\(n\\\\) observations from the same distribution\ndescribed by \\\\(p\\\\) features. Consider now that we add one more observation\nto that data set. Is the new observation so different from the others that we\ncan doubt it is regular? (i.e. does it come from the same distribution?) Or on\nthe contrary, is it so similar to the other that we cannot distinguish it from\nthe original observations? This is the question addressed by the novelty\ndetection tools and methods.\n\nIn general, it is about to learn a rough, close frontier delimiting the\ncontour of the initial observations distribution, plotted in embedding\n\\\\(p\\\\)-dimensional space. Then, if further observations lay within the\nfrontier-delimited subspace, they are considered as coming from the same\npopulation than the initial observations. Otherwise, if they lay outside the\nfrontier, we can say that they are abnormal with a given confidence in our\nassessment.\n\nThe One-Class SVM has been introduced by Sch\u00f6lkopf et al. for that purpose and\nimplemented in the Support Vector Machines module in the `svm.OneClassSVM`\nobject. It requires the choice of a kernel and a scalar parameter to define a\nfrontier. The RBF kernel is usually chosen although there exists no exact\nformula or algorithm to set its bandwidth parameter. This is the default in\nthe scikit-learn implementation. The `nu` parameter, also known as the margin\nof the One-Class SVM, corresponds to the probability of finding a new, but\nregular, observation outside the frontier.\n\nReferences:\n\nExamples:\n\nOutlier detection is similar to novelty detection in the sense that the goal\nis to separate a core of regular observations from some polluting ones, called\noutliers. Yet, in the case of outlier detection, we don\u2019t have a clean data\nset representing the population of regular observations that can be used to\ntrain any tool.\n\nOne common way of performing outlier detection is to assume that the regular\ndata come from a known distribution (e.g. data are Gaussian distributed). From\nthis assumption, we generally try to define the \u201cshape\u201d of the data, and can\ndefine outlying observations as observations which stand far enough from the\nfit shape.\n\nThe scikit-learn provides an object `covariance.EllipticEnvelope` that fits a\nrobust covariance estimate to the data, and thus fits an ellipse to the\ncentral data points, ignoring points outside the central mode.\n\nFor instance, assuming that the inlier data are Gaussian distributed, it will\nestimate the inlier location and covariance in a robust way (i.e. without\nbeing influenced by outliers). The Mahalanobis distances obtained from this\nestimate is used to derive a measure of outlyingness. This strategy is\nillustrated below.\n\nExamples:\n\nReferences:\n\nOne efficient way of performing outlier detection in high-dimensional datasets\nis to use random forests. The `ensemble.IsolationForest` \u2018isolates\u2019\nobservations by randomly selecting a feature and then randomly selecting a\nsplit value between the maximum and minimum values of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a measure of\nnormality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies. Hence,\nwhen a forest of random trees collectively produce shorter path lengths for\nparticular samples, they are highly likely to be anomalies.\n\nThe implementation of `ensemble.IsolationForest` is based on an ensemble of\n`tree.ExtraTreeRegressor`. Following Isolation Forest original paper, the\nmaximum depth of each tree is set to \\\\(\\lceil \\log_2(n) \\rceil\\\\) where\n\\\\(n\\\\) is the number of samples used to build the tree (see (Liu et al.,\n2008) for more details).\n\nThis algorithm is illustrated below.\n\nThe `ensemble.IsolationForest` supports `warm_start=True` which allows you to\nadd more trees to an already fitted model:\n\nExamples:\n\nReferences:\n\nAnother efficient way to perform outlier detection on moderately high\ndimensional datasets is to use the Local Outlier Factor (LOF) algorithm.\n\nThe `neighbors.LocalOutlierFactor` (LOF) algorithm computes a score (called\nlocal outlier factor) reflecting the degree of abnormality of the\nobservations. It measures the local density deviation of a given data point\nwith respect to its neighbors. The idea is to detect the samples that have a\nsubstantially lower density than their neighbors.\n\nIn practice the local density is obtained from the k-nearest neighbors. The\nLOF score of an observation is equal to the ratio of the average local density\nof his k-nearest neighbors, and its own local density: a normal instance is\nexpected to have a local density similar to that of its neighbors, while\nabnormal data are expected to have much smaller local density.\n\nThe number k of neighbors considered, (alias parameter n_neighbors) is\ntypically chosen 1) greater than the minimum number of objects a cluster has\nto contain, so that other objects can be local outliers relative to this\ncluster, and 2) smaller than the maximum number of close by objects that can\npotentially be local outliers. In practice, such informations are generally\nnot available, and taking n_neighbors=20 appears to work well in general. When\nthe proportion of outliers is high (i.e. greater than 10 %, as in the example\nbelow), n_neighbors should be greater (n_neighbors=35 in the example below).\n\nThe strength of the LOF algorithm is that it takes both local and global\nproperties of datasets into consideration: it can perform well even in\ndatasets where abnormal samples have different underlying densities. The\nquestion is not, how isolated the sample is, but how isolated it is with\nrespect to the surrounding neighborhood.\n\nWhen applying LOF for outlier detection, there are no `predict`,\n`decision_function` and `score_samples` methods but only a `fit_predict`\nmethod. The scores of abnormality of the training samples are accessible\nthrough the `negative_outlier_factor_` attribute. Note that `predict`,\n`decision_function` and `score_samples` can be used on new unseen data when\nLOF is applied for novelty detection, i.e. when the `novelty` parameter is set\nto `True`. See Novelty detection with Local Outlier Factor.\n\nThis strategy is illustrated below.\n\nExamples:\n\nReferences:\n\nTo use `neighbors.LocalOutlierFactor` for novelty detection, i.e. predict\nlabels or compute the score of abnormality of new unseen data, you need to\ninstantiate the estimator with the `novelty` parameter set to `True` before\nfitting the estimator:\n\nNote that `fit_predict` is not available in this case.\n\nWarning\n\nNovelty detection with Local Outlier Factor`\n\nWhen `novelty` is set to `True` be aware that you must only use `predict`,\n`decision_function` and `score_samples` on new unseen data and not on the\ntraining samples as this would lead to wrong results. The scores of\nabnormality of the training samples are always accessible through the\n`negative_outlier_factor_` attribute.\n\nNovelty detection with Local Outlier Factor is illustrated below.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.8. Density Estimation", "path": "modules/density", "type": "Guide", "text": "\nDensity estimation walks the line between unsupervised learning, feature\nengineering, and data modeling. Some of the most popular and useful density\nestimation techniques are mixture models such as Gaussian Mixtures\n(`GaussianMixture`), and neighbor-based approaches such as the kernel density\nestimate (`KernelDensity`). Gaussian Mixtures are discussed more fully in the\ncontext of clustering, because the technique is also useful as an unsupervised\nclustering scheme.\n\nDensity estimation is a very simple concept, and most people are already\nfamiliar with one common density estimation technique: the histogram.\n\nA histogram is a simple visualization of data where bins are defined, and the\nnumber of data points within each bin is tallied. An example of a histogram\ncan be seen in the upper-left panel of the following figure:\n\nA major problem with histograms, however, is that the choice of binning can\nhave a disproportionate effect on the resulting visualization. Consider the\nupper-right panel of the above figure. It shows a histogram over the same\ndata, with the bins shifted right. The results of the two visualizations look\nentirely different, and might lead to different interpretations of the data.\n\nIntuitively, one can also think of a histogram as a stack of blocks, one block\nper point. By stacking the blocks in the appropriate grid space, we recover\nthe histogram. But what if, instead of stacking the blocks on a regular grid,\nwe center each block on the point it represents, and sum the total height at\neach location? This idea leads to the lower-left visualization. It is perhaps\nnot as clean as a histogram, but the fact that the data drive the block\nlocations mean that it is a much better representation of the underlying data.\n\nThis visualization is an example of a kernel density estimation, in this case\nwith a top-hat kernel (i.e. a square block at each point). We can recover a\nsmoother distribution by using a smoother kernel. The bottom-right plot shows\na Gaussian kernel density estimate, in which each point contributes a Gaussian\ncurve to the total. The result is a smooth density estimate which is derived\nfrom the data, and functions as a powerful non-parametric model of the\ndistribution of points.\n\nKernel density estimation in scikit-learn is implemented in the\n`KernelDensity` estimator, which uses the Ball Tree or KD Tree for efficient\nqueries (see Nearest Neighbors for a discussion of these). Though the above\nexample uses a 1D data set for simplicity, kernel density estimation can be\nperformed in any number of dimensions, though in practice the curse of\ndimensionality causes its performance to degrade in high dimensions.\n\nIn the following figure, 100 points are drawn from a bimodal distribution, and\nthe kernel density estimates are shown for three choices of kernels:\n\nIt\u2019s clear how the kernel shape affects the smoothness of the resulting\ndistribution. The scikit-learn kernel density estimator can be used as\nfollows:\n\nHere we have used `kernel='gaussian'`, as seen above. Mathematically, a kernel\nis a positive function \\\\(K(x;h)\\\\) which is controlled by the bandwidth\nparameter \\\\(h\\\\). Given this kernel form, the density estimate at a point\n\\\\(y\\\\) within a group of points \\\\(x_i; i=1\\cdots N\\\\) is given by:\n\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\nbetween bias and variance in the result. A large bandwidth leads to a very\nsmooth (i.e. high-bias) density distribution. A small bandwidth leads to an\nunsmooth (i.e. high-variance) density distribution.\n\n`KernelDensity` implements several common kernel forms, which are shown in the\nfollowing figure:\n\nThe form of these kernels is as follows:\n\nGaussian kernel (`kernel = 'gaussian'`)\n\n\\\\(K(x; h) \\propto \\exp(- \\frac{x^2}{2h^2} )\\\\)\n\nTophat kernel (`kernel = 'tophat'`)\n\n\\\\(K(x; h) \\propto 1\\\\) if \\\\(x < h\\\\)\n\nEpanechnikov kernel (`kernel = 'epanechnikov'`)\n\n\\\\(K(x; h) \\propto 1 - \\frac{x^2}{h^2}\\\\)\n\nExponential kernel (`kernel = 'exponential'`)\n\n\\\\(K(x; h) \\propto \\exp(-x/h)\\\\)\n\nLinear kernel (`kernel = 'linear'`)\n\n\\\\(K(x; h) \\propto 1 - x/h\\\\) if \\\\(x < h\\\\)\n\nCosine kernel (`kernel = 'cosine'`)\n\n\\\\(K(x; h) \\propto \\cos(\\frac{\\pi x}{2h})\\\\) if \\\\(x < h\\\\)\n\nThe kernel density estimator can be used with any of the valid distance\nmetrics (see `DistanceMetric` for a list of available metrics), though the\nresults are properly normalized only for the Euclidean metric. One\nparticularly useful metric is the Haversine distance which measures the\nangular distance between points on a sphere. Here is an example of using a\nkernel density estimate for a visualization of geospatial data, in this case\nthe distribution of observations of two different species on the South\nAmerican continent:\n\nOne other useful application of kernel density estimation is to learn a non-\nparametric generative model of a dataset in order to efficiently draw new\nsamples from this generative model. Here is an example of using this process\nto create a new set of hand-written digits, using a Gaussian kernel learned on\na PCA projection of the data:\n\nThe \u201cnew\u201d data consists of linear combinations of the input data, with weights\nprobabilistically drawn given the KDE model.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "2.9. Neural network models", "path": "modules/neural_networks_unsupervised", "type": "Guide", "text": "\nRestricted Boltzmann machines (RBM) are unsupervised nonlinear feature\nlearners based on a probabilistic model. The features extracted by an RBM or a\nhierarchy of RBMs often give good results when fed into a linear classifier\nsuch as a linear SVM or a perceptron.\n\nThe model makes assumptions regarding the distribution of inputs. At the\nmoment, scikit-learn only provides `BernoulliRBM`, which assumes the inputs\nare either binary values or values between 0 and 1, each encoding the\nprobability that the specific feature would be turned on.\n\nThe RBM tries to maximize the likelihood of the data using a particular\ngraphical model. The parameter learning algorithm used (Stochastic Maximum\nLikelihood) prevents the representations from straying far from the input\ndata, which makes them capture interesting regularities, but makes the model\nless useful for small datasets, and usually not useful for density estimation.\n\nThe method gained popularity for initializing deep neural networks with the\nweights of independent RBMs. This method is known as unsupervised pre-\ntraining.\n\nExamples:\n\nThe graphical model of an RBM is a fully-connected bipartite graph.\n\nThe nodes are random variables whose states depend on the state of the other\nnodes they are connected to. The model is therefore parameterized by the\nweights of the connections, as well as one intercept (bias) term for each\nvisible and hidden unit, omitted from the image for simplicity.\n\nThe energy function measures the quality of a joint assignment:\n\nIn the formula above, \\\\(\\mathbf{b}\\\\) and \\\\(\\mathbf{c}\\\\) are the intercept\nvectors for the visible and hidden layers, respectively. The joint probability\nof the model is defined in terms of the energy:\n\nThe word restricted refers to the bipartite structure of the model, which\nprohibits direct interaction between hidden units, or between visible units.\nThis means that the following conditional independencies are assumed:\n\nThe bipartite structure allows for the use of efficient block Gibbs sampling\nfor inference.\n\nIn the `BernoulliRBM`, all units are binary stochastic units. This means that\nthe input data should either be binary, or real-valued between 0 and 1\nsignifying the probability that the visible unit would turn on or off. This is\na good model for character recognition, where the interest is on which pixels\nare active and which aren\u2019t. For images of natural scenes it no longer fits\nbecause of background, depth and the tendency of neighbouring pixels to take\nthe same values.\n\nThe conditional probability distribution of each unit is given by the logistic\nsigmoid activation function of the input it receives:\n\nwhere \\\\(\\sigma\\\\) is the logistic sigmoid function:\n\nThe training algorithm implemented in `BernoulliRBM` is known as Stochastic\nMaximum Likelihood (SML) or Persistent Contrastive Divergence (PCD).\nOptimizing maximum likelihood directly is infeasible because of the form of\nthe data likelihood:\n\nFor simplicity the equation above is written for a single training example.\nThe gradient with respect to the weights is formed of two terms corresponding\nto the ones above. They are usually known as the positive gradient and the\nnegative gradient, because of their respective signs. In this implementation,\nthe gradients are estimated over mini-batches of samples.\n\nIn maximizing the log-likelihood, the positive gradient makes the model prefer\nhidden states that are compatible with the observed training data. Because of\nthe bipartite structure of RBMs, it can be computed efficiently. The negative\ngradient, however, is intractable. Its goal is to lower the energy of joint\nstates that the model prefers, therefore making it stay true to the data. It\ncan be approximated by Markov chain Monte Carlo using block Gibbs sampling by\niteratively sampling each of \\\\(v\\\\) and \\\\(h\\\\) given the other, until the\nchain mixes. Samples generated in this way are sometimes referred as fantasy\nparticles. This is inefficient and it is difficult to determine whether the\nMarkov chain mixes.\n\nThe Contrastive Divergence method suggests to stop the chain after a small\nnumber of iterations, \\\\(k\\\\), usually even 1. This method is fast and has low\nvariance, but the samples are far from the model distribution.\n\nPersistent Contrastive Divergence addresses this. Instead of starting a new\nchain each time the gradient is needed, and performing only one Gibbs sampling\nstep, in PCD we keep a number of chains (fantasy particles) that are updated\n\\\\(k\\\\) Gibbs steps after each weight update. This allows the particles to\nexplore the space more thoroughly.\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "3.1. Cross-validation", "path": "modules/cross_validation", "type": "Guide", "text": "\nLearning the parameters of a prediction function and testing it on the same\ndata is a methodological mistake: a model that would just repeat the labels of\nthe samples that it has just seen would have a perfect score but would fail to\npredict anything useful on yet-unseen data. This situation is called\noverfitting. To avoid it, it is common practice when performing a (supervised)\nmachine learning experiment to hold out part of the available data as a test\nset `X_test, y_test`. Note that the word \u201cexperiment\u201d is not intended to\ndenote academic use only, because even in commercial settings machine learning\nusually starts out experimentally. Here is a flowchart of typical cross\nvalidation workflow in model training. The best parameters can be determined\nby grid search techniques.\n\nIn scikit-learn a random split into training and test sets can be quickly\ncomputed with the `train_test_split` helper function. Let\u2019s load the iris data\nset to fit a linear support vector machine on it:\n\nWe can now quickly sample a training set while holding out 40% of the data for\ntesting (evaluating) our classifier:\n\nWhen evaluating different settings (\u201chyperparameters\u201d) for estimators, such as\nthe `C` setting that must be manually set for an SVM, there is still a risk of\noverfitting on the test set because the parameters can be tweaked until the\nestimator performs optimally. This way, knowledge about the test set can\n\u201cleak\u201d into the model and evaluation metrics no longer report on\ngeneralization performance. To solve this problem, yet another part of the\ndataset can be held out as a so-called \u201cvalidation set\u201d: training proceeds on\nthe training set, after which evaluation is done on the validation set, and\nwhen the experiment seems to be successful, final evaluation can be done on\nthe test set.\n\nHowever, by partitioning the available data into three sets, we drastically\nreduce the number of samples which can be used for learning the model, and the\nresults can depend on a particular random choice for the pair of (train,\nvalidation) sets.\n\nA solution to this problem is a procedure called cross-validation (CV for\nshort). A test set should still be held out for final evaluation, but the\nvalidation set is no longer needed when doing CV. In the basic approach,\ncalled k-fold CV, the training set is split into k smaller sets (other\napproaches are described below, but generally follow the same principles). The\nfollowing procedure is followed for each of the k \u201cfolds\u201d:\n\nThe performance measure reported by k-fold cross-validation is then the\naverage of the values computed in the loop. This approach can be\ncomputationally expensive, but does not waste too much data (as is the case\nwhen fixing an arbitrary validation set), which is a major advantage in\nproblems such as inverse inference where the number of samples is very small.\n\nThe simplest way to use cross-validation is to call the `cross_val_score`\nhelper function on the estimator and the dataset.\n\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset by splitting the data,\nfitting a model and computing the score 5 consecutive times (with different\nsplits each time):\n\nThe mean score and the standard deviation are hence given by:\n\nBy default, the score computed at each CV iteration is the `score` method of\nthe estimator. It is possible to change this by using the scoring parameter:\n\nSee The scoring parameter: defining model evaluation rules for details. In the\ncase of the Iris dataset, the samples are balanced across target classes hence\nthe accuracy and the F1-score are almost equal.\n\nWhen the `cv` argument is an integer, `cross_val_score` uses the `KFold` or\n`StratifiedKFold` strategies by default, the latter being used if the\nestimator derives from `ClassifierMixin`.\n\nIt is also possible to use other cross validation strategies by passing a\ncross validation iterator instead, for instance:\n\nAnother option is to use an iterable yielding (train, test) splits as arrays\nof indices, for example:\n\nData transformation with held out data\n\nJust as it is important to test a predictor on data held-out from training,\npreprocessing (such as standardization, feature selection, etc.) and similar\ndata transformations similarly should be learnt from a training set and\napplied to held-out data for prediction:\n\nA `Pipeline` makes it easier to compose estimators, providing this behavior\nunder cross-validation:\n\nSee Pipelines and composite estimators.\n\nThe `cross_validate` function differs from `cross_val_score` in two ways:\n\nFor single metric evaluation, where the scoring parameter is a string,\ncallable or None, the keys will be - `['test_score', 'fit_time',\n'score_time']`\n\nAnd for multiple metric evaluation, the return value is a dict with the\nfollowing keys - `['test_<scorer1_name>', 'test_<scorer2_name>',\n'test_<scorer...>', 'fit_time', 'score_time']`\n\n`return_train_score` is set to `False` by default to save computation time. To\nevaluate the scores on the training set as well you need to be set to `True`.\n\nYou may also retain the estimator fitted on each training set by setting\n`return_estimator=True`.\n\nThe multiple metrics can be specified either as a list, tuple or set of\npredefined scorer names:\n\nOr as a dict mapping scorer name to a predefined or custom scoring function:\n\nHere is an example of `cross_validate` using a single metric:\n\nThe function `cross_val_predict` has a similar interface to `cross_val_score`,\nbut returns, for each element in the input, the prediction that was obtained\nfor that element when it was in the test set. Only cross-validation strategies\nthat assign all elements to a test set exactly once can be used (otherwise, an\nexception is raised).\n\nWarning\n\nNote on inappropriate usage of cross_val_predict\n\nThe result of `cross_val_predict` may be different from those obtained using\n`cross_val_score` as the elements are grouped in different ways. The function\n`cross_val_score` takes an average over cross-validation folds, whereas\n`cross_val_predict` simply returns the labels (or probabilities) from several\ndistinct models undistinguished. Thus, `cross_val_predict` is not an\nappropriate measure of generalisation error.\n\nThe available cross validation iterators are introduced in the following\nsection.\n\nExamples\n\nThe following sections list utilities to generate indices that can be used to\ngenerate dataset splits according to different cross validation strategies.\n\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\nmaking the assumption that all samples stem from the same generative process\nand that the generative process is assumed to have no memory of past generated\nsamples.\n\nThe following cross-validators can be used in such cases.\n\nNote\n\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\nholds in practice. If one knows that the samples have been generated using a\ntime-dependent process, it is safer to use a time-series aware cross-\nvalidation scheme. Similarly, if we know that the generative process has a\ngroup structure (samples collected from different subjects, experiments,\nmeasurement devices), it is safer to use group-wise cross-validation.\n\n`KFold` divides all the samples in \\\\(k\\\\) groups of samples, called folds (if\n\\\\(k = n\\\\), this is equivalent to the Leave One Out strategy), of equal sizes\n(if possible). The prediction function is learned using \\\\(k - 1\\\\) folds, and\nthe fold left out is used for test.\n\nExample of 2-fold cross-validation on a dataset with 4 samples:\n\nHere is a visualization of the cross-validation behavior. Note that `KFold` is\nnot affected by classes or groups.\n\nEach fold is constituted by two arrays: the first one is related to the\ntraining set, and the second one to the test set. Thus, one can create the\ntraining/test sets using numpy indexing:\n\n`RepeatedKFold` repeats K-Fold n times. It can be used when one requires to\nrun `KFold` n times, producing different splits in each repetition.\n\nExample of 2-fold K-Fold repeated 2 times:\n\nSimilarly, `RepeatedStratifiedKFold` repeats Stratified K-Fold n times with\ndifferent randomization in each repetition.\n\n`LeaveOneOut` (or LOO) is a simple cross-validation. Each learning set is\ncreated by taking all the samples except one, the test set being the sample\nleft out. Thus, for \\\\(n\\\\) samples, we have \\\\(n\\\\) different training sets\nand \\\\(n\\\\) different tests set. This cross-validation procedure does not\nwaste much data as only one sample is removed from the training set:\n\nPotential users of LOO for model selection should weigh a few known caveats.\nWhen compared with \\\\(k\\\\)-fold cross validation, one builds \\\\(n\\\\) models\nfrom \\\\(n\\\\) samples instead of \\\\(k\\\\) models, where \\\\(n > k\\\\). Moreover,\neach is trained on \\\\(n - 1\\\\) samples rather than \\\\((k-1) n / k\\\\). In both\nways, assuming \\\\(k\\\\) is not too large and \\\\(k < n\\\\), LOO is more\ncomputationally expensive than \\\\(k\\\\)-fold cross validation.\n\nIn terms of accuracy, LOO often results in high variance as an estimator for\nthe test error. Intuitively, since \\\\(n - 1\\\\) of the \\\\(n\\\\) samples are used\nto build each model, models constructed from folds are virtually identical to\neach other and to the model built from the entire training set.\n\nHowever, if the learning curve is steep for the training size in question,\nthen 5- or 10- fold cross validation can overestimate the generalization\nerror.\n\nAs a general rule, most authors, and empirical evidence, suggest that 5- or\n10- fold cross validation should be preferred to LOO.\n\nReferences:\n\n`LeavePOut` is very similar to `LeaveOneOut` as it creates all the possible\ntraining/test sets by removing \\\\(p\\\\) samples from the complete set. For\n\\\\(n\\\\) samples, this produces \\\\({n \\choose p}\\\\) train-test pairs. Unlike\n`LeaveOneOut` and `KFold`, the test sets will overlap for \\\\(p > 1\\\\).\n\nExample of Leave-2-Out on a dataset with 4 samples:\n\nThe `ShuffleSplit` iterator will generate a user defined number of independent\ntrain / test dataset splits. Samples are first shuffled and then split into a\npair of train and test sets.\n\nIt is possible to control the randomness for reproducibility of the results by\nexplicitly seeding the `random_state` pseudo random number generator.\n\nHere is a usage example:\n\nHere is a visualization of the cross-validation behavior. Note that\n`ShuffleSplit` is not affected by classes or groups.\n\n`ShuffleSplit` is thus a good alternative to `KFold` cross validation that\nallows a finer control on the number of iterations and the proportion of\nsamples on each side of the train / test split.\n\nSome classification problems can exhibit a large imbalance in the distribution\nof the target classes: for instance there could be several times more negative\nsamples than positive samples. In such cases it is recommended to use\nstratified sampling as implemented in `StratifiedKFold` and\n`StratifiedShuffleSplit` to ensure that relative class frequencies is\napproximately preserved in each train and validation fold.\n\n`StratifiedKFold` is a variation of k-fold which returns stratified folds:\neach set contains approximately the same percentage of samples of each target\nclass as the complete set.\n\nHere is an example of stratified 3-fold cross-validation on a dataset with 50\nsamples from two unbalanced classes. We show the number of samples in each\nclass and compare with `KFold`.\n\nWe can see that `StratifiedKFold` preserves the class ratios (approximately 1\n/ 10) in both train and test dataset.\n\nHere is a visualization of the cross-validation behavior.\n\n`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold n times with\ndifferent randomization in each repetition.\n\n`StratifiedShuffleSplit` is a variation of ShuffleSplit, which returns\nstratified splits, i.e which creates splits by preserving the same percentage\nfor each target class as in the complete set.\n\nHere is a visualization of the cross-validation behavior.\n\nThe i.i.d. assumption is broken if the underlying generative process yield\ngroups of dependent samples.\n\nSuch a grouping of data is domain specific. An example would be when there is\nmedical data collected from multiple patients, with multiple samples taken\nfrom each patient. And such data is likely to be dependent on the individual\ngroup. In our example, the patient id for each sample will be its group\nidentifier.\n\nIn this case we would like to know if a model trained on a particular set of\ngroups generalizes well to the unseen groups. To measure this, we need to\nensure that all the samples in the validation fold come from groups that are\nnot represented at all in the paired training fold.\n\nThe following cross-validation splitters can be used to do that. The grouping\nidentifier for the samples is specified via the `groups` parameter.\n\n`GroupKFold` is a variation of k-fold which ensures that the same group is not\nrepresented in both testing and training sets. For example if the data is\nobtained from different subjects with several samples per-subject and if the\nmodel is flexible enough to learn from highly person specific features it\ncould fail to generalize to new subjects. `GroupKFold` makes it possible to\ndetect this kind of overfitting situations.\n\nImagine you have three subjects, each with an associated number from 1 to 3:\n\nEach subject is in a different testing fold, and the same subject is never in\nboth testing and training. Notice that the folds do not have exactly the same\nsize due to the imbalance in the data.\n\nHere is a visualization of the cross-validation behavior.\n\n`LeaveOneGroupOut` is a cross-validation scheme which holds out the samples\naccording to a third-party provided array of integer groups. This group\ninformation can be used to encode arbitrary domain specific pre-defined cross-\nvalidation folds.\n\nEach training set is thus constituted by all the samples except the ones\nrelated to a specific group.\n\nFor example, in the cases of multiple experiments, `LeaveOneGroupOut` can be\nused to create a cross-validation based on the different experiments: we\ncreate a training set using the samples of all the experiments except one:\n\nAnother common application is to use time information: for instance the groups\ncould be the year of collection of the samples and thus allow for cross-\nvalidation against time-based splits.\n\n`LeavePGroupsOut` is similar as `LeaveOneGroupOut`, but removes samples\nrelated to \\\\(P\\\\) groups for each training/test set.\n\nExample of Leave-2-Group Out:\n\nThe `GroupShuffleSplit` iterator behaves as a combination of `ShuffleSplit`\nand `LeavePGroupsOut`, and generates a sequence of randomized partitions in\nwhich a subset of groups are held out for each split.\n\nHere is a usage example:\n\nHere is a visualization of the cross-validation behavior.\n\nThis class is useful when the behavior of `LeavePGroupsOut` is desired, but\nthe number of groups is large enough that generating all possible partitions\nwith \\\\(P\\\\) groups withheld would be prohibitively expensive. In such a\nscenario, `GroupShuffleSplit` provides a random sample (with replacement) of\nthe train / test splits generated by `LeavePGroupsOut`.\n\nFor some datasets, a pre-defined split of the data into training- and\nvalidation fold or into several cross-validation folds already exists. Using\n`PredefinedSplit` it is possible to use these folds e.g. when searching for\nhyperparameters.\n\nFor example, when using a validation set, set the `test_fold` to 0 for all\nsamples that are part of the validation set, and to -1 for all other samples.\n\nThe above group cross-validation functions may also be useful for spitting a\ndataset into training and testing subsets. Note that the convenience function\n`train_test_split` is a wrapper around `ShuffleSplit` and thus only allows for\nstratified splitting (using the class labels) and cannot account for groups.\n\nTo perform the train and test split, use the indices for the train and test\nsubsets yielded by the generator output by the `split()` method of the cross-\nvalidation splitter. For example:\n\nTime series data is characterised by the correlation between observations that\nare near in time (autocorrelation). However, classical cross-validation\ntechniques such as `KFold` and `ShuffleSplit` assume the samples are\nindependent and identically distributed, and would result in unreasonable\ncorrelation between training and testing instances (yielding poor estimates of\ngeneralisation error) on time series data. Therefore, it is very important to\nevaluate our model for time series data on the \u201cfuture\u201d observations least\nlike those that are used to train the model. To achieve this, one solution is\nprovided by `TimeSeriesSplit`.\n\n`TimeSeriesSplit` is a variation of k-fold which returns first \\\\(k\\\\) folds\nas train set and the \\\\((k+1)\\\\) th fold as test set. Note that unlike\nstandard cross-validation methods, successive training sets are supersets of\nthose that come before them. Also, it adds all surplus data to the first\ntraining partition, which is always used to train the model.\n\nThis class can be used to cross-validate time series data samples that are\nobserved at fixed time intervals.\n\nExample of 3-split time series cross-validation on a dataset with 6 samples:\n\nHere is a visualization of the cross-validation behavior.\n\nIf the data ordering is not arbitrary (e.g. samples with the same class label\nare contiguous), shuffling it first may be essential to get a meaningful\ncross- validation result. However, the opposite may be true if the samples are\nnot independently and identically distributed. For example, if samples\ncorrespond to news articles, and are ordered by their time of publication,\nthen shuffling the data will likely lead to a model that is overfit and an\ninflated validation score: it will be tested on samples that are artificially\nsimilar (close in time) to training samples.\n\nSome cross validation iterators, such as `KFold`, have an inbuilt option to\nshuffle the data indices before splitting them. Note that:\n\nFor more details on how to control the randomness of cv splitters and avoid\ncommon pitfalls, see Controlling randomness.\n\nCross validation iterators can also be used to directly perform model\nselection using Grid Search for the optimal hyperparameters of the model. This\nis the topic of the next section: Tuning the hyper-parameters of an estimator.\n\n`permutation_test_score` offers another way to evaluate the performance of\nclassifiers. It provides a permutation-based p-value, which represents how\nlikely an observed performance of the classifier would be obtained by chance.\nThe null hypothesis in this test is that the classifier fails to leverage any\nstatistical dependency between the features and the labels to make correct\npredictions on left out data. `permutation_test_score` generates a null\ndistribution by calculating `n_permutations` different permutations of the\ndata. In each permutation the labels are randomly shuffled, thereby removing\nany dependency between the features and the labels. The p-value output is the\nfraction of permutations for which the average cross-validation score obtained\nby the model is better than the cross-validation score obtained by the model\nusing the original data. For reliable results `n_permutations` should\ntypically be larger than 100 and `cv` between 3-10 folds.\n\nA low p-value provides evidence that the dataset contains real dependency\nbetween features and labels and the classifier was able to utilize this to\nobtain good results. A high p-value could be due to a lack of dependency\nbetween features and labels (there is no difference in feature values between\nthe classes) or because the classifier was not able to use the dependency in\nthe data. In the latter case, using a more appropriate classifier that is able\nto utilize the structure in the data, would result in a low p-value.\n\nCross-validation provides information about how well a classifier generalizes,\nspecifically the range of expected errors of the classifier. However, a\nclassifier trained on a high dimensional dataset with no structure may still\nperform better than expected on cross-validation, just by chance. This can\ntypically happen with small datasets with less than a few hundred samples.\n`permutation_test_score` provides information on whether the classifier has\nfound a real class structure and can help in evaluating the performance of the\nclassifier.\n\nIt is important to note that this test has been shown to produce low p-values\neven if there is only weak structure in the data because in the corresponding\npermutated datasets there is absolutely no structure. This test is therefore\nonly able to show when the model reliably outperforms random guessing.\n\nFinally, `permutation_test_score` is computed using brute force and interally\nfits `(n_permutations + 1) * n_cv` models. It is therefore only tractable with\nsmall datasets for which fitting an individual model is very fast.\n\nExamples\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "3.2. Tuning the hyper-parameters of an estimator", "path": "modules/grid_search", "type": "Guide", "text": "\nHyper-parameters are parameters that are not directly learnt within\nestimators. In scikit-learn they are passed as arguments to the constructor of\nthe estimator classes. Typical examples include `C`, `kernel` and `gamma` for\nSupport Vector Classifier, `alpha` for Lasso, etc.\n\nIt is possible and recommended to search the hyper-parameter space for the\nbest cross validation score.\n\nAny parameter provided when constructing an estimator may be optimized in this\nmanner. Specifically, to find the names and current values for all parameters\nfor a given estimator, use:\n\nA search consists of:\n\nTwo generic approaches to parameter search are provided in scikit-learn: for\ngiven values, `GridSearchCV` exhaustively considers all parameter\ncombinations, while `RandomizedSearchCV` can sample a given number of\ncandidates from a parameter space with a specified distribution. Both these\ntools have successive halving counterparts `HalvingGridSearchCV` and\n`HalvingRandomSearchCV`, which can be much faster at finding a good parameter\ncombination.\n\nAfter describing these tools we detail best practices applicable to these\napproaches. Some models allow for specialized, efficient parameter search\nstrategies, outlined in Alternatives to brute force parameter search.\n\nNote that it is common that a small subset of those parameters can have a\nlarge impact on the predictive or computation performance of the model while\nothers can be left to their default values. It is recommended to read the\ndocstring of the estimator class to get a finer understanding of their\nexpected behavior, possibly by reading the enclosed reference to the\nliterature.\n\nThe grid search provided by `GridSearchCV` exhaustively generates candidates\nfrom a grid of parameter values specified with the `param_grid` parameter. For\ninstance, the following `param_grid`:\n\nspecifies that two grids should be explored: one with a linear kernel and C\nvalues in [1, 10, 100, 1000], and the second one with an RBF kernel, and the\ncross-product of C values ranging in [1, 10, 100, 1000] and gamma values in\n[0.001, 0.0001].\n\nThe `GridSearchCV` instance implements the usual estimator API: when \u201cfitting\u201d\nit on a dataset all the possible combinations of parameter values are\nevaluated and the best combination is retained.\n\nExamples:\n\nWhile using a grid of parameter settings is currently the most widely used\nmethod for parameter optimization, other search methods have more favourable\nproperties. `RandomizedSearchCV` implements a randomized search over\nparameters, where each setting is sampled from a distribution over possible\nparameter values. This has two main benefits over an exhaustive search:\n\nSpecifying how parameters should be sampled is done using a dictionary, very\nsimilar to specifying parameters for `GridSearchCV`. Additionally, a\ncomputation budget, being the number of sampled candidates or sampling\niterations, is specified using the `n_iter` parameter. For each parameter,\neither a distribution over possible values or a list of discrete choices\n(which will be sampled uniformly) can be specified:\n\nThis example uses the `scipy.stats` module, which contains many useful\ndistributions for sampling parameters, such as `expon`, `gamma`, `uniform` or\n`randint`.\n\nIn principle, any function can be passed that provides a `rvs` (random variate\nsample) method to sample a value. A call to the `rvs` function should provide\nindependent random samples from possible parameter values on consecutive\ncalls.\n\nWarning\n\nThe distributions in `scipy.stats` prior to version scipy 0.16 do not allow\nspecifying a random state. Instead, they use the global numpy random state,\nthat can be seeded via `np.random.seed` or set using `np.random.set_state`.\nHowever, beginning scikit-learn 0.18, the `sklearn.model_selection` module\nsets the random state provided by the user if scipy >= 0.16 is also available.\n\nFor continuous parameters, such as `C` above, it is important to specify a\ncontinuous distribution to take full advantage of the randomization. This way,\nincreasing `n_iter` will always lead to a finer search.\n\nA continuous log-uniform random variable is available through `loguniform`.\nThis is a continuous version of log-spaced parameters. For example to specify\n`C` above, `loguniform(1, 100)` can be used instead of `[1, 10, 100]` or\n`np.logspace(0, 2, num=1000)`. This is an alias to SciPy\u2019s stats.reciprocal.\n\nMirroring the example above in grid search, we can specify a continuous random\nvariable that is log-uniformly distributed between `1e0` and `1e3`:\n\nExamples:\n\nReferences:\n\nScikit-learn also provides the `HalvingGridSearchCV` and\n`HalvingRandomSearchCV` estimators that can be used to search a parameter\nspace using successive halving 1 2. Successive halving (SH) is like a\ntournament among candidate parameter combinations. SH is an iterative\nselection process where all candidates (the parameter combinations) are\nevaluated with a small amount of resources at the first iteration. Only some\nof these candidates are selected for the next iteration, which will be\nallocated more resources. For parameter tuning, the resource is typically the\nnumber of training samples, but it can also be an arbitrary numeric parameter\nsuch as `n_estimators` in a random forest.\n\nAs illustrated in the figure below, only a subset of candidates \u2018survive\u2019\nuntil the last iteration. These are the candidates that have consistently\nranked among the top-scoring candidates across all iterations. Each iteration\nis allocated an increasing amount of resources per candidate, here the number\nof samples.\n\nWe here briefly describe the main parameters, but each parameter and their\ninteractions are described in more details in the sections below. The `factor`\n(> 1) parameter controls the rate at which the resources grow, and the rate at\nwhich the number of candidates decreases. In each iteration, the number of\nresources per candidate is multiplied by `factor` and the number of candidates\nis divided by the same factor. Along with `resource` and `min_resources`,\n`factor` is the most important parameter to control the search in our\nimplementation, though a value of 3 usually works well. `factor` effectively\ncontrols the number of iterations in `HalvingGridSearchCV` and the number of\ncandidates (by default) and iterations in `HalvingRandomSearchCV`.\n`aggressive_elimination=True` can also be used if the number of available\nresources is small. More control is available through tuning the\n`min_resources` parameter.\n\nThese estimators are still experimental: their predictions and their API might\nchange without any deprecation cycle. To use them, you need to explicitly\nimport `enable_halving_search_cv`:\n\nExamples:\n\nBeside `factor`, the two main parameters that influence the behaviour of a\nsuccessive halving search are the `min_resources` parameter, and the number of\ncandidates (or parameter combinations) that are evaluated. `min_resources` is\nthe amount of resources allocated at the first iteration for each candidate.\nThe number of candidates is specified directly in `HalvingRandomSearchCV`, and\nis determined from the `param_grid` parameter of `HalvingGridSearchCV`.\n\nConsider a case where the resource is the number of samples, and where we have\n1000 samples. In theory, with `min_resources=10` and `factor=2`, we are able\nto run at most 7 iterations with the following number of samples: `[10, 20,\n40, 80, 160, 320, 640]`.\n\nBut depending on the number of candidates, we might run less than 7\niterations: if we start with a small number of candidates, the last iteration\nmight use less than 640 samples, which means not using all the available\nresources (samples). For example if we start with 5 candidates, we only need 2\niterations: 5 candidates for the first iteration, then `5 // 2 = 2` candidates\nat the second iteration, after which we know which candidate performs the best\n(so we don\u2019t need a third one). We would only be using at most 20 samples\nwhich is a waste since we have 1000 samples at our disposal. On the other\nhand, if we start with a high number of candidates, we might end up with a lot\nof candidates at the last iteration, which may not always be ideal: it means\nthat many candidates will run with the full resources, basically reducing the\nprocedure to standard search.\n\nIn the case of `HalvingRandomSearchCV`, the number of candidates is set by\ndefault such that the last iteration uses as much of the available resources\nas possible. For `HalvingGridSearchCV`, the number of candidates is determined\nby the `param_grid` parameter. Changing the value of `min_resources` will\nimpact the number of possible iterations, and as a result will also have an\neffect on the ideal number of candidates.\n\nAnother consideration when choosing `min_resources` is whether or not it is\neasy to discriminate between good and bad candidates with a small amount of\nresources. For example, if you need a lot of samples to distinguish between\ngood and bad parameters, a high `min_resources` is recommended. On the other\nhand if the distinction is clear even with a small amount of samples, then a\nsmall `min_resources` may be preferable since it would speed up the\ncomputation.\n\nNotice in the example above that the last iteration does not use the maximum\namount of resources available: 1000 samples are available, yet only 640 are\nused, at most. By default, both `HalvingRandomSearchCV` and\n`HalvingGridSearchCV` try to use as many resources as possible in the last\niteration, with the constraint that this amount of resources must be a\nmultiple of both `min_resources` and `factor` (this constraint will be clear\nin the next section). `HalvingRandomSearchCV` achieves this by sampling the\nright amount of candidates, while `HalvingGridSearchCV` achieves this by\nproperly setting `min_resources`. Please see Exhausting the available\nresources for details.\n\nAt any iteration `i`, each candidate is allocated a given amount of resources\nwhich we denote `n_resources_i`. This quantity is controlled by the parameters\n`factor` and `min_resources` as follows (`factor` is strictly greater than 1):\n\nor equivalently:\n\nwhere `min_resources == n_resources_0` is the amount of resources used at the\nfirst iteration. `factor` also defines the proportions of candidates that will\nbe selected for the next iteration:\n\nor equivalently:\n\nSo in the first iteration, we use `min_resources` resources `n_candidates`\ntimes. In the second iteration, we use `min_resources * factor` resources\n`n_candidates // factor` times. The third again multiplies the resources per\ncandidate and divides the number of candidates. This process stops when the\nmaximum amount of resource per candidate is reached, or when we have\nidentified the best candidate. The best candidate is identified at the\niteration that is evaluating `factor` or less candidates (see just below for\nan explanation).\n\nHere is an example with `min_resources=3` and `factor=2`, starting with 70\ncandidates:\n\n`n_resources_i`\n\n`n_candidates_i`\n\n3 (=min_resources)\n\n70 (=n_candidates)\n\n3 * 2 = 6\n\n70 // 2 = 35\n\n6 * 2 = 12\n\n35 // 2 = 17\n\n12 * 2 = 24\n\n17 // 2 = 8\n\n24 * 2 = 48\n\n8 // 2 = 4\n\n48 * 2 = 96\n\n4 // 2 = 2\n\nWe can note that:\n\nThe amount of resources that is used at each iteration can be found in the\n`n_resources_` attribute.\n\nBy default, the resource is defined in terms of number of samples. That is,\neach iteration will use an increasing amount of samples to train on. You can\nhowever manually specify a parameter to use as the resource with the\n`resource` parameter. Here is an example where the resource is defined in\nterms of the number of estimators of a random forest:\n\nNote that it is not possible to budget on a parameter that is part of the\nparameter grid.\n\nAs mentioned above, the number of resources that is used at each iteration\ndepends on the `min_resources` parameter. If you have a lot of resources\navailable but start with a low number of resources, some of them might be\nwasted (i.e. not used):\n\nThe search process will only use 80 resources at most, while our maximum\namount of available resources is `n_samples=1000`. Here, we have\n`min_resources = r_0 = 20`.\n\nFor `HalvingGridSearchCV`, by default, the `min_resources` parameter is set to\n\u2018exhaust\u2019. This means that `min_resources` is automatically set such that the\nlast iteration can use as many resources as possible, within the\n`max_resources` limit:\n\n`min_resources` was here automatically set to 250, which results in the last\niteration using all the resources. The exact value that is used depends on the\nnumber of candidate parameter, on `max_resources` and on `factor`.\n\nFor `HalvingRandomSearchCV`, exhausting the resources can be done in 2 ways:\n\nBoth options are mutally exclusive: using `min_resources='exhaust'` requires\nknowing the number of candidates, and symmetrically `n_candidates='exhaust'`\nrequires knowing `min_resources`.\n\nIn general, exhausting the total number of resources leads to a better final\ncandidate parameter, and is slightly more time-intensive.\n\nIdeally, we want the last iteration to evaluate `factor` candidates (see\nAmount of resource and number of candidates at each iteration). We then just\nhave to pick the best one. When the number of available resources is small\nwith respect to the number of candidates, the last iteration may have to\nevaluate more than `factor` candidates:\n\nSince we cannot use more than `max_resources=40` resources, the process has to\nstop at the second iteration which evaluates more than `factor=2` candidates.\n\nUsing the `aggressive_elimination` parameter, you can force the search process\nto end up with less than `factor` candidates at the last iteration. To do\nthis, the process will eliminate as many candidates as necessary using\n`min_resources` resources:\n\nNotice that we end with 2 candidates at the last iteration since we have\neliminated enough candidates during the first iterations, using `n_resources =\nmin_resources = 20`.\n\nThe `cv_results_` attribute contains useful information for analysing the\nresults of a search. It can be converted to a pandas dataframe with `df =\npd.DataFrame(est.cv_results_)`. The `cv_results_` attribute of\n`HalvingGridSearchCV` and `HalvingRandomSearchCV` is similar to that of\n`GridSearchCV` and `RandomizedSearchCV`, with additional information related\nto the successive halving process.\n\nHere is an example with some of the columns of a (truncated) dataframe:\n\niter\n\nn_resources\n\nmean_test_score\n\nparams\n\n0\n\n0\n\n125\n\n0.983667\n\n{\u2018criterion\u2019: \u2018entropy\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 9,\n\u2018min_samples_split\u2019: 5}\n\n1\n\n0\n\n125\n\n0.983667\n\n{\u2018criterion\u2019: \u2018gini\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 8,\n\u2018min_samples_split\u2019: 7}\n\n2\n\n0\n\n125\n\n0.983667\n\n{\u2018criterion\u2019: \u2018gini\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 10,\n\u2018min_samples_split\u2019: 10}\n\n3\n\n0\n\n125\n\n0.983667\n\n{\u2018criterion\u2019: \u2018entropy\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 6,\n\u2018min_samples_split\u2019: 6}\n\n\u2026\n\n\u2026\n\n\u2026\n\n\u2026\n\n\u2026\n\n15\n\n2\n\n500\n\n0.951958\n\n{\u2018criterion\u2019: \u2018entropy\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 9,\n\u2018min_samples_split\u2019: 10}\n\n16\n\n2\n\n500\n\n0.947958\n\n{\u2018criterion\u2019: \u2018gini\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 10,\n\u2018min_samples_split\u2019: 10}\n\n17\n\n2\n\n500\n\n0.951958\n\n{\u2018criterion\u2019: \u2018gini\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 10,\n\u2018min_samples_split\u2019: 4}\n\n18\n\n3\n\n1000\n\n0.961009\n\n{\u2018criterion\u2019: \u2018entropy\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 9,\n\u2018min_samples_split\u2019: 10}\n\n19\n\n3\n\n1000\n\n0.955989\n\n{\u2018criterion\u2019: \u2018gini\u2019, \u2018max_depth\u2019: None, \u2018max_features\u2019: 10,\n\u2018min_samples_split\u2019: 4}\n\nEach row corresponds to a given parameter combination (a candidate) and a\ngiven iteration. The iteration is given by the `iter` column. The\n`n_resources` column tells you how many resources were used.\n\nIn the example above, the best parameter combination is `{'criterion':\n'entropy', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}`\nsince it has reached the last iteration (3) with the highest score: 0.96.\n\nReferences:\n\nK. Jamieson, A. Talwalkar, Non-stochastic Best Arm Identification and\nHyperparameter Optimization, in proc. of Machine Learning Research, 2016.\n\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar, Hyperband: A\nNovel Bandit-Based Approach to Hyperparameter Optimization, in Machine\nLearning Research 18, 2018.\n\nBy default, parameter search uses the `score` function of the estimator to\nevaluate a parameter setting. These are the `sklearn.metrics.accuracy_score`\nfor classification and `sklearn.metrics.r2_score` for regression. For some\napplications, other scoring functions are better suited (for example in\nunbalanced classification, the accuracy score is often uninformative). An\nalternative scoring function can be specified via the `scoring` parameter of\nmost parameter search tools. See The scoring parameter: defining model\nevaluation rules for more details.\n\n`GridSearchCV` and `RandomizedSearchCV` allow specifying multiple metrics for\nthe `scoring` parameter.\n\nMultimetric scoring can either be specified as a list of strings of predefined\nscores names or a dict mapping the scorer name to the scorer function and/or\nthe predefined scorer name(s). See Using multiple metric evaluation for more\ndetails.\n\nWhen specifying multiple metrics, the `refit` parameter must be set to the\nmetric (string) for which the `best_params_` will be found and used to build\nthe `best_estimator_` on the whole dataset. If the search should not be refit,\nset `refit=False`. Leaving refit to the default value `None` will result in an\nerror when using multiple metrics.\n\nSee Demonstration of multi-metric evaluation on cross_val_score and\nGridSearchCV for an example usage.\n\n`HalvingRandomSearchCV` and `HalvingGridSearchCV` do not support multimetric\nscoring.\n\n`GridSearchCV` and `RandomizedSearchCV` allow searching over parameters of\ncomposite or nested estimators such as `Pipeline`, `ColumnTransformer`,\n`VotingClassifier` or `CalibratedClassifierCV` using a dedicated\n`<estimator>__<parameter>` syntax:\n\nHere, `<estimator>` is the parameter name of the nested estimator, in this\ncase `base_estimator`. If the meta-estimator is constructed as a collection of\nestimators as in `pipeline.Pipeline`, then `<estimator>` refers to the name of\nthe estimator, see Nested parameters. In practice, there can be several levels\nof nesting:\n\nPlease refer to Pipeline: chaining estimators for performing parameter\nsearches over pipelines.\n\nModel selection by evaluating various parameter settings can be seen as a way\nto use the labeled data to \u201ctrain\u201d the parameters of the grid.\n\nWhen evaluating the resulting model it is important to do it on held-out\nsamples that were not seen during the grid search process: it is recommended\nto split the data into a development set (to be fed to the `GridSearchCV`\ninstance) and an evaluation set to compute performance metrics.\n\nThis can be done by using the `train_test_split` utility function.\n\nThe parameter search tools evaluate each parameter combination on each data\nfold independently. Computations can be run in parallel by using the keyword\n`n_jobs=-1`. See function signature for more details, and also the Glossary\nentry for n_jobs.\n\nSome parameter settings may result in a failure to `fit` one or more folds of\nthe data. By default, this will cause the entire search to fail, even if some\nparameter settings could be fully evaluated. Setting `error_score=0` (or\n`=np.NaN`) will make the procedure robust to such failure, issuing a warning\nand setting the score for that fold to 0 (or `NaN`), but completing the\nsearch.\n\nSome models can fit data for a range of values of some parameter almost as\nefficiently as fitting the estimator for a single value of the parameter. This\nfeature can be leveraged to perform a more efficient cross-validation used for\nmodel selection of this parameter.\n\nThe most common parameter amenable to this strategy is the parameter encoding\nthe strength of the regularizer. In this case we say that we compute the\nregularization path of the estimator.\n\nHere is the list of such models:\n\n`linear_model.ElasticNetCV`(*[, l1_ratio, \u2026])\n\nElastic Net model with iterative fitting along a regularization path.\n\n`linear_model.LarsCV`(*[, fit_intercept, \u2026])\n\nCross-validated Least Angle Regression model.\n\n`linear_model.LassoCV`(*[, eps, n_alphas, \u2026])\n\nLasso linear model with iterative fitting along a regularization path.\n\n`linear_model.LassoLarsCV`(*[, fit_intercept, \u2026])\n\nCross-validated Lasso, using the LARS algorithm.\n\n`linear_model.LogisticRegressionCV`(*[, Cs, \u2026])\n\nLogistic Regression CV (aka logit, MaxEnt) classifier.\n\n`linear_model.MultiTaskElasticNetCV`(*[, \u2026])\n\nMulti-task L1/L2 ElasticNet with built-in cross-validation.\n\n`linear_model.MultiTaskLassoCV`(*[, eps, \u2026])\n\nMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\n`linear_model.OrthogonalMatchingPursuitCV`(*)\n\nCross-validated Orthogonal Matching Pursuit model (OMP).\n\n`linear_model.RidgeCV`([alphas, \u2026])\n\nRidge regression with built-in cross-validation.\n\n`linear_model.RidgeClassifierCV`([alphas, \u2026])\n\nRidge classifier with built-in cross-validation.\n\nSome models can offer an information-theoretic closed-form formula of the\noptimal estimate of the regularization parameter by computing a single\nregularization path (instead of several when using cross-validation).\n\nHere is the list of models benefiting from the Akaike Information Criterion\n(AIC) or the Bayesian Information Criterion (BIC) for automated model\nselection:\n\n`linear_model.LassoLarsIC`([criterion, \u2026])\n\nLasso model fit with Lars using BIC or AIC for model selection\n\nWhen using ensemble methods base upon bagging, i.e. generating new training\nsets using sampling with replacement, part of the training set remains unused.\nFor each classifier in the ensemble, a different part of the training set is\nleft out.\n\nThis left out portion can be used to estimate the generalization error without\nhaving to rely on a separate validation set. This estimate comes \u201cfor free\u201d as\nno additional data is needed and can be used for model selection.\n\nThis is currently implemented in the following classes:\n\n`ensemble.RandomForestClassifier`([\u2026])\n\nA random forest classifier.\n\n`ensemble.RandomForestRegressor`([\u2026])\n\nA random forest regressor.\n\n`ensemble.ExtraTreesClassifier`([\u2026])\n\nAn extra-trees classifier.\n\n`ensemble.ExtraTreesRegressor`([n_estimators, \u2026])\n\nAn extra-trees regressor.\n\n`ensemble.GradientBoostingClassifier`(*[, \u2026])\n\nGradient Boosting for classification.\n\n`ensemble.GradientBoostingRegressor`(*[, \u2026])\n\nGradient Boosting for regression.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "3.3. Metrics and scoring", "path": "modules/model_evaluation", "type": "Guide", "text": "\nThere are 3 different APIs for evaluating the quality of a model\u2019s\npredictions:\n\nFinally, Dummy estimators are useful to get a baseline value of those metrics\nfor random predictions.\n\nSee also\n\nFor \u201cpairwise\u201d metrics, between samples and not estimators or predictions, see\nthe Pairwise metrics, Affinities and Kernels section.\n\nModel selection and evaluation using tools, such as\n`model_selection.GridSearchCV` and `model_selection.cross_val_score`, take a\n`scoring` parameter that controls what metric they apply to the estimators\nevaluated.\n\nFor the most common use cases, you can designate a scorer object with the\n`scoring` parameter; the table below shows all possible values. All scorer\nobjects follow the convention that higher return values are better than lower\nreturn values. Thus metrics which measure the distance between the model and\nthe data, like `metrics.mean_squared_error`, are available as\nneg_mean_squared_error which return the negated value of the metric.\n\nScoring\n\nFunction\n\nComment\n\nClassification\n\n\u2018accuracy\u2019\n\n`metrics.accuracy_score`\n\n\u2018balanced_accuracy\u2019\n\n`metrics.balanced_accuracy_score`\n\n\u2018top_k_accuracy\u2019\n\n`metrics.top_k_accuracy_score`\n\n\u2018average_precision\u2019\n\n`metrics.average_precision_score`\n\n\u2018neg_brier_score\u2019\n\n`metrics.brier_score_loss`\n\n\u2018f1\u2019\n\n`metrics.f1_score`\n\nfor binary targets\n\n\u2018f1_micro\u2019\n\n`metrics.f1_score`\n\nmicro-averaged\n\n\u2018f1_macro\u2019\n\n`metrics.f1_score`\n\nmacro-averaged\n\n\u2018f1_weighted\u2019\n\n`metrics.f1_score`\n\nweighted average\n\n\u2018f1_samples\u2019\n\n`metrics.f1_score`\n\nby multilabel sample\n\n\u2018neg_log_loss\u2019\n\n`metrics.log_loss`\n\nrequires `predict_proba` support\n\n\u2018precision\u2019 etc.\n\n`metrics.precision_score`\n\nsuffixes apply as with \u2018f1\u2019\n\n\u2018recall\u2019 etc.\n\n`metrics.recall_score`\n\nsuffixes apply as with \u2018f1\u2019\n\n\u2018jaccard\u2019 etc.\n\n`metrics.jaccard_score`\n\nsuffixes apply as with \u2018f1\u2019\n\n\u2018roc_auc\u2019\n\n`metrics.roc_auc_score`\n\n\u2018roc_auc_ovr\u2019\n\n`metrics.roc_auc_score`\n\n\u2018roc_auc_ovo\u2019\n\n`metrics.roc_auc_score`\n\n\u2018roc_auc_ovr_weighted\u2019\n\n`metrics.roc_auc_score`\n\n\u2018roc_auc_ovo_weighted\u2019\n\n`metrics.roc_auc_score`\n\nClustering\n\n\u2018adjusted_mutual_info_score\u2019\n\n`metrics.adjusted_mutual_info_score`\n\n\u2018adjusted_rand_score\u2019\n\n`metrics.adjusted_rand_score`\n\n\u2018completeness_score\u2019\n\n`metrics.completeness_score`\n\n\u2018fowlkes_mallows_score\u2019\n\n`metrics.fowlkes_mallows_score`\n\n\u2018homogeneity_score\u2019\n\n`metrics.homogeneity_score`\n\n\u2018mutual_info_score\u2019\n\n`metrics.mutual_info_score`\n\n\u2018normalized_mutual_info_score\u2019\n\n`metrics.normalized_mutual_info_score`\n\n\u2018rand_score\u2019\n\n`metrics.rand_score`\n\n\u2018v_measure_score\u2019\n\n`metrics.v_measure_score`\n\nRegression\n\n\u2018explained_variance\u2019\n\n`metrics.explained_variance_score`\n\n\u2018max_error\u2019\n\n`metrics.max_error`\n\n\u2018neg_mean_absolute_error\u2019\n\n`metrics.mean_absolute_error`\n\n\u2018neg_mean_squared_error\u2019\n\n`metrics.mean_squared_error`\n\n\u2018neg_root_mean_squared_error\u2019\n\n`metrics.mean_squared_error`\n\n\u2018neg_mean_squared_log_error\u2019\n\n`metrics.mean_squared_log_error`\n\n\u2018neg_median_absolute_error\u2019\n\n`metrics.median_absolute_error`\n\n\u2018r2\u2019\n\n`metrics.r2_score`\n\n\u2018neg_mean_poisson_deviance\u2019\n\n`metrics.mean_poisson_deviance`\n\n\u2018neg_mean_gamma_deviance\u2019\n\n`metrics.mean_gamma_deviance`\n\n\u2018neg_mean_absolute_percentage_error\u2019\n\n`metrics.mean_absolute_percentage_error`\n\nUsage examples:\n\nNote\n\nThe values listed by the `ValueError` exception correspond to the functions\nmeasuring prediction accuracy described in the following sections. The scorer\nobjects for those functions are stored in the dictionary\n`sklearn.metrics.SCORERS`.\n\nThe module `sklearn.metrics` also exposes a set of simple functions measuring\na prediction error given ground truth and prediction:\n\nMetrics available for various machine learning tasks are detailed in sections\nbelow.\n\nMany metrics are not given names to be used as `scoring` values, sometimes\nbecause they require additional parameters, such as `fbeta_score`. In such\ncases, you need to generate an appropriate scoring object. The simplest way to\ngenerate a callable object for scoring is by using `make_scorer`. That\nfunction converts metrics into callables that can be used for model\nevaluation.\n\nOne typical use case is to wrap an existing metric function from the library\nwith non-default values for its parameters, such as the `beta` parameter for\nthe `fbeta_score` function:\n\nThe second use case is to build a completely custom scorer object from a\nsimple python function using `make_scorer`, which can take several parameters:\n\nHere is an example of building custom scorers, and of using the\n`greater_is_better` parameter:\n\nYou can generate even more flexible model scorers by constructing your own\nscoring object from scratch, without using the `make_scorer` factory. For a\ncallable to be a scorer, it needs to meet the protocol specified by the\nfollowing two rules:\n\nNote\n\nUsing custom scorers in functions where n_jobs > 1\n\nWhile defining the custom scoring function alongside the calling function\nshould work out of the box with the default joblib backend (loky), importing\nit from another module will be a more robust approach and work independently\nof the joblib backend.\n\nFor example, to use `n_jobs` greater than 1 in the example below,\n`custom_scoring_function` function is saved in a user-created module\n(`custom_scorer_module.py`) and imported:\n\nScikit-learn also permits evaluation of multiple metrics in `GridSearchCV`,\n`RandomizedSearchCV` and `cross_validate`.\n\nThere are three ways to specify multiple scoring metrics for the `scoring`\nparameter:\n\nNote that the dict values can either be scorer functions or one of the\npredefined metric strings.\n\nAs a callable that returns a dictionary of scores:\n\nThe `sklearn.metrics` module implements several loss, score, and utility\nfunctions to measure classification performance. Some metrics might require\nprobability estimates of the positive class, confidence values, or binary\ndecisions values. Most implementations allow each sample to provide a weighted\ncontribution to the overall score, through the `sample_weight` parameter.\n\nSome of these are restricted to the binary classification case:\n\n`precision_recall_curve`(y_true, probas_pred, *)\n\nCompute precision-recall pairs for different probability thresholds.\n\n`roc_curve`(y_true, y_score, *[, pos_label, \u2026])\n\nCompute Receiver operating characteristic (ROC).\n\n`det_curve`(y_true, y_score[, pos_label, \u2026])\n\nCompute error rates for different probability thresholds.\n\nOthers also work in the multiclass case:\n\n`balanced_accuracy_score`(y_true, y_pred, *[, \u2026])\n\nCompute the balanced accuracy.\n\n`cohen_kappa_score`(y1, y2, *[, labels, \u2026])\n\nCohen\u2019s kappa: a statistic that measures inter-annotator agreement.\n\n`confusion_matrix`(y_true, y_pred, *[, \u2026])\n\nCompute confusion matrix to evaluate the accuracy of a classification.\n\n`hinge_loss`(y_true, pred_decision, *[, \u2026])\n\nAverage hinge loss (non-regularized).\n\n`matthews_corrcoef`(y_true, y_pred, *[, \u2026])\n\nCompute the Matthews correlation coefficient (MCC).\n\n`roc_auc_score`(y_true, y_score, *[, average, \u2026])\n\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from\nprediction scores.\n\n`top_k_accuracy_score`(y_true, y_score, *[, \u2026])\n\nTop-k Accuracy classification score.\n\nSome also work in the multilabel case:\n\n`accuracy_score`(y_true, y_pred, *[, \u2026])\n\nAccuracy classification score.\n\n`classification_report`(y_true, y_pred, *[, \u2026])\n\nBuild a text report showing the main classification metrics.\n\n`f1_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the F1 score, also known as balanced F-score or F-measure.\n\n`fbeta_score`(y_true, y_pred, *, beta[, \u2026])\n\nCompute the F-beta score.\n\n`hamming_loss`(y_true, y_pred, *[, sample_weight])\n\nCompute the average Hamming loss.\n\n`jaccard_score`(y_true, y_pred, *[, labels, \u2026])\n\nJaccard similarity coefficient score.\n\n`log_loss`(y_true, y_pred, *[, eps, \u2026])\n\nLog loss, aka logistic loss or cross-entropy loss.\n\n`multilabel_confusion_matrix`(y_true, y_pred, *)\n\nCompute a confusion matrix for each class or sample.\n\n`precision_recall_fscore_support`(y_true, \u2026)\n\nCompute precision, recall, F-measure and support for each class.\n\n`precision_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the precision.\n\n`recall_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the recall.\n\n`roc_auc_score`(y_true, y_score, *[, average, \u2026])\n\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from\nprediction scores.\n\n`zero_one_loss`(y_true, y_pred, *[, \u2026])\n\nZero-one classification loss.\n\nAnd some work with binary and multilabel (but not multiclass) problems:\n\n`average_precision_score`(y_true, y_score, *)\n\nCompute average precision (AP) from prediction scores.\n\nIn the following sub-sections, we will describe each of those functions,\npreceded by some notes on common API and metric definition.\n\nSome metrics are essentially defined for binary classification tasks (e.g.\n`f1_score`, `roc_auc_score`). In these cases, by default only the positive\nlabel is evaluated, assuming by default that the positive class is labelled\n`1` (though this may be configurable through the `pos_label` parameter).\n\nIn extending a binary metric to multiclass or multilabel problems, the data is\ntreated as a collection of binary problems, one for each class. There are then\na number of ways to average binary metric calculations across the set of\nclasses, each of which may be useful in some scenario. Where available, you\nshould select among these using the `average` parameter.\n\nWhile multiclass data is provided to the metric, like binary targets, as an\narray of class labels, multilabel data is specified as an indicator matrix, in\nwhich cell `[i, j]` has value 1 if sample `i` has label `j` and value 0\notherwise.\n\nThe `accuracy_score` function computes the accuracy, either the fraction\n(default) or the count (normalize=False) of correct predictions.\n\nIn multilabel classification, the function returns the subset accuracy. If the\nentire set of predicted labels for a sample strictly match with the true set\nof labels, then the subset accuracy is 1.0; otherwise it is 0.0.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample and\n\\\\(y_i\\\\) is the corresponding true value, then the fraction of correct\npredictions over \\\\(n_\\text{samples}\\\\) is defined as\n\nwhere \\\\(1(x)\\\\) is the indicator function.\n\nIn the multilabel case with binary label indicators:\n\nExample:\n\nThe `top_k_accuracy_score` function is a generalization of `accuracy_score`.\nThe difference is that a prediction is considered correct as long as the true\nlabel is associated with one of the `k` highest predicted scores.\n`accuracy_score` is the special case of `k = 1`.\n\nThe function covers the binary and multiclass classification cases but not the\nmultilabel case.\n\nIf \\\\(\\hat{f}_{i,j}\\\\) is the predicted class for the \\\\(i\\\\)-th sample\ncorresponding to the \\\\(j\\\\)-th largest predicted score and \\\\(y_i\\\\) is the\ncorresponding true value, then the fraction of correct predictions over\n\\\\(n_\\text{samples}\\\\) is defined as\n\nwhere \\\\(k\\\\) is the number of guesses allowed and \\\\(1(x)\\\\) is the indicator\nfunction.\n\nThe `balanced_accuracy_score` function computes the balanced accuracy, which\navoids inflated performance estimates on imbalanced datasets. It is the macro-\naverage of recall scores per class or, equivalently, raw accuracy where each\nsample is weighted according to the inverse prevalence of its true class. Thus\nfor balanced datasets, the score is equal to accuracy.\n\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\nsensitivity (true positive rate) and specificity (true negative rate), or the\narea under the ROC curve with binary predictions rather than scores:\n\nIf the classifier performs equally well on either class, this term reduces to\nthe conventional accuracy (i.e., the number of correct predictions divided by\nthe total number of predictions).\n\nIn contrast, if the conventional accuracy is above chance only because the\nclassifier takes advantage of an imbalanced test set, then the balanced\naccuracy, as appropriate, will drop to \\\\(\\frac{1}{n\\\\_classes}\\\\).\n\nThe score ranges from 0 to 1, or when `adjusted=True` is used, it rescaled to\nthe range \\\\(\\frac{1}{1 - n\\\\_classes}\\\\) to 1, inclusive, with performance at\nrandom scoring 0.\n\nIf \\\\(y_i\\\\) is the true value of the \\\\(i\\\\)-th sample, and \\\\(w_i\\\\) is the\ncorresponding sample weight, then we adjust the sample weight to:\n\nwhere \\\\(1(x)\\\\) is the indicator function. Given predicted \\\\(\\hat{y}_i\\\\)\nfor sample \\\\(i\\\\), balanced accuracy is defined as:\n\nWith `adjusted=True`, balanced accuracy reports the relative increase from\n\\\\(\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) = \\frac{1}{n\\\\_classes}\\\\). In\nthe binary case, this is also known as *Youden\u2019s J statistic*, or\ninformedness.\n\nNote\n\nThe multiclass definition here seems the most reasonable extension of the\nmetric used in binary classification, though there is no certain consensus in\nthe literature:\n\nReferences:\n\nI. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N.\nMaci\u00e0, B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, Design of the 2015\nChaLearn AutoML Challenge, IJCNN 2015.\n\nL. Mosley, A balanced approach to the multi-class imbalance problem, IJCV\n2010.\n\nJohn. D. Kelleher, Brian Mac Namee, Aoife D\u2019Arcy, Fundamentals of Machine\nLearning for Predictive Data Analytics: Algorithms, Worked Examples, and Case\nStudies, 2015.\n\nUrbanowicz R.J., Moore, J.H. ExSTraCS 2.0: description and evaluation of a\nscalable learning classifier system, Evol. Intel. (2015) 8: 89.\n\nThe function `cohen_kappa_score` computes Cohen\u2019s kappa statistic. This\nmeasure is intended to compare labelings by different human annotators, not a\nclassifier versus a ground truth.\n\nThe kappa score (see docstring) is a number between -1 and 1. Scores above .8\nare generally considered good agreement; zero or lower means no agreement\n(practically random labels).\n\nKappa scores can be computed for binary or multiclass problems, but not for\nmultilabel problems (except by manually computing a per-label score) and not\nfor more than two annotators.\n\nThe `confusion_matrix` function evaluates classification accuracy by computing\nthe confusion matrix with each row corresponding to the true class (Wikipedia\nand other references may use different convention for axes).\n\nBy definition, entry \\\\(i, j\\\\) in a confusion matrix is the number of\nobservations actually in group \\\\(i\\\\), but predicted to be in group \\\\(j\\\\).\nHere is an example:\n\n`plot_confusion_matrix` can be used to visually represent a confusion matrix\nas shown in the Confusion matrix example, which creates the following figure:\n\nThe parameter `normalize` allows to report ratios instead of counts. The\nconfusion matrix can be normalized in 3 different ways: `'pred'`, `'true'`,\nand `'all'` which will divide the counts by the sum of each columns, rows, or\nthe entire matrix, respectively.\n\nFor binary problems, we can get counts of true negatives, false positives,\nfalse negatives and true positives as follows:\n\nExample:\n\nThe `classification_report` function builds a text report showing the main\nclassification metrics. Here is a small example with custom `target_names` and\ninferred labels:\n\nExample:\n\nThe `hamming_loss` computes the average Hamming loss or Hamming distance\nbetween two sets of samples.\n\nIf \\\\(\\hat{y}_j\\\\) is the predicted value for the \\\\(j\\\\)-th label of a given\nsample, \\\\(y_j\\\\) is the corresponding true value, and \\\\(n_\\text{labels}\\\\)\nis the number of classes or labels, then the Hamming loss \\\\(L_{Hamming}\\\\)\nbetween two samples is defined as:\n\nwhere \\\\(1(x)\\\\) is the indicator function.\n\nIn the multilabel case with binary label indicators:\n\nNote\n\nIn multiclass classification, the Hamming loss corresponds to the Hamming\ndistance between `y_true` and `y_pred` which is similar to the Zero one loss\nfunction. However, while zero-one loss penalizes prediction sets that do not\nstrictly match true sets, the Hamming loss penalizes individual labels. Thus\nthe Hamming loss, upper bounded by the zero-one loss, is always between zero\nand one, inclusive; and predicting a proper subset or superset of the true\nlabels will give a Hamming loss between zero and one, exclusive.\n\nIntuitively, precision is the ability of the classifier not to label as\npositive a sample that is negative, and recall is the ability of the\nclassifier to find all the positive samples.\n\nThe F-measure (\\\\(F_\\beta\\\\) and \\\\(F_1\\\\) measures) can be interpreted as a\nweighted harmonic mean of the precision and recall. A \\\\(F_\\beta\\\\) measure\nreaches its best value at 1 and its worst score at 0. With \\\\(\\beta = 1\\\\),\n\\\\(F_\\beta\\\\) and \\\\(F_1\\\\) are equivalent, and the recall and the precision\nare equally important.\n\nThe `precision_recall_curve` computes a precision-recall curve from the ground\ntruth label and a score given by the classifier by varying a decision\nthreshold.\n\nThe `average_precision_score` function computes the average precision (AP)\nfrom prediction scores. The value is between 0 and 1 and higher is better. AP\nis defined as\n\nwhere \\\\(P_n\\\\) and \\\\(R_n\\\\) are the precision and recall at the nth\nthreshold. With random predictions, the AP is the fraction of positive\nsamples.\n\nReferences [Manning2008] and [Everingham2010] present alternative variants of\nAP that interpolate the precision-recall curve. Currently,\n`average_precision_score` does not implement any interpolated variant.\nReferences [Davis2006] and [Flach2015] describe why a linear interpolation of\npoints on the precision-recall curve provides an overly-optimistic measure of\nclassifier performance. This linear interpolation is used when computing area\nunder the curve with the trapezoidal rule in `auc`.\n\nSeveral functions allow you to analyze the precision, recall and F-measures\nscore:\n\n`average_precision_score`(y_true, y_score, *)\n\nCompute average precision (AP) from prediction scores.\n\n`f1_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the F1 score, also known as balanced F-score or F-measure.\n\n`fbeta_score`(y_true, y_pred, *, beta[, \u2026])\n\nCompute the F-beta score.\n\n`precision_recall_curve`(y_true, probas_pred, *)\n\nCompute precision-recall pairs for different probability thresholds.\n\n`precision_recall_fscore_support`(y_true, \u2026)\n\nCompute precision, recall, F-measure and support for each class.\n\n`precision_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the precision.\n\n`recall_score`(y_true, y_pred, *[, labels, \u2026])\n\nCompute the recall.\n\nNote that the `precision_recall_curve` function is restricted to the binary\ncase. The `average_precision_score` function works only in binary\nclassification and multilabel indicator format. The\n`plot_precision_recall_curve` function plots the precision recall as follows.\n\nExamples:\n\nReferences:\n\nC.D. Manning, P. Raghavan, H. Sch\u00fctze, Introduction to Information Retrieval,\n2008.\n\nM. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman, The Pascal\nVisual Object Classes (VOC) Challenge, IJCV 2010.\n\nJ. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC\nCurves, ICML 2006.\n\nP.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right,\nNIPS 2015.\n\nIn a binary classification task, the terms \u2018\u2019positive\u2019\u2019 and \u2018\u2019negative\u2019\u2019 refer\nto the classifier\u2019s prediction, and the terms \u2018\u2019true\u2019\u2019 and \u2018\u2019false\u2019\u2019 refer to\nwhether that prediction corresponds to the external judgment (sometimes known\nas the \u2018\u2019observation\u2019\u2019). Given these definitions, we can formulate the\nfollowing table:\n\nActual class (observation)\n\nPredicted class (expectation)\n\ntp (true positive) Correct result\n\nfp (false positive) Unexpected result\n\nfn (false negative) Missing result\n\ntn (true negative) Correct absence of result\n\nIn this context, we can define the notions of precision, recall and F-measure:\n\nHere are some small examples in binary classification:\n\nIn multiclass and multilabel classification task, the notions of precision,\nrecall, and F-measures can be applied to each label independently. There are a\nfew ways to combine results across labels, specified by the `average` argument\nto the `average_precision_score` (multilabel only), `f1_score`, `fbeta_score`,\n`precision_recall_fscore_support`, `precision_score` and `recall_score`\nfunctions, as described above. Note that if all labels are included,\n\u201cmicro\u201d-averaging in a multiclass setting will produce precision, recall and\n\\\\(F\\\\) that are all identical to accuracy. Also note that \u201cweighted\u201d\naveraging may produce an F-score that is not between precision and recall.\n\nTo make this more explicit, consider the following notation:\n\nThen the metrics are defined as:\n\n`average`\n\nPrecision\n\nRecall\n\nF_beta\n\n`\"micro\"`\n\n\\\\(P(y, \\hat{y})\\\\)\n\n\\\\(R(y, \\hat{y})\\\\)\n\n\\\\(F_\\beta(y, \\hat{y})\\\\)\n\n`\"samples\"`\n\n\\\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} P(y_s, \\hat{y}_s)\\\\)\n\n\\\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} R(y_s, \\hat{y}_s)\\\\)\n\n\\\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)\\\\)\n\n`\"macro\"`\n\n\\\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} P(y_l, \\hat{y}_l)\\\\)\n\n\\\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} R(y_l, \\hat{y}_l)\\\\)\n\n\\\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)\\\\)\n\n`\"weighted\"`\n\n\\\\(\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l \\in L}\n\\left|\\hat{y}_l\\right| P(y_l, \\hat{y}_l)\\\\)\n\n\\\\(\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l \\in L}\n\\left|\\hat{y}_l\\right| R(y_l, \\hat{y}_l)\\\\)\n\n\\\\(\\frac{1}{\\sum_{l \\in L} \\left|\\hat{y}_l\\right|} \\sum_{l \\in L}\n\\left|\\hat{y}_l\\right| F_\\beta(y_l, \\hat{y}_l)\\\\)\n\n`None`\n\n\\\\(\\langle P(y_l, \\hat{y}_l) | l \\in L \\rangle\\\\)\n\n\\\\(\\langle R(y_l, \\hat{y}_l) | l \\in L \\rangle\\\\)\n\n\\\\(\\langle F_\\beta(y_l, \\hat{y}_l) | l \\in L \\rangle\\\\)\n\nFor multiclass classification with a \u201cnegative class\u201d, it is possible to\nexclude some labels:\n\nSimilarly, labels not present in the data sample may be accounted for in\nmacro-averaging.\n\nThe `jaccard_score` function computes the average of Jaccard similarity\ncoefficients, also called the Jaccard index, between pairs of label sets.\n\nThe Jaccard similarity coefficient of the \\\\(i\\\\)-th samples, with a ground\ntruth label set \\\\(y_i\\\\) and predicted label set \\\\(\\hat{y}_i\\\\), is defined\nas\n\n`jaccard_score` works like `precision_recall_fscore_support` as a naively set-\nwise measure applying natively to binary targets, and extended to apply to\nmultilabel and multiclass through the use of `average` (see above).\n\nIn the binary case:\n\nIn the multilabel case with binary label indicators:\n\nMulticlass problems are binarized and treated like the corresponding\nmultilabel problem:\n\nThe `hinge_loss` function computes the average distance between the model and\nthe data using hinge loss, a one-sided metric that considers only prediction\nerrors. (Hinge loss is used in maximal margin classifiers such as support\nvector machines.)\n\nIf the labels are encoded with +1 and -1, \\\\(y\\\\): is the true value, and\n\\\\(w\\\\) is the predicted decisions as output by `decision_function`, then the\nhinge loss is defined as:\n\nIf there are more than two labels, `hinge_loss` uses a multiclass variant due\nto Crammer & Singer. Here is the paper describing it.\n\nIf \\\\(y_w\\\\) is the predicted decision for true label and \\\\(y_t\\\\) is the\nmaximum of the predicted decisions for all other labels, where predicted\ndecisions are output by decision function, then multiclass hinge loss is\ndefined by:\n\nHere a small example demonstrating the use of the `hinge_loss` function with a\nsvm classifier in a binary class problem:\n\nHere is an example demonstrating the use of the `hinge_loss` function with a\nsvm classifier in a multiclass problem:\n\nLog loss, also called logistic regression loss or cross-entropy loss, is\ndefined on probability estimates. It is commonly used in (multinomial)\nlogistic regression and neural networks, as well as in some variants of\nexpectation-maximization, and can be used to evaluate the probability outputs\n(`predict_proba`) of a classifier instead of its discrete predictions.\n\nFor binary classification with a true label \\\\(y \\in \\\\{0,1\\\\}\\\\) and a\nprobability estimate \\\\(p = \\operatorname{Pr}(y = 1)\\\\), the log loss per\nsample is the negative log-likelihood of the classifier given the true label:\n\nThis extends to the multiclass case as follows. Let the true labels for a set\nof samples be encoded as a 1-of-K binary indicator matrix \\\\(Y\\\\), i.e.,\n\\\\(y_{i,k} = 1\\\\) if sample \\\\(i\\\\) has label \\\\(k\\\\) taken from a set of\n\\\\(K\\\\) labels. Let \\\\(P\\\\) be a matrix of probability estimates, with\n\\\\(p_{i,k} = \\operatorname{Pr}(y_{i,k} = 1)\\\\). Then the log loss of the whole\nset is\n\nTo see how this generalizes the binary log loss given above, note that in the\nbinary case, \\\\(p_{i,0} = 1 - p_{i,1}\\\\) and \\\\(y_{i,0} = 1 - y_{i,1}\\\\), so\nexpanding the inner sum over \\\\(y_{i,k} \\in \\\\{0,1\\\\}\\\\) gives the binary log\nloss.\n\nThe `log_loss` function computes log loss given a list of ground-truth labels\nand a probability matrix, as returned by an estimator\u2019s `predict_proba`\nmethod.\n\nThe first `[.9, .1]` in `y_pred` denotes 90% probability that the first sample\nhas label 0. The log loss is non-negative.\n\nThe `matthews_corrcoef` function computes the Matthew\u2019s correlation\ncoefficient (MCC) for binary classes. Quoting Wikipedia:\n\n\u201cThe Matthews correlation coefficient is used in machine learning as a measure\nof the quality of binary (two-class) classifications. It takes into account\ntrue and false positives and negatives and is generally regarded as a balanced\nmeasure which can be used even if the classes are of very different sizes. The\nMCC is in essence a correlation coefficient value between -1 and +1. A\ncoefficient of +1 represents a perfect prediction, 0 an average random\nprediction and -1 an inverse prediction. The statistic is also known as the\nphi coefficient.\u201d\n\nIn the binary (two-class) case, \\\\(tp\\\\), \\\\(tn\\\\), \\\\(fp\\\\) and \\\\(fn\\\\) are\nrespectively the number of true positives, true negatives, false positives and\nfalse negatives, the MCC is defined as\n\nIn the multiclass case, the Matthews correlation coefficient can be defined in\nterms of a `confusion_matrix` \\\\(C\\\\) for \\\\(K\\\\) classes. To simplify the\ndefinition consider the following intermediate variables:\n\nThen the multiclass MCC is defined as:\n\nWhen there are more than two labels, the value of the MCC will no longer range\nbetween -1 and +1. Instead the minimum value will be somewhere between -1 and\n0 depending on the number and distribution of ground true labels. The maximum\nvalue is always +1.\n\nHere is a small example illustrating the usage of the `matthews_corrcoef`\nfunction:\n\nThe `multilabel_confusion_matrix` function computes class-wise (default) or\nsample-wise (samplewise=True) multilabel confusion matrix to evaluate the\naccuracy of a classification. multilabel_confusion_matrix also treats\nmulticlass data as if it were multilabel, as this is a transformation commonly\napplied to evaluate multiclass problems with binary classification metrics\n(such as precision, recall, etc.).\n\nWhen calculating class-wise multilabel confusion matrix \\\\(C\\\\), the count of\ntrue negatives for class \\\\(i\\\\) is \\\\(C_{i,0,0}\\\\), false negatives is\n\\\\(C_{i,1,0}\\\\), true positives is \\\\(C_{i,1,1}\\\\) and false positives is\n\\\\(C_{i,0,1}\\\\).\n\nHere is an example demonstrating the use of the `multilabel_confusion_matrix`\nfunction with multilabel indicator matrix input:\n\nOr a confusion matrix can be constructed for each sample\u2019s labels:\n\nHere is an example demonstrating the use of the `multilabel_confusion_matrix`\nfunction with multiclass input:\n\nHere are some examples demonstrating the use of the\n`multilabel_confusion_matrix` function to calculate recall (or sensitivity),\nspecificity, fall out and miss rate for each class in a problem with\nmultilabel indicator matrix input.\n\nCalculating recall (also called the true positive rate or the sensitivity) for\neach class:\n\nCalculating specificity (also called the true negative rate) for each class:\n\nCalculating fall out (also called the false positive rate) for each class:\n\nCalculating miss rate (also called the false negative rate) for each class:\n\nThe function `roc_curve` computes the receiver operating characteristic curve,\nor ROC curve. Quoting Wikipedia :\n\n\u201cA receiver operating characteristic (ROC), or simply ROC curve, is a\ngraphical plot which illustrates the performance of a binary classifier system\nas its discrimination threshold is varied. It is created by plotting the\nfraction of true positives out of the positives (TPR = true positive rate) vs.\nthe fraction of false positives out of the negatives (FPR = false positive\nrate), at various threshold settings. TPR is also known as sensitivity, and\nFPR is one minus the specificity or true negative rate.\u201d\n\nThis function requires the true binary value and the target scores, which can\neither be probability estimates of the positive class, confidence values, or\nbinary decisions. Here is a small example of how to use the `roc_curve`\nfunction:\n\nThis figure shows an example of such an ROC curve:\n\nThe `roc_auc_score` function computes the area under the receiver operating\ncharacteristic (ROC) curve, which is also denoted by AUC or AUROC. By\ncomputing the area under the roc curve, the curve information is summarized in\none number. For more information see the Wikipedia article on AUC.\n\nCompared to metrics such as the subset accuracy, the Hamming loss, or the F1\nscore, ROC doesn\u2019t require optimizing a threshold for each label.\n\nIn the binary case, you can either provide the probability estimates, using\nthe `classifier.predict_proba()` method, or the non-thresholded decision\nvalues given by the `classifier.decision_function()` method. In the case of\nproviding the probability estimates, the probability of the class with the\n\u201cgreater label\u201d should be provided. The \u201cgreater label\u201d corresponds to\n`classifier.classes_[1]` and thus `classifier.predict_proba(X)[:, 1]`.\nTherefore, the `y_score` parameter is of size (n_samples,).\n\nWe can use the probability estimates corresponding to `clf.classes_[1]`.\n\nOtherwise, we can use the non-thresholded decision values\n\nThe `roc_auc_score` function can also be used in multi-class classification.\nTwo averaging strategies are currently supported: the one-vs-one algorithm\ncomputes the average of the pairwise ROC AUC scores, and the one-vs-rest\nalgorithm computes the average of the ROC AUC scores for each class against\nall other classes. In both cases, the predicted labels are provided in an\narray with values from 0 to `n_classes`, and the scores correspond to the\nprobability estimates that a sample belongs to a particular class. The OvO and\nOvR algorithms support weighting uniformly (`average='macro'`) and by\nprevalence (`average='weighted'`).\n\nOne-vs-one Algorithm: Computes the average AUC of all possible pairwise\ncombinations of classes. [HT2001] defines a multiclass AUC metric weighted\nuniformly:\n\nwhere \\\\(c\\\\) is the number of classes and \\\\(\\text{AUC}(j | k)\\\\) is the AUC\nwith class \\\\(j\\\\) as the positive class and class \\\\(k\\\\) as the negative\nclass. In general, \\\\(\\text{AUC}(j | k) \\neq \\text{AUC}(k | j))\\\\) in the\nmulticlass case. This algorithm is used by setting the keyword argument\n`multiclass` to `'ovo'` and `average` to `'macro'`.\n\nThe [HT2001] multiclass AUC metric can be extended to be weighted by the\nprevalence:\n\nwhere \\\\(c\\\\) is the number of classes. This algorithm is used by setting the\nkeyword argument `multiclass` to `'ovo'` and `average` to `'weighted'`. The\n`'weighted'` option returns a prevalence-weighted average as described in\n[FC2009].\n\nOne-vs-rest Algorithm: Computes the AUC of each class against the rest\n[PD2000]. The algorithm is functionally the same as the multilabel case. To\nenable this algorithm set the keyword argument `multiclass` to `'ovr'`. Like\nOvO, OvR supports two types of averaging: `'macro'` [F2006] and `'weighted'`\n[F2001].\n\nIn applications where a high false positive rate is not tolerable the\nparameter `max_fpr` of `roc_auc_score` can be used to summarize the ROC curve\nup to the given limit.\n\nIn multi-label classification, the `roc_auc_score` function is extended by\naveraging over the labels as above. In this case, you should provide a\n`y_score` of shape `(n_samples, n_classes)`. Thus, when using the probability\nestimates, one needs to select the probability of the class with the greater\nlabel for each output.\n\nAnd the decision values do not require such processing.\n\nExamples:\n\nReferences:\n\nHand, D.J. and Till, R.J., (2001). A simple generalisation of the area under\nthe ROC curve for multiple class classification problems. Machine learning,\n45(2), pp.171-186.\n\nFerri, C\u00e8sar & Hernandez-Orallo, Jose & Modroiu, R. (2009). An Experimental\nComparison of Performance Measures for Classification. Pattern Recognition\nLetters. 30. 27-38.\n\nProvost, F., Domingos, P. (2000). Well-trained PETs: Improving probability\nestimation trees (Section 6.2), CeDER Working Paper #IS-00-04, Stern School of\nBusiness, New York University.\n\nFawcett, T., 2006. An introduction to ROC analysis. Pattern Recognition\nLetters, 27(8), pp. 861-874.\n\nFawcett, T., 2001. Using rule sets to maximize ROC performance In Data Mining,\n2001. Proceedings IEEE International Conference, pp. 131-138.\n\nThe function `det_curve` computes the detection error tradeoff curve (DET)\ncurve [WikipediaDET2017]. Quoting Wikipedia:\n\n\u201cA detection error tradeoff (DET) graph is a graphical plot of error rates for\nbinary classification systems, plotting false reject rate vs. false accept\nrate. The x- and y-axes are scaled non-linearly by their standard normal\ndeviates (or just by logarithmic transformation), yielding tradeoff curves\nthat are more linear than ROC curves, and use most of the image area to\nhighlight the differences of importance in the critical operating region.\u201d\n\nDET curves are a variation of receiver operating characteristic (ROC) curves\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\nRate. DET curves are commonly plotted in normal deviate scale by\ntransformation with \\\\(\\phi^{-1}\\\\) (with \\\\(\\phi\\\\) being the cumulative\ndistribution function). The resulting performance curves explicitly visualize\nthe tradeoff of error types for given classification algorithms. See\n[Martin1997] for examples and further motivation.\n\nThis figure compares the ROC and DET curves of two example classifiers on the\nsame classification task:\n\nProperties:\n\nApplications and limitations:\n\nDET curves are intuitive to read and hence allow quick visual assessment of a\nclassifier\u2019s performance. Additionally DET curves can be consulted for\nthreshold analysis and operating point selection. This is particularly helpful\nif a comparison of error types is required.\n\nOne the other hand DET curves do not provide their metric as a single number.\nTherefore for either automated evaluation or comparison to other\nclassification tasks metrics like the derived area under ROC curve might be\nbetter suited.\n\nExamples:\n\nReferences:\n\nWikipedia contributors. Detection error tradeoff. Wikipedia, The Free\nEncyclopedia. September 4, 2017, 23:33 UTC. Available at:\nhttps://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.\nAccessed February 19, 2018.\n\nA. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki, The DET\nCurve in Assessment of Detection Task Performance, NIST 1997.\n\nJ. Navractil and D. Klusacek, \u201cOn Linear DETs,\u201d 2007 IEEE International\nConference on Acoustics, Speech and Signal Processing - ICASSP \u201807, Honolulu,\nHI, 2007, pp. IV-229-IV-232.\n\nThe `zero_one_loss` function computes the sum or the average of the 0-1\nclassification loss (\\\\(L_{0-1}\\\\)) over \\\\(n_{\\text{samples}}\\\\). By default,\nthe function normalizes over the sample. To get the sum of the \\\\(L_{0-1}\\\\),\nset `normalize` to `False`.\n\nIn multilabel classification, the `zero_one_loss` scores a subset as one if\nits labels strictly match the predictions, and as a zero if there are any\nerrors. By default, the function returns the percentage of imperfectly\npredicted subsets. To get the count of such subsets instead, set `normalize`\nto `False`\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample and\n\\\\(y_i\\\\) is the corresponding true value, then the 0-1 loss \\\\(L_{0-1}\\\\) is\ndefined as:\n\nwhere \\\\(1(x)\\\\) is the indicator function.\n\nIn the multilabel case with binary label indicators, where the first label set\n[0,1] has an error:\n\nExample:\n\nThe `brier_score_loss` function computes the Brier score for binary classes\n[Brier1950]. Quoting Wikipedia:\n\n\u201cThe Brier score is a proper score function that measures the accuracy of\nprobabilistic predictions. It is applicable to tasks in which predictions must\nassign probabilities to a set of mutually exclusive discrete outcomes.\u201d\n\nThis function returns the mean squared error of the actual outcome \\\\(y \\in\n\\\\{0,1\\\\}\\\\) and the predicted probability estimate \\\\(p = \\operatorname{Pr}(y\n= 1)\\\\) (predict_proba) as outputted by:\n\nThe Brier score loss is also between 0 to 1 and the lower the value (the mean\nsquare difference is smaller), the more accurate the prediction is.\n\nHere is a small example of usage of this function:\n\nThe Brier score can be used to assess how well a classifier is calibrated.\nHowever, a lower Brier score loss does not always mean a better calibration.\nThis is because, by analogy with the bias-variance decomposition of the mean\nsquared error, the Brier score loss can be decomposed as the sum of\ncalibration loss and refinement loss [Bella2012]. Calibration loss is defined\nas the mean squared deviation from empirical probabilities derived from the\nslope of ROC segments. Refinement loss can be defined as the expected optimal\nloss as measured by the area under the optimal cost curve. Refinement loss can\nchange independently from calibration loss, thus a lower Brier score loss does\nnot necessarily mean a better calibrated model. \u201cOnly when refinement loss\nremains the same does a lower Brier score loss always mean better calibration\u201d\n[Bella2012], [Flach2008].\n\nExample:\n\nReferences:\n\nG. Brier, Verification of forecasts expressed in terms of probability, Monthly\nweather review 78.1 (1950)\n\nBella, Ferri, Hern\u00e1ndez-Orallo, and Ram\u00edrez-Quintana \u201cCalibration of Machine\nLearning Models\u201d in Khosrow-Pour, M. \u201cMachine learning: concepts,\nmethodologies, tools and applications.\u201d Hershey, PA: Information Science\nReference (2012).\n\nFlach, Peter, and Edson Matsubara. \u201cOn classification, ranking, and\nprobability estimation.\u201d Dagstuhl Seminar Proceedings. Schloss Dagstuhl-\nLeibniz-Zentrum fr Informatik (2008).\n\nIn multilabel learning, each sample can have any number of ground truth labels\nassociated with it. The goal is to give high scores and better rank to the\nground truth labels.\n\nThe `coverage_error` function computes the average number of labels that have\nto be included in the final prediction such that all true labels are\npredicted. This is useful if you want to know how many top-scored-labels you\nhave to predict in average without missing any true one. The best value of\nthis metrics is thus the average number of true labels.\n\nNote\n\nOur implementation\u2019s score is 1 greater than the one given in Tsoumakas et\nal., 2010. This extends it to handle the degenerate case in which an instance\nhas 0 true labels.\n\nFormally, given a binary indicator matrix of the ground truth labels \\\\(y \\in\n\\left\\\\{0, 1\\right\\\\}^{n_\\text{samples} \\times n_\\text{labels}}\\\\) and the\nscore associated with each label \\\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples}\n\\times n_\\text{labels}}\\\\), the coverage is defined as\n\nwith \\\\(\\text{rank}_{ij} = \\left|\\left\\\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij}\n\\right\\\\}\\right|\\\\). Given the rank definition, ties in `y_scores` are broken\nby giving the maximal rank that would have been assigned to all tied values.\n\nHere is a small example of usage of this function:\n\nThe `label_ranking_average_precision_score` function implements label ranking\naverage precision (LRAP). This metric is linked to the\n`average_precision_score` function, but is based on the notion of label\nranking instead of precision and recall.\n\nLabel ranking average precision (LRAP) averages over the samples the answer to\nthe following question: for each ground truth label, what fraction of higher-\nranked labels were true labels? This performance measure will be higher if you\nare able to give better rank to the labels associated with each sample. The\nobtained score is always strictly greater than 0, and the best value is 1. If\nthere is exactly one relevant label per sample, label ranking average\nprecision is equivalent to the mean reciprocal rank.\n\nFormally, given a binary indicator matrix of the ground truth labels \\\\(y \\in\n\\left\\\\{0, 1\\right\\\\}^{n_\\text{samples} \\times n_\\text{labels}}\\\\) and the\nscore associated with each label \\\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples}\n\\times n_\\text{labels}}\\\\), the average precision is defined as\n\nwhere \\\\(\\mathcal{L}_{ij} = \\left\\\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq\n\\hat{f}_{ij} \\right\\\\}\\\\), \\\\(\\text{rank}_{ij} = \\left|\\left\\\\{k: \\hat{f}_{ik}\n\\geq \\hat{f}_{ij} \\right\\\\}\\right|\\\\), \\\\(|\\cdot|\\\\) computes the cardinality\nof the set (i.e., the number of elements in the set), and \\\\(||\\cdot||_0\\\\) is\nthe \\\\(\\ell_0\\\\) \u201cnorm\u201d (which computes the number of nonzero elements in a\nvector).\n\nHere is a small example of usage of this function:\n\nThe `label_ranking_loss` function computes the ranking loss which averages\nover the samples the number of label pairs that are incorrectly ordered, i.e.\ntrue labels have a lower score than false labels, weighted by the inverse of\nthe number of ordered pairs of false and true labels. The lowest achievable\nranking loss is zero.\n\nFormally, given a binary indicator matrix of the ground truth labels \\\\(y \\in\n\\left\\\\{0, 1\\right\\\\}^{n_\\text{samples} \\times n_\\text{labels}}\\\\) and the\nscore associated with each label \\\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples}\n\\times n_\\text{labels}}\\\\), the ranking loss is defined as\n\nwhere \\\\(|\\cdot|\\\\) computes the cardinality of the set (i.e., the number of\nelements in the set) and \\\\(||\\cdot||_0\\\\) is the \\\\(\\ell_0\\\\) \u201cnorm\u201d (which\ncomputes the number of nonzero elements in a vector).\n\nHere is a small example of usage of this function:\n\nReferences:\n\nDiscounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain\n(NDCG) are ranking metrics implemented in `dcg_score` and `ndcg_score` ; they\ncompare a predicted order to ground-truth scores, such as the relevance of\nanswers to a query.\n\nFrom the Wikipedia page for Discounted Cumulative Gain:\n\n\u201cDiscounted cumulative gain (DCG) is a measure of ranking quality. In\ninformation retrieval, it is often used to measure effectiveness of web search\nengine algorithms or related applications. Using a graded relevance scale of\ndocuments in a search-engine result set, DCG measures the usefulness, or gain,\nof a document based on its position in the result list. The gain is\naccumulated from the top of the result list to the bottom, with the gain of\neach result discounted at lower ranks\u201d\n\nDCG orders the true targets (e.g. relevance of query answers) in the predicted\norder, then multiplies them by a logarithmic decay and sums the result. The\nsum can be truncated after the first \\\\(K\\\\) results, in which case we call it\nDCG@K. NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect\nprediction, so that it is always between 0 and 1. Usually, NDCG is preferred\nto DCG.\n\nCompared with the ranking loss, NDCG can take into account relevance scores,\nrather than a ground-truth ranking. So if the ground-truth consists only of an\nordering, the ranking loss should be preferred; if the ground-truth consists\nof actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\nrelevant), NDCG can be used.\n\nFor one sample, given the vector of continuous ground-truth values for each\ntarget \\\\(y \\in \\mathbb{R}^{M}\\\\), where \\\\(M\\\\) is the number of outputs, and\nthe prediction \\\\(\\hat{y}\\\\), which induces the ranking function \\\\(f\\\\), the\nDCG score is\n\nand the NDCG score is the DCG score divided by the DCG score obtained for\n\\\\(y\\\\).\n\nReferences:\n\nThe `sklearn.metrics` module implements several loss, score, and utility\nfunctions to measure regression performance. Some of those have been enhanced\nto handle the multioutput case: `mean_squared_error`, `mean_absolute_error`,\n`explained_variance_score` and `r2_score`.\n\nThese functions have an `multioutput` keyword argument which specifies the way\nthe scores or losses for each individual target should be averaged. The\ndefault is `'uniform_average'`, which specifies a uniformly weighted mean over\noutputs. If an `ndarray` of shape `(n_outputs,)` is passed, then its entries\nare interpreted as weights and an according weighted average is returned. If\n`multioutput` is `'raw_values'` is specified, then all unaltered individual\nscores or losses will be returned in an array of shape `(n_outputs,)`.\n\nThe `r2_score` and `explained_variance_score` accept an additional value\n`'variance_weighted'` for the `multioutput` parameter. This option leads to a\nweighting of each individual score by the variance of the corresponding target\nvariable. This setting quantifies the globally captured unscaled variance. If\nthe target variables are of different scale, then this score puts more\nimportance on well explaining the higher variance variables.\n`multioutput='variance_weighted'` is the default value for `r2_score` for\nbackward compatibility. This will be changed to `uniform_average` in the\nfuture.\n\nThe `explained_variance_score` computes the explained variance regression\nscore.\n\nIf \\\\(\\hat{y}\\\\) is the estimated target output, \\\\(y\\\\) the corresponding\n(correct) target output, and \\\\(Var\\\\) is Variance, the square of the standard\ndeviation, then the explained variance is estimated as follow:\n\nThe best possible score is 1.0, lower values are worse.\n\nHere is a small example of usage of the `explained_variance_score` function:\n\nThe `max_error` function computes the maximum residual error , a metric that\ncaptures the worst case error between the predicted value and the true value.\nIn a perfectly fitted single output regression model, `max_error` would be `0`\non the training set and though this would be highly unlikely in the real\nworld, this metric shows the extent of error that the model had when it was\nfitted.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample, and\n\\\\(y_i\\\\) is the corresponding true value, then the max error is defined as\n\nHere is a small example of usage of the `max_error` function:\n\nThe `max_error` does not support multioutput.\n\nThe `mean_absolute_error` function computes mean absolute error, a risk metric\ncorresponding to the expected value of the absolute error loss or\n\\\\(l1\\\\)-norm loss.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample, and\n\\\\(y_i\\\\) is the corresponding true value, then the mean absolute error (MAE)\nestimated over \\\\(n_{\\text{samples}}\\\\) is defined as\n\nHere is a small example of usage of the `mean_absolute_error` function:\n\nThe `mean_squared_error` function computes mean square error, a risk metric\ncorresponding to the expected value of the squared (quadratic) error or loss.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample, and\n\\\\(y_i\\\\) is the corresponding true value, then the mean squared error (MSE)\nestimated over \\\\(n_{\\text{samples}}\\\\) is defined as\n\nHere is a small example of usage of the `mean_squared_error` function:\n\nExamples:\n\nThe `mean_squared_log_error` function computes a risk metric corresponding to\nthe expected value of the squared logarithmic (quadratic) error or loss.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample, and\n\\\\(y_i\\\\) is the corresponding true value, then the mean squared logarithmic\nerror (MSLE) estimated over \\\\(n_{\\text{samples}}\\\\) is defined as\n\nWhere \\\\(\\log_e (x)\\\\) means the natural logarithm of \\\\(x\\\\). This metric is\nbest to use when targets having exponential growth, such as population counts,\naverage sales of a commodity over a span of years etc. Note that this metric\npenalizes an under-predicted estimate greater than an over-predicted estimate.\n\nHere is a small example of usage of the `mean_squared_log_error` function:\n\nThe `mean_absolute_percentage_error` (MAPE), also known as mean absolute\npercentage deviation (MAPD), is an evaluation metric for regression problems.\nThe idea of this metric is to be sensitive to relative errors. It is for\nexample not changed by a global scaling of the target variable.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample and\n\\\\(y_i\\\\) is the corresponding true value, then the mean absolute percentage\nerror (MAPE) estimated over \\\\(n_{\\text{samples}}\\\\) is defined as\n\nwhere \\\\(\\epsilon\\\\) is an arbitrary small yet strictly positive number to\navoid undefined results when y is zero.\n\nThe `mean_absolute_percentage_error` function supports multioutput.\n\nHere is a small example of usage of the `mean_absolute_percentage_error`\nfunction:\n\nIn above example, if we had used `mean_absolute_error`, it would have ignored\nthe small magnitude values and only reflected the error in prediction of\nhighest magnitude value. But that problem is resolved in case of MAPE because\nit calculates relative percentage error with respect to actual output.\n\nThe `median_absolute_error` is particularly interesting because it is robust\nto outliers. The loss is calculated by taking the median of all absolute\ndifferences between the target and the prediction.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample and\n\\\\(y_i\\\\) is the corresponding true value, then the median absolute error\n(MedAE) estimated over \\\\(n_{\\text{samples}}\\\\) is defined as\n\nThe `median_absolute_error` does not support multioutput.\n\nHere is a small example of usage of the `median_absolute_error` function:\n\nThe `r2_score` function computes the coefficient of determination, usually\ndenoted as R\u00b2.\n\nIt represents the proportion of variance (of y) that has been explained by the\nindependent variables in the model. It provides an indication of goodness of\nfit and therefore a measure of how well unseen samples are likely to be\npredicted by the model, through the proportion of explained variance.\n\nAs such variance is dataset dependent, R\u00b2 may not be meaningfully comparable\nacross different datasets. Best possible score is 1.0 and it can be negative\n(because the model can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features, would get a\nR\u00b2 score of 0.0.\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample and\n\\\\(y_i\\\\) is the corresponding true value for total \\\\(n\\\\) samples, the\nestimated R\u00b2 is defined as:\n\nwhere \\\\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\\\) and \\\\(\\sum_{i=1}^{n}\n(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2\\\\).\n\nNote that `r2_score` calculates unadjusted R\u00b2 without correcting for bias in\nsample variance of y.\n\nHere is a small example of usage of the `r2_score` function:\n\nExample:\n\nThe `mean_tweedie_deviance` function computes the mean Tweedie deviance error\nwith a `power` parameter (\\\\(p\\\\)). This is a metric that elicits predicted\nexpectation values of regression targets.\n\nFollowing special cases exist,\n\nIf \\\\(\\hat{y}_i\\\\) is the predicted value of the \\\\(i\\\\)-th sample, and\n\\\\(y_i\\\\) is the corresponding true value, then the mean Tweedie deviance\nerror (D) for power \\\\(p\\\\), estimated over \\\\(n_{\\text{samples}}\\\\) is\ndefined as\n\nTweedie deviance is a homogeneous function of degree `2-power`. Thus, Gamma\ndistribution with `power=2` means that simultaneously scaling `y_true` and\n`y_pred` has no effect on the deviance. For Poisson distribution `power=1` the\ndeviance scales linearly, and for Normal distribution (`power=0`),\nquadratically. In general, the higher `power` the less weight is given to\nextreme deviations between true and predicted targets.\n\nFor instance, let\u2019s compare the two predictions 1.0 and 100 that are both 50%\nof their corresponding true value.\n\nThe mean squared error (`power=0`) is very sensitive to the prediction\ndifference of the second point,:\n\nIf we increase `power` to 1,:\n\nthe difference in errors decreases. Finally, by setting, `power=2`:\n\nwe would get identical errors. The deviance when `power=2` is thus only\nsensitive to relative errors.\n\nThe `sklearn.metrics` module implements several loss, score, and utility\nfunctions. For more information see the Clustering performance evaluation\nsection for instance clustering, and Biclustering evaluation for biclustering.\n\nWhen doing supervised learning, a simple sanity check consists of comparing\none\u2019s estimator against simple rules of thumb. `DummyClassifier` implements\nseveral such simple strategies for classification:\n\nA major motivation of this method is F1-scoring, when the positive class is in\nthe minority.\n\nNote that with all these strategies, the `predict` method completely ignores\nthe input data!\n\nTo illustrate `DummyClassifier`, first let\u2019s create an imbalanced dataset:\n\nNext, let\u2019s compare the accuracy of `SVC` and `most_frequent`:\n\nWe see that `SVC` doesn\u2019t do much better than a dummy classifier. Now, let\u2019s\nchange the kernel:\n\nWe see that the accuracy was boosted to almost 100%. A cross validation\nstrategy is recommended for a better estimate of the accuracy, if it is not\ntoo CPU costly. For more information see the Cross-validation: evaluating\nestimator performance section. Moreover if you want to optimize over the\nparameter space, it is highly recommended to use an appropriate methodology;\nsee the Tuning the hyper-parameters of an estimator section for details.\n\nMore generally, when the accuracy of a classifier is too close to random, it\nprobably means that something went wrong: features are not helpful, a\nhyperparameter is not correctly tuned, the classifier is suffering from class\nimbalance, etc\u2026\n\n`DummyRegressor` also implements four simple rules of thumb for regression:\n\nIn all these strategies, the `predict` method completely ignores the input\ndata.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "3.4. Validation curves", "path": "modules/learning_curve", "type": "Guide", "text": "\nEvery estimator has its advantages and drawbacks. Its generalization error can\nbe decomposed in terms of bias, variance and noise. The bias of an estimator\nis its average error for different training sets. The variance of an estimator\nindicates how sensitive it is to varying training sets. Noise is a property of\nthe data.\n\nIn the following plot, we see a function \\\\(f(x) = \\cos (\\frac{3}{2} \\pi x)\\\\)\nand some noisy samples from that function. We use three different estimators\nto fit the function: linear regression with polynomial features of degree 1, 4\nand 15. We see that the first estimator can at best provide only a poor fit to\nthe samples and the true function because it is too simple (high bias), the\nsecond estimator approximates it almost perfectly and the last estimator\napproximates the training data perfectly but does not fit the true function\nvery well, i.e. it is very sensitive to varying training data (high variance).\n\nBias and variance are inherent properties of estimators and we usually have to\nselect learning algorithms and hyperparameters so that both bias and variance\nare as low as possible (see Bias-variance dilemma). Another way to reduce the\nvariance of a model is to use more training data. However, you should only\ncollect more training data if the true function is too complex to be\napproximated by an estimator with a lower variance.\n\nIn the simple one-dimensional problem that we have seen in the example it is\neasy to see whether the estimator suffers from bias or variance. However, in\nhigh-dimensional spaces, models can become very difficult to visualize. For\nthis reason, it is often helpful to use the tools described below.\n\nExamples:\n\nTo validate a model we need a scoring function (see Metrics and scoring:\nquantifying the quality of predictions), for example accuracy for classifiers.\nThe proper way of choosing multiple hyperparameters of an estimator are of\ncourse grid search or similar methods (see Tuning the hyper-parameters of an\nestimator) that select the hyperparameter with the maximum score on a\nvalidation set or multiple validation sets. Note that if we optimized the\nhyperparameters based on a validation score the validation score is biased and\nnot a good estimate of the generalization any longer. To get a proper estimate\nof the generalization we have to compute the score on another test set.\n\nHowever, it is sometimes helpful to plot the influence of a single\nhyperparameter on the training score and the validation score to find out\nwhether the estimator is overfitting or underfitting for some hyperparameter\nvalues.\n\nThe function `validation_curve` can help in this case:\n\nIf the training score and the validation score are both low, the estimator\nwill be underfitting. If the training score is high and the validation score\nis low, the estimator is overfitting and otherwise it is working very well. A\nlow training score and a high validation score is usually not possible.\nUnderfitting, overfitting, and a working model are shown in the in the plot\nbelow where we vary the parameter \\\\(\\gamma\\\\) of an SVM on the digits\ndataset.\n\nA learning curve shows the validation and training score of an estimator for\nvarying numbers of training samples. It is a tool to find out how much we\nbenefit from adding more training data and whether the estimator suffers more\nfrom a variance error or a bias error. Consider the following example where we\nplot the learning curve of a naive Bayes classifier and an SVM.\n\nFor the naive Bayes, both the validation score and the training score converge\nto a value that is quite low with increasing size of the training set. Thus,\nwe will probably not benefit much from more training data.\n\nIn contrast, for small amounts of data, the training score of the SVM is much\ngreater than the validation score. Adding more training samples will most\nlikely increase generalization.\n\nWe can use the function `learning_curve` to generate the values that are\nrequired to plot such a learning curve (number of samples that have been used,\nthe average scores on the training sets and the average scores on the\nvalidation sets):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "4.1. Partial Dependence and Individual Conditional Expectation plots", "path": "modules/partial_dependence", "type": "Guide", "text": "\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\nplots can be used to visualize and analyze interaction between the target\nresponse 1 and a set of input features of interest.\n\nBoth PDPs and ICEs assume that the input features of interest are independent\nfrom the complement features, and this assumption is often violated in\npractice. Thus, in the case of correlated features, we will create absurd data\npoints to compute the PDP/ICE.\n\nPartial dependence plots (PDP) show the dependence between the target response\nand a set of input features of interest, marginalizing over the values of all\nother input features (the \u2018complement\u2019 features). Intuitively, we can\ninterpret the partial dependence as the expected target response as a function\nof the input features of interest.\n\nDue to the limits of human perception the size of the set of input feature of\ninterest must be small (usually, one or two) thus the input features of\ninterest are usually chosen among the most important features.\n\nThe figure below shows two one-way and one two-way partial dependence plots\nfor the California housing dataset, with a `HistGradientBoostingRegressor`:\n\nOne-way PDPs tell us about the interaction between the target response and an\ninput feature of interest feature (e.g. linear, non-linear). The left plot in\nthe above figure shows the effect of the average occupancy on the median house\nprice; we can clearly see a linear relationship among them when the average\noccupancy is inferior to 3 persons. Similarly, we could analyze the effect of\nthe house age on the median house price (middle plot). Thus, these\ninterpretations are marginal, considering a feature at a time.\n\nPDPs with two input features of interest show the interactions among the two\nfeatures. For example, the two-variable PDP in the above figure shows the\ndependence of median house price on joint values of house age and average\noccupants per household. We can clearly see an interaction between the two\nfeatures: for an average occupancy greater than two, the house price is nearly\nindependent of the house age, whereas for values less than 2 there is a strong\ndependence on age.\n\nThe `sklearn.inspection` module provides a convenience function\n`plot_partial_dependence` to create one-way and two-way partial dependence\nplots. In the below example we show how to create a grid of partial dependence\nplots: two one-way PDPs for the features `0` and `1` and a two-way PDP between\nthe two features:\n\nYou can access the newly created figure and Axes objects using `plt.gcf()` and\n`plt.gca()`.\n\nFor multi-class classification, you need to set the class label for which the\nPDPs should be created via the `target` argument:\n\nThe same parameter `target` is used to specify the target in multi-output\nregression settings.\n\nIf you need the raw values of the partial dependence function rather than the\nplots, you can use the `sklearn.inspection.partial_dependence` function:\n\nThe values at which the partial dependence should be evaluated are directly\ngenerated from `X`. For 2-way partial dependence, a 2D-grid of values is\ngenerated. The `values` field returned by\n`sklearn.inspection.partial_dependence` gives the actual values used in the\ngrid for each input feature of interest. They also correspond to the axis of\nthe plots.\n\nSimilar to a PDP, an individual conditional expectation (ICE) plot shows the\ndependence between the target function and an input feature of interest.\nHowever, unlike a PDP, which shows the average effect of the input feature, an\nICE plot visualizes the dependence of the prediction on a feature for each\nsample separately with one line per sample. Due to the limits of human\nperception, only one input feature of interest is supported for ICE plots.\n\nThe figures below show four ICE plots for the California housing dataset, with\na `HistGradientBoostingRegressor`. The second figure plots the corresponding\nPD line overlaid on ICE lines.\n\nWhile the PDPs are good at showing the average effect of the target features,\nthey can obscure a heterogeneous relationship created by interactions. When\ninteractions are present the ICE plot will provide many more insights. For\nexample, we could observe a linear relationship between the median income and\nthe house price in the PD line. However, the ICE lines show that there are\nsome exceptions, where the house price remains constant in some ranges of the\nmedian income.\n\nThe `sklearn.inspection` module\u2019s `plot_partial_dependence` convenience\nfunction can be used to create ICE plots by setting `kind='individual'`. In\nthe example below, we show how to create a grid of ICE plots:\n\nIn ICE plots it might not be easy to see the average effect of the input\nfeature of interest. Hence, it is recommended to use ICE plots alongside PDPs.\nThey can be plotted together with `kind='both'`.\n\nLet \\\\(X_S\\\\) be the set of input features of interest (i.e. the `features`\nparameter) and let \\\\(X_C\\\\) be its complement.\n\nThe partial dependence of the response \\\\(f\\\\) at a point \\\\(x_S\\\\) is defined\nas:\n\nwhere \\\\(f(x_S, x_C)\\\\) is the response function (predict, predict_proba or\ndecision_function) for a given sample whose values are defined by \\\\(x_S\\\\)\nfor the features in \\\\(X_S\\\\), and by \\\\(x_C\\\\) for the features in \\\\(X_C\\\\).\nNote that \\\\(x_S\\\\) and \\\\(x_C\\\\) may be tuples.\n\nComputing this integral for various values of \\\\(x_S\\\\) produces a PDP plot as\nabove. An ICE line is defined as a single \\\\(f(x_{S}, x_{C}^{(i)})\\\\)\nevaluated at \\\\(x_{S}\\\\).\n\nThere are two main methods to approximate the integral above, namely the\n\u2018brute\u2019 and \u2018recursion\u2019 methods. The `method` parameter controls which method\nto use.\n\nThe \u2018brute\u2019 method is a generic method that works with any estimator. Note\nthat computing ICE plots is only supported with the \u2018brute\u2019 method. It\napproximates the above integral by computing an average over the data `X`:\n\nwhere \\\\(x_C^{(i)}\\\\) is the value of the i-th sample for the features in\n\\\\(X_C\\\\). For each value of \\\\(x_S\\\\), this method requires a full pass over\nthe dataset `X` which is computationally intensive.\n\nEach of the \\\\(f(x_{S}, x_{C}^{(i)})\\\\) corresponds to one ICE line evaluated\nat \\\\(x_{S}\\\\). Computing this for multiple values of \\\\(x_{S}\\\\), one obtains\na full ICE line. As one can see, the average of the ICE lines correspond to\nthe partial dependence line.\n\nThe \u2018recursion\u2019 method is faster than the \u2018brute\u2019 method, but it is only\nsupported for PDP plots by some tree-based estimators. It is computed as\nfollows. For a given point \\\\(x_S\\\\), a weighted tree traversal is performed:\nif a split node involves an input feature of interest, the corresponding left\nor right branch is followed; otherwise both branches are followed, each branch\nbeing weighted by the fraction of training samples that entered that branch.\nFinally, the partial dependence is given by a weighted average of all the\nvisited leaves values.\n\nWith the \u2018brute\u2019 method, the parameter `X` is used both for generating the\ngrid of values \\\\(x_S\\\\) and the complement feature values \\\\(x_C\\\\). However\nwith the \u2018recursion\u2019 method, `X` is only used for the grid values: implicitly,\nthe \\\\(x_C\\\\) values are those of the training data.\n\nBy default, the \u2018recursion\u2019 method is used for plotting PDPs on tree-based\nestimators that support it, and \u2018brute\u2019 is used for the rest.\n\nNote\n\nWhile both methods should be close in general, they might differ in some\nspecific settings. The \u2018brute\u2019 method assumes the existence of the data points\n\\\\((x_S, x_C^{(i)})\\\\). When the features are correlated, such artificial\nsamples may have a very low probability mass. The \u2018brute\u2019 and \u2018recursion\u2019\nmethods will likely disagree regarding the value of the partial dependence,\nbecause they will treat these unlikely samples differently. Remember, however,\nthat the primary assumption for interpreting PDPs is that the features should\nbe independent.\n\nExamples:\n\nFor classification, the target response may be the probability of a class (the\npositive class for binary classification), or the decision function.\n\nReferences\n\nT. Hastie, R. Tibshirani and J. Friedman, The Elements of Statistical\nLearning, Second Edition, Section 10.13.2, Springer, 2009.\n\nC. Molnar, Interpretable Machine Learning, Section 5.1, 2019.\n\nA. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, Peeking Inside the Black\nBox: Visualizing Statistical Learning With Plots of Individual Conditional\nExpectation, Journal of Computational and Graphical Statistics, 24(1): 44-65,\nSpringer, 2015.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "4.2. Permutation feature importance", "path": "modules/permutation_importance", "type": "Guide", "text": "\nPermutation feature importance is a model inspection technique that can be\nused for any fitted estimator when the data is tabular. This is especially\nuseful for non-linear or opaque estimators. The permutation feature importance\nis defined to be the decrease in a model score when a single feature value is\nrandomly shuffled 1. This procedure breaks the relationship between the\nfeature and the target, thus the drop in the model score is indicative of how\nmuch the model depends on the feature. This technique benefits from being\nmodel agnostic and can be calculated many times with different permutations of\nthe feature.\n\nThe `permutation_importance` function calculates the feature importance of\nestimators for a given dataset. The `n_repeats` parameter sets the number of\ntimes a feature is randomly shuffled and returns a sample of feature\nimportances.\n\nLet\u2019s consider the following trained regression model:\n\nIts validation performance, measured via the \\\\(R^2\\\\) score, is significantly\nlarger than the chance level. This makes it possible to use the\n`permutation_importance` function to probe which features are most predictive:\n\nNote that the importance values for the top features represent a large\nfraction of the reference score of 0.356.\n\nPermutation importances can be computed either on the training set or on a\nheld-out testing or validation set. Using a held-out set makes it possible to\nhighlight which features contribute the most to the generalization power of\nthe inspected model. Features that are important on the training set but not\non the held-out set might cause the model to overfit.\n\nWarning\n\nFeatures that are deemed of low importance for a bad model (low cross-\nvalidation score) could be very important for a good model. Therefore it is\nalways important to evaluate the predictive power of a model using a held-out\nset (or better with cross-validation) prior to computing importances.\nPermutation importance does not reflect to the intrinsic predictive value of a\nfeature by itself but how important this feature is for a particular model.\n\nFor each feature \\\\(j\\\\) (column of \\\\(D\\\\)):\n\nFor each repetition \\\\(k\\\\) in \\\\({1, ..., K}\\\\):\n\nCompute importance \\\\(i_j\\\\) for feature \\\\(f_j\\\\) defined as:\n\nTree-based models provide an alternative measure of feature importances based\non the mean decrease in impurity (MDI). Impurity is quantified by the\nsplitting criterion of the decision trees (Gini, Entropy or Mean Squared\nError). However, this method can give high importance to features that may not\nbe predictive on unseen data when the model is overfitting. Permutation-based\nfeature importance, on the other hand, avoids this issue, since it can be\ncomputed on unseen data.\n\nFurthermore, impurity-based feature importance for trees are strongly biased\nand favor high cardinality features (typically numerical features) over low\ncardinality features such as binary features or categorical variables with a\nsmall number of possible categories.\n\nPermutation-based feature importances do not exhibit such a bias.\nAdditionally, the permutation feature importance may be computed performance\nmetric on the model predictions predictions and can be used to analyze any\nmodel class (not just tree-based models).\n\nThe following example highlights the limitations of impurity-based feature\nimportance in contrast to permutation-based feature importance: Permutation\nImportance vs Random Forest Feature Importance (MDI).\n\nWhen two features are correlated and one of the features is permuted, the\nmodel will still have access to the feature through its correlated feature.\nThis will result in a lower importance value for both features, where they\nmight actually be important.\n\nOne way to handle this is to cluster features that are correlated and only\nkeep one feature from each cluster. This strategy is explored in the following\nexample: Permutation Importance with Multicollinear or Correlated Features.\n\nExamples:\n\nReferences:\n\nL. Breiman, \u201cRandom Forests\u201d, Machine Learning, 45(1), 5-32, 2001.\nhttps://doi.org/10.1023/A:1010933404324\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.1. Pipelines and composite estimators", "path": "modules/compose", "type": "Guide", "text": "\nTransformers are usually combined with classifiers, regressors or other\nestimators to build a composite estimator. The most common tool is a Pipeline.\nPipeline is often used in combination with FeatureUnion which concatenates the\noutput of transformers into a composite feature space.\nTransformedTargetRegressor deals with transforming the target (i.e. log-\ntransform y). In contrast, Pipelines only transform the observed data (X).\n\n`Pipeline` can be used to chain multiple estimators into one. This is useful\nas there is often a fixed sequence of steps in processing the data, for\nexample feature selection, normalization and classification. `Pipeline` serves\nmultiple purposes here:\n\nYou only have to call fit and predict once on your data to fit a whole\nsequence of estimators.\n\nYou can grid search over parameters of all estimators in the pipeline at once.\n\nPipelines help avoid leaking statistics from your test data into the trained\nmodel in cross-validation, by ensuring that the same samples are used to train\nthe transformers and predictors.\n\nAll estimators in a pipeline, except the last one, must be transformers (i.e.\nmust have a transform method). The last estimator may be any type\n(transformer, classifier, etc.).\n\nThe `Pipeline` is built using a list of `(key, value)` pairs, where the `key`\nis a string containing the name you want to give this step and `value` is an\nestimator object:\n\nThe utility function `make_pipeline` is a shorthand for constructing\npipelines; it takes a variable number of estimators and returns a pipeline,\nfilling in the names automatically:\n\nThe estimators of a pipeline are stored as a list in the `steps` attribute,\nbut can be accessed by index or name by indexing (with `[idx]`) the Pipeline:\n\nPipeline\u2019s `named_steps` attribute allows accessing steps by name with tab\ncompletion in interactive environments:\n\nA sub-pipeline can also be extracted using the slicing notation commonly used\nfor Python Sequences such as lists or strings (although only a step of 1 is\npermitted). This is convenient for performing only some of the transformations\n(or their inverse):\n\nParameters of the estimators in the pipeline can be accessed using the\n`<estimator>__<parameter>` syntax:\n\nThis is particularly important for doing grid searches:\n\nIndividual steps may also be replaced as parameters, and non-final steps may\nbe ignored by setting them to `'passthrough'`:\n\nThe estimators of the pipeline can be retrieved by index:\n\nor by name:\n\nExamples:\n\nSee Also:\n\nCalling `fit` on the pipeline is the same as calling `fit` on each estimator\nin turn, `transform` the input and pass it on to the next step. The pipeline\nhas all the methods that the last estimator in the pipeline has, i.e. if the\nlast estimator is a classifier, the `Pipeline` can be used as a classifier. If\nthe last estimator is a transformer, again, so is the pipeline.\n\nFitting transformers may be computationally expensive. With its `memory`\nparameter set, `Pipeline` will cache each transformer after calling `fit`.\nThis feature is used to avoid computing the fit transformers within a pipeline\nif the parameters and input data are identical. A typical example is the case\nof a grid search in which the transformers can be fitted only once and reused\nfor each configuration.\n\nThe parameter `memory` is needed in order to cache the transformers. `memory`\ncan be either a string containing the directory where to cache the\ntransformers or a joblib.Memory object:\n\nWarning\n\nSide effect of caching transformers\n\nUsing a `Pipeline` without cache enabled, it is possible to inspect the\noriginal instance such as:\n\nEnabling caching triggers a clone of the transformers before fitting.\nTherefore, the transformer instance given to the pipeline cannot be inspected\ndirectly. In following example, accessing the `PCA` instance `pca2` will raise\nan `AttributeError` since `pca2` will be an unfitted transformer. Instead, use\nthe attribute `named_steps` to inspect estimators within the pipeline:\n\nExamples:\n\n`TransformedTargetRegressor` transforms the targets `y` before fitting a\nregression model. The predictions are mapped back to the original space via an\ninverse transform. It takes as an argument the regressor that will be used for\nprediction, and the transformer that will be applied to the target variable:\n\nFor simple transformations, instead of a Transformer object, a pair of\nfunctions can be passed, defining the transformation and its inverse mapping:\n\nSubsequently, the object is created as:\n\nBy default, the provided functions are checked at each fit to be the inverse\nof each other. However, it is possible to bypass this checking by setting\n`check_inverse` to `False`:\n\nNote\n\nThe transformation can be triggered by setting either `transformer` or the\npair of functions `func` and `inverse_func`. However, setting both options\nwill raise an error.\n\nExamples:\n\n`FeatureUnion` combines several transformer objects into a new transformer\nthat combines their output. A `FeatureUnion` takes a list of transformer\nobjects. During fitting, each of these is fit to the data independently. The\ntransformers are applied in parallel, and the feature matrices they output are\nconcatenated side-by-side into a larger matrix.\n\nWhen you want to apply different transformations to each field of the data,\nsee the related class `ColumnTransformer` (see user guide).\n\n`FeatureUnion` serves the same purposes as `Pipeline` \\- convenience and joint\nparameter estimation and validation.\n\n`FeatureUnion` and `Pipeline` can be combined to create complex models.\n\n(A `FeatureUnion` has no way of checking whether two transformers might\nproduce identical features. It only produces a union when the feature sets are\ndisjoint, and making sure they are is the caller\u2019s responsibility.)\n\nA `FeatureUnion` is built using a list of `(key, value)` pairs, where the\n`key` is the name you want to give to a given transformation (an arbitrary\nstring; it only serves as an identifier) and `value` is an estimator object:\n\nLike pipelines, feature unions have a shorthand constructor called\n`make_union` that does not require explicit naming of the components.\n\nLike `Pipeline`, individual steps may be replaced using `set_params`, and\nignored by setting to `'drop'`:\n\nExamples:\n\nMany datasets contain features of different types, say text, floats, and\ndates, where each type of feature requires separate preprocessing or feature\nextraction steps. Often it is easiest to preprocess data before applying\nscikit-learn methods, for example using pandas. Processing your data before\npassing it to scikit-learn might be problematic for one of the following\nreasons:\n\nThe `ColumnTransformer` helps performing different transformations for\ndifferent columns of the data, within a `Pipeline` that is safe from data\nleakage and that can be parametrized. `ColumnTransformer` works on arrays,\nsparse matrices, and pandas DataFrames.\n\nTo each column, a different transformation can be applied, such as\npreprocessing or a specific feature extraction method:\n\nFor this data, we might want to encode the `'city'` column as a categorical\nvariable using `OneHotEncoder` but apply a `CountVectorizer` to the `'title'`\ncolumn. As we might use multiple feature extraction methods on the same\ncolumn, we give each transformer a unique name, say `'city_category'` and\n`'title_bow'`. By default, the remaining rating columns are ignored\n(`remainder='drop'`):\n\nIn the above example, the `CountVectorizer` expects a 1D array as input and\ntherefore the columns were specified as a string (`'title'`). However,\n`OneHotEncoder` as most of other transformers expects 2D data, therefore in\nthat case you need to specify the column as a list of strings (`['city']`).\n\nApart from a scalar or a single item list, the column selection can be\nspecified as a list of multiple items, an integer array, a slice, a boolean\nmask, or with a `make_column_selector`. The `make_column_selector` is used to\nselect columns based on data type or column name:\n\nStrings can reference columns if the input is a DataFrame, integers are always\ninterpreted as the positional columns.\n\nWe can keep the remaining rating columns by setting `remainder='passthrough'`.\nThe values are appended to the end of the transformation:\n\nThe `remainder` parameter can be set to an estimator to transform the\nremaining rating columns. The transformed values are appended to the end of\nthe transformation:\n\nThe `make_column_transformer` function is available to more easily create a\n`ColumnTransformer` object. Specifically, the names will be given\nautomatically. The equivalent for the above example would be:\n\nEstimators can be displayed with a HTML representation when shown in a jupyter\nnotebook. This can be useful to diagnose or visualize a Pipeline with many\nestimators. This visualization is activated by setting the `display` option in\n`set_config`:\n\nAn example of the HTML output can be seen in the HTML representation of\nPipeline section of Column Transformer with Mixed Types. As an alternative,\nthe HTML can be written to a file using `estimator_html_repr`:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.2. Feature extraction", "path": "modules/feature_extraction", "type": "Guide", "text": "\nThe `sklearn.feature_extraction` module can be used to extract features in a\nformat supported by machine learning algorithms from datasets consisting of\nformats such as text and image.\n\nNote\n\nFeature extraction is very different from Feature selection: the former\nconsists in transforming arbitrary data, such as text or images, into\nnumerical features usable for machine learning. The latter is a machine\nlearning technique applied on these features.\n\nThe class `DictVectorizer` can be used to convert feature arrays represented\nas lists of standard Python `dict` objects to the NumPy/SciPy representation\nused by scikit-learn estimators.\n\nWhile not particularly fast to process, Python\u2019s `dict` has the advantages of\nbeing convenient to use, being sparse (absent features need not be stored) and\nstoring feature names in addition to values.\n\n`DictVectorizer` implements what is called one-of-K or \u201cone-hot\u201d coding for\ncategorical (aka nominal, discrete) features. Categorical features are\n\u201cattribute-value\u201d pairs where the value is restricted to a list of discrete of\npossibilities without ordering (e.g. topic identifiers, types of objects,\ntags, names\u2026).\n\nIn the following, \u201ccity\u201d is a categorical attribute while \u201ctemperature\u201d is a\ntraditional numerical feature:\n\n`DictVectorizer` accepts multiple string values for one feature, like, e.g.,\nmultiple categories for a movie.\n\nAssume a database classifies each movie using some categories (not\nmandatories) and its year of release.\n\n`DictVectorizer` is also a useful representation transformation for training\nsequence classifiers in Natural Language Processing models that typically work\nby extracting feature windows around a particular word of interest.\n\nFor example, suppose that we have a first algorithm that extracts Part of\nSpeech (PoS) tags that we want to use as complementary tags for training a\nsequence classifier (e.g. a chunker). The following dict could be such a\nwindow of features extracted around the word \u2018sat\u2019 in the sentence \u2018The cat\nsat on the mat.\u2019:\n\nThis description can be vectorized into a sparse two-dimensional matrix\nsuitable for feeding into a classifier (maybe after being piped into a\n`TfidfTransformer` for normalization):\n\nAs you can imagine, if one extracts such a context around each individual word\nof a corpus of documents the resulting matrix will be very wide (many one-hot-\nfeatures) with most of them being valued to zero most of the time. So as to\nmake the resulting data structure able to fit in memory the `DictVectorizer`\nclass uses a `scipy.sparse` matrix by default instead of a `numpy.ndarray`.\n\nThe class `FeatureHasher` is a high-speed, low-memory vectorizer that uses a\ntechnique known as feature hashing, or the \u201chashing trick\u201d. Instead of\nbuilding a hash table of the features encountered in training, as the\nvectorizers do, instances of `FeatureHasher` apply a hash function to the\nfeatures to determine their column index in sample matrices directly. The\nresult is increased speed and reduced memory usage, at the expense of\ninspectability; the hasher does not remember what the input features looked\nlike and has no `inverse_transform` method.\n\nSince the hash function might cause collisions between (unrelated) features, a\nsigned hash function is used and the sign of the hash value determines the\nsign of the value stored in the output matrix for a feature. This way,\ncollisions are likely to cancel out rather than accumulate error, and the\nexpected mean of any output feature\u2019s value is zero. This mechanism is enabled\nby default with `alternate_sign=True` and is particularly useful for small\nhash table sizes (`n_features < 10000`). For large hash table sizes, it can be\ndisabled, to allow the output to be passed to estimators like `MultinomialNB`\nor `chi2` feature selectors that expect non-negative inputs.\n\n`FeatureHasher` accepts either mappings (like Python\u2019s `dict` and its variants\nin the `collections` module), `(feature, value)` pairs, or strings, depending\non the constructor parameter `input_type`. Mapping are treated as lists of\n`(feature, value)` pairs, while single strings have an implicit value of 1, so\n`['feat1', 'feat2', 'feat3']` is interpreted as `[('feat1', 1), ('feat2', 1),\n('feat3', 1)]`. If a single feature occurs multiple times in a sample, the\nassociated values will be summed (so `('feat', 2)` and `('feat', 3.5)` become\n`('feat', 5.5)`). The output from `FeatureHasher` is always a `scipy.sparse`\nmatrix in the CSR format.\n\nFeature hashing can be employed in document classification, but unlike\n`CountVectorizer`, `FeatureHasher` does not do word splitting or any other\npreprocessing except Unicode-to-UTF-8 encoding; see Vectorizing a large text\ncorpus with the hashing trick, below, for a combined tokenizer/hasher.\n\nAs an example, consider a word-level natural language processing task that\nneeds features extracted from `(token, part_of_speech)` pairs. One could use a\nPython generator function to extract features:\n\nThen, the `raw_X` to be fed to `FeatureHasher.transform` can be constructed\nusing:\n\nand fed to a hasher with:\n\nto get a `scipy.sparse` matrix `X`.\n\nNote the use of a generator comprehension, which introduces laziness into the\nfeature extraction: tokens are only processed on demand from the hasher.\n\n`FeatureHasher` uses the signed 32-bit variant of MurmurHash3. As a result\n(and because of limitations in `scipy.sparse`), the maximum number of features\nsupported is currently \\\\(2^{31} - 1\\\\).\n\nThe original formulation of the hashing trick by Weinberger et al. used two\nseparate hash functions \\\\(h\\\\) and \\\\(\\xi\\\\) to determine the column index\nand sign of a feature, respectively. The present implementation works under\nthe assumption that the sign bit of MurmurHash3 is independent of its other\nbits.\n\nSince a simple modulo is used to transform the hash function to a column\nindex, it is advisable to use a power of two as the `n_features` parameter;\notherwise the features will not be mapped evenly to the columns.\n\nReferences:\n\nText Analysis is a major application field for machine learning algorithms.\nHowever the raw data, a sequence of symbols cannot be fed directly to the\nalgorithms themselves as most of them expect numerical feature vectors with a\nfixed size rather than the raw text documents with variable length.\n\nIn order to address this, scikit-learn provides utilities for the most common\nways to extract numerical features from text content, namely:\n\nIn this scheme, features and samples are defined as follows:\n\nA corpus of documents can thus be represented by a matrix with one row per\ndocument and one column per token (e.g. word) occurring in the corpus.\n\nWe call vectorization the general process of turning a collection of text\ndocuments into numerical feature vectors. This specific strategy\n(tokenization, counting and normalization) is called the Bag of Words or \u201cBag\nof n-grams\u201d representation. Documents are described by word occurrences while\ncompletely ignoring the relative position information of the words in the\ndocument.\n\nAs most documents will typically use a very small subset of the words used in\nthe corpus, the resulting matrix will have many feature values that are zeros\n(typically more than 99% of them).\n\nFor instance a collection of 10,000 short text documents (such as emails) will\nuse a vocabulary with a size in the order of 100,000 unique words in total\nwhile each document will use 100 to 1000 unique words individually.\n\nIn order to be able to store such a matrix in memory but also to speed up\nalgebraic operations matrix / vector, implementations will typically use a\nsparse representation such as the implementations available in the\n`scipy.sparse` package.\n\n`CountVectorizer` implements both tokenization and occurrence counting in a\nsingle class:\n\nThis model has many parameters, however the default values are quite\nreasonable (please see the reference documentation for the details):\n\nLet\u2019s use it to tokenize and count the word occurrences of a minimalistic\ncorpus of text documents:\n\nThe default configuration tokenizes the string by extracting words of at least\n2 letters. The specific function that does this step can be requested\nexplicitly:\n\nEach term found by the analyzer during the fit is assigned a unique integer\nindex corresponding to a column in the resulting matrix. This interpretation\nof the columns can be retrieved as follows:\n\nThe converse mapping from feature name to column index is stored in the\n`vocabulary_` attribute of the vectorizer:\n\nHence words that were not seen in the training corpus will be completely\nignored in future calls to the transform method:\n\nNote that in the previous corpus, the first and the last documents have\nexactly the same words hence are encoded in equal vectors. In particular we\nlose the information that the last document is an interrogative form. To\npreserve some of the local ordering information we can extract 2-grams of\nwords in addition to the 1-grams (individual words):\n\nThe vocabulary extracted by this vectorizer is hence much bigger and can now\nresolve ambiguities encoded in local positioning patterns:\n\nIn particular the interrogative form \u201cIs this\u201d is only present in the last\ndocument:\n\nStop words are words like \u201cand\u201d, \u201cthe\u201d, \u201chim\u201d, which are presumed to be\nuninformative in representing the content of a text, and which may be removed\nto avoid them being construed as signal for prediction. Sometimes, however,\nsimilar words are useful for prediction, such as in classifying writing style\nor personality.\n\nThere are several known issues in our provided \u2018english\u2019 stop word list. It\ndoes not aim to be a general, \u2018one-size-fits-all\u2019 solution as some tasks may\nrequire a more custom solution. See [NQY18] for more details.\n\nPlease take care in choosing a stop word list. Popular stop word lists may\ninclude words that are highly informative to some tasks, such as computer.\n\nYou should also make sure that the stop word list has had the same\npreprocessing and tokenization applied as the one used in the vectorizer. The\nword we\u2019ve is split into we and ve by CountVectorizer\u2019s default tokenizer, so\nif we\u2019ve is in `stop_words`, but ve is not, ve will be retained from we\u2019ve in\ntransformed text. Our vectorizers will try to identify and warn about some\nkinds of inconsistencies.\n\nReferences\n\nJ. Nothman, H. Qin and R. Yurchak (2018). \u201cStop Word Lists in Free Open-source\nSoftware Packages\u201d. In Proc. Workshop for NLP Open Source Software.\n\nIn a large text corpus, some words will be very present (e.g. \u201cthe\u201d, \u201ca\u201d, \u201cis\u201d\nin English) hence carrying very little meaningful information about the actual\ncontents of the document. If we were to feed the direct count data directly to\na classifier those very frequent terms would shadow the frequencies of rarer\nyet more interesting terms.\n\nIn order to re-weight the count features into floating point values suitable\nfor usage by a classifier it is very common to use the tf\u2013idf transform.\n\nTf means term-frequency while tf\u2013idf means term-frequency times inverse\ndocument-frequency: \\\\(\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times\n\\text{idf(t)}\\\\).\n\nUsing the `TfidfTransformer`\u2019s default settings, `TfidfTransformer(norm='l2',\nuse_idf=True, smooth_idf=True, sublinear_tf=False)` the term frequency, the\nnumber of times a term occurs in a given document, is multiplied with idf\ncomponent, which is computed as\n\n\\\\(\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1\\\\),\n\nwhere \\\\(n\\\\) is the total number of documents in the document set, and\n\\\\(\\text{df}(t)\\\\) is the number of documents in the document set that contain\nterm \\\\(t\\\\). The resulting tf-idf vectors are then normalized by the\nEuclidean norm:\n\n\\\\(v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots +\nv{_n}^2}}\\\\).\n\nThis was originally a term weighting scheme developed for information\nretrieval (as a ranking function for search engines results) that has also\nfound good use in document classification and clustering.\n\nThe following sections contain further explanations and examples that\nillustrate how the tf-idfs are computed exactly and how the tf-idfs computed\nin scikit-learn\u2019s `TfidfTransformer` and `TfidfVectorizer` differ slightly\nfrom the standard textbook notation that defines the idf as\n\n\\\\(\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.\\\\)\n\nIn the `TfidfTransformer` and `TfidfVectorizer` with `smooth_idf=False`, the\n\u201c1\u201d count is added to the idf instead of the idf\u2019s denominator:\n\n\\\\(\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1\\\\)\n\nThis normalization is implemented by the `TfidfTransformer` class:\n\nAgain please see the reference documentation for the details on all the\nparameters.\n\nLet\u2019s take an example with the following counts. The first term is present\n100% of the time hence not very interesting. The two other features only in\nless than 50% of the time hence probably more representative of the content of\nthe documents:\n\nEach row is normalized to have unit Euclidean norm:\n\n\\\\(v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots +\nv{_n}^2}}\\\\)\n\nFor example, we can compute the tf-idf of the first term in the first document\nin the `counts` array as follows:\n\n\\\\(n = 6\\\\)\n\n\\\\(\\text{df}(t)_{\\text{term1}} = 6\\\\)\n\n\\\\(\\text{idf}(t)_{\\text{term1}} = \\log \\frac{n}{\\text{df}(t)} + 1 = \\log(1)+1\n= 1\\\\)\n\n\\\\(\\text{tf-idf}_{\\text{term1}} = \\text{tf} \\times \\text{idf} = 3 \\times 1 =\n3\\\\)\n\nNow, if we repeat this computation for the remaining 2 terms in the document,\nwe get\n\n\\\\(\\text{tf-idf}_{\\text{term2}} = 0 \\times (\\log(6/1)+1) = 0\\\\)\n\n\\\\(\\text{tf-idf}_{\\text{term3}} = 1 \\times (\\log(6/2)+1) \\approx 2.0986\\\\)\n\nand the vector of raw tf-idfs:\n\n\\\\(\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].\\\\)\n\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs for\ndocument 1:\n\n\\\\(\\frac{[3, 0, 2.0986]}{\\sqrt{\\big(3^2 + 0^2 + 2.0986^2\\big)}} = [ 0.819, 0,\n0.573].\\\\)\n\nFurthermore, the default parameter `smooth_idf=True` adds \u201c1\u201d to the numerator\nand denominator as if an extra document was seen containing every term in the\ncollection exactly once, which prevents zero divisions:\n\n\\\\(\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1\\\\)\n\nUsing this modification, the tf-idf of the third term in document 1 changes to\n1.8473:\n\n\\\\(\\text{tf-idf}_{\\text{term3}} = 1 \\times \\log(7/3)+1 \\approx 1.8473\\\\)\n\nAnd the L2-normalized tf-idf changes to\n\n\\\\(\\frac{[3, 0, 1.8473]}{\\sqrt{\\big(3^2 + 0^2 + 1.8473^2\\big)}} = [0.8515, 0,\n0.5243]\\\\):\n\nThe weights of each feature computed by the `fit` method call are stored in a\nmodel attribute:\n\nAs tf\u2013idf is very often used for text features, there is also another class\ncalled `TfidfVectorizer` that combines all the options of `CountVectorizer`\nand `TfidfTransformer` in a single model:\n\nWhile the tf\u2013idf normalization is often very useful, there might be cases\nwhere the binary occurrence markers might offer better features. This can be\nachieved by using the `binary` parameter of `CountVectorizer`. In particular,\nsome estimators such as Bernoulli Naive Bayes explicitly model discrete\nboolean random variables. Also, very short texts are likely to have noisy\ntf\u2013idf values while the binary occurrence info is more stable.\n\nAs usual the best way to adjust the feature extraction parameters is to use a\ncross-validated grid search, for instance by pipelining the feature extractor\nwith a classifier:\n\nText is made of characters, but files are made of bytes. These bytes represent\ncharacters according to some encoding. To work with text files in Python,\ntheir bytes must be decoded to a character set called Unicode. Common\nencodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the\nuniversal encodings UTF-8 and UTF-16. Many others exist.\n\nNote\n\nAn encoding can also be called a \u2018character set\u2019, but this term is less\naccurate: several encodings can exist for a single character set.\n\nThe text feature extractors in scikit-learn know how to decode text files, but\nonly if you tell them what encoding the files are in. The `CountVectorizer`\ntakes an `encoding` parameter for this purpose. For modern text files, the\ncorrect encoding is probably UTF-8, which is therefore the default\n(`encoding=\"utf-8\"`).\n\nIf the text you are loading is not actually encoded with UTF-8, however, you\nwill get a `UnicodeDecodeError`. The vectorizers can be told to be silent\nabout decoding errors by setting the `decode_error` parameter to either\n`\"ignore\"` or `\"replace\"`. See the documentation for the Python function\n`bytes.decode` for more details (type `help(bytes.decode)` at the Python\nprompt).\n\nIf you are having trouble decoding text, here are some things to try:\n\nFor example, the following snippet uses `chardet` (not shipped with scikit-\nlearn, must be installed separately) to figure out the encoding of three\ntexts. It then vectorizes the texts and prints the learned vocabulary. The\noutput is not shown here.\n\n(Depending on the version of `chardet`, it might get the first one wrong.)\n\nFor an introduction to Unicode and character encodings in general, see Joel\nSpolsky\u2019s Absolute Minimum Every Software Developer Must Know About Unicode.\n\nThe bag of words representation is quite simplistic but surprisingly useful in\npractice.\n\nIn particular in a supervised setting it can be successfully combined with\nfast and scalable linear models to train document classifiers, for instance:\n\nIn an unsupervised setting it can be used to group similar documents together\nby applying clustering algorithms such as K-means:\n\nFinally it is possible to discover the main topics of a corpus by relaxing the\nhard assignment constraint of clustering, for instance by using Non-negative\nmatrix factorization (NMF or NNMF):\n\nA collection of unigrams (what bag of words is) cannot capture phrases and\nmulti-word expressions, effectively disregarding any word order dependence.\nAdditionally, the bag of words model doesn\u2019t account for potential\nmisspellings or word derivations.\n\nN-grams to the rescue! Instead of building a simple collection of unigrams\n(n=1), one might prefer a collection of bigrams (n=2), where occurrences of\npairs of consecutive words are counted.\n\nOne might alternatively consider a collection of character n-grams, a\nrepresentation resilient against misspellings and derivations.\n\nFor example, let\u2019s say we\u2019re dealing with a corpus of two documents:\n`['words', 'wprds']`. The second document contains a misspelling of the word\n\u2018words\u2019. A simple bag of words representation would consider these two as very\ndistinct documents, differing in both of the two possible features. A\ncharacter 2-gram representation, however, would find the documents matching in\n4 out of 8 features, which may help the preferred classifier decide better:\n\nIn the above example, `char_wb` analyzer is used, which creates n-grams only\nfrom characters inside word boundaries (padded with space on each side). The\n`char` analyzer, alternatively, creates n-grams that span across words:\n\nThe word boundaries-aware variant `char_wb` is especially interesting for\nlanguages that use white-spaces for word separation as it generates\nsignificantly less noisy features than the raw `char` variant in that case.\nFor such languages it can increase both the predictive accuracy and\nconvergence speed of classifiers trained using such features while retaining\nthe robustness with regards to misspellings and word derivations.\n\nWhile some local positioning information can be preserved by extracting\nn-grams instead of individual words, bag of words and bag of n-grams destroy\nmost of the inner structure of the document and hence most of the meaning\ncarried by that internal structure.\n\nIn order to address the wider task of Natural Language Understanding, the\nlocal structure of sentences and paragraphs should thus be taken into account.\nMany such models will thus be casted as \u201cStructured output\u201d problems which are\ncurrently outside of the scope of scikit-learn.\n\nThe above vectorization scheme is simple but the fact that it holds an in-\nmemory mapping from the string tokens to the integer feature indices (the\n`vocabulary_` attribute) causes several problems when dealing with large\ndatasets:\n\nIt is possible to overcome those limitations by combining the \u201chashing trick\u201d\n(Feature hashing) implemented by the `FeatureHasher` class and the text\npreprocessing and tokenization features of the `CountVectorizer`.\n\nThis combination is implementing in `HashingVectorizer`, a transformer class\nthat is mostly API compatible with `CountVectorizer`. `HashingVectorizer` is\nstateless, meaning that you don\u2019t have to call `fit` on it:\n\nYou can see that 16 non-zero feature tokens were extracted in the vector\noutput: this is less than the 19 non-zeros extracted previously by the\n`CountVectorizer` on the same toy corpus. The discrepancy comes from hash\nfunction collisions because of the low value of the `n_features` parameter.\n\nIn a real world setting, the `n_features` parameter can be left to its default\nvalue of `2 ** 20` (roughly one million possible features). If memory or\ndownstream models size is an issue selecting a lower value such as `2 ** 18`\nmight help without introducing too many additional collisions on typical text\nclassification tasks.\n\nNote that the dimensionality does not affect the CPU training time of\nalgorithms which operate on CSR matrices (`LinearSVC(dual=True)`,\n`Perceptron`, `SGDClassifier`, `PassiveAggressive`) but it does for algorithms\nthat work with CSC matrices (`LinearSVC(dual=False)`, `Lasso()`, etc).\n\nLet\u2019s try again with the default setting:\n\nWe no longer get the collisions, but this comes at the expense of a much\nlarger dimensionality of the output space. Of course, other terms than the 19\nused here might still collide with each other.\n\nThe `HashingVectorizer` also comes with the following limitations:\n\nAn interesting development of using a `HashingVectorizer` is the ability to\nperform out-of-core scaling. This means that we can learn from data that does\nnot fit into the computer\u2019s main memory.\n\nA strategy to implement out-of-core scaling is to stream data to the estimator\nin mini-batches. Each mini-batch is vectorized using `HashingVectorizer` so as\nto guarantee that the input space of the estimator has always the same\ndimensionality. The amount of memory used at any time is thus bounded by the\nsize of a mini-batch. Although there is no limit to the amount of data that\ncan be ingested using such an approach, from a practical point of view the\nlearning time is often limited by the CPU time one wants to spend on the task.\n\nFor a full-fledged example of out-of-core scaling in a text classification\ntask see Out-of-core classification of text documents.\n\nIt is possible to customize the behavior by passing a callable to the\nvectorizer constructor:\n\nIn particular we name:\n\n(Lucene users might recognize these names, but be aware that scikit-learn\nconcepts may not map one-to-one onto Lucene concepts.)\n\nTo make the preprocessor, tokenizer and analyzers aware of the model\nparameters it is possible to derive from the class and override the\n`build_preprocessor`, `build_tokenizer` and `build_analyzer` factory methods\ninstead of passing custom functions.\n\nSome tips and tricks:\n\nFancy token-level analysis such as stemming, lemmatizing, compound splitting,\nfiltering based on part-of-speech, etc. are not included in the scikit-learn\ncodebase, but can be added by customizing either the tokenizer or the\nanalyzer. Here\u2019s a `CountVectorizer` with a tokenizer and lemmatizer using\nNLTK:\n\n(Note that this will not filter out punctuation.)\n\nThe following example will, for instance, transform some British spelling to\nAmerican spelling:\n\nfor other styles of preprocessing; examples include stemming, lemmatization,\nor normalizing numerical tokens, with the latter illustrated in:\n\nCustomizing the vectorizer can also be useful when handling Asian languages\nthat do not use an explicit word separator such as whitespace.\n\nThe `extract_patches_2d` function extracts patches from an image stored as a\ntwo-dimensional array, or three-dimensional with color information along the\nthird axis. For rebuilding an image from all its patches, use\n`reconstruct_from_patches_2d`. For example let use generate a 4x4 pixel\npicture with 3 color channels (e.g. in RGB format):\n\nLet us now try to reconstruct the original image from the patches by averaging\non overlapping areas:\n\nThe `PatchExtractor` class works in the same way as `extract_patches_2d`, only\nit supports multiple images as input. It is implemented as an estimator, so it\ncan be used in pipelines. See:\n\nSeveral estimators in the scikit-learn can use connectivity information\nbetween features or samples. For instance Ward clustering (Hierarchical\nclustering) can cluster together only neighboring pixels of an image, thus\nforming contiguous patches:\n\nFor this purpose, the estimators use a \u2018connectivity\u2019 matrix, giving which\nsamples are connected.\n\nThe function `img_to_graph` returns such a matrix from a 2D or 3D image.\nSimilarly, `grid_to_graph` build a connectivity matrix for images given the\nshape of these image.\n\nThese matrices can be used to impose connectivity in estimators that use\nconnectivity information, such as Ward clustering (Hierarchical clustering),\nbut also to build precomputed kernels, or similarity matrices.\n\nNote\n\nExamples\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.3. Preprocessing data", "path": "modules/preprocessing", "type": "Guide", "text": "\nThe `sklearn.preprocessing` package provides several common utility functions\nand transformer classes to change raw feature vectors into a representation\nthat is more suitable for the downstream estimators.\n\nIn general, learning algorithms benefit from standardization of the data set.\nIf some outliers are present in the set, robust scalers or transformers are\nmore appropriate. The behaviors of the different scalers, transformers, and\nnormalizers on a dataset containing marginal outliers is highlighted in\nCompare the effect of different scalers on data with outliers.\n\nStandardization of datasets is a common requirement for many machine learning\nestimators implemented in scikit-learn; they might behave badly if the\nindividual features do not more or less look like standard normally\ndistributed data: Gaussian with zero mean and unit variance.\n\nIn practice we often ignore the shape of the distribution and just transform\nthe data to center it by removing the mean value of each feature, then scale\nit by dividing non-constant features by their standard deviation.\n\nFor instance, many elements used in the objective function of a learning\nalgorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2\nregularizers of linear models) assume that all features are centered around\nzero and have variance in the same order. If a feature has a variance that is\norders of magnitude larger than others, it might dominate the objective\nfunction and make the estimator unable to learn from other features correctly\nas expected.\n\nThe `preprocessing` module provides the `StandardScaler` utility class, which\nis a quick and easy way to perform the following operation on an array-like\ndataset:\n\nScaled data has zero mean and unit variance:\n\nThis class implements the `Transformer` API to compute the mean and standard\ndeviation on a training set so as to be able to later re-apply the same\ntransformation on the testing set. This class is hence suitable for use in the\nearly steps of a `Pipeline`:\n\nIt is possible to disable either centering or scaling by either passing\n`with_mean=False` or `with_std=False` to the constructor of `StandardScaler`.\n\nAn alternative standardization is scaling features to lie between a given\nminimum and maximum value, often between zero and one, or so that the maximum\nabsolute value of each feature is scaled to unit size. This can be achieved\nusing `MinMaxScaler` or `MaxAbsScaler`, respectively.\n\nThe motivation to use this scaling include robustness to very small standard\ndeviations of features and preserving zero entries in sparse data.\n\nHere is an example to scale a toy data matrix to the `[0, 1]` range:\n\nThe same instance of the transformer can then be applied to some new test data\nunseen during the fit call: the same scaling and shifting operations will be\napplied to be consistent with the transformation performed on the train data:\n\nIt is possible to introspect the scaler attributes to find about the exact\nnature of the transformation learned on the training data:\n\nIf `MinMaxScaler` is given an explicit `feature_range=(min, max)` the full\nformula is:\n\n`MaxAbsScaler` works in a very similar fashion, but scales in a way that the\ntraining data lies within the range `[-1, 1]` by dividing through the largest\nmaximum value in each feature. It is meant for data that is already centered\nat zero or sparse data.\n\nHere is how to use the toy data from the previous example with this scaler:\n\nCentering sparse data would destroy the sparseness structure in the data, and\nthus rarely is a sensible thing to do. However, it can make sense to scale\nsparse inputs, especially if features are on different scales.\n\n`MaxAbsScaler` was specifically designed for scaling sparse data, and is the\nrecommended way to go about this. However, `StandardScaler` can accept\n`scipy.sparse` matrices as input, as long as `with_mean=False` is explicitly\npassed to the constructor. Otherwise a `ValueError` will be raised as silently\ncentering would break the sparsity and would often crash the execution by\nallocating excessive amounts of memory unintentionally. `RobustScaler` cannot\nbe fitted to sparse inputs, but you can use the `transform` method on sparse\ninputs.\n\nNote that the scalers accept both Compressed Sparse Rows and Compressed Sparse\nColumns format (see `scipy.sparse.csr_matrix` and `scipy.sparse.csc_matrix`).\nAny other sparse input will be converted to the Compressed Sparse Rows\nrepresentation. To avoid unnecessary memory copies, it is recommended to\nchoose the CSR or CSC representation upstream.\n\nFinally, if the centered data is expected to be small enough, explicitly\nconverting the input to an array using the `toarray` method of sparse matrices\nis another option.\n\nIf your data contains many outliers, scaling using the mean and variance of\nthe data is likely to not work very well. In these cases, you can use\n`RobustScaler` as a drop-in replacement instead. It uses more robust estimates\nfor the center and range of your data.\n\nReferences:\n\nFurther discussion on the importance of centering and scaling data is\navailable on this FAQ: Should I normalize/standardize/rescale the data?\n\nScaling vs Whitening\n\nIt is sometimes not enough to center and scale the features independently,\nsince a downstream model can further make some assumption on the linear\nindependence of the features.\n\nTo address this issue you can use `PCA` with `whiten=True` to further remove\nthe linear correlation across features.\n\nIf you have a kernel matrix of a kernel \\\\(K\\\\) that computes a dot product in\na feature space defined by function \\\\(\\phi\\\\), a `KernelCenterer` can\ntransform the kernel matrix so that it contains inner products in the feature\nspace defined by \\\\(\\phi\\\\) followed by removal of the mean in that space.\n\nTwo types of transformations are available: quantile transforms and power\ntransforms. Both quantile and power transforms are based on monotonic\ntransformations of the features and thus preserve the rank of the values along\neach feature.\n\nQuantile transforms put all features into the same desired distribution based\non the formula \\\\(G^{-1}(F(X))\\\\) where \\\\(F\\\\) is the cumulative distribution\nfunction of the feature and \\\\(G^{-1}\\\\) the quantile function of the desired\noutput distribution \\\\(G\\\\). This formula is using the two following facts:\n(i) if \\\\(X\\\\) is a random variable with a continuous cumulative distribution\nfunction \\\\(F\\\\) then \\\\(F(X)\\\\) is uniformly distributed on \\\\([0,1]\\\\); (ii)\nif \\\\(U\\\\) is a random variable with uniform distribution on \\\\([0,1]\\\\) then\n\\\\(G^{-1}(U)\\\\) has distribution \\\\(G\\\\). By performing a rank transformation,\na quantile transform smooths out unusual distributions and is less influenced\nby outliers than scaling methods. It does, however, distort correlations and\ndistances within and across features.\n\nPower transforms are a family of parametric transformations that aim to map\ndata from any distribution to as close to a Gaussian distribution.\n\n`QuantileTransformer` provides a non-parametric transformation to map the data\nto a uniform distribution with values between 0 and 1:\n\nThis feature corresponds to the sepal length in cm. Once the quantile\ntransformation applied, those landmarks approach closely the percentiles\npreviously defined:\n\nThis can be confirmed on a independent testing set with similar remarks:\n\nIn many modeling scenarios, normality of the features in a dataset is\ndesirable. Power transforms are a family of parametric, monotonic\ntransformations that aim to map data from any distribution to as close to a\nGaussian distribution as possible in order to stabilize variance and minimize\nskewness.\n\n`PowerTransformer` currently provides two such power transformations, the Yeo-\nJohnson transform and the Box-Cox transform.\n\nThe Yeo-Johnson transform is given by:\n\nwhile the Box-Cox transform is given by:\n\nBox-Cox can only be applied to strictly positive data. In both methods, the\ntransformation is parameterized by \\\\(\\lambda\\\\), which is determined through\nmaximum likelihood estimation. Here is an example of using Box-Cox to map\nsamples drawn from a lognormal distribution to a normal distribution:\n\nWhile the above example sets the `standardize` option to `False`,\n`PowerTransformer` will apply zero-mean, unit-variance normalization to the\ntransformed output by default.\n\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\ndistributions. Note that when applied to certain distributions, the power\ntransforms achieve very Gaussian-like results, but with others, they are\nineffective. This highlights the importance of visualizing the data before and\nafter transformation.\n\nIt is also possible to map data to a normal distribution using\n`QuantileTransformer` by setting `output_distribution='normal'`. Using the\nearlier example with the iris dataset:\n\nThus the median of the input becomes the mean of the output, centered at 0.\nThe normal output is clipped so that the input\u2019s minimum and maximum \u2014\ncorresponding to the 1e-7 and 1 - 1e-7 quantiles respectively \u2014 do not become\ninfinite under the transformation.\n\nNormalization is the process of scaling individual samples to have unit norm.\nThis process can be useful if you plan to use a quadratic form such as the\ndot-product or any other kernel to quantify the similarity of any pair of\nsamples.\n\nThis assumption is the base of the Vector Space Model often used in text\nclassification and clustering contexts.\n\nThe function `normalize` provides a quick and easy way to perform this\noperation on a single array-like dataset, either using the `l1`, `l2`, or\n`max` norms:\n\nThe `preprocessing` module further provides a utility class `Normalizer` that\nimplements the same operation using the `Transformer` API (even though the\n`fit` method is useless in this case: the class is stateless as this operation\ntreats samples independently).\n\nThis class is hence suitable for use in the early steps of a `Pipeline`:\n\nThe normalizer instance can then be used on sample vectors as any transformer:\n\nNote: L2 normalization is also known as spatial sign preprocessing.\n\nSparse input\n\n`normalize` and `Normalizer` accept both dense array-like and sparse matrices\nfrom scipy.sparse as input.\n\nFor sparse input the data is converted to the Compressed Sparse Rows\nrepresentation (see `scipy.sparse.csr_matrix`) before being fed to efficient\nCython routines. To avoid unnecessary memory copies, it is recommended to\nchoose the CSR representation upstream.\n\nOften features are not given as continuous values but categorical. For example\na person could have features `[\"male\", \"female\"]`, `[\"from Europe\", \"from US\",\n\"from Asia\"]`, `[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet\nExplorer\"]`. Such features can be efficiently coded as integers, for instance\n`[\"male\", \"from US\", \"uses Internet Explorer\"]` could be expressed as `[0, 1,\n3]` while `[\"female\", \"from Asia\", \"uses Chrome\"]` would be `[1, 2, 1]`.\n\nTo convert categorical features to such integer codes, we can use the\n`OrdinalEncoder`. This estimator transforms each categorical feature to one\nnew feature of integers (0 to n_categories - 1):\n\nSuch integer representation can, however, not be used directly with all\nscikit-learn estimators, as these expect continuous input, and would interpret\nthe categories as being ordered, which is often not desired (i.e. the set of\nbrowsers was ordered arbitrarily).\n\nAnother possibility to convert categorical features to features that can be\nused with scikit-learn estimators is to use a one-of-K, also known as one-hot\nor dummy encoding. This type of encoding can be obtained with the\n`OneHotEncoder`, which transforms each categorical feature with `n_categories`\npossible values into `n_categories` binary features, with one of them 1, and\nall others 0.\n\nContinuing the example above:\n\nBy default, the values each feature can take is inferred automatically from\nthe dataset and can be found in the `categories_` attribute:\n\nIt is possible to specify this explicitly using the parameter `categories`.\nThere are two genders, four possible continents and four web browsers in our\ndataset:\n\nIf there is a possibility that the training data might have missing\ncategorical features, it can often be better to specify\n`handle_unknown='ignore'` instead of setting the `categories` manually as\nabove. When `handle_unknown='ignore'` is specified and unknown categories are\nencountered during transform, no error will be raised but the resulting one-\nhot encoded columns for this feature will be all zeros\n(`handle_unknown='ignore'` is only supported for one-hot encoding):\n\nIt is also possible to encode each column into `n_categories - 1` columns\ninstead of `n_categories` columns by using the `drop` parameter. This\nparameter allows the user to specify a category for each feature to be\ndropped. This is useful to avoid co-linearity in the input matrix in some\nclassifiers. Such functionality is useful, for example, when using non-\nregularized regression (`LinearRegression`), since co-linearity would cause\nthe covariance matrix to be non-invertible. When this parameter is not None,\n`handle_unknown` must be set to `error`:\n\nOne might want to drop one of the two columns only for features with 2\ncategories. In this case, you can set the parameter `drop='if_binary'`.\n\nIn the transformed `X`, the first column is the encoding of the feature with\ncategories \u201cmale\u201d/\u201dfemale\u201d, while the remaining 6 columns is the encoding of\nthe 2 features with respectively 3 categories each.\n\n`OneHotEncoder` supports categorical features with missing values by\nconsidering the missing values as an additional category:\n\nIf a feature contains both `np.nan` and `None`, they will be considered\nseparate categories:\n\nSee Loading features from dicts for categorical features that are represented\nas a dict, not as scalars.\n\nDiscretization (otherwise known as quantization or binning) provides a way to\npartition continuous features into discrete values. Certain datasets with\ncontinuous features may benefit from discretization, because discretization\ncan transform the dataset of continuous attributes to one with only nominal\nattributes.\n\nOne-hot encoded discretized features can make a model more expressive, while\nmaintaining interpretability. For instance, pre-processing with a discretizer\ncan introduce nonlinearity to linear models.\n\n`KBinsDiscretizer` discretizes features into `k` bins:\n\nBy default the output is one-hot encoded into a sparse matrix (See Encoding\ncategorical features) and this can be configured with the `encode` parameter.\nFor each feature, the bin edges are computed during `fit` and together with\nthe number of bins, they will define the intervals. Therefore, for the current\nexample, these intervals are defined as:\n\nBased on these bin intervals, `X` is transformed as follows:\n\nThe resulting dataset contains ordinal attributes which can be further used in\na `Pipeline`.\n\nDiscretization is similar to constructing histograms for continuous data.\nHowever, histograms focus on counting features which fall into particular\nbins, whereas discretization focuses on assigning feature values to these\nbins.\n\n`KBinsDiscretizer` implements different binning strategies, which can be\nselected with the `strategy` parameter. The \u2018uniform\u2019 strategy uses constant-\nwidth bins. The \u2018quantile\u2019 strategy uses the quantiles values to have equally\npopulated bins in each feature. The \u2018kmeans\u2019 strategy defines bins based on a\nk-means clustering procedure performed on each feature independently.\n\nBe aware that one can specify custom bins by passing a callable defining the\ndiscretization strategy to `FunctionTransformer`. For instance, we can use the\nPandas function `pandas.cut`:\n\nExamples:\n\nFeature binarization is the process of thresholding numerical features to get\nboolean values. This can be useful for downstream probabilistic estimators\nthat make assumption that the input data is distributed according to a multi-\nvariate Bernoulli distribution. For instance, this is the case for the\n`BernoulliRBM`.\n\nIt is also common among the text processing community to use binary feature\nvalues (probably to simplify the probabilistic reasoning) even if normalized\ncounts (a.k.a. term frequencies) or TF-IDF valued features often perform\nslightly better in practice.\n\nAs for the `Normalizer`, the utility class `Binarizer` is meant to be used in\nthe early stages of `Pipeline`. The `fit` method does nothing as each sample\nis treated independently of others:\n\nIt is possible to adjust the threshold of the binarizer:\n\nAs for the `Normalizer` class, the preprocessing module provides a companion\nfunction `binarize` to be used when the transformer API is not necessary.\n\nNote that the `Binarizer` is similar to the `KBinsDiscretizer` when `k = 2`,\nand when the bin edge is at the value `threshold`.\n\nSparse input\n\n`binarize` and `Binarizer` accept both dense array-like and sparse matrices\nfrom scipy.sparse as input.\n\nFor sparse input the data is converted to the Compressed Sparse Rows\nrepresentation (see `scipy.sparse.csr_matrix`). To avoid unnecessary memory\ncopies, it is recommended to choose the CSR representation upstream.\n\nTools for imputing missing values are discussed at Imputation of missing\nvalues.\n\nOften it\u2019s useful to add complexity to the model by considering nonlinear\nfeatures of the input data. A simple and common method to use is polynomial\nfeatures, which can get features\u2019 high-order and interaction terms. It is\nimplemented in `PolynomialFeatures`:\n\nThe features of X have been transformed from \\\\((X_1, X_2)\\\\) to \\\\((1, X_1,\nX_2, X_1^2, X_1X_2, X_2^2)\\\\).\n\nIn some cases, only interaction terms among features are required, and it can\nbe gotten with the setting `interaction_only=True`:\n\nThe features of X have been transformed from \\\\((X_1, X_2, X_3)\\\\) to \\\\((1,\nX_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\\\\).\n\nNote that polynomial features are used implicitly in kernel methods (e.g.,\n`SVC`, `KernelPCA`) when using polynomial Kernel functions.\n\nSee Polynomial interpolation for Ridge regression using created polynomial\nfeatures.\n\nOften, you will want to convert an existing Python function into a transformer\nto assist in data cleaning or processing. You can implement a transformer from\nan arbitrary function with `FunctionTransformer`. For example, to build a\ntransformer that applies a log transformation in a pipeline, do:\n\nYou can ensure that `func` and `inverse_func` are the inverse of each other by\nsetting `check_inverse=True` and calling `fit` before `transform`. Please note\nthat a warning is raised and can be turned into an error with a\n`filterwarnings`:\n\nFor a full code example that demonstrates using a `FunctionTransformer` to\nextract features from text data see Column Transformer with Heterogeneous Data\nSources\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.4. Imputation of missing values", "path": "modules/impute", "type": "Guide", "text": "\nFor various reasons, many real world datasets contain missing values, often\nencoded as blanks, NaNs or other placeholders. Such datasets however are\nincompatible with scikit-learn estimators which assume that all values in an\narray are numerical, and that all have and hold meaning. A basic strategy to\nuse incomplete datasets is to discard entire rows and/or columns containing\nmissing values. However, this comes at the price of losing data which may be\nvaluable (even though incomplete). A better strategy is to impute the missing\nvalues, i.e., to infer them from the known part of the data. See the Glossary\nof Common Terms and API Elements entry on imputation.\n\nOne type of imputation algorithm is univariate, which imputes values in the\ni-th feature dimension using only non-missing values in that feature dimension\n(e.g. `impute.SimpleImputer`). By contrast, multivariate imputation algorithms\nuse the entire set of available feature dimensions to estimate the missing\nvalues (e.g. `impute.IterativeImputer`).\n\nThe `SimpleImputer` class provides basic strategies for imputing missing\nvalues. Missing values can be imputed with a provided constant value, or using\nthe statistics (mean, median or most frequent) of each column in which the\nmissing values are located. This class also allows for different missing\nvalues encodings.\n\nThe following snippet demonstrates how to replace missing values, encoded as\n`np.nan`, using the mean value of the columns (axis 0) that contain the\nmissing values:\n\nThe `SimpleImputer` class also supports sparse matrices:\n\nNote that this format is not meant to be used to implicitly store missing\nvalues in the matrix because it would densify it at transform time. Missing\nvalues encoded by 0 must be used with dense input.\n\nThe `SimpleImputer` class also supports categorical data represented as string\nvalues or pandas categoricals when using the `'most_frequent'` or `'constant'`\nstrategy:\n\nA more sophisticated approach is to use the `IterativeImputer` class, which\nmodels each feature with missing values as a function of other features, and\nuses that estimate for imputation. It does so in an iterated round-robin\nfashion: at each step, a feature column is designated as output `y` and the\nother feature columns are treated as inputs `X`. A regressor is fit on `(X,\ny)` for known `y`. Then, the regressor is used to predict the missing values\nof `y`. This is done for each feature in an iterative fashion, and then is\nrepeated for `max_iter` imputation rounds. The results of the final imputation\nround are returned.\n\nNote\n\nThis estimator is still experimental for now: default parameters or details of\nbehaviour might change without any deprecation cycle. Resolving the following\nissues would help stabilize `IterativeImputer`: convergence criteria (#14338),\ndefault estimators (#13286), and use of random state (#15611). To use it, you\nneed to explicitly import `enable_iterative_imputer`.\n\nBoth `SimpleImputer` and `IterativeImputer` can be used in a Pipeline as a way\nto build a composite estimator that supports imputation. See Imputing missing\nvalues before building an estimator.\n\nThere are many well-established imputation packages in the R data science\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\nout to be a particular instance of different sequential imputation algorithms\nthat can all be implemented with `IterativeImputer` by passing in different\nregressors to be used for predicting missing feature values. In the case of\nmissForest, this regressor is a Random Forest. See Imputing missing values\nwith variants of IterativeImputer.\n\nIn the statistics community, it is common practice to perform multiple\nimputations, generating, for example, `m` separate imputations for a single\nfeature matrix. Each of these `m` imputations is then put through the\nsubsequent analysis pipeline (e.g. feature engineering, clustering,\nregression, classification). The `m` final analysis results (e.g. held-out\nvalidation errors) allow the data scientist to obtain understanding of how\nanalytic results may differ as a consequence of the inherent uncertainty\ncaused by the missing values. The above practice is called multiple\nimputation.\n\nOur implementation of `IterativeImputer` was inspired by the R MICE package\n(Multivariate Imputation by Chained Equations) 1, but differs from it by\nreturning a single imputation instead of multiple imputations. However,\n`IterativeImputer` can also be used for multiple imputations by applying it\nrepeatedly to the same dataset with different random seeds when\n`sample_posterior=True`. See 2, chapter 4 for more discussion on multiple vs.\nsingle imputations.\n\nIt is still an open problem as to how useful single vs. multiple imputation is\nin the context of prediction and classification when the user is not\ninterested in measuring uncertainty due to missing values.\n\nNote that a call to the `transform` method of `IterativeImputer` is not\nallowed to change the number of samples. Therefore multiple imputations cannot\nbe achieved by a single call to `transform`.\n\nStef van Buuren, Karin Groothuis-Oudshoorn (2011). \u201cmice: Multivariate\nImputation by Chained Equations in R\u201d. Journal of Statistical Software 45:\n1-67.\n\nRoderick J A Little and Donald B Rubin (1986). \u201cStatistical Analysis with\nMissing Data\u201d. John Wiley & Sons, Inc., New York, NY, USA.\n\nThe `KNNImputer` class provides imputation for filling in missing values using\nthe k-Nearest Neighbors approach. By default, a euclidean distance metric that\nsupports missing values, `nan_euclidean_distances`, is used to find the\nnearest neighbors. Each missing feature is imputed using values from\n`n_neighbors` nearest neighbors that have a value for the feature. The feature\nof the neighbors are averaged uniformly or weighted by distance to each\nneighbor. If a sample has more than one feature missing, then the neighbors\nfor that sample can be different depending on the particular feature being\nimputed. When the number of available neighbors is less than `n_neighbors` and\nthere are no defined distances to the training set, the training set average\nfor that feature is used during imputation. If there is at least one neighbor\nwith a defined distance, the weighted or unweighted average of the remaining\nneighbors will be used during imputation. If a feature is always missing in\ntraining, it is removed during `transform`. For more information on the\nmethodology, see ref. [OL2001].\n\nThe following snippet demonstrates how to replace missing values, encoded as\n`np.nan`, using the mean feature value of the two nearest neighbors of samples\nwith missing values:\n\nOlga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown, Trevor Hastie,\nRobert Tibshirani, David Botstein and Russ B. Altman, Missing value estimation\nmethods for DNA microarrays, BIOINFORMATICS Vol. 17 no. 6, 2001 Pages 520-525.\n\nThe `MissingIndicator` transformer is useful to transform a dataset into\ncorresponding binary matrix indicating the presence of missing values in the\ndataset. This transformation is useful in conjunction with imputation. When\nusing imputation, preserving the information about which values had been\nmissing can be informative. Note that both the `SimpleImputer` and\n`IterativeImputer` have the boolean parameter `add_indicator` (`False` by\ndefault) which when set to `True` provides a convenient way of stacking the\noutput of the `MissingIndicator` transformer with the output of the imputer.\n\n`NaN` is usually used as the placeholder for missing values. However, it\nenforces the data type to be float. The parameter `missing_values` allows to\nspecify other placeholder such as integer. In the following example, we will\nuse `-1` as missing values:\n\nThe `features` parameter is used to choose the features for which the mask is\nconstructed. By default, it is `'missing-only'` which returns the imputer mask\nof the features containing missing values at `fit` time:\n\nThe `features` parameter can be set to `'all'` to return all features whether\nor not they contain missing values:\n\nWhen using the `MissingIndicator` in a `Pipeline`, be sure to use the\n`FeatureUnion` or `ColumnTransformer` to add the indicator features to the\nregular features. First we obtain the `iris` dataset, and add some missing\nvalues to it.\n\nNow we create a `FeatureUnion`. All features will be imputed using\n`SimpleImputer`, in order to enable classifiers to work with this data.\nAdditionally, it adds the indicator variables from `MissingIndicator`.\n\nOf course, we cannot use the transformer to make any predictions. We should\nwrap this in a `Pipeline` with a classifier (e.g., a `DecisionTreeClassifier`)\nto be able to make predictions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.5. Unsupervised dimensionality reduction", "path": "modules/unsupervised_reduction", "type": "Guide", "text": "\nIf your number of features is high, it may be useful to reduce it with an\nunsupervised step prior to supervised steps. Many of the Unsupervised learning\nmethods implement a `transform` method that can be used to reduce the\ndimensionality. Below we discuss two specific example of this pattern that are\nheavily used.\n\nPipelining\n\nThe unsupervised data reduction and the supervised estimator can be chained in\none step. See Pipeline: chaining estimators.\n\n`decomposition.PCA` looks for a combination of features that capture well the\nvariance of the original features. See Decomposing signals in components\n(matrix factorization problems).\n\nExamples\n\nThe module: `random_projection` provides several tools for data reduction by\nrandom projections. See the relevant section of the documentation: Random\nProjection.\n\nExamples\n\n`cluster.FeatureAgglomeration` applies Hierarchical clustering to group\ntogether features that behave similarly.\n\nExamples\n\nFeature scaling\n\nNote that if features have very different scaling or statistical properties,\n`cluster.FeatureAgglomeration` may not be able to capture the links between\nrelated features. Using a `preprocessing.StandardScaler` can be useful in\nthese settings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.6. Random Projection", "path": "modules/random_projection", "type": "Guide", "text": "\nThe `sklearn.random_projection` module implements a simple and computationally\nefficient way to reduce the dimensionality of the data by trading a controlled\namount of accuracy (as additional variance) for faster processing times and\nsmaller model sizes. This module implements two types of unstructured random\nmatrix: Gaussian random matrix and sparse random matrix.\n\nThe dimensions and distribution of random projections matrices are controlled\nso as to preserve the pairwise distances between any two samples of the\ndataset. Thus random projection is a suitable approximation technique for\ndistance based method.\n\nReferences:\n\nThe main theoretical result behind the efficiency of random projection is the\nJohnson-Lindenstrauss lemma (quoting Wikipedia):\n\nIn mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-\ndistortion embeddings of points from high-dimensional into low-dimensional\nEuclidean space. The lemma states that a small set of points in a high-\ndimensional space can be embedded into a space of much lower dimension in such\na way that distances between the points are nearly preserved. The map used for\nthe embedding is at least Lipschitz, and can even be taken to be an orthogonal\nprojection.\n\nKnowing only the number of samples, the `johnson_lindenstrauss_min_dim`\nestimates conservatively the minimal size of the random subspace to guarantee\na bounded distortion introduced by the random projection:\n\nExample:\n\nReferences:\n\nThe `GaussianRandomProjection` reduces the dimensionality by projecting the\noriginal input space on a randomly generated matrix where components are drawn\nfrom the following distribution \\\\(N(0, \\frac{1}{n_{components}})\\\\).\n\nHere a small excerpt which illustrates how to use the Gaussian random\nprojection transformer:\n\nThe `SparseRandomProjection` reduces the dimensionality by projecting the\noriginal input space using a sparse random matrix.\n\nSparse random matrices are an alternative to dense Gaussian random projection\nmatrix that guarantees similar embedding quality while being much more memory\nefficient and allowing faster computation of the projected data.\n\nIf we define `s = 1 / density`, the elements of the random matrix are drawn\nfrom\n\nwhere \\\\(n_{\\text{components}}\\\\) is the size of the projected subspace. By\ndefault the density of non zero elements is set to the minimum density as\nrecommended by Ping Li et al.: \\\\(1 / \\sqrt{n_{\\text{features}}}\\\\).\n\nHere a small excerpt which illustrates how to use the sparse random projection\ntransformer:\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.7. Kernel Approximation", "path": "modules/kernel_approximation", "type": "Guide", "text": "\nThis submodule contains functions that approximate the feature mappings that\ncorrespond to certain kernels, as they are used for example in support vector\nmachines (see Support Vector Machines). The following feature functions\nperform non-linear transformations of the input, which can serve as a basis\nfor linear classification or other algorithms.\n\nThe advantage of using approximate explicit feature maps compared to the\nkernel trick, which makes use of feature maps implicitly, is that explicit\nmappings can be better suited for online learning and can significantly reduce\nthe cost of learning with very large datasets. Standard kernelized SVMs do not\nscale well to large datasets, but using an approximate kernel map it is\npossible to use much more efficient linear SVMs. In particular, the\ncombination of kernel map approximations with `SGDClassifier` can make non-\nlinear learning on large datasets possible.\n\nSince there has not been much empirical work using approximate embeddings, it\nis advisable to compare results against exact kernel methods when possible.\n\nSee also\n\nPolynomial regression: extending linear models with basis functions for an\nexact polynomial transformation.\n\nThe Nystroem method, as implemented in `Nystroem` is a general method for low-\nrank approximations of kernels. It achieves this by essentially subsampling\nthe data on which the kernel is evaluated. By default `Nystroem` uses the\n`rbf` kernel, but it can use any kernel function or a precomputed kernel\nmatrix. The number of samples used - which is also the dimensionality of the\nfeatures computed - is given by the parameter `n_components`.\n\nThe `RBFSampler` constructs an approximate mapping for the radial basis\nfunction kernel, also known as Random Kitchen Sinks [RR2007]. This\ntransformation can be used to explicitly model a kernel map, prior to applying\na linear algorithm, for example a linear SVM:\n\nThe mapping relies on a Monte Carlo approximation to the kernel values. The\n`fit` function performs the Monte Carlo sampling, whereas the `transform`\nmethod performs the mapping of the data. Because of the inherent randomness of\nthe process, results may vary between different calls to the `fit` function.\n\nThe `fit` function takes two arguments: `n_components`, which is the target\ndimensionality of the feature transform, and `gamma`, the parameter of the\nRBF-kernel. A higher `n_components` will result in a better approximation of\nthe kernel and will yield results more similar to those produced by a kernel\nSVM. Note that \u201cfitting\u201d the feature function does not actually depend on the\ndata given to the `fit` function. Only the dimensionality of the data is used.\nDetails on the method can be found in [RR2007].\n\nFor a given value of `n_components` `RBFSampler` is often less accurate as\n`Nystroem`. `RBFSampler` is cheaper to compute, though, making use of larger\nfeature spaces more efficient.\n\nComparing an exact RBF kernel (left) with the approximation (right)\n\nExamples:\n\nThe additive chi squared kernel is a kernel on histograms, often used in\ncomputer vision.\n\nThe additive chi squared kernel as used here is given by\n\nThis is not exactly the same as `sklearn.metrics.additive_chi2_kernel`. The\nauthors of [VZ2010] prefer the version above as it is always positive\ndefinite. Since the kernel is additive, it is possible to treat all components\n\\\\(x_i\\\\) separately for embedding. This makes it possible to sample the\nFourier transform in regular intervals, instead of approximating using Monte\nCarlo sampling.\n\nThe class `AdditiveChi2Sampler` implements this component wise deterministic\nsampling. Each component is sampled \\\\(n\\\\) times, yielding \\\\(2n+1\\\\)\ndimensions per input dimension (the multiple of two stems from the real and\ncomplex part of the Fourier transform). In the literature, \\\\(n\\\\) is usually\nchosen to be 1 or 2, transforming the dataset to size `n_samples * 5 *\nn_features` (in the case of \\\\(n=2\\\\)).\n\nThe approximate feature map provided by `AdditiveChi2Sampler` can be combined\nwith the approximate feature map provided by `RBFSampler` to yield an\napproximate feature map for the exponentiated chi squared kernel. See the\n[VZ2010] for details and [VVZ2010] for combination with the `RBFSampler`.\n\nThe skewed chi squared kernel is given by:\n\nIt has properties that are similar to the exponentiated chi squared kernel\noften used in computer vision, but allows for a simple Monte Carlo\napproximation of the feature map.\n\nThe usage of the `SkewedChi2Sampler` is the same as the usage described above\nfor the `RBFSampler`. The only difference is in the free parameter, that is\ncalled \\\\(c\\\\). For a motivation for this mapping and the mathematical details\nsee [LS2010].\n\nThe polynomial kernel is a popular type of kernel function given by:\n\nwhere:\n\nIntuitively, the feature space of the polynomial kernel of degree `d` consists\nof all possible degree-`d` products among input features, which enables\nlearning algorithms using this kernel to account for interactions between\nfeatures.\n\nThe TensorSketch [PP2013] method, as implemented in `PolynomialCountSketch`,\nis a scalable, input data independent method for polynomial kernel\napproximation. It is based on the concept of Count sketch [WIKICS] [CCF2002] ,\na dimensionality reduction technique similar to feature hashing, which instead\nuses several independent hash functions. TensorSketch obtains a Count Sketch\nof the outer product of two vectors (or a vector with itself), which can be\nused as an approximation of the polynomial kernel feature space. In\nparticular, instead of explicitly computing the outer product, TensorSketch\ncomputes the Count Sketch of the vectors and then uses polynomial\nmultiplication via the Fast Fourier Transform to compute the Count Sketch of\ntheir outer product.\n\nConveniently, the training phase of TensorSketch simply consists of\ninitializing some random variables. It is thus independent of the input data,\ni.e. it only depends on the number of input features, but not the data values.\nIn addition, this method can transform samples in\n\\\\(\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}}\n\\log(n_{\\text{components}})))\\\\) time, where \\\\(n_{\\text{components}}\\\\) is\nthe desired output dimension, determined by `n_components`.\n\nExamples:\n\nKernel methods like support vector machines or kernelized PCA rely on a\nproperty of reproducing kernel Hilbert spaces. For any positive definite\nkernel function \\\\(k\\\\) (a so called Mercer kernel), it is guaranteed that\nthere exists a mapping \\\\(\\phi\\\\) into a Hilbert space \\\\(\\mathcal{H}\\\\), such\nthat\n\nWhere \\\\(\\langle \\cdot, \\cdot \\rangle\\\\) denotes the inner product in the\nHilbert space.\n\nIf an algorithm, such as a linear support vector machine or PCA, relies only\non the scalar product of data points \\\\(x_i\\\\), one may use the value of\n\\\\(k(x_i, x_j)\\\\), which corresponds to applying the algorithm to the mapped\ndata points \\\\(\\phi(x_i)\\\\). The advantage of using \\\\(k\\\\) is that the\nmapping \\\\(\\phi\\\\) never has to be calculated explicitly, allowing for\narbitrary large features (even infinite).\n\nOne drawback of kernel methods is, that it might be necessary to store many\nkernel values \\\\(k(x_i, x_j)\\\\) during optimization. If a kernelized\nclassifier is applied to new data \\\\(y_j\\\\), \\\\(k(x_i, y_j)\\\\) needs to be\ncomputed to make predictions, possibly for many different \\\\(x_i\\\\) in the\ntraining set.\n\nThe classes in this submodule allow to approximate the embedding \\\\(\\phi\\\\),\nthereby working explicitly with the representations \\\\(\\phi(x_i)\\\\), which\nobviates the need to apply the kernel or store training examples.\n\nReferences:\n\n\u201cRandom features for large-scale kernel machines\u201d Rahimi, A. and Recht, B. -\nAdvances in neural information processing 2007,\n\n\u201cRandom Fourier approximations for skewed multiplicative histogram kernels\u201d\nRandom Fourier approximations for skewed multiplicative histogram kernels -\nLecture Notes for Computer Sciencd (DAGM)\n\n\u201cEfficient additive kernels via explicit feature maps\u201d Vedaldi, A. and\nZisserman, A. - Computer Vision and Pattern Recognition 2010\n\n\u201cGeneralized RBF feature maps for Efficient Detection\u201d Vempati, S. and\nVedaldi, A. and Zisserman, A. and Jawahar, CV - 2010\n\n\u201cFast and scalable polynomial kernels via explicit feature maps\u201d Pham, N., &\nPagh, R. - 2013\n\n\u201cFinding frequent items in data streams\u201d Charikar, M., Chen, K., & Farach-\nColton - 2002\n\n\u201cWikipedia: Count sketch\u201d\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.8. Pairwise metrics, Affinities and Kernels", "path": "modules/metrics", "type": "Guide", "text": "\nThe `sklearn.metrics.pairwise` submodule implements utilities to evaluate\npairwise distances or affinity of sets of samples.\n\nThis module contains both distance metrics and kernels. A brief summary is\ngiven on the two here.\n\nDistance metrics are functions `d(a, b)` such that `d(a, b) < d(a, c)` if\nobjects `a` and `b` are considered \u201cmore similar\u201d than objects `a` and `c`.\nTwo objects exactly alike would have a distance of zero. One of the most\npopular examples is Euclidean distance. To be a \u2018true\u2019 metric, it must obey\nthe following four conditions:\n\nKernels are measures of similarity, i.e. `s(a, b) > s(a, c)` if objects `a`\nand `b` are considered \u201cmore similar\u201d than objects `a` and `c`. A kernel must\nalso be positive semi-definite.\n\nThere are a number of ways to convert between a distance metric and a\nsimilarity measure, such as a kernel. Let `D` be the distance, and `S` be the\nkernel:\n\nThe distances between the row vectors of `X` and the row vectors of `Y` can be\nevaluated using `pairwise_distances`. If `Y` is omitted the pairwise distances\nof the row vectors of `X` are calculated. Similarly,\n`pairwise.pairwise_kernels` can be used to calculate the kernel between `X`\nand `Y` using different kernel functions. See the API reference for more\ndetails.\n\n`cosine_similarity` computes the L2-normalized dot product of vectors. That\nis, if \\\\(x\\\\) and \\\\(y\\\\) are row vectors, their cosine similarity \\\\(k\\\\) is\ndefined as:\n\nThis is called cosine similarity, because Euclidean (L2) normalization\nprojects the vectors onto the unit sphere, and their dot product is then the\ncosine of the angle between the points denoted by the vectors.\n\nThis kernel is a popular choice for computing the similarity of documents\nrepresented as tf-idf vectors. `cosine_similarity` accepts `scipy.sparse`\nmatrices. (Note that the tf-idf functionality in\n`sklearn.feature_extraction.text` can produce normalized vectors, in which\ncase `cosine_similarity` is equivalent to `linear_kernel`, only slower.)\n\nReferences:\n\nThe function `linear_kernel` computes the linear kernel, that is, a special\ncase of `polynomial_kernel` with `degree=1` and `coef0=0` (homogeneous). If\n`x` and `y` are column vectors, their linear kernel is:\n\nThe function `polynomial_kernel` computes the degree-d polynomial kernel\nbetween two vectors. The polynomial kernel represents the similarity between\ntwo vectors. Conceptually, the polynomial kernels considers not only the\nsimilarity between vectors under the same dimension, but also across\ndimensions. When used in machine learning algorithms, this allows to account\nfor feature interaction.\n\nThe polynomial kernel is defined as:\n\nwhere:\n\nIf \\\\(c_0 = 0\\\\) the kernel is said to be homogeneous.\n\nThe function `sigmoid_kernel` computes the sigmoid kernel between two vectors.\nThe sigmoid kernel is also known as hyperbolic tangent, or Multilayer\nPerceptron (because, in the neural network field, it is often used as neuron\nactivation function). It is defined as:\n\nwhere:\n\nThe function `rbf_kernel` computes the radial basis function (RBF) kernel\nbetween two vectors. This kernel is defined as:\n\nwhere `x` and `y` are the input vectors. If \\\\(\\gamma = \\sigma^{-2}\\\\) the\nkernel is known as the Gaussian kernel of variance \\\\(\\sigma^2\\\\).\n\nThe function `laplacian_kernel` is a variant on the radial basis function\nkernel defined as:\n\nwhere `x` and `y` are the input vectors and \\\\(\\|x-y\\|_1\\\\) is the Manhattan\ndistance between the input vectors.\n\nIt has proven useful in ML applied to noiseless data. See e.g. Machine\nlearning for quantum mechanics in a nutshell.\n\nThe chi-squared kernel is a very popular choice for training non-linear SVMs\nin computer vision applications. It can be computed using `chi2_kernel` and\nthen passed to an `SVC` with `kernel=\"precomputed\"`:\n\nIt can also be directly used as the `kernel` argument:\n\nThe chi squared kernel is given by\n\nThe data is assumed to be non-negative, and is often normalized to have an\nL1-norm of one. The normalization is rationalized with the connection to the\nchi squared distance, which is a distance between discrete probability\ndistributions.\n\nThe chi squared kernel is most commonly used on histograms (bags) of visual\nwords.\n\nReferences:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "6.9. Transforming the prediction target", "path": "modules/preprocessing_targets", "type": "Guide", "text": "\nThese are transformers that are not intended to be used on features, only on\nsupervised learning targets. See also Transforming target in regression if you\nwant to transform the prediction target for learning, but evaluate the model\nin the original (untransformed) space.\n\n`LabelBinarizer` is a utility class to help create a label indicator matrix\nfrom a list of multiclass labels:\n\nUsing this format can enable multiclass classification in estimators that\nsupport the label indicator matrix format.\n\nWarning\n\nLabelBinarizer is not needed if you are using an estimator that already\nsupports multiclass data.\n\nFor more information about multiclass classification, refer to Multiclass\nclassification.\n\nIn multilabel learning, the joint set of binary classification tasks is\nexpressed with a label binary indicator array: each sample is one row of a 2d\narray of shape (n_samples, n_classes) with binary values where the one, i.e.\nthe non zero elements, corresponds to the subset of labels for that sample. An\narray such as `np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]])` represents label 0\nin the first sample, labels 1 and 2 in the second sample, and no labels in the\nthird sample.\n\nProducing multilabel data as a list of sets of labels may be more intuitive.\nThe `MultiLabelBinarizer` transformer can be used to convert between a\ncollection of collections of labels and the indicator format:\n\nFor more information about multilabel classification, refer to Multilabel\nclassification.\n\n`LabelEncoder` is a utility class to help normalize labels such that they\ncontain only values between 0 and n_classes-1. This is sometimes useful for\nwriting efficient Cython routines. `LabelEncoder` can be used as follows:\n\nIt can also be used to transform non-numerical labels (as long as they are\nhashable and comparable) to numerical labels:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "7. Dataset loading utilities", "path": "datasets", "type": "Guide", "text": "\nThe `sklearn.datasets` package embeds some small toy datasets as introduced in\nthe Getting Started section.\n\nThis package also features helpers to fetch larger datasets commonly used by\nthe machine learning community to benchmark algorithms on data that comes from\nthe \u2018real world\u2019.\n\nTo evaluate the impact of the scale of the dataset (`n_samples` and\n`n_features`) while controlling the statistical properties of the data\n(typically the correlation and informativeness of the features), it is also\npossible to generate synthetic data.\n\nGeneral dataset API. There are three main kinds of dataset interfaces that can\nbe used to get datasets depending on the desired type of dataset.\n\nThe dataset loaders. They can be used to load small standard datasets,\ndescribed in the Toy datasets section.\n\nThe dataset fetchers. They can be used to download and load larger datasets,\ndescribed in the Real world datasets section.\n\nBoth loaders and fetchers functions return a `Bunch` object holding at least\ntwo items: an array of shape `n_samples` * `n_features` with key `data`\n(except for 20newsgroups) and a numpy array of length `n_samples`, containing\nthe target values, with key `target`.\n\nThe Bunch object is a dictionary that exposes its keys as attributes. For more\ninformation about Bunch object, see `Bunch`.\n\nIt\u2019s also possible for almost all of these function to constrain the output to\nbe a tuple containing only the data and the target, by setting the\n`return_X_y` parameter to `True`.\n\nThe datasets also contain a full description in their `DESCR` attribute and\nsome contain `feature_names` and `target_names`. See the dataset descriptions\nbelow for details.\n\nThe dataset generation functions. They can be used to generate controlled\nsynthetic datasets, described in the Generated datasets section.\n\nThese functions return a tuple `(X, y)` consisting of a `n_samples` *\n`n_features` numpy array `X` and an array of length `n_samples` containing the\ntargets `y`.\n\nIn addition, there are also miscellaneous tools to load datasets of other\nformats or from other locations, described in the Loading other datasets\nsection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "API Reference", "path": "modules/classes", "type": "Guide", "text": "\nThis is the class and function reference of scikit-learn. Please refer to the\nfull user guide for further details, as the class and function raw\nspecifications may not be enough to give full guidelines on their uses. For\nreference on concepts repeated across the API, see Glossary of Common Terms\nand API Elements.\n\nBase classes for all estimators.\n\n`base.BaseEstimator`\n\nBase class for all estimators in scikit-learn.\n\n`base.BiclusterMixin`\n\nMixin class for all bicluster estimators in scikit-learn.\n\n`base.ClassifierMixin`\n\nMixin class for all classifiers in scikit-learn.\n\n`base.ClusterMixin`\n\nMixin class for all cluster estimators in scikit-learn.\n\n`base.DensityMixin`\n\nMixin class for all density estimators in scikit-learn.\n\n`base.RegressorMixin`\n\nMixin class for all regression estimators in scikit-learn.\n\n`base.TransformerMixin`\n\nMixin class for all transformers in scikit-learn.\n\n`feature_selection.SelectorMixin`\n\nTransformer mixin that performs feature selection given a support mask\n\n`base.clone`(estimator, *[, safe])\n\nConstructs a new unfitted estimator with the same parameters.\n\n`base.is_classifier`(estimator)\n\nReturn True if the given estimator is (probably) a classifier.\n\n`base.is_regressor`(estimator)\n\nReturn True if the given estimator is (probably) a regressor.\n\n`config_context`(**new_config)\n\nContext manager for global scikit-learn configuration\n\n`get_config`()\n\nRetrieve current values for configuration set by `set_config`\n\n`set_config`([assume_finite, working_memory, \u2026])\n\nSet global scikit-learn configuration\n\n`show_versions`()\n\nPrint useful debugging information\u201d\n\nCalibration of predicted probabilities.\n\nUser guide: See the Probability calibration section for further details.\n\n`calibration.CalibratedClassifierCV`([\u2026])\n\nProbability calibration with isotonic regression or logistic regression.\n\n`calibration.calibration_curve`(y_true, y_prob, *)\n\nCompute true and predicted probabilities for a calibration curve.\n\nThe `sklearn.cluster` module gathers popular unsupervised clustering\nalgorithms.\n\nUser guide: See the Clustering and Biclustering sections for further details.\n\n`cluster.AffinityPropagation`(*[, damping, \u2026])\n\nPerform Affinity Propagation Clustering of data.\n\n`cluster.AgglomerativeClustering`([\u2026])\n\nAgglomerative Clustering\n\n`cluster.Birch`(*[, threshold, \u2026])\n\nImplements the Birch clustering algorithm.\n\n`cluster.DBSCAN`([eps, min_samples, metric, \u2026])\n\nPerform DBSCAN clustering from vector array or distance matrix.\n\n`cluster.FeatureAgglomeration`([n_clusters, \u2026])\n\nAgglomerate features.\n\n`cluster.KMeans`([n_clusters, init, n_init, \u2026])\n\nK-Means clustering.\n\n`cluster.MiniBatchKMeans`([n_clusters, init, \u2026])\n\nMini-Batch K-Means clustering.\n\n`cluster.MeanShift`(*[, bandwidth, seeds, \u2026])\n\nMean shift clustering using a flat kernel.\n\n`cluster.OPTICS`(*[, min_samples, max_eps, \u2026])\n\nEstimate clustering structure from vector array.\n\n`cluster.SpectralClustering`([n_clusters, \u2026])\n\nApply clustering to a projection of the normalized Laplacian.\n\n`cluster.SpectralBiclustering`([n_clusters, \u2026])\n\nSpectral biclustering (Kluger, 2003).\n\n`cluster.SpectralCoclustering`([n_clusters, \u2026])\n\nSpectral Co-Clustering algorithm (Dhillon, 2001).\n\n`cluster.affinity_propagation`(S, *[, \u2026])\n\nPerform Affinity Propagation Clustering of data.\n\n`cluster.cluster_optics_dbscan`(*, \u2026)\n\nPerforms DBSCAN extraction for an arbitrary epsilon.\n\n`cluster.cluster_optics_xi`(*, reachability, \u2026)\n\nAutomatically extract clusters according to the Xi-steep method.\n\n`cluster.compute_optics_graph`(X, *, \u2026)\n\nComputes the OPTICS reachability graph.\n\n`cluster.dbscan`(X[, eps, min_samples, \u2026])\n\nPerform DBSCAN clustering from vector array or distance matrix.\n\n`cluster.estimate_bandwidth`(X, *[, quantile, \u2026])\n\nEstimate the bandwidth to use with the mean-shift algorithm.\n\n`cluster.k_means`(X, n_clusters, *[, \u2026])\n\nK-means clustering algorithm.\n\n`cluster.kmeans_plusplus`(X, n_clusters, *[, \u2026])\n\nInit n_clusters seeds according to k-means++\n\n`cluster.mean_shift`(X, *[, bandwidth, seeds, \u2026])\n\nPerform mean shift clustering of data using a flat kernel.\n\n`cluster.spectral_clustering`(affinity, *[, \u2026])\n\nApply clustering to a projection of the normalized Laplacian.\n\n`cluster.ward_tree`(X, *[, connectivity, \u2026])\n\nWard clustering based on a Feature matrix.\n\nMeta-estimators for building composite models with transformers\n\nIn addition to its current contents, this module will eventually be home to\nrefurbished versions of Pipeline and FeatureUnion.\n\nUser guide: See the Pipelines and composite estimators section for further\ndetails.\n\n`compose.ColumnTransformer`(transformers, *[, \u2026])\n\nApplies transformers to columns of an array or pandas DataFrame.\n\n`compose.TransformedTargetRegressor`([\u2026])\n\nMeta-estimator to regress on a transformed target.\n\n`compose.make_column_transformer`(*transformers)\n\nConstruct a ColumnTransformer from the given transformers.\n\n`compose.make_column_selector`([pattern, \u2026])\n\nCreate a callable to select columns to be used with `ColumnTransformer`.\n\nThe `sklearn.covariance` module includes methods and algorithms to robustly\nestimate the covariance of features given a set of points. The precision\nmatrix defined as the inverse of the covariance is also estimated. Covariance\nestimation is closely related to the theory of Gaussian Graphical Models.\n\nUser guide: See the Covariance estimation section for further details.\n\n`covariance.EmpiricalCovariance`(*[, \u2026])\n\nMaximum likelihood covariance estimator\n\n`covariance.EllipticEnvelope`(*[, \u2026])\n\nAn object for detecting outliers in a Gaussian distributed dataset.\n\n`covariance.GraphicalLasso`([alpha, mode, \u2026])\n\nSparse inverse covariance estimation with an l1-penalized estimator.\n\n`covariance.GraphicalLassoCV`(*[, alphas, \u2026])\n\nSparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\n`covariance.LedoitWolf`(*[, store_precision, \u2026])\n\nLedoitWolf Estimator\n\n`covariance.MinCovDet`(*[, store_precision, \u2026])\n\nMinimum Covariance Determinant (MCD): robust estimator of covariance.\n\n`covariance.OAS`(*[, store_precision, \u2026])\n\nOracle Approximating Shrinkage Estimator\n\n`covariance.ShrunkCovariance`(*[, \u2026])\n\nCovariance estimator with shrinkage\n\n`covariance.empirical_covariance`(X, *[, \u2026])\n\nComputes the Maximum likelihood covariance estimator\n\n`covariance.graphical_lasso`(emp_cov, alpha, *)\n\nl1-penalized covariance estimator\n\n`covariance.ledoit_wolf`(X, *[, \u2026])\n\nEstimates the shrunk Ledoit-Wolf covariance matrix.\n\n`covariance.oas`(X, *[, assume_centered])\n\nEstimate covariance with the Oracle Approximating Shrinkage algorithm.\n\n`covariance.shrunk_covariance`(emp_cov[, \u2026])\n\nCalculates a covariance matrix shrunk on the diagonal\n\nUser guide: See the Cross decomposition section for further details.\n\n`cross_decomposition.CCA`([n_components, \u2026])\n\nCanonical Correlation Analysis, also known as \u201cMode B\u201d PLS.\n\n`cross_decomposition.PLSCanonical`([\u2026])\n\nPartial Least Squares transformer and regressor.\n\n`cross_decomposition.PLSRegression`([\u2026])\n\nPLS regression\n\n`cross_decomposition.PLSSVD`([n_components, \u2026])\n\nPartial Least Square SVD.\n\nThe `sklearn.datasets` module includes utilities to load datasets, including\nmethods to load and fetch popular reference datasets. It also features some\nartificial data generators.\n\nUser guide: See the Dataset loading utilities section for further details.\n\n`datasets.clear_data_home`([data_home])\n\nDelete all the content of the data home cache.\n\n`datasets.dump_svmlight_file`(X, y, f, *[, \u2026])\n\nDump the dataset in svmlight / libsvm file format.\n\n`datasets.fetch_20newsgroups`(*[, data_home, \u2026])\n\nLoad the filenames and data from the 20 newsgroups dataset (classification).\n\n`datasets.fetch_20newsgroups_vectorized`(*[, \u2026])\n\nLoad and vectorize the 20 newsgroups dataset (classification).\n\n`datasets.fetch_california_housing`(*[, \u2026])\n\nLoad the California housing dataset (regression).\n\n`datasets.fetch_covtype`(*[, data_home, \u2026])\n\nLoad the covertype dataset (classification).\n\n`datasets.fetch_kddcup99`(*[, subset, \u2026])\n\nLoad the kddcup99 dataset (classification).\n\n`datasets.fetch_lfw_pairs`(*[, subset, \u2026])\n\nLoad the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n\n`datasets.fetch_lfw_people`(*[, data_home, \u2026])\n\nLoad the Labeled Faces in the Wild (LFW) people dataset (classification).\n\n`datasets.fetch_olivetti_faces`(*[, \u2026])\n\nLoad the Olivetti faces data-set from AT&T (classification).\n\n`datasets.fetch_openml`([name, version, \u2026])\n\nFetch dataset from openml by name or dataset id.\n\n`datasets.fetch_rcv1`(*[, data_home, subset, \u2026])\n\nLoad the RCV1 multilabel dataset (classification).\n\n`datasets.fetch_species_distributions`(*[, \u2026])\n\nLoader for species distribution dataset from Phillips et.\n\n`datasets.get_data_home`([data_home])\n\nReturn the path of the scikit-learn data dir.\n\n`datasets.load_boston`(*[, return_X_y])\n\nLoad and return the boston house-prices dataset (regression).\n\n`datasets.load_breast_cancer`(*[, return_X_y, \u2026])\n\nLoad and return the breast cancer wisconsin dataset (classification).\n\n`datasets.load_diabetes`(*[, return_X_y, as_frame])\n\nLoad and return the diabetes dataset (regression).\n\n`datasets.load_digits`(*[, n_class, \u2026])\n\nLoad and return the digits dataset (classification).\n\n`datasets.load_files`(container_path, *[, \u2026])\n\nLoad text files with categories as subfolder names.\n\n`datasets.load_iris`(*[, return_X_y, as_frame])\n\nLoad and return the iris dataset (classification).\n\n`datasets.load_linnerud`(*[, return_X_y, as_frame])\n\nLoad and return the physical excercise linnerud dataset.\n\n`datasets.load_sample_image`(image_name)\n\nLoad the numpy array of a single sample image\n\n`datasets.load_sample_images`()\n\nLoad sample images for image manipulation.\n\n`datasets.load_svmlight_file`(f, *[, \u2026])\n\nLoad datasets in the svmlight / libsvm format into sparse CSR matrix\n\n`datasets.load_svmlight_files`(files, *[, \u2026])\n\nLoad dataset from multiple files in SVMlight format\n\n`datasets.load_wine`(*[, return_X_y, as_frame])\n\nLoad and return the wine dataset (classification).\n\n`datasets.make_biclusters`(shape, n_clusters, *)\n\nGenerate an array with constant block diagonal structure for biclustering.\n\n`datasets.make_blobs`([n_samples, n_features, \u2026])\n\nGenerate isotropic Gaussian blobs for clustering.\n\n`datasets.make_checkerboard`(shape, n_clusters, *)\n\nGenerate an array with block checkerboard structure for biclustering.\n\n`datasets.make_circles`([n_samples, shuffle, \u2026])\n\nMake a large circle containing a smaller circle in 2d.\n\n`datasets.make_classification`([n_samples, \u2026])\n\nGenerate a random n-class classification problem.\n\n`datasets.make_friedman1`([n_samples, \u2026])\n\nGenerate the \u201cFriedman #1\u201d regression problem.\n\n`datasets.make_friedman2`([n_samples, noise, \u2026])\n\nGenerate the \u201cFriedman #2\u201d regression problem.\n\n`datasets.make_friedman3`([n_samples, noise, \u2026])\n\nGenerate the \u201cFriedman #3\u201d regression problem.\n\n`datasets.make_gaussian_quantiles`(*[, mean, \u2026])\n\nGenerate isotropic Gaussian and label samples by quantile.\n\n`datasets.make_hastie_10_2`([n_samples, \u2026])\n\nGenerates data for binary classification used in Hastie et al.\n\n`datasets.make_low_rank_matrix`([n_samples, \u2026])\n\nGenerate a mostly low rank matrix with bell-shaped singular values.\n\n`datasets.make_moons`([n_samples, shuffle, \u2026])\n\nMake two interleaving half circles.\n\n`datasets.make_multilabel_classification`([\u2026])\n\nGenerate a random multilabel classification problem.\n\n`datasets.make_regression`([n_samples, \u2026])\n\nGenerate a random regression problem.\n\n`datasets.make_s_curve`([n_samples, noise, \u2026])\n\nGenerate an S curve dataset.\n\n`datasets.make_sparse_coded_signal`(n_samples, \u2026)\n\nGenerate a signal as a sparse combination of dictionary elements.\n\n`datasets.make_sparse_spd_matrix`([dim, \u2026])\n\nGenerate a sparse symmetric definite positive matrix.\n\n`datasets.make_sparse_uncorrelated`([\u2026])\n\nGenerate a random regression problem with sparse uncorrelated design.\n\n`datasets.make_spd_matrix`(n_dim, *[, \u2026])\n\nGenerate a random symmetric, positive-definite matrix.\n\n`datasets.make_swiss_roll`([n_samples, noise, \u2026])\n\nGenerate a swiss roll dataset.\n\nThe `sklearn.decomposition` module includes matrix decomposition algorithms,\nincluding among others PCA, NMF or ICA. Most of the algorithms of this module\ncan be regarded as dimensionality reduction techniques.\n\nUser guide: See the Decomposing signals in components (matrix factorization\nproblems) section for further details.\n\n`decomposition.DictionaryLearning`([\u2026])\n\nDictionary learning\n\n`decomposition.FactorAnalysis`([n_components, \u2026])\n\nFactor Analysis (FA).\n\n`decomposition.FastICA`([n_components, \u2026])\n\nFastICA: a fast algorithm for Independent Component Analysis.\n\n`decomposition.IncrementalPCA`([n_components, \u2026])\n\nIncremental principal components analysis (IPCA).\n\n`decomposition.KernelPCA`([n_components, \u2026])\n\nKernel Principal component analysis (KPCA).\n\n`decomposition.LatentDirichletAllocation`([\u2026])\n\nLatent Dirichlet Allocation with online variational Bayes algorithm\n\n`decomposition.MiniBatchDictionaryLearning`([\u2026])\n\nMini-batch dictionary learning\n\n`decomposition.MiniBatchSparsePCA`([\u2026])\n\nMini-batch Sparse Principal Components Analysis\n\n`decomposition.NMF`([n_components, init, \u2026])\n\nNon-Negative Matrix Factorization (NMF).\n\n`decomposition.PCA`([n_components, copy, \u2026])\n\nPrincipal component analysis (PCA).\n\n`decomposition.SparsePCA`([n_components, \u2026])\n\nSparse Principal Components Analysis (SparsePCA).\n\n`decomposition.SparseCoder`(dictionary, *[, \u2026])\n\nSparse coding\n\n`decomposition.TruncatedSVD`([n_components, \u2026])\n\nDimensionality reduction using truncated SVD (aka LSA).\n\n`decomposition.dict_learning`(X, n_components, \u2026)\n\nSolves a dictionary learning matrix factorization problem.\n\n`decomposition.dict_learning_online`(X[, \u2026])\n\nSolves a dictionary learning matrix factorization problem online.\n\n`decomposition.fastica`(X[, n_components, \u2026])\n\nPerform Fast Independent Component Analysis.\n\n`decomposition.non_negative_factorization`(X)\n\nCompute Non-negative Matrix Factorization (NMF).\n\n`decomposition.sparse_encode`(X, dictionary, *)\n\nSparse coding\n\nLinear Discriminant Analysis and Quadratic Discriminant Analysis\n\nUser guide: See the Linear and Quadratic Discriminant Analysis section for\nfurther details.\n\n`discriminant_analysis.LinearDiscriminantAnalysis`([\u2026])\n\nLinear Discriminant Analysis\n\n`discriminant_analysis.QuadraticDiscriminantAnalysis`(*)\n\nQuadratic Discriminant Analysis\n\nUser guide: See the Metrics and scoring: quantifying the quality of\npredictions section for further details.\n\n`dummy.DummyClassifier`(*[, strategy, \u2026])\n\nDummyClassifier is a classifier that makes predictions using simple rules.\n\n`dummy.DummyRegressor`(*[, strategy, \u2026])\n\nDummyRegressor is a regressor that makes predictions using simple rules.\n\nThe `sklearn.ensemble` module includes ensemble-based methods for\nclassification, regression and anomaly detection.\n\nUser guide: See the Ensemble methods section for further details.\n\n`ensemble.AdaBoostClassifier`([\u2026])\n\nAn AdaBoost classifier.\n\n`ensemble.AdaBoostRegressor`([base_estimator, \u2026])\n\nAn AdaBoost regressor.\n\n`ensemble.BaggingClassifier`([base_estimator, \u2026])\n\nA Bagging classifier.\n\n`ensemble.BaggingRegressor`([base_estimator, \u2026])\n\nA Bagging regressor.\n\n`ensemble.ExtraTreesClassifier`([\u2026])\n\nAn extra-trees classifier.\n\n`ensemble.ExtraTreesRegressor`([n_estimators, \u2026])\n\nAn extra-trees regressor.\n\n`ensemble.GradientBoostingClassifier`(*[, \u2026])\n\nGradient Boosting for classification.\n\n`ensemble.GradientBoostingRegressor`(*[, \u2026])\n\nGradient Boosting for regression.\n\n`ensemble.IsolationForest`(*[, n_estimators, \u2026])\n\nIsolation Forest Algorithm.\n\n`ensemble.RandomForestClassifier`([\u2026])\n\nA random forest classifier.\n\n`ensemble.RandomForestRegressor`([\u2026])\n\nA random forest regressor.\n\n`ensemble.RandomTreesEmbedding`([\u2026])\n\nAn ensemble of totally random trees.\n\n`ensemble.StackingClassifier`(estimators[, \u2026])\n\nStack of estimators with a final classifier.\n\n`ensemble.StackingRegressor`(estimators[, \u2026])\n\nStack of estimators with a final regressor.\n\n`ensemble.VotingClassifier`(estimators, *[, \u2026])\n\nSoft Voting/Majority Rule classifier for unfitted estimators.\n\n`ensemble.VotingRegressor`(estimators, *[, \u2026])\n\nPrediction voting regressor for unfitted estimators.\n\n`ensemble.HistGradientBoostingRegressor`([\u2026])\n\nHistogram-based Gradient Boosting Regression Tree.\n\n`ensemble.HistGradientBoostingClassifier`([\u2026])\n\nHistogram-based Gradient Boosting Classification Tree.\n\nThe `sklearn.exceptions` module includes all custom warnings and error classes\nused across scikit-learn.\n\n`exceptions.ConvergenceWarning`\n\nCustom warning to capture convergence problems\n\n`exceptions.DataConversionWarning`\n\nWarning used to notify implicit data conversions happening in the code.\n\n`exceptions.DataDimensionalityWarning`\n\nCustom warning to notify potential issues with data dimensionality.\n\n`exceptions.EfficiencyWarning`\n\nWarning used to notify the user of inefficient computation.\n\n`exceptions.FitFailedWarning`\n\nWarning class used if there is an error while fitting the estimator.\n\n`exceptions.NotFittedError`\n\nException class to raise if estimator is used before fitting.\n\n`exceptions.UndefinedMetricWarning`\n\nWarning used when the metric is invalid\n\nThe `sklearn.experimental` module provides importable modules that enable the\nuse of experimental features or estimators.\n\nThe features and estimators that are experimental aren\u2019t subject to\ndeprecation cycles. Use them at your own risks!\n\n`experimental.enable_hist_gradient_boosting`\n\nEnables histogram-based gradient boosting estimators.\n\n`experimental.enable_iterative_imputer`\n\nEnables IterativeImputer\n\n`experimental.enable_halving_search_cv`\n\nEnables Successive Halving search-estimators\n\nThe `sklearn.feature_extraction` module deals with feature extraction from raw\ndata. It currently includes methods to extract features from text and images.\n\nUser guide: See the Feature extraction section for further details.\n\n`feature_extraction.DictVectorizer`(*[, \u2026])\n\nTransforms lists of feature-value mappings to vectors.\n\n`feature_extraction.FeatureHasher`([\u2026])\n\nImplements feature hashing, aka the hashing trick.\n\nThe `sklearn.feature_extraction.image` submodule gathers utilities to extract\nfeatures from images.\n\n`feature_extraction.image.extract_patches_2d`(\u2026)\n\nReshape a 2D image into a collection of patches\n\n`feature_extraction.image.grid_to_graph`(n_x, n_y)\n\nGraph of the pixel-to-pixel connections\n\n`feature_extraction.image.img_to_graph`(img, *)\n\nGraph of the pixel-to-pixel gradient connections\n\n`feature_extraction.image.reconstruct_from_patches_2d`(\u2026)\n\nReconstruct the image from all of its patches.\n\n`feature_extraction.image.PatchExtractor`(*[, \u2026])\n\nExtracts patches from a collection of images\n\nThe `sklearn.feature_extraction.text` submodule gathers utilities to build\nfeature vectors from text documents.\n\n`feature_extraction.text.CountVectorizer`(*[, \u2026])\n\nConvert a collection of text documents to a matrix of token counts\n\n`feature_extraction.text.HashingVectorizer`(*)\n\nConvert a collection of text documents to a matrix of token occurrences\n\n`feature_extraction.text.TfidfTransformer`(*)\n\nTransform a count matrix to a normalized tf or tf-idf representation\n\n`feature_extraction.text.TfidfVectorizer`(*[, \u2026])\n\nConvert a collection of raw documents to a matrix of TF-IDF features.\n\nThe `sklearn.feature_selection` module implements feature selection\nalgorithms. It currently includes univariate filter selection methods and the\nrecursive feature elimination algorithm.\n\nUser guide: See the Feature selection section for further details.\n\n`feature_selection.GenericUnivariateSelect`([\u2026])\n\nUnivariate feature selector with configurable strategy.\n\n`feature_selection.SelectPercentile`([\u2026])\n\nSelect features according to a percentile of the highest scores.\n\n`feature_selection.SelectKBest`([score_func, k])\n\nSelect features according to the k highest scores.\n\n`feature_selection.SelectFpr`([score_func, alpha])\n\nFilter: Select the pvalues below alpha based on a FPR test.\n\n`feature_selection.SelectFdr`([score_func, alpha])\n\nFilter: Select the p-values for an estimated false discovery rate\n\n`feature_selection.SelectFromModel`(estimator, *)\n\nMeta-transformer for selecting features based on importance weights.\n\n`feature_selection.SelectFwe`([score_func, alpha])\n\nFilter: Select the p-values corresponding to Family-wise error rate\n\n`feature_selection.SequentialFeatureSelector`(\u2026)\n\nTransformer that performs Sequential Feature Selection.\n\n`feature_selection.RFE`(estimator, *[, \u2026])\n\nFeature ranking with recursive feature elimination.\n\n`feature_selection.RFECV`(estimator, *[, \u2026])\n\nFeature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\n`feature_selection.VarianceThreshold`([threshold])\n\nFeature selector that removes all low-variance features.\n\n`feature_selection.chi2`(X, y)\n\nCompute chi-squared stats between each non-negative feature and class.\n\n`feature_selection.f_classif`(X, y)\n\nCompute the ANOVA F-value for the provided sample.\n\n`feature_selection.f_regression`(X, y, *[, center])\n\nUnivariate linear regression tests.\n\n`feature_selection.mutual_info_classif`(X, y, *)\n\nEstimate mutual information for a discrete target variable.\n\n`feature_selection.mutual_info_regression`(X, y, *)\n\nEstimate mutual information for a continuous target variable.\n\nThe `sklearn.gaussian_process` module implements Gaussian Process based\nregression and classification.\n\nUser guide: See the Gaussian Processes section for further details.\n\n`gaussian_process.GaussianProcessClassifier`([\u2026])\n\nGaussian process classification (GPC) based on Laplace approximation.\n\n`gaussian_process.GaussianProcessRegressor`([\u2026])\n\nGaussian process regression (GPR).\n\nKernels:\n\n`gaussian_process.kernels.CompoundKernel`(kernels)\n\nKernel which is composed of a set of other kernels.\n\n`gaussian_process.kernels.ConstantKernel`([\u2026])\n\nConstant kernel.\n\n`gaussian_process.kernels.DotProduct`([\u2026])\n\nDot-Product kernel.\n\n`gaussian_process.kernels.ExpSineSquared`([\u2026])\n\nExp-Sine-Squared kernel (aka periodic kernel).\n\n`gaussian_process.kernels.Exponentiation`(\u2026)\n\nThe Exponentiation kernel takes one base kernel and a scalar parameter \\\\(p\\\\)\nand combines them via\n\n`gaussian_process.kernels.Hyperparameter`(\u2026)\n\nA kernel hyperparameter\u2019s specification in form of a namedtuple.\n\n`gaussian_process.kernels.Kernel`()\n\nBase class for all kernels.\n\n`gaussian_process.kernels.Matern`([\u2026])\n\nMatern kernel.\n\n`gaussian_process.kernels.PairwiseKernel`([\u2026])\n\nWrapper for kernels in sklearn.metrics.pairwise.\n\n`gaussian_process.kernels.Product`(k1, k2)\n\nThe `Product` kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and combines\nthem via\n\n`gaussian_process.kernels.RBF`([length_scale, \u2026])\n\nRadial-basis function kernel (aka squared-exponential kernel).\n\n`gaussian_process.kernels.RationalQuadratic`([\u2026])\n\nRational Quadratic kernel.\n\n`gaussian_process.kernels.Sum`(k1, k2)\n\nThe `Sum` kernel takes two kernels \\\\(k_1\\\\) and \\\\(k_2\\\\) and combines them\nvia\n\n`gaussian_process.kernels.WhiteKernel`([\u2026])\n\nWhite kernel.\n\nTransformers for missing value imputation\n\nUser guide: See the Imputation of missing values section for further details.\n\n`impute.SimpleImputer`(*[, missing_values, \u2026])\n\nImputation transformer for completing missing values.\n\n`impute.IterativeImputer`([estimator, \u2026])\n\nMultivariate imputer that estimates each feature from all the others.\n\n`impute.MissingIndicator`(*[, missing_values, \u2026])\n\nBinary indicators for missing values.\n\n`impute.KNNImputer`(*[, missing_values, \u2026])\n\nImputation for completing missing values using k-Nearest Neighbors.\n\nThe `sklearn.inspection` module includes tools for model inspection.\n\n`inspection.partial_dependence`(estimator, X, \u2026)\n\nPartial dependence of `features`.\n\n`inspection.permutation_importance`(estimator, \u2026)\n\nPermutation importance for feature evaluation [Rd9e56ef97513-BRE].\n\n`inspection.PartialDependenceDisplay`(\u2026[, \u2026])\n\nPartial Dependence Plot (PDP).\n\n`inspection.plot_partial_dependence`(\u2026[, \u2026])\n\nPartial dependence (PD) and individual conditional expectation (ICE) plots.\n\nUser guide: See the Isotonic regression section for further details.\n\n`isotonic.IsotonicRegression`(*[, y_min, \u2026])\n\nIsotonic regression model.\n\n`isotonic.check_increasing`(x, y)\n\nDetermine whether y is monotonically correlated with x.\n\n`isotonic.isotonic_regression`(y, *[, \u2026])\n\nSolve the isotonic regression model.\n\nThe `sklearn.kernel_approximation` module implements several approximate\nkernel feature maps based on Fourier transforms and Count Sketches.\n\nUser guide: See the Kernel Approximation section for further details.\n\n`kernel_approximation.AdditiveChi2Sampler`(*)\n\nApproximate feature map for additive chi2 kernel.\n\n`kernel_approximation.Nystroem`([kernel, \u2026])\n\nApproximate a kernel map using a subset of the training data.\n\n`kernel_approximation.PolynomialCountSketch`(*)\n\nPolynomial kernel approximation via Tensor Sketch.\n\n`kernel_approximation.RBFSampler`(*[, gamma, \u2026])\n\nApproximates feature map of an RBF kernel by Monte Carlo approximation of its\nFourier transform.\n\n`kernel_approximation.SkewedChi2Sampler`(*[, \u2026])\n\nApproximates feature map of the \u201cskewed chi-squared\u201d kernel by Monte Carlo\napproximation of its Fourier transform.\n\nModule `sklearn.kernel_ridge` implements kernel ridge regression.\n\nUser guide: See the Kernel ridge regression section for further details.\n\n`kernel_ridge.KernelRidge`([alpha, kernel, \u2026])\n\nKernel ridge regression.\n\nThe `sklearn.linear_model` module implements a variety of linear models.\n\nUser guide: See the Linear Models section for further details.\n\nThe following subsections are only rough guidelines: the same estimator can\nfall into multiple categories, depending on its parameters.\n\n`linear_model.LogisticRegression`([penalty, \u2026])\n\nLogistic Regression (aka logit, MaxEnt) classifier.\n\n`linear_model.LogisticRegressionCV`(*[, Cs, \u2026])\n\nLogistic Regression CV (aka logit, MaxEnt) classifier.\n\n`linear_model.PassiveAggressiveClassifier`(*)\n\nPassive Aggressive Classifier\n\n`linear_model.Perceptron`(*[, penalty, alpha, \u2026])\n\nRead more in the User Guide.\n\n`linear_model.RidgeClassifier`([alpha, \u2026])\n\nClassifier using Ridge regression.\n\n`linear_model.RidgeClassifierCV`([alphas, \u2026])\n\nRidge classifier with built-in cross-validation.\n\n`linear_model.SGDClassifier`([loss, penalty, \u2026])\n\nLinear classifiers (SVM, logistic regression, etc.) with SGD training.\n\n`linear_model.LinearRegression`(*[, \u2026])\n\nOrdinary least squares Linear Regression.\n\n`linear_model.Ridge`([alpha, fit_intercept, \u2026])\n\nLinear least squares with l2 regularization.\n\n`linear_model.RidgeCV`([alphas, \u2026])\n\nRidge regression with built-in cross-validation.\n\n`linear_model.SGDRegressor`([loss, penalty, \u2026])\n\nLinear model fitted by minimizing a regularized empirical loss with SGD\n\nThe following estimators have built-in variable selection fitting procedures,\nbut any estimator using a L1 or elastic-net penalty also performs variable\nselection: typically `SGDRegressor` or `SGDClassifier` with an appropriate\npenalty.\n\n`linear_model.ElasticNet`([alpha, l1_ratio, \u2026])\n\nLinear regression with combined L1 and L2 priors as regularizer.\n\n`linear_model.ElasticNetCV`(*[, l1_ratio, \u2026])\n\nElastic Net model with iterative fitting along a regularization path.\n\n`linear_model.Lars`(*[, fit_intercept, \u2026])\n\nLeast Angle Regression model a.k.a.\n\n`linear_model.LarsCV`(*[, fit_intercept, \u2026])\n\nCross-validated Least Angle Regression model.\n\n`linear_model.Lasso`([alpha, fit_intercept, \u2026])\n\nLinear Model trained with L1 prior as regularizer (aka the Lasso)\n\n`linear_model.LassoCV`(*[, eps, n_alphas, \u2026])\n\nLasso linear model with iterative fitting along a regularization path.\n\n`linear_model.LassoLars`([alpha, \u2026])\n\nLasso model fit with Least Angle Regression a.k.a.\n\n`linear_model.LassoLarsCV`(*[, fit_intercept, \u2026])\n\nCross-validated Lasso, using the LARS algorithm.\n\n`linear_model.LassoLarsIC`([criterion, \u2026])\n\nLasso model fit with Lars using BIC or AIC for model selection\n\n`linear_model.OrthogonalMatchingPursuit`(*[, \u2026])\n\nOrthogonal Matching Pursuit model (OMP).\n\n`linear_model.OrthogonalMatchingPursuitCV`(*)\n\nCross-validated Orthogonal Matching Pursuit model (OMP).\n\n`linear_model.ARDRegression`(*[, n_iter, tol, \u2026])\n\nBayesian ARD regression.\n\n`linear_model.BayesianRidge`(*[, n_iter, tol, \u2026])\n\nBayesian ridge regression.\n\nThese estimators fit multiple regression problems (or tasks) jointly, while\ninducing sparse coefficients. While the inferred coefficients may differ\nbetween the tasks, they are constrained to agree on the features that are\nselected (non-zero coefficients).\n\n`linear_model.MultiTaskElasticNet`([alpha, \u2026])\n\nMulti-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.\n\n`linear_model.MultiTaskElasticNetCV`(*[, \u2026])\n\nMulti-task L1/L2 ElasticNet with built-in cross-validation.\n\n`linear_model.MultiTaskLasso`([alpha, \u2026])\n\nMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\n`linear_model.MultiTaskLassoCV`(*[, eps, \u2026])\n\nMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\n\nAny estimator using the Huber loss would also be robust to outliers, e.g.\n`SGDRegressor` with `loss='huber'`.\n\n`linear_model.HuberRegressor`(*[, epsilon, \u2026])\n\nLinear regression model that is robust to outliers.\n\n`linear_model.RANSACRegressor`([\u2026])\n\nRANSAC (RANdom SAmple Consensus) algorithm.\n\n`linear_model.TheilSenRegressor`(*[, \u2026])\n\nTheil-Sen Estimator: robust multivariate regression model.\n\nThese models allow for response variables to have error distributions other\nthan a normal distribution:\n\n`linear_model.PoissonRegressor`(*[, alpha, \u2026])\n\nGeneralized Linear Model with a Poisson distribution.\n\n`linear_model.TweedieRegressor`(*[, power, \u2026])\n\nGeneralized Linear Model with a Tweedie distribution.\n\n`linear_model.GammaRegressor`(*[, alpha, \u2026])\n\nGeneralized Linear Model with a Gamma distribution.\n\n`linear_model.PassiveAggressiveRegressor`(*[, \u2026])\n\nPassive Aggressive Regressor\n\n`linear_model.enet_path`(X, y, *[, l1_ratio, \u2026])\n\nCompute elastic net path with coordinate descent.\n\n`linear_model.lars_path`(X, y[, Xy, Gram, \u2026])\n\nCompute Least Angle Regression or Lasso path using LARS algorithm [1]\n\n`linear_model.lars_path_gram`(Xy, Gram, *, \u2026)\n\nlars_path in the sufficient stats mode [1]\n\n`linear_model.lasso_path`(X, y, *[, eps, \u2026])\n\nCompute Lasso path with coordinate descent\n\n`linear_model.orthogonal_mp`(X, y, *[, \u2026])\n\nOrthogonal Matching Pursuit (OMP).\n\n`linear_model.orthogonal_mp_gram`(Gram, Xy, *)\n\nGram Orthogonal Matching Pursuit (OMP).\n\n`linear_model.ridge_regression`(X, y, alpha, *)\n\nSolve the ridge equation by the method of normal equations.\n\nThe `sklearn.manifold` module implements data embedding techniques.\n\nUser guide: See the Manifold learning section for further details.\n\n`manifold.Isomap`(*[, n_neighbors, \u2026])\n\nIsomap Embedding\n\n`manifold.LocallyLinearEmbedding`(*[, \u2026])\n\nLocally Linear Embedding\n\n`manifold.MDS`([n_components, metric, n_init, \u2026])\n\nMultidimensional scaling.\n\n`manifold.SpectralEmbedding`([n_components, \u2026])\n\nSpectral embedding for non-linear dimensionality reduction.\n\n`manifold.TSNE`([n_components, perplexity, \u2026])\n\nt-distributed Stochastic Neighbor Embedding.\n\n`manifold.locally_linear_embedding`(X, *, \u2026)\n\nPerform a Locally Linear Embedding analysis on the data.\n\n`manifold.smacof`(dissimilarities, *[, \u2026])\n\nComputes multidimensional scaling using the SMACOF algorithm.\n\n`manifold.spectral_embedding`(adjacency, *[, \u2026])\n\nProject the sample on the first eigenvectors of the graph Laplacian.\n\n`manifold.trustworthiness`(X, X_embedded, *[, \u2026])\n\nExpresses to what extent the local structure is retained.\n\nSee the Metrics and scoring: quantifying the quality of predictions section\nand the Pairwise metrics, Affinities and Kernels section of the user guide for\nfurther details.\n\nThe `sklearn.metrics` module includes score functions, performance metrics and\npairwise metrics and distance computations.\n\nSee the The scoring parameter: defining model evaluation rules section of the\nuser guide for further details.\n\n`metrics.check_scoring`(estimator[, scoring, \u2026])\n\nDetermine scorer from user options.\n\n`metrics.get_scorer`(scoring)\n\nGet a scorer from string.\n\n`metrics.make_scorer`(score_func, *[, \u2026])\n\nMake a scorer from a performance metric or loss function.\n\nSee the Classification metrics section of the user guide for further details.\n\n`metrics.accuracy_score`(y_true, y_pred, *[, \u2026])\n\nAccuracy classification score.\n\n`metrics.auc`(x, y)\n\nCompute Area Under the Curve (AUC) using the trapezoidal rule.\n\n`metrics.average_precision_score`(y_true, \u2026)\n\nCompute average precision (AP) from prediction scores.\n\n`metrics.balanced_accuracy_score`(y_true, \u2026)\n\nCompute the balanced accuracy.\n\n`metrics.brier_score_loss`(y_true, y_prob, *)\n\nCompute the Brier score loss.\n\n`metrics.classification_report`(y_true, y_pred, *)\n\nBuild a text report showing the main classification metrics.\n\n`metrics.cohen_kappa_score`(y1, y2, *[, \u2026])\n\nCohen\u2019s kappa: a statistic that measures inter-annotator agreement.\n\n`metrics.confusion_matrix`(y_true, y_pred, *)\n\nCompute confusion matrix to evaluate the accuracy of a classification.\n\n`metrics.dcg_score`(y_true, y_score, *[, k, \u2026])\n\nCompute Discounted Cumulative Gain.\n\n`metrics.det_curve`(y_true, y_score[, \u2026])\n\nCompute error rates for different probability thresholds.\n\n`metrics.f1_score`(y_true, y_pred, *[, \u2026])\n\nCompute the F1 score, also known as balanced F-score or F-measure.\n\n`metrics.fbeta_score`(y_true, y_pred, *, beta)\n\nCompute the F-beta score.\n\n`metrics.hamming_loss`(y_true, y_pred, *[, \u2026])\n\nCompute the average Hamming loss.\n\n`metrics.hinge_loss`(y_true, pred_decision, *)\n\nAverage hinge loss (non-regularized).\n\n`metrics.jaccard_score`(y_true, y_pred, *[, \u2026])\n\nJaccard similarity coefficient score.\n\n`metrics.log_loss`(y_true, y_pred, *[, eps, \u2026])\n\nLog loss, aka logistic loss or cross-entropy loss.\n\n`metrics.matthews_corrcoef`(y_true, y_pred, *)\n\nCompute the Matthews correlation coefficient (MCC).\n\n`metrics.multilabel_confusion_matrix`(y_true, \u2026)\n\nCompute a confusion matrix for each class or sample.\n\n`metrics.ndcg_score`(y_true, y_score, *[, k, \u2026])\n\nCompute Normalized Discounted Cumulative Gain.\n\n`metrics.precision_recall_curve`(y_true, \u2026)\n\nCompute precision-recall pairs for different probability thresholds.\n\n`metrics.precision_recall_fscore_support`(\u2026)\n\nCompute precision, recall, F-measure and support for each class.\n\n`metrics.precision_score`(y_true, y_pred, *[, \u2026])\n\nCompute the precision.\n\n`metrics.recall_score`(y_true, y_pred, *[, \u2026])\n\nCompute the recall.\n\n`metrics.roc_auc_score`(y_true, y_score, *[, \u2026])\n\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from\nprediction scores.\n\n`metrics.roc_curve`(y_true, y_score, *[, \u2026])\n\nCompute Receiver operating characteristic (ROC).\n\n`metrics.top_k_accuracy_score`(y_true, y_score, *)\n\nTop-k Accuracy classification score.\n\n`metrics.zero_one_loss`(y_true, y_pred, *[, \u2026])\n\nZero-one classification loss.\n\nSee the Regression metrics section of the user guide for further details.\n\n`metrics.explained_variance_score`(y_true, \u2026)\n\nExplained variance regression score function.\n\n`metrics.max_error`(y_true, y_pred)\n\nmax_error metric calculates the maximum residual error.\n\n`metrics.mean_absolute_error`(y_true, y_pred, *)\n\nMean absolute error regression loss.\n\n`metrics.mean_squared_error`(y_true, y_pred, *)\n\nMean squared error regression loss.\n\n`metrics.mean_squared_log_error`(y_true, y_pred, *)\n\nMean squared logarithmic error regression loss.\n\n`metrics.median_absolute_error`(y_true, y_pred, *)\n\nMedian absolute error regression loss.\n\n`metrics.mean_absolute_percentage_error`(\u2026)\n\nMean absolute percentage error regression loss.\n\n`metrics.r2_score`(y_true, y_pred, *[, \u2026])\n\nR^2 (coefficient of determination) regression score function.\n\n`metrics.mean_poisson_deviance`(y_true, y_pred, *)\n\nMean Poisson deviance regression loss.\n\n`metrics.mean_gamma_deviance`(y_true, y_pred, *)\n\nMean Gamma deviance regression loss.\n\n`metrics.mean_tweedie_deviance`(y_true, y_pred, *)\n\nMean Tweedie deviance regression loss.\n\nSee the Multilabel ranking metrics section of the user guide for further\ndetails.\n\n`metrics.coverage_error`(y_true, y_score, *[, \u2026])\n\nCoverage error measure.\n\n`metrics.label_ranking_average_precision_score`(\u2026)\n\nCompute ranking-based average precision.\n\n`metrics.label_ranking_loss`(y_true, y_score, *)\n\nCompute Ranking loss measure.\n\nSee the Clustering performance evaluation section of the user guide for\nfurther details.\n\nThe `sklearn.metrics.cluster` submodule contains evaluation metrics for\ncluster analysis results. There are two forms of evaluation:\n\n`metrics.adjusted_mutual_info_score`(\u2026[, \u2026])\n\nAdjusted Mutual Information between two clusterings.\n\n`metrics.adjusted_rand_score`(labels_true, \u2026)\n\nRand index adjusted for chance.\n\n`metrics.calinski_harabasz_score`(X, labels)\n\nCompute the Calinski and Harabasz score.\n\n`metrics.davies_bouldin_score`(X, labels)\n\nComputes the Davies-Bouldin score.\n\n`metrics.completeness_score`(labels_true, \u2026)\n\nCompleteness metric of a cluster labeling given a ground truth.\n\n`metrics.cluster.contingency_matrix`(\u2026[, \u2026])\n\nBuild a contingency matrix describing the relationship between labels.\n\n`metrics.cluster.pair_confusion_matrix`(\u2026)\n\nPair confusion matrix arising from two clusterings.\n\n`metrics.fowlkes_mallows_score`(labels_true, \u2026)\n\nMeasure the similarity of two clusterings of a set of points.\n\n`metrics.homogeneity_completeness_v_measure`(\u2026)\n\nCompute the homogeneity and completeness and V-Measure scores at once.\n\n`metrics.homogeneity_score`(labels_true, \u2026)\n\nHomogeneity metric of a cluster labeling given a ground truth.\n\n`metrics.mutual_info_score`(labels_true, \u2026)\n\nMutual Information between two clusterings.\n\n`metrics.normalized_mutual_info_score`(\u2026[, \u2026])\n\nNormalized Mutual Information between two clusterings.\n\n`metrics.rand_score`(labels_true, labels_pred)\n\nRand index.\n\n`metrics.silhouette_score`(X, labels, *[, \u2026])\n\nCompute the mean Silhouette Coefficient of all samples.\n\n`metrics.silhouette_samples`(X, labels, *[, \u2026])\n\nCompute the Silhouette Coefficient for each sample.\n\n`metrics.v_measure_score`(labels_true, \u2026[, beta])\n\nV-measure cluster labeling given a ground truth.\n\nSee the Biclustering evaluation section of the user guide for further details.\n\n`metrics.consensus_score`(a, b, *[, similarity])\n\nThe similarity of two sets of biclusters.\n\nSee the Pairwise metrics, Affinities and Kernels section of the user guide for\nfurther details.\n\n`metrics.pairwise.additive_chi2_kernel`(X[, Y])\n\nComputes the additive chi-squared kernel between observations in X and Y.\n\n`metrics.pairwise.chi2_kernel`(X[, Y, gamma])\n\nComputes the exponential chi-squared kernel X and Y.\n\n`metrics.pairwise.cosine_similarity`(X[, Y, \u2026])\n\nCompute cosine similarity between samples in X and Y.\n\n`metrics.pairwise.cosine_distances`(X[, Y])\n\nCompute cosine distance between samples in X and Y.\n\n`metrics.pairwise.distance_metrics`()\n\nValid metrics for pairwise_distances.\n\n`metrics.pairwise.euclidean_distances`(X[, Y, \u2026])\n\nConsidering the rows of X (and Y=X) as vectors, compute the distance matrix\nbetween each pair of vectors.\n\n`metrics.pairwise.haversine_distances`(X[, Y])\n\nCompute the Haversine distance between samples in X and Y.\n\n`metrics.pairwise.kernel_metrics`()\n\nValid metrics for pairwise_kernels.\n\n`metrics.pairwise.laplacian_kernel`(X[, Y, gamma])\n\nCompute the laplacian kernel between X and Y.\n\n`metrics.pairwise.linear_kernel`(X[, Y, \u2026])\n\nCompute the linear kernel between X and Y.\n\n`metrics.pairwise.manhattan_distances`(X[, Y, \u2026])\n\nCompute the L1 distances between the vectors in X and Y.\n\n`metrics.pairwise.nan_euclidean_distances`(X)\n\nCalculate the euclidean distances in the presence of missing values.\n\n`metrics.pairwise.pairwise_kernels`(X[, Y, \u2026])\n\nCompute the kernel between arrays X and optional array Y.\n\n`metrics.pairwise.polynomial_kernel`(X[, Y, \u2026])\n\nCompute the polynomial kernel between X and Y.\n\n`metrics.pairwise.rbf_kernel`(X[, Y, gamma])\n\nCompute the rbf (gaussian) kernel between X and Y.\n\n`metrics.pairwise.sigmoid_kernel`(X[, Y, \u2026])\n\nCompute the sigmoid kernel between X and Y.\n\n`metrics.pairwise.paired_euclidean_distances`(X, Y)\n\nComputes the paired euclidean distances between X and Y.\n\n`metrics.pairwise.paired_manhattan_distances`(X, Y)\n\nCompute the L1 distances between the vectors in X and Y.\n\n`metrics.pairwise.paired_cosine_distances`(X, Y)\n\nComputes the paired cosine distances between X and Y.\n\n`metrics.pairwise.paired_distances`(X, Y, *[, \u2026])\n\nComputes the paired distances between X and Y.\n\n`metrics.pairwise_distances`(X[, Y, metric, \u2026])\n\nCompute the distance matrix from a vector array X and optional Y.\n\n`metrics.pairwise_distances_argmin`(X, Y, *[, \u2026])\n\nCompute minimum distances between one point and a set of points.\n\n`metrics.pairwise_distances_argmin_min`(X, Y, *)\n\nCompute minimum distances between one point and a set of points.\n\n`metrics.pairwise_distances_chunked`(X[, Y, \u2026])\n\nGenerate a distance matrix chunk by chunk with optional reduction.\n\nSee the Visualizations section of the user guide for further details.\n\n`metrics.plot_confusion_matrix`(estimator, X, \u2026)\n\nPlot Confusion Matrix.\n\n`metrics.plot_det_curve`(estimator, X, y, *[, \u2026])\n\nPlot detection error tradeoff (DET) curve.\n\n`metrics.plot_precision_recall_curve`(\u2026[, \u2026])\n\nPlot Precision Recall Curve for binary classifiers.\n\n`metrics.plot_roc_curve`(estimator, X, y, *[, \u2026])\n\nPlot Receiver operating characteristic (ROC) curve.\n\n`metrics.ConfusionMatrixDisplay`(\u2026[, \u2026])\n\nConfusion Matrix visualization.\n\n`metrics.DetCurveDisplay`(*, fpr, fnr[, \u2026])\n\nDET curve visualization.\n\n`metrics.PrecisionRecallDisplay`(precision, \u2026)\n\nPrecision Recall visualization.\n\n`metrics.RocCurveDisplay`(*, fpr, tpr[, \u2026])\n\nROC Curve visualization.\n\nThe `sklearn.mixture` module implements mixture modeling algorithms.\n\nUser guide: See the Gaussian mixture models section for further details.\n\n`mixture.BayesianGaussianMixture`(*[, \u2026])\n\nVariational Bayesian estimation of a Gaussian mixture.\n\n`mixture.GaussianMixture`([n_components, \u2026])\n\nGaussian Mixture.\n\nUser guide: See the Cross-validation: evaluating estimator performance, Tuning\nthe hyper-parameters of an estimator and Learning curve sections for further\ndetails.\n\n`model_selection.GroupKFold`([n_splits])\n\nK-fold iterator variant with non-overlapping groups.\n\n`model_selection.GroupShuffleSplit`([\u2026])\n\nShuffle-Group(s)-Out cross-validation iterator\n\n`model_selection.KFold`([n_splits, shuffle, \u2026])\n\nK-Folds cross-validator\n\n`model_selection.LeaveOneGroupOut`()\n\nLeave One Group Out cross-validator\n\n`model_selection.LeavePGroupsOut`(n_groups)\n\nLeave P Group(s) Out cross-validator\n\n`model_selection.LeaveOneOut`()\n\nLeave-One-Out cross-validator\n\n`model_selection.LeavePOut`(p)\n\nLeave-P-Out cross-validator\n\n`model_selection.PredefinedSplit`(test_fold)\n\nPredefined split cross-validator\n\n`model_selection.RepeatedKFold`(*[, n_splits, \u2026])\n\nRepeated K-Fold cross validator.\n\n`model_selection.RepeatedStratifiedKFold`(*[, \u2026])\n\nRepeated Stratified K-Fold cross validator.\n\n`model_selection.ShuffleSplit`([n_splits, \u2026])\n\nRandom permutation cross-validator\n\n`model_selection.StratifiedKFold`([n_splits, \u2026])\n\nStratified K-Folds cross-validator.\n\n`model_selection.StratifiedShuffleSplit`([\u2026])\n\nStratified ShuffleSplit cross-validator\n\n`model_selection.TimeSeriesSplit`([n_splits, \u2026])\n\nTime Series cross-validator\n\n`model_selection.check_cv`([cv, y, classifier])\n\nInput checker utility for building a cross-validator\n\n`model_selection.train_test_split`(*arrays[, \u2026])\n\nSplit arrays or matrices into random train and test subsets\n\n`model_selection.GridSearchCV`(estimator, \u2026)\n\nExhaustive search over specified parameter values for an estimator.\n\n`model_selection.HalvingGridSearchCV`(\u2026[, \u2026])\n\nSearch over specified parameter values with successive halving.\n\n`model_selection.ParameterGrid`(param_grid)\n\nGrid of parameters with a discrete number of values for each.\n\n`model_selection.ParameterSampler`(\u2026[, \u2026])\n\nGenerator on parameters sampled from given distributions.\n\n`model_selection.RandomizedSearchCV`(\u2026[, \u2026])\n\nRandomized search on hyper parameters.\n\n`model_selection.HalvingRandomSearchCV`(\u2026[, \u2026])\n\nRandomized search on hyper parameters.\n\n`model_selection.cross_validate`(estimator, X)\n\nEvaluate metric(s) by cross-validation and also record fit/score times.\n\n`model_selection.cross_val_predict`(estimator, X)\n\nGenerate cross-validated estimates for each input data point\n\n`model_selection.cross_val_score`(estimator, X)\n\nEvaluate a score by cross-validation\n\n`model_selection.learning_curve`(estimator, X, \u2026)\n\nLearning curve.\n\n`model_selection.permutation_test_score`(\u2026)\n\nEvaluate the significance of a cross-validated score with permutations\n\n`model_selection.validation_curve`(estimator, \u2026)\n\nValidation curve.\n\nThe estimators provided in this module are meta-estimators: they require a\nbase estimator to be provided in their constructor. For example, it is\npossible to use these estimators to turn a binary classifier or a regressor\ninto a multiclass classifier. It is also possible to use these estimators with\nmulticlass estimators in the hope that their accuracy or runtime performance\nimproves.\n\nAll classifiers in scikit-learn implement multiclass classification; you only\nneed to use this module if you want to experiment with custom multiclass\nstrategies.\n\nThe one-vs-the-rest meta-classifier also implements a `predict_proba` method,\nso long as such a method is implemented by the base classifier. This method\nreturns probabilities of class membership in both the single label and\nmultilabel case. Note that in the multilabel case, probabilities are the\nmarginal probability that a given sample falls in the given class. As such, in\nthe multilabel case the sum of these probabilities over all possible labels\nfor a given sample will not sum to unity, as they do in the single label case.\n\nUser guide: See the Multiclass classification section for further details.\n\n`multiclass.OneVsRestClassifier`(estimator, *)\n\nOne-vs-the-rest (OvR) multiclass strategy.\n\n`multiclass.OneVsOneClassifier`(estimator, *)\n\nOne-vs-one multiclass strategy\n\n`multiclass.OutputCodeClassifier`(estimator, *)\n\n(Error-Correcting) Output-Code multiclass strategy\n\nThis module implements multioutput regression and classification.\n\nThe estimators provided in this module are meta-estimators: they require a\nbase estimator to be provided in their constructor. The meta-estimator extends\nsingle output estimators to multioutput estimators.\n\nUser guide: See the Multilabel classification, Multiclass-multioutput\nclassification, and Multioutput regression sections for further details.\n\n`multioutput.ClassifierChain`(base_estimator, *)\n\nA multi-label model that arranges binary classifiers into a chain.\n\n`multioutput.MultiOutputRegressor`(estimator, *)\n\nMulti target regression\n\n`multioutput.MultiOutputClassifier`(estimator, *)\n\nMulti target classification\n\n`multioutput.RegressorChain`(base_estimator, *)\n\nA multi-label model that arranges regressions into a chain.\n\nThe `sklearn.naive_bayes` module implements Naive Bayes algorithms. These are\nsupervised learning methods based on applying Bayes\u2019 theorem with strong\n(naive) feature independence assumptions.\n\nUser guide: See the Naive Bayes section for further details.\n\n`naive_bayes.BernoulliNB`(*[, alpha, \u2026])\n\nNaive Bayes classifier for multivariate Bernoulli models.\n\n`naive_bayes.CategoricalNB`(*[, alpha, \u2026])\n\nNaive Bayes classifier for categorical features\n\n`naive_bayes.ComplementNB`(*[, alpha, \u2026])\n\nThe Complement Naive Bayes classifier described in Rennie et al.\n\n`naive_bayes.GaussianNB`(*[, priors, \u2026])\n\nGaussian Naive Bayes (GaussianNB)\n\n`naive_bayes.MultinomialNB`(*[, alpha, \u2026])\n\nNaive Bayes classifier for multinomial models\n\nThe `sklearn.neighbors` module implements the k-nearest neighbors algorithm.\n\nUser guide: See the Nearest Neighbors section for further details.\n\n`neighbors.BallTree`(X[, leaf_size, metric])\n\nBallTree for fast generalized N-point problems\n\n`neighbors.DistanceMetric`\n\nDistanceMetric class\n\n`neighbors.KDTree`(X[, leaf_size, metric])\n\nKDTree for fast generalized N-point problems\n\n`neighbors.KernelDensity`(*[, bandwidth, \u2026])\n\nKernel Density Estimation.\n\n`neighbors.KNeighborsClassifier`([\u2026])\n\nClassifier implementing the k-nearest neighbors vote.\n\n`neighbors.KNeighborsRegressor`([n_neighbors, \u2026])\n\nRegression based on k-nearest neighbors.\n\n`neighbors.KNeighborsTransformer`(*[, mode, \u2026])\n\nTransform X into a (weighted) graph of k nearest neighbors\n\n`neighbors.LocalOutlierFactor`([n_neighbors, \u2026])\n\nUnsupervised Outlier Detection using Local Outlier Factor (LOF)\n\n`neighbors.RadiusNeighborsClassifier`([\u2026])\n\nClassifier implementing a vote among neighbors within a given radius\n\n`neighbors.RadiusNeighborsRegressor`([radius, \u2026])\n\nRegression based on neighbors within a fixed radius.\n\n`neighbors.RadiusNeighborsTransformer`(*[, \u2026])\n\nTransform X into a (weighted) graph of neighbors nearer than a radius\n\n`neighbors.NearestCentroid`([metric, \u2026])\n\nNearest centroid classifier.\n\n`neighbors.NearestNeighbors`(*[, n_neighbors, \u2026])\n\nUnsupervised learner for implementing neighbor searches.\n\n`neighbors.NeighborhoodComponentsAnalysis`([\u2026])\n\nNeighborhood Components Analysis\n\n`neighbors.kneighbors_graph`(X, n_neighbors, *)\n\nComputes the (weighted) graph of k-Neighbors for points in X\n\n`neighbors.radius_neighbors_graph`(X, radius, *)\n\nComputes the (weighted) graph of Neighbors for points in X\n\nThe `sklearn.neural_network` module includes models based on neural networks.\n\nUser guide: See the Neural network models (supervised) and Neural network\nmodels (unsupervised) sections for further details.\n\n`neural_network.BernoulliRBM`([n_components, \u2026])\n\nBernoulli Restricted Boltzmann Machine (RBM).\n\n`neural_network.MLPClassifier`([\u2026])\n\nMulti-layer Perceptron classifier.\n\n`neural_network.MLPRegressor`([\u2026])\n\nMulti-layer Perceptron regressor.\n\nThe `sklearn.pipeline` module implements utilities to build a composite\nestimator, as a chain of transforms and estimators.\n\nUser guide: See the Pipelines and composite estimators section for further\ndetails.\n\n`pipeline.FeatureUnion`(transformer_list, *[, \u2026])\n\nConcatenates results of multiple transformer objects.\n\n`pipeline.Pipeline`(steps, *[, memory, verbose])\n\nPipeline of transforms with a final estimator.\n\n`pipeline.make_pipeline`(*steps[, memory, verbose])\n\nConstruct a Pipeline from the given estimators.\n\n`pipeline.make_union`(*transformers[, n_jobs, \u2026])\n\nConstruct a FeatureUnion from the given transformers.\n\nThe `sklearn.preprocessing` module includes scaling, centering, normalization,\nbinarization methods.\n\nUser guide: See the Preprocessing data section for further details.\n\n`preprocessing.Binarizer`(*[, threshold, copy])\n\nBinarize data (set feature values to 0 or 1) according to a threshold.\n\n`preprocessing.FunctionTransformer`([func, \u2026])\n\nConstructs a transformer from an arbitrary callable.\n\n`preprocessing.KBinsDiscretizer`([n_bins, \u2026])\n\nBin continuous data into intervals.\n\n`preprocessing.KernelCenterer`()\n\nCenter a kernel matrix.\n\n`preprocessing.LabelBinarizer`(*[, neg_label, \u2026])\n\nBinarize labels in a one-vs-all fashion.\n\n`preprocessing.LabelEncoder`()\n\nEncode target labels with value between 0 and n_classes-1.\n\n`preprocessing.MultiLabelBinarizer`(*[, \u2026])\n\nTransform between iterable of iterables and a multilabel format.\n\n`preprocessing.MaxAbsScaler`(*[, copy])\n\nScale each feature by its maximum absolute value.\n\n`preprocessing.MinMaxScaler`([feature_range, \u2026])\n\nTransform features by scaling each feature to a given range.\n\n`preprocessing.Normalizer`([norm, copy])\n\nNormalize samples individually to unit norm.\n\n`preprocessing.OneHotEncoder`(*[, categories, \u2026])\n\nEncode categorical features as a one-hot numeric array.\n\n`preprocessing.OrdinalEncoder`(*[, \u2026])\n\nEncode categorical features as an integer array.\n\n`preprocessing.PolynomialFeatures`([degree, \u2026])\n\nGenerate polynomial and interaction features.\n\n`preprocessing.PowerTransformer`([method, \u2026])\n\nApply a power transform featurewise to make data more Gaussian-like.\n\n`preprocessing.QuantileTransformer`(*[, \u2026])\n\nTransform features using quantiles information.\n\n`preprocessing.RobustScaler`(*[, \u2026])\n\nScale features using statistics that are robust to outliers.\n\n`preprocessing.StandardScaler`(*[, copy, \u2026])\n\nStandardize features by removing the mean and scaling to unit variance\n\n`preprocessing.add_dummy_feature`(X[, value])\n\nAugment dataset with an additional dummy feature.\n\n`preprocessing.binarize`(X, *[, threshold, copy])\n\nBoolean thresholding of array-like or scipy.sparse matrix.\n\n`preprocessing.label_binarize`(y, *, classes)\n\nBinarize labels in a one-vs-all fashion.\n\n`preprocessing.maxabs_scale`(X, *[, axis, copy])\n\nScale each feature to the [-1, 1] range without breaking the sparsity.\n\n`preprocessing.minmax_scale`(X[, \u2026])\n\nTransform features by scaling each feature to a given range.\n\n`preprocessing.normalize`(X[, norm, axis, \u2026])\n\nScale input vectors individually to unit norm (vector length).\n\n`preprocessing.quantile_transform`(X, *[, \u2026])\n\nTransform features using quantiles information.\n\n`preprocessing.robust_scale`(X, *[, axis, \u2026])\n\nStandardize a dataset along any axis\n\n`preprocessing.scale`(X, *[, axis, with_mean, \u2026])\n\nStandardize a dataset along any axis.\n\n`preprocessing.power_transform`(X[, method, \u2026])\n\nPower transforms are a family of parametric, monotonic transformations that\nare applied to make data more Gaussian-like.\n\nRandom Projection transformers.\n\nRandom Projections are a simple and computationally efficient way to reduce\nthe dimensionality of the data by trading a controlled amount of accuracy (as\nadditional variance) for faster processing times and smaller model sizes.\n\nThe dimensions and distribution of Random Projections matrices are controlled\nso as to preserve the pairwise distances between any two samples of the\ndataset.\n\nThe main theoretical result behind the efficiency of random projection is the\nJohnson-Lindenstrauss lemma (quoting Wikipedia):\n\nIn mathematics, the Johnson-Lindenstrauss lemma is a result concerning low-\ndistortion embeddings of points from high-dimensional into low-dimensional\nEuclidean space. The lemma states that a small set of points in a high-\ndimensional space can be embedded into a space of much lower dimension in such\na way that distances between the points are nearly preserved. The map used for\nthe embedding is at least Lipschitz, and can even be taken to be an orthogonal\nprojection.\n\nUser guide: See the Random Projection section for further details.\n\n`random_projection.GaussianRandomProjection`([\u2026])\n\nReduce dimensionality through Gaussian random projection.\n\n`random_projection.SparseRandomProjection`([\u2026])\n\nReduce dimensionality through sparse random projection.\n\n`random_projection.johnson_lindenstrauss_min_dim`(\u2026)\n\nFind a \u2018safe\u2019 number of components to randomly project to.\n\nThe `sklearn.semi_supervised` module implements semi-supervised learning\nalgorithms. These algorithms utilize small amounts of labeled data and large\namounts of unlabeled data for classification tasks. This module includes Label\nPropagation.\n\nUser guide: See the Semi-supervised learning section for further details.\n\n`semi_supervised.LabelPropagation`([kernel, \u2026])\n\nLabel Propagation classifier\n\n`semi_supervised.LabelSpreading`([kernel, \u2026])\n\nLabelSpreading model for semi-supervised learning\n\n`semi_supervised.SelfTrainingClassifier`(\u2026)\n\nSelf-training classifier.\n\nThe `sklearn.svm` module includes Support Vector Machine algorithms.\n\nUser guide: See the Support Vector Machines section for further details.\n\n`svm.LinearSVC`([penalty, loss, dual, tol, C, \u2026])\n\nLinear Support Vector Classification.\n\n`svm.LinearSVR`(*[, epsilon, tol, C, loss, \u2026])\n\nLinear Support Vector Regression.\n\n`svm.NuSVC`(*[, nu, kernel, degree, gamma, \u2026])\n\nNu-Support Vector Classification.\n\n`svm.NuSVR`(*[, nu, C, kernel, degree, gamma, \u2026])\n\nNu Support Vector Regression.\n\n`svm.OneClassSVM`(*[, kernel, degree, gamma, \u2026])\n\nUnsupervised Outlier Detection.\n\n`svm.SVC`(*[, C, kernel, degree, gamma, \u2026])\n\nC-Support Vector Classification.\n\n`svm.SVR`(*[, kernel, degree, gamma, coef0, \u2026])\n\nEpsilon-Support Vector Regression.\n\n`svm.l1_min_c`(X, y, *[, loss, fit_intercept, \u2026])\n\nReturn the lowest bound for C such that for C in (l1_min_C, infinity) the\nmodel is guaranteed not to be empty.\n\nThe `sklearn.tree` module includes decision tree-based models for\nclassification and regression.\n\nUser guide: See the Decision Trees section for further details.\n\n`tree.DecisionTreeClassifier`(*[, criterion, \u2026])\n\nA decision tree classifier.\n\n`tree.DecisionTreeRegressor`(*[, criterion, \u2026])\n\nA decision tree regressor.\n\n`tree.ExtraTreeClassifier`(*[, criterion, \u2026])\n\nAn extremely randomized tree classifier.\n\n`tree.ExtraTreeRegressor`(*[, criterion, \u2026])\n\nAn extremely randomized tree regressor.\n\n`tree.export_graphviz`(decision_tree[, \u2026])\n\nExport a decision tree in DOT format.\n\n`tree.export_text`(decision_tree, *[, \u2026])\n\nBuild a text report showing the rules of a decision tree.\n\n`tree.plot_tree`(decision_tree, *[, \u2026])\n\nPlot a decision tree.\n\nThe `sklearn.utils` module includes various utilities.\n\nDeveloper guide: See the Utilities for Developers page for further details.\n\n`utils.arrayfuncs.min_pos`\n\nFind the minimum value of an array over positive values\n\n`utils.as_float_array`(X, *[, copy, \u2026])\n\nConverts an array-like to an array of floats.\n\n`utils.assert_all_finite`(X, *[, allow_nan])\n\nThrow a ValueError if X contains NaN or infinity.\n\n`utils.Bunch`(**kwargs)\n\nContainer object exposing keys as attributes.\n\n`utils.check_X_y`(X, y[, accept_sparse, \u2026])\n\nInput validation for standard estimators.\n\n`utils.check_array`(array[, accept_sparse, \u2026])\n\nInput validation on an array, list, sparse matrix or similar.\n\n`utils.check_scalar`(x, name, target_type, *)\n\nValidate scalar parameters type and value.\n\n`utils.check_consistent_length`(*arrays)\n\nCheck that all arrays have consistent first dimensions.\n\n`utils.check_random_state`(seed)\n\nTurn seed into a np.random.RandomState instance\n\n`utils.class_weight.compute_class_weight`(\u2026)\n\nEstimate class weights for unbalanced datasets.\n\n`utils.class_weight.compute_sample_weight`(\u2026)\n\nEstimate sample weights by class for unbalanced datasets.\n\n`utils.deprecated`([extra])\n\nDecorator to mark a function or class as deprecated.\n\n`utils.estimator_checks.check_estimator`(Estimator)\n\nCheck if estimator adheres to scikit-learn conventions.\n\n`utils.estimator_checks.parametrize_with_checks`(\u2026)\n\nPytest specific decorator for parametrizing estimator checks.\n\n`utils.estimator_html_repr`(estimator)\n\nBuild a HTML representation of an estimator.\n\n`utils.extmath.safe_sparse_dot`(a, b, *[, \u2026])\n\nDot product that handle the sparse matrix case correctly.\n\n`utils.extmath.randomized_range_finder`(A, *, \u2026)\n\nComputes an orthonormal matrix whose range approximates the range of A.\n\n`utils.extmath.randomized_svd`(M, n_components, *)\n\nComputes a truncated randomized SVD.\n\n`utils.extmath.fast_logdet`(A)\n\nCompute log(det(A)) for A symmetric.\n\n`utils.extmath.density`(w, **kwargs)\n\nCompute density of a sparse vector.\n\n`utils.extmath.weighted_mode`(a, w, *[, axis])\n\nReturns an array of the weighted modal (most common) value in a.\n\n`utils.gen_even_slices`(n, n_packs, *[, n_samples])\n\nGenerator to create n_packs slices going up to n.\n\n`utils.graph.single_source_shortest_path_length`(\u2026)\n\nReturn the shortest path length from source to all reachable nodes.\n\n`utils.graph_shortest_path.graph_shortest_path`\n\nPerform a shortest-path graph search on a positive directed or undirected\ngraph.\n\n`utils.indexable`(*iterables)\n\nMake arrays indexable for cross-validation.\n\n`utils.metaestimators.if_delegate_has_method`(\u2026)\n\nCreate a decorator for methods that are delegated to a sub-estimator\n\n`utils.multiclass.type_of_target`(y)\n\nDetermine the type of data indicated by the target.\n\n`utils.multiclass.is_multilabel`(y)\n\nCheck if `y` is in a multilabel format.\n\n`utils.multiclass.unique_labels`(*ys)\n\nExtract an ordered array of unique labels.\n\n`utils.murmurhash3_32`\n\nCompute the 32bit murmurhash3 of key at seed.\n\n`utils.resample`(*arrays[, replace, \u2026])\n\nResample arrays or sparse matrices in a consistent way.\n\n`utils._safe_indexing`(X, indices, *[, axis])\n\nReturn rows, items or columns of X using indices.\n\n`utils.safe_mask`(X, mask)\n\nReturn a mask which is safe to use on X.\n\n`utils.safe_sqr`(X, *[, copy])\n\nElement wise squaring of array-likes and sparse matrices.\n\n`utils.shuffle`(*arrays[, random_state, n_samples])\n\nShuffle arrays or sparse matrices in a consistent way.\n\n`utils.sparsefuncs.incr_mean_variance_axis`(X, \u2026)\n\nCompute incremental mean and variance along an axis on a CSR or CSC matrix.\n\n`utils.sparsefuncs.inplace_column_scale`(X, scale)\n\nInplace column scaling of a CSC/CSR matrix.\n\n`utils.sparsefuncs.inplace_row_scale`(X, scale)\n\nInplace row scaling of a CSR or CSC matrix.\n\n`utils.sparsefuncs.inplace_swap_row`(X, m, n)\n\nSwaps two rows of a CSC/CSR matrix in-place.\n\n`utils.sparsefuncs.inplace_swap_column`(X, m, n)\n\nSwaps two columns of a CSC/CSR matrix in-place.\n\n`utils.sparsefuncs.mean_variance_axis`(X, axis)\n\nCompute mean and variance along an axis on a CSR or CSC matrix.\n\n`utils.sparsefuncs.inplace_csr_column_scale`(X, \u2026)\n\nInplace column scaling of a CSR matrix.\n\n`utils.sparsefuncs_fast.inplace_csr_row_normalize_l1`\n\nInplace row normalize using the l1 norm\n\n`utils.sparsefuncs_fast.inplace_csr_row_normalize_l2`\n\nInplace row normalize using the l2 norm\n\n`utils.random.sample_without_replacement`\n\nSample integers without replacement.\n\n`utils.validation.check_is_fitted`(estimator)\n\nPerform is_fitted validation for estimator.\n\n`utils.validation.check_memory`(memory)\n\nCheck that `memory` is joblib.Memory-like.\n\n`utils.validation.check_symmetric`(array, *[, \u2026])\n\nMake sure that array is 2D, square and symmetric.\n\n`utils.validation.column_or_1d`(y, *[, warn])\n\nRavel column or 1d numpy array, else raises an error.\n\n`utils.validation.has_fit_parameter`(\u2026)\n\nChecks whether the estimator\u2019s fit method supports the given parameter.\n\n`utils.all_estimators`([type_filter])\n\nGet a list of all estimators from sklearn.\n\nUtilities from joblib:\n\n`utils.parallel_backend`(backend[, n_jobs, \u2026])\n\nChange the default backend used by Parallel inside a with block.\n\n`utils.register_parallel_backend`(name, factory)\n\nRegister a new Parallel backend factory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BaseEstimator", "path": "modules/generated/sklearn.base.baseestimator", "type": "base", "text": "\nBase class for all estimators in scikit-learn.\n\nAll estimators should specify all the parameters that can be set at the class\nlevel in their `__init__` as explicit keyword arguments (no `*args` or\n`**kwargs`).\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nInductive Clustering\n\nApproximate nearest neighbors in TSNE\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BaseEstimator", "path": "modules/generated/sklearn.base.baseestimator#sklearn.base.BaseEstimator", "type": "base", "text": "\nBase class for all estimators in scikit-learn.\n\nAll estimators should specify all the parameters that can be set at the class\nlevel in their `__init__` as explicit keyword arguments (no `*args` or\n`**kwargs`).\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BaseEstimator.get_params()", "path": "modules/generated/sklearn.base.baseestimator#sklearn.base.BaseEstimator.get_params", "type": "base", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BaseEstimator.set_params()", "path": "modules/generated/sklearn.base.baseestimator#sklearn.base.BaseEstimator.set_params", "type": "base", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin", "path": "modules/generated/sklearn.base.biclustermixin", "type": "base", "text": "\nMixin class for all bicluster estimators in scikit-learn.\n\nConvenient way to get row and column indicators together.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin", "path": "modules/generated/sklearn.base.biclustermixin#sklearn.base.BiclusterMixin", "type": "base", "text": "\nMixin class for all bicluster estimators in scikit-learn.\n\nConvenient way to get row and column indicators together.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin.biclusters_()", "path": "modules/generated/sklearn.base.biclustermixin#sklearn.base.BiclusterMixin.biclusters_", "type": "base", "text": "\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin.get_indices()", "path": "modules/generated/sklearn.base.biclustermixin#sklearn.base.BiclusterMixin.get_indices", "type": "base", "text": "\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin.get_shape()", "path": "modules/generated/sklearn.base.biclustermixin#sklearn.base.BiclusterMixin.get_shape", "type": "base", "text": "\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.BiclusterMixin.get_submatrix()", "path": "modules/generated/sklearn.base.biclustermixin#sklearn.base.BiclusterMixin.get_submatrix", "type": "base", "text": "\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClassifierMixin", "path": "modules/generated/sklearn.base.classifiermixin", "type": "base", "text": "\nMixin class for all classifiers in scikit-learn.\n\n`score`(X, y[, sample_weight])\n\nReturn the mean accuracy on the given test data and labels.\n\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClassifierMixin", "path": "modules/generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin", "type": "base", "text": "\nMixin class for all classifiers in scikit-learn.\n\n`score`(X, y[, sample_weight])\n\nReturn the mean accuracy on the given test data and labels.\n\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClassifierMixin.score()", "path": "modules/generated/sklearn.base.classifiermixin#sklearn.base.ClassifierMixin.score", "type": "base", "text": "\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.clone()", "path": "modules/generated/sklearn.base.clone#sklearn.base.clone", "type": "base", "text": "\nConstructs a new unfitted estimator with the same parameters.\n\nClone does a deep copy of the model in an estimator without actually copying\nattached data. It yields a new estimator with the same parameters that has not\nbeen fitted on any data.\n\nIf the estimator\u2019s `random_state` parameter is an integer (or if the estimator\ndoesn\u2019t have a `random_state` parameter), an exact clone is returned: the\nclone and the original estimator will give the exact same results. Otherwise,\nstatistical clone is returned: the clone might yield different results from\nthe original estimator. More details can be found in Controlling randomness.\n\nThe estimator or group of estimators to be cloned.\n\nIf safe is False, clone will fall back to a deep copy on objects that are not\nestimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClusterMixin", "path": "modules/generated/sklearn.base.clustermixin", "type": "base", "text": "\nMixin class for all cluster estimators in scikit-learn.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClusterMixin", "path": "modules/generated/sklearn.base.clustermixin#sklearn.base.ClusterMixin", "type": "base", "text": "\nMixin class for all cluster estimators in scikit-learn.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.ClusterMixin.fit_predict()", "path": "modules/generated/sklearn.base.clustermixin#sklearn.base.ClusterMixin.fit_predict", "type": "base", "text": "\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.DensityMixin", "path": "modules/generated/sklearn.base.densitymixin", "type": "base", "text": "\nMixin class for all density estimators in scikit-learn.\n\n`score`(X[, y])\n\nReturn the score of the model on the data `X`.\n\nReturn the score of the model on the data `X`.\n\nTest samples.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.DensityMixin", "path": "modules/generated/sklearn.base.densitymixin#sklearn.base.DensityMixin", "type": "base", "text": "\nMixin class for all density estimators in scikit-learn.\n\n`score`(X[, y])\n\nReturn the score of the model on the data `X`.\n\nReturn the score of the model on the data `X`.\n\nTest samples.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.DensityMixin.score()", "path": "modules/generated/sklearn.base.densitymixin#sklearn.base.DensityMixin.score", "type": "base", "text": "\nReturn the score of the model on the data `X`.\n\nTest samples.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.is_classifier()", "path": "modules/generated/sklearn.base.is_classifier#sklearn.base.is_classifier", "type": "base", "text": "\nReturn True if the given estimator is (probably) a classifier.\n\nEstimator object to test.\n\nTrue if estimator is a classifier and False otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.is_regressor()", "path": "modules/generated/sklearn.base.is_regressor#sklearn.base.is_regressor", "type": "base", "text": "\nReturn True if the given estimator is (probably) a regressor.\n\nEstimator object to test.\n\nTrue if estimator is a regressor and False otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.RegressorMixin", "path": "modules/generated/sklearn.base.regressormixin#sklearn.base.RegressorMixin", "type": "base", "text": "\nMixin class for all regression estimators in scikit-learn.\n\n`score`(X, y[, sample_weight])\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.RegressorMixin", "path": "modules/generated/sklearn.base.regressormixin", "type": "base", "text": "\nMixin class for all regression estimators in scikit-learn.\n\n`score`(X, y[, sample_weight])\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.RegressorMixin.score()", "path": "modules/generated/sklearn.base.regressormixin#sklearn.base.RegressorMixin.score", "type": "base", "text": "\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.TransformerMixin", "path": "modules/generated/sklearn.base.transformermixin", "type": "base", "text": "\nMixin class for all transformers in scikit-learn.\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\nApproximate nearest neighbors in TSNE\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.TransformerMixin", "path": "modules/generated/sklearn.base.transformermixin#sklearn.base.TransformerMixin", "type": "base", "text": "\nMixin class for all transformers in scikit-learn.\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "base.TransformerMixin.fit_transform()", "path": "modules/generated/sklearn.base.transformermixin#sklearn.base.TransformerMixin.fit_transform", "type": "base", "text": "\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV", "type": "calibration", "text": "\nProbability calibration with isotonic regression or logistic regression.\n\nThis class uses cross-validation to both estimate the parameters of a\nclassifier and subsequently calibrate a classifier. With default\n`ensemble=True`, for each cv split it fits a copy of the base estimator to the\ntraining subset, and calibrates it using the testing subset. For prediction,\npredicted probabilities are averaged across these individual calibrated\nclassifiers. When `ensemble=False`, cross-validation is used to obtain\nunbiased predictions, via `cross_val_predict`, which are then used for\ncalibration. For prediction, the base estimator, trained using all the data,\nis used. This is the method implemented when `probabilities=True` for\n`sklearn.svm` estimators.\n\nAlready fitted classifiers can be calibrated via the parameter `cv=\"prefit\"`.\nIn this case, no cross-validation is used and all provided data is used for\ncalibration. The user has to take care manually that data for model fitting\nand calibration are disjoint.\n\nThe calibration is based on the decision_function method of the\n`base_estimator` if it exists, else on predict_proba.\n\nRead more in the User Guide.\n\nThe classifier whose output need to be calibrated to provide more accurate\n`predict_proba` outputs. The default classifier is a `LinearSVC`.\n\nThe method to use for calibration. Can be \u2018sigmoid\u2019 which corresponds to\nPlatt\u2019s method (i.e. a logistic regression model) or \u2018isotonic\u2019 which is a\nnon-parametric approach. It is not advised to use isotonic calibration with\ntoo few calibration samples `(<<1000)` since it tends to overfit.\n\nDetermines the cross-validation splitting strategy. Possible inputs for cv\nare:\n\nFor integer/None inputs, if `y` is binary or multiclass, `StratifiedKFold` is\nused. If `y` is neither binary nor multiclass, `KFold` is used.\n\nRefer to the User Guide for the various cross-validation strategies that can\nbe used here.\n\nIf \u201cprefit\u201d is passed, it is assumed that `base_estimator` has been fitted\nalready and all data is used for calibration.\n\nChanged in version 0.22: `cv` default value if None changed from 3-fold to\n5-fold.\n\nNumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors.\n\nBase estimator clones are fitted in parallel across cross-validation\niterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\nSee Glossary for more details.\n\nNew in version 0.24.\n\nDetermines how the calibrator is fitted when `cv` is not `'prefit'`. Ignored\nif `cv='prefit'`.\n\nIf `True`, the `base_estimator` is fitted using training data and calibrated\nusing testing data, for each `cv` fold. The final estimator is an ensemble of\n`n_cv` fitted classifer and calibrator pairs, where `n_cv` is the number of\ncross-validation folds. The output is the average predicted probabilities of\nall pairs.\n\nIf `False`, `cv` is used to compute unbiased predictions, via\n`cross_val_predict`, which are then used for calibration. At prediction time,\nthe classifier used is the `base_estimator` trained on all the data. Note that\nthis method is also internally implemented in `sklearn.svm` estimators with\nthe `probabilities=True` parameter.\n\nNew in version 0.24.\n\nThe class labels.\n\nThe list of classifier and calibrator pairs.\n\nChanged in version 0.24: Single calibrated classifier case when\n`ensemble=False`.\n\nObtaining calibrated probability estimates from decision trees and naive\nBayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\nTransforming Classifier Scores into Accurate Multiclass Probability Estimates,\nB. Zadrozny & C. Elkan, (KDD 2002)\n\nProbabilistic Outputs for Support Vector Machines and Comparisons to\nRegularized Likelihood Methods, J. Platt, (1999)\n\nPredicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil &\nR. Caruana, ICML 2005\n\n`fit`(X, y[, sample_weight])\n\nFit the calibrated model.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the target of new samples.\n\n`predict_proba`(X)\n\nCalibrated probabilities of classification.\n\n`score`(X, y[, sample_weight])\n\nReturn the mean accuracy on the given test data and labels.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the calibrated model.\n\nTraining data.\n\nTarget values.\n\nSample weights. If None, then samples are equally weighted.\n\nReturns an instance of self.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the target of new samples. The predicted class is the class that has\nthe highest probability, and can thus be different from the prediction of the\nuncalibrated classifier.\n\nThe samples.\n\nThe predicted class.\n\nCalibrated probabilities of classification.\n\nThis function returns calibrated probabilities of classification according to\neach class on an array of test vectors X.\n\nThe samples.\n\nThe predicted probas.\n\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv", "type": "calibration", "text": "\nProbability calibration with isotonic regression or logistic regression.\n\nThis class uses cross-validation to both estimate the parameters of a\nclassifier and subsequently calibrate a classifier. With default\n`ensemble=True`, for each cv split it fits a copy of the base estimator to the\ntraining subset, and calibrates it using the testing subset. For prediction,\npredicted probabilities are averaged across these individual calibrated\nclassifiers. When `ensemble=False`, cross-validation is used to obtain\nunbiased predictions, via `cross_val_predict`, which are then used for\ncalibration. For prediction, the base estimator, trained using all the data,\nis used. This is the method implemented when `probabilities=True` for\n`sklearn.svm` estimators.\n\nAlready fitted classifiers can be calibrated via the parameter `cv=\"prefit\"`.\nIn this case, no cross-validation is used and all provided data is used for\ncalibration. The user has to take care manually that data for model fitting\nand calibration are disjoint.\n\nThe calibration is based on the decision_function method of the\n`base_estimator` if it exists, else on predict_proba.\n\nRead more in the User Guide.\n\nThe classifier whose output need to be calibrated to provide more accurate\n`predict_proba` outputs. The default classifier is a `LinearSVC`.\n\nThe method to use for calibration. Can be \u2018sigmoid\u2019 which corresponds to\nPlatt\u2019s method (i.e. a logistic regression model) or \u2018isotonic\u2019 which is a\nnon-parametric approach. It is not advised to use isotonic calibration with\ntoo few calibration samples `(<<1000)` since it tends to overfit.\n\nDetermines the cross-validation splitting strategy. Possible inputs for cv\nare:\n\nFor integer/None inputs, if `y` is binary or multiclass, `StratifiedKFold` is\nused. If `y` is neither binary nor multiclass, `KFold` is used.\n\nRefer to the User Guide for the various cross-validation strategies that can\nbe used here.\n\nIf \u201cprefit\u201d is passed, it is assumed that `base_estimator` has been fitted\nalready and all data is used for calibration.\n\nChanged in version 0.22: `cv` default value if None changed from 3-fold to\n5-fold.\n\nNumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors.\n\nBase estimator clones are fitted in parallel across cross-validation\niterations. Therefore parallelism happens only when `cv != \"prefit\"`.\n\nSee Glossary for more details.\n\nNew in version 0.24.\n\nDetermines how the calibrator is fitted when `cv` is not `'prefit'`. Ignored\nif `cv='prefit'`.\n\nIf `True`, the `base_estimator` is fitted using training data and calibrated\nusing testing data, for each `cv` fold. The final estimator is an ensemble of\n`n_cv` fitted classifer and calibrator pairs, where `n_cv` is the number of\ncross-validation folds. The output is the average predicted probabilities of\nall pairs.\n\nIf `False`, `cv` is used to compute unbiased predictions, via\n`cross_val_predict`, which are then used for calibration. At prediction time,\nthe classifier used is the `base_estimator` trained on all the data. Note that\nthis method is also internally implemented in `sklearn.svm` estimators with\nthe `probabilities=True` parameter.\n\nNew in version 0.24.\n\nThe class labels.\n\nThe list of classifier and calibrator pairs.\n\nChanged in version 0.24: Single calibrated classifier case when\n`ensemble=False`.\n\nObtaining calibrated probability estimates from decision trees and naive\nBayesian classifiers, B. Zadrozny & C. Elkan, ICML 2001\n\nTransforming Classifier Scores into Accurate Multiclass Probability Estimates,\nB. Zadrozny & C. Elkan, (KDD 2002)\n\nProbabilistic Outputs for Support Vector Machines and Comparisons to\nRegularized Likelihood Methods, J. Platt, (1999)\n\nPredicting Good Probabilities with Supervised Learning, A. Niculescu-Mizil &\nR. Caruana, ICML 2005\n\n`fit`(X, y[, sample_weight])\n\nFit the calibrated model.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the target of new samples.\n\n`predict_proba`(X)\n\nCalibrated probabilities of classification.\n\n`score`(X, y[, sample_weight])\n\nReturn the mean accuracy on the given test data and labels.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the calibrated model.\n\nTraining data.\n\nTarget values.\n\nSample weights. If None, then samples are equally weighted.\n\nReturns an instance of self.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the target of new samples. The predicted class is the class that has\nthe highest probability, and can thus be different from the prediction of the\nuncalibrated classifier.\n\nThe samples.\n\nThe predicted class.\n\nCalibrated probabilities of classification.\n\nThis function returns calibrated probabilities of classification according to\neach class on an array of test vectors X.\n\nThe samples.\n\nThe predicted probas.\n\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nProbability Calibration curves\n\nProbability calibration of classifiers\n\nProbability Calibration for 3-class classification\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.fit()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.fit", "type": "calibration", "text": "\nFit the calibrated model.\n\nTraining data.\n\nTarget values.\n\nSample weights. If None, then samples are equally weighted.\n\nReturns an instance of self.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.get_params()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.get_params", "type": "calibration", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.predict()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.predict", "type": "calibration", "text": "\nPredict the target of new samples. The predicted class is the class that has\nthe highest probability, and can thus be different from the prediction of the\nuncalibrated classifier.\n\nThe samples.\n\nThe predicted class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.predict_proba()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.predict_proba", "type": "calibration", "text": "\nCalibrated probabilities of classification.\n\nThis function returns calibrated probabilities of classification according to\neach class on an array of test vectors X.\n\nThe samples.\n\nThe predicted probas.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.score()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.score", "type": "calibration", "text": "\nReturn the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for `X`.\n\nSample weights.\n\nMean accuracy of `self.predict(X)` wrt. `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.CalibratedClassifierCV.set_params()", "path": "modules/generated/sklearn.calibration.calibratedclassifiercv#sklearn.calibration.CalibratedClassifierCV.set_params", "type": "calibration", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "calibration.calibration_curve()", "path": "modules/generated/sklearn.calibration.calibration_curve#sklearn.calibration.calibration_curve", "type": "calibration", "text": "\nCompute true and predicted probabilities for a calibration curve.\n\nThe method assumes the inputs come from a binary classifier, and discretize\nthe [0, 1] interval into bins.\n\nCalibration curves may also be referred to as reliability diagrams.\n\nRead more in the User Guide.\n\nTrue targets.\n\nProbabilities of the positive class.\n\nWhether y_prob needs to be normalized into the [0, 1] interval, i.e. is not a\nproper probability. If True, the smallest value in y_prob is linearly mapped\nonto 0 and the largest one onto 1.\n\nNumber of bins to discretize the [0, 1] interval. A bigger number requires\nmore data. Bins with no samples (i.e. without corresponding values in\n`y_prob`) will not be returned, thus the returned arrays may have less than\n`n_bins` values.\n\nStrategy used to define the widths of the bins.\n\nThe bins have identical widths.\n\nThe bins have the same number of samples and depend on `y_prob`.\n\nThe proportion of samples whose class is the positive class, in each bin\n(fraction of positives).\n\nThe mean predicted probability in each bin.\n\nAlexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good\nProbabilities With Supervised Learning, in Proceedings of the 22nd\nInternational Conference on Machine Learning (ICML). See section 4\n(Qualitative Analysis of Predictions).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation", "type": "cluster", "text": "\nPerform Affinity Propagation Clustering of data.\n\nRead more in the User Guide.\n\nDamping factor (between 0.5 and 1) is the extent to which the current value is\nmaintained relative to incoming values (weighted 1 - damping). This in order\nto avoid numerical oscillations when updating these values (messages).\n\nMaximum number of iterations.\n\nNumber of iterations with no change in the number of estimated clusters that\nstops the convergence.\n\nMake a copy of input data.\n\nPreferences for each point - points with larger values of preferences are more\nlikely to be chosen as exemplars. The number of exemplars, ie of clusters, is\ninfluenced by the input preferences value. If the preferences are not passed\nas arguments, they will be set to the median of the input similarities.\n\nWhich affinity to use. At the moment \u2018precomputed\u2019 and `euclidean` are\nsupported. \u2018euclidean\u2019 uses the negative squared euclidean distance between\npoints.\n\nWhether to be verbose.\n\nPseudo-random number generator to control the starting state. Use an int for\nreproducible results across function calls. See the Glossary.\n\nNew in version 0.23: this parameter was previously hardcoded as 0.\n\nIndices of cluster centers.\n\nCluster centers (if affinity != `precomputed`).\n\nLabels of each point.\n\nStores the affinity matrix used in `fit`.\n\nNumber of iterations taken to converge.\n\nFor an example, see examples/cluster/plot_affinity_propagation.py.\n\nThe algorithmic complexity of affinity propagation is quadratic in the number\nof points.\n\nWhen `fit` does not converge, `cluster_centers_` becomes an empty array and\nall training samples will be labelled as `-1`. In addition, `predict` will\nthen label every sample as `-1`.\n\nWhen all training samples have equal similarities and equal preferences, the\nassignment of cluster centers and labels depends on the preference. If the\npreference is smaller than the similarities, `fit` will result in a single\ncluster center and label `0` for every sample. Otherwise, every training\nsample becomes its own cluster center and is assigned a unique label.\n\nBrendan J. Frey and Delbert Dueck, \u201cClustering by Passing Messages Between\nData Points\u201d, Science Feb. 2007\n\n`fit`(X[, y])\n\nFit the clustering from features, or affinity matrix.\n\n`fit_predict`(X[, y])\n\nFit the clustering from features or affinity matrix, and return cluster\nlabels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the closest cluster each sample in X belongs to.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nFit the clustering from features or affinity matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict. If a sparse matrix is provided, it will be converted into\na sparse `csr_matrix`.\n\nCluster labels.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation()", "path": "modules/generated/sklearn.cluster.affinitypropagation", "type": "cluster", "text": "\nPerform Affinity Propagation Clustering of data.\n\nRead more in the User Guide.\n\nDamping factor (between 0.5 and 1) is the extent to which the current value is\nmaintained relative to incoming values (weighted 1 - damping). This in order\nto avoid numerical oscillations when updating these values (messages).\n\nMaximum number of iterations.\n\nNumber of iterations with no change in the number of estimated clusters that\nstops the convergence.\n\nMake a copy of input data.\n\nPreferences for each point - points with larger values of preferences are more\nlikely to be chosen as exemplars. The number of exemplars, ie of clusters, is\ninfluenced by the input preferences value. If the preferences are not passed\nas arguments, they will be set to the median of the input similarities.\n\nWhich affinity to use. At the moment \u2018precomputed\u2019 and `euclidean` are\nsupported. \u2018euclidean\u2019 uses the negative squared euclidean distance between\npoints.\n\nWhether to be verbose.\n\nPseudo-random number generator to control the starting state. Use an int for\nreproducible results across function calls. See the Glossary.\n\nNew in version 0.23: this parameter was previously hardcoded as 0.\n\nIndices of cluster centers.\n\nCluster centers (if affinity != `precomputed`).\n\nLabels of each point.\n\nStores the affinity matrix used in `fit`.\n\nNumber of iterations taken to converge.\n\nFor an example, see examples/cluster/plot_affinity_propagation.py.\n\nThe algorithmic complexity of affinity propagation is quadratic in the number\nof points.\n\nWhen `fit` does not converge, `cluster_centers_` becomes an empty array and\nall training samples will be labelled as `-1`. In addition, `predict` will\nthen label every sample as `-1`.\n\nWhen all training samples have equal similarities and equal preferences, the\nassignment of cluster centers and labels depends on the preference. If the\npreference is smaller than the similarities, `fit` will result in a single\ncluster center and label `0` for every sample. Otherwise, every training\nsample becomes its own cluster center and is assigned a unique label.\n\nBrendan J. Frey and Delbert Dueck, \u201cClustering by Passing Messages Between\nData Points\u201d, Science Feb. 2007\n\n`fit`(X[, y])\n\nFit the clustering from features, or affinity matrix.\n\n`fit_predict`(X[, y])\n\nFit the clustering from features or affinity matrix, and return cluster\nlabels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the closest cluster each sample in X belongs to.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nFit the clustering from features or affinity matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict. If a sparse matrix is provided, it will be converted into\na sparse `csr_matrix`.\n\nCluster labels.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nDemo of affinity propagation clustering algorithm\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation.fit()", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation.fit", "type": "cluster", "text": "\nFit the clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation.fit_predict()", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation.fit_predict", "type": "cluster", "text": "\nFit the clustering from features or affinity matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse feature matrix is provided, it will\nbe converted into a sparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation.get_params()", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation.predict()", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation.predict", "type": "cluster", "text": "\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict. If a sparse matrix is provided, it will be converted into\na sparse `csr_matrix`.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AffinityPropagation.set_params()", "path": "modules/generated/sklearn.cluster.affinitypropagation#sklearn.cluster.AffinityPropagation.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.affinity_propagation()", "path": "modules/generated/sklearn.cluster.affinity_propagation#sklearn.cluster.affinity_propagation", "type": "cluster", "text": "\nPerform Affinity Propagation Clustering of data.\n\nRead more in the User Guide.\n\nMatrix of similarities between points.\n\nPreferences for each point - points with larger values of preferences are more\nlikely to be chosen as exemplars. The number of exemplars, i.e. of clusters,\nis influenced by the input preferences value. If the preferences are not\npassed as arguments, they will be set to the median of the input similarities\n(resulting in a moderate number of clusters). For a smaller amount of\nclusters, this can be set to the minimum value of the similarities.\n\nNumber of iterations with no change in the number of estimated clusters that\nstops the convergence.\n\nMaximum number of iterations\n\nDamping factor between 0.5 and 1.\n\nIf copy is False, the affinity matrix is modified inplace by the algorithm,\nfor memory efficiency.\n\nThe verbosity level.\n\nWhether or not to return the number of iterations.\n\nPseudo-random number generator to control the starting state. Use an int for\nreproducible results across function calls. See the Glossary.\n\nNew in version 0.23: this parameter was previously hardcoded as 0.\n\nIndex of clusters centers.\n\nCluster labels for each point.\n\nNumber of iterations run. Returned only if `return_n_iter` is set to True.\n\nFor an example, see examples/cluster/plot_affinity_propagation.py.\n\nWhen the algorithm does not converge, it returns an empty array as\n`cluster_center_indices` and `-1` as label for each training sample.\n\nWhen all training samples have equal similarities and equal preferences, the\nassignment of cluster centers and labels depends on the preference. If the\npreference is smaller than the similarities, a single cluster center and label\n`0` for every sample will be returned. Otherwise, every training sample\nbecomes its own cluster center and is assigned a unique label.\n\nBrendan J. Frey and Delbert Dueck, \u201cClustering by Passing Messages Between\nData Points\u201d, Science Feb. 2007\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering", "path": "modules/generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering", "type": "cluster", "text": "\nAgglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases a given\nlinkage distance.\n\nRead more in the User Guide.\n\nThe number of clusters to find. It must be `None` if `distance_threshold` is\nnot `None`.\n\nMetric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d,\n\u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d. If linkage is \u201cward\u201d, only\n\u201ceuclidean\u201d is accepted. If \u201cprecomputed\u201d, a distance matrix (instead of a\nsimilarity matrix) is needed as input for the fit method.\n\nUsed to cache the output of the computation of the tree. By default, no\ncaching is done. If a string is given, it is the path to the caching\ndirectory.\n\nConnectivity matrix. Defines for each sample the neighboring samples following\na given structure of the data. This can be a connectivity matrix itself or a\ncallable that transforms the data into a connectivity matrix, such as derived\nfrom kneighbors_graph. Default is `None`, i.e, the hierarchical clustering\nalgorithm is unstructured.\n\nStop early the construction of the tree at `n_clusters`. This is useful to\ndecrease computation time if the number of clusters is not small compared to\nthe number of samples. This option is useful only when specifying a\nconnectivity matrix. Note also that when varying the number of clusters and\nusing caching, it may be advantageous to compute the full tree. It must be\n`True` if `distance_threshold` is not `None`. By default `compute_full_tree`\nis \u201cauto\u201d, which is equivalent to `True` when `distance_threshold` is not\n`None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 *\nn_samples`. Otherwise, \u201cauto\u201d is equivalent to `False`.\n\nWhich linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of observation. The algorithm will merge the\npairs of cluster that minimize this criterion.\n\nNew in version 0.20: Added the \u2018single\u2019 option\n\nThe linkage distance threshold above which, clusters will not be merged. If\nnot `None`, `n_clusters` must be `None` and `compute_full_tree` must be\n`True`.\n\nNew in version 0.21.\n\nComputes distances between clusters even if `distance_threshold` is not used.\nThis can be used to make dendrogram visualization, but introduces a\ncomputational and memory overhead.\n\nNew in version 0.24.\n\nThe number of clusters found by the algorithm. If `distance_threshold=None`,\nit will be equal to the given `n_clusters`.\n\ncluster labels for each point\n\nNumber of leaves in the hierarchical tree.\n\nThe estimated number of connected components in the graph.\n\nNew in version 0.21: `n_connected_components_` was added to replace\n`n_components_`.\n\nThe children of each non-leaf node. Values less than `n_samples` correspond to\nleaves of the tree which are the original samples. A node `i` greater than or\nequal to `n_samples` is a non-leaf node and has children `children_[i -\nn_samples]`. Alternatively at the i-th iteration, children[i][0] and\nchildren[i][1] are merged to form node `n_samples + i`\n\nDistances between nodes in the corresponding place in `children_`. Only\ncomputed if `distance_threshold` is used or `compute_distances` is set to\n`True`.\n\n`fit`(X[, y])\n\nFit the hierarchical clustering from features, or distance matrix.\n\n`fit_predict`(X[, y])\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the hierarchical clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering()", "path": "modules/generated/sklearn.cluster.agglomerativeclustering", "type": "cluster", "text": "\nAgglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases a given\nlinkage distance.\n\nRead more in the User Guide.\n\nThe number of clusters to find. It must be `None` if `distance_threshold` is\nnot `None`.\n\nMetric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d,\n\u201cmanhattan\u201d, \u201ccosine\u201d, or \u201cprecomputed\u201d. If linkage is \u201cward\u201d, only\n\u201ceuclidean\u201d is accepted. If \u201cprecomputed\u201d, a distance matrix (instead of a\nsimilarity matrix) is needed as input for the fit method.\n\nUsed to cache the output of the computation of the tree. By default, no\ncaching is done. If a string is given, it is the path to the caching\ndirectory.\n\nConnectivity matrix. Defines for each sample the neighboring samples following\na given structure of the data. This can be a connectivity matrix itself or a\ncallable that transforms the data into a connectivity matrix, such as derived\nfrom kneighbors_graph. Default is `None`, i.e, the hierarchical clustering\nalgorithm is unstructured.\n\nStop early the construction of the tree at `n_clusters`. This is useful to\ndecrease computation time if the number of clusters is not small compared to\nthe number of samples. This option is useful only when specifying a\nconnectivity matrix. Note also that when varying the number of clusters and\nusing caching, it may be advantageous to compute the full tree. It must be\n`True` if `distance_threshold` is not `None`. By default `compute_full_tree`\nis \u201cauto\u201d, which is equivalent to `True` when `distance_threshold` is not\n`None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 *\nn_samples`. Otherwise, \u201cauto\u201d is equivalent to `False`.\n\nWhich linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of observation. The algorithm will merge the\npairs of cluster that minimize this criterion.\n\nNew in version 0.20: Added the \u2018single\u2019 option\n\nThe linkage distance threshold above which, clusters will not be merged. If\nnot `None`, `n_clusters` must be `None` and `compute_full_tree` must be\n`True`.\n\nNew in version 0.21.\n\nComputes distances between clusters even if `distance_threshold` is not used.\nThis can be used to make dendrogram visualization, but introduces a\ncomputational and memory overhead.\n\nNew in version 0.24.\n\nThe number of clusters found by the algorithm. If `distance_threshold=None`,\nit will be equal to the given `n_clusters`.\n\ncluster labels for each point\n\nNumber of leaves in the hierarchical tree.\n\nThe estimated number of connected components in the graph.\n\nNew in version 0.21: `n_connected_components_` was added to replace\n`n_components_`.\n\nThe children of each non-leaf node. Values less than `n_samples` correspond to\nleaves of the tree which are the original samples. A node `i` greater than or\nequal to `n_samples` is a non-leaf node and has children `children_[i -\nn_samples]`. Alternatively at the i-th iteration, children[i][0] and\nchildren[i][1] are merged to form node `n_samples + i`\n\nDistances between nodes in the corresponding place in `children_`. Only\ncomputed if `distance_threshold` is used or `compute_distances` is set to\n`True`.\n\n`fit`(X[, y])\n\nFit the hierarchical clustering from features, or distance matrix.\n\n`fit_predict`(X[, y])\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the hierarchical clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nPlot Hierarchical Clustering Dendrogram\n\nAgglomerative clustering with and without structure\n\nVarious Agglomerative Clustering on a 2D embedding of digits\n\nA demo of structured Ward hierarchical clustering on an image of coins\n\nHierarchical clustering: structured vs unstructured ward\n\nAgglomerative clustering with different metrics\n\nInductive Clustering\n\nComparing different hierarchical linkage methods on toy datasets\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering.fit()", "path": "modules/generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering.fit", "type": "cluster", "text": "\nFit the hierarchical clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering.fit_predict()", "path": "modules/generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering.fit_predict", "type": "cluster", "text": "\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering.get_params()", "path": "modules/generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.AgglomerativeClustering.set_params()", "path": "modules/generated/sklearn.cluster.agglomerativeclustering#sklearn.cluster.AgglomerativeClustering.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch", "type": "cluster", "text": "\nImplements the Birch clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an alternative\nto `MiniBatchKMeans`. It constructs a tree data structure with the cluster\ncentroids being read off the leaf. These can be either the final cluster\ncentroids or can be provided as input to another clustering algorithm such as\n`AgglomerativeClustering`.\n\nRead more in the User Guide.\n\nNew in version 0.16.\n\nThe radius of the subcluster obtained by merging a new sample and the closest\nsubcluster should be lesser than the threshold. Otherwise a new subcluster is\nstarted. Setting this value to be very low promotes splitting and vice-versa.\n\nMaximum number of CF subclusters in each node. If a new samples enters such\nthat the number of subclusters exceed the branching_factor then that node is\nsplit into two nodes with the subclusters redistributed in each. The parent\nsubcluster of that node is removed and two new subclusters are added as\nparents of the 2 split nodes.\n\nNumber of clusters after the final clustering step, which treats the\nsubclusters from the leaves as new samples.\n\nWhether or not to compute labels for each fit.\n\nWhether or not to make a copy of the given data. If set to False, the initial\ndata will be overwritten.\n\nRoot of the CFTree.\n\nStart pointer to all the leaves.\n\nCentroids of all subclusters read directly from the leaves.\n\nLabels assigned to the centroids of the subclusters after they are clustered\nglobally.\n\nArray of labels assigned to the input data. if partial_fit is used instead of\nfit, they are assigned to the last batch of data.\n\nSee also\n\nAlternative implementation that does incremental updates of the centers\u2019\npositions using mini-batches.\n\nThe tree data structure consists of nodes with each node consisting of a\nnumber of subclusters. The maximum number of subclusters in a node is\ndetermined by the branching factor. Each subcluster maintains a linear sum,\nsquared sum and the number of samples in that subcluster. In addition, each\nsubcluster can also have a node as its child, if the subcluster is not a\nmember of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest to\nit and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of the\nleaf node are updated.\n\n`fit`(X[, y])\n\nBuild a CF Tree for the input data.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`partial_fit`([X, y])\n\nOnline learning.\n\n`predict`(X)\n\nPredict data using the `centroids_` of subclusters.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X into subcluster centroids dimension.\n\nBuild a CF Tree for the input data.\n\nInput data.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nOnline learning. Prevents rebuilding of CFTree from scratch.\n\nInput data. If X is not provided, only the global clustering step is done.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\nPredict data using the `centroids_` of subclusters.\n\nAvoid computation of the row norms of X.\n\nInput data.\n\nLabelled data.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each cluster\ncentroid.\n\nInput data.\n\nTransformed data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch()", "path": "modules/generated/sklearn.cluster.birch", "type": "cluster", "text": "\nImplements the Birch clustering algorithm.\n\nIt is a memory-efficient, online-learning algorithm provided as an alternative\nto `MiniBatchKMeans`. It constructs a tree data structure with the cluster\ncentroids being read off the leaf. These can be either the final cluster\ncentroids or can be provided as input to another clustering algorithm such as\n`AgglomerativeClustering`.\n\nRead more in the User Guide.\n\nNew in version 0.16.\n\nThe radius of the subcluster obtained by merging a new sample and the closest\nsubcluster should be lesser than the threshold. Otherwise a new subcluster is\nstarted. Setting this value to be very low promotes splitting and vice-versa.\n\nMaximum number of CF subclusters in each node. If a new samples enters such\nthat the number of subclusters exceed the branching_factor then that node is\nsplit into two nodes with the subclusters redistributed in each. The parent\nsubcluster of that node is removed and two new subclusters are added as\nparents of the 2 split nodes.\n\nNumber of clusters after the final clustering step, which treats the\nsubclusters from the leaves as new samples.\n\nWhether or not to compute labels for each fit.\n\nWhether or not to make a copy of the given data. If set to False, the initial\ndata will be overwritten.\n\nRoot of the CFTree.\n\nStart pointer to all the leaves.\n\nCentroids of all subclusters read directly from the leaves.\n\nLabels assigned to the centroids of the subclusters after they are clustered\nglobally.\n\nArray of labels assigned to the input data. if partial_fit is used instead of\nfit, they are assigned to the last batch of data.\n\nSee also\n\nAlternative implementation that does incremental updates of the centers\u2019\npositions using mini-batches.\n\nThe tree data structure consists of nodes with each node consisting of a\nnumber of subclusters. The maximum number of subclusters in a node is\ndetermined by the branching factor. Each subcluster maintains a linear sum,\nsquared sum and the number of samples in that subcluster. In addition, each\nsubcluster can also have a node as its child, if the subcluster is not a\nmember of a leaf node.\n\nFor a new point entering the root, it is merged with the subcluster closest to\nit and the linear sum, squared sum and the number of samples of that\nsubcluster are updated. This is done recursively till the properties of the\nleaf node are updated.\n\n`fit`(X[, y])\n\nBuild a CF Tree for the input data.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`partial_fit`([X, y])\n\nOnline learning.\n\n`predict`(X)\n\nPredict data using the `centroids_` of subclusters.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X into subcluster centroids dimension.\n\nBuild a CF Tree for the input data.\n\nInput data.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nOnline learning. Prevents rebuilding of CFTree from scratch.\n\nInput data. If X is not provided, only the global clustering step is done.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\nPredict data using the `centroids_` of subclusters.\n\nAvoid computation of the row norms of X.\n\nInput data.\n\nLabelled data.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each cluster\ncentroid.\n\nInput data.\n\nTransformed data.\n\nCompare BIRCH and MiniBatchKMeans\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.fit()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.fit", "type": "cluster", "text": "\nBuild a CF Tree for the input data.\n\nInput data.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.fit_predict()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.fit_predict", "type": "cluster", "text": "\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.fit_transform()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.fit_transform", "type": "cluster", "text": "\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.get_params()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.partial_fit()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.partial_fit", "type": "cluster", "text": "\nOnline learning. Prevents rebuilding of CFTree from scratch.\n\nInput data. If X is not provided, only the global clustering step is done.\n\nNot used, present here for API consistency by convention.\n\nFitted estimator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.predict()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.predict", "type": "cluster", "text": "\nPredict data using the `centroids_` of subclusters.\n\nAvoid computation of the row norms of X.\n\nInput data.\n\nLabelled data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.set_params()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.Birch.transform()", "path": "modules/generated/sklearn.cluster.birch#sklearn.cluster.Birch.transform", "type": "cluster", "text": "\nTransform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each cluster\ncentroid.\n\nInput data.\n\nTransformed data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.cluster_optics_dbscan()", "path": "modules/generated/sklearn.cluster.cluster_optics_dbscan#sklearn.cluster.cluster_optics_dbscan", "type": "cluster", "text": "\nPerforms DBSCAN extraction for an arbitrary epsilon.\n\nExtracting the clusters runs in linear time. Note that this results in\n`labels_` which are close to a `DBSCAN` with similar settings and `eps`, only\nif `eps` is close to `max_eps`.\n\nReachability distances calculated by OPTICS (`reachability_`)\n\nDistances at which points become core (`core_distances_`)\n\nOPTICS ordered point indices (`ordering_`)\n\nDBSCAN `eps` parameter. Must be set to < `max_eps`. Results will be close to\nDBSCAN algorithm if `eps` and `max_eps` are close to one another.\n\nThe estimated labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.cluster_optics_xi()", "path": "modules/generated/sklearn.cluster.cluster_optics_xi#sklearn.cluster.cluster_optics_xi", "type": "cluster", "text": "\nAutomatically extract clusters according to the Xi-steep method.\n\nReachability distances calculated by OPTICS (`reachability_`)\n\nPredecessors calculated by OPTICS.\n\nOPTICS ordered point indices (`ordering_`)\n\nThe same as the min_samples given to OPTICS. Up and down steep regions can\u2019t\nhave more then `min_samples` consecutive non-steep points. Expressed as an\nabsolute number or a fraction of the number of samples (rounded to be at least\n2).\n\nMinimum number of samples in an OPTICS cluster, expressed as an absolute\nnumber or a fraction of the number of samples (rounded to be at least 2). If\n`None`, the value of `min_samples` is used instead.\n\nDetermines the minimum steepness on the reachability plot that constitutes a\ncluster boundary. For example, an upwards point in the reachability plot is\ndefined by the ratio from one point to its successor being at most 1-xi.\n\nCorrect clusters based on the calculated predecessors.\n\nThe labels assigned to samples. Points which are not included in any cluster\nare labeled as -1.\n\nThe list of clusters in the form of `[start, end]` in each row, with all\nindices inclusive. The clusters are ordered according to `(end, -start)`\n(ascending) so that larger clusters encompassing smaller clusters come after\nsuch nested smaller clusters. Since `labels` does not reflect the hierarchy,\nusually `len(clusters) > np.unique(labels)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.compute_optics_graph()", "path": "modules/generated/sklearn.cluster.compute_optics_graph#sklearn.cluster.compute_optics_graph", "type": "cluster", "text": "\nComputes the OPTICS reachability graph.\n\nRead more in the User Guide.\n\nA feature array, or array of distances between samples if metric=\u2019precomputed\u2019\n\nThe number of samples in a neighborhood for a point to be considered as a core\npoint. Expressed as an absolute number or a fraction of the number of samples\n(rounded to be at least 2).\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. Default value of `np.inf` will identify clusters\nacross all scales; reducing `max_eps` will result in shorter run times.\n\nMetric to use for distance computation. Any metric from scikit-learn or\nscipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each pair of instances\n(rows) and the resulting value recorded. The callable should take two arrays\nas input and return one value indicating the distance between them. This works\nfor Scipy\u2019s metrics, but is less efficient than passing the metric name as a\nstring. If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and\nmust be square.\n\nValid values for metric are:\n\nSee the documentation for scipy.spatial.distance for details on these metrics.\n\nParameter for the Minkowski metric from `pairwise_distances`. When p = 1, this\nis equivalent to using manhattan_distance (l1), and euclidean_distance (l2)\nfor p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nAdditional keyword arguments for the metric function.\n\nAlgorithm used to compute the nearest neighbors:\n\nNote: fitting on sparse input will override the setting of this parameter,\nusing brute force.\n\nLeaf size passed to `BallTree` or `KDTree`. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe number of parallel jobs to run for neighbors search. `None` means 1 unless\nin a `joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nThe cluster ordered list of sample indices.\n\nDistance at which each sample becomes a core point, indexed by object order.\nPoints which will never be core have a distance of inf. Use\n`clust.core_distances_[clust.ordering_]` to access in cluster order.\n\nReachability distances per sample, indexed by object order. Use\n`clust.reachability_[clust.ordering_]` to access in cluster order.\n\nPoint that a sample was reached from, indexed by object order. Seed points\nhave a predecessor of -1.\n\nAnkerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander.\n\u201cOPTICS: ordering points to identify the clustering structure.\u201d ACM SIGMOD\nRecord 28, no. 2 (1999): 49-60.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN", "path": "modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN", "type": "cluster", "text": "\nPerform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds\ncore samples of high density and expands clusters from them. Good for data\nwhich contains clusters of similar density.\n\nRead more in the User Guide.\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. This is not a maximum bound on the distances of\npoints within a cluster. This is the most important DBSCAN parameter to choose\nappropriately for your data set and distance function.\n\nThe number of samples (or total weight) in a neighborhood for a point to be\nconsidered as a core point. This includes the point itself.\n\nThe metric to use when calculating distance between instances in a feature\narray. If metric is a string or callable, it must be one of the options\nallowed by `sklearn.metrics.pairwise_distances` for its metric parameter. If\nmetric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and must be\nsquare. X may be a Glossary, in which case only \u201cnonzero\u201d elements may be\nconsidered neighbors for DBSCAN.\n\nNew in version 0.17: metric precomputed to accept precomputed sparse matrix.\n\nAdditional keyword arguments for the metric function.\n\nNew in version 0.19.\n\nThe algorithm to be used by the NearestNeighbors module to compute pointwise\ndistances and find nearest neighbors. See NearestNeighbors module\ndocumentation for details.\n\nLeaf size passed to BallTree or cKDTree. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe power of the Minkowski metric to be used to calculate distance between\npoints. If None, then `p=2` (equivalent to the Euclidean distance).\n\nThe number of parallel jobs to run. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nIndices of core samples.\n\nCopy of each core sample found by training.\n\nCluster labels for each point in the dataset given to fit(). Noisy samples are\ngiven the label -1.\n\nSee also\n\nA similar clustering at multiple values of eps. Our implementation is\noptimized for memory usage.\n\nFor an example, see examples/cluster/plot_dbscan.py.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending on the\n`algorithm`.\n\nOne way to avoid the query complexity is to pre-compute sparse neighborhoods\nin chunks using `NearestNeighbors.radius_neighbors_graph` with\n`mode='distance'`, then using `metric='precomputed'` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use `sample_weight` instead.\n\n`cluster.OPTICS` provides a similar clustering with lower memory usage.\n\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \u201cA Density-Based Algorithm for\nDiscovering Clusters in Large Spatial Databases with Noise\u201d. In: Proceedings\nof the 2nd International Conference on Knowledge Discovery and Data Mining,\nPortland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN\nrevisited, revisited: why and how you should (still) use DBSCAN. ACM\nTransactions on Database Systems (TODS), 42(3), 19.\n\n`fit`(X[, y, sample_weight])\n\nPerform DBSCAN clustering from features, or distance matrix.\n\n`fit_predict`(X[, y, sample_weight])\n\nPerform DBSCAN clustering from features or distance matrix, and return cluster\nlabels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform DBSCAN clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\nPerform DBSCAN clustering from features or distance matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\nCluster labels. Noisy samples are given the label -1.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN()", "path": "modules/generated/sklearn.cluster.dbscan", "type": "cluster", "text": "\nPerform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds\ncore samples of high density and expands clusters from them. Good for data\nwhich contains clusters of similar density.\n\nRead more in the User Guide.\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. This is not a maximum bound on the distances of\npoints within a cluster. This is the most important DBSCAN parameter to choose\nappropriately for your data set and distance function.\n\nThe number of samples (or total weight) in a neighborhood for a point to be\nconsidered as a core point. This includes the point itself.\n\nThe metric to use when calculating distance between instances in a feature\narray. If metric is a string or callable, it must be one of the options\nallowed by `sklearn.metrics.pairwise_distances` for its metric parameter. If\nmetric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and must be\nsquare. X may be a Glossary, in which case only \u201cnonzero\u201d elements may be\nconsidered neighbors for DBSCAN.\n\nNew in version 0.17: metric precomputed to accept precomputed sparse matrix.\n\nAdditional keyword arguments for the metric function.\n\nNew in version 0.19.\n\nThe algorithm to be used by the NearestNeighbors module to compute pointwise\ndistances and find nearest neighbors. See NearestNeighbors module\ndocumentation for details.\n\nLeaf size passed to BallTree or cKDTree. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe power of the Minkowski metric to be used to calculate distance between\npoints. If None, then `p=2` (equivalent to the Euclidean distance).\n\nThe number of parallel jobs to run. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nIndices of core samples.\n\nCopy of each core sample found by training.\n\nCluster labels for each point in the dataset given to fit(). Noisy samples are\ngiven the label -1.\n\nSee also\n\nA similar clustering at multiple values of eps. Our implementation is\noptimized for memory usage.\n\nFor an example, see examples/cluster/plot_dbscan.py.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending on the\n`algorithm`.\n\nOne way to avoid the query complexity is to pre-compute sparse neighborhoods\nin chunks using `NearestNeighbors.radius_neighbors_graph` with\n`mode='distance'`, then using `metric='precomputed'` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use `sample_weight` instead.\n\n`cluster.OPTICS` provides a similar clustering with lower memory usage.\n\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \u201cA Density-Based Algorithm for\nDiscovering Clusters in Large Spatial Databases with Noise\u201d. In: Proceedings\nof the 2nd International Conference on Knowledge Discovery and Data Mining,\nPortland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN\nrevisited, revisited: why and how you should (still) use DBSCAN. ACM\nTransactions on Database Systems (TODS), 42(3), 19.\n\n`fit`(X[, y, sample_weight])\n\nPerform DBSCAN clustering from features, or distance matrix.\n\n`fit_predict`(X[, y, sample_weight])\n\nPerform DBSCAN clustering from features or distance matrix, and return cluster\nlabels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform DBSCAN clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\nPerform DBSCAN clustering from features or distance matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\nCluster labels. Noisy samples are given the label -1.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nDemo of DBSCAN clustering algorithm\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.dbscan()", "path": "modules/generated/dbscan-function#sklearn.cluster.dbscan", "type": "cluster", "text": "\nPerform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the User Guide.\n\nA feature array, or array of distances between samples if\n`metric='precomputed'`.\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. This is not a maximum bound on the distances of\npoints within a cluster. This is the most important DBSCAN parameter to choose\nappropriately for your data set and distance function.\n\nThe number of samples (or total weight) in a neighborhood for a point to be\nconsidered as a core point. This includes the point itself.\n\nThe metric to use when calculating distance between instances in a feature\narray. If metric is a string or callable, it must be one of the options\nallowed by `sklearn.metrics.pairwise_distances` for its metric parameter. If\nmetric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and must be\nsquare during fit. X may be a sparse graph, in which case only \u201cnonzero\u201d\nelements may be considered neighbors.\n\nAdditional keyword arguments for the metric function.\n\nNew in version 0.19.\n\nThe algorithm to be used by the NearestNeighbors module to compute pointwise\ndistances and find nearest neighbors. See NearestNeighbors module\ndocumentation for details.\n\nLeaf size passed to BallTree or cKDTree. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe power of the Minkowski metric to be used to calculate distance between\npoints.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nThe number of parallel jobs to run for neighbors search. `None` means 1 unless\nin a `joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details. If precomputed distance are used, parallel\nexecution is not available and thus n_jobs will have no effect.\n\nIndices of core samples.\n\nCluster labels for each point. Noisy samples are given the label -1.\n\nSee also\n\nAn estimator interface for this clustering algorithm.\n\nA similar estimator interface clustering at multiple values of eps. Our\nimplementation is optimized for memory usage.\n\nFor an example, see examples/cluster/plot_dbscan.py.\n\nThis implementation bulk-computes all neighborhood queries, which increases\nthe memory complexity to O(n.d) where d is the average number of neighbors,\nwhile original DBSCAN had memory complexity O(n). It may attract a higher\nmemory complexity when querying these nearest neighborhoods, depending on the\n`algorithm`.\n\nOne way to avoid the query complexity is to pre-compute sparse neighborhoods\nin chunks using `NearestNeighbors.radius_neighbors_graph` with\n`mode='distance'`, then using `metric='precomputed'` here.\n\nAnother way to reduce memory and computation time is to remove\n(near-)duplicate points and use `sample_weight` instead.\n\n`cluster.optics` provides a similar clustering with lower memory usage.\n\nEster, M., H. P. Kriegel, J. Sander, and X. Xu, \u201cA Density-Based Algorithm for\nDiscovering Clusters in Large Spatial Databases with Noise\u201d. In: Proceedings\nof the 2nd International Conference on Knowledge Discovery and Data Mining,\nPortland, OR, AAAI Press, pp. 226-231. 1996\n\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). DBSCAN\nrevisited, revisited: why and how you should (still) use DBSCAN. ACM\nTransactions on Database Systems (TODS), 42(3), 19.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN.fit()", "path": "modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN.fit", "type": "cluster", "text": "\nPerform DBSCAN clustering from features, or distance matrix.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN.fit_predict()", "path": "modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN.fit_predict", "type": "cluster", "text": "\nPerform DBSCAN clustering from features or distance matrix, and return cluster\nlabels.\n\nTraining instances to cluster, or distances between instances if\n`metric='precomputed'`. If a sparse matrix is provided, it will be converted\ninto a sparse `csr_matrix`.\n\nWeight of each sample, such that a sample with a weight of at least\n`min_samples` is by itself a core sample; a sample with a negative weight may\ninhibit its eps-neighbor from being core. Note that weights are absolute, and\ndefault to 1.\n\nNot used, present here for API consistency by convention.\n\nCluster labels. Noisy samples are given the label -1.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN.get_params()", "path": "modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.DBSCAN.set_params()", "path": "modules/generated/sklearn.cluster.dbscan#sklearn.cluster.DBSCAN.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.estimate_bandwidth()", "path": "modules/generated/sklearn.cluster.estimate_bandwidth#sklearn.cluster.estimate_bandwidth", "type": "cluster", "text": "\nEstimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it\u2019s wise to set that parameter to a small value.\n\nInput points.\n\nshould be between [0, 1] 0.5 means that the median of all pairwise distances\nis used.\n\nThe number of samples to use. If not given, all samples are used.\n\nThe generator used to randomly select the samples from input points for\nbandwidth estimation. Use an int to make the randomness deterministic. See\nGlossary.\n\nThe number of parallel jobs to run for neighbors search. `None` means 1 unless\nin a `joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nThe bandwidth parameter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration", "type": "cluster", "text": "\nAgglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features instead of\nsamples.\n\nRead more in the User Guide.\n\nThe number of clusters to find. It must be `None` if `distance_threshold` is\nnot `None`.\n\nMetric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d,\n\u201cmanhattan\u201d, \u201ccosine\u201d, or \u2018precomputed\u2019. If linkage is \u201cward\u201d, only\n\u201ceuclidean\u201d is accepted.\n\nUsed to cache the output of the computation of the tree. By default, no\ncaching is done. If a string is given, it is the path to the caching\ndirectory.\n\nConnectivity matrix. Defines for each feature the neighboring features\nfollowing a given structure of the data. This can be a connectivity matrix\nitself or a callable that transforms the data into a connectivity matrix, such\nas derived from kneighbors_graph. Default is None, i.e, the hierarchical\nclustering algorithm is unstructured.\n\nStop early the construction of the tree at n_clusters. This is useful to\ndecrease computation time if the number of clusters is not small compared to\nthe number of features. This option is useful only when specifying a\nconnectivity matrix. Note also that when varying the number of clusters and\nusing caching, it may be advantageous to compute the full tree. It must be\n`True` if `distance_threshold` is not `None`. By default `compute_full_tree`\nis \u201cauto\u201d, which is equivalent to `True` when `distance_threshold` is not\n`None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 *\nn_samples`. Otherwise, \u201cauto\u201d is equivalent to `False`.\n\nWhich linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of features. The algorithm will merge the pairs\nof cluster that minimize this criterion.\n\nThis combines the values of agglomerated features into a single value, and\nshould accept an array of shape [M, N] and the keyword argument `axis=1`, and\nreduce it to an array of size [M].\n\nThe linkage distance threshold above which, clusters will not be merged. If\nnot `None`, `n_clusters` must be `None` and `compute_full_tree` must be\n`True`.\n\nNew in version 0.21.\n\nComputes distances between clusters even if `distance_threshold` is not used.\nThis can be used to make dendrogram visualization, but introduces a\ncomputational and memory overhead.\n\nNew in version 0.24.\n\nThe number of clusters found by the algorithm. If `distance_threshold=None`,\nit will be equal to the given `n_clusters`.\n\ncluster labels for each feature.\n\nNumber of leaves in the hierarchical tree.\n\nThe estimated number of connected components in the graph.\n\nNew in version 0.21: `n_connected_components_` was added to replace\n`n_components_`.\n\nThe children of each non-leaf node. Values less than `n_features` correspond\nto leaves of the tree which are the original samples. A node `i` greater than\nor equal to `n_features` is a non-leaf node and has children `children_[i -\nn_features]`. Alternatively at the i-th iteration, children[i][0] and\nchildren[i][1] are merged to form node `n_features + i`\n\nDistances between nodes in the corresponding place in `children_`. Only\ncomputed if `distance_threshold` is used or `compute_distances` is set to\n`True`.\n\n`fit`(X[, y])\n\nFit the hierarchical clustering on the data\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`inverse_transform`(Xred)\n\nInverse the transformation.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform a new matrix using the built clustering\n\nFit the hierarchical clustering on the data\n\nThe data\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nInverse the transformation. Return a vector of size nb_features with the\nvalues of Xred assigned to each group of features\n\nThe values to be assigned to each cluster of samples\n\nA vector of size n_samples with the values of Xred assigned to each of the\ncluster of samples.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform a new matrix using the built clustering\n\nA M by N array of M observations in N dimensions or a length M array of M one-\ndimensional observations.\n\nThe pooled values for each feature cluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration()", "path": "modules/generated/sklearn.cluster.featureagglomeration", "type": "cluster", "text": "\nAgglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features instead of\nsamples.\n\nRead more in the User Guide.\n\nThe number of clusters to find. It must be `None` if `distance_threshold` is\nnot `None`.\n\nMetric used to compute the linkage. Can be \u201ceuclidean\u201d, \u201cl1\u201d, \u201cl2\u201d,\n\u201cmanhattan\u201d, \u201ccosine\u201d, or \u2018precomputed\u2019. If linkage is \u201cward\u201d, only\n\u201ceuclidean\u201d is accepted.\n\nUsed to cache the output of the computation of the tree. By default, no\ncaching is done. If a string is given, it is the path to the caching\ndirectory.\n\nConnectivity matrix. Defines for each feature the neighboring features\nfollowing a given structure of the data. This can be a connectivity matrix\nitself or a callable that transforms the data into a connectivity matrix, such\nas derived from kneighbors_graph. Default is None, i.e, the hierarchical\nclustering algorithm is unstructured.\n\nStop early the construction of the tree at n_clusters. This is useful to\ndecrease computation time if the number of clusters is not small compared to\nthe number of features. This option is useful only when specifying a\nconnectivity matrix. Note also that when varying the number of clusters and\nusing caching, it may be advantageous to compute the full tree. It must be\n`True` if `distance_threshold` is not `None`. By default `compute_full_tree`\nis \u201cauto\u201d, which is equivalent to `True` when `distance_threshold` is not\n`None` or that `n_clusters` is inferior to the maximum between 100 or `0.02 *\nn_samples`. Otherwise, \u201cauto\u201d is equivalent to `False`.\n\nWhich linkage criterion to use. The linkage criterion determines which\ndistance to use between sets of features. The algorithm will merge the pairs\nof cluster that minimize this criterion.\n\nThis combines the values of agglomerated features into a single value, and\nshould accept an array of shape [M, N] and the keyword argument `axis=1`, and\nreduce it to an array of size [M].\n\nThe linkage distance threshold above which, clusters will not be merged. If\nnot `None`, `n_clusters` must be `None` and `compute_full_tree` must be\n`True`.\n\nNew in version 0.21.\n\nComputes distances between clusters even if `distance_threshold` is not used.\nThis can be used to make dendrogram visualization, but introduces a\ncomputational and memory overhead.\n\nNew in version 0.24.\n\nThe number of clusters found by the algorithm. If `distance_threshold=None`,\nit will be equal to the given `n_clusters`.\n\ncluster labels for each feature.\n\nNumber of leaves in the hierarchical tree.\n\nThe estimated number of connected components in the graph.\n\nNew in version 0.21: `n_connected_components_` was added to replace\n`n_components_`.\n\nThe children of each non-leaf node. Values less than `n_features` correspond\nto leaves of the tree which are the original samples. A node `i` greater than\nor equal to `n_features` is a non-leaf node and has children `children_[i -\nn_features]`. Alternatively at the i-th iteration, children[i][0] and\nchildren[i][1] are merged to form node `n_features + i`\n\nDistances between nodes in the corresponding place in `children_`. Only\ncomputed if `distance_threshold` is used or `compute_distances` is set to\n`True`.\n\n`fit`(X[, y])\n\nFit the hierarchical clustering on the data\n\n`fit_transform`(X[, y])\n\nFit to data, then transform it.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`inverse_transform`(Xred)\n\nInverse the transformation.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform a new matrix using the built clustering\n\nFit the hierarchical clustering on the data\n\nThe data\n\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nInverse the transformation. Return a vector of size nb_features with the\nvalues of Xred assigned to each group of features\n\nThe values to be assigned to each cluster of samples\n\nA vector of size n_samples with the values of Xred assigned to each of the\ncluster of samples.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform a new matrix using the built clustering\n\nA M by N array of M observations in N dimensions or a length M array of M one-\ndimensional observations.\n\nThe pooled values for each feature cluster.\n\nFeature agglomeration\n\nFeature agglomeration vs. univariate selection\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.fit()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.fit", "type": "cluster", "text": "\nFit the hierarchical clustering on the data\n\nThe data\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.fit_predict()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.fit_predict", "type": "cluster", "text": "\nFit the hierarchical clustering from features or distance matrix, and return\ncluster labels.\n\nTraining instances to cluster, or distances between instances if\n`affinity='precomputed'`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.fit_transform()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.fit_transform", "type": "cluster", "text": "\nFit to data, then transform it.\n\nFits transformer to `X` and `y` with optional parameters `fit_params` and\nreturns a transformed version of `X`.\n\nInput samples.\n\nTarget values (None for unsupervised transformations).\n\nAdditional fit parameters.\n\nTransformed array.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.get_params()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.inverse_transform()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.inverse_transform", "type": "cluster", "text": "\nInverse the transformation. Return a vector of size nb_features with the\nvalues of Xred assigned to each group of features\n\nThe values to be assigned to each cluster of samples\n\nA vector of size n_samples with the values of Xred assigned to each of the\ncluster of samples.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.set_params()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.FeatureAgglomeration.transform()", "path": "modules/generated/sklearn.cluster.featureagglomeration#sklearn.cluster.FeatureAgglomeration.transform", "type": "cluster", "text": "\nTransform a new matrix using the built clustering\n\nA M by N array of M observations in N dimensions or a length M array of M one-\ndimensional observations.\n\nThe pooled values for each feature cluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans", "type": "cluster", "text": "\nK-Means clustering.\n\nRead more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate.\n\nMethod for initialization:\n\n\u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart\nway to speed up convergence. See section Notes in k_init for more details.\n\n\u2018random\u2019: choose `n_clusters` observations (rows) at random from data for the\ninitial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features) and\ngives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a random\nstate and return an initialization.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nMaximum number of iterations of the k-means algorithm for a single run.\n\nRelative tolerance with regards to Frobenius norm of the difference in the\ncluster centers of two consecutive iterations to declare convergence.\n\nPrecompute distances (faster but takes more memory).\n\n\u2018auto\u2019 : do not precompute distances if n_samples * n_clusters > 12 million.\nThis corresponds to about 100MB overhead per job using double precision.\n\nTrue : always precompute distances.\n\nFalse : never precompute distances.\n\nDeprecated since version 0.23: \u2018precompute_distances\u2019 was deprecated in\nversion 0.22 and will be removed in 1.0 (renaming of 0.25). It has no effect.\n\nVerbosity mode.\n\nDetermines random number generation for centroid initialization. Use an int to\nmake the randomness deterministic. See Glossary.\n\nWhen pre-computing distances it is more numerically accurate to center the\ndata first. If copy_x is True (default), then the original data is not\nmodified. If False, the original data is modified, and put back before the\nfunction returns, but small numerical differences may be introduced by\nsubtracting and then adding the data mean. Note that if the original data is\nnot C-contiguous, a copy will be made even if copy_x is False. If the original\ndata is sparse, but not in CSR format, a copy will be made even if copy_x is\nFalse.\n\nThe number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its closest\ncenter.\n\n`None` or `-1` means using all processors.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nK-means algorithm to use. The classical EM-style algorithm is \u201cfull\u201d. The\n\u201celkan\u201d variation is more efficient on data with well-defined clusters, by\nusing the triangle inequality. However it\u2019s more memory intensive due to the\nallocation of an extra array of shape (n_samples, n_clusters).\n\nFor now \u201cauto\u201d (kept for backward compatibiliy) chooses \u201celkan\u201d but it might\nchange in the future for a better heuristic.\n\nChanged in version 0.18: Added Elkan algorithm\n\nCoordinates of cluster centers. If the algorithm stops before fully converging\n(see `tol` and `max_iter`), these will not be consistent with `labels_`.\n\nLabels of each point\n\nSum of squared distances of samples to their closest cluster center.\n\nNumber of iterations run.\n\nSee also\n\nAlternative online implementation that does incremental updates of the centers\npositions using mini-batches. For large scale learning (say n_samples > 10k)\nMiniBatchKMeans is probably much faster than the default batch implementation.\n\nThe k-means problem is solved using either Lloyd\u2019s or Elkan\u2019s algorithm.\n\nThe average complexity is given by O(k n T), were n is the number of samples\nand T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p =\nn_features. (D. Arthur and S. Vassilvitskii, \u2018How slow is the k-means method?\u2019\nSoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering\nalgorithms available), but it falls in local minima. That\u2019s why it can be\nuseful to restart it several times.\n\nIf the algorithm stops before fully converging (because of `tol` or\n`max_iter`), `labels_` and `cluster_centers_` will not be consistent, i.e. the\n`cluster_centers_` will not be the means of the points in each cluster. Also,\nthe estimator will reassign `labels_` after the last iteration to make\n`labels_` consistent with `predict` on the training set.\n\n`fit`(X[, y, sample_weight])\n\nCompute k-means clustering.\n\n`fit_predict`(X[, y, sample_weight])\n\nCompute cluster centers and predict cluster index for each sample.\n\n`fit_transform`(X[, y, sample_weight])\n\nCompute clustering and transform X to cluster-distance space.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X[, sample_weight])\n\nPredict the closest cluster each sample in X belongs to.\n\n`score`(X[, y, sample_weight])\n\nOpposite of the value of X on the K-means objective.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X to a cluster-distance space.\n\nCompute k-means clustering.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous. If a sparse matrix is passed, a copy will be made if it\u2019s\nnot in CSR format.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nNew in version 0.20.\n\nFitted estimator.\n\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans()", "path": "modules/generated/sklearn.cluster.kmeans", "type": "cluster", "text": "\nK-Means clustering.\n\nRead more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate.\n\nMethod for initialization:\n\n\u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart\nway to speed up convergence. See section Notes in k_init for more details.\n\n\u2018random\u2019: choose `n_clusters` observations (rows) at random from data for the\ninitial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features) and\ngives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a random\nstate and return an initialization.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nMaximum number of iterations of the k-means algorithm for a single run.\n\nRelative tolerance with regards to Frobenius norm of the difference in the\ncluster centers of two consecutive iterations to declare convergence.\n\nPrecompute distances (faster but takes more memory).\n\n\u2018auto\u2019 : do not precompute distances if n_samples * n_clusters > 12 million.\nThis corresponds to about 100MB overhead per job using double precision.\n\nTrue : always precompute distances.\n\nFalse : never precompute distances.\n\nDeprecated since version 0.23: \u2018precompute_distances\u2019 was deprecated in\nversion 0.22 and will be removed in 1.0 (renaming of 0.25). It has no effect.\n\nVerbosity mode.\n\nDetermines random number generation for centroid initialization. Use an int to\nmake the randomness deterministic. See Glossary.\n\nWhen pre-computing distances it is more numerically accurate to center the\ndata first. If copy_x is True (default), then the original data is not\nmodified. If False, the original data is modified, and put back before the\nfunction returns, but small numerical differences may be introduced by\nsubtracting and then adding the data mean. Note that if the original data is\nnot C-contiguous, a copy will be made even if copy_x is False. If the original\ndata is sparse, but not in CSR format, a copy will be made even if copy_x is\nFalse.\n\nThe number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its closest\ncenter.\n\n`None` or `-1` means using all processors.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nK-means algorithm to use. The classical EM-style algorithm is \u201cfull\u201d. The\n\u201celkan\u201d variation is more efficient on data with well-defined clusters, by\nusing the triangle inequality. However it\u2019s more memory intensive due to the\nallocation of an extra array of shape (n_samples, n_clusters).\n\nFor now \u201cauto\u201d (kept for backward compatibiliy) chooses \u201celkan\u201d but it might\nchange in the future for a better heuristic.\n\nChanged in version 0.18: Added Elkan algorithm\n\nCoordinates of cluster centers. If the algorithm stops before fully converging\n(see `tol` and `max_iter`), these will not be consistent with `labels_`.\n\nLabels of each point\n\nSum of squared distances of samples to their closest cluster center.\n\nNumber of iterations run.\n\nSee also\n\nAlternative online implementation that does incremental updates of the centers\npositions using mini-batches. For large scale learning (say n_samples > 10k)\nMiniBatchKMeans is probably much faster than the default batch implementation.\n\nThe k-means problem is solved using either Lloyd\u2019s or Elkan\u2019s algorithm.\n\nThe average complexity is given by O(k n T), were n is the number of samples\nand T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p =\nn_features. (D. Arthur and S. Vassilvitskii, \u2018How slow is the k-means method?\u2019\nSoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering\nalgorithms available), but it falls in local minima. That\u2019s why it can be\nuseful to restart it several times.\n\nIf the algorithm stops before fully converging (because of `tol` or\n`max_iter`), `labels_` and `cluster_centers_` will not be consistent, i.e. the\n`cluster_centers_` will not be the means of the points in each cluster. Also,\nthe estimator will reassign `labels_` after the last iteration to make\n`labels_` consistent with `predict` on the training set.\n\n`fit`(X[, y, sample_weight])\n\nCompute k-means clustering.\n\n`fit_predict`(X[, y, sample_weight])\n\nCompute cluster centers and predict cluster index for each sample.\n\n`fit_transform`(X[, y, sample_weight])\n\nCompute clustering and transform X to cluster-distance space.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X[, sample_weight])\n\nPredict the closest cluster each sample in X belongs to.\n\n`score`(X[, y, sample_weight])\n\nOpposite of the value of X on the K-means objective.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X to a cluster-distance space.\n\nCompute k-means clustering.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous. If a sparse matrix is passed, a copy will be made if it\u2019s\nnot in CSR format.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nNew in version 0.20.\n\nFitted estimator.\n\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\nRelease Highlights for scikit-learn 0.23\n\nDemonstration of k-means assumptions\n\nVector Quantization Example\n\nK-means Clustering\n\nColor Quantization using K-Means\n\nEmpirical evaluation of the impact of k-means initialization\n\nComparison of the K-Means and MiniBatchKMeans clustering algorithms\n\nA demo of K-Means clustering on the handwritten digits data\n\nSelecting the number of clusters with silhouette analysis on KMeans clustering\n\nClustering text documents using k-means\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.fit()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.fit", "type": "cluster", "text": "\nCompute k-means clustering.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous. If a sparse matrix is passed, a copy will be made if it\u2019s\nnot in CSR format.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nNew in version 0.20.\n\nFitted estimator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.fit_predict()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.fit_predict", "type": "cluster", "text": "\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.fit_transform()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.fit_transform", "type": "cluster", "text": "\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.get_params()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.predict()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.predict", "type": "cluster", "text": "\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.score()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.score", "type": "cluster", "text": "\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.set_params()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.KMeans.transform()", "path": "modules/generated/sklearn.cluster.kmeans#sklearn.cluster.KMeans.transform", "type": "cluster", "text": "\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.kmeans_plusplus()", "path": "modules/generated/sklearn.cluster.kmeans_plusplus#sklearn.cluster.kmeans_plusplus", "type": "cluster", "text": "\nInit n_clusters seeds according to k-means++\n\nNew in version 0.24.\n\nThe data to pick seeds from.\n\nThe number of centroids to initialize\n\nSquared Euclidean norm of each data point.\n\nDetermines random number generation for centroid initialization. Pass an int\nfor reproducible output across multiple function calls. See Glossary.\n\nThe number of seeding trials for each center (except the first), of which the\none reducing inertia the most is greedily chosen. Set to None to make the\nnumber of trials depend logarithmically on the number of seeds (2+log(k)).\n\nThe inital centers for k-means.\n\nThe index location of the chosen centers in the data array X. For a given\nindex and center, X[index] = center.\n\nSelects initial cluster centers for k-mean clustering in a smart way to speed\nup convergence. see: Arthur, D. and Vassilvitskii, S. \u201ck-means++: the\nadvantages of careful seeding\u201d. ACM-SIAM symposium on Discrete algorithms.\n2007\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.k_means()", "path": "modules/generated/sklearn.cluster.k_means#sklearn.cluster.k_means", "type": "cluster", "text": "\nK-means clustering algorithm.\n\nRead more in the User Guide.\n\nThe observations to cluster. It must be noted that the data will be converted\nto C ordering, which will cause a memory copy if the given data is not\nC-contiguous.\n\nThe number of clusters to form as well as the number of centroids to generate.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nMethod for initialization:\n\n\u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart\nway to speed up convergence. See section Notes in k_init for more details.\n\n\u2018random\u2019: choose `n_clusters` observations (rows) at random from data for the\ninitial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features) and\ngives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a random\nstate and return an initialization.\n\nPrecompute distances (faster but takes more memory).\n\n\u2018auto\u2019 : do not precompute distances if n_samples * n_clusters > 12 million.\nThis corresponds to about 100MB overhead per job using double precision.\n\nTrue : always precompute distances\n\nFalse : never precompute distances\n\nDeprecated since version 0.23: \u2018precompute_distances\u2019 was deprecated in\nversion 0.23 and will be removed in 1.0 (renaming of 0.25). It has no effect.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nMaximum number of iterations of the k-means algorithm to run.\n\nVerbosity mode.\n\nRelative tolerance with regards to Frobenius norm of the difference in the\ncluster centers of two consecutive iterations to declare convergence.\n\nDetermines random number generation for centroid initialization. Use an int to\nmake the randomness deterministic. See Glossary.\n\nWhen pre-computing distances it is more numerically accurate to center the\ndata first. If copy_x is True (default), then the original data is not\nmodified. If False, the original data is modified, and put back before the\nfunction returns, but small numerical differences may be introduced by\nsubtracting and then adding the data mean. Note that if the original data is\nnot C-contiguous, a copy will be made even if copy_x is False. If the original\ndata is sparse, but not in CSR format, a copy will be made even if copy_x is\nFalse.\n\nThe number of OpenMP threads to use for the computation. Parallelism is\nsample-wise on the main cython loop which assigns each sample to its closest\ncenter.\n\n`None` or `-1` means using all processors.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nK-means algorithm to use. The classical EM-style algorithm is \u201cfull\u201d. The\n\u201celkan\u201d variation is more efficient on data with well-defined clusters, by\nusing the triangle inequality. However it\u2019s more memory intensive due to the\nallocation of an extra array of shape (n_samples, n_clusters).\n\nFor now \u201cauto\u201d (kept for backward compatibiliy) chooses \u201celkan\u201d but it might\nchange in the future for a better heuristic.\n\nWhether or not to return the number of iterations.\n\nCentroids found at the last iteration of k-means.\n\nlabel[i] is the code or index of the centroid the i\u2019th observation is closest\nto.\n\nThe final value of the inertia criterion (sum of squared distances to the\nclosest centroid for all observations in the training set).\n\nNumber of iterations corresponding to the best results. Returned only if\n`return_n_iter` is set to True.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift", "type": "cluster", "text": "\nMean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \u201cblobs\u201d in a smooth density of samples.\nIt is a centroid-based algorithm, which works by updating candidates for\ncentroids to be the mean of the points within a given region. These candidates\nare then filtered in a post-processing stage to eliminate near-duplicates to\nform the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the User Guide.\n\nBandwidth used in the RBF kernel.\n\nIf not given, the bandwidth is estimated using\nsklearn.cluster.estimate_bandwidth; see the documentation for that function\nfor hints on scalability (see also the Notes, below).\n\nSeeds used to initialize kernels. If not set, the seeds are calculated by\nclustering.get_bin_seeds with bandwidth as the grid size and default values\nfor other parameters.\n\nIf true, initial kernel locations are not locations of all points, but rather\nthe location of the discretized version of points, where points are binned\nonto a grid whose coarseness corresponds to the bandwidth. Setting this option\nto True will speed up the algorithm because fewer seeds will be initialized.\nThe default value is False. Ignored if seeds argument is not None.\n\nTo speed up the algorithm, accept only those bins with at least min_bin_freq\npoints as seeds.\n\nIf true, then all points are clustered, even those orphans that are not within\nany kernel. Orphans are assigned to the nearest kernel. If false, then orphans\nare given cluster label -1.\n\nThe number of jobs to use for the computation. This works by computing each of\nthe n_init runs in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nMaximum number of iterations, per seed point before the clustering operation\nterminates (for that seed point), if has not converged yet.\n\nNew in version 0.22.\n\nCoordinates of cluster centers.\n\nLabels of each point.\n\nMaximum number of iterations performed on each seed.\n\nNew in version 0.22.\n\nScalability:\n\nBecause this implementation uses a flat kernel and a Ball Tree to look up\nmembers of each kernel, the complexity will tend towards O(T*n*log(n)) in\nlower dimensions, with n the number of samples and T the number of points. In\nhigher dimensions the complexity will tend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using a higher\nvalue of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the mean\nshift algorithm and will be the bottleneck if it is used.\n\nDorin Comaniciu and Peter Meer, \u201cMean Shift: A robust approach toward feature\nspace analysis\u201d. IEEE Transactions on Pattern Analysis and Machine\nIntelligence. 2002. pp. 603-619.\n\n`fit`(X[, y])\n\nPerform clustering.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the closest cluster each sample in X belongs to.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform clustering.\n\nSamples to cluster.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict.\n\nIndex of the cluster each sample belongs to.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift()", "path": "modules/generated/sklearn.cluster.meanshift", "type": "cluster", "text": "\nMean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \u201cblobs\u201d in a smooth density of samples.\nIt is a centroid-based algorithm, which works by updating candidates for\ncentroids to be the mean of the points within a given region. These candidates\nare then filtered in a post-processing stage to eliminate near-duplicates to\nform the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the User Guide.\n\nBandwidth used in the RBF kernel.\n\nIf not given, the bandwidth is estimated using\nsklearn.cluster.estimate_bandwidth; see the documentation for that function\nfor hints on scalability (see also the Notes, below).\n\nSeeds used to initialize kernels. If not set, the seeds are calculated by\nclustering.get_bin_seeds with bandwidth as the grid size and default values\nfor other parameters.\n\nIf true, initial kernel locations are not locations of all points, but rather\nthe location of the discretized version of points, where points are binned\nonto a grid whose coarseness corresponds to the bandwidth. Setting this option\nto True will speed up the algorithm because fewer seeds will be initialized.\nThe default value is False. Ignored if seeds argument is not None.\n\nTo speed up the algorithm, accept only those bins with at least min_bin_freq\npoints as seeds.\n\nIf true, then all points are clustered, even those orphans that are not within\nany kernel. Orphans are assigned to the nearest kernel. If false, then orphans\nare given cluster label -1.\n\nThe number of jobs to use for the computation. This works by computing each of\nthe n_init runs in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nMaximum number of iterations, per seed point before the clustering operation\nterminates (for that seed point), if has not converged yet.\n\nNew in version 0.22.\n\nCoordinates of cluster centers.\n\nLabels of each point.\n\nMaximum number of iterations performed on each seed.\n\nNew in version 0.22.\n\nScalability:\n\nBecause this implementation uses a flat kernel and a Ball Tree to look up\nmembers of each kernel, the complexity will tend towards O(T*n*log(n)) in\nlower dimensions, with n the number of samples and T the number of points. In\nhigher dimensions the complexity will tend towards O(T*n^2).\n\nScalability can be boosted by using fewer seeds, for example by using a higher\nvalue of min_bin_freq in the get_bin_seeds function.\n\nNote that the estimate_bandwidth function is much less scalable than the mean\nshift algorithm and will be the bottleneck if it is used.\n\nDorin Comaniciu and Peter Meer, \u201cMean Shift: A robust approach toward feature\nspace analysis\u201d. IEEE Transactions on Pattern Analysis and Machine\nIntelligence. 2002. pp. 603-619.\n\n`fit`(X[, y])\n\nPerform clustering.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict the closest cluster each sample in X belongs to.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform clustering.\n\nSamples to cluster.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict.\n\nIndex of the cluster each sample belongs to.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nA demo of the mean-shift clustering algorithm\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift.fit()", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift.fit", "type": "cluster", "text": "\nPerform clustering.\n\nSamples to cluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift.fit_predict()", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift.fit_predict", "type": "cluster", "text": "\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift.get_params()", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift.predict()", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift.predict", "type": "cluster", "text": "\nPredict the closest cluster each sample in X belongs to.\n\nNew data to predict.\n\nIndex of the cluster each sample belongs to.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MeanShift.set_params()", "path": "modules/generated/sklearn.cluster.meanshift#sklearn.cluster.MeanShift.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.mean_shift()", "path": "modules/generated/sklearn.cluster.mean_shift#sklearn.cluster.mean_shift", "type": "cluster", "text": "\nPerform mean shift clustering of data using a flat kernel.\n\nRead more in the User Guide.\n\nInput data.\n\nKernel bandwidth.\n\nIf bandwidth is not given, it is determined using a heuristic based on the\nmedian of all pairwise distances. This will take quadratic time in the number\nof samples. The sklearn.cluster.estimate_bandwidth function can be used to do\nthis more efficiently.\n\nPoint used as initial kernel locations. If None and bin_seeding=False, each\ndata point is used as a seed. If None and bin_seeding=True, see bin_seeding.\n\nIf true, initial kernel locations are not locations of all points, but rather\nthe location of the discretized version of points, where points are binned\nonto a grid whose coarseness corresponds to the bandwidth. Setting this option\nto True will speed up the algorithm because fewer seeds will be initialized.\nIgnored if seeds argument is not None.\n\nTo speed up the algorithm, accept only those bins with at least min_bin_freq\npoints as seeds.\n\nIf true, then all points are clustered, even those orphans that are not within\nany kernel. Orphans are assigned to the nearest kernel. If false, then orphans\nare given cluster label -1.\n\nMaximum number of iterations, per seed point before the clustering operation\nterminates (for that seed point), if has not converged yet.\n\nThe number of jobs to use for the computation. This works by computing each of\nthe n_init runs in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nNew in version 0.17: Parallel Execution using n_jobs.\n\nCoordinates of cluster centers.\n\nCluster labels for each point.\n\nFor an example, see examples/cluster/plot_mean_shift.py.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans", "type": "cluster", "text": "\nMini-Batch K-Means clustering.\n\nRead more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate.\n\nMethod for initialization:\n\n\u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart\nway to speed up convergence. See section Notes in k_init for more details.\n\n\u2018random\u2019: choose `n_clusters` observations (rows) at random from data for the\ninitial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features) and\ngives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a random\nstate and return an initialization.\n\nMaximum number of iterations over the complete dataset before stopping\nindependently of any early stopping criterion heuristics.\n\nSize of the mini batches.\n\nVerbosity mode.\n\nCompute label assignment and inertia for the complete dataset once the\nminibatch optimization has converged in fit.\n\nDetermines random number generation for centroid initialization and random\nreassignment. Use an int to make the randomness deterministic. See Glossary.\n\nControl early stopping based on the relative center changes as measured by a\nsmoothed, variance-normalized of the mean center squared position changes.\nThis early stopping heuristics is closer to the one used for the batch variant\nof the algorithms but induces a slight computational and memory overhead over\nthe inertia heuristic.\n\nTo disable convergence detection based on normalized center change, set tol to\n0.0 (default).\n\nControl early stopping based on the consecutive number of mini batches that\ndoes not yield an improvement on the smoothed inertia.\n\nTo disable convergence detection based on inertia, set max_no_improvement to\nNone.\n\nNumber of samples to randomly sample for speeding up the initialization\n(sometimes at the expense of accuracy): the only algorithm is initialized by\nrunning a batch KMeans on a random subset of the data. This needs to be larger\nthan n_clusters.\n\nIf `None`, `init_size= 3 * batch_size`.\n\nNumber of random initializations that are tried. In contrast to KMeans, the\nalgorithm is only run once, using the best of the `n_init` initializations as\nmeasured by inertia.\n\nControl the fraction of the maximum number of counts for a center to be\nreassigned. A higher value means that low count centers are more easily\nreassigned, which means that the model will take longer to converge, but\nshould converge in a better clustering.\n\nCoordinates of cluster centers.\n\nLabels of each point (if compute_labels is set to True).\n\nThe value of the inertia criterion associated with the chosen partition (if\ncompute_labels is set to True). The inertia is defined as the sum of square\ndistances of samples to their nearest neighbor.\n\nNumber of batches processed.\n\nWeigth sum of each cluster.\n\nDeprecated since version 0.24: This attribute is deprecated in 0.24 and will\nbe removed in 1.1 (renaming of 0.26).\n\nThe effective number of samples used for the initialization.\n\nDeprecated since version 0.24: This attribute is deprecated in 0.24 and will\nbe removed in 1.1 (renaming of 0.26).\n\nSee also\n\nThe classic implementation of the clustering method based on the Lloyd\u2019s\nalgorithm. It consumes the whole set of input data at each iteration.\n\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n`fit`(X[, y, sample_weight])\n\nCompute the centroids on X by chunking it into mini-batches.\n\n`fit_predict`(X[, y, sample_weight])\n\nCompute cluster centers and predict cluster index for each sample.\n\n`fit_transform`(X[, y, sample_weight])\n\nCompute clustering and transform X to cluster-distance space.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`partial_fit`(X[, y, sample_weight])\n\nUpdate k means estimate on a single mini-batch X.\n\n`predict`(X[, sample_weight])\n\nPredict the closest cluster each sample in X belongs to.\n\n`score`(X[, y, sample_weight])\n\nOpposite of the value of X on the K-means objective.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X to a cluster-distance space.\n\nCompute the centroids on X by chunking it into mini-batches.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nNew in version 0.20.\n\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nUpdate k means estimate on a single mini-batch X.\n\nCoordinates of the data points to cluster. It must be noted that X will be\ncopied if it is not C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nIndex of the cluster each sample belongs to.\n\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans()", "path": "modules/generated/sklearn.cluster.minibatchkmeans", "type": "cluster", "text": "\nMini-Batch K-Means clustering.\n\nRead more in the User Guide.\n\nThe number of clusters to form as well as the number of centroids to generate.\n\nMethod for initialization:\n\n\u2018k-means++\u2019 : selects initial cluster centers for k-mean clustering in a smart\nway to speed up convergence. See section Notes in k_init for more details.\n\n\u2018random\u2019: choose `n_clusters` observations (rows) at random from data for the\ninitial centroids.\n\nIf an array is passed, it should be of shape (n_clusters, n_features) and\ngives the initial centers.\n\nIf a callable is passed, it should take arguments X, n_clusters and a random\nstate and return an initialization.\n\nMaximum number of iterations over the complete dataset before stopping\nindependently of any early stopping criterion heuristics.\n\nSize of the mini batches.\n\nVerbosity mode.\n\nCompute label assignment and inertia for the complete dataset once the\nminibatch optimization has converged in fit.\n\nDetermines random number generation for centroid initialization and random\nreassignment. Use an int to make the randomness deterministic. See Glossary.\n\nControl early stopping based on the relative center changes as measured by a\nsmoothed, variance-normalized of the mean center squared position changes.\nThis early stopping heuristics is closer to the one used for the batch variant\nof the algorithms but induces a slight computational and memory overhead over\nthe inertia heuristic.\n\nTo disable convergence detection based on normalized center change, set tol to\n0.0 (default).\n\nControl early stopping based on the consecutive number of mini batches that\ndoes not yield an improvement on the smoothed inertia.\n\nTo disable convergence detection based on inertia, set max_no_improvement to\nNone.\n\nNumber of samples to randomly sample for speeding up the initialization\n(sometimes at the expense of accuracy): the only algorithm is initialized by\nrunning a batch KMeans on a random subset of the data. This needs to be larger\nthan n_clusters.\n\nIf `None`, `init_size= 3 * batch_size`.\n\nNumber of random initializations that are tried. In contrast to KMeans, the\nalgorithm is only run once, using the best of the `n_init` initializations as\nmeasured by inertia.\n\nControl the fraction of the maximum number of counts for a center to be\nreassigned. A higher value means that low count centers are more easily\nreassigned, which means that the model will take longer to converge, but\nshould converge in a better clustering.\n\nCoordinates of cluster centers.\n\nLabels of each point (if compute_labels is set to True).\n\nThe value of the inertia criterion associated with the chosen partition (if\ncompute_labels is set to True). The inertia is defined as the sum of square\ndistances of samples to their nearest neighbor.\n\nNumber of batches processed.\n\nWeigth sum of each cluster.\n\nDeprecated since version 0.24: This attribute is deprecated in 0.24 and will\nbe removed in 1.1 (renaming of 0.26).\n\nThe effective number of samples used for the initialization.\n\nDeprecated since version 0.24: This attribute is deprecated in 0.24 and will\nbe removed in 1.1 (renaming of 0.26).\n\nSee also\n\nThe classic implementation of the clustering method based on the Lloyd\u2019s\nalgorithm. It consumes the whole set of input data at each iteration.\n\nSee https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf\n\n`fit`(X[, y, sample_weight])\n\nCompute the centroids on X by chunking it into mini-batches.\n\n`fit_predict`(X[, y, sample_weight])\n\nCompute cluster centers and predict cluster index for each sample.\n\n`fit_transform`(X[, y, sample_weight])\n\nCompute clustering and transform X to cluster-distance space.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`partial_fit`(X[, y, sample_weight])\n\nUpdate k means estimate on a single mini-batch X.\n\n`predict`(X[, sample_weight])\n\nPredict the closest cluster each sample in X belongs to.\n\n`score`(X[, y, sample_weight])\n\nOpposite of the value of X on the K-means objective.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X to a cluster-distance space.\n\nCompute the centroids on X by chunking it into mini-batches.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nNew in version 0.20.\n\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nUpdate k means estimate on a single mini-batch X.\n\nCoordinates of the data points to cluster. It must be noted that X will be\ncopied if it is not C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nIndex of the cluster each sample belongs to.\n\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\nBiclustering documents with the Spectral Co-clustering algorithm\n\nOnline learning of a dictionary of parts of faces\n\nCompare BIRCH and MiniBatchKMeans\n\nEmpirical evaluation of the impact of k-means initialization\n\nComparison of the K-Means and MiniBatchKMeans clustering algorithms\n\nComparing different clustering algorithms on toy datasets\n\nFaces dataset decompositions\n\nClustering text documents using k-means\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.fit()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.fit", "type": "cluster", "text": "\nCompute the centroids on X by chunking it into mini-batches.\n\nTraining instances to cluster. It must be noted that the data will be\nconverted to C ordering, which will cause a memory copy if the given data is\nnot C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nNew in version 0.20.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.fit_predict()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.fit_predict", "type": "cluster", "text": "\nCompute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by predict(X).\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nIndex of the cluster each sample belongs to.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.fit_transform()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.fit_transform", "type": "cluster", "text": "\nCompute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n\nNew data to transform.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.get_params()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.partial_fit()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.partial_fit", "type": "cluster", "text": "\nUpdate k means estimate on a single mini-batch X.\n\nCoordinates of the data points to cluster. It must be noted that X will be\ncopied if it is not C-contiguous.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.predict()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.predict", "type": "cluster", "text": "\nPredict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called the code\nbook and each value returned by `predict` is the index of the closest code in\nthe code book.\n\nNew data to predict.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight (default: None).\n\nIndex of the cluster each sample belongs to.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.score()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.score", "type": "cluster", "text": "\nOpposite of the value of X on the K-means objective.\n\nNew data.\n\nNot used, present here for API consistency by convention.\n\nThe weights for each observation in X. If None, all observations are assigned\nequal weight.\n\nOpposite of the value of X on the K-means objective.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.set_params()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.MiniBatchKMeans.transform()", "path": "modules/generated/sklearn.cluster.minibatchkmeans#sklearn.cluster.MiniBatchKMeans.transform", "type": "cluster", "text": "\nTransform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster centers. Note\nthat even if X is sparse, the array returned by `transform` will typically be\ndense.\n\nNew data to transform.\n\nX transformed in the new space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS", "path": "modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS", "type": "cluster", "text": "\nEstimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related\nto DBSCAN, finds core sample of high density and expands clusters from them\n[1]. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood\nradius. Better suited for usage on large datasets than the current sklearn\nimplementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method (cluster_method =\n\u2018dbscan\u2019) or an automatic technique proposed in [1] (cluster_method = \u2018xi\u2019).\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the User Guide.\n\nThe number of samples in a neighborhood for a point to be considered as a core\npoint. Also, up and down steep regions can\u2019t have more than `min_samples`\nconsecutive non-steep points. Expressed as an absolute number or a fraction of\nthe number of samples (rounded to be at least 2).\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. Default value of `np.inf` will identify clusters\nacross all scales; reducing `max_eps` will result in shorter run times.\n\nMetric to use for distance computation. Any metric from scikit-learn or\nscipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each pair of instances\n(rows) and the resulting value recorded. The callable should take two arrays\nas input and return one value indicating the distance between them. This works\nfor Scipy\u2019s metrics, but is less efficient than passing the metric name as a\nstring. If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and\nmust be square.\n\nValid values for metric are:\n\nSee the documentation for scipy.spatial.distance for details on these metrics.\n\nParameter for the Minkowski metric from `pairwise_distances`. When p = 1, this\nis equivalent to using manhattan_distance (l1), and euclidean_distance (l2)\nfor p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nAdditional keyword arguments for the metric function.\n\nThe extraction method used to extract clusters using the calculated\nreachability and ordering. Possible values are \u201cxi\u201d and \u201cdbscan\u201d.\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. By default it assumes the same value as `max_eps`.\nUsed only when `cluster_method='dbscan'`.\n\nDetermines the minimum steepness on the reachability plot that constitutes a\ncluster boundary. For example, an upwards point in the reachability plot is\ndefined by the ratio from one point to its successor being at most 1-xi. Used\nonly when `cluster_method='xi'`.\n\nCorrect clusters according to the predecessors calculated by OPTICS [2]. This\nparameter has minimal effect on most datasets. Used only when\n`cluster_method='xi'`.\n\nMinimum number of samples in an OPTICS cluster, expressed as an absolute\nnumber or a fraction of the number of samples (rounded to be at least 2). If\n`None`, the value of `min_samples` is used instead. Used only when\n`cluster_method='xi'`.\n\nAlgorithm used to compute the nearest neighbors:\n\nNote: fitting on sparse input will override the setting of this parameter,\nusing brute force.\n\nLeaf size passed to `BallTree` or `KDTree`. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe number of parallel jobs to run for neighbors search. `None` means 1 unless\nin a `joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nCluster labels for each point in the dataset given to fit(). Noisy samples and\npoints which are not included in a leaf cluster of `cluster_hierarchy_` are\nlabeled as -1.\n\nReachability distances per sample, indexed by object order. Use\n`clust.reachability_[clust.ordering_]` to access in cluster order.\n\nThe cluster ordered list of sample indices.\n\nDistance at which each sample becomes a core point, indexed by object order.\nPoints which will never be core have a distance of inf. Use\n`clust.core_distances_[clust.ordering_]` to access in cluster order.\n\nPoint that a sample was reached from, indexed by object order. Seed points\nhave a predecessor of -1.\n\nThe list of clusters in the form of `[start, end]` in each row, with all\nindices inclusive. The clusters are ordered according to `(end, -start)`\n(ascending) so that larger clusters encompassing smaller clusters come after\nthose smaller ones. Since `labels_` does not reflect the hierarchy, usually\n`len(cluster_hierarchy_) > np.unique(optics.labels_)`. Please also note that\nthese indices are of the `ordering_`, i.e. `X[ordering_][start:end + 1]` form\na cluster. Only available when `cluster_method='xi'`.\n\nSee also\n\nA similar clustering for a specified neighborhood radius (eps). Our\nimplementation is optimized for runtime.\n\nAnkerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander.\n\u201cOPTICS: ordering points to identify the clustering structure.\u201d ACM SIGMOD\nRecord 28, no. 2 (1999): 49-60.\n\nSchubert, Erich, Michael Gertz. \u201cImproving the Cluster Structure Extracted\nfrom OPTICS Plots.\u201d Proc. of the Conference \u201cLernen, Wissen, Daten, Analysen\u201d\n(LWDA) (2018): 318-329.\n\n`fit`(X[, y])\n\nPerform OPTICS clustering.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and performs\ninitial clustering using `max_eps` distance specified at OPTICS object\ninstantiation.\n\nA feature array, or array of distances between samples if\nmetric=\u2019precomputed\u2019.\n\nIgnored.\n\nThe instance.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS()", "path": "modules/generated/sklearn.cluster.optics", "type": "cluster", "text": "\nEstimate clustering structure from vector array.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related\nto DBSCAN, finds core sample of high density and expands clusters from them\n[1]. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood\nradius. Better suited for usage on large datasets than the current sklearn\nimplementation of DBSCAN.\n\nClusters are then extracted using a DBSCAN-like method (cluster_method =\n\u2018dbscan\u2019) or an automatic technique proposed in [1] (cluster_method = \u2018xi\u2019).\n\nThis implementation deviates from the original OPTICS by first performing\nk-nearest-neighborhood searches on all points to identify core sizes, then\ncomputing only the distances to unprocessed points when constructing the\ncluster order. Note that we do not employ a heap to manage the expansion\ncandidates, so the time complexity will be O(n^2).\n\nRead more in the User Guide.\n\nThe number of samples in a neighborhood for a point to be considered as a core\npoint. Also, up and down steep regions can\u2019t have more than `min_samples`\nconsecutive non-steep points. Expressed as an absolute number or a fraction of\nthe number of samples (rounded to be at least 2).\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. Default value of `np.inf` will identify clusters\nacross all scales; reducing `max_eps` will result in shorter run times.\n\nMetric to use for distance computation. Any metric from scikit-learn or\nscipy.spatial.distance can be used.\n\nIf metric is a callable function, it is called on each pair of instances\n(rows) and the resulting value recorded. The callable should take two arrays\nas input and return one value indicating the distance between them. This works\nfor Scipy\u2019s metrics, but is less efficient than passing the metric name as a\nstring. If metric is \u201cprecomputed\u201d, X is assumed to be a distance matrix and\nmust be square.\n\nValid values for metric are:\n\nSee the documentation for scipy.spatial.distance for details on these metrics.\n\nParameter for the Minkowski metric from `pairwise_distances`. When p = 1, this\nis equivalent to using manhattan_distance (l1), and euclidean_distance (l2)\nfor p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\nAdditional keyword arguments for the metric function.\n\nThe extraction method used to extract clusters using the calculated\nreachability and ordering. Possible values are \u201cxi\u201d and \u201cdbscan\u201d.\n\nThe maximum distance between two samples for one to be considered as in the\nneighborhood of the other. By default it assumes the same value as `max_eps`.\nUsed only when `cluster_method='dbscan'`.\n\nDetermines the minimum steepness on the reachability plot that constitutes a\ncluster boundary. For example, an upwards point in the reachability plot is\ndefined by the ratio from one point to its successor being at most 1-xi. Used\nonly when `cluster_method='xi'`.\n\nCorrect clusters according to the predecessors calculated by OPTICS [2]. This\nparameter has minimal effect on most datasets. Used only when\n`cluster_method='xi'`.\n\nMinimum number of samples in an OPTICS cluster, expressed as an absolute\nnumber or a fraction of the number of samples (rounded to be at least 2). If\n`None`, the value of `min_samples` is used instead. Used only when\n`cluster_method='xi'`.\n\nAlgorithm used to compute the nearest neighbors:\n\nNote: fitting on sparse input will override the setting of this parameter,\nusing brute force.\n\nLeaf size passed to `BallTree` or `KDTree`. This can affect the speed of the\nconstruction and query, as well as the memory required to store the tree. The\noptimal value depends on the nature of the problem.\n\nThe number of parallel jobs to run for neighbors search. `None` means 1 unless\nin a `joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nCluster labels for each point in the dataset given to fit(). Noisy samples and\npoints which are not included in a leaf cluster of `cluster_hierarchy_` are\nlabeled as -1.\n\nReachability distances per sample, indexed by object order. Use\n`clust.reachability_[clust.ordering_]` to access in cluster order.\n\nThe cluster ordered list of sample indices.\n\nDistance at which each sample becomes a core point, indexed by object order.\nPoints which will never be core have a distance of inf. Use\n`clust.core_distances_[clust.ordering_]` to access in cluster order.\n\nPoint that a sample was reached from, indexed by object order. Seed points\nhave a predecessor of -1.\n\nThe list of clusters in the form of `[start, end]` in each row, with all\nindices inclusive. The clusters are ordered according to `(end, -start)`\n(ascending) so that larger clusters encompassing smaller clusters come after\nthose smaller ones. Since `labels_` does not reflect the hierarchy, usually\n`len(cluster_hierarchy_) > np.unique(optics.labels_)`. Please also note that\nthese indices are of the `ordering_`, i.e. `X[ordering_][start:end + 1]` form\na cluster. Only available when `cluster_method='xi'`.\n\nSee also\n\nA similar clustering for a specified neighborhood radius (eps). Our\nimplementation is optimized for runtime.\n\nAnkerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and J\u00f6rg Sander.\n\u201cOPTICS: ordering points to identify the clustering structure.\u201d ACM SIGMOD\nRecord 28, no. 2 (1999): 49-60.\n\nSchubert, Erich, Michael Gertz. \u201cImproving the Cluster Structure Extracted\nfrom OPTICS Plots.\u201d Proc. of the Conference \u201cLernen, Wissen, Daten, Analysen\u201d\n(LWDA) (2018): 318-329.\n\n`fit`(X[, y])\n\nPerform OPTICS clustering.\n\n`fit_predict`(X[, y])\n\nPerform clustering on `X` and returns cluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and performs\ninitial clustering using `max_eps` distance specified at OPTICS object\ninstantiation.\n\nA feature array, or array of distances between samples if\nmetric=\u2019precomputed\u2019.\n\nIgnored.\n\nThe instance.\n\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nDemo of OPTICS clustering algorithm\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS.fit()", "path": "modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS.fit", "type": "cluster", "text": "\nPerform OPTICS clustering.\n\nExtracts an ordered list of points and reachability distances, and performs\ninitial clustering using `max_eps` distance specified at OPTICS object\ninstantiation.\n\nA feature array, or array of distances between samples if\nmetric=\u2019precomputed\u2019.\n\nIgnored.\n\nThe instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS.fit_predict()", "path": "modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS.fit_predict", "type": "cluster", "text": "\nPerform clustering on `X` and returns cluster labels.\n\nInput data.\n\nNot used, present for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS.get_params()", "path": "modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.OPTICS.set_params()", "path": "modules/generated/sklearn.cluster.optics#sklearn.cluster.OPTICS.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering", "type": "cluster", "text": "\nSpectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has an\nunderlying checkerboard structure. For instance, if there are two row\npartitions and three column partitions, each row will belong to three\nbiclusters, and each column will belong to two biclusters. The outer product\nof the corresponding row and column label vectors gives this checkerboard\nstructure.\n\nRead more in the User Guide.\n\nThe number of row and column clusters in the checkerboard structure.\n\nMethod of normalizing and converting singular vectors into biclusters. May be\none of \u2018scale\u2019, \u2018bistochastic\u2019, or \u2018log\u2019. The authors recommend using \u2018log\u2019.\nIf the data is sparse, however, log normalization will not work, which is why\nthe default is \u2018bistochastic\u2019.\n\nWarning\n\nif `method='log'`, the data must be sparse.\n\nNumber of singular vectors to check.\n\nNumber of best singular vectors to which to project the data for clustering.\n\nSelects the algorithm for finding singular vectors. May be \u2018randomized\u2019 or\n\u2018arpack\u2019. If \u2018randomized\u2019, uses `randomized_svd`, which may be faster for\nlarge matrices. If \u2018arpack\u2019, uses `scipy.sparse.linalg.svds`, which is more\naccurate, but possibly slower in some cases.\n\nNumber of vectors to use in calculating the SVD. Corresponds to `ncv` when\n`svd_method=arpack` and `n_oversamples` when `svd_method` is \u2018randomized`.\n\nWhether to use mini-batch k-means, which is faster but may get different\nresults.\n\nMethod for initialization of k-means algorithm; defaults to \u2018k-means++\u2019.\n\nNumber of random initializations that are tried with the k-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is chosen and the\nalgorithm runs once. Otherwise, the algorithm is run for each initialization\nand the best solution chosen.\n\nThe number of jobs to use for the computation. This works by breaking down the\npairwise matrix into n_jobs even slices and computing them in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nUsed for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nResults of the clustering. `rows[i, r]` is True if cluster `i` contains row\n`r`. Available only after calling `fit`.\n\nResults of the clustering, like `rows`.\n\nRow partition labels.\n\nColumn partition labels.\n\n`fit`(X[, y])\n\nCreates a biclustering for X.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nCreates a biclustering for X.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering()", "path": "modules/generated/sklearn.cluster.spectralbiclustering", "type": "cluster", "text": "\nSpectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has an\nunderlying checkerboard structure. For instance, if there are two row\npartitions and three column partitions, each row will belong to three\nbiclusters, and each column will belong to two biclusters. The outer product\nof the corresponding row and column label vectors gives this checkerboard\nstructure.\n\nRead more in the User Guide.\n\nThe number of row and column clusters in the checkerboard structure.\n\nMethod of normalizing and converting singular vectors into biclusters. May be\none of \u2018scale\u2019, \u2018bistochastic\u2019, or \u2018log\u2019. The authors recommend using \u2018log\u2019.\nIf the data is sparse, however, log normalization will not work, which is why\nthe default is \u2018bistochastic\u2019.\n\nWarning\n\nif `method='log'`, the data must be sparse.\n\nNumber of singular vectors to check.\n\nNumber of best singular vectors to which to project the data for clustering.\n\nSelects the algorithm for finding singular vectors. May be \u2018randomized\u2019 or\n\u2018arpack\u2019. If \u2018randomized\u2019, uses `randomized_svd`, which may be faster for\nlarge matrices. If \u2018arpack\u2019, uses `scipy.sparse.linalg.svds`, which is more\naccurate, but possibly slower in some cases.\n\nNumber of vectors to use in calculating the SVD. Corresponds to `ncv` when\n`svd_method=arpack` and `n_oversamples` when `svd_method` is \u2018randomized`.\n\nWhether to use mini-batch k-means, which is faster but may get different\nresults.\n\nMethod for initialization of k-means algorithm; defaults to \u2018k-means++\u2019.\n\nNumber of random initializations that are tried with the k-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is chosen and the\nalgorithm runs once. Otherwise, the algorithm is run for each initialization\nand the best solution chosen.\n\nThe number of jobs to use for the computation. This works by breaking down the\npairwise matrix into n_jobs even slices and computing them in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nUsed for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nResults of the clustering. `rows[i, r]` is True if cluster `i` contains row\n`r`. Available only after calling `fit`.\n\nResults of the clustering, like `rows`.\n\nRow partition labels.\n\nColumn partition labels.\n\n`fit`(X[, y])\n\nCreates a biclustering for X.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nCreates a biclustering for X.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nA demo of the Spectral Biclustering algorithm\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.biclusters_()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.biclusters_", "type": "cluster", "text": "\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.fit()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.fit", "type": "cluster", "text": "\nCreates a biclustering for X.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.get_indices()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.get_indices", "type": "cluster", "text": "\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.get_params()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.get_shape()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.get_shape", "type": "cluster", "text": "\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.get_submatrix()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.get_submatrix", "type": "cluster", "text": "\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralBiclustering.set_params()", "path": "modules/generated/sklearn.cluster.spectralbiclustering#sklearn.cluster.SpectralBiclustering.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering", "path": "modules/generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering", "type": "cluster", "text": "\nApply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of the\nindividual clusters is highly non-convex or more generally when a measure of\nthe center and spread of the cluster is not a suitable description of the\ncomplete cluster. For instance when clusters are nested circles on the 2D\nplane.\n\nIf affinity is the adjacency matrix of a graph, this method can be used to\nfind normalized graph cuts.\n\nWhen calling `fit`, an affinity matrix is constructed using either kernel\nfunction such the Gaussian (aka RBF) kernel of the euclidean distanced `d(X,\nX)`:\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, using `precomputed`, a user-provided affinity matrix can be\nused.\n\nRead more in the User Guide.\n\nThe dimension of the projection subspace.\n\nThe eigenvalue decomposition strategy to use. AMG requires pyamg to be\ninstalled. It can be faster on very large, sparse problems, but may also lead\nto instabilities. If None, then `'arpack'` is used.\n\nNumber of eigen vectors to use for the spectral embedding\n\nA pseudo random number generator used for the initialization of the lobpcg\neigen vectors decomposition when `eigen_solver='amg'` and by the K-Means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nKernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored\nfor `affinity='nearest_neighbors'`.\n\nOnly kernels that produce similarity scores (non-negative values that increase\nwith similarity) should be used. This property is not checked by the\nclustering algorithm.\n\nNumber of neighbors to use when constructing the affinity matrix using the\nnearest neighbors method. Ignored for `affinity='rbf'`.\n\nStopping criterion for eigendecomposition of the Laplacian matrix when\n`eigen_solver='arpack'`.\n\nThe strategy to use to assign labels in the embedding space. There are two\nways to assign labels after the laplacian embedding. k-means can be applied\nand is a popular choice. But it can also be sensitive to initialization.\nDiscretization is another approach which is less sensitive to random\ninitialization.\n\nDegree of the polynomial kernel. Ignored by other kernels.\n\nZero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.\n\nParameters (keyword arguments) and values for kernel passed as callable\nobject. Ignored by other kernels.\n\nThe number of parallel jobs to run when `affinity='nearest_neighbors'` or\n`affinity='precomputed_nearest_neighbors'`. The neighbors search will be done\nin parallel. `None` means 1 unless in a `joblib.parallel_backend` context.\n`-1` means using all processors. See Glossary for more details.\n\nVerbosity mode.\n\nNew in version 0.24.\n\nAffinity matrix used for clustering. Available only if after calling `fit`.\n\nLabels of each point\n\nIf you have an affinity matrix, such as a distance matrix, for which 0 means\nidentical elements, and high values means very dissimilar elements, it can be\ntransformed in a similarity matrix that is well suited for the algorithm by\napplying the Gaussian (RBF, heat) kernel:\n\nWhere `delta` is a free parameter representing the width of the Gaussian\nkernel.\n\nAnother alternative is to take a symmetric version of the k nearest neighbors\nconnectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly speeds up\ncomputation.\n\n`fit`(X[, y])\n\nPerform spectral clustering from features, or affinity matrix.\n\n`fit_predict`(X[, y])\n\nPerform spectral clustering from features, or affinity matrix, and return\ncluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform spectral clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nPerform spectral clustering from features, or affinity matrix, and return\ncluster labels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering()", "path": "modules/generated/sklearn.cluster.spectralclustering", "type": "cluster", "text": "\nApply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of the\nindividual clusters is highly non-convex or more generally when a measure of\nthe center and spread of the cluster is not a suitable description of the\ncomplete cluster. For instance when clusters are nested circles on the 2D\nplane.\n\nIf affinity is the adjacency matrix of a graph, this method can be used to\nfind normalized graph cuts.\n\nWhen calling `fit`, an affinity matrix is constructed using either kernel\nfunction such the Gaussian (aka RBF) kernel of the euclidean distanced `d(X,\nX)`:\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, using `precomputed`, a user-provided affinity matrix can be\nused.\n\nRead more in the User Guide.\n\nThe dimension of the projection subspace.\n\nThe eigenvalue decomposition strategy to use. AMG requires pyamg to be\ninstalled. It can be faster on very large, sparse problems, but may also lead\nto instabilities. If None, then `'arpack'` is used.\n\nNumber of eigen vectors to use for the spectral embedding\n\nA pseudo random number generator used for the initialization of the lobpcg\neigen vectors decomposition when `eigen_solver='amg'` and by the K-Means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nKernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored\nfor `affinity='nearest_neighbors'`.\n\nOnly kernels that produce similarity scores (non-negative values that increase\nwith similarity) should be used. This property is not checked by the\nclustering algorithm.\n\nNumber of neighbors to use when constructing the affinity matrix using the\nnearest neighbors method. Ignored for `affinity='rbf'`.\n\nStopping criterion for eigendecomposition of the Laplacian matrix when\n`eigen_solver='arpack'`.\n\nThe strategy to use to assign labels in the embedding space. There are two\nways to assign labels after the laplacian embedding. k-means can be applied\nand is a popular choice. But it can also be sensitive to initialization.\nDiscretization is another approach which is less sensitive to random\ninitialization.\n\nDegree of the polynomial kernel. Ignored by other kernels.\n\nZero coefficient for polynomial and sigmoid kernels. Ignored by other kernels.\n\nParameters (keyword arguments) and values for kernel passed as callable\nobject. Ignored by other kernels.\n\nThe number of parallel jobs to run when `affinity='nearest_neighbors'` or\n`affinity='precomputed_nearest_neighbors'`. The neighbors search will be done\nin parallel. `None` means 1 unless in a `joblib.parallel_backend` context.\n`-1` means using all processors. See Glossary for more details.\n\nVerbosity mode.\n\nNew in version 0.24.\n\nAffinity matrix used for clustering. Available only if after calling `fit`.\n\nLabels of each point\n\nIf you have an affinity matrix, such as a distance matrix, for which 0 means\nidentical elements, and high values means very dissimilar elements, it can be\ntransformed in a similarity matrix that is well suited for the algorithm by\napplying the Gaussian (RBF, heat) kernel:\n\nWhere `delta` is a free parameter representing the width of the Gaussian\nkernel.\n\nAnother alternative is to take a symmetric version of the k nearest neighbors\nconnectivity matrix of the points.\n\nIf the pyamg package is installed, it is used: this greatly speeds up\ncomputation.\n\n`fit`(X[, y])\n\nPerform spectral clustering from features, or affinity matrix.\n\n`fit_predict`(X[, y])\n\nPerform spectral clustering from features, or affinity matrix, and return\ncluster labels.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nPerform spectral clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nPerform spectral clustering from features, or affinity matrix, and return\ncluster labels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nComparing different clustering algorithms on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering.fit()", "path": "modules/generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering.fit", "type": "cluster", "text": "\nPerform spectral clustering from features, or affinity matrix.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering.fit_predict()", "path": "modules/generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering.fit_predict", "type": "cluster", "text": "\nPerform spectral clustering from features, or affinity matrix, and return\ncluster labels.\n\nTraining instances to cluster, or similarities / affinities between instances\nif `affinity='precomputed'`. If a sparse matrix is provided in a format other\nthan `csr_matrix`, `csc_matrix`, or `coo_matrix`, it will be converted into a\nsparse `csr_matrix`.\n\nNot used, present here for API consistency by convention.\n\nCluster labels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering.get_params()", "path": "modules/generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralClustering.set_params()", "path": "modules/generated/sklearn.cluster.spectralclustering#sklearn.cluster.SpectralClustering.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering", "type": "cluster", "text": "\nSpectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed normalized cut\nof the bipartite graph created from `X` as follows: the edge between row\nvertex `i` and column vertex `j` has weight `X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each row and each\ncolumn belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the User Guide.\n\nThe number of biclusters to find.\n\nSelects the algorithm for finding singular vectors. May be \u2018randomized\u2019 or\n\u2018arpack\u2019. If \u2018randomized\u2019, use `sklearn.utils.extmath.randomized_svd`, which\nmay be faster for large matrices. If \u2018arpack\u2019, use `scipy.sparse.linalg.svds`,\nwhich is more accurate, but possibly slower in some cases.\n\nNumber of vectors to use in calculating the SVD. Corresponds to `ncv` when\n`svd_method=arpack` and `n_oversamples` when `svd_method` is \u2018randomized`.\n\nWhether to use mini-batch k-means, which is faster but may get different\nresults.\n\nMethod for initialization of k-means algorithm; defaults to \u2018k-means++\u2019.\n\nNumber of random initializations that are tried with the k-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is chosen and the\nalgorithm runs once. Otherwise, the algorithm is run for each initialization\nand the best solution chosen.\n\nThe number of jobs to use for the computation. This works by breaking down the\npairwise matrix into n_jobs even slices and computing them in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nUsed for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nResults of the clustering. `rows[i, r]` is True if cluster `i` contains row\n`r`. Available only after calling `fit`.\n\nResults of the clustering, like `rows`.\n\nThe bicluster label of each row.\n\nThe bicluster label of each column.\n\n`fit`(X[, y])\n\nCreates a biclustering for X.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nCreates a biclustering for X.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering()", "path": "modules/generated/sklearn.cluster.spectralcoclustering", "type": "cluster", "text": "\nSpectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed normalized cut\nof the bipartite graph created from `X` as follows: the edge between row\nvertex `i` and column vertex `j` has weight `X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each row and each\ncolumn belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the User Guide.\n\nThe number of biclusters to find.\n\nSelects the algorithm for finding singular vectors. May be \u2018randomized\u2019 or\n\u2018arpack\u2019. If \u2018randomized\u2019, use `sklearn.utils.extmath.randomized_svd`, which\nmay be faster for large matrices. If \u2018arpack\u2019, use `scipy.sparse.linalg.svds`,\nwhich is more accurate, but possibly slower in some cases.\n\nNumber of vectors to use in calculating the SVD. Corresponds to `ncv` when\n`svd_method=arpack` and `n_oversamples` when `svd_method` is \u2018randomized`.\n\nWhether to use mini-batch k-means, which is faster but may get different\nresults.\n\nMethod for initialization of k-means algorithm; defaults to \u2018k-means++\u2019.\n\nNumber of random initializations that are tried with the k-means algorithm.\n\nIf mini-batch k-means is used, the best initialization is chosen and the\nalgorithm runs once. Otherwise, the algorithm is run for each initialization\nand the best solution chosen.\n\nThe number of jobs to use for the computation. This works by breaking down the\npairwise matrix into n_jobs even slices and computing them in parallel.\n\n`None` means 1 unless in a `joblib.parallel_backend` context. `-1` means using\nall processors. See Glossary for more details.\n\nDeprecated since version 0.23: `n_jobs` was deprecated in version 0.23 and\nwill be removed in 1.0 (renaming of 0.25).\n\nUsed for randomizing the singular value decomposition and the k-means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nResults of the clustering. `rows[i, r]` is True if cluster `i` contains row\n`r`. Available only after calling `fit`.\n\nResults of the clustering, like `rows`.\n\nThe bicluster label of each row.\n\nThe bicluster label of each column.\n\n`fit`(X[, y])\n\nCreates a biclustering for X.\n\n`get_indices`(i)\n\nRow and column indices of the `i`\u2019th bicluster.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_shape`(i)\n\nShape of the `i`\u2019th bicluster.\n\n`get_submatrix`(i, data)\n\nReturn the submatrix corresponding to bicluster `i`.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\nCreates a biclustering for X.\n\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nA demo of the Spectral Co-Clustering algorithm\n\nBiclustering documents with the Spectral Co-clustering algorithm\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.biclusters_()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.biclusters_", "type": "cluster", "text": "\nConvenient way to get row and column indicators together.\n\nReturns the `rows_` and `columns_` members.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.fit()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.fit", "type": "cluster", "text": "\nCreates a biclustering for X.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.get_indices()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.get_indices", "type": "cluster", "text": "\nRow and column indices of the `i`\u2019th bicluster.\n\nOnly works if `rows_` and `columns_` attributes exist.\n\nThe index of the cluster.\n\nIndices of rows in the dataset that belong to the bicluster.\n\nIndices of columns in the dataset that belong to the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.get_params()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.get_params", "type": "cluster", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.get_shape()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.get_shape", "type": "cluster", "text": "\nShape of the `i`\u2019th bicluster.\n\nThe index of the cluster.\n\nNumber of rows in the bicluster.\n\nNumber of columns in the bicluster.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.get_submatrix()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.get_submatrix", "type": "cluster", "text": "\nReturn the submatrix corresponding to bicluster `i`.\n\nThe index of the cluster.\n\nThe data.\n\nThe submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if `rows_` and `columns_` attributes\nexist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.SpectralCoclustering.set_params()", "path": "modules/generated/sklearn.cluster.spectralcoclustering#sklearn.cluster.SpectralCoclustering.set_params", "type": "cluster", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.spectral_clustering()", "path": "modules/generated/sklearn.cluster.spectral_clustering#sklearn.cluster.spectral_clustering", "type": "cluster", "text": "\nApply clustering to a projection of the normalized Laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of the\nindividual clusters is highly non-convex or more generally when a measure of\nthe center and spread of the cluster is not a suitable description of the\ncomplete cluster. For instance, when clusters are nested circles on the 2D\nplane.\n\nIf affinity is the adjacency matrix of a graph, this method can be used to\nfind normalized graph cuts.\n\nRead more in the User Guide.\n\nThe affinity matrix describing the relationship of the samples to embed. Must\nbe symmetric.\n\nNumber of clusters to extract.\n\nNumber of eigen vectors to use for the spectral embedding\n\nThe eigenvalue decomposition strategy to use. AMG requires pyamg to be\ninstalled. It can be faster on very large, sparse problems, but may also lead\nto instabilities. If None, then `'arpack'` is used.\n\nA pseudo random number generator used for the initialization of the lobpcg\neigen vectors decomposition when eigen_solver == \u2018amg\u2019 and by the K-Means\ninitialization. Use an int to make the randomness deterministic. See Glossary.\n\nNumber of time the k-means algorithm will be run with different centroid\nseeds. The final results will be the best output of n_init consecutive runs in\nterms of inertia.\n\nStopping criterion for eigendecomposition of the Laplacian matrix when using\narpack eigen_solver.\n\nThe strategy to use to assign labels in the embedding space. There are two\nways to assign labels after the laplacian embedding. k-means can be applied\nand is a popular choice. But it can also be sensitive to initialization.\nDiscretization is another approach which is less sensitive to random\ninitialization. See the \u2018Multiclass spectral clustering\u2019 paper referenced\nbelow for more details on the discretization approach.\n\nVerbosity mode.\n\nNew in version 0.24.\n\nThe labels of the clusters.\n\nThe graph should contain only one connect component, elsewhere the results\nmake little sense.\n\nThis algorithm solves the normalized cut for k=2: it is a normalized spectral\nclustering.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "cluster.ward_tree()", "path": "modules/generated/sklearn.cluster.ward_tree#sklearn.cluster.ward_tree", "type": "cluster", "text": "\nWard clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases within-\ncluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the User Guide.\n\nfeature matrix representing n_samples samples to be clustered\n\nconnectivity matrix. Defines for each sample the neighboring samples following\na given structure of the data. The matrix is assumed to be symmetric and only\nthe upper triangular half is used. Default is None, i.e, the Ward algorithm is\nunstructured.\n\nStop early the construction of the tree at n_clusters. This is useful to\ndecrease computation time if the number of clusters is not small compared to\nthe number of samples. In this case, the complete tree is not computed, thus\nthe \u2018children\u2019 output is of limited use, and the \u2018parents\u2019 output should\nrather be used. This option is valid only when specifying a connectivity\nmatrix.\n\nIf True, return the distance between the clusters.\n\nThe children of each non-leaf node. Values less than `n_samples` correspond to\nleaves of the tree which are the original samples. A node `i` greater than or\nequal to `n_samples` is a non-leaf node and has children `children_[i -\nn_samples]`. Alternatively at the i-th iteration, children[i][0] and\nchildren[i][1] are merged to form node `n_samples + i`\n\nThe number of connected components in the graph.\n\nThe number of leaves in the tree\n\nThe parent of each node. Only returned when a connectivity matrix is\nspecified, elsewhere \u2018None\u2019 is returned.\n\nOnly returned if return_distance is set to True (for compatibility). The\ndistances between the centers of the nodes. `distances[i]` corresponds to a\nweighted euclidean distance between the nodes `children[i, 1]` and\n`children[i, 2]`. If the nodes refer to leaves of the tree, then\n`distances[i]` is their unweighted euclidean distance. Distances are updated\nin the following way (from scipy.hierarchy.linkage):\n\nThe new entry \\\\(d(u,v)\\\\) is computed as follows,\n\nwhere \\\\(u\\\\) is the newly joined cluster consisting of clusters \\\\(s\\\\) and\n\\\\(t\\\\), \\\\(v\\\\) is an unused cluster in the forest, \\\\(T=|v|+|s|+|t|\\\\), and\n\\\\(|*|\\\\) is the cardinality of its argument. This is also known as the\nincremental algorithm.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer", "type": "compose", "text": "\nApplies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input to be\ntransformed separately and the features generated by each transformer will be\nconcatenated to form a single feature space. This is useful for heterogeneous\nor columnar data, to combine several feature extraction mechanisms or\ntransformations into a single transformer.\n\nRead more in the User Guide.\n\nNew in version 0.20.\n\nList of (name, transformer, columns) tuples specifying the transformer objects\nto be applied to subsets of the data.\n\nLike in Pipeline and FeatureUnion, this allows the transformer and its\nparameters to be set using `set_params` and searched in grid search.\n\nEstimator must support fit and transform. Special-cased strings \u2018drop\u2019 and\n\u2018passthrough\u2019 are accepted as well, to indicate to drop the columns or to pass\nthem through untransformed, respectively.\n\nIndexes the data on its second axis. Integers are interpreted as positional\ncolumns, while strings can reference DataFrame columns by name. A scalar\nstring or int should be used where `transformer` expects X to be a 1d array-\nlike (vector), otherwise a 2d array will be passed to the transformer. A\ncallable is passed the input data `X` and can return any of the above. To\nselect multiple columns by name or dtype, you can use `make_column_selector`.\n\nBy default, only the specified columns in `transformers` are transformed and\ncombined in the output, and the non-specified columns are dropped. (default of\n`'drop'`). By specifying `remainder='passthrough'`, all remaining columns that\nwere not specified in `transformers` will be automatically passed through.\nThis subset of columns is concatenated with the output of the transformers. By\nsetting `remainder` to be an estimator, the remaining non-specified columns\nwill use the `remainder` estimator. The estimator must support fit and\ntransform. Note that using this feature requires that the DataFrame columns\ninput at fit and transform have identical order.\n\nIf the output of the different transformers contains sparse matrices, these\nwill be stacked as a sparse matrix if the overall density is lower than this\nvalue. Use `sparse_threshold=0` to always return dense. When the transformed\noutput consists of all dense data, the stacked result will be dense, and this\nkeyword will be ignored.\n\nNumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nMultiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names, values\nthe weights.\n\nIf True, the time elapsed while fitting each transformer will be printed as it\nis completed.\n\nThe collection of fitted transformers as tuples of (name, fitted_transformer,\ncolumn). `fitted_transformer` can be an estimator, \u2018drop\u2019, or \u2018passthrough\u2019.\nIn case there were no columns selected, this will be the unfitted transformer.\nIf there are remaining columns, the final element is a tuple of the form:\n(\u2018remainder\u2019, transformer, remaining_columns) corresponding to the `remainder`\nparameter. If there are remaining columns, then\n`len(transformers_)==len(transformers)+1`, otherwise\n`len(transformers_)==len(transformers)`.\n\nAccess the fitted transformer by name.\n\nBoolean flag indicating whether the output of `transform` is a sparse matrix\nor a dense numpy array, which depends on the output of the individual\ntransformers and the `sparse_threshold` keyword.\n\nSee also\n\nConvenience function for combining the outputs of multiple transformer objects\napplied to column subsets of the original feature space.\n\nConvenience function for selecting columns based on datatype or the columns\nname with a regex pattern.\n\nThe order of the columns in the transformed feature matrix follows the order\nof how the columns are specified in the `transformers` list. Columns of the\noriginal feature matrix that are not specified are dropped from the resulting\ntransformed feature matrix, unless specified in the `passthrough` keyword.\nThose columns specified with `passthrough` are added at the right to the\noutput of the transformers.\n\n`fit`(X[, y])\n\nFit all transformers using X.\n\n`fit_transform`(X[, y])\n\nFit all transformers, transform the data and concatenate results.\n\n`get_feature_names`()\n\nGet feature names from all transformers.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**kwargs)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X separately by each transformer, concatenate results.\n\nFit all transformers using X.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nThis estimator\n\nFit all transformers, transform the data and concatenate results.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\nGet feature names from all transformers.\n\nNames of the features produced by transform.\n\nGet parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the estimators\ncontained within the `transformers` of the `ColumnTransformer`.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nAccess the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name. Keys are\ntransformer names and values are the fitted transformer objects.\n\nSet the parameters of this estimator.\n\nValid parameter keys can be listed with `get_params()`. Note that you can\ndirectly set the parameters of the estimators contained in `transformers` of\n`ColumnTransformer`.\n\nTransform X separately by each transformer, concatenate results.\n\nThe data to be transformed by subset.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer()", "path": "modules/generated/sklearn.compose.columntransformer", "type": "compose", "text": "\nApplies transformers to columns of an array or pandas DataFrame.\n\nThis estimator allows different columns or column subsets of the input to be\ntransformed separately and the features generated by each transformer will be\nconcatenated to form a single feature space. This is useful for heterogeneous\nor columnar data, to combine several feature extraction mechanisms or\ntransformations into a single transformer.\n\nRead more in the User Guide.\n\nNew in version 0.20.\n\nList of (name, transformer, columns) tuples specifying the transformer objects\nto be applied to subsets of the data.\n\nLike in Pipeline and FeatureUnion, this allows the transformer and its\nparameters to be set using `set_params` and searched in grid search.\n\nEstimator must support fit and transform. Special-cased strings \u2018drop\u2019 and\n\u2018passthrough\u2019 are accepted as well, to indicate to drop the columns or to pass\nthem through untransformed, respectively.\n\nIndexes the data on its second axis. Integers are interpreted as positional\ncolumns, while strings can reference DataFrame columns by name. A scalar\nstring or int should be used where `transformer` expects X to be a 1d array-\nlike (vector), otherwise a 2d array will be passed to the transformer. A\ncallable is passed the input data `X` and can return any of the above. To\nselect multiple columns by name or dtype, you can use `make_column_selector`.\n\nBy default, only the specified columns in `transformers` are transformed and\ncombined in the output, and the non-specified columns are dropped. (default of\n`'drop'`). By specifying `remainder='passthrough'`, all remaining columns that\nwere not specified in `transformers` will be automatically passed through.\nThis subset of columns is concatenated with the output of the transformers. By\nsetting `remainder` to be an estimator, the remaining non-specified columns\nwill use the `remainder` estimator. The estimator must support fit and\ntransform. Note that using this feature requires that the DataFrame columns\ninput at fit and transform have identical order.\n\nIf the output of the different transformers contains sparse matrices, these\nwill be stacked as a sparse matrix if the overall density is lower than this\nvalue. Use `sparse_threshold=0` to always return dense. When the transformed\noutput consists of all dense data, the stacked result will be dense, and this\nkeyword will be ignored.\n\nNumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nMultiplicative weights for features per transformer. The output of the\ntransformer is multiplied by these weights. Keys are transformer names, values\nthe weights.\n\nIf True, the time elapsed while fitting each transformer will be printed as it\nis completed.\n\nThe collection of fitted transformers as tuples of (name, fitted_transformer,\ncolumn). `fitted_transformer` can be an estimator, \u2018drop\u2019, or \u2018passthrough\u2019.\nIn case there were no columns selected, this will be the unfitted transformer.\nIf there are remaining columns, the final element is a tuple of the form:\n(\u2018remainder\u2019, transformer, remaining_columns) corresponding to the `remainder`\nparameter. If there are remaining columns, then\n`len(transformers_)==len(transformers)+1`, otherwise\n`len(transformers_)==len(transformers)`.\n\nAccess the fitted transformer by name.\n\nBoolean flag indicating whether the output of `transform` is a sparse matrix\nor a dense numpy array, which depends on the output of the individual\ntransformers and the `sparse_threshold` keyword.\n\nSee also\n\nConvenience function for combining the outputs of multiple transformer objects\napplied to column subsets of the original feature space.\n\nConvenience function for selecting columns based on datatype or the columns\nname with a regex pattern.\n\nThe order of the columns in the transformed feature matrix follows the order\nof how the columns are specified in the `transformers` list. Columns of the\noriginal feature matrix that are not specified are dropped from the resulting\ntransformed feature matrix, unless specified in the `passthrough` keyword.\nThose columns specified with `passthrough` are added at the right to the\noutput of the transformers.\n\n`fit`(X[, y])\n\nFit all transformers using X.\n\n`fit_transform`(X[, y])\n\nFit all transformers, transform the data and concatenate results.\n\n`get_feature_names`()\n\nGet feature names from all transformers.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`set_params`(**kwargs)\n\nSet the parameters of this estimator.\n\n`transform`(X)\n\nTransform X separately by each transformer, concatenate results.\n\nFit all transformers using X.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nThis estimator\n\nFit all transformers, transform the data and concatenate results.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\nGet feature names from all transformers.\n\nNames of the features produced by transform.\n\nGet parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the estimators\ncontained within the `transformers` of the `ColumnTransformer`.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nAccess the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name. Keys are\ntransformer names and values are the fitted transformer objects.\n\nSet the parameters of this estimator.\n\nValid parameter keys can be listed with `get_params()`. Note that you can\ndirectly set the parameters of the estimators contained in `transformers` of\n`ColumnTransformer`.\n\nTransform X separately by each transformer, concatenate results.\n\nThe data to be transformed by subset.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\nPoisson regression and non-normal loss\n\nTweedie regression on insurance claims\n\nPermutation Importance vs Random Forest Feature Importance (MDI)\n\nColumn Transformer with Mixed Types\n\nColumn Transformer with Heterogeneous Data Sources\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.fit()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.fit", "type": "compose", "text": "\nFit all transformers using X.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nThis estimator\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.fit_transform()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.fit_transform", "type": "compose", "text": "\nFit all transformers, transform the data and concatenate results.\n\nInput data, of which specified subsets are used to fit the transformers.\n\nTargets for supervised learning.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.get_feature_names()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.get_feature_names", "type": "compose", "text": "\nGet feature names from all transformers.\n\nNames of the features produced by transform.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.get_params()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.get_params", "type": "compose", "text": "\nGet parameters for this estimator.\n\nReturns the parameters given in the constructor as well as the estimators\ncontained within the `transformers` of the `ColumnTransformer`.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.named_transformers_()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.named_transformers_", "type": "compose", "text": "\nAccess the fitted transformer by name.\n\nRead-only attribute to access any transformer by given name. Keys are\ntransformer names and values are the fitted transformer objects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.set_params()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.set_params", "type": "compose", "text": "\nSet the parameters of this estimator.\n\nValid parameter keys can be listed with `get_params()`. Note that you can\ndirectly set the parameters of the estimators contained in `transformers` of\n`ColumnTransformer`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.ColumnTransformer.transform()", "path": "modules/generated/sklearn.compose.columntransformer#sklearn.compose.ColumnTransformer.transform", "type": "compose", "text": "\nTransform X separately by each transformer, concatenate results.\n\nThe data to be transformed by subset.\n\nhstack of results of transformers. sum_n_components is the sum of n_components\n(output dimension) over transformers. If any result is a sparse matrix,\neverything will be converted to sparse matrices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.make_column_selector()", "path": "modules/generated/sklearn.compose.make_column_selector#sklearn.compose.make_column_selector", "type": "compose", "text": "\nCreate a callable to select columns to be used with `ColumnTransformer`.\n\n`make_column_selector` can select columns based on datatype or the columns\nname with a regex. When using multiple selection criteria, all criteria must\nmatch for a column to be selected.\n\nName of columns containing this regex pattern will be included. If None,\ncolumn selection will not be selected based on pattern.\n\nA selection of dtypes to include. For more details, see\n`pandas.DataFrame.select_dtypes`.\n\nA selection of dtypes to exclude. For more details, see\n`pandas.DataFrame.select_dtypes`.\n\nCallable for column selection to be used by a `ColumnTransformer`.\n\nSee also\n\nClass that allows combining the outputs of multiple transformer objects used\non column subsets of the data into a single feature space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.make_column_transformer()", "path": "modules/generated/sklearn.compose.make_column_transformer#sklearn.compose.make_column_transformer", "type": "compose", "text": "\nConstruct a ColumnTransformer from the given transformers.\n\nThis is a shorthand for the ColumnTransformer constructor; it does not\nrequire, and does not permit, naming the transformers. Instead, they will be\ngiven names automatically based on their types. It also does not allow\nweighting with `transformer_weights`.\n\nRead more in the User Guide.\n\nTuples of the form (transformer, columns) specifying the transformer objects\nto be applied to subsets of the data.\n\nEstimator must support fit and transform. Special-cased strings \u2018drop\u2019 and\n\u2018passthrough\u2019 are accepted as well, to indicate to drop the columns or to pass\nthem through untransformed, respectively.\n\nIndexes the data on its second axis. Integers are interpreted as positional\ncolumns, while strings can reference DataFrame columns by name. A scalar\nstring or int should be used where `transformer` expects X to be a 1d array-\nlike (vector), otherwise a 2d array will be passed to the transformer. A\ncallable is passed the input data `X` and can return any of the above. To\nselect multiple columns by name or dtype, you can use `make_column_selector`.\n\nBy default, only the specified columns in `transformers` are transformed and\ncombined in the output, and the non-specified columns are dropped. (default of\n`'drop'`). By specifying `remainder='passthrough'`, all remaining columns that\nwere not specified in `transformers` will be automatically passed through.\nThis subset of columns is concatenated with the output of the transformers. By\nsetting `remainder` to be an estimator, the remaining non-specified columns\nwill use the `remainder` estimator. The estimator must support fit and\ntransform.\n\nIf the transformed output consists of a mix of sparse and dense data, it will\nbe stacked as a sparse matrix if the density is lower than this value. Use\n`sparse_threshold=0` to always return dense. When the transformed output\nconsists of all sparse or all dense data, the stacked result will be sparse or\ndense, respectively, and this keyword will be ignored.\n\nNumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nIf True, the time elapsed while fitting each transformer will be printed as it\nis completed.\n\nSee also\n\nClass that allows combining the outputs of multiple transformer objects used\non column subsets of the data into a single feature space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor", "type": "compose", "text": "\nMeta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target `y` in\nregression problems. This transformation can be given as a Transformer such as\nthe QuantileTransformer or as a function and its inverse such as `log` and\n`exp`.\n\nThe computation during `fit` is:\n\nor:\n\nThe computation during `predict` is:\n\nor:\n\nRead more in the User Guide.\n\nNew in version 0.20.\n\nRegressor object such as derived from `RegressorMixin`. This regressor will\nautomatically be cloned each time prior to fitting. If regressor is `None`,\n`LinearRegression()` is created and used.\n\nEstimator object such as derived from `TransformerMixin`. Cannot be set at the\nsame time as `func` and `inverse_func`. If `transformer` is `None` as well as\n`func` and `inverse_func`, the transformer will be an identity transformer.\nNote that the transformer will be cloned during fitting. Also, the transformer\nis restricting `y` to be a numpy array.\n\nFunction to apply to `y` before passing to `fit`. Cannot be set at the same\ntime as `transformer`. The function needs to return a 2-dimensional array. If\n`func` is `None`, the function used will be the identity function.\n\nFunction to apply to the prediction of the regressor. Cannot be set at the\nsame time as `transformer` as well. The function needs to return a\n2-dimensional array. The inverse function is used to return predictions to the\nsame space of the original training labels.\n\nWhether to check that `transform` followed by `inverse_transform` or `func`\nfollowed by `inverse_func` leads to the original targets.\n\nFitted regressor.\n\nTransformer used in `fit` and `predict`.\n\nInternally, the target `y` is always converted into a 2-dimensional array to\nbe used by scikit-learn transformers. At the time of prediction, the output\nwill be reshaped to a have the same number of dimensions as `y`.\n\nSee examples/compose/plot_transformed_target.py.\n\n`fit`(X, y, **fit_params)\n\nFit the model according to the given training data.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict using the base regressor, applying inverse.\n\n`score`(X, y[, sample_weight])\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the model according to the given training data.\n\nTraining vector, where n_samples is the number of samples and n_features is\nthe number of features.\n\nTarget values.\n\nParameters passed to the `fit` method of the underlying regressor.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the `inverse_func` or `inverse_transform`\nis applied before returning the prediction.\n\nSamples.\n\nPredicted values.\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor()", "path": "modules/generated/sklearn.compose.transformedtargetregressor", "type": "compose", "text": "\nMeta-estimator to regress on a transformed target.\n\nUseful for applying a non-linear transformation to the target `y` in\nregression problems. This transformation can be given as a Transformer such as\nthe QuantileTransformer or as a function and its inverse such as `log` and\n`exp`.\n\nThe computation during `fit` is:\n\nor:\n\nThe computation during `predict` is:\n\nor:\n\nRead more in the User Guide.\n\nNew in version 0.20.\n\nRegressor object such as derived from `RegressorMixin`. This regressor will\nautomatically be cloned each time prior to fitting. If regressor is `None`,\n`LinearRegression()` is created and used.\n\nEstimator object such as derived from `TransformerMixin`. Cannot be set at the\nsame time as `func` and `inverse_func`. If `transformer` is `None` as well as\n`func` and `inverse_func`, the transformer will be an identity transformer.\nNote that the transformer will be cloned during fitting. Also, the transformer\nis restricting `y` to be a numpy array.\n\nFunction to apply to `y` before passing to `fit`. Cannot be set at the same\ntime as `transformer`. The function needs to return a 2-dimensional array. If\n`func` is `None`, the function used will be the identity function.\n\nFunction to apply to the prediction of the regressor. Cannot be set at the\nsame time as `transformer` as well. The function needs to return a\n2-dimensional array. The inverse function is used to return predictions to the\nsame space of the original training labels.\n\nWhether to check that `transform` followed by `inverse_transform` or `func`\nfollowed by `inverse_func` leads to the original targets.\n\nFitted regressor.\n\nTransformer used in `fit` and `predict`.\n\nInternally, the target `y` is always converted into a 2-dimensional array to\nbe used by scikit-learn transformers. At the time of prediction, the output\nwill be reshaped to a have the same number of dimensions as `y`.\n\nSee examples/compose/plot_transformed_target.py.\n\n`fit`(X, y, **fit_params)\n\nFit the model according to the given training data.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`predict`(X)\n\nPredict using the base regressor, applying inverse.\n\n`score`(X, y[, sample_weight])\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nFit the model according to the given training data.\n\nTraining vector, where n_samples is the number of samples and n_features is\nthe number of features.\n\nTarget values.\n\nParameters passed to the `fit` method of the underlying regressor.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nPredict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the `inverse_func` or `inverse_transform`\nis applied before returning the prediction.\n\nSamples.\n\nPredicted values.\n\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nPoisson regression and non-normal loss\n\nCommon pitfalls in interpretation of coefficients of linear models\n\nEffect of transforming the targets in regression model\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor.fit()", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor.fit", "type": "compose", "text": "\nFit the model according to the given training data.\n\nTraining vector, where n_samples is the number of samples and n_features is\nthe number of features.\n\nTarget values.\n\nParameters passed to the `fit` method of the underlying regressor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor.get_params()", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor.get_params", "type": "compose", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor.predict()", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor.predict", "type": "compose", "text": "\nPredict using the base regressor, applying inverse.\n\nThe regressor is used to predict and the `inverse_func` or `inverse_transform`\nis applied before returning the prediction.\n\nSamples.\n\nPredicted values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor.score()", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor.score", "type": "compose", "text": "\nReturn the coefficient of determination \\\\(R^2\\\\) of the prediction.\n\nThe coefficient \\\\(R^2\\\\) is defined as \\\\((1 - \\frac{u}{v})\\\\), where \\\\(u\\\\)\nis the residual sum of squares `((y_true - y_pred) ** 2).sum()` and \\\\(v\\\\) is\nthe total sum of squares `((y_true - y_true.mean()) ** 2).sum()`. The best\npossible score is 1.0 and it can be negative (because the model can be\narbitrarily worse). A constant model that always predicts the expected value\nof `y`, disregarding the input features, would get a \\\\(R^2\\\\) score of 0.0.\n\nTest samples. For some estimators this may be a precomputed kernel matrix or a\nlist of generic objects instead with shape `(n_samples, n_samples_fitted)`,\nwhere `n_samples_fitted` is the number of samples used in the fitting for the\nestimator.\n\nTrue values for `X`.\n\nSample weights.\n\n\\\\(R^2\\\\) of `self.predict(X)` wrt. `y`.\n\nThe \\\\(R^2\\\\) score used when calling `score` on a regressor uses\n`multioutput='uniform_average'` from version 0.23 to keep consistent with\ndefault value of `r2_score`. This influences the `score` method of all the\nmultioutput regressors (except for `MultiOutputRegressor`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "compose.TransformedTargetRegressor.set_params()", "path": "modules/generated/sklearn.compose.transformedtargetregressor#sklearn.compose.TransformedTargetRegressor.set_params", "type": "compose", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "config_context()", "path": "modules/generated/sklearn.config_context#sklearn.config_context", "type": "sklearn", "text": "\nContext manager for global scikit-learn configuration\n\nIf True, validation for finiteness will be skipped, saving time, but leading\nto potential crashes. If False, validation for finiteness will be performed,\navoiding error. Global default: False.\n\nIf set, scikit-learn will attempt to limit the size of temporary arrays to\nthis number of MiB (per job when parallelised), often saving both computation\ntime and memory on expensive operations that can be performed in chunks.\nGlobal default: 1024.\n\nIf True, only the parameters that were set to non-default values will be\nprinted when printing an estimator. For example, `print(SVC())` while True\nwill only print \u2018SVC()\u2019, but would print \u2018SVC(C=1.0, cache_size=200, \u2026)\u2019 with\nall the non-changed parameters when False. Default is True.\n\nChanged in version 0.23: Default changed from False to True.\n\nIf \u2018diagram\u2019, estimators will be displayed as a diagram in a Jupyter lab or\nnotebook context. If \u2018text\u2019, estimators will be displayed as text. Default is\n\u2018text\u2019.\n\nNew in version 0.23.\n\nSee also\n\nSet global scikit-learn configuration.\n\nRetrieve current values of the global configuration.\n\nAll settings, not just those presently modified, will be returned to their\nprevious values when the context manager is exited. This is not thread-safe.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope", "type": "covariance", "text": "\nAn object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, the support of robust location and covariance estimates is computed,\nand a covariance estimate is recomputed from it, without centering the data.\nUseful to work with data whose mean is significantly equal to zero but is not\nexactly zero. If False, the robust location and covariance are directly\ncomputed with the FastMCD algorithm without additional treatment.\n\nThe proportion of points to be included in the support of the raw MCD\nestimate. If None, the minimum value of support_fraction will be used within\nthe algorithm: `[n_sample + n_features + 1] / 2`. Range is (0, 1).\n\nThe amount of contamination of the data set, i.e. the proportion of outliers\nin the data set. Range is (0, 0.5).\n\nDetermines the pseudo random number generator for shuffling the data. Pass an\nint for reproducible results across multiple function calls. See :term:\n`Glossary <random_state>`.\n\nEstimated robust location.\n\nEstimated robust covariance matrix.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nA mask of the observations that have been used to compute the robust estimates\nof location and shape.\n\nOffset used to define the decision function from the raw scores. We have the\nrelation: `decision_function = score_samples - offset_`. The offset depends on\nthe contamination parameter and is defined in such a way we obtain the\nexpected number of outliers (samples with decision function < 0) in training.\n\nNew in version 0.20.\n\nThe raw robust estimated location before correction and re-weighting.\n\nThe raw robust estimated covariance before correction and re-weighting.\n\nA mask of the observations that have been used to compute the raw robust\nestimates of location and shape, before correction and re-weighting.\n\nMahalanobis distances of the training set (on which `fit` is called)\nobservations.\n\nSee also\n\nOutlier detection from covariance estimation may break or not perform well in\nhigh-dimensional settings. In particular, one will always take care to work\nwith `n_samples > n_features ** 2`.\n\nRousseeuw, P.J., Van Driessen, K. \u201cA fast algorithm for the minimum covariance\ndeterminant estimator\u201d Technometrics 41(3), 212 (1999)\n\n`correct_covariance`(data)\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\n`decision_function`(X)\n\nCompute the decision function of the given observations.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFit the EllipticEnvelope model.\n\n`fit_predict`(X[, y])\n\nPerform fit on X and returns labels for X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`predict`(X)\n\nPredict the labels (1 inlier, -1 outlier) of X according to the fitted model.\n\n`reweight_covariance`(data)\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\n`score`(X, y[, sample_weight])\n\nReturns the mean accuracy on the given test data and labels.\n\n`score_samples`(X)\n\nCompute the negative Mahalanobis distances.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nCompute the decision function of the given observations.\n\nThe data matrix.\n\nDecision function of the samples. It is equal to the shifted Mahalanobis\ndistances. The threshold for being an outlier is 0, which ensures a\ncompatibility with other outlier detection algorithms.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFit the EllipticEnvelope model.\n\nTraining data.\n\nNot used, present for API consistency by convention.\n\nPerform fit on X and returns labels for X.\n\nReturns -1 for outliers and 1 for inliers.\n\nNot used, present for API consistency by convention.\n\n1 for inliers, -1 for outliers.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nPredict the labels (1 inlier, -1 outlier) of X according to the fitted model.\n\nThe data matrix.\n\nReturns -1 for anomalies/outliers and +1 for inliers.\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nReturns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for X.\n\nSample weights.\n\nMean accuracy of self.predict(X) w.r.t. y.\n\nCompute the negative Mahalanobis distances.\n\nThe data matrix.\n\nOpposite of the Mahalanobis distances.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope()", "path": "modules/generated/sklearn.covariance.ellipticenvelope", "type": "covariance", "text": "\nAn object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, the support of robust location and covariance estimates is computed,\nand a covariance estimate is recomputed from it, without centering the data.\nUseful to work with data whose mean is significantly equal to zero but is not\nexactly zero. If False, the robust location and covariance are directly\ncomputed with the FastMCD algorithm without additional treatment.\n\nThe proportion of points to be included in the support of the raw MCD\nestimate. If None, the minimum value of support_fraction will be used within\nthe algorithm: `[n_sample + n_features + 1] / 2`. Range is (0, 1).\n\nThe amount of contamination of the data set, i.e. the proportion of outliers\nin the data set. Range is (0, 0.5).\n\nDetermines the pseudo random number generator for shuffling the data. Pass an\nint for reproducible results across multiple function calls. See :term:\n`Glossary <random_state>`.\n\nEstimated robust location.\n\nEstimated robust covariance matrix.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nA mask of the observations that have been used to compute the robust estimates\nof location and shape.\n\nOffset used to define the decision function from the raw scores. We have the\nrelation: `decision_function = score_samples - offset_`. The offset depends on\nthe contamination parameter and is defined in such a way we obtain the\nexpected number of outliers (samples with decision function < 0) in training.\n\nNew in version 0.20.\n\nThe raw robust estimated location before correction and re-weighting.\n\nThe raw robust estimated covariance before correction and re-weighting.\n\nA mask of the observations that have been used to compute the raw robust\nestimates of location and shape, before correction and re-weighting.\n\nMahalanobis distances of the training set (on which `fit` is called)\nobservations.\n\nSee also\n\nOutlier detection from covariance estimation may break or not perform well in\nhigh-dimensional settings. In particular, one will always take care to work\nwith `n_samples > n_features ** 2`.\n\nRousseeuw, P.J., Van Driessen, K. \u201cA fast algorithm for the minimum covariance\ndeterminant estimator\u201d Technometrics 41(3), 212 (1999)\n\n`correct_covariance`(data)\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\n`decision_function`(X)\n\nCompute the decision function of the given observations.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFit the EllipticEnvelope model.\n\n`fit_predict`(X[, y])\n\nPerform fit on X and returns labels for X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`predict`(X)\n\nPredict the labels (1 inlier, -1 outlier) of X according to the fitted model.\n\n`reweight_covariance`(data)\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\n`score`(X, y[, sample_weight])\n\nReturns the mean accuracy on the given test data and labels.\n\n`score_samples`(X)\n\nCompute the negative Mahalanobis distances.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nCompute the decision function of the given observations.\n\nThe data matrix.\n\nDecision function of the samples. It is equal to the shifted Mahalanobis\ndistances. The threshold for being an outlier is 0, which ensures a\ncompatibility with other outlier detection algorithms.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFit the EllipticEnvelope model.\n\nTraining data.\n\nNot used, present for API consistency by convention.\n\nPerform fit on X and returns labels for X.\n\nReturns -1 for outliers and 1 for inliers.\n\nNot used, present for API consistency by convention.\n\n1 for inliers, -1 for outliers.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nPredict the labels (1 inlier, -1 outlier) of X according to the fitted model.\n\nThe data matrix.\n\nReturns -1 for anomalies/outliers and +1 for inliers.\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nReturns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for X.\n\nSample weights.\n\nMean accuracy of self.predict(X) w.r.t. y.\n\nCompute the negative Mahalanobis distances.\n\nThe data matrix.\n\nOpposite of the Mahalanobis distances.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nOutlier detection on a real data set\n\nComparing anomaly detection algorithms for outlier detection on toy datasets\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.correct_covariance()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.correct_covariance", "type": "covariance", "text": "\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.decision_function()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.decision_function", "type": "covariance", "text": "\nCompute the decision function of the given observations.\n\nThe data matrix.\n\nDecision function of the samples. It is equal to the shifted Mahalanobis\ndistances. The threshold for being an outlier is 0, which ensures a\ncompatibility with other outlier detection algorithms.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.error_norm()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.fit()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.fit", "type": "covariance", "text": "\nFit the EllipticEnvelope model.\n\nTraining data.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.fit_predict()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.fit_predict", "type": "covariance", "text": "\nPerform fit on X and returns labels for X.\n\nReturns -1 for outliers and 1 for inliers.\n\nNot used, present for API consistency by convention.\n\n1 for inliers, -1 for outliers.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.get_params()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.get_precision()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.mahalanobis()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.predict()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.predict", "type": "covariance", "text": "\nPredict the labels (1 inlier, -1 outlier) of X according to the fitted model.\n\nThe data matrix.\n\nReturns -1 for anomalies/outliers and +1 for inliers.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.reweight_covariance()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.reweight_covariance", "type": "covariance", "text": "\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.score()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.score", "type": "covariance", "text": "\nReturns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy which is a harsh\nmetric since you require for each sample that each label set be correctly\npredicted.\n\nTest samples.\n\nTrue labels for X.\n\nSample weights.\n\nMean accuracy of self.predict(X) w.r.t. y.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.score_samples()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.score_samples", "type": "covariance", "text": "\nCompute the negative Mahalanobis distances.\n\nThe data matrix.\n\nOpposite of the Mahalanobis distances.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EllipticEnvelope.set_params()", "path": "modules/generated/sklearn.covariance.ellipticenvelope#sklearn.covariance.EllipticEnvelope.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance", "type": "covariance", "text": "\nMaximum likelihood covariance estimator\n\nRead more in the User Guide.\n\nSpecifies if the estimated precision is stored.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False (default), data are\ncentered before computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix\n\nEstimated pseudo-inverse matrix. (stored only if store_precision is True)\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the Maximum Likelihood Estimator covariance model according to the given\ntraining data and parameters.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the Maximum Likelihood Estimator covariance model according to the given\ntraining data and parameters.\n\nTraining data, where n_samples is the number of samples and n_features is the\nnumber of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance()", "path": "modules/generated/sklearn.covariance.empiricalcovariance", "type": "covariance", "text": "\nMaximum likelihood covariance estimator\n\nRead more in the User Guide.\n\nSpecifies if the estimated precision is stored.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False (default), data are\ncentered before computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix\n\nEstimated pseudo-inverse matrix. (stored only if store_precision is True)\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the Maximum Likelihood Estimator covariance model according to the given\ntraining data and parameters.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the Maximum Likelihood Estimator covariance model according to the given\ntraining data and parameters.\n\nTraining data, where n_samples is the number of samples and n_features is the\nnumber of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n\nRobust covariance estimation and Mahalanobis distances relevance\n\nRobust vs Empirical covariance estimate\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.error_norm()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.fit()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.fit", "type": "covariance", "text": "\nFits the Maximum Likelihood Estimator covariance model according to the given\ntraining data and parameters.\n\nTraining data, where n_samples is the number of samples and n_features is the\nnumber of features.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.get_params()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.get_precision()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.mahalanobis()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.score()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.score", "type": "covariance", "text": "\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.EmpiricalCovariance.set_params()", "path": "modules/generated/sklearn.covariance.empiricalcovariance#sklearn.covariance.EmpiricalCovariance.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.empirical_covariance()", "path": "modules/generated/sklearn.covariance.empirical_covariance#sklearn.covariance.empirical_covariance", "type": "covariance", "text": "\nComputes the Maximum likelihood covariance estimator\n\nData from which to compute the covariance estimate\n\nIf True, data will not be centered before computation. Useful when working\nwith data whose mean is almost, but not exactly zero. If False, data will be\ncentered before computation.\n\nEmpirical covariance (Maximum Likelihood Estimator).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso", "type": "covariance", "text": "\nSparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the User Guide.\n\nChanged in version v0.20: GraphLasso has been renamed to GraphicalLasso\n\nThe regularization parameter: the higher alpha, the more regularization, the\nsparser the inverse covariance. Range is (0, inf].\n\nThe Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse\nunderlying graphs, where p > n. Elsewhere prefer cd which is more numerically\nstable.\n\nThe tolerance to declare convergence: if the dual gap goes below this value,\niterations are stopped. Range is (0, inf].\n\nThe tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction for a\ngiven column update, not of the overall parameter estimate. Only used for\nmode=\u2019cd\u2019. Range is (0, inf].\n\nThe maximum number of iterations.\n\nIf verbose is True, the objective function and dual gap are plotted at each\niteration.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False, data are centered\nbefore computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix\n\nEstimated pseudo inverse matrix.\n\nNumber of iterations run.\n\nSee also\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the GraphicalLasso model to X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the GraphicalLasso model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso()", "path": "modules/generated/sklearn.covariance.graphicallasso", "type": "covariance", "text": "\nSparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the User Guide.\n\nChanged in version v0.20: GraphLasso has been renamed to GraphicalLasso\n\nThe regularization parameter: the higher alpha, the more regularization, the\nsparser the inverse covariance. Range is (0, inf].\n\nThe Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse\nunderlying graphs, where p > n. Elsewhere prefer cd which is more numerically\nstable.\n\nThe tolerance to declare convergence: if the dual gap goes below this value,\niterations are stopped. Range is (0, inf].\n\nThe tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction for a\ngiven column update, not of the overall parameter estimate. Only used for\nmode=\u2019cd\u2019. Range is (0, inf].\n\nThe maximum number of iterations.\n\nIf verbose is True, the objective function and dual gap are plotted at each\niteration.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False, data are centered\nbefore computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix\n\nEstimated pseudo inverse matrix.\n\nNumber of iterations run.\n\nSee also\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the GraphicalLasso model to X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the GraphicalLasso model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.error_norm()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.fit()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.fit", "type": "covariance", "text": "\nFits the GraphicalLasso model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.get_params()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.get_precision()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.mahalanobis()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.score()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.score", "type": "covariance", "text": "\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLasso.set_params()", "path": "modules/generated/sklearn.covariance.graphicallasso#sklearn.covariance.GraphicalLasso.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV", "type": "covariance", "text": "\nSparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for cross-validation estimator.\n\nRead more in the User Guide.\n\nChanged in version v0.20: GraphLassoCV has been renamed to GraphicalLassoCV\n\nIf an integer is given, it fixes the number of points on the grids of alpha to\nbe used. If a list is given, it gives the grid to be used. See the notes in\nthe class docstring for more details. Range is (0, inf] when floats given.\n\nThe number of times the grid is refined. Not used if explicit values of alphas\nare passed. Range is [1, inf).\n\nDetermines the cross-validation splitting strategy. Possible inputs for cv\nare:\n\nFor integer/None inputs `KFold` is used.\n\nRefer User Guide for the various cross-validation strategies that can be used\nhere.\n\nChanged in version 0.20: `cv` default value if None changed from 3-fold to\n5-fold.\n\nThe tolerance to declare convergence: if the dual gap goes below this value,\niterations are stopped. Range is (0, inf].\n\nThe tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction for a\ngiven column update, not of the overall parameter estimate. Only used for\nmode=\u2019cd\u2019. Range is (0, inf].\n\nMaximum number of iterations.\n\nThe Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse\nunderlying graphs, where number of features is greater than number of samples.\nElsewhere prefer cd which is more numerically stable.\n\nnumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nChanged in version v0.20: `n_jobs` default changed from 1 to None\n\nIf verbose is True, the objective function and duality gap are printed at each\niteration.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False, data are centered\nbefore computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix.\n\nEstimated precision matrix (inverse covariance).\n\nPenalization parameter selected.\n\nAll penalization parameters explored.\n\nDeprecated since version 0.24: The `cv_alphas_` attribute is deprecated in\nversion 0.24 in favor of `cv_results_['alphas']` and will be removed in\nversion 1.1 (renaming of 0.26).\n\nLog-likelihood score on left-out data across folds.\n\nDeprecated since version 0.24: The `grid_scores_` attribute is deprecated in\nversion 0.24 in favor of `cv_results_` and will be removed in version 1.1\n(renaming of 0.26).\n\nA dict with keys:\n\nAll penalization parameters explored.\n\nLog-likelihood score on left-out data across (k)th fold.\n\nMean of scores over the folds.\n\nStandard deviation of scores over the folds.\n\nNew in version 0.24.\n\nNumber of iterations run for the optimal alpha.\n\nSee also\n\nThe search for the optimal penalization parameter (alpha) is done on an\niteratively refined grid: first the cross-validated scores on a grid are\ncomputed, then a new refined grid is centered around the maximum, and so on.\n\nOne of the challenges which is faced here is that the solvers can fail to\nconverge to a well-conditioned estimate. The corresponding values of alpha\nthen come out as missing values, but the optimum may be close to these missing\nvalues.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the GraphicalLasso covariance model to X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the GraphicalLasso covariance model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV()", "path": "modules/generated/sklearn.covariance.graphicallassocv", "type": "covariance", "text": "\nSparse inverse covariance w/ cross-validated choice of the l1 penalty.\n\nSee glossary entry for cross-validation estimator.\n\nRead more in the User Guide.\n\nChanged in version v0.20: GraphLassoCV has been renamed to GraphicalLassoCV\n\nIf an integer is given, it fixes the number of points on the grids of alpha to\nbe used. If a list is given, it gives the grid to be used. See the notes in\nthe class docstring for more details. Range is (0, inf] when floats given.\n\nThe number of times the grid is refined. Not used if explicit values of alphas\nare passed. Range is [1, inf).\n\nDetermines the cross-validation splitting strategy. Possible inputs for cv\nare:\n\nFor integer/None inputs `KFold` is used.\n\nRefer User Guide for the various cross-validation strategies that can be used\nhere.\n\nChanged in version 0.20: `cv` default value if None changed from 3-fold to\n5-fold.\n\nThe tolerance to declare convergence: if the dual gap goes below this value,\niterations are stopped. Range is (0, inf].\n\nThe tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction for a\ngiven column update, not of the overall parameter estimate. Only used for\nmode=\u2019cd\u2019. Range is (0, inf].\n\nMaximum number of iterations.\n\nThe Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse\nunderlying graphs, where number of features is greater than number of samples.\nElsewhere prefer cd which is more numerically stable.\n\nnumber of jobs to run in parallel. `None` means 1 unless in a\n`joblib.parallel_backend` context. `-1` means using all processors. See\nGlossary for more details.\n\nChanged in version v0.20: `n_jobs` default changed from 1 to None\n\nIf verbose is True, the objective function and duality gap are printed at each\niteration.\n\nIf True, data are not centered before computation. Useful when working with\ndata whose mean is almost, but not exactly zero. If False, data are centered\nbefore computation.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated covariance matrix.\n\nEstimated precision matrix (inverse covariance).\n\nPenalization parameter selected.\n\nAll penalization parameters explored.\n\nDeprecated since version 0.24: The `cv_alphas_` attribute is deprecated in\nversion 0.24 in favor of `cv_results_['alphas']` and will be removed in\nversion 1.1 (renaming of 0.26).\n\nLog-likelihood score on left-out data across folds.\n\nDeprecated since version 0.24: The `grid_scores_` attribute is deprecated in\nversion 0.24 in favor of `cv_results_` and will be removed in version 1.1\n(renaming of 0.26).\n\nA dict with keys:\n\nAll penalization parameters explored.\n\nLog-likelihood score on left-out data across (k)th fold.\n\nMean of scores over the folds.\n\nStandard deviation of scores over the folds.\n\nNew in version 0.24.\n\nNumber of iterations run for the optimal alpha.\n\nSee also\n\nThe search for the optimal penalization parameter (alpha) is done on an\niteratively refined grid: first the cross-validated scores on a grid are\ncomputed, then a new refined grid is centered around the maximum, and so on.\n\nOne of the challenges which is faced here is that the solvers can fail to\nconverge to a well-conditioned estimate. The corresponding values of alpha\nthen come out as missing values, but the optimum may be close to these missing\nvalues.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits the GraphicalLasso covariance model to X.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits the GraphicalLasso covariance model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nSparse inverse covariance estimation\n\nVisualizing the stock market structure\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.error_norm()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.fit()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.fit", "type": "covariance", "text": "\nFits the GraphicalLasso covariance model to X.\n\nData from which to compute the covariance estimate\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.get_params()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.get_precision()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.mahalanobis()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.score()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.score", "type": "covariance", "text": "\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.GraphicalLassoCV.set_params()", "path": "modules/generated/sklearn.covariance.graphicallassocv#sklearn.covariance.GraphicalLassoCV.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.graphical_lasso()", "path": "modules/generated/sklearn.covariance.graphical_lasso#sklearn.covariance.graphical_lasso", "type": "covariance", "text": "\nl1-penalized covariance estimator\n\nRead more in the User Guide.\n\nChanged in version v0.20: graph_lasso has been renamed to graphical_lasso\n\nEmpirical covariance from which to compute the covariance estimate.\n\nThe regularization parameter: the higher alpha, the more regularization, the\nsparser the inverse covariance. Range is (0, inf].\n\nThe initial guess for the covariance. If None, then the empirical covariance\nis used.\n\nThe Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse\nunderlying graphs, where p > n. Elsewhere prefer cd which is more numerically\nstable.\n\nThe tolerance to declare convergence: if the dual gap goes below this value,\niterations are stopped. Range is (0, inf].\n\nThe tolerance for the elastic net solver used to calculate the descent\ndirection. This parameter controls the accuracy of the search direction for a\ngiven column update, not of the overall parameter estimate. Only used for\nmode=\u2019cd\u2019. Range is (0, inf].\n\nThe maximum number of iterations.\n\nIf verbose is True, the objective function and dual gap are printed at each\niteration.\n\nIf return_costs is True, the objective function and dual gap at each iteration\nare returned.\n\nThe machine-precision regularization in the computation of the Cholesky\ndiagonal factors. Increase this for very ill-conditioned systems. Default is\n`np.finfo(np.float64).eps`.\n\nWhether or not to return the number of iterations.\n\nThe estimated covariance matrix.\n\nThe estimated (sparse) precision matrix.\n\nThe list of values of the objective function and the dual gap at each\niteration. Returned only if return_costs is True.\n\nNumber of iterations. Returned only if `return_n_iter` is set to True.\n\nSee also\n\nThe algorithm employed to solve this problem is the GLasso algorithm, from the\nFriedman 2008 Biostatistics paper. It is the same algorithm as in the R\n`glasso` package.\n\nOne possible difference with the `glasso` R package is that the diagonal\ncoefficients are not penalized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf", "type": "covariance", "text": "\nLedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient\nis computed using O. Ledoit and M. Wolf\u2019s formula as described in \u201cA Well-\nConditioned Estimator for Large-Dimensional Covariance Matrices\u201d, Ledoit and\nWolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004,\npages 365-411.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, data will not be centered before computation. Useful when working\nwith data whose mean is almost, but not exactly zero. If False (default), data\nwill be centered before computation.\n\nSize of blocks into which the covariance matrix will be split during its\nLedoit-Wolf estimation. This is purely a memory optimization and does not\naffect results.\n\nEstimated covariance matrix.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nCoefficient in the convex combination used for the computation of the shrunk\nestimate. Range is [0, 1].\n\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features and shrinkage is given by the Ledoit and\nWolf formula (see References)\n\n\u201cA Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\u201d,\nLedoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFit the Ledoit-Wolf shrunk covariance model according to the given training\ndata and parameters.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFit the Ledoit-Wolf shrunk covariance model according to the given training\ndata and parameters.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf()", "path": "modules/generated/sklearn.covariance.ledoitwolf", "type": "covariance", "text": "\nLedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient\nis computed using O. Ledoit and M. Wolf\u2019s formula as described in \u201cA Well-\nConditioned Estimator for Large-Dimensional Covariance Matrices\u201d, Ledoit and\nWolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004,\npages 365-411.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, data will not be centered before computation. Useful when working\nwith data whose mean is almost, but not exactly zero. If False (default), data\nwill be centered before computation.\n\nSize of blocks into which the covariance matrix will be split during its\nLedoit-Wolf estimation. This is purely a memory optimization and does not\naffect results.\n\nEstimated covariance matrix.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nCoefficient in the convex combination used for the computation of the shrunk\nestimate. Range is [0, 1].\n\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features and shrinkage is given by the Ledoit and\nWolf formula (see References)\n\n\u201cA Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\u201d,\nLedoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2,\nFebruary 2004, pages 365-411.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFit the Ledoit-Wolf shrunk covariance model according to the given training\ndata and parameters.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFit the Ledoit-Wolf shrunk covariance model according to the given training\ndata and parameters.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nLedoit-Wolf vs OAS estimation\n\nShrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood\n\nModel selection with Probabilistic PCA and Factor Analysis (FA)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.error_norm()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.fit()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.fit", "type": "covariance", "text": "\nFit the Ledoit-Wolf shrunk covariance model according to the given training\ndata and parameters.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.get_params()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.get_precision()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.mahalanobis()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.score()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.score", "type": "covariance", "text": "\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.LedoitWolf.set_params()", "path": "modules/generated/sklearn.covariance.ledoitwolf#sklearn.covariance.LedoitWolf.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.ledoit_wolf()", "path": "modules/generated/sklearn.covariance.ledoit_wolf#sklearn.covariance.ledoit_wolf", "type": "covariance", "text": "\nEstimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the User Guide.\n\nData from which to compute the covariance estimate\n\nIf True, data will not be centered before computation. Useful to work with\ndata whose mean is significantly equal to zero but is not exactly zero. If\nFalse, data will be centered before computation.\n\nSize of blocks into which the covariance matrix will be split. This is purely\na memory optimization and does not affect results.\n\nShrunk covariance.\n\nCoefficient in the convex combination used for the computation of the shrunk\nestimate.\n\nThe regularized (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet", "type": "covariance", "text": "\nMinimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied on\nGaussian-distributed data, but could still be relevant on data drawn from a\nunimodal, symmetric distribution. It is not meant to be used with multi-modal\ndata (the algorithm used to fit a MinCovDet object is likely to fail in such a\ncase). One should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, the support of the robust location and the covariance estimates is\ncomputed, and a covariance estimate is recomputed from it, without centering\nthe data. Useful to work with data whose mean is significantly equal to zero\nbut is not exactly zero. If False, the robust location and covariance are\ndirectly computed with the FastMCD algorithm without additional treatment.\n\nThe proportion of points to be included in the support of the raw MCD\nestimate. Default is None, which implies that the minimum value of\nsupport_fraction will be used within the algorithm: `(n_sample + n_features +\n1) / 2`. The parameter must be in the range (0, 1).\n\nDetermines the pseudo random number generator for shuffling the data. Pass an\nint for reproducible results across multiple function calls. See :term:\n`Glossary <random_state>`.\n\nThe raw robust estimated location before correction and re-weighting.\n\nThe raw robust estimated covariance before correction and re-weighting.\n\nA mask of the observations that have been used to compute the raw robust\nestimates of location and shape, before correction and re-weighting.\n\nEstimated robust location.\n\nEstimated robust covariance matrix.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nA mask of the observations that have been used to compute the robust estimates\nof location and shape.\n\nMahalanobis distances of the training set (on which `fit` is called)\nobservations.\n\nP. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871,\n1984.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nR. W. Butler, P. L. Davies and M. Jhun, Asymptotics For The Minimum Covariance\nDeterminant Estimator, The Annals of Statistics, 1993, Vol. 21, No. 3,\n1385-1400\n\n`correct_covariance`(data)\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`reweight_covariance`(data)\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet()", "path": "modules/generated/sklearn.covariance.mincovdet", "type": "covariance", "text": "\nMinimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied on\nGaussian-distributed data, but could still be relevant on data drawn from a\nunimodal, symmetric distribution. It is not meant to be used with multi-modal\ndata (the algorithm used to fit a MinCovDet object is likely to fail in such a\ncase). One should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the User Guide.\n\nSpecify if the estimated precision is stored.\n\nIf True, the support of the robust location and the covariance estimates is\ncomputed, and a covariance estimate is recomputed from it, without centering\nthe data. Useful to work with data whose mean is significantly equal to zero\nbut is not exactly zero. If False, the robust location and covariance are\ndirectly computed with the FastMCD algorithm without additional treatment.\n\nThe proportion of points to be included in the support of the raw MCD\nestimate. Default is None, which implies that the minimum value of\nsupport_fraction will be used within the algorithm: `(n_sample + n_features +\n1) / 2`. The parameter must be in the range (0, 1).\n\nDetermines the pseudo random number generator for shuffling the data. Pass an\nint for reproducible results across multiple function calls. See :term:\n`Glossary <random_state>`.\n\nThe raw robust estimated location before correction and re-weighting.\n\nThe raw robust estimated covariance before correction and re-weighting.\n\nA mask of the observations that have been used to compute the raw robust\nestimates of location and shape, before correction and re-weighting.\n\nEstimated robust location.\n\nEstimated robust covariance matrix.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\nA mask of the observations that have been used to compute the robust estimates\nof location and shape.\n\nMahalanobis distances of the training set (on which `fit` is called)\nobservations.\n\nP. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871,\n1984.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nR. W. Butler, P. L. Davies and M. Jhun, Asymptotics For The Minimum Covariance\nDeterminant Estimator, The Annals of Statistics, 1993, Vol. 21, No. 3,\n1385-1400\n\n`correct_covariance`(data)\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFits a Minimum Covariance Determinant with the FastMCD algorithm.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`reweight_covariance`(data)\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\nRobust covariance estimation and Mahalanobis distances relevance\n\nRobust vs Empirical covariance estimate\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.correct_covariance()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.correct_covariance", "type": "covariance", "text": "\nApply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested by Rousseeuw and\nVan Driessen in [RVD].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nCorrected robust covariance estimate.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.error_norm()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.error_norm", "type": "covariance", "text": "\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.fit()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.fit", "type": "covariance", "text": "\nFits a Minimum Covariance Determinant with the FastMCD algorithm.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.get_params()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.get_params", "type": "covariance", "text": "\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.get_precision()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.get_precision", "type": "covariance", "text": "\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.mahalanobis()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.mahalanobis", "type": "covariance", "text": "\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.reweight_covariance()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.reweight_covariance", "type": "covariance", "text": "\nRe-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw\u2019s method (equivalent to deleting\noutlying observations from the data set before computing location and\ncovariance estimates) described in [RVDriessen].\n\nThe data matrix, with p features and n samples. The data set must be the one\nwhich was used to compute the raw estimates.\n\nRe-weighted robust location estimate.\n\nRe-weighted robust covariance estimate.\n\nA mask of the observations that have been used to compute the re-weighted\nrobust location and covariance estimates.\n\nA Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999,\nAmerican Statistical Association and the American Society for Quality,\nTECHNOMETRICS\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.score()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.score", "type": "covariance", "text": "\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.MinCovDet.set_params()", "path": "modules/generated/sklearn.covariance.mincovdet#sklearn.covariance.MinCovDet.set_params", "type": "covariance", "text": "\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.OAS", "path": "modules/generated/sklearn.covariance.oas#sklearn.covariance.OAS", "type": "covariance", "text": "\nOracle Approximating Shrinkage Estimator\n\nRead more in the User Guide.\n\nOAS is a particular form of shrinkage described in \u201cShrinkage Algorithms for\nMMSE Covariance Estimation\u201d Chen et al., IEEE Trans. on Sign. Proc., Volume\n58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the article. In\nthe original article, formula (23) states that 2/p is multiplied by\nTrace(cov*cov) in both the numerator and denominator, but this operation is\nomitted because for a large p, the value of 2/p is so small that it doesn\u2019t\naffect the value of the estimator.\n\nSpecify if the estimated precision is stored.\n\nIf True, data will not be centered before computation. Useful when working\nwith data whose mean is almost, but not exactly zero. If False (default), data\nwill be centered before computation.\n\nEstimated covariance matrix.\n\nEstimated location, i.e. the estimated mean.\n\nEstimated pseudo inverse matrix. (stored only if store_precision is True)\n\ncoefficient in the convex combination used for the computation of the shrunk\nestimate. Range is [0, 1].\n\nThe regularised covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features and shrinkage is given by the OAS formula\n(see References)\n\n\u201cShrinkage Algorithms for MMSE Covariance Estimation\u201d Chen et al., IEEE Trans.\non Sign. Proc., Volume 58, Issue 10, October 2010.\n\n`error_norm`(comp_cov[, norm, scaling, squared])\n\nComputes the Mean Squared Error between two covariance estimators.\n\n`fit`(X[, y])\n\nFit the Oracle Approximating Shrinkage covariance model according to the given\ntraining data and parameters.\n\n`get_params`([deep])\n\nGet parameters for this estimator.\n\n`get_precision`()\n\nGetter for the precision matrix.\n\n`mahalanobis`(X)\n\nComputes the squared Mahalanobis distances of given observations.\n\n`score`(X_test[, y])\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\n`set_params`(**params)\n\nSet the parameters of this estimator.\n\nComputes the Mean Squared Error between two covariance estimators. (In the\nsense of the Frobenius norm).\n\nThe covariance to compare with.\n\nThe type of norm used to compute the error. Available error types: -\n\u2018frobenius\u2019 (default): sqrt(tr(A^t.A)) - \u2018spectral\u2019:\nsqrt(max(eigenvalues(A^t.A)) where A is the error `(comp_cov -\nself.covariance_)`.\n\nIf True (default), the squared error norm is divided by n_features. If False,\nthe squared error norm is not rescaled.\n\nWhether to compute the squared error norm or the error norm. If True\n(default), the squared error norm is returned. If False, the error norm is\nreturned.\n\nThe Mean Squared Error (in the sense of the Frobenius norm) between `self` and\n`comp_cov` covariance estimators.\n\nFit the Oracle Approximating Shrinkage covariance model according to the given\ntraining data and parameters.\n\nTraining data, where `n_samples` is the number of samples and `n_features` is\nthe number of features.\n\nNot used, present for API consistency by convention.\n\nGet parameters for this estimator.\n\nIf True, will return the parameters for this estimator and contained\nsubobjects that are estimators.\n\nParameter names mapped to their values.\n\nGetter for the precision matrix.\n\nThe precision matrix associated to the current covariance object.\n\nComputes the squared Mahalanobis distances of given observations.\n\nThe observations, the Mahalanobis distances of the which we compute.\nObservations are assumed to be drawn from the same distribution than the data\nused in fit.\n\nSquared Mahalanobis distances of the observations.\n\nComputes the log-likelihood of a Gaussian data set with `self.covariance_` as\nan estimator of its covariance matrix.\n\nTest data of which we compute the likelihood, where n_samples is the number of\nsamples and n_features is the number of features. X_test is assumed to be\ndrawn from the same distribution than the data used in fit (including\ncentering).\n\nNot used, present for API consistency by convention.\n\nThe likelihood of the data set with `self.covariance_` as an estimator of its\ncovariance matrix.\n\nSet the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects (such as\n`Pipeline`). The latter have parameters of the form `<component>__<parameter>`\nso that it\u2019s possible to update each component of a nested object.\n\nEstimator parameters.\n\nEstimator instance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.oas()", "path": "modules/generated/oas-function#sklearn.covariance.oas", "type": "covariance", "text": "\nEstimate covariance with the Oracle Approximating Shrinkage algorithm.\n\nData from which to compute the covariance estimate.\n\nIf True, data will not be centered before computation. Useful to work with\ndata whose mean is significantly equal to zero but is not exactly zero. If\nFalse, data will be centered before computation.\n\nShrunk covariance.\n\nCoefficient in the convex combination used for the computation of the shrunk\nestimate.\n\nThe regularised (shrunk) covariance is:\n\n(1 - shrinkage) * cov + shrinkage * mu * np.identity(n_features)\n\nwhere mu = trace(cov) / n_features\n\nThe formula we used to implement the OAS is slightly modified compared to the\none given in the article. See `OAS` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "covariance.OAS()", "path": "modules/generated/sklearn.covariance.oas", "type": "covariance", "text": "\nOracle Approximating Shrinkage Estimator\n\nRead more in the User Guide.\n\nOAS is a particular form of shrinkage described in \u201cShrinkage Algorithms for\nMMSE Covariance Estimation\u201d Chen et al., IEEE Trans. on Sign. Proc., Volume\n58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the article. In\nthe original article, formula (23) states that 2/p is multiplied by\nTrace(cov*cov) in both the numerator and denominator, but this operation is\nomitted because for a large p, the value of 2/p is so small tha