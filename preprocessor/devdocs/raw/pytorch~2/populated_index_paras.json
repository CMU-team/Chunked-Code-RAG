[{"name": "torch.__config__", "path": "config_mod", "type": "Miscellaneous", "text": ["Return a human-readable string with descriptions of the configuration of PyTorch.", "Returns detailed string with parallelization settings"]}, {"name": "torch.__config__.parallel_info()", "path": "config_mod#torch.__config__.parallel_info", "type": "Miscellaneous", "text": ["Returns detailed string with parallelization settings"]}, {"name": "torch.__config__.show()", "path": "config_mod#torch.__config__.show", "type": "Miscellaneous", "text": ["Return a human-readable string with descriptions of the configuration of PyTorch."]}, {"name": "torch._assert", "path": "generated/torch._assert", "type": "Torch", "text": ["A wrapper around Python\u2019s assert which is symbolically traceable."]}, {"name": "torch._foreach_abs", "path": "generated/torch._foreach_abs", "type": "Torch", "text": ["Apply torch.abs() to each Tensor of the input list."]}, {"name": "torch._foreach_abs_", "path": "generated/torch._foreach_abs_", "type": "Torch", "text": ["Apply torch.abs() to each Tensor of the input list."]}, {"name": "torch._foreach_acos", "path": "generated/torch._foreach_acos", "type": "Torch", "text": ["Apply torch.acos() to each Tensor of the input list."]}, {"name": "torch._foreach_acos_", "path": "generated/torch._foreach_acos_", "type": "Torch", "text": ["Apply torch.acos() to each Tensor of the input list."]}, {"name": "torch._foreach_asin", "path": "generated/torch._foreach_asin", "type": "Torch", "text": ["Apply torch.asin() to each Tensor of the input list."]}, {"name": "torch._foreach_asin_", "path": "generated/torch._foreach_asin_", "type": "Torch", "text": ["Apply torch.asin() to each Tensor of the input list."]}, {"name": "torch._foreach_atan", "path": "generated/torch._foreach_atan", "type": "Torch", "text": ["Apply torch.atan() to each Tensor of the input list."]}, {"name": "torch._foreach_atan_", "path": "generated/torch._foreach_atan_", "type": "Torch", "text": ["Apply torch.atan() to each Tensor of the input list."]}, {"name": "torch._foreach_ceil", "path": "generated/torch._foreach_ceil", "type": "Torch", "text": ["Apply torch.ceil() to each Tensor of the input list."]}, {"name": "torch._foreach_ceil_", "path": "generated/torch._foreach_ceil_", "type": "Torch", "text": ["Apply torch.ceil() to each Tensor of the input list."]}, {"name": "torch._foreach_cos", "path": "generated/torch._foreach_cos", "type": "Torch", "text": ["Apply torch.cos() to each Tensor of the input list."]}, {"name": "torch._foreach_cos_", "path": "generated/torch._foreach_cos_", "type": "Torch", "text": ["Apply torch.cos() to each Tensor of the input list."]}, {"name": "torch._foreach_cosh", "path": "generated/torch._foreach_cosh", "type": "Torch", "text": ["Apply torch.cosh() to each Tensor of the input list."]}, {"name": "torch._foreach_cosh_", "path": "generated/torch._foreach_cosh_", "type": "Torch", "text": ["Apply torch.cosh() to each Tensor of the input list."]}, {"name": "torch._foreach_erf", "path": "generated/torch._foreach_erf", "type": "Torch", "text": ["Apply torch.erf() to each Tensor of the input list."]}, {"name": "torch._foreach_erf_", "path": "generated/torch._foreach_erf_", "type": "Torch", "text": ["Apply torch.erf() to each Tensor of the input list."]}, {"name": "torch._foreach_erfc", "path": "generated/torch._foreach_erfc", "type": "Torch", "text": ["Apply torch.erfc() to each Tensor of the input list."]}, {"name": "torch._foreach_erfc_", "path": "generated/torch._foreach_erfc_", "type": "Torch", "text": ["Apply torch.erfc() to each Tensor of the input list."]}, {"name": "torch._foreach_exp", "path": "generated/torch._foreach_exp", "type": "Torch", "text": ["Apply torch.exp() to each Tensor of the input list."]}, {"name": "torch._foreach_exp_", "path": "generated/torch._foreach_exp_", "type": "Torch", "text": ["Apply torch.exp() to each Tensor of the input list."]}, {"name": "torch._foreach_expm1", "path": "generated/torch._foreach_expm1", "type": "Torch", "text": ["Apply torch.expm1() to each Tensor of the input list."]}, {"name": "torch._foreach_expm1_", "path": "generated/torch._foreach_expm1_", "type": "Torch", "text": ["Apply torch.expm1() to each Tensor of the input list."]}, {"name": "torch._foreach_floor", "path": "generated/torch._foreach_floor", "type": "Torch", "text": ["Apply torch.floor() to each Tensor of the input list."]}, {"name": "torch._foreach_floor_", "path": "generated/torch._foreach_floor_", "type": "Torch", "text": ["Apply torch.floor() to each Tensor of the input list."]}, {"name": "torch._foreach_frac", "path": "generated/torch._foreach_frac", "type": "Torch", "text": ["Apply torch.frac() to each Tensor of the input list."]}, {"name": "torch._foreach_frac_", "path": "generated/torch._foreach_frac_", "type": "Torch", "text": ["Apply torch.frac() to each Tensor of the input list."]}, {"name": "torch._foreach_lgamma", "path": "generated/torch._foreach_lgamma", "type": "Torch", "text": ["Apply torch.lgamma() to each Tensor of the input list."]}, {"name": "torch._foreach_lgamma_", "path": "generated/torch._foreach_lgamma_", "type": "Torch", "text": ["Apply torch.lgamma() to each Tensor of the input list."]}, {"name": "torch._foreach_log", "path": "generated/torch._foreach_log", "type": "Torch", "text": ["Apply torch.log() to each Tensor of the input list."]}, {"name": "torch._foreach_log10", "path": "generated/torch._foreach_log10", "type": "Torch", "text": ["Apply torch.log10() to each Tensor of the input list."]}, {"name": "torch._foreach_log10_", "path": "generated/torch._foreach_log10_", "type": "Torch", "text": ["Apply torch.log10() to each Tensor of the input list."]}, {"name": "torch._foreach_log1p", "path": "generated/torch._foreach_log1p", "type": "Torch", "text": ["Apply torch.log1p() to each Tensor of the input list."]}, {"name": "torch._foreach_log1p_", "path": "generated/torch._foreach_log1p_", "type": "Torch", "text": ["Apply torch.log1p() to each Tensor of the input list."]}, {"name": "torch._foreach_log2", "path": "generated/torch._foreach_log2", "type": "Torch", "text": ["Apply torch.log2() to each Tensor of the input list."]}, {"name": "torch._foreach_log2_", "path": "generated/torch._foreach_log2_", "type": "Torch", "text": ["Apply torch.log2() to each Tensor of the input list."]}, {"name": "torch._foreach_log_", "path": "generated/torch._foreach_log_", "type": "Torch", "text": ["Apply torch.log() to each Tensor of the input list."]}, {"name": "torch._foreach_neg", "path": "generated/torch._foreach_neg", "type": "Torch", "text": ["Apply torch.neg() to each Tensor of the input list."]}, {"name": "torch._foreach_neg_", "path": "generated/torch._foreach_neg_", "type": "Torch", "text": ["Apply torch.neg() to each Tensor of the input list."]}, {"name": "torch._foreach_reciprocal", "path": "generated/torch._foreach_reciprocal", "type": "Torch", "text": ["Apply torch.reciprocal() to each Tensor of the input list."]}, {"name": "torch._foreach_reciprocal_", "path": "generated/torch._foreach_reciprocal_", "type": "Torch", "text": ["Apply torch.reciprocal() to each Tensor of the input list."]}, {"name": "torch._foreach_round", "path": "generated/torch._foreach_round", "type": "Torch", "text": ["Apply torch.round() to each Tensor of the input list."]}, {"name": "torch._foreach_round_", "path": "generated/torch._foreach_round_", "type": "Torch", "text": ["Apply torch.round() to each Tensor of the input list."]}, {"name": "torch._foreach_sigmoid", "path": "generated/torch._foreach_sigmoid", "type": "Torch", "text": ["Apply torch.sigmoid() to each Tensor of the input list."]}, {"name": "torch._foreach_sigmoid_", "path": "generated/torch._foreach_sigmoid_", "type": "Torch", "text": ["Apply torch.sigmoid() to each Tensor of the input list."]}, {"name": "torch._foreach_sin", "path": "generated/torch._foreach_sin", "type": "Torch", "text": ["Apply torch.sin() to each Tensor of the input list."]}, {"name": "torch._foreach_sin_", "path": "generated/torch._foreach_sin_", "type": "Torch", "text": ["Apply torch.sin() to each Tensor of the input list."]}, {"name": "torch._foreach_sinh", "path": "generated/torch._foreach_sinh", "type": "Torch", "text": ["Apply torch.sinh() to each Tensor of the input list."]}, {"name": "torch._foreach_sinh_", "path": "generated/torch._foreach_sinh_", "type": "Torch", "text": ["Apply torch.sinh() to each Tensor of the input list."]}, {"name": "torch._foreach_sqrt", "path": "generated/torch._foreach_sqrt", "type": "Torch", "text": ["Apply torch.sqrt() to each Tensor of the input list."]}, {"name": "torch._foreach_sqrt_", "path": "generated/torch._foreach_sqrt_", "type": "Torch", "text": ["Apply torch.sqrt() to each Tensor of the input list."]}, {"name": "torch._foreach_tan", "path": "generated/torch._foreach_tan", "type": "Torch", "text": ["Apply torch.tan() to each Tensor of the input list."]}, {"name": "torch._foreach_tan_", "path": "generated/torch._foreach_tan_", "type": "Torch", "text": ["Apply torch.tan() to each Tensor of the input list."]}, {"name": "torch._foreach_trunc", "path": "generated/torch._foreach_trunc", "type": "Torch", "text": ["Apply torch.trunc() to each Tensor of the input list."]}, {"name": "torch._foreach_trunc_", "path": "generated/torch._foreach_trunc_", "type": "Torch", "text": ["Apply torch.trunc() to each Tensor of the input list."]}, {"name": "torch._foreach_zero_", "path": "generated/torch._foreach_zero_", "type": "Torch", "text": ["Apply torch.zero() to each Tensor of the input list."]}, {"name": "torch._logging", "path": "logging", "type": "Miscellaneous", "text": ["PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component\u2019s log messages can be completely disabled, while another component\u2019s log messages can be set to maximum verbosity.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Warning", "This feature has not been expanded to control the log messages of all components in PyTorch yet.", "There are two ways to configure the logging system: through the environment variable TORCH_LOGS or the python API torch._logging.set_logs.", "Sets the log level for individual components and toggles individual log artifact types.", "The environment variable TORCH_LOGS is a comma-separated list of [+-]<component> pairs, where <component> is a component specified below. The + prefix will decrease the log level of the component, displaying more log messages while the - prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in TORCH_LOGS. In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with + or - will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be off_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the TORCH_LOGS environment variable (see torch._logging.set_logs for the python API):", "Special component which configures the default log level of all components. Default: logging.WARN", "The log level for the TorchDynamo component. Default: logging.WARN", "The log level for the AOTAutograd component. Default: logging.WARN", "The log level for the TorchInductor component. Default: logging.WARN", "The log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default: logging.WARN", "Whether to emit the original and generated bytecode from TorchDynamo. Default: False", "Whether to emit the graphs generated by AOTAutograd. Default: False", "Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: False", "Whether to emit graphs generated by DDPOptimizer. Default: False", "Whether to emit the graph captured by TorchDynamo in tabular format. Default: False", "Whether to emit the python source of the graph captured by TorchDynamo. Default: False", "Whether to emit a message when a unique graph break is encountered during TorchDynamo tracing. Default: False", "Whether to emit the guards generated by TorchDynamo for each compiled function. Default: False", "Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: False", "Whether to emit the TorchInductor output code. Default: False", "Whether to emit the TorchInductor schedule. Default: False", "TORCH_LOGS=\"+dynamo,aot\" will set the log level of TorchDynamo to logging.DEBUG and AOT to logging.INFO", "TORCH_LOGS=\"-dynamo,+inductor\" will set the log level of TorchDynamo to logging.ERROR and TorchInductor to logging.DEBUG", "TORCH_LOGS=\"aot_graphs\" will enable the aot_graphs artifact", "TORCH_LOGS=\"+dynamo,schedule\" will enable set the log level of TorchDynamo to logging.DEBUG and enable the schedule artifact", "TORCH_LOGS=\"+some.random.module,schedule\" will set the log level of some.random.module to logging.DEBUG and enable the schedule artifact"]}, {"name": "torch._logging.set_logs()", "path": "generated/torch._logging.set_logs#torch._logging.set_logs", "type": "Miscellaneous", "text": ["Sets the log level for individual components and toggles individual log artifact types.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Note", "The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.", "A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:", "See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels", "An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.", "Example:"]}, {"name": "torch._logging.torch._logging.set_logs", "path": "generated/torch._logging.set_logs", "type": "Miscellaneous", "text": ["Sets the log level for individual components and toggles individual log artifact types.", "Warning", "This feature is a prototype and may have compatibility breaking changes in the future.", "Note", "The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.", "A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:", "See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels", "An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.", "Example:"]}, {"name": "torch.abs", "path": "generated/torch.abs", "type": "Torch", "text": ["Computes the absolute value of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.absolute", "path": "generated/torch.absolute", "type": "Torch", "text": ["Alias for torch.abs()"]}, {"name": "torch.acos", "path": "generated/torch.acos", "type": "Torch", "text": ["Computes the inverse cosine of each element in input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.acosh", "path": "generated/torch.acosh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic cosine of the elements of input.", "Note", "The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.add", "path": "generated/torch.add", "type": "Torch", "text": ["Adds other, scaled by alpha, to input.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.", "Examples:"]}, {"name": "torch.addbmm", "path": "generated/torch.addbmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.addcdiv", "path": "generated/torch.addcdiv", "type": "Torch", "text": ["Performs the element-wise division of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.", "Warning", "Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.", "The shapes of input, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addcmul", "path": "generated/torch.addcmul", "type": "Torch", "text": ["Performs the element-wise multiplication of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.", "The shapes of tensor, tensor1, and tensor2 must be broadcastable.", "For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.", "Example:"]}, {"name": "torch.addmm", "path": "generated/torch.addmm", "type": "Torch", "text": ["Performs a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result.", "If mat1 is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor.", "alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operation has support for arguments with sparse layouts. If input is sparse the result will have the same layout and if out is provided it must have the same layout as input.", "Warning", "Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.addmv", "path": "generated/torch.addmv", "type": "Torch", "text": ["Performs a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result.", "If mat is a (n\u00d7m)(n \\times m) tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n.", "alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "Example:"]}, {"name": "torch.addr", "path": "generated/torch.addr", "type": "Torch", "text": ["Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.", "Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m) and out will be a matrix of size (n\u00d7m)(n \\times m).", "Example:"]}, {"name": "torch.adjoint", "path": "generated/torch.adjoint", "type": "Torch", "text": ["Returns a view of the tensor conjugated and with the last two dimensions transposed.", "x.adjoint() is equivalent to x.transpose(-2, -1).conj() for complex tensors and to x.transpose(-2, -1) for real tensors."]}, {"name": "torch.all", "path": "generated/torch.all", "type": "Torch", "text": ["Tests if all elements in input evaluate to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.allclose", "path": "generated/torch.allclose", "type": "Torch", "text": ["This function checks if input and other satisfy the condition:", "elementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose", "Example:"]}, {"name": "torch.amax", "path": "generated/torch.amax", "type": "Torch", "text": ["Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.amin", "path": "generated/torch.amin", "type": "Torch", "text": ["Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.", "Note", "If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.aminmax", "path": "generated/torch.aminmax", "type": "Torch", "text": ["Computes the minimum and maximum values of the input tensor.", "input (Tensor) \u2013 The input tensor", "A named tuple (min, max) containing the minimum and maximum values.", "RuntimeError \u2013 If any of the dimensions to compute the values over has size 0.", "Note", "NaN values are propagated to the output if at least one value is NaN.", "See also", "torch.amin() computes just the minimum value torch.amax() computes just the maximum value", "Example:"]}, {"name": "torch.angle", "path": "generated/torch.angle", "type": "Torch", "text": ["Computes the element-wise angle (in radians) of the given input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Note", "Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.", "Example:"]}, {"name": "torch.any", "path": "generated/torch.any", "type": "Torch", "text": ["Tests if any element in input evaluates to True.", "Note", "This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.", "Example:", "For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise.", "If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.ao.nn.intrinsic.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.bnrelu2d#torch.ao.nn.intrinsic.BNReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the BatchNorm 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.bnrelu3d#torch.ao.nn.intrinsic.BNReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the BatchNorm 3d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.convbn1d#torch.ao.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.convbn2d#torch.ao.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.convbn3d#torch.ao.nn.intrinsic.ConvBn3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 3d and Batch Norm 3d modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu1d#torch.ao.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu2d#torch.ao.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu3d#torch.ao.nn.intrinsic.ConvBnReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.convrelu1d#torch.ao.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv1d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.convrelu2d#torch.ao.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv2d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.convrelu3d#torch.ao.nn.intrinsic.ConvReLU3d", "type": "Quantization", "text": ["This is a sequential container which calls the Conv3d and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.linearrelu#torch.ao.nn.intrinsic.LinearReLU", "type": "Quantization", "text": ["This is a sequential container which calls the Linear and ReLU modules. During quantization this will be replaced with the corresponding fused module."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn1d#torch.ao.nn.intrinsic.qat.ConvBn1d", "type": "Quantization", "text": ["A ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d.", "Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn2d#torch.ao.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": ["A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn3d#torch.ao.nn.intrinsic.qat.ConvBn3d", "type": "Quantization", "text": ["A ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu1d#torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "type": "Quantization", "text": ["A ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d and torch.nn.ReLU.", "Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu2d#torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": ["A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu3d#torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "type": "Quantization", "text": ["A ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training.", "We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d and torch.nn.ReLU.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu2d#torch.ao.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.", "We combined the interface of Conv2d and BatchNorm2d.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu3d#torch.ao.nn.intrinsic.qat.ConvReLU3d", "type": "Quantization", "text": ["A ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training.", "We combined the interface of Conv3d and BatchNorm3d.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.intrinsic.qat.freeze_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats#torch.ao.nn.intrinsic.qat.freeze_bn_stats", "type": "Quantization", "text": []}, {"name": "torch.ao.nn.intrinsic.qat.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.qat.linearrelu#torch.ao.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training.", "We adopt the same interface as torch.nn.Linear.", "Similar to torch.ao.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.", "weight (torch.Tensor) \u2013 fake quant module for weight", "Examples:"]}, {"name": "torch.ao.nn.intrinsic.qat.update_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.update_bn_stats#torch.ao.nn.intrinsic.qat.update_bn_stats", "type": "Quantization", "text": []}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu2d#torch.ao.nn.intrinsic.quantized.BNReLU2d", "type": "Quantization", "text": ["A BNReLU2d module is a fused module of BatchNorm2d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.BatchNorm2d.", "torch.ao.nn.quantized.BatchNorm2d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu3d#torch.ao.nn.intrinsic.quantized.BNReLU3d", "type": "Quantization", "text": ["A BNReLU3d module is a fused module of BatchNorm3d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.BatchNorm3d.", "torch.ao.nn.quantized.BatchNorm3d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu1d#torch.ao.nn.intrinsic.quantized.ConvReLU1d", "type": "Quantization", "text": ["A ConvReLU1d module is a fused module of Conv1d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv1d.", "torch.ao.nn.quantized.Conv1d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu2d#torch.ao.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": ["A ConvReLU2d module is a fused module of Conv2d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv2d.", "torch.ao.nn.quantized.Conv2d (Same as) \u2013 "]}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu3d#torch.ao.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": ["A ConvReLU3d module is a fused module of Conv3d and ReLU", "We adopt the same interface as torch.ao.nn.quantized.Conv3d.", "Attributes: Same as torch.ao.nn.quantized.Conv3d"]}, {"name": "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.dynamic.linearrelu#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization. Supports both, FP16 and INT8 quantization.", "We adopt the same interface as torch.ao.nn.quantized.dynamic.Linear.", "torch.ao.nn.quantized.dynamic.Linear (Same as) \u2013 ", "Examples:"]}, {"name": "torch.ao.nn.intrinsic.quantized.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.linearrelu#torch.ao.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": ["A LinearReLU module fused from Linear and ReLU modules", "We adopt the same interface as torch.ao.nn.quantized.Linear.", "torch.ao.nn.quantized.Linear (Same as) \u2013 ", "Examples:"]}, {"name": "torch.ao.nn.qat.Conv2d", "path": "generated/torch.ao.nn.qat.conv2d#torch.ao.nn.qat.Conv2d", "type": "Quantization", "text": ["A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation.", "Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.qat.Conv3d", "path": "generated/torch.ao.nn.qat.conv3d#torch.ao.nn.qat.Conv3d", "type": "Quantization", "text": ["A Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Conv3d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv3d#torch.nn.Conv3d for documentation.", "Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.", "weight_fake_quant \u2013 fake quant module for weight"]}, {"name": "torch.ao.nn.qat.dynamic.Linear", "path": "generated/torch.ao.nn.qat.dynamic.linear#torch.ao.nn.qat.dynamic.Linear", "type": "Quantization", "text": ["A linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training.", "We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, with FakeQuantize modules initialized to default."]}, {"name": "torch.ao.nn.qat.Linear", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear", "type": "Quantization", "text": ["A linear module attached with FakeQuantize modules for weight, used for quantization aware training.", "We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.", "weight (torch.Tensor) \u2013 fake quant module for weight", "Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user"]}, {"name": "torch.ao.nn.qat.Linear.from_float()", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear.from_float", "type": "Quantization", "text": ["Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user"]}, {"name": "torch.ao.nn.quantizable.LSTM", "path": "generated/torch.ao.nn.quantizable.lstm#torch.ao.nn.quantizable.LSTM", "type": "Quantization", "text": ["A quantizable long short-term memory (LSTM).", "For the description and the argument types, please, refer to LSTM", "layers \u2013 instances of the _LSTMLayer", "Note", "To access the weights and biases, you need to access them per layer. See examples below.", "Examples:"]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention", "type": "Quantization", "text": ["Utility to convert the quantized MHA back to float.", "The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float.", "Please, refer to forward() for more information", "Tuple[Tensor, Optional[Tensor]]"]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.dequantize()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.dequantize", "type": "Quantization", "text": ["Utility to convert the quantized MHA back to float.", "The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float."]}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.forward()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.forward", "type": "Quantization", "text": ["Please, refer to forward() for more information", "Tuple[Tensor, Optional[Tensor]]"]}, {"name": "torch.ao.nn.quantized.BatchNorm2d", "path": "generated/torch.ao.nn.quantized.batchnorm2d#torch.ao.nn.quantized.BatchNorm2d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm2d."]}, {"name": "torch.ao.nn.quantized.BatchNorm3d", "path": "generated/torch.ao.nn.quantized.batchnorm3d#torch.ao.nn.quantized.BatchNorm3d", "type": "Quantization", "text": ["This is the quantized version of BatchNorm3d."]}, {"name": "torch.ao.nn.quantized.Conv1d", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv1d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv1d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv1d.from_float()", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv2d", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv2d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv2d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv2d.from_float()", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv3d", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized input signal composed of several quantized input planes.", "For details on input arguments, parameters, and implementation see Conv3d.", "Note", "Only zeros is supported for the padding_mode argument.", "Note", "Only torch.quint8 is supported for the input data type.", "See Conv3d for other attributes.", "Examples:", "Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Conv3d.from_float()", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": ["Creates a quantized module from a float module or qparams_dict.", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.ConvTranspose1d", "path": "generated/torch.ao.nn.quantized.convtranspose1d#torch.ao.nn.quantized.ConvTranspose1d", "type": "Quantization", "text": ["Applies a 1D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose1d.", "Note", "Currently only the QNNPACK engine is implemented. Please, set the torch.backends.quantized.engine = \u2018qnnpack\u2019", "For special notes, please, see Conv1d", "See ConvTranspose2d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ConvTranspose2d", "path": "generated/torch.ao.nn.quantized.convtranspose2d#torch.ao.nn.quantized.ConvTranspose2d", "type": "Quantization", "text": ["Applies a 2D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose2d.", "For special notes, please, see Conv2d", "See ConvTranspose2d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ConvTranspose3d", "path": "generated/torch.ao.nn.quantized.convtranspose3d#torch.ao.nn.quantized.ConvTranspose3d", "type": "Quantization", "text": ["Applies a 3D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose3d.", "Note", "Currently only the FBGEMM engine is implemented. Please, set the torch.backends.quantized.engine = \u2018fbgemm\u2019", "For special notes, please, see Conv3d", "See ConvTranspose3d for other attributes.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.GRU", "path": "generated/torch.ao.nn.quantized.dynamic.gru#torch.ao.nn.quantized.dynamic.GRU", "type": "Quantization", "text": ["Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.", "For each element in the input sequence, each layer computes the following function:", "where hth_t is the hidden state at time t, xtx_t is the input at time t, h(t\u22121)h_{(t-1)} is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t, ztz_t, ntn_t are the reset, update, and new gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.", "In a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a Bernoulli random variable which is 00 with probability dropout.", "output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively.", "Similarly, the directions can be separated in the packed case.", "h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len", "Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).", "Note", "All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}", "Note", "The calculation of new gate ntn_t subtly differs from the original paper and other frameworks. In the original implementation, the Hadamard product (\u2217)(*) between rtr_t and the previous hidden state h(t\u22121)h_{(t-1)} is done before the multiplication with the weight matrix W and addition of bias:", "This is in contrast to PyTorch implementation, which is done after Whnh(t\u22121)W_{hn} h_{(t-1)}", "This implementation differs on purpose for efficiency.", "Note", "If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.GRUCell", "path": "generated/torch.ao.nn.quantized.dynamic.grucell#torch.ao.nn.quantized.dynamic.GRUCell", "type": "Quantization", "text": ["A gated recurrent unit (GRU) cell", "A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear", "type": "Quantization", "text": ["A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user", "Create a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user:"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_float", "type": "Quantization", "text": ["Create a dynamic quantized module from a float module or qparams_dict", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_reference", "type": "Quantization", "text": ["Create a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user:"]}, {"name": "torch.ao.nn.quantized.dynamic.LSTM", "path": "generated/torch.ao.nn.quantized.dynamic.lstm#torch.ao.nn.quantized.dynamic.LSTM", "type": "Quantization", "text": ["A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.LSTMCell", "path": "generated/torch.ao.nn.quantized.dynamic.lstmcell#torch.ao.nn.quantized.dynamic.LSTMCell", "type": "Quantization", "text": ["A long short-term memory (LSTM) cell.", "A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.dynamic.RNNCell", "path": "generated/torch.ao.nn.quantized.dynamic.rnncell#torch.ao.nn.quantized.dynamic.RNNCell", "type": "Quantization", "text": ["An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.", "Examples:"]}, {"name": "torch.ao.nn.quantized.ELU", "path": "generated/torch.ao.nn.quantized.elu#torch.ao.nn.quantized.ELU", "type": "Quantization", "text": ["This is the quantized equivalent of ELU."]}, {"name": "torch.ao.nn.quantized.Embedding", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding", "type": "Quantization", "text": ["A quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as torch.nn.Embedding, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation.", "Similar to Embedding, attributes will be randomly initialized at module creation time and will be overwritten later", "weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).", "Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.Embedding.from_float()", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding.from_float", "type": "Quantization", "text": ["Create a quantized embedding module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.EmbeddingBag", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag", "type": "Quantization", "text": ["A quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as torch.nn.EmbeddingBag, please see https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation.", "Similar to EmbeddingBag, attributes will be randomly initialized at module creation time and will be overwritten later", "weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).", "Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.EmbeddingBag.from_float()", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": ["Create a quantized embedding_bag module from a float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user"]}, {"name": "torch.ao.nn.quantized.FloatFunctional", "path": "generated/torch.ao.nn.quantized.floatfunctional#torch.ao.nn.quantized.FloatFunctional", "type": "Quantization", "text": ["State collector class for float operations.", "The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d#torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": ["Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters propagate to the output.", "See AdaptiveAvgPool2d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d#torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "type": "Quantization", "text": ["Applies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters propagate to the output.", "See AdaptiveAvgPool3d for details and output shape.", "output_size (None) \u2013 the target output size (single integer or double-integer tuple)", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool2d#torch.ao.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": ["Applies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps. The number of output features is equal to the number of input planes.", "Note", "The input quantization parameters propagate to the output.", "See AvgPool2d for details and output shape."]}, {"name": "torch.ao.nn.quantized.functional.avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool3d#torch.ao.nn.quantized.functional.avg_pool3d", "type": "Quantization", "text": ["Applies 3D average-pooling operation in kDtimeskH\u00d7kWkD \\ times kH \\times kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sW steps. The number of output features is equal to the number of input planes.", "Note", "The input quantization parameters propagate to the output."]}, {"name": "torch.ao.nn.quantized.functional.celu", "path": "generated/torch.ao.nn.quantized.functional.celu#torch.ao.nn.quantized.functional.celu", "type": "Quantization", "text": ["Applies the quantized CELU function element-wise.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.clamp", "path": "generated/torch.ao.nn.quantized.functional.clamp#torch.ao.nn.quantized.functional.clamp", "type": "Quantization", "text": ["float(input, min_, max_) -> Tensor", "Applies the clamp function element-wise. See clamp for more details.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.conv1d", "path": "generated/torch.ao.nn.quantized.functional.conv1d#torch.ao.nn.quantized.functional.conv1d", "type": "Quantization", "text": ["Applies a 1D convolution over a quantized 1D input composed of several input planes.", "See Conv1d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.conv2d", "path": "generated/torch.ao.nn.quantized.functional.conv2d#torch.ao.nn.quantized.functional.conv2d", "type": "Quantization", "text": ["Applies a 2D convolution over a quantized 2D input composed of several input planes.", "See Conv2d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.conv3d", "path": "generated/torch.ao.nn.quantized.functional.conv3d#torch.ao.nn.quantized.functional.conv3d", "type": "Quantization", "text": ["Applies a 3D convolution over a quantized 3D input composed of several input planes.", "See Conv3d for details and output shape.", "Examples:"]}, {"name": "torch.ao.nn.quantized.functional.elu", "path": "generated/torch.ao.nn.quantized.functional.elu#torch.ao.nn.quantized.functional.elu", "type": "Quantization", "text": ["This is the quantized version of elu().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardsigmoid", "path": "generated/torch.ao.nn.quantized.functional.hardsigmoid#torch.ao.nn.quantized.functional.hardsigmoid", "type": "Quantization", "text": ["This is the quantized version of hardsigmoid().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardswish", "path": "generated/torch.ao.nn.quantized.functional.hardswish#torch.ao.nn.quantized.functional.hardswish", "type": "Quantization", "text": ["This is the quantized version of hardswish().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.hardtanh", "path": "generated/torch.ao.nn.quantized.functional.hardtanh#torch.ao.nn.quantized.functional.hardtanh", "type": "Quantization", "text": ["This is the quantized version of hardtanh().", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.interpolate", "path": "generated/torch.ao.nn.quantized.functional.interpolate#torch.ao.nn.quantized.functional.interpolate", "type": "Quantization", "text": ["Down/up samples the input to either the given size or the given scale_factor", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D/3D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:"]}, {"name": "torch.ao.nn.quantized.functional.leaky_relu", "path": "generated/torch.ao.nn.quantized.functional.leaky_relu#torch.ao.nn.quantized.functional.leaky_relu", "type": "Quantization", "text": ["Quantized version of the. leaky_relu(input, negative_slope=0.01, inplace=False, scale, zero_point) -> Tensor", "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)", "See LeakyReLU for more details."]}, {"name": "torch.ao.nn.quantized.functional.linear", "path": "generated/torch.ao.nn.quantized.functional.linear#torch.ao.nn.quantized.functional.linear", "type": "Quantization", "text": ["Applies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + b. See Linear", "Note", "Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.max_pool1d", "path": "generated/torch.ao.nn.quantized.functional.max_pool1d#torch.ao.nn.quantized.functional.max_pool1d", "type": "Quantization", "text": ["Applies a 1D max pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters are propagated to the output.", "See MaxPool1d for details."]}, {"name": "torch.ao.nn.quantized.functional.max_pool2d", "path": "generated/torch.ao.nn.quantized.functional.max_pool2d#torch.ao.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": ["Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.", "Note", "The input quantization parameters are propagated to the output.", "See MaxPool2d for details."]}, {"name": "torch.ao.nn.quantized.functional.threshold", "path": "generated/torch.ao.nn.quantized.functional.threshold#torch.ao.nn.quantized.functional.threshold", "type": "Quantization", "text": ["Applies the quantized version of the threshold function element-wise:", "See Threshold for more details.", "Tensor"]}, {"name": "torch.ao.nn.quantized.functional.upsample", "path": "generated/torch.ao.nn.quantized.functional.upsample#torch.ao.nn.quantized.functional.upsample", "type": "Quantization", "text": ["Upsamples the input to either the given size or the given scale_factor", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).", "See torch.nn.functional.interpolate() for implementation details.", "The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D input is supported for quantized inputs", "Note", "Only the following modes are supported for the quantized inputs:", "Warning", "With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs."]}, {"name": "torch.ao.nn.quantized.functional.upsample_bilinear", "path": "generated/torch.ao.nn.quantized.functional.upsample_bilinear#torch.ao.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": ["Upsamples the input, using bilinear upsampling.", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.ao.nn.quantized.functional.upsample_nearest", "path": "generated/torch.ao.nn.quantized.functional.upsample_nearest#torch.ao.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": ["Upsamples the input, using nearest neighbours\u2019 pixel values.", "Warning", "This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').", "Note", "The input quantization parameters propagate to the output.", "Note", "Only 2D inputs are supported"]}, {"name": "torch.ao.nn.quantized.FXFloatFunctional", "path": "generated/torch.ao.nn.quantized.fxfloatfunctional#torch.ao.nn.quantized.FXFloatFunctional", "type": "Quantization", "text": ["module to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly"]}, {"name": "torch.ao.nn.quantized.GroupNorm", "path": "generated/torch.ao.nn.quantized.groupnorm#torch.ao.nn.quantized.GroupNorm", "type": "Quantization", "text": ["This is the quantized version of GroupNorm."]}, {"name": "torch.ao.nn.quantized.Hardswish", "path": "generated/torch.ao.nn.quantized.hardswish#torch.ao.nn.quantized.Hardswish", "type": "Quantization", "text": ["This is the quantized version of Hardswish."]}, {"name": "torch.ao.nn.quantized.InstanceNorm1d", "path": "generated/torch.ao.nn.quantized.instancenorm1d#torch.ao.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm1d."]}, {"name": "torch.ao.nn.quantized.InstanceNorm2d", "path": "generated/torch.ao.nn.quantized.instancenorm2d#torch.ao.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm2d."]}, {"name": "torch.ao.nn.quantized.InstanceNorm3d", "path": "generated/torch.ao.nn.quantized.instancenorm3d#torch.ao.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": ["This is the quantized version of InstanceNorm3d."]}, {"name": "torch.ao.nn.quantized.LayerNorm", "path": "generated/torch.ao.nn.quantized.layernorm#torch.ao.nn.quantized.LayerNorm", "type": "Quantization", "text": ["This is the quantized version of LayerNorm."]}, {"name": "torch.ao.nn.quantized.LeakyReLU", "path": "generated/torch.ao.nn.quantized.leakyrelu#torch.ao.nn.quantized.LeakyReLU", "type": "Quantization", "text": ["This is the quantized equivalent of LeakyReLU."]}, {"name": "torch.ao.nn.quantized.Linear", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear", "type": "Quantization", "text": ["A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.", "Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later", "Examples:", "Create a quantized module from an observed float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user", "Create a (fbgemm/qnnpack) quantized module from a reference quantized module"]}, {"name": "torch.ao.nn.quantized.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_float", "type": "Quantization", "text": ["Create a quantized module from an observed float module", "mod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user"]}, {"name": "torch.ao.nn.quantized.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_reference", "type": "Quantization", "text": ["Create a (fbgemm/qnnpack) quantized module from a reference quantized module"]}, {"name": "torch.ao.nn.quantized.QFunctional", "path": "generated/torch.ao.nn.quantized.qfunctional#torch.ao.nn.quantized.QFunctional", "type": "Quantization", "text": ["Wrapper class for quantized operations.", "The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.", "Note", "This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).", "Examples:"]}, {"name": "torch.ao.nn.quantized.ReLU6", "path": "generated/torch.ao.nn.quantized.relu6#torch.ao.nn.quantized.ReLU6", "type": "Quantization", "text": ["Applies the element-wise function:", "ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)), where x0x_0 is the zero_point, and q(6)q(6) is the quantized representation of number 6.", "inplace (bool) \u2013 can optionally do the operation in-place. Default: False", "Examples:"]}, {"name": "torch.ao.nn.quantized.Sigmoid", "path": "generated/torch.ao.nn.quantized.sigmoid#torch.ao.nn.quantized.Sigmoid", "type": "Quantization", "text": ["This is the quantized equivalent of Sigmoid."]}, {"name": "torch.ao.ns._numeric_suite.compare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_outputs", "type": "Quantization", "text": ["Compare output activations between float and quantized models at corresponding locations for the same input. Return a dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the activations of quantized model and float model at matching locations. This dict can be used to compare and compute the propagation quantization error.", "Example usage:", "dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations", "act_compare_dict"]}, {"name": "torch.ao.ns._numeric_suite.compare_model_stub()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_stub", "type": "Quantization", "text": ["Compare quantized module in a model with its floating point counterpart, feeding both of them the same input. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the output tensors of quantized and its matching float shadow module. This dict can be used to compare and compute the module level quantization error.", "This function first call prepare_model_with_stubs() to swap the quantized module that we want to compare with the Shadow module, which takes quantized module, corresponding float module and logger as input, and creates a forward path inside to make the float module to shadow quantized module sharing the same input. The logger can be customizable, default logger is ShadowLogger and it will save the outputs of the quantized module and float module that can be used to compute the module level quantization error.", "Example usage:", "Dict[str, Dict]"]}, {"name": "torch.ao.ns._numeric_suite.compare_weights()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_weights", "type": "Quantization", "text": ["Compare the weights of the float module with its corresponding quantized module. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights. This dict can be used to compare and compute the quantization error of the weights of float and quantized models.", "Example usage:", "dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights", "weight_dict"]}, {"name": "torch.ao.ns._numeric_suite.get_logger_dict()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_logger_dict", "type": "Quantization", "text": ["Traverse the modules and save all logger stats into target dict. This is mainly used for quantization accuracy debug.", "ShadowLogger: used to log the outputs of the quantized module and its matching float shadow module, OutputLogger: used to log the outputs of the modules", "the dictionary used to save all logger stats", "target_dict"]}, {"name": "torch.ao.ns._numeric_suite.get_matching_activations()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_matching_activations", "type": "Quantization", "text": ["Find the matching activation between float and quantized modules.", "dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations", "act_dict"]}, {"name": "torch.ao.ns._numeric_suite.Logger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger", "type": "Quantization", "text": ["Base class for stats logging"]}, {"name": "torch.ao.ns._numeric_suite.Logger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite.OutputLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger", "type": "Quantization", "text": ["Class used to log the outputs of the module"]}, {"name": "torch.ao.ns._numeric_suite.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite.prepare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_outputs", "type": "Quantization", "text": ["Prepare the model by attaching the logger to both float module and quantized module if they are in the allow_list."]}, {"name": "torch.ao.ns._numeric_suite.prepare_model_with_stubs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_with_stubs", "type": "Quantization", "text": ["Prepare the model by attaching the float module to its matching quantized module as the shadow if the float module type is in module_swap_list.", "Example usage:"]}, {"name": "torch.ao.ns._numeric_suite.Shadow", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow", "type": "Quantization", "text": ["Shadow module attaches the float module to its matching quantized module as the shadow. Then it uses Logger module to process the outputs of both modules.", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor", "Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_relu()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_relu", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_scalar", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.cat()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.cat", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.forward", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul_scalar", "type": "Quantization", "text": ["Tensor"]}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger", "type": "Quantization", "text": ["Class used in Shadow module to record the outputs of the original and shadow modules."]}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.add_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_loggers", "type": "Quantization", "text": ["Instrument model A and model B with loggers.", "Returns a tuple of (model_a_with_loggers, model_b_with_loggers). Modifies both models inplace.", "Tuple[Module, Module]"]}, {"name": "torch.ao.ns._numeric_suite_fx.add_shadow_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_shadow_loggers", "type": "Quantization", "text": ["Instrument model A and model B with shadow loggers."]}, {"name": "torch.ao.ns._numeric_suite_fx.convert_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.convert_n_shadows_model", "type": "Quantization", "text": ["Given a model from prepare_n_shadows_model, runs convert_fx on each shadow submodule.", "GraphModule"]}, {"name": "torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison", "type": "Quantization", "text": ["Compares the logged values from model_name_2 against the corresponding values in model_name_1, using comparison_fn. Records the result in model_name_2\u2019s results under comparison_name. Modifies results inplace."]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_logger_info", "type": "Quantization", "text": ["Traverse all loggers in model_a and model_b, and extract the logged information.", "NSResultsType, containing the logged comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model", "type": "Quantization", "text": ["Extracts logger results from model.", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info", "type": "Quantization", "text": ["Traverse all loggers in a shadow model, and extract the logged information.", "NSResultsType, containing the logged comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.extract_weights()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_weights", "type": "Quantization", "text": ["Extract weights from model A and model B, and return a comparison.", "NSResultsType, containing the weight comparisons", "Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]"]}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_enabled()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_enabled", "type": "Quantization", "text": ["Sets the enabled setting on a model\u2019s loggers"]}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_save_activations()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_save_activations", "type": "Quantization", "text": ["Sets the save_activations setting on a model\u2019s loggers"]}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer", "type": "Quantization", "text": ["Just like a regular FX quantization tracer, but treats observers and fake_quantize modules as leaf modules.", "bool"]}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module", "type": "Quantization", "text": ["bool"]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "type": "Quantization", "text": ["Same as OutputLogger, but also requires the original activation in order to calculate the comparison at calibration time"]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger", "type": "Quantization", "text": ["Base class for capturing intermediate values."]}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger.forward", "type": "Quantization", "text": []}, {"name": "torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model", "type": "Quantization", "text": ["Given a model with a graph with M ops such as", "args_kwargs_m -> op_m -> output_m", "And a set of N qconfigs for each op, creates a new model, with each of the subgraph of op_m transformed into", "Where op_m_n is op_m wrapped in a submodule and transformed with qconfig_n, and its inner graph looks like", "This is useful for testing different quantization of multiple layers in a single pass through the model.", "High level TODOs for future PRs: * figure out a better way to name the output structure * return a results data structure instead of printing it out * add examples to docblocks", "GraphModule"]}, {"name": "torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model", "type": "Quantization", "text": ["Prints a summary of extracted results."]}, {"name": "torch.ao.ns.fx.utils.compute_cosine_similarity()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_cosine_similarity", "type": "Quantization", "text": []}, {"name": "torch.ao.ns.fx.utils.compute_normalized_l2_error()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_normalized_l2_error", "type": "Quantization", "text": []}, {"name": "torch.ao.ns.fx.utils.compute_sqnr()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_sqnr", "type": "Quantization", "text": []}, {"name": "torch.ao.quantization.add_quant_dequant", "path": "generated/torch.ao.quantization.add_quant_dequant#torch.ao.quantization.add_quant_dequant", "type": "Quantization", "text": ["Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.", "Either the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it."]}, {"name": "torch.ao.quantization.backend_config.BackendConfig", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig", "type": "Quantization", "text": ["Config that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns.", "A pattern in this context refers to a module, a functional, an operator, or a directed acyclic graph of the above. Each pattern supported on the target backend can be individually configured through BackendPatternConfig in terms of:", "The format of the patterns is described in: https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/README.md", "Example usage:", "Return a copy of the list of configs set in this BackendConfig.", "Create a BackendConfig from a dictionary with the following items:", "\u201cname\u201d: the name of the target backend", "\u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig", "BackendConfig", "Set the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.", "BackendConfig", "Set the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.", "BackendConfig", "Set the name of the target backend.", "BackendConfig", "Convert this BackendConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.configs", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.configs", "type": "Quantization", "text": ["Return a copy of the list of configs set in this BackendConfig."]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.from_dict", "type": "Quantization", "text": ["Create a BackendConfig from a dictionary with the following items:", "\u201cname\u201d: the name of the target backend", "\u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config", "type": "Quantization", "text": ["Set the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs", "type": "Quantization", "text": ["Set the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_name()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_name", "type": "Quantization", "text": ["Set the name of the target backend.", "BackendConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.to_dict", "type": "Quantization", "text": ["Convert this BackendConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig", "type": "Quantization", "text": ["Config object that specifies quantization behavior for a given operator pattern. For a detailed example usage, see BackendConfig.", "Add a set of supported data types passed as arguments to quantize ops in the reference model spec.", "BackendPatternConfig", "Create a BackendPatternConfig from a dictionary with the following items:", "\u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)", "BackendPatternConfig", "Set the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.", "BackendPatternConfig", "Set the module that represents the fused implementation for this pattern.", "BackendPatternConfig", "Set the function that specifies how to fuse this BackendPatternConfig\u2019s pattern.", "The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module.", "For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:", "return torch.ao.nn.intrinsic.LinearReLU(linear, relu)", "For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.", "BackendPatternConfig", "Set how observers should be inserted in the graph for this pattern.", "Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output.", "There are two observation types:", "OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type.", "OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat.", "Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.", "BackendPatternConfig", "Set the pattern to configure.", "The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.", "BackendPatternConfig", "Set the module that represents the QAT implementation for this pattern.", "BackendPatternConfig", "Set the module that represents the reference quantized implementation for this pattern\u2019s root module.", "For more detail, see set_root_module().", "BackendPatternConfig", "Set the module that represents the root for this pattern.", "When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.", "BackendPatternConfig", "Convert this BackendPatternConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config", "type": "Quantization", "text": ["Add a set of supported data types passed as arguments to quantize ops in the reference model spec.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict", "type": "Quantization", "text": ["Create a BackendPatternConfig from a dictionary with the following items:", "\u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs", "type": "Quantization", "text": ["Set the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module", "type": "Quantization", "text": ["Set the module that represents the fused implementation for this pattern.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method", "type": "Quantization", "text": ["Set the function that specifies how to fuse this BackendPatternConfig\u2019s pattern.", "The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module.", "For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:", "return torch.ao.nn.intrinsic.LinearReLU(linear, relu)", "For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type", "type": "Quantization", "text": ["Set how observers should be inserted in the graph for this pattern.", "Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output.", "There are two observation types:", "OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type.", "OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat.", "Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern", "type": "Quantization", "text": ["Set the pattern to configure.", "The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module", "type": "Quantization", "text": ["Set the module that represents the QAT implementation for this pattern.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module", "type": "Quantization", "text": ["Set the module that represents the reference quantized implementation for this pattern\u2019s root module.", "For more detail, see set_root_module().", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module", "type": "Quantization", "text": ["Set the module that represents the root for this pattern.", "When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.", "BackendPatternConfig"]}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.to_dict", "type": "Quantization", "text": ["Convert this BackendPatternConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig", "type": "Quantization", "text": ["Config object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases.", "For example, consider the following reference model:", "quant1 - [dequant1 - fp32_linear - quant2] - dequant2", "The pattern in the square brackets refers to the reference pattern of statically quantized linear. Setting the input dtype as torch.quint8 in the DTypeConfig means we pass in torch.quint8 as the dtype argument to the first quantize op (quant1). Similarly, setting the output dtype as torch.quint8 means we pass in torch.quint8 as the dtype argument to the second quantize op (quant2).", "Note that the dtype here does not refer to the interface dtypes of the op. For example, the \u201cinput dtype\u201d here is not the dtype of the input tensor passed to the quantized linear op. Though it can still be the same as the interface dtype, this is not always the case, e.g. the interface dtype is fp32 in dynamic quantization but the \u201cinput dtype\u201d specified in the DTypeConfig would still be quint8. The semantics of dtypes here are the same as the semantics of the dtypes specified in the observers.", "These dtypes are matched against the ones specified in the user\u2019s QConfig. If there is a match, and the QConfig satisfies the constraints specified in the DTypeConfig (if any), then we will quantize the given pattern using this DTypeConfig. Otherwise, the QConfig is ignored and the pattern will not be quantized.", "Example usage:", "\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool", "DTypeConfig", "Convert this DTypeConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.from_dict", "type": "Quantization", "text": ["\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool", "DTypeConfig"]}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.to_dict", "type": "Quantization", "text": ["Convert this DTypeConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.backend_config.DTypeWithConstraints", "path": "generated/torch.ao.quantization.backend_config.dtypewithconstraints#torch.ao.quantization.backend_config.DTypeWithConstraints", "type": "Quantization", "text": ["Config for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in DTypeConfig.", "The constraints currently supported are:"]}, {"name": "torch.ao.quantization.backend_config.ObservationType", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType", "type": "Quantization", "text": ["An enum that represents different ways of how an operator/operator pattern should be observed", "this means the input and output are never observed example: x.shape, x.size", "this means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool", "this means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "type": "Quantization", "text": ["this means the input and output are never observed example: x.shape, x.size"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "type": "Quantization", "text": ["this means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool"]}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "type": "Quantization", "text": ["this means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax"]}, {"name": "torch.ao.quantization.convert", "path": "generated/torch.ao.quantization.convert#torch.ao.quantization.convert", "type": "Quantization", "text": ["Converts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True."]}, {"name": "torch.ao.quantization.default_eval_fn", "path": "generated/torch.ao.quantization.default_eval_fn#torch.ao.quantization.default_eval_fn", "type": "Quantization", "text": ["Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset"]}, {"name": "torch.ao.quantization.DeQuantStub", "path": "generated/torch.ao.quantization.dequantstub#torch.ao.quantization.DeQuantStub", "type": "Quantization", "text": ["Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules"]}, {"name": "torch.ao.quantization.fake_quantize.default_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fake_quant#torch.ao.quantization.fake_quantize.default_fake_quant", "type": "Quantization", "text": ["Default fake_quant for activations."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "type": "Quantization", "text": ["Fused version of default_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "type": "Quantization", "text": ["Fused version of default_per_channel_weight_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "type": "Quantization", "text": ["Fused version of default_weight_fake_quant, with improved performance."]}, {"name": "torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant#torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "type": "Quantization", "text": ["Fake_quant for activations using a histogram.."]}, {"name": "torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "type": "Quantization", "text": ["Default fake_quant for per-channel weights. Observer is memoryless since averaging_constant is 1."]}, {"name": "torch.ao.quantization.fake_quantize.default_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant#torch.ao.quantization.fake_quantize.default_weight_fake_quant", "type": "Quantization", "text": ["Default fake_quant for weights. Observer is memoryless since averaging_constant is 1."]}, {"name": "torch.ao.quantization.fake_quantize.disable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.disable_fake_quant#torch.ao.quantization.fake_quantize.disable_fake_quant", "type": "Quantization", "text": ["Disable fake quantization for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.disable_observer", "path": "generated/torch.ao.quantization.fake_quantize.disable_observer#torch.ao.quantization.fake_quantize.disable_observer", "type": "Quantization", "text": ["Disable observation for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.enable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.enable_fake_quant#torch.ao.quantization.fake_quantize.enable_fake_quant", "type": "Quantization", "text": ["Enable fake quantization for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.enable_observer", "path": "generated/torch.ao.quantization.fake_quantize.enable_observer#torch.ao.quantization.fake_quantize.enable_observer", "type": "Quantization", "text": ["Enable observation for this module, if applicable. Example usage:"]}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fakequantize#torch.ao.quantization.fake_quantize.FakeQuantize", "type": "Quantization", "text": ["Simulate the quantize and dequantize operations in training time. The output of this module is given by:", "allowable values are torch.qint8 and torch.quint8.", "activation_post_process (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point."]}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantizeBase", "path": "generated/torch.ao.quantization.fake_quantize.fakequantizebase#torch.ao.quantization.fake_quantize.FakeQuantizeBase", "type": "Quantization", "text": ["Base fake quantize module Any fake quantize implementation should derive from this class.", "Concrete fake quantize module should follow the same API. In forward, they will update the statistics of the observed Tensor and fake quantize the input. They should also provide a calculate_qparams function that computes the quantization parameters given the collected statistics."]}, {"name": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fixedqparamsfakequantize#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "type": "Quantization", "text": ["Simulate quantize and dequantize with fixed quantization parameters in training time. Only per tensor quantization is supported."]}, {"name": "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fusedmovingavgobsfakequantize#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "type": "Quantization", "text": ["Fused module that is used to observe the input tensor (compute min/max), compute scale/zero_point and fake_quantize the tensor. This module uses calculation similar MovingAverageMinMaxObserver for the inputs, to compute the min/max values in order to compute the scale/zero_point. The qscheme input in the observer is used to differentiate between symmetric/affine quantization scheme.", "The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale", "Similar to FakeQuantize, and accepts the same attributes as the base class."]}, {"name": "torch.ao.quantization.fuse_modules", "path": "generated/torch.ao.quantization.fuse_modules#torch.ao.quantization.fuse_modules", "type": "Quantization", "text": ["Fuses a list of modules into a single module", "Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.", "model with fused modules. A new copy is created if inplace=True.", "Examples:"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "type": "Quantization", "text": ["Custom configuration for convert_fx().", "Example usage:", "Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "ConvertCustomConfig", "Set the mapping from a custom observed module class to a custom quantized module class.", "The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.", "ConvertCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "ConvertCustomConfig", "Convert this ConvertCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict", "type": "Quantization", "text": ["Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping", "type": "Quantization", "text": ["Set the mapping from a custom observed module class to a custom quantized module class.", "The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "ConvertCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this ConvertCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig", "type": "Quantization", "text": ["Custom configuration for fuse_fx().", "Example usage:", "Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "FuseCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "FuseCustomConfig", "Convert this FuseCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict", "type": "Quantization", "text": ["Create a ConvertCustomConfig from a dictionary with the following items:", "\u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "FuseCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "FuseCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this FuseCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "type": "Quantization", "text": ["Custom configuration for prepare_fx() and prepare_qat_fx().", "Example usage:", "Create a PrepareCustomConfig from a dictionary with the following items:", "\u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}}", "\u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "PrepareCustomConfig", "Set the mapping from a custom float module class to a custom observed module class.", "The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.", "PrepareCustomConfig", "Set the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig", "Set the modules that are not symbolically traceable, identified by class.", "PrepareCustomConfig", "Set the modules that are not symbolically traceable, identified by name.", "PrepareCustomConfig", "Set the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig", "Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "PrepareCustomConfig", "Set the configuration for running a standalone module identified by module_class.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig", "Set the configuration for running a standalone module identified by module_name.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig", "Convert this PrepareCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict", "type": "Quantization", "text": ["Create a PrepareCustomConfig from a dictionary with the following items:", "\u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples", "\u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}}", "\u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward", "This function is primarily for backward compatibility and may be removed in the future.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping", "type": "Quantization", "text": ["Set the mapping from a custom float module class to a custom observed module class.", "The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes", "type": "Quantization", "text": ["Set the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes", "type": "Quantization", "text": ["Set the modules that are not symbolically traceable, identified by class.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names", "type": "Quantization", "text": ["Set the modules that are not symbolically traceable, identified by name.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes", "type": "Quantization", "text": ["Set the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes", "type": "Quantization", "text": ["Set the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class", "type": "Quantization", "text": ["Set the configuration for running a standalone module identified by module_class.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name", "type": "Quantization", "text": ["Set the configuration for running a standalone module identified by module_name.", "If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.", "PrepareCustomConfig"]}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict", "type": "Quantization", "text": ["Convert this PrepareCustomConfig to a dictionary with the items described in from_dict().", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "path": "generated/torch.ao.quantization.fx.custom_config.standalonemoduleconfigentry#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "type": "Quantization", "text": []}, {"name": "torch.ao.quantization.observer.default_debug_observer", "path": "generated/torch.ao.quantization.observer.default_debug_observer#torch.ao.quantization.observer.default_debug_observer", "type": "Quantization", "text": ["Default debug-only observer."]}, {"name": "torch.ao.quantization.observer.default_dynamic_quant_observer", "path": "generated/torch.ao.quantization.observer.default_dynamic_quant_observer#torch.ao.quantization.observer.default_dynamic_quant_observer", "type": "Quantization", "text": ["Default observer for dynamic quantization."]}, {"name": "torch.ao.quantization.observer.default_float_qparams_observer", "path": "generated/torch.ao.quantization.observer.default_float_qparams_observer#torch.ao.quantization.observer.default_float_qparams_observer", "type": "Quantization", "text": ["Default observer for a floating point zero-point."]}, {"name": "torch.ao.quantization.observer.default_histogram_observer", "path": "generated/torch.ao.quantization.observer.default_histogram_observer#torch.ao.quantization.observer.default_histogram_observer", "type": "Quantization", "text": ["Default histogram observer, usually used for PTQ."]}, {"name": "torch.ao.quantization.observer.default_observer", "path": "generated/torch.ao.quantization.observer.default_observer#torch.ao.quantization.observer.default_observer", "type": "Quantization", "text": ["Default observer for static quantization, usually used for debugging."]}, {"name": "torch.ao.quantization.observer.default_per_channel_weight_observer", "path": "generated/torch.ao.quantization.observer.default_per_channel_weight_observer#torch.ao.quantization.observer.default_per_channel_weight_observer", "type": "Quantization", "text": ["Default per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as fbgemm."]}, {"name": "torch.ao.quantization.observer.default_placeholder_observer", "path": "generated/torch.ao.quantization.observer.default_placeholder_observer#torch.ao.quantization.observer.default_placeholder_observer", "type": "Quantization", "text": ["Default placeholder observer, usually used for quantization to torch.float16."]}, {"name": "torch.ao.quantization.observer.default_weight_observer", "path": "generated/torch.ao.quantization.observer.default_weight_observer#torch.ao.quantization.observer.default_weight_observer", "type": "Quantization", "text": ["Default weight observer."]}, {"name": "torch.ao.quantization.observer.get_observer_state_dict", "path": "generated/torch.ao.quantization.observer.get_observer_state_dict#torch.ao.quantization.observer.get_observer_state_dict", "type": "Quantization", "text": ["Returns the state dict corresponding to the observer stats. Traverse the model state_dict and extract out the stats."]}, {"name": "torch.ao.quantization.observer.HistogramObserver", "path": "generated/torch.ao.quantization.observer.histogramobserver#torch.ao.quantization.observer.HistogramObserver", "type": "Quantization", "text": ["The module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.", "The scale and zero point are computed as follows:", "The histogram is computed continuously, and the ranges per bin change with every new tensor observed.", "The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.", "MinMaxObserver"]}, {"name": "torch.ao.quantization.observer.load_observer_state_dict", "path": "generated/torch.ao.quantization.observer.load_observer_state_dict#torch.ao.quantization.observer.load_observer_state_dict", "type": "Quantization", "text": ["Given input model and a state_dict containing model observer stats, load the stats back into the model. The observer state_dict can be saved using torch.ao.quantization.get_observer_state_dict"]}, {"name": "torch.ao.quantization.observer.MinMaxObserver", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running min and max values.", "This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "Given running min/max as xminx_\\text{min} and xmaxx_\\text{max}, scale ss and zero point zz are computed as:", "The running minimum/maximum xmin/maxx_\\text{min/max} is computed as:", "where XX is the observed tensor.", "The scale ss and zero point zz are then computed as:", "where QminQ_\\text{min} and QmaxQ_\\text{max} are the minimum and maximum of the quantized data type.", "Warning", "dtype can only take torch.qint8 or torch.quint8.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.", "Calculates the quantization parameters.", "Records the running minimum and maximum of x.", "Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.calculate_qparams()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams", "type": "Quantization", "text": ["Calculates the quantization parameters."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.forward()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.forward", "type": "Quantization", "text": ["Records the running minimum and maximum of x."]}, {"name": "torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": ["Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.MovingAverageMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageminmaxobserver#torch.ao.quantization.observer.MovingAverageMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the moving average of the min and max values.", "This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The moving average min/max is computed as follows", "where xmin/maxx_\\text{min/max} is the running average min/max, XX is is the incoming tensor, and cc is the averaging_constant.", "The scale and zero point are then computed as in MinMaxObserver.", "Note", "Only works with torch.per_tensor_affine quantization scheme.", "Note", "If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0."]}, {"name": "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageperchannelminmaxobserver#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0."]}, {"name": "torch.ao.quantization.observer.NoopObserver", "path": "generated/torch.ao.quantization.observer.noopobserver#torch.ao.quantization.observer.NoopObserver", "type": "Quantization", "text": ["Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Primarily used for quantization to float16 which doesn\u2019t require determining ranges."]}, {"name": "torch.ao.quantization.observer.ObserverBase", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase", "type": "Quantization", "text": ["Base observer Module. Any observer implementation should derive from this class.", "Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.", "dtype \u2013 dtype argument to the quantize node needed to implement the reference model spec.", "Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args", "Example:", "Wrapper that allows creation of class factories args that need to be called at construction time.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args", "Example:"]}, {"name": "torch.ao.quantization.observer.ObserverBase.with_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_args", "type": "Quantization", "text": ["Wrapper that allows creation of class factories.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args", "Example:"]}, {"name": "torch.ao.quantization.observer.ObserverBase.with_callable_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_callable_args", "type": "Quantization", "text": ["Wrapper that allows creation of class factories args that need to be called at construction time.", "This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args", "Example:"]}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver", "type": "Quantization", "text": ["Observer module for computing the quantization parameters based on the running per channel min and max values.", "This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.", "The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.", "Note", "If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.", "Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": ["Resets the min/max values."]}, {"name": "torch.ao.quantization.observer.PlaceholderObserver", "path": "generated/torch.ao.quantization.observer.placeholderobserver#torch.ao.quantization.observer.PlaceholderObserver", "type": "Quantization", "text": ["Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float().", "Can be used for quantization to float16 which doesn\u2019t require determining ranges."]}, {"name": "torch.ao.quantization.observer.RecordingObserver", "path": "generated/torch.ao.quantization.observer.recordingobserver#torch.ao.quantization.observer.RecordingObserver", "type": "Quantization", "text": ["The module is mainly for debug and records the tensor values during runtime."]}, {"name": "torch.ao.quantization.prepare", "path": "generated/torch.ao.quantization.prepare#torch.ao.quantization.prepare", "type": "Quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.", "The model will be attached with observer or fake quant modules, and qconfig will be propagated."]}, {"name": "torch.ao.quantization.prepare_qat", "path": "generated/torch.ao.quantization.prepare_qat#torch.ao.quantization.prepare_qat", "type": "Quantization", "text": ["Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.", "Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute."]}, {"name": "torch.ao.quantization.propagate_qconfig_", "path": "generated/torch.ao.quantization.propagate_qconfig_#torch.ao.quantization.propagate_qconfig_", "type": "Quantization", "text": ["Propagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module", "None, module is modified inplace with qconfig attached"]}, {"name": "torch.ao.quantization.qconfig.default_activation_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_activation_only_qconfig#torch.ao.quantization.qconfig.default_activation_only_qconfig", "type": "Quantization", "text": ["Default qconfig for quantizing activations only."]}, {"name": "torch.ao.quantization.qconfig.default_debug_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_debug_qconfig#torch.ao.quantization.qconfig.default_debug_qconfig", "type": "Quantization", "text": ["Default qconfig configuration for debugging."]}, {"name": "torch.ao.quantization.qconfig.default_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_dynamic_qconfig#torch.ao.quantization.qconfig.default_dynamic_qconfig", "type": "Quantization", "text": ["Default dynamic qconfig."]}, {"name": "torch.ao.quantization.qconfig.default_per_channel_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_per_channel_qconfig#torch.ao.quantization.qconfig.default_per_channel_qconfig", "type": "Quantization", "text": ["Default qconfig configuration for per channel weight quantization."]}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig#torch.ao.quantization.qconfig.default_qat_qconfig", "type": "Quantization", "text": ["Default qconfig for QAT."]}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig_v2", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2#torch.ao.quantization.qconfig.default_qat_qconfig_v2", "type": "Quantization", "text": ["Fused version of default_qat_config, has performance benefits."]}, {"name": "torch.ao.quantization.qconfig.default_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qconfig#torch.ao.quantization.qconfig.default_qconfig", "type": "Quantization", "text": ["Default qconfig configuration."]}, {"name": "torch.ao.quantization.qconfig.default_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_weight_only_qconfig#torch.ao.quantization.qconfig.default_weight_only_qconfig", "type": "Quantization", "text": ["Default qconfig for quantizing weights only."]}, {"name": "torch.ao.quantization.qconfig.float16_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig#torch.ao.quantization.qconfig.float16_dynamic_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized to torch.float16."]}, {"name": "torch.ao.quantization.qconfig.float16_static_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_static_qconfig#torch.ao.quantization.qconfig.float16_static_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with both activations and weights quantized to torch.float16."]}, {"name": "torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized with a floating point zero_point."]}, {"name": "torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "type": "Quantization", "text": ["Dynamic qconfig with weights quantized per channel."]}, {"name": "torch.ao.quantization.qconfig.QConfig", "path": "generated/torch.ao.quantization.qconfig.qconfig#torch.ao.quantization.qconfig.QConfig", "type": "Quantization", "text": ["Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.", "Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.", "Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):"]}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "type": "Quantization", "text": ["Return the default QConfigMapping for quantization aware training.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "type": "Quantization", "text": ["Return the default QConfigMapping for post training quantization.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping", "type": "Quantization", "text": ["Mapping from model ops to torch.ao.quantization.QConfig s.", "The user can specify QConfigs using the following methods (in increasing match priority):", "set_global : sets the global (default) QConfig", "set_object_type : sets the QConfig for a given module type, function, or method name", "set_module_name_regex : sets the QConfig for modules matching the given regex string", "set_module_name : sets the QConfig for modules matching the given module name", "set_module_name_object_type_order : sets the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears", "Example usage:", "Create a QConfigMapping from a dictionary with the following keys (all optional):", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are expected to be lists of tuples.", "QConfigMapping", "Set the global (default) QConfig.", "QConfigMapping", "Set the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.", "QConfigMapping", "Set the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears.", "If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.", "QConfigMapping", "Set the QConfig for modules matching the given regex string.", "Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.:", "In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3.", "If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.", "QConfigMapping", "Set the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.", "QConfigMapping", "Convert this QConfigMapping to a dictionary with the following keys:", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are lists of tuples.", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict", "type": "Quantization", "text": ["Create a QConfigMapping from a dictionary with the following keys (all optional):", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are expected to be lists of tuples.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global", "type": "Quantization", "text": ["Set the global (default) QConfig.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name", "type": "Quantization", "text": ["Set the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order", "type": "Quantization", "text": ["Set the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears.", "If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex", "type": "Quantization", "text": ["Set the QConfig for modules matching the given regex string.", "Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.:", "In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3.", "If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type", "type": "Quantization", "text": ["Set the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.", "QConfigMapping"]}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict", "type": "Quantization", "text": ["Convert this QConfigMapping to a dictionary with the following keys:", "\u201c\u201d (for global QConfig)", "\u201cobject_type\u201d", "\u201cmodule_name_regex\u201d", "\u201cmodule_name\u201d", "\u201cmodule_name_object_type_order\u201d", "The values of this dictionary are lists of tuples.", "Dict[str, Any]"]}, {"name": "torch.ao.quantization.quantize", "path": "generated/torch.ao.quantization.quantize#torch.ao.quantization.quantize", "type": "Quantization", "text": ["Quantize the input float model with post training static quantization.", "First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.", "Quantized model."]}, {"name": "torch.ao.quantization.quantize_dynamic", "path": "generated/torch.ao.quantization.quantize_dynamic#torch.ao.quantization.quantize_dynamic", "type": "Quantization", "text": ["Converts a float model to dynamic (i.e. weights-only) quantized model.", "Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.", "For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.", "Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.", "qconfig_spec \u2013 ", "Either:"]}, {"name": "torch.ao.quantization.quantize_fx.convert_fx", "path": "generated/torch.ao.quantization.quantize_fx.convert_fx#torch.ao.quantization.quantize_fx.convert_fx", "type": "Quantization", "text": ["Convert a calibrated or trained model to a quantized model", "qconfig_mapping (*) \u2013 ", "config for specifying how to convert a model for quantization.", "The keys must include the ones in the qconfig_mapping passed to prepare_fx or prepare_qat_fx, with the same values or None. Additional keys can be specified with values set to None.", "For each entry whose value is set to None, we skip quantizing that entry in the model:", "operators should be quantized in the backend, this includes quantization mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.), observer placement for each operators and fused operators. See BackendConfig for more details", "A quantized model (torch.nn.Module)", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.fuse_fx", "path": "generated/torch.ao.quantization.quantize_fx.fuse_fx#torch.ao.quantization.quantize_fx.fuse_fx", "type": "Quantization", "text": ["Fuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode. Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.prepare_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_fx#torch.ao.quantization.quantize_fx.prepare_fx", "type": "Quantization", "text": ["Prepare a model for post training static quantization", "A GraphModule with observer (configured by qconfig_mapping), ready for calibration", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_fx.prepare_qat_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_qat_fx#torch.ao.quantization.quantize_fx.prepare_qat_fx", "type": "Quantization", "text": ["Prepare a model for quantization aware training", "A GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for quantization aware training", "GraphModule", "Example:"]}, {"name": "torch.ao.quantization.quantize_qat", "path": "generated/torch.ao.quantization.quantize_qat#torch.ao.quantization.quantize_qat", "type": "Quantization", "text": ["Do quantization aware training and output a quantized model", "Quantized model."]}, {"name": "torch.ao.quantization.QuantStub", "path": "generated/torch.ao.quantization.quantstub#torch.ao.quantization.QuantStub", "type": "Quantization", "text": ["Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.", "qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules"]}, {"name": "torch.ao.quantization.QuantWrapper", "path": "generated/torch.ao.quantization.quantwrapper#torch.ao.quantization.QuantWrapper", "type": "Quantization", "text": ["A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.", "This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub."]}, {"name": "torch.ao.quantization.swap_module", "path": "generated/torch.ao.quantization.swap_module#torch.ao.quantization.swap_module", "type": "Quantization", "text": ["Swaps the module if it has a quantized counterpart and it has an observer attached.", "The corresponding quantized module of mod"]}, {"name": "torch.arange", "path": "generated/torch.arange", "type": "Torch", "text": ["Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil with values from the interval [start, end) taken with common difference step beginning from start.", "Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise subtracting a small epsilon from end in such cases.", "Example:"]}, {"name": "torch.arccos", "path": "generated/torch.arccos", "type": "Torch", "text": ["Alias for torch.acos()."]}, {"name": "torch.arccosh", "path": "generated/torch.arccosh", "type": "Torch", "text": ["Alias for torch.acosh()."]}, {"name": "torch.arcsin", "path": "generated/torch.arcsin", "type": "Torch", "text": ["Alias for torch.asin()."]}, {"name": "torch.arcsinh", "path": "generated/torch.arcsinh", "type": "Torch", "text": ["Alias for torch.asinh()."]}, {"name": "torch.arctan", "path": "generated/torch.arctan", "type": "Torch", "text": ["Alias for torch.atan()."]}, {"name": "torch.arctan2", "path": "generated/torch.arctan2", "type": "Torch", "text": ["Alias for torch.atan2()."]}, {"name": "torch.arctanh", "path": "generated/torch.arctanh", "type": "Torch", "text": ["Alias for torch.atanh()."]}, {"name": "torch.are_deterministic_algorithms_enabled", "path": "generated/torch.are_deterministic_algorithms_enabled", "type": "Torch", "text": ["Returns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details.", "bool"]}, {"name": "torch.argmax", "path": "generated/torch.argmax", "type": "Torch", "text": ["Returns the indices of the maximum value of all elements in the input tensor.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple maximal values then the indices of the first maximal value are returned.", "input (Tensor) \u2013 the input tensor.", "Example:", "Returns the indices of the maximum values of a tensor across a dimension.", "This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.", "Example:"]}, {"name": "torch.argmin", "path": "generated/torch.argmin", "type": "Torch", "text": ["Returns the indices of the minimum value(s) of the flattened tensor or along a dimension", "This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.", "Note", "If there are multiple minimal values then the indices of the first minimal value are returned.", "Example:"]}, {"name": "torch.argsort", "path": "generated/torch.argsort", "type": "Torch", "text": ["Returns the indices that sort a tensor along a given dimension in ascending order by value.", "This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method.", "If stable is True then the sorting routine becomes stable, preserving the order of equivalent elements. If False, the relative order of values which compare equal is not guaranteed. True is slower.", "Example:"]}, {"name": "torch.argwhere", "path": "generated/torch.argwhere", "type": "Torch", "text": ["Returns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style).", "If input has nn dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n), where zz is the total number of non-zero elements in the input tensor.", "Note", "This function is similar to NumPy\u2019s argwhere.", "When input is on CUDA, this function causes host-device synchronization.", "{input} \u2013 ", "Example:"]}, {"name": "torch.as_strided", "path": "generated/torch.as_strided", "type": "Torch", "text": ["Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.", "Warning", "Prefer using other view functions, like torch.Tensor.expand(), to setting a view\u2019s strides manually with as_strided, as this function\u2019s behavior depends on the implementation of a tensor\u2019s storage. The constructed view of the storage must only refer to elements within the storage or a runtime error will be thrown, and if the view is \u201coverlapped\u201d (with multiple indices referring to the same element in memory) its behavior is undefined.", "Example:"]}, {"name": "torch.as_tensor", "path": "generated/torch.as_tensor", "type": "Torch", "text": ["Converts data into a tensor, sharing data and preserving autograd history if possible.", "If data is already a tensor with the requested dtype and device then data itself is returned, but if data is a tensor with a different dtype or device then it\u2019s copied as if using data.to(dtype=dtype, device=device).", "If data is a NumPy array (an ndarray) with the same dtype and device then a tensor is constructed using torch.from_numpy().", "See also", "torch.tensor() never shares its data and creates a new \u201cleaf tensor\u201d (see Autograd mechanics).", "Example:"]}, {"name": "torch.asarray", "path": "generated/torch.asarray", "type": "Torch", "text": ["Converts obj to a tensor.", "obj can be one of:", "When obj is a tensor, NumPy array, or DLPack capsule the returned tensor will, by default, not require a gradient, have the same datatype as obj, be on the same device, and share memory with it. These properties can be controlled with the dtype, device, copy, and requires_grad keyword arguments. If the returned tensor is of a different datatype, on a different device, or a copy is requested then it will not share its memory with obj. If requires_grad is True then the returned tensor will require a gradient, and if obj is also a tensor with an autograd history then the returned tensor will have the same history.", "When obj is not a tensor, NumPy array, or DLPack capsule but implements Python\u2019s buffer protocol then the buffer is interpreted as an array of bytes grouped according to the size of the datatype passed to the dtype keyword argument. (If no datatype is passed then the default floating point datatype is used, instead.) The returned tensor will have the specified datatype (or default floating point datatype if none is specified) and, by default, be on the CPU device and share memory with the buffer.", "When obj is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on the CPU and that doesn\u2019t share its memory (i.e. copy=True). By default datatype will be the PyTorch datatype corresponding to the NumPy\u2019s scalar\u2019s datatype.", "When obj is none of the above but a scalar, or a sequence of scalars then the returned tensor will, by default, infer its datatype from the scalar values, be on the current default device, and not share its memory.", "See also", "torch.tensor() creates a tensor that always copies the data from the input object. torch.from_numpy() creates a tensor that always shares memory from NumPy arrays. torch.frombuffer() creates a tensor that always shares memory from objects that implement the buffer protocol. torch.from_dlpack() creates a tensor that always shares memory from DLPack capsules.", "obj (object) \u2013 a tensor, NumPy array, DLPack Capsule, object that implements Python\u2019s buffer protocol, scalar, or sequence of scalars.", "Example:"]}, {"name": "torch.asin", "path": "generated/torch.asin", "type": "Torch", "text": ["Returns a new tensor with the arcsine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.asinh", "path": "generated/torch.asinh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic sine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atan", "path": "generated/torch.atan", "type": "Torch", "text": ["Returns a new tensor with the arctangent of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atan2", "path": "generated/torch.atan2", "type": "Torch", "text": ["Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i} with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i}) and vector (1,0)(1, 0). (Note that otheri\\text{other}_{i}, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}, the first parameter, is the y-coordinate.)", "The shapes of input and other must be broadcastable.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atanh", "path": "generated/torch.atanh", "type": "Torch", "text": ["Returns a new tensor with the inverse hyperbolic tangent of the elements of input.", "Note", "The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.atleast_1d", "path": "generated/torch.atleast_1d", "type": "Torch", "text": ["Returns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)", "Example:"]}, {"name": "torch.atleast_2d", "path": "generated/torch.atleast_2d", "type": "Torch", "text": ["Returns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)", "Example:"]}, {"name": "torch.atleast_3d", "path": "generated/torch.atleast_3d", "type": "Torch", "text": ["Returns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is.", "input (Tensor or list of Tensors) \u2013 ", "output (Tensor or tuple of Tensors)"]}, {"name": "torch.autocast", "path": "amp#torch.autocast", "type": "Automatic Mixed Precision", "text": ["Instances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision.", "In these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details.", "When entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting.", "autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.", "Example for CUDA Devices:", "See the CUDA Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).", "autocast can also be used as a decorator, e.g., on the forward method of your model:", "Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. CUDA Example:", "CPU Training Example:", "CPU Inference Example:", "CPU Inference Example with Jit Trace:", "Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue.", "autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use:", "The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs)."]}, {"name": "torch.autograd.backward()", "path": "generated/torch.autograd.backward#torch.autograd.backward", "type": "Automatic Differentiation", "text": ["Computes the sum of gradients of given tensors with respect to graph leaves.", "The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors).", "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.", "Note", "Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.", "Note", "If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "When inputs are provided and a given input is not a leaf, the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details."]}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "Automatic Differentiation", "text": ["Context-manager that enable anomaly detection for the autograd engine.", "This does two things:", "Warning", "This mode should be enabled only for debugging as the different tests will slow down your program execution."]}, {"name": "torch.autograd.forward_ad.dual_level", "path": "generated/torch.autograd.forward_ad.dual_level#torch.autograd.forward_ad.dual_level", "type": "Automatic Differentiation", "text": ["Context-manager that enables forward AD. All forward AD computation must be performed in a dual_level context.", "Note", "The dual_level context appropriately enters and exit the dual level to controls the current forward AD level, which is used by default by the other functions in this API.", "We currently don\u2019t plan to support nested dual_level contexts, however, so only a single forward AD level is supported. To compute higher-order forward grads, one can use torch.func.jvp().", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.forward_ad.make_dual()", "path": "generated/torch.autograd.forward_ad.make_dual#torch.autograd.forward_ad.make_dual", "type": "Automatic Differentiation", "text": ["Associates a tensor value with a forward gradient, the tangent, to create a \u201cdual tensor\u201d, which is used to compute forward AD gradients. The result is a new tensor aliased to tensor with tangent embedded as an attribute as-is if it has the same storage layout or copied otherwise. The tangent attribute can be recovered with unpack_dual().", "This function is backward differentiable.", "Given a function f whose jacobian is J, it allows one to compute the Jacobian-vector product (jvp) between J and a given vector v as follows.", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.forward_ad.unpack_dual()", "path": "generated/torch.autograd.forward_ad.unpack_dual#torch.autograd.forward_ad.unpack_dual", "type": "Automatic Differentiation", "text": ["Unpacks a \u201cdual tensor\u201d to get both its Tensor value and its forward AD gradient. The result is a namedtuple (primal, tangent) where primal is a view of tensor\u2019s primal and tangent is tensor\u2019s tangent as-is. Neither of these tensors can be dual tensor of level level.", "This function is backward differentiable.", "Example:", "Please see the forward-mode AD tutorial for detailed steps on how to use this API."]}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "Automatic Differentiation", "text": ["Base class to create custom autograd.Function", "To create a custom autograd.Function, subclass this class and implement the forward() and backward() static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call forward() directly.", "To ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using torch.autograd.gradcheck().", "See Extending torch.autograd for more details on how to use this class.", "Examples:"]}, {"name": "torch.autograd.Function.backward()", "path": "generated/torch.autograd.function.backward#torch.autograd.Function.backward", "type": "Automatic Differentiation", "text": ["Defines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function).", "This function is to be overridden by all subclasses.", "It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input.", "The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.", "Any"]}, {"name": "torch.autograd.Function.forward()", "path": "generated/torch.autograd.function.forward#torch.autograd.Function.forward", "type": "Automatic Differentiation", "text": ["This function is to be overridden by all subclasses. There are two ways to define forward:", "Usage 1 (Combined forward and ctx):", "Usage 2 (Separate forward and ctx):", "The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.", "Any"]}, {"name": "torch.autograd.function.FunctionCtx.mark_dirty()", "path": "generated/torch.autograd.function.functionctx.mark_dirty#torch.autograd.function.FunctionCtx.mark_dirty", "type": "Automatic Differentiation", "text": ["Marks given tensors as modified in an in-place operation.", "This should be called at most once, only from inside the forward() method, and all arguments should be inputs.", "Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification."]}, {"name": "torch.autograd.function.FunctionCtx.mark_non_differentiable()", "path": "generated/torch.autograd.function.functionctx.mark_non_differentiable#torch.autograd.function.FunctionCtx.mark_non_differentiable", "type": "Automatic Differentiation", "text": ["Marks outputs as non-differentiable.", "This should be called at most once, only from inside the forward() method, and all arguments should be tensor outputs.", "This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output."]}, {"name": "torch.autograd.function.FunctionCtx.save_for_backward()", "path": "generated/torch.autograd.function.functionctx.save_for_backward#torch.autograd.function.FunctionCtx.save_for_backward", "type": "Automatic Differentiation", "text": ["Saves given tensors for a future call to backward().", "save_for_backward should be called at most once, only from inside the forward() method, and only with tensors.", "All tensors intended to be used in the backward pass should be saved with save_for_backward (as opposed to directly on ctx) to prevent incorrect gradients and memory leaks, and enable the application of saved tensor hooks. See torch.autograd.graph.saved_tensors_hooks.", "Note that if intermediary tensors, tensors that are neither inputs nor outputs of forward(), are saved for backward, your custom Function may not support double backward. Custom Functions that do not support double backward should decorate their backward() method with @once_differentiable so that performing double backward raises an error. If you\u2019d like to support double backward, you can either recompute intermediaries based on the inputs during backward or return the intermediaries as the outputs of the custom Function. See the double backward tutorial for more details.", "In backward(), saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content.", "Arguments can also be None. This is a no-op.", "See Extending torch.autograd for more details on how to use this method."]}, {"name": "torch.autograd.function.FunctionCtx.set_materialize_grads()", "path": "generated/torch.autograd.function.functionctx.set_materialize_grads#torch.autograd.function.FunctionCtx.set_materialize_grads", "type": "Automatic Differentiation", "text": ["Sets whether to materialize grad tensors. Default is True.", "This should be called only from inside the forward() method", "If True, undefined grad tensors will be expanded to tensors full of zeros prior to calling the backward() and jvp() methods."]}, {"name": "torch.autograd.Function.jvp()", "path": "generated/torch.autograd.function.jvp#torch.autograd.Function.jvp", "type": "Automatic Differentiation", "text": ["Defines a formula for differentiating the operation with forward mode automatic differentiation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many inputs as the forward() got (None will be passed in for non tensor inputs of the forward function), and it should return as many tensors as there were outputs to forward(). Each argument is the gradient w.r.t the given input, and each returned value should be the gradient w.r.t. the corresponding output. If an output is not a Tensor or the function is not differentiable with respect to that output, you can just pass None as a gradient for that input.", "You can use the ctx object to pass any value from the forward to this functions.", "Any"]}, {"name": "torch.autograd.Function.vmap()", "path": "generated/torch.autograd.function.vmap#torch.autograd.Function.vmap", "type": "Automatic Differentiation", "text": ["Defines a rule for the behavior of this autograd.Function underneath torch.vmap(). For a torch.autograd.Function() to support torch.vmap(), you must either override this staticmethod, or set generate_vmap_rule to True (you may not do both).", "If you choose to override this staticmethod: it must accept", "The return of the vmap staticmethod is a tuple of (output, out_dims). Similar to in_dims, out_dims should be of the same structure as output and contain one out_dim per output that specifies if the output has the vmapped dimension and what index it is in.", "Please see Extending torch.func with autograd.Function for more details."]}, {"name": "torch.autograd.functional.hessian()", "path": "generated/torch.autograd.functional.hessian#torch.autograd.functional.hessian", "type": "Automatic Differentiation", "text": ["Function that computes the Hessian of a given scalar function.", "if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.", "Hessian (Tensor or a tuple of tuple of Tensors)"]}, {"name": "torch.autograd.functional.hvp()", "path": "generated/torch.autograd.functional.hvp#torch.autograd.functional.hvp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)", "Note", "This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation."]}, {"name": "torch.autograd.functional.jacobian()", "path": "generated/torch.autograd.functional.jacobian#torch.autograd.functional.jacobian", "type": "Automatic Differentiation", "text": ["Function that computes the Jacobian of a given function.", "if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input. If strategy is forward-mode, the dtype will be that of the output; otherwise, the input.", "Jacobian (Tensor or nested tuple of Tensors)"]}, {"name": "torch.autograd.functional.jvp()", "path": "generated/torch.autograd.functional.jvp#torch.autograd.functional.jvp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.", "output (tuple)", "Note", "autograd.functional.jvp computes the jvp by using the backward of the backward (sometimes called the double backwards trick). This is not the most performant way of computing the jvp. Please consider using torch.func.jvp() or the low-level forward-mode AD API instead."]}, {"name": "torch.autograd.functional.vhp()", "path": "generated/torch.autograd.functional.vhp#torch.autograd.functional.vhp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.functional.vjp()", "path": "generated/torch.autograd.functional.vjp#torch.autograd.functional.vjp", "type": "Automatic Differentiation", "text": ["Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.", "func_output (tuple of Tensors or Tensor): output of func(inputs)", "vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.", "output (tuple)"]}, {"name": "torch.autograd.grad()", "path": "generated/torch.autograd.grad#torch.autograd.grad", "type": "Automatic Differentiation", "text": ["Computes and returns the sum of gradients of outputs with respect to the inputs.", "grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None).", "Note", "If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.", "Note", "only_inputs argument is deprecated and is ignored now (defaults to True). To accumulate gradient for other parts of the graph, please use torch.autograd.backward.", "Tuple[Tensor, \u2026]"]}, {"name": "torch.autograd.gradcheck()", "path": "generated/torch.autograd.gradcheck#torch.autograd.gradcheck", "type": "Automatic Differentiation", "text": ["Check gradients computed via small finite differences against analytical gradients wrt tensors in inputs that are of floating point or complex type and with requires_grad=True.", "The check between numerical and analytical gradients uses allclose().", "For most of the complex functions we consider for optimization purposes, no notion of Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient computation is done under the assumption that the overall function has a real-valued output, we treat functions with complex output in a special way. For these functions, gradcheck is applied to two real-valued functions corresponding to taking the real components of the complex outputs for the first, and taking the imaginary components of the complex outputs for the second. For more details, check out Autograd for Complex Numbers.", "Note", "The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.", "Note", "Gradcheck may fail when evaluated on non-differentiable points because the numerically computed gradients via finite differencing may differ those computed analytically (not necessarily because either is incorrect). For more context, see Gradients for non-differentiable functions.", "Warning", "If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "bool"]}, {"name": "torch.autograd.gradgradcheck()", "path": "generated/torch.autograd.gradgradcheck#torch.autograd.gradgradcheck", "type": "Automatic Differentiation", "text": ["Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True.", "This function checks that backpropagating through the gradients computed to the given grad_outputs are correct.", "The check between numerical and analytical gradients uses allclose().", "Note", "The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.", "Warning", "If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.", "True if all differences satisfy allclose condition", "bool"]}, {"name": "torch.autograd.graph.allow_mutation_on_saved_tensors", "path": "autograd#torch.autograd.graph.allow_mutation_on_saved_tensors", "type": "Automatic Differentiation", "text": ["Context manager under which mutating tensors saved for backward is allowed", "Under this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it\u2019s used during backward.", "To ensure the correct behavior, both the forward and backward should be run under the same context manager.", "An _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.", "Example:"]}, {"name": "torch.autograd.graph.disable_saved_tensors_hooks", "path": "autograd#torch.autograd.graph.disable_saved_tensors_hooks", "type": "Automatic Differentiation", "text": ["Context-manager that disables the saved tensors default hooks feature.", "Useful for if you are creating a feature that does not work with saved tensors default hooks.", "error_message (str) \u2013 When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised.", "Example:"]}, {"name": "torch.autograd.graph.Node.metadata()", "path": "generated/torch.autograd.graph.node.metadata#torch.autograd.graph.Node.metadata", "type": "Automatic Differentiation", "text": ["Returns the metadata.", "dict"]}, {"name": "torch.autograd.graph.Node.name()", "path": "generated/torch.autograd.graph.node.name#torch.autograd.graph.Node.name", "type": "Automatic Differentiation", "text": ["Returns the name.", "Example:", "str"]}, {"name": "torch.autograd.graph.Node.next_functions", "path": "generated/torch.autograd.graph.node.next_functions#torch.autograd.graph.Node.next_functions", "type": "Automatic Differentiation", "text": []}, {"name": "torch.autograd.graph.Node.register_hook()", "path": "generated/torch.autograd.graph.node.register_hook#torch.autograd.graph.Node.register_hook", "type": "Automatic Differentiation", "text": ["Registers a backward hook.", "The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_inputs.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:", "RemovableHandle"]}, {"name": "torch.autograd.graph.Node.register_prehook()", "path": "generated/torch.autograd.graph.node.register_prehook#torch.autograd.graph.Node.register_prehook", "type": "Automatic Differentiation", "text": ["Registers a backward pre-hook.", "The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature:", "The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_outputs.", "This function returns a handle with a method handle.remove() that removes the hook from the module.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:", "RemovableHandle"]}, {"name": "torch.autograd.graph.register_multi_grad_hook", "path": "autograd#torch.autograd.graph.register_multi_grad_hook", "type": "Automatic Differentiation", "text": ["Registers a multi-grad backward hook.", "The hook will be called after gradients with respect to every tensor in tensors have been computed. If a tensor is in tensors but is not part of the graph, or if a tensor is not needed to compute the gradients for any inputs specified for the current .backward() or .grad() call, this tensor will be ignored and the hook will not wait for its gradient to be computed.", "After every non-ignored tensor\u2019s gradient has been computed, fn will be called with those gradients. None will be passed for tensors that did not have their gradients computed.", "The hook should not modify its arguments.", "This function returns a handle with a method handle.remove() that removes the hook.", "Note", "See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.", "Example:"]}, {"name": "torch.autograd.graph.save_on_cpu", "path": "autograd#torch.autograd.graph.save_on_cpu", "type": "Automatic Differentiation", "text": ["Context-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.", "When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.", "Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesn\u2019t fit in GPU memory during training).", "pin_memory (bool) \u2013 If True tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to False. Also see Use pinned memory buffers.", "Example:"]}, {"name": "torch.autograd.graph.saved_tensors_hooks", "path": "autograd#torch.autograd.graph.saved_tensors_hooks", "type": "Automatic Differentiation", "text": ["Context-manager that sets a pair of pack / unpack hooks for saved tensors.", "Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.", "In that context, the pack_hook function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using save_for_backward() but also those recorded by a PyTorch-defined operation). The output of pack_hook is then stored in the computation graph instead of the original tensor.", "The unpack_hook is called when the saved tensor needs to be accessed, namely when executing torch.Tensor.backward() or torch.autograd.grad(). It takes as argument the packed object returned by pack_hook and should return a tensor which has the same content as the original tensor (passed as input to the corresponding pack_hook).", "The hooks should have the following signatures:", "pack_hook(tensor: Tensor) -> Any", "unpack_hook(Any) -> Tensor", "where the return value of pack_hook is a valid input to unpack_hook.", "In general, you want unpack_hook(pack_hook(t)) to be equal to t in terms of value, size, dtype and device.", "Example:", "Warning", "Performing an inplace operation on the input to either hooks may lead to undefined behavior.", "Warning", "Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied."]}, {"name": "torch.autograd.profiler.emit_itt", "path": "autograd#torch.autograd.profiler.emit_itt", "type": "Automatic Differentiation", "text": ["Context manager that makes every autograd operation emit an ITT range.", "It is useful when running the program under Intel(R) VTune Profiler:", "The Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI."]}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "Automatic Differentiation", "text": ["Context manager that makes every autograd operation emit an NVTX range.", "It is useful when running the program under nvprof:", "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.", "Forward-backward correlation", "When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates.", "During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function.", "Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass.", "Double-backward", "If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass."]}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "generated/torch.autograd.profiler.load_nvprof#torch.autograd.profiler.load_nvprof", "type": "Automatic Differentiation", "text": ["Opens an nvprof trace file and parses autograd annotations.", "path (str) \u2013 path to nvprof trace"]}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "Automatic Differentiation", "text": ["Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks"]}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "generated/torch.autograd.profiler.profile.export_chrome_trace#torch.autograd.profiler.profile.export_chrome_trace", "type": "Automatic Differentiation", "text": ["Exports an EventList as a Chrome tracing tools file.", "The checkpoint can be later loaded and inspected under chrome://tracing URL.", "path (str) \u2013 Path where the trace will be written."]}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "generated/torch.autograd.profiler.profile.key_averages#torch.autograd.profiler.profile.key_averages", "type": "Automatic Differentiation", "text": ["Averages all function events over their keys.", "An EventList containing FunctionEventAvg objects."]}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total", "path": "generated/torch.autograd.profiler.profile.self_cpu_time_total#torch.autograd.profiler.profile.self_cpu_time_total", "type": "Automatic Differentiation", "text": ["Returns total time spent on CPU obtained as a sum of all self times across all the events."]}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "generated/torch.autograd.profiler.profile.total_average#torch.autograd.profiler.profile.total_average", "type": "Automatic Differentiation", "text": ["Averages all events.", "A FunctionEventAvg object."]}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "Automatic Differentiation", "text": ["Context-manager that sets the anomaly detection for the autograd engine on or off.", "set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function.", "See detect_anomaly above for details of the anomaly detection behaviour."]}, {"name": "torch.backends", "path": "backends", "type": "Backends", "text": ["torch.backends controls the behavior of various backends that PyTorch supports.", "These backends include:", "Returns cpu capability as a string value.", "Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d", "str", "Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.", "A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "A bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs.", "A bool that controls whether reduced precision reductions are allowed with bf16 GEMMs.", "cufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].", "A readonly int that shows the number of plans currently in a cuFFT plan cache.", "A int that controls the capacity of a cuFFT plan cache.", "Clears a cuFFT plan cache.", "Warning", "This flag is experimental and subject to change.", "When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.", "Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs.", "Currently supported linalg operators:", "_LinalgBackend", "Enum class for the scaled dot product attention backends.", "Warning", "This class is in beta and subject to change.", "This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h", "Warning", "This flag is beta and subject to change.", "Returns whether flash scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables memory efficient scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "Returns whether memory efficient scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables flash scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "Returns whether math scaled dot product attention is enabled or not.", "Warning", "This flag is beta and subject to change.", "Enables or disables math scaled dot product attention.", "Warning", "This flag is beta and subject to change.", "This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.", "Returns the version of cuDNN", "Returns a bool indicating if CUDNN is currently available.", "A bool that controls whether cuDNN is enabled.", "A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices.", "A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms().", "A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.", "A int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.", "Returns a bool indicating if MPS is currently available.", "bool", "Returns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.", "bool", "Returns whether PyTorch is built with MKL support.", "On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing", "Returns whether PyTorch is built with MKL-DNN support.", "On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation", "Returns whether PyTorch is built with OpenMP support.", "Returns a bool indicating if opt_einsum is currently available.", "bool", "Returns the opt_einsum package if opt_einsum is currently available, else None.", "Any", "A :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.", "If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.", "A :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)."]}, {"name": "torch.backends.cpu.get_cpu_capability()", "path": "backends#torch.backends.cpu.get_cpu_capability", "type": "Backends", "text": ["Returns cpu capability as a string value.", "Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d", "str"]}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "Backends", "text": ["cufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].", "A readonly int that shows the number of plans currently in a cuFFT plan cache.", "A int that controls the capacity of a cuFFT plan cache.", "Clears a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.clear()", "path": "backends#torch.backends.cuda.cufft_plan_cache.clear", "type": "Backends", "text": ["Clears a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.max_size", "path": "backends#torch.backends.cuda.cufft_plan_cache.max_size", "type": "Backends", "text": ["A int that controls the capacity of a cuFFT plan cache."]}, {"name": "torch.backends.cuda.cufft_plan_cache.size", "path": "backends#torch.backends.cuda.cufft_plan_cache.size", "type": "Backends", "text": ["A readonly int that shows the number of plans currently in a cuFFT plan cache."]}, {"name": "torch.backends.cuda.enable_flash_sdp()", "path": "backends#torch.backends.cuda.enable_flash_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables flash scaled dot product attention."]}, {"name": "torch.backends.cuda.enable_math_sdp()", "path": "backends#torch.backends.cuda.enable_math_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables math scaled dot product attention."]}, {"name": "torch.backends.cuda.enable_mem_efficient_sdp()", "path": "backends#torch.backends.cuda.enable_mem_efficient_sdp", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Enables or disables memory efficient scaled dot product attention."]}, {"name": "torch.backends.cuda.flash_sdp_enabled()", "path": "backends#torch.backends.cuda.flash_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether flash scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "Backends", "text": ["Returns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it."]}, {"name": "torch.backends.cuda.math_sdp_enabled()", "path": "backends#torch.backends.cuda.math_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether math scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "type": "Backends", "text": ["A bool that controls whether reduced precision reductions are allowed with bf16 GEMMs."]}, {"name": "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "type": "Backends", "text": ["A bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs."]}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "Backends", "text": ["A bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cuda.mem_efficient_sdp_enabled()", "path": "backends#torch.backends.cuda.mem_efficient_sdp_enabled", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "Returns whether memory efficient scaled dot product attention is enabled or not."]}, {"name": "torch.backends.cuda.preferred_linalg_library()", "path": "backends#torch.backends.cuda.preferred_linalg_library", "type": "Backends", "text": ["Warning", "This flag is experimental and subject to change.", "When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.", "Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs.", "Currently supported linalg operators:", "_LinalgBackend"]}, {"name": "torch.backends.cuda.sdp_kernel()", "path": "backends#torch.backends.cuda.sdp_kernel", "type": "Backends", "text": ["Warning", "This flag is beta and subject to change.", "This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored."]}, {"name": "torch.backends.cuda.SDPBackend", "path": "backends#torch.backends.cuda.SDPBackend", "type": "Backends", "text": ["Enum class for the scaled dot product attention backends.", "Warning", "This class is in beta and subject to change.", "This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h"]}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "Backends", "text": ["A bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices."]}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "Backends", "text": ["A bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest."]}, {"name": "torch.backends.cudnn.benchmark_limit", "path": "backends#torch.backends.cudnn.benchmark_limit", "type": "Backends", "text": ["A int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API."]}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "Backends", "text": ["A bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms()."]}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "Backends", "text": ["A bool that controls whether cuDNN is enabled."]}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "Backends", "text": ["Returns a bool indicating if CUDNN is currently available."]}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "Backends", "text": ["Returns the version of cuDNN"]}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with MKL support."]}, {"name": "torch.backends.mkl.verbose", "path": "backends#torch.backends.mkl.verbose", "type": "Backends", "text": ["On-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing"]}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with MKL-DNN support."]}, {"name": "torch.backends.mkldnn.verbose", "path": "backends#torch.backends.mkldnn.verbose", "type": "Backends", "text": ["On-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.", "level \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation"]}, {"name": "torch.backends.mps.is_available()", "path": "backends#torch.backends.mps.is_available", "type": "Backends", "text": ["Returns a bool indicating if MPS is currently available.", "bool"]}, {"name": "torch.backends.mps.is_built()", "path": "backends#torch.backends.mps.is_built", "type": "Backends", "text": ["Returns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.", "bool"]}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "Backends", "text": ["Returns whether PyTorch is built with OpenMP support."]}, {"name": "torch.backends.opt_einsum.enabled", "path": "backends#torch.backends.opt_einsum.enabled", "type": "Backends", "text": ["A :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance.", "If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right."]}, {"name": "torch.backends.opt_einsum.get_opt_einsum()", "path": "backends#torch.backends.opt_einsum.get_opt_einsum", "type": "Backends", "text": ["Returns the opt_einsum package if opt_einsum is currently available, else None.", "Any"]}, {"name": "torch.backends.opt_einsum.is_available()", "path": "backends#torch.backends.opt_einsum.is_available", "type": "Backends", "text": ["Returns a bool indicating if opt_einsum is currently available.", "bool"]}, {"name": "torch.backends.opt_einsum.strategy", "path": "backends#torch.backends.opt_einsum.strategy", "type": "Backends", "text": ["A :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html)."]}, {"name": "torch.baddbmm", "path": "generated/torch.baddbmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result.", "batch1 and batch2 must be 3-D tensors each containing the same number of matrices.", "If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().", "If beta is 0, then input will be ignored, and nan and inf in it will not be propagated.", "For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Example:"]}, {"name": "torch.bartlett_window", "path": "generated/torch.bartlett_window", "type": "Torch", "text": ["Bartlett window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window", "Tensor"]}, {"name": "torch.bernoulli", "path": "generated/torch.bernoulli", "type": "Torch", "text": ["Draws binary random numbers (0 or 1) from a Bernoulli distribution.", "The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1.", "The ith\\text{i}^{th} element of the output tensor will draw a value 11 according to the ith\\text{i}^{th} probability value given in input.", "The returned out tensor only has values 0 or 1 and is of the same shape as input.", "out can have integral dtype, but input must have floating point dtype.", "input (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution", "Example:"]}, {"name": "torch.BFloat16Storage", "path": "storage#torch.BFloat16Storage", "type": "Storage", "text": []}, {"name": "torch.BFloat16Storage.dtype", "path": "storage#torch.BFloat16Storage.dtype", "type": "Storage", "text": []}, {"name": "torch.bincount", "path": "generated/torch.bincount", "type": "Torch", "text": ["Count the frequency of each value in an array of non-negative ints.", "The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.", "Note", "This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.", "a tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)", "output (Tensor)", "Example:"]}, {"name": "torch.bitwise_and", "path": "generated/torch.bitwise_and", "type": "Torch", "text": ["Computes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_left_shift", "path": "generated/torch.bitwise_left_shift", "type": "Torch", "text": ["Computes the left arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion.", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_not", "path": "generated/torch.bitwise_not", "type": "Torch", "text": ["Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_or", "path": "generated/torch.bitwise_or", "type": "Torch", "text": ["Computes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_right_shift", "path": "generated/torch.bitwise_right_shift", "type": "Torch", "text": ["Computes the right arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion. In any case, if the value of the right operand is negative or is greater or equal to the number of bits in the promoted left operand, the behavior is undefined.", "The operation applied is:", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.bitwise_xor", "path": "generated/torch.bitwise_xor", "type": "Torch", "text": ["Computes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.blackman_window", "path": "generated/torch.blackman_window", "type": "Torch", "text": ["Blackman window function.", "where NN is the full window size.", "The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).", "Note", "If window_length =1=1, the returned window contains a single value 1.", "A 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window", "Tensor"]}, {"name": "torch.block_diag", "path": "generated/torch.block_diag", "type": "Torch", "text": ["Create a block diagonal matrix from provided tensors.", "*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.", "A 2 dimensional tensor with all the input tensors arranged in order such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.", "Tensor", "Example:"]}, {"name": "torch.bmm", "path": "generated/torch.bmm", "type": "Torch", "text": ["Performs a batch matrix-matrix product of matrices stored in input and mat2.", "input and mat2 must be 3-D tensors each containing the same number of matrices.", "If input is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor.", "This operator supports TensorFloat32.", "On certain ROCm devices, when using float16 inputs this module will use different precision for backward.", "Note", "This function does not broadcast. For broadcasting matrix products, see torch.matmul().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.BoolStorage", "path": "storage#torch.BoolStorage", "type": "Storage", "text": []}, {"name": "torch.BoolStorage.dtype", "path": "storage#torch.BoolStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.broadcast_shapes", "path": "generated/torch.broadcast_shapes", "type": "Torch", "text": ["Similar to broadcast_tensors() but for shapes.", "This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices.", "Example:", "*shapes (torch.Size) \u2013 Shapes of tensors.", "A shape compatible with all input shapes.", "shape (torch.Size)", "RuntimeError \u2013 If shapes are incompatible."]}, {"name": "torch.broadcast_tensors", "path": "generated/torch.broadcast_tensors", "type": "Torch", "text": ["Broadcasts the given tensors according to Broadcasting semantics.", "*tensors \u2013 any number of tensors of the same type", "Warning", "More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.", "Example:"]}, {"name": "torch.broadcast_to", "path": "generated/torch.broadcast_to", "type": "Torch", "text": ["Broadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.", "Example:"]}, {"name": "torch.bucketize", "path": "generated/torch.bucketize", "type": "Torch", "text": ["Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is open. Note that this behavior is opposite the behavior of numpy.digitize. More formally, the returned index satisfies the following rules:", "right", "returned index satisfies", "False", "boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]", "True", "boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]", "Example:"]}, {"name": "torch.ByteStorage", "path": "storage#torch.ByteStorage", "type": "Storage", "text": []}, {"name": "torch.ByteStorage.dtype", "path": "storage#torch.ByteStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.can_cast", "path": "generated/torch.can_cast", "type": "Torch", "text": ["Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.", "Example:"]}, {"name": "torch.cartesian_prod", "path": "generated/torch.cartesian_prod", "type": "Torch", "text": ["Do cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.", "*tensors (Tensor) \u2013 any number of 1 dimensional tensors.", "A tensor equivalent to converting all the input tensors into lists, do itertools.product on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.cat", "path": "generated/torch.cat", "type": "Torch", "text": ["Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.", "torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk().", "torch.cat() can be best understood via examples.", "See also", "torch.stack() concatenates the given sequence along a new dimension.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cdist", "path": "generated/torch.cdist", "type": "Torch", "text": ["Computes batched the p-norm distance between each pair of the two collections of row vectors.", "Tensor", "If x1 has shape B\u00d7P\u00d7MB \\times P \\times M and x2 has shape B\u00d7R\u00d7MB \\times R \\times M then the output will have shape B\u00d7P\u00d7RB \\times P \\times R.", "This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty). When p=0p = 0 it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty, the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())."]}, {"name": "torch.ceil", "path": "generated/torch.ceil", "type": "Torch", "text": ["Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.chain_matmul", "path": "generated/torch.chain_matmul", "type": "Torch", "text": ["Returns the matrix product of the NN 2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN is 1, then this is a no-op - the original matrix is returned as is.", "Warning", "torch.chain_matmul() is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot() instead, which accepts a list of two or more tensors rather than multiple arguments.", "if the ithi^{th} tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}, then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}.", "Tensor", "Example:"]}, {"name": "torch.CharStorage", "path": "storage#torch.CharStorage", "type": "Storage", "text": []}, {"name": "torch.CharStorage.dtype", "path": "storage#torch.CharStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.cholesky", "path": "generated/torch.cholesky", "type": "Torch", "text": ["Computes the Cholesky decomposition of a symmetric positive-definite matrix AA or for batches of symmetric positive-definite matrices.", "If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:", "If upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:", "If upper is True, and AA is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.", "Warning", "torch.cholesky() is deprecated in favor of torch.linalg.cholesky() and will be removed in a future PyTorch release.", "L = torch.cholesky(A) should be replaced with", "U = torch.cholesky(A, upper=True) should be replaced with", "This transform will produce equivalent results for all valid (symmetric positive definite) inputs.", "out (Tensor, optional) \u2013 the output matrix", "Example:"]}, {"name": "torch.cholesky_inverse", "path": "generated/torch.cholesky_inverse", "type": "Torch", "text": ["Computes the inverse of a symmetric positive-definite matrix AA using its Cholesky factor uu: returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines).", "If upper is False, uu is lower triangular such that the returned tensor is", "If upper is True or not provided, uu is upper triangular such that the returned tensor is", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if AA is a batch of matrices then the output has the same batch dimensions.", "out (Tensor, optional) \u2013 the output tensor for inv", "Example:"]}, {"name": "torch.cholesky_solve", "path": "generated/torch.cholesky_solve", "type": "Torch", "text": ["Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu.", "If upper is False, uu is and lower triangular and c is returned such that:", "If upper is True or not provided, uu is upper triangular and c is returned such that:", "torch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c", "Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.", "out (Tensor, optional) \u2013 the output tensor for c", "Example:"]}, {"name": "torch.chunk", "path": "generated/torch.chunk", "type": "Torch", "text": ["Attempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor.", "Note", "This function may return fewer than the specified number of chunks!", "See also", "torch.tensor_split() a function that always returns exactly the specified number of chunks", "If the tensor size along the given dimension dim is divisible by chunks, all returned chunks will be the same size. If the tensor size along the given dimension dim is not divisible by chunks, all returned chunks will be the same size, except the last one. If such division is not possible, this function may return fewer than the specified number of chunks."]}, {"name": "torch.clamp", "path": "generated/torch.clamp", "type": "Torch", "text": ["Clamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:", "If min is None, there is no lower bound. Or, if max is None there is no upper bound.", "Note", "If min is greater than max torch.clamp(..., min, max) sets all elements in input to the value of max.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.clip", "path": "generated/torch.clip", "type": "Torch", "text": ["Alias for torch.clamp()."]}, {"name": "torch.clone", "path": "generated/torch.clone", "type": "Torch", "text": ["Returns a copy of input.", "Note", "This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().", "input (Tensor) \u2013 the input tensor.", "memory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format."]}, {"name": "torch.column_stack", "path": "generated/torch.column_stack", "type": "Torch", "text": ["Creates a new tensor by horizontally stacking the tensors in tensors.", "Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.combinations", "path": "generated/torch.combinations", "type": "Torch", "text": ["Compute combinations of length rr of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.", "A tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.", "Tensor", "Example:"]}, {"name": "torch.compile", "path": "generated/torch.compile", "type": "Torch", "text": ["Optimizes given model/function using TorchDynamo and specified backend.", "Concretely, for every frame executed within the compiled region, we will attempt to compile it and cache the compiled result on the code object for future use. A single frame may be compiled multiple times if previous compiled results are not applicable for subsequent calls (this is called a \u201cguard failure), you can use TORCH_LOGS=guards to debug these situations. Multiple compiled results can be associated with a frame up to torch._dynamo.config.cache_size_limit, which defaults to 64; at which point we will fall back to eager. Note that compile caches are per code object, not frame; if you dynamically create multiple copies of a function, they will all share the same code cache.", "backend (str or Callable) \u2013 ", "backend to be used", "mode (str) \u2013 ", "Can be either \u201cdefault\u201d, \u201creduce-overhead\u201d, \u201cmax-autotune\u201d or \u201cmax-autotune-no-cudagraphs\u201d", "options (dict) \u2013 ", "A dictionary of options to pass to the backend. Some notable ones to try out are", "Callable", "Example:"]}, {"name": "torch.compiled_with_cxx11_abi", "path": "generated/torch.compiled_with_cxx11_abi", "type": "Torch", "text": ["Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1", "bool"]}, {"name": "torch.compiler", "path": "torch.compiler", "type": "Miscellaneous", "text": ["torch.compiler is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is torch.compile.", "torch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. torch.compile is written in Python and it marks the transition of PyTorch from C++ to Python.", "torch.compile leverages the following underlying technologies:", "Note", "In some cases, the terms torch.compile, TorchDynamo, torch.compiler might be used interchangeably in this documentation.", "As mentioned above, to run your workflows faster, torch.compile through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as inductor, TorchDynamo has a list of supported backends developed by our partners, which can be see by running torch.compiler.list_backends() each of which with its optional dependencies.", "Some of the most commonly used backends include:", "Training & inference backends", "Backend", "Description", "torch.compile(m, backend=\"inductor\")", "Uses the TorchInductor backend. Read more", "torch.compile(m, backend=\"cudagraphs\")", "CUDA graphs with AOT Autograd. Read more", "torch.compile(m, backend=\"ipex\")", "Uses IPEX on CPU. Read more", "torch.compile(m, backend=\"onnxrt\")", "Uses ONNX Runtime for training on CPU/GPU. Read more", "Inference-only backends", "Backend", "Description", "torch.compile(m, backend=\"tensorrt\")", "Uses ONNX Runtime to run TensorRT for inference optimizations. Read more", "torch.compile(m, backend=\"ipex\")", "Uses IPEX for inference on CPU. Read more", "torch.compile(m, backend=\"tvm\")", "Uses Apache TVM for inference optimizations. Read more", "Getting Started for PyTorch Users", "Deep Dive for PyTorch Developers", "HowTo for PyTorch Backend Vendors"]}, {"name": "torch.compiler.allow_in_graph()", "path": "generated/torch.compiler.allow_in_graph#torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": ["Customize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions", "fn \u2013 A callable representing the function to be included in the graph.", "Warning", "allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues."]}, {"name": "torch.compiler.assume_constant_result()", "path": "generated/torch.compiler.assume_constant_result#torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": ["This function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn", "fn \u2013 The function to be marked as having a constant result.", "Warning", "assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not"]}, {"name": "torch.compiler.Best Practices for Backends", "path": "torch.compiler_best_practices_for_backends", "type": "Miscellaneous", "text": ["Compiled workloads on modern x86 CPUs are usually optimized by Single Instruction Multiple Data (SIMD) instruction sets. SIMD is a typical parallel processing technique for high performance computing, such as deep learning model training and inference. With SIMD applied, each compute unit performs the same instruction with different allocated data at any given time slot. The most commonly deployed x86 instruction set architectures (ISAs) enabling SIMD include AVX, AVX2, AVX-512 and AMX.", "You can check supported ISAs for your machine by using the collect_env script. As the script provides complete environment information for PyTorch, we can use grep to extract the line containing ISA information:", "Normally, if AVX-512 is supported, instructions start with \u201cavx512\u201d (like avx512f, avx512bw, avx512_vnni) should be observed. If AMX is supported, instructions start with \u201camx\u201d (like amx_tile, amx_bf16, amx_int8) should be observed.", "Specifically, with a server having AMX instructions enabled, workloads performance can be further boosted by leveraging AMX."]}, {"name": "torch.compiler.compile()", "path": "generated/torch.compiler.compile#torch.compiler.compile", "type": "Miscellaneous", "text": ["See torch.compile() for details on the arguments for this function."]}, {"name": "torch.compiler.CUDAGraph Trees", "path": "torch.compiler_cudagraph_trees", "type": "Miscellaneous", "text": ["For a longer background on CUDAGraphs, read accelerating pytorch with CUDAGraphs.", "CUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads.", "CUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.", "PyTorch provides a convenience wrapper around CUDAGraphs that handles a couple of tricky interactions with PyTorch\u2019s caching allocator.", "The CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs.", "Using a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both.", "Make Graphed Callables is a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth.", "Running with cudagraph_trees=False does not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward.", "Like Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let\u2019s take a look at an illustrative example:", "In this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4.", "We share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.", "All of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3?", "Graph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph.", "First, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation.", "The second time we hit graph 3 we are warmed up and ready to record. We record graph 2 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!", "Because CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation.", "Let\u2019s say we are benchmarking running inference with the following code:", "In the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. Similarly, in CUDA Graph Trees, naively, the live output of the first run would force a dependency between the first run and the second run, and we would never hit the optimized cudagraph replay invocation. CUDA Graph Trees will ignore outputs from a previous run of torch.compile and not force a memory dependency. In training, we will not ignore outputs from a previous run of torch.compile if we have pending backwards that have not been invoked. TODO - add API to increment generation manually, error on access of prior storage", "Footguns", "Separate CudaGraph", "CUDAGraph Trees", "Memory Can Increase", "On each graph compilation (new sizes, etc.)", "If you are also running non-cudagraph memory", "Recordings", "On any new invocation of a graph", "Will re-record on any new, unique path you take through your program", "Footguns", "Invocation of one graph will overwrite prior invocation", "Cannot persist memory between separate runs through your model - one training loop training, or one run of inference"]}, {"name": "torch.compiler.Custom Backends", "path": "torch.compiler_custom_backends", "type": "Miscellaneous", "text": ["torch.compile provides a straightforward method to enable users to define custom backends.", "A backend function has the contract (gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable.", "Backend functions can be called by TorchDynamo, the graph tracing component of torch.compile, after tracing an FX graph and are expected to return a compiled function that is equivalent to the traced FX graph. The returned callable should have the same contract as the forward function of the original torch.fx.GraphModule passed into the backend: (*args: torch.Tensor) -> List[torch.Tensor].", "In order for TorchDynamo to call your backend, pass your backend function as the backend kwarg in torch.compile. For example,", "See below for more examples.", "You can register your backend using the register_backend decorator, for example,", "Besides the register_backend decorator, if your backend is in another python package, you could also register your backend through entry points of python package, which provides a way for a package to register a plugin for another one.", "Hint", "You can learn more about entry_points in the python packaging documentation.", "To register your backend through entry_points, you could add your backend function to the torch_dynamo_backends entry point group in the setup.py file of your package like:", "Please replace the my_compiler before = to the name of your backend\u2019s name and replace the part after = to the module and function name of your backend function. The entry point will be added to your python environment after the installation of the package. When you call torch.compile(model, backend=\"my_compiler\"), PyTorch would first search the backend named my_compiler that has been registered with register_backend. If not found, it will continue to search in all backends registered via entry_points.", "Registration serves two purposes:", "It is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo. This is useful for 2 main reasons:", "Wrap your backend with torch._dynamo.optimizations.training.aot_autograd and use torch.compile with the backend kwarg as before. Backend functions wrapped by aot_autograd should have the same contract as before.", "Backend functions are passed to aot_autograd through the fw_compiler (forward compiler) or bw_compiler (backward compiler) kwargs. If bw_compiler is not specified, the backward compile function defaults to the forward compile function.", "One caveat is that AOTAutograd requires compiled functions returned by backends to be \u201cboxed\u201d. This can be done by wrapping the compiled function with functorch.compile.make_boxed_func.", "For example,", "If you want to better understand what is going on during a compilation, you can create a custom compiler, which is referred to as backend in this section, that will print pretty print the fx GraphModule extracted from Dynamo\u2019s bytecode analysis and return a forward() callable.", "For example:", "Running the above example produces the following output:", "This works for torch.nn.Module as well as shown below:", "Let\u2019s take a look at one more example with control flow:", "Running this example produces the following output:", "The order of the last two graphs is nondeterministic depending on which one is encountered first by the just-in-time compiler.", "Integrating a custom backend that offers superior performance is also easy and we\u2019ll integrate a real one with optimize_for_inference:", "And then you should be able to optimize any existing code with:", "TorchDynamo includes many backends, which can be found in backends.py or torch._dynamo.list_backends(). You can combine these backends together with the following code:"]}, {"name": "torch.compiler.disable()", "path": "generated/torch.compiler.disable#torch.compiler.disable", "type": "Miscellaneous", "text": ["This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.Frequently Asked Questions", "path": "torch.compiler_faq", "type": "Miscellaneous", "text": ["Author: Mark Saroufim", "torch.compile supports training, using AOTAutograd to capture backwards:", "torch.compile supports DistributedDataParallel (DDP). Support for other distributed training libraries is being considered.", "The main reason why Distributed code is challenging with dynamo is because AOTAutograd unrolls both the forward and backward pass and provides 2 graphs for backends to optimize. This is a problem for distributed code because we\u2019d like to ideally overlap communication operations with computations. Eager pytorch accomplishes this in different ways for DDP/FSDP- using autograd hooks, module hooks, and modifications/mutations of module states. In a naive application of dynamo, hooks that should run directly after an operation during backwards may be delayed until after the entire compiled region of backwards ops, due to how AOTAutograd compiled functions interact with dispatcher hooks.", "The basic strategy for optimizing DDP with Dynamo is outlined in distributed.py where the main idea will be to graph break on DDP bucket boundaries.", "When each node in DDP needs to synchronize its weights with the other nodes it organizes its gradients and parameters into buckets which reduces communication times and allows a node to broadcast a fraction of its gradients to other waiting nodes.", "Graph breaks in distributed code mean you can expect dynamo and its backends to optimize the compute overhead of a distributed program but not its communication overhead. Graph-breaks may interfere with compilation speedups, if the reduced graph-size robs the compiler of fusion opportunities. However, there are diminishing returns with increasing graph size since most of the current compute optimizations are local fusions. So in practice this approach may be sufficient.", "For the vast majority of models you probably don\u2019t and you can use torch.compile() as is but there are a few situations where full graphs are necessary and you can can ensure a full graph by simply running torch.compile(..., nopython=True). These situations include:", "Future work will include tracing communication operations into graphs, coordinating these operations with compute optimizations, and optimizing the communication operations.", "If your code ran just fine without torch.compile and started to crash with it is enabled, then the most important first step is figuring out which part of the stack your failure occurred. To troubleshoot that, follow the steps below and only try the next step if the previous one succeeded.", "In some cases, you may not want unexpected compiles after a program has warmed up. For example, if you are serving production traffic in a latency critical application. For this, TorchDynamo provides an alternate mode where prior compiled graphs are used, but no new ones are generated:", "There are 3 major ways to accelerate PyTorch code:", "The above are general principles for accelerating PyTorch code but different backends will each make different tradeoffs on what to optimize. For example Inductor first takes care of fusing whatever it can and only then generates Triton kernels. It can also", "Triton in addition offers speedups because of automatic memory coalescing, memory management and scheduling within each Streaming Multiprocessor and has been designed to handle tiled computations.", "However, regardless of the backend you use it\u2019s best to use a benchmark and see approach so try out the PyTorch profiler, visually inspect the generated kernels and try to see what\u2019s going on for yourself.", "The main reason you won\u2019t see the speedups you\u2019d like to by using dynamo is excessive graph breaks. So what\u2019s a graph break?", "Given a program like:", "Torchdynamo will attempt to compile all of the torch/tensor operations within some_fun() into a single FX graph, but it may fail to capture everything into one graph.", "Some graph break reasons are insurmountable to TorchDynamo like calling into a C extension other than PyTorch is invisible to TorchDynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse.", "To maximize performance, it\u2019s important to have as few graph breaks as possible.", "To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage:", "To throw an error on the first graph break encountered you can use disable python fallback by using nopython=True, this should be familiar if you\u2019ve worked with export based compilers.", "If you enabled dynamic shapes by setting env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py then your code won\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes which avoids recompilations in the case when shapes vary by less than a factor of 2. This is especially useful in scenarios like varying image sizes in CV or variable sequence length in NLP. In inference scenarios it\u2019s often not possible to know what a batch size will be beforehand because you take what you can get from different client apps.", "In general, TorchDynamo tries very hard not to recompile things unnecessarily so if for example TorchDynamo finds 3 graphs and your change only modified one graph then only that graph will recompile. So another tip to avoid potentially slow compilation times is to warmup a model by compiling it once after which subsequent compilations will be much faster. Cold start compile times is still a metric we track visibly.", "Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler.", "If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True", "Dynamo is still an alpha product so there\u2019s a few sources of OOMs and if you\u2019re seeing an OOM try disabling the following configurations in this order and then open an issue on GitHub so we can solve the root problem 1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled them by default: env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py 2. CUDA graphs with Triton are enabled by default in inductor but removing them may alleviate some OOM issues: torch._inductor.config.triton.cudagraphs = False.", "Applying a torch.func transform to a function that uses torch.compile does not work:", "This code will not work. There is an issue that you can track for this.", "As a workaround, use torch.compile outside of the torch.func function:", "Note", "This is an experimental feature and can be used by setting torch._dynamo.config.capture_func_transforms=True", "There are currently a few cases which are not supported and lead to graph breaks (that is, torch.compile falls back to eager-mode PyTorch on these). We are working on improving the situation for the next release (PyTorch 2.2)", "1. The inputs and outputs of the function being transformed over must be tensors. We do not yet support things like tuple of Tensors.", "3. Functions with observable side effects. For example, it is OK to mutate a list created in the function, but not OK to mutate a list created outside of the function.", "Note", "\u2018stride\u2019, \u2018requires_grad\u2019, \u2018storage_offset\u2019, \u2018layout\u2019, \u2018data\u2019, \u2018is_coalesced\u2019, \u2018is_complex\u2019, \u2018is_conj\u2019, \u2018is_contiguous\u2019, \u2018is_cpu\u2019, \u2018is_cuda\u2019, \u2018is_distributed\u2019, \u2018is_floating_point\u2019, \u2018is_inference\u2019, \u2018is_ipu\u2019, \u2018is_leaf\u2019, \u2018is_meta\u2019, \u2018is_mkldnn\u2019, \u2018is_mps\u2019, \u2018is_neg\u2019, \u2018is_nested\u2019, \u2018is_nonzero\u2019, \u2018is_ort\u2019, \u2018is_pinned\u2019, \u2018is_quantized\u2019, \u2018is_same_size\u2019, \u2018is_set_to\u2019, \u2018is_shared\u2019, \u2018is_signed\u2019, \u2018is_sparse\u2019, \u2018is_sparse_csr\u2019, \u2018is_vulkan\u2019, \u2018is_xla\u2019, \u2018is_xpu\u2019", "For other transforms, as a workaround, use torch._dynamo.allow_in_graph", "allow_in_graph is an escape hatch. If your code does not work with torch.compile, which introspects Python bytecode, but you believe it will work via a symbolic tracing approach (like jax.jit), then use allow_in_graph.", "By using allow_in_graph to annotate a function, you must make sure your code meets the following requirements:", "A common pitfall is using allow_in_graph to annotate a function that invokes an nn.Module. This is because the outputs now depend on the parameters of the nn.Module. To get this to work, use torch.func.functional_call to extract the module state.", "Starting in 2.1, torch.compile understands native NumPy programs that work on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch to NumPy and back via x.numpy(), torch.from_numpy, and related functions.", "NumPy within torch.compile follows NumPy 2.0 pre-release.", "Generally, torch.compile is able to trace through most NumPy constructions, and when it cannot, it falls back to eager and lets NumPy execute that piece of code. Even then, there are a few features where torch.compile semantics slightly deviate from those of NumPy:", "There are other features for which we do not support tracing and we gracefully fallback to NumPy for their execution:", "Yes you can! To do so, you may simply execute your code within a torch.device(\"cuda\") context. Consider the example", "In this example, numpy_fn will be executed in CUDA. For this to be possible, torch.compile automatically moves X and Y from CPU to CUDA, and then it moves the result Z from CUDA to CPU. If we are executing this function several times in the same program run, we may want to avoid all these rather expensive memory copies. To do so, we just need to tweak our numpy_fn so that it accepts cuda Tensors and returns tensors:", "By doing this, we explicitly create the tensors in CUDA memory, and we keep them there. In this case X.numpy() and from_numpy() are hints to the compiler but no real data movement happens. Note that the original program would not run on eager mode now. If you want to run it in eager mode, you would need to call .numpy(force=True) doing Z = Z.cuda() before returning Z. Of course, doing this would execute the program on eager mode NumPy, and on CPU.", "Debugging JIT compiled code is challenging, given the complexity of modern compilers and the daunting errors that they raise. The tutorial on how to diagnose runtime errors within torch.compile contains a few tips and tricks on how to tackle this task.", "If the above is not enough to pinpoint the origin of the issue, there are still a few other NumPy-specific tools we can use. We can discern whether the bug is entirely in the PyTorch code by disabling tracing through NumPy functions:", "If the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without torch.compile) using PyTorch as a backend by importing import torch._numpy as np. This should just be used for debugging purposes and is in no way a replacement for the PyTorch API, as it is much less performant and, as a private API, may change without notice. At any rate, torch._numpy is a Python implementation of NumPy in terms of PyTorch and it is used internally by torch.compile to transform NumPy code into Pytorch code. It is rather easy to read and modify, so if you find any bug in it feel free to submit a PR fixing it or simply open an issue.", "If the program does work when importing torch._numpy as np, chances are that the bug is in TorchDynamo. If this is the case, please feel open an issue with a minimal reproducer.", "The best place to start is the tutorial with general advice for how to debug these sort of torch.compile issues.", "Some graph breaks may happen because of the use of unsupported features. See Which NumPy features does torch.compile support?. More generally, it is useful to keep in mind that some widely used NumPy features do not play well with compilers. For example, in-place modifications make reasoning difficult within the compiler and often yield worse performance than their out-of-place counterparts.As such, it is best to avoid them. Same goes for the use of the out= parameter. Instead, prefer out-of-place ops and let torch.compile optimize the memory use. Same goes for data-dependent ops like masked indexing through boolean masks, or data-dependent control flow like if or while constructions.", "In some cases, you might need to exclude small parts of your code from the torch.compile compilations. This section provides some of the answers and you can find more information in TorchDynamo APIs for fine-grained tracing.", "Graph break on a function is not enough to sufficiently express what you want PyTorch to do. You need to be more specific about your use case. Some of the most common use cases you might want to consider:", "Some of the uncommon use cases include:", "Disallow-in-graph works at the level of operators, or more specifically, the operators that you see in the TorchDynamo extracted graphs.", "Disable works at the function frame level and decides if TorchDynamo should look into the function frame or not.", "Note", "torch._dynamo_skip is deprecated.", "You most likely need torch._dynamo.disable. But in an unlikely scenario, you might need even finer control. Suppose you want to disable the tracing on just the a_fn function, but want to continue the tracing back in aa_fn and ab_fn. The image below demonstrates this use case:", "In this case, you can use torch._dynamo.disable(recursive=False). In previous versions, this functionality was provided by torch._dynamo.skip. This is now supported by the recursive flag inside torch._dynamo.disable."]}, {"name": "torch.compiler.Getting Started", "path": "torch.compiler_get_started", "type": "Miscellaneous", "text": ["Before you read this section, make sure to read the torch.compiler.", "Let\u2019s start by looking at a simple torch.compile example that demonstrates how to use torch.compile for inference. This example demonstrates the torch.cos() and torch.sin() features which are examples of pointwise operators as they operate element by element on a vector. This example might not show significant performance gains but should help you form an intuitive understanding of how you can use torch.compile in your own programs.", "Note", "To run this script, you need to have at least one GPU on your machine. If you do not have a GPU, you can remove the cuda() code in the snippet below and it will run on CPU.", "A more famous pointwise operator you might want to use would be something like torch.relu(). Pointwise ops in eager mode are suboptimal because each one would need to read a tensor from the memory, make some changes, and then write back those changes. The single most important optimization that inductor performs is fusion. In the example above we can turn 2 reads and 2 writes into 1 read and 1 write which is crucial especially for newer GPUs where the bottleneck is memory bandwidth (how quickly you can send data to a GPU) rather than compute (how quickly your GPU can crunch floating point operations).", "Another major optimization that inductor provides is automatic support for CUDA graphs. CUDA graphs help eliminate the overhead from launching individual kernels from a Python program which is especially relevant for newer GPUs.", "TorchDynamo supports many different backends, but TorchInductor specifically works by generating Triton kernels. Let\u2019s save our example above into a file called example.py. We can inspect the code generated Triton kernels by running TORCH_COMPILE_DEBUG=1 python example.py. As the script executes, you should see DEBUG messages printed to the terminal. Closer to the end of the log, you should see a path to a folder that contains torchinductor_<your_username>. In that folder, you can find the output_code.py file that contains the generated kernel code similar to the following:", "Note", "The above code snippet is an example. Depending on your hardware, you might see different code generated.", "And you can verify that fusing the cos and sin did actually occur because the cos and sin operations occur within a single Triton kernel and the temporary variables are held in registers with very fast access.", "Read more on Triton\u2019s performance here. Because the code is written in Python, it\u2019s fairly easy to understand even if you have not written all that many CUDA kernels.", "Next, let\u2019s try a real model like resnet50 from the PyTorch hub.", "And that is not the only available backend, you can run in a REPL torch.compiler.list_backends() to see all the available backends. Try out the cudagraphs next as inspiration.", "PyTorch users frequently leverage pretrained models from transformers or TIMM and one of the design goals is TorchDynamo and TorchInductor is to work out of the box with any model that people would like to author.", "Let\u2019s download a pretrained model directly from the HuggingFace hub and optimize it:", "If you remove the to(device=\"cuda:0\") from the model and encoded_input, then Triton will generate C++ kernels that will be optimized for running on your CPU. You can inspect both Triton or C++ kernels for BERT. They are more complex than the trigonometry example we tried above but you can similarly skim through it and see if you understand how PyTorch works.", "Similarly, let\u2019s try out a TIMM example:", "In this section, we have reviewed a few inference examples and developed a basic understanding of how torch.compile works. Here is what you check out next:"]}, {"name": "torch.compiler.Guards Overview", "path": "torch.compiler_guards_overview", "type": "Miscellaneous", "text": ["From a UX perspective, TorchDynamo is very easy to use. The user invokes torchdynamo.optimize as an annotation:", "Where a complete example looks like this:", "This allows TorchDynamo to capture the interpreted Python frames, grab any and all relevant information, and speed things up wherever it can. The speedup comes from a few places, and can be rather dependent on the backend (my_compiler in the example above) provided, but the one speedup that is important in this section is caching. Caching itself is not a direct speedup but a critical enablement that prevents recompilation. We dig a hole with dynamo, and caching allows us to get out. It enables us to hold perf neutrality while then enabling backends - the true source of our speedups.", "With even a pass-through no-op backend provided:", "We can see TorchDynamo speeding up Python execution even on regular Python, not just PyTorch.", "TorchDynamo operates through caching transformed (by TorchDynamo) user bytecode. When TorchDynamo receives a frame for evaluation, it checks if the objects referenced in the frame have changed in certain ways, and if not, TorchDynamo reads the previously transformed user bytecode to evaluate it. In this section, we will focus on how we can identify whether or not the objects referenced in the frame have changed. This is a critical piece of functionality in TorchDynamo, because it drives the entire invalidation lifecycle. This functionality is called guards.", "At a very high level, the flow can be summarized like this:", "For the objects captured in (2), TorchDynamo creates tracking objects that are:", "The functionality of TorchDynamo is based on PEP 523.", "TorchDynamo installs a frame evaluation function on Python by using _PyInterpreterState_SetEvalFrameFunc. TorchDynamo has a hook where Python can hand control back to us during evaluation.", "The function we have installed is convert_frame or convert_frame_assert in the nopython=True case, but glossing over that nuance for now, let\u2019s take a look at convert_frame_assert, as convert_frame proxies to it.", "We can find it on line 222 of convert_frame.py,", "with a signature as follows:", "This function wraps the entry point of where Python invokes TorchDynamo with a frame:", "Here is what this function does:", "Passes the frame, alongside a function that creates an InstructionTranslator through bytecode transformation, via transform_code_object. A few crucial things happen under the hood here:", "Now that we have learned about frame evaluation, let\u2019s review InstructionTranslator, and see how it turns the frame we handed it over into TorchDynamo internal types.", "InstructionTranslator does a lot! We won\u2019t cover the details of everything it does, but most importantly for this document, it produces a mapping of symbolic_locals which maintains a mapping from the frame\u2019s f_locals to TorchDynamo internal Variable objects (more on these in a moment. symbolic_locals is filled via traversing the frame\u2019s locals:", "The important component here is the invocation of a call into VariableBuilder. VariableBuilder\u2019s call implementation proxies into a function called _wrap, which in turn both constructs instances of VariableTracker and calls make_guards on them. More on that later.", "This mapping, in turn, is critical as each Variable has associated guards, which are then passed to self.output, the instance of OutputGraph, an fx tracer, mentioned in 4.2 of the section above. If you recall, this OutputGraph, stored in a variable called output is where our guards are stored before being passed on to become GuardedCode", "How does InstructionTranslator do this? At the heart of it, there is a loop that is pumped, which drives a function step.", "step is just that - a single processing step, taking exactly one instruction and doing something with it.", "Note", "These are real instructions processed by TorchDynamo\u2019s transform_code_object, and it is pretty cool.", "Note", "This section purposely skips the details of dis.get_instructions.", "For the example above, here is a snippet of a what a few Instruction's may look like:", "This is the core functionality of this function. Take a look at the opname, and then take a look at this little snippet from inside step;", "As we can see, the function checks if the current class, the InstructionTranslator has an attribute set matching the operator name (for example, LOAD_CONST). If it does, the function invokes it, passing the whole instruction object in. If it does not, the function drops the frame as unimplemented.", "For the LOAD_CONST example, we can see that we do indeed support it, with a relatively straightforward definition:", "We can see that this function creates a new instance of the class ConstantVariable , with a value, in our example case, -1, and then pushes it onto the stack.", "There are dozens of such methods - see symbolic_convert.py for all of them. Generally, we implement as many matching methods to Python bytecode instructions as possible.", "Across both the logic downstream of step and the logic from invoking VariableBuilder - we now have a lot of VariableTrackers and of course, we\u2019ve spoken about creating guards quiet a bit. Let\u2019s dig into what Variables are, and get a little closer to understanding guards.", "A ConstantVariable is an instance of VariableTracker. VariableTracker represents a tracked Python local or stack value.", "When it comes to representing an object inside TorchDynamo, a VariableTracker does exactly what it says - it tracks a given variable. It is an extremely flexible class, but there are a few points to keep in mind:", "It manages the guard relationship around the underlying object through:", "It acts as a proxy on behalf of the underlying object, implementing methods for the rest of TorchDynamo to get information about the tracked object:", "And this class (VariableTracker) is built around subclassing, somewhere between a full Abstract Base Class and fully fleshed out class - it leaves many methods raising NotImplementedError - with reliance on subclasses. See torchdynamo/variables/ for all subclasses to fulfill contracts and custom behaviors.", "Knowing what we know now, we can see an example of how an instruction from dis, BUILD_TUPLE:", "BUILD_TUPLE(count) Creates a tuple consuming count items from the stack, and pushes the resulting tuple onto the stack.", "In our case, our signature will be a little different due to the way we create Instruction objects, but the gist of it will be the same. Instead of passing in count, we pass in an object with a little extra bookkeeping, and of course, we deal with turning regular old python objects into TorchDynamo notions:", "Here is what this code does:", "Note", "Where did the first guards come from? Propagation is a good technique, but we need something created before it can be propagated. VariableBuilder calls make_guards as it creates VariableTracker instances, from f_locals. This in turn calls into the source, to have it create guards.", "After all this, bytecode translation is done and we are one step closer to producing GuardedCode. We now understand how locals become VariableTrackers, how instructions are handled, and where guards are called on for creation. Before we can go into seeing how code and guards are combined into a GuardedCode object, we need to dig a little bit into those make_guard and source.make_guard calls above. We can then understand, what was going on when we made guards alongside, and on, VariableTracker instances.", "Guards are just Python objects, of the class Guard. Let\u2019s look at them in more detail.", "Looking at the definition of the dataclass (and therefore, ctor signature), we see that it has a name, a source, and a create function.", "The name should be the name of the variable.", "The source here is an enum indicating what kind of source the guard belongs to.", "Note", "Not to be confused with Source and the other types in source.py, as stored on VariableTracker.", "create_fn provides the main functionality to transition from a simple dataclass to actually producing valid Python code to be invoked for knowing whether or not things have changed in between invocations, and whether we can safely read from the code cache or not.", "The most common code paths for getting an instance of a guard are through make_guards on VariableTracker. make_guards -> source.make_guard -> return Guard(self.name(), self.guard_source(), fn)", "Or, in a concrete example:", "Since source was set at the construction time of this VariableTracker, all that was needed here was to provide the fn, GuardBuilder.EQUALS_MATCH to the create_fn field.", "This create_fn must be a method on GuardBuilder. The reason for this becomes apparent in our next step. Once we have all the guards created for a frame, we move on to CheckFunctionManager and compile_check_fn.", "Before the convert_frame function can produce a GuardedCode, it needs to run the CheckFunctionManager, with all the guards, to produce a check_fn which will then, in turn get passed in alongside the code into GuardedCode. This is the same check_fn that we store in our cache entry, and the same one we run to know whether or not to retrieve the code stored alongside. For reference, here is that code:", "We now know how a check_fn function is used, and who makes it, and what it is composed of, but what we do not yet know is how. How does a list of Guard objects become a function we can run later on?", "First, we iterate these guards:", "Calling guard.create runs that create_fn we set on the Guard class above (don\u2019t confuse it with the check_fn we are working on producing, the names are similar, so it can get a little confusing). In our example above, our create_fn is GuardBuilder.EQUALS_MATCH. So we are now invoking it, passing in the self, the guard itself, in.", "The signature is: def EQUALS_MATCH(self, guard: Guard):", "And internally to that function, we can use the name on the guard to get back our original object, querying it for data and type information, which in turn gets us to the most important bit: appending code.", "At its simplest, EQUALS_MATCH appends just one line of code: self.code.append(f\"{ref} == {val!r}\"). Where ref is the name of the variable, and val is the value. It might produce code like this:", "This is a basic example. But if we append a few other kinds of GuardBuilder functions and then combine them all with and in between each statement (as we do), we might get something like this:", "Here is what this code performs:", "This becomes the heart of the code our check_fn, which in turn is evaluated the next time we encounter this code. It will then check:", "If all of these are still true, then we can use the code cached alongside this check_fn.", "Note", "For a deeper dive for how and where this happens you can read static PyCodeObject *lookup(CacheEntry *e, PyObject *f_locals) { of _eval_frame.c.", "If not, then, we can move on to recompiling the code anew, and storing that in the cache alongside this code, and a whole new check_fn, again to be checked on yet another subsequent frame.", "There are lots of other such functions on GuardBuilder which get coalesced into, at times massive, strings which then get evaluated as Python code and stored into check_fn. The example above illustrates of a simple case. To understand this functionality better, read the other functions on GuardBuilder, or better yet, dump the code variable in compile_check_fn to see what is getting produced, especially on larger, real models.", "In this section, we have reviewed:", "We covered how user provided code wrapped in a TorchDynamo context goes on to get traced and tracked internally, organized into VariableTrackers Sources and subsequently Guards, and how those Guards in turn guide cache entry selection and invalidation when handing Python code."]}, {"name": "torch.compiler.list_backends()", "path": "generated/torch.compiler.list_backends#torch.compiler.list_backends", "type": "Miscellaneous", "text": ["Return valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).", "exclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.", "List[str]"]}, {"name": "torch.compiler.Profiling to understand torch.compile performance", "path": "torch.compiler_profiling_torch_compile", "type": "Miscellaneous", "text": ["torch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and GPU utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance.", "To understand kernel-level performance, other toosl exist. NVIDIA\u2019s ncu tool can be used, or inductor\u2019s profiling tools.", "See also the general pytorch profiler guide.", "Example program: We\u2019ll use this example of profiling resnet18. Notice the following parts of this example program:", "Viewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the \u201cw\u201d and \u201cs\u201d keys to zoom in and out, and use \u201ca\u201d and \u201cd\u201d to scroll left and right. \u201c?\u201d will show a \u201chelp\u201d screen with a list of shortcuts.", "Here, we observe: * CompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions. * CPU events at the top, and GPU events at the bottom.", "Flows between CPU and GPU events", "Every kernel on the GPU occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. \u201cflows\u201d) between the GPU and CPU events to show which CPU event launched a GPU kernel. This is particularly helpful because, with a few exceptions, GPU kernels are launched asynchronously.", "To view a flow connection, click on a GPU kernel and click \u201cac2g\u201d:", "Alternatively, turn on all flows with the \u201cFlow events\u201d dropdown at the top.", "When CUDA graphs are enabled, some cuda configurations (driver version under 525.85.12 or CUDA < 12) can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program:", "To understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool.", "Note: roughly the same information can also be obtained in non-graphical format with torch._dynamo.utils.compile_times(). This utility won\u2019t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead.", "See an example below:", "Note a few things:", "Although there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying graph breaks.", "When gradients are required for any inputs, graph breaks are easy to identify: each graph break will interrupt a CompiledFunction block, splitting it in two.", "See the synthetic example below for a demonstration:", "One common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:", "This is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes.", "When using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern."]}, {"name": "torch.compiler.PyTorch 2.0 NNModule Support", "path": "torch.compiler_nn_module", "type": "Miscellaneous", "text": ["Author: Will Constable", "torch.compile has special handling for torch.nn.Module objects, tracing them differently than it traces arbitrary python classes, with the intent of producing faster code by making assumptions about the structure.", "This doc describes some of the tradeoffs or edge cases that come up due to this specialization.", "Previously, torch.compile had no support for hooks on nn.Modules, and if hooks were registered they would simply be ignored in the compiled program. Indeed many users do not use nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases for composing nn.Module hooks with torch.compile.", "Hooks that are orchestrated via nn.Module.__call__ implementation include _forward_pre_hooks, forward_hooks, _backward_pre_hooks, and _backward_hooks, and will be referred to as \u2018call hooks\u2019. These hooks are partially supported by torch.compile with limitations described below.", "Another category of hooks includes _state_dict_hooks and its pre and load_ variants, and are still unsupported by torch.compile.", "By default, torch.compile will trace the contents of nn.Module.__call__ which means it will encounter and run forward/pre-forward hooks. If you install hooks before calling torch.compile and then do not remove or alter the hooks later, your use case should be supported by default.", "Backward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo occur when accessing backward_hooks dicts, which is probably avoiable with some work. Graph-breaks also impact the timing of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at the same time. Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would still expect the backward hooks for a series of modules to all fire together after the whole compiled graph\u2019s backward ran.", "hooks on \u2018allowed modules\u2019 torch.compile treats common modules such as torch.conv, as well as modules that are difficult to trace, specially by allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo. For such modules, hooks currently trigger a graph-break so that the affected modules run outside of dynamo. Depending on the model, this could introduce a significant performance regression, and additional work is required to improve this support.", "skip_nnmodule_hook_guards By default, torch._dynamo.config.skip_nnmodule_hook_guards is set to True, meaning no guards will be installed on each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing if any hook dict is changed after compilation.", "If you want to be able to remove or modify hooks after compilation and have torch.compile react appropriately (by recompiling), then you need to set skip_nnmodule_hook_guards=False and expect a runtime penalty for the added guards.", "TODO: confirm if backward/pre_backward hooks are working or not and document accordingly", "State dict hooks have not yet been supported in torch.compile.", "TODO: warn_once if graph-breaking on hooks. warn_once to point to this doc if hooks are present."]}, {"name": "torch.compiler.PyTorch 2.0 Performance Dashboard", "path": "torch.compiler_performance_dashboard", "type": "Miscellaneous", "text": ["Author: Bin Bao and Huy Do", "PyTorch 2.0\u2019s performance is tracked nightly on this dashboard. The performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and a 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be found here.", "The landing page shows tables for all three benchmark suites we measure, TorchBench, Huggingface, and TIMM, and graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP training performance trend in the past 7 days for TorchBench. Droplists on the top of that page can be selected to view tables and graphs with different options. In addition to the pass rate, there are 3 key performance metrics reported there: Geometric mean speedup, Mean compilation time, and Peak memory footprint compression ratio. Both Geometric mean speedup and Peak memory footprint compression ratio are compared against the PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked, which will bring you to a view with detailed numbers for all the tests in that specific benchmark suite.", "All the dashboard tests are defined in this function. The exact test configurations are subject to change, but at the moment, we measure both inference and training performance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor, including default, with_cudagraphs (default + cudagraphs), and dynamic (default + dynamic_shapes).", "Individual dashboard runs can be triggered manually by clicking the Run workflow button here and submitting with your PR\u2019s branch selected. This will kick off a whole dashboard run with your PR\u2019s changes. Once it is done, you can check the results by selecting the corresponding branch name and commit ID on the performance dashboard UI. Be aware that this is an expensive CI run. With the limited resources, please use this functionality wisely.", "The exact command lines used during a complete dashboard run can be found in any recent CI run logs. The workflow page is a good place to look for logs from some of the recent runs. In those logs, you can search for lines like python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda and run them locally if you have a GPU working with PyTorch 2.0. python benchmarks/dynamo/huggingface.py -h will give you a detailed explanation on options of the benchmarking script."]}, {"name": "torch.compiler.PyTorch 2.0 Troubleshooting", "path": "torch.compiler_troubleshooting", "type": "Miscellaneous", "text": ["Author: Michael Lazos", "We are actively developing debug tools, profilers, and improving our error and warning messages. Below is a table of the available tools and their typical usage. For additional help see Diagnosing Runtime Errors.", "Tool", "Purpose", "Usage", "Info logging", "View summarized steps of compilation", "torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\"", "Debug logging", "View detailed steps of compilation (print every instruction traced)", "torch._logging.set_logs(dynamo = logging.DEBUG) and torch._dynamo.config.verbose = True, or TORCH_LOGS=\"+dynamo\" TORCHDYNAMO_VERBOSE=1", "Minifier for any backend", "Find smallest subgraph which reproduces errors for any backend", "set environment variable TORCHDYNAMO_REPRO_AFTER=\"dynamo\"", "Minifier for TorchInductor", "If the error is known to occur after AOTAutograd find smallest subgraph which reproduces errors during TorchInductor lowering", "set environment variable TORCHDYNAMO_REPRO_AFTER=\"aot\"", "Dynamo accuracy minifier", "Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in AOTAutograd", "TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4", "Inductor accuracy minifier", "Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in the backend (e.g., inductor). If this doesn\u2019t work, try the Dynamo accuracy minifier instead.", "TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4", "torch._dynamo.explain", "Find graph breaks and display reasoning for them", "torch._dynamo.explain(fn)(*inputs)", "Record/Replay", "Record and replay frames which to reproduce errors during graph capture", "torch._dynamo.config.replay_record_enabled = True", "TorchDynamo function name filtering", "Only compile functions with the given name to reduce noise when debugging an issue", "set environment variable TORCHDYNAMO_DEBUG_FUNCTION=<name>", "TorchInductor Debug logging", "Print general TorchInductor debug info and generated Triton/C++ code", "torch._inductor.config.debug = True", "TorchInductor Tracing", "Show time taken in each TorchInductor stage + output code and graph visualization", "set the environment variable TORCH_COMPILE_DEBUG=1 or torch._inductor.config.trace.enabled = True", "In addition to info and debug logging, you can use torch._logging for more fine-grained logging.", "At a high level, the TorchDynamo stack consists of a graph capture from Python code (TorchDynamo) and a backend compiler. For example, a backend compiler may consist of backward graph tracing (AOTAutograd) and graph lowering (TorchInductor)*. Errors can occur in any component of the stack and will provide full stack traces.", "To determine in which component an error occurred, you may use info-level logging torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\" and look for Step #: ... outputs. Logs are made at the beginning and end of each step, so the step that an error should correspond to is the most recently logged step whose end has not yet been logged. The steps correspond to the following parts of the stack:", "Step", "Component", "1", "TorchDynamo", "2", "Compiler Backend", "3", "TorchInductor", "If info logging is insufficient, you can use available backend options. These options include:", "The general procedure to narrow down an issue is the following:", "Each of these cases are analyzed in the following sections.", "Note", "The TorchInductor backend consists of both AOTAutograd tracing and the TorchInductor compiler itself. We will disambiguate by referring to TorchInductor as the backend, and TorchInductor lowering as the phase which lowers the graph traced by AOTAutograd.", "If the error that is generated occurs with the \"eager\" backend, then TorchDynamo is most likely the source of the error. Here is a sample code which will generate an error.", "The code above generates the following error:", "As the message suggests you can set torch._dynamo.config.verbose=True to get a full stack trace to both the error in TorchDynamo and the user code. In addition to this flag, you can also set the log_level of TorchDynamo through torch._dynamo.config.log_level. These levels include:", "If a model is very large, the logs can become overwhelming. If an error occurs deep within a model\u2019s Python code, it can be useful to execute only the frame in which the error occurs to enable easier debugging. There are two tools available to enable this:", "If the error does not occur with the \"eager\" backend, then the backend compiler is the source of the error (example error). There are different choices for backend compilers for TorchDynamo, with TorchInductor fitting the needs of most users. This section focuses on TorchInductor as the motivating example, but some tools can also be used with other backend compilers.", "Below is the portion of the stack which we are focusing on:", "With TorchInductor as the chosen backend, AOTAutograd is used to generate the backward graph from the forward graph captured by torchdynamo. It is important to note that errors can occur during this tracing and also while TorchInductor lowers the forward and backward graphs to GPU code or C++. A model can often consist of hundreds or thousands of FX nodes, so narrowing the exact nodes where this problem occurred can be very difficult. Fortunately, there are tools available to automatically minify these input graphs to the nodes which are causing the issue. The first step is to determine whether the error occurs during tracing of the backward graph with AOTAutograd or during TorchInductor lowering. As mentioned above in step 2, the \"aot_eager\" backend can be used to run only AOTAutograd in isolation without lowering. If the error still occurs with this backend, this indicates that the error is occurring during AOTAutograd tracing.", "Here is an example:", "Running this should give you this error with a longer stack trace below it:", "error with full stack trace", "If you then change torch.compile(backend=\"inductor\") to torch.compile(backend=\"aot_eager\"), it will run without error, because the issue is in the TorchInductor lowering process, not in AOTAutograd.", "From here, let\u2019s run the minifier to get a minimal repro. Setting the environment variable TORCHDYNAMO_REPRO_AFTER=\u201caot\u201d (or setting torch._dynamo.config.repro_after=\"aot\" directly) will generate a Python program which reduces the graph produced by AOTAutograd to the smallest subgraph which reproduces the error. (See below for an example where we minify the graph produced by TorchDynamo) Running the program with this environment variable should show nearly identical output, with an additional line indicating where minifier_launcher.py has been written to. The output directory is configurable by setting torch._dynamo.config.base_dir to a valid directory name. The final step is to run the minifier and check that it runs successfully. A successful run looks like this. If the minifier runs successfully, it generates runnable python code which reproduces the exact error. For our example this is the following code:", "The forward method of the Repro module contains the exact op which causes the issue. When filing an issue, please include any minified repros to aid in debugging.", "With backend compilers other than TorchInductor the process for finding the subgraph causing the error is nearly identical to the procedure in errors in TorchInductor with one important caveat. Namely, that the minifier will now be run on the graph that is traced by TorchDynamo, not the output graph of AOTAutograd. Let\u2019s walk through an example.", "In order to run the code after TorchDynamo has traced the forward graph, you can use the TORCHDYNAMO_REPRO_AFTER environment variable. Running this program with TORCHDYNAMO_REPRO_AFTER=\u201cdynamo\u201d (or torch._dynamo.config.repro_after=\"dynamo\") should produce this outputand the following code in {torch._dynamo.config.base_dir}/repro.py.", "Note", "The other option for TORCHDYNAMO_REPRO_AFTER is \"aot\", which will run the minifier after the backward graph has been generated.", "The minifier successfully reduced the graph to the op that raises the error in toy_compiler. The other difference from the procedure in TorchInductor Errors is that the minifier is automatically run after encountering a backend compiler error. After a successful run, the minifier writes repro.py to torch._dynamo.config.base_dir.", "TorchDynamo has a built-in stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by calling torch._dynamo.utils.compile_times() after executing Torch._Dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name.", "TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump. This is a debugging tool designed to make it easier to understand and troubleshoot the internals of TorchInductor.", "Let\u2019s run an example with the following test program (repro.py):", "Setting the environment variable TORCH_COMPILE_DEBUG=1 will cause a debug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug (this can be overridden in the torchdynamo configuration field debug_dir_root and also the env var TORCH_COMPILE_DEBUG_DIR). Inside this directory, each run will have a separate folder named with the timestamp and process id of the run:", "In the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor folder which contains a subfolder for each compiled kernel with inductor debug artifacts.", "Moving further into the torchinductor directory, the \\*.log files are logs from the AOT Autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts.", "Here is a summary of the contents:", "Here are example debug directory contents for the test program:", "Each file in that debug trace can be enabled and disabled through torch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate.", "A single node in this new debug format looks like:", "See the example debug directory output for more examples.", "Given a program like this:", "TorchDynamo will attempt to compile all of the torch/tensor operations within some_fun into a single FX graph, but it may fail to capture everything into one graph.", "Some graph break reasons are insurmountable to TorchDynamo, and can\u2019t be easily fixed. - calling into a C extension other than torch is invisible to torchdynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse. Graph breaks can hinder performance if the resulting fragments are small. To maximize performance, it\u2019s important to have as few graph breaks as possible.", "To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage:", "Outputs include:", "To throw an error on the first graph break encountered, use the nopython mode. This mode disables TorchDynamo\u2019s Python fallback, and only succeeds if the entire program is convertible into a single graph. Example usage:", "When TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up to torch._dynamo.config.cache_size_limit times. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it.", "The compile profiler automates the process of setting TorchDynamo\u2019s cache limit to 1 and running your program under an observation-only \u2018compiler\u2019 that records the causes of any guard failures. You should be sure to run your program for at least as long (as many iterations) as you were running when you ran into trouble, and the profiler will accumulate statistics over this duration.", "If your program exhibits a bounded amount of dynamism, you may be able to tune the TorchDynamo cache limit to allow for each variation to be compiled and cached, but if the cache limit is too high you may find the cost of recompilation outweighs any optimization benefits.", "TorchDynamo plans to support many common cases of dynamic tensor shapes, such as varying batch size or sequence length. It does not plan to support rank-dynamism. In the meantime, setting a specific cache limit can be used in coordination with bucketing techniques to achieve an acceptable number of recompilations for some dynamic models.", "Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler.", "If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True"]}, {"name": "torch.compiler.reset()", "path": "generated/torch.compiler.reset#torch.compiler.reset", "type": "Miscellaneous", "text": ["This function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation"]}, {"name": "torch.compiler.torch.compiler API reference", "path": "torch.compiler_api", "type": "Miscellaneous", "text": ["For a quick overview of torch.compiler, see torch.compiler.", "See torch.compile() for details on the arguments for this function.", "This function clears all compilation caches and restores the system to its initial state.", "Customize which functions compilation will include in the generated graph.", "This function is used to mark a function fn as having a constant result.", "Return valid strings that can be passed to torch.compile(..., backend=\"name\").", "This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.allow_in_graph", "path": "generated/torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": ["Customize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions", "fn \u2013 A callable representing the function to be included in the graph.", "Warning", "allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues."]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.assume_constant_result", "path": "generated/torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": ["This function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn", "fn \u2013 The function to be marked as having a constant result.", "Warning", "assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.compile", "path": "generated/torch.compiler.compile", "type": "Miscellaneous", "text": ["See torch.compile() for details on the arguments for this function."]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.disable", "path": "generated/torch.compiler.disable", "type": "Miscellaneous", "text": ["This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.list_backends", "path": "generated/torch.compiler.list_backends", "type": "Miscellaneous", "text": ["Return valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).", "exclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.", "List[str]"]}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.reset", "path": "generated/torch.compiler.reset", "type": "Miscellaneous", "text": ["This function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation"]}, {"name": "torch.compiler.TorchDynamo APIs for fine-grained tracing", "path": "torch.compiler_fine_grain_apis", "type": "Miscellaneous", "text": ["Note", "In this document torch.compiler.compile and torch.compile are used interchangeably. Both versions will work in your code.", "torch.compile performs TorchDynamo tracing on the whole user model. However, it is possible that a small part of the model code cannot be handeled by torch.compiler. In this case, you might want to disable the compiler on that particular portion, while running compilation on the rest of the model. This section describe the existing APIs that use to define parts of your code in which you want to skip compilation and the relevant use cases.", "The API that you can use to define portions of the code on which you can disable compilation are listed in the following table:", "API", "Description", "When to use?", "torch.compiler.disable", "Disables Dynamo on the decorated function as well as recursively invoked functions.", "Excellent for unblocking a user, if a small portion of the model cannot be handeled with torch.compile.", "torch._dynamo.disallow_in_graph", "Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.nnThis is suitable for the ops, while torch.compiler.disable is suitable for decorating functions.", "This API is excellent for both debugging and unblocking if a custom op like torch.ops.fbgemm.* is causing issues with the torch.compile function.", "torch.compile.allow_in_graph", "The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.nnNote that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamo-level concept.", "This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or autograd.Function. However, each usage of allow_in_graph must be carefully screened (no graph breaks, no closures).", "torch._dynamo.graph_break", "Adds a graph break. The code before and after the graph break goes through TorchDynamo.", "Rarely useful for deployment - If you think you need this, most probably you need either disable or disallow_in_graph.", "torch.compiler.disable disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame.", "TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function fn calls functions a_fn and b_fn. And a_fn calls aa_fn and ab_fn. When you use the PyTorch eager mode rather than torch.compile, these function frames run as is. With torch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color):", "Let\u2019s imagine, that function a_fn is causing troubles with torch.compile. And this is a non-critical portion of the model. You can use compiler.disable on function a_fn. As shown above, TorchDynamo will stop looking at frames originating from the a_fn call (white color indicates original Python behavior).", "To skip compilation, you can decorate the offending function with @torch.compiler.disable.", "You can also use the non-decorator syntax if you don\u2019t want to change the source code However, we recommend that you avoid this style if possible. Here, you have to take care that all users of the original function are now using the patched version.", "torch._dynamo.disallow_in_graph disallows an operator but not the function to be present in the TorchDynamo extracted graph. Note that this is suitable for operators and not general functions as in the case of _dynamo.disable.", "Let\u2019s imagine you compile your model with PyTorch. TorchDynamo is able to extract a graph, but then you see the downstream compiler failing. For example, the meta kernel is missing, or some Autograd dispatch key is set incorrectly for a particular operator. Then you can mark that operator as disallow_in_graph, and TorchDynamo will cause a graph break and run that operator by using the PyTorch eager mode.", "The catch is that you will have to find the corresponding Dynamo level operator, and not the ATen level operator. See more in the Limitations section of the doc.", "Warning", "torch._dynamo.disallow_in_graph is a global flag. If you are comparing different backend compilers, you might have to call allow_in_graph for the disallowed operator when switching to the other compiler.", "torch.compiler.allow_in_graph is useful when the relevant function frame has some known hard-to-support TorchDynamo feature, such as hooks and autograd.Function, and you are confident that downstream PyTorch components such as AOTAutograd can safely trace through the decorated function. When a function is decorated with allow_in_graph, TorchDynamo treats it as a black-box and puts it as is in the generated graph.", "Warning", "allow_in_graph skips TorchDynamo completely on the decorated function omitting all TorchDynamo safety checks, including graph breaks, handling closures, and others. Use allow_in_graph with caution. PyTorch downstream components, such as AOTAutograd rely on TorchDynamo to handle complex Python features, but allow_in_graph bypasses TorchDynamo. Using allow_in_graph could lead to soundness and hard-to-debug issues.", "All the existing APIs are applied at the TorchDynamo level. Therefore, these APIs have visibility to only what TorchDynamo sees. This can lead to confusing scenarios.", "For example, torch._dynamo.disallow_in_graph will not work for ATen operators because they are visible to AOT Autograd. For example, torch._dynamo.disallow_in_graph(torch.ops.aten.add) will not work in the above example."]}, {"name": "torch.compiler.TorchInductor GPU Profiling", "path": "torch.compiler_inductor_profiling", "type": "Miscellaneous", "text": ["This section lists useful commands and workflows that can help you dive into a model\u2019s performance in TorchInductor. When a model is not running as fast as expected, you may want to check individual kernels of the model. Usually, those kernels taking the majority of the GPU time are the most interesting ones. After that, you may also want to run individual kernels directly and inspect its perf. PyTorch provides tools to cover everything mentioned above.", "You can use the following environment variables in your analysis:", "TORCHINDUCTOR_UNIQUE_KERNEL_NAMES", "TORCHINDUCTOR_BENCHMARK_KERNEL", "TORCHINDUCTOR_MAX_AUTOTUNE", "Below are the steps to breakdown execution time of a model into individual kernels. We take mixnet_l as an example.", "Run the benchmark script for the model:", "Note", "The tool relies on kernel name to decide its category. Enabling TORCHINDUCTOR_UNIQUE_KERNEL_NAMES is crucial for that.", "In the output log, look for lines:", "We have one line for each compiled module. If there are no extra graph breaks, we would see 2 such lines in the log, one for the forward graph and one for the backward graph.", "For our example command, we get the following compiled module for the forward and backward graphs respectively:", "Now we can dive into the perf for each individual compiled module. Let\u2019s pick the one for the forward graph for illustration purposes. I\u2019ll name it fwd.py for convenience. Run it directly with the -p argument:", "See the full output log in this example gist.", "In the output, you can notice the following:", "Chrome trace for the profile is written to /tmp/compiled_module_profile.json", "Loading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:", "You can zoom in and out to check the profile.", "We report the percent of GPU time regarding to the wall time by log line like:", "Percent of time when GPU is busy: 102.88%", "Sometimes you may see a value larger than 100%. The reason is because PyTorch uses the kernel execution time with profiling enabled while using wall time with profiling disabled. Profiling may distort the kernel execution time a bit. But overall it should not be a big deal.", "If we run the model like densenet121 with a small batch size, we would see low percent of time when GPU is busy:", "This means the model has a lot of CPU overhead. This is consistent with the fact that enabling cudagraphs improve densenet121\u2019s perf a lot.", "We can break down the GPU time to different categories of kernels. In the mixnet_l example, we see", "This information can be found in the summary line (last line) of the report for each kernel category.", "We also call zoom into a certain category of kernels. For example, let\u2019s check reduction kernels:", "We can see an ordered table of execution time for each individual reduction kernel. We also see how many times a kernel is executed. This is helpful for a few reasons:", "Let\u2019s say we want to take a closer look at triton_red_fused\\__native_batch_norm_legit_functional_16 which is the most expensive reduction kernel and takes 2.19% of overall wall time for the forward graph.", "We can lookup the kernel name in the fwd.py, and find comment like:", "# kernel path: /tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py", "I\u2019ll rename it k.py for convenience. Here is a paste for this file.", "k.py is a standalone Python module containing the kernel code and its benchmark.", "Run k.py directly will report its execution time and bandwidth:", "We can check if max-autotune helps this kernel, by running:", "We may also temporarily add more reduction heuristics and run the script again to check how that helps with the kernel."]}, {"name": "torch.complex", "path": "generated/torch.complex", "type": "Torch", "text": ["Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.", "out (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.", "Example:"]}, {"name": "torch.ComplexDoubleStorage", "path": "storage#torch.ComplexDoubleStorage", "type": "Storage", "text": []}, {"name": "torch.ComplexDoubleStorage.dtype", "path": "storage#torch.ComplexDoubleStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.ComplexFloatStorage", "path": "storage#torch.ComplexFloatStorage", "type": "Storage", "text": []}, {"name": "torch.ComplexFloatStorage.dtype", "path": "storage#torch.ComplexFloatStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.concat", "path": "generated/torch.concat", "type": "Torch", "text": ["Alias of torch.cat()."]}, {"name": "torch.concatenate", "path": "generated/torch.concatenate", "type": "Torch", "text": ["Alias of torch.cat()."]}, {"name": "torch.conj", "path": "generated/torch.conj", "type": "Torch", "text": ["Returns a view of input with a flipped conjugate bit. If input has a non-complex dtype, this function just returns input.", "Note", "torch.conj() performs a lazy conjugation, but the actual conjugated tensor can be materialized at any time using torch.resolve_conj().", "Warning", "In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.", "input (Tensor) \u2013 the input tensor.", "Example:"]}, {"name": "torch.conj_physical", "path": "generated/torch.conj_physical", "type": "Torch", "text": ["Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype, this function just returns input.", "Note", "This performs the conjugate operation regardless of the fact conjugate bit is set or not.", "Warning", "In the future, torch.conj_physical() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.copysign", "path": "generated/torch.copysign", "type": "Torch", "text": ["Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.", "Supports broadcasting to a common shape, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "copysign handles signed zeros. If the other argument has a negative zero (-0), the corresponding output value will be negative."]}, {"name": "torch.corrcoef", "path": "generated/torch.corrcoef", "type": "Torch", "text": ["Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.", "Note", "The correlation coefficient matrix R is computed using the covariance matrix C as given by Rij=CijCii\u2217CjjR_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} * C_{jj} } }", "Note", "Due to floating point rounding, the resulting array may not be Hermitian and its diagonal elements may not be 1. The real and imaginary values are clipped to the interval [-1, 1] in an attempt to improve this situation.", "input (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.", "(Tensor) The correlation coefficient matrix of the variables.", "See also", "torch.cov() covariance matrix.", "Example:"]}, {"name": "torch.cos", "path": "generated/torch.cos", "type": "Torch", "text": ["Returns a new tensor with the cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cosh", "path": "generated/torch.cosh", "type": "Torch", "text": ["Returns a new tensor with the hyperbolic cosine of the elements of input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:", "Note", "When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details."]}, {"name": "torch.count_nonzero", "path": "generated/torch.count_nonzero", "type": "Torch", "text": ["Counts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.", "Example:"]}, {"name": "torch.cov", "path": "generated/torch.cov", "type": "Torch", "text": ["Estimates the covariance matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.", "A covariance matrix is a square matrix giving the covariance of each pair of variables. The diagonal contains the variance of each variable (covariance of a variable with itself). By definition, if input represents a single variable (Scalar or 1D) then its variance is returned.", "The sample covariance of the variables xx and yy is given by:", "where x\u02c9\\bar{x} and y\u02c9\\bar{y} are the simple means of the xx and yy respectively, and \u03b4N\\delta N is the correction.", "If fweights and/or aweights are provided, the weighted covariance is calculated, which is given by:", "where ww denotes fweights or aweights (f and a for brevity) based on whichever is provided, or w=f\u00d7aw = f \\times a if both are provided, and \u03bcx\u2217=\u2211i=1Nwixi\u2211i=1Nwi\\mu_x^* = \\frac{\\sum^{N}_{i = 1}w_ix_{i} }{\\sum^{N}_{i = 1}w_i} is the weighted mean of the variable. If not provided, f and/or a can be seen as a 1\\mathbb{1} vector of appropriate size.", "input (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.", "(Tensor) The covariance matrix of the variables.", "See also", "torch.corrcoef() normalized covariance matrix."]}, {"name": "torch.cpu", "path": "cpu", "type": "Miscellaneous", "text": ["This package implements abstractions found in torch.cuda to facilitate writing device-agnostic code.", "Returns the currently selected Stream for a given device.", "Returns a bool indicating if CPU is currently available.", "Waits for all kernels in all streams on the CPU device to complete.", "Wrapper around the Context-manager StreamContext that selects a given stream.", "Returns number of CPU devices (not cores).", "Context-manager that selects a given stream.", "N.B."]}, {"name": "torch.cpu.amp.autocast", "path": "amp#torch.cpu.amp.autocast", "type": "Automatic Mixed Precision", "text": ["See torch.autocast. torch.cpu.amp.autocast(args...) is equivalent to torch.autocast(\"cpu\", args...)"]}, {"name": "torch.cpu.current_stream()", "path": "generated/torch.cpu.current_stream#torch.cpu.current_stream", "type": "Miscellaneous", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 Ignored.", "Stream", "N.B. This function only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.device_count()", "path": "generated/torch.cpu.device_count#torch.cpu.device_count", "type": "Miscellaneous", "text": ["Returns number of CPU devices (not cores). Always 1.", "N.B. This function only exists to facilitate device-agnostic code", "int"]}, {"name": "torch.cpu.is_available()", "path": "generated/torch.cpu.is_available#torch.cpu.is_available", "type": "Miscellaneous", "text": ["Returns a bool indicating if CPU is currently available.", "N.B. This function only exists to facilitate device-agnostic code", "bool"]}, {"name": "torch.cpu.stream()", "path": "generated/torch.cpu.stream#torch.cpu.stream", "type": "Miscellaneous", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "N.B. This function only exists to facilitate device-agnostic code", "AbstractContextManager"]}, {"name": "torch.cpu.StreamContext", "path": "generated/torch.cpu.streamcontext", "type": "Miscellaneous", "text": ["Context-manager that selects a given stream.", "N.B. This class only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.synchronize()", "path": "generated/torch.cpu.synchronize#torch.cpu.synchronize", "type": "Miscellaneous", "text": ["Waits for all kernels in all streams on the CPU device to complete.", "device (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.", "N.B. This function only exists to facilitate device-agnostic code."]}, {"name": "torch.cpu.torch.cpu.current_stream", "path": "generated/torch.cpu.current_stream", "type": "Miscellaneous", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 Ignored.", "Stream", "N.B. This function only exists to facilitate device-agnostic code"]}, {"name": "torch.cpu.torch.cpu.device_count", "path": "generated/torch.cpu.device_count", "type": "Miscellaneous", "text": ["Returns number of CPU devices (not cores). Always 1.", "N.B. This function only exists to facilitate device-agnostic code", "int"]}, {"name": "torch.cpu.torch.cpu.is_available", "path": "generated/torch.cpu.is_available", "type": "Miscellaneous", "text": ["Returns a bool indicating if CPU is currently available.", "N.B. This function only exists to facilitate device-agnostic code", "bool"]}, {"name": "torch.cpu.torch.cpu.stream", "path": "generated/torch.cpu.stream", "type": "Miscellaneous", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "N.B. This function only exists to facilitate device-agnostic code", "AbstractContextManager"]}, {"name": "torch.cpu.torch.cpu.synchronize", "path": "generated/torch.cpu.synchronize", "type": "Miscellaneous", "text": ["Waits for all kernels in all streams on the CPU device to complete.", "device (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.", "N.B. This function only exists to facilitate device-agnostic code."]}, {"name": "torch.cross", "path": "generated/torch.cross", "type": "Torch", "text": ["Returns the cross product of vectors in dimension dim of input and other.", "Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of vectors, for which it computes the product along the dimension dim. In this case, the output has the same batch dimensions as the inputs.", "If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.", "See also", "torch.linalg.cross() which requires specifying dim (defaulting to -1).", "Warning", "This function may change in a future PyTorch release to match the default behaviour in torch.linalg.cross(). We recommend using torch.linalg.cross().", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.cuda", "path": "cuda", "type": "CUDA", "text": ["This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.", "It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA.", "CUDA semantics has more details about working with CUDA.", "Context-manager that selects a given stream.", "Checks if peer access between two devices is possible.", "Returns cublasHandle_t pointer to current cuBLAS handle", "Returns the index of a currently selected device.", "Returns the currently selected Stream for a given device.", "Returns the default Stream for a given device.", "Context-manager that changes the selected device.", "Returns the number of GPUs available.", "Context-manager that changes the current device to that of given object.", "Returns list CUDA architectures this library was compiled for.", "Gets the cuda capability of a device.", "Gets the name of a device.", "Gets the properties of a device.", "Returns NVCC gencode flags this library was compiled with.", "Returns current value of debug mode for cuda synchronizing operations.", "Initialize PyTorch's CUDA state.", "Force collects GPU memory after it has been released by CUDA IPC.", "Returns a bool indicating if CUDA is currently available.", "Returns whether PyTorch's CUDA state has been initialized.", "Returns the percent of time over the past sample period during which global (device) memory was being read or written.", "Sets the current device.", "Sets the current stream.This is a wrapper API to set the stream.", "Sets the debug mode for cuda synchronizing operations.", "Wrapper around the Context-manager StreamContext that selects a given stream.", "Waits for all kernels in all streams on a CUDA device to complete.", "Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "Returns the average temperature of the GPU sensor in Degrees C (Centigrades)", "Returns the average power draw of the GPU sensor in mW (MilliWatts)", "Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "Exception raised when CUDA is out of memory", "Returns the random number generator state of the specified GPU as a ByteTensor.", "Returns a list of ByteTensor representing the random number states of all devices.", "Sets the random number generator state of the specified GPU.", "Sets the random number generator state of all devices.", "Sets the seed for generating random numbers for the current GPU.", "Sets the seed for generating random numbers on all GPUs.", "Sets the seed for generating random numbers to a random number for the current GPU.", "Sets the seed for generating random numbers to a random number on all GPUs.", "Returns the current random seed of the current GPU.", "comm.broadcast", "Broadcasts a tensor to specified GPU devices.", "comm.broadcast_coalesced", "Broadcasts a sequence tensors to the specified GPUs.", "comm.reduce_add", "Sums tensors from multiple GPUs.", "comm.scatter", "Scatters tensor across multiple GPUs.", "comm.gather", "Gathers tensors from multiple GPU devices.", "Wrapper around a CUDA stream.", "Wrapper around an externally allocated CUDA stream.", "Wrapper around a CUDA event.", "Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "Returns an opaque token representing the id of a graph memory pool.", "Wrapper around a CUDA graph.", "Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.", "Accepts callables (functions or nn.Modules) and returns graphed versions.", "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "Returns a dictionary of CUDA memory allocator statistics for a given device.", "Returns a human-readable printout of the current memory allocator statistics for a given device.", "Returns a snapshot of the CUDA memory allocator state across all devices.", "Returns the current GPU memory occupied by tensors in bytes for a given device.", "Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "Set memory fraction for a process.", "Deprecated; see memory_reserved().", "Deprecated; see max_memory_reserved().", "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "Resets the \"peak\" stats tracked by the CUDA memory allocator.", "Performs a memory allocation using the CUDA memory allocator.", "Deletes memory allocated using the CUDA memory allocator.", "Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.", "CUDA memory allocator loaded from a so file.", "Changes the currently used memory allocator to be the one provided.", "nvtx.mark", "Describe an instantaneous event that occurred at some point.", "nvtx.range_push", "Pushes a range onto a stack of nested range span.", "nvtx.range_pop", "Pops a range off of a stack of nested range spans.", "jiterator._create_jit_fn", "Create a jiterator-generated cuda kernel for an elementwise op.", "jiterator._create_multi_output_jit_fn", "Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it."]}, {"name": "torch.cuda._sanitizer.enable_cuda_sanitizer()", "path": "cuda._sanitizer#torch.cuda._sanitizer.enable_cuda_sanitizer", "type": "CUDA", "text": ["Enables CUDA Sanitizer.", "The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program."]}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "Automatic Mixed Precision", "text": ["See torch.autocast. torch.cuda.amp.autocast(args...) is equivalent to torch.autocast(\"cuda\", args...)"]}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "Automatic Mixed Precision", "text": ["Helper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail."]}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "Automatic Mixed Precision", "text": ["Helper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.", "cast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.", "Note", "If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect."]}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale backoff factor.", "Returns a Python float containing the scale growth factor.", "Returns a Python int containing the growth interval.", "Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync.", "Returns a bool indicating whether this instance is enabled.", "Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().", "Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.", "new_scale (float) \u2013 Value to use as the new scale backoff factor.", "new_scale (float) \u2013 Value to use as the new scale growth factor.", "new_interval (int) \u2013 Value to use as the new growth interval.", "Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().", "step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported.", "Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.", "Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.", "Warning", "For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges."]}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "Automatic Mixed Precision", "text": ["Returns a Python int containing the growth interval."]}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "Automatic Mixed Precision", "text": ["Returns a Python float containing the current scale, or 1.0 if scaling is disabled.", "Warning", "get_scale() incurs a CPU-GPU sync."]}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "Automatic Mixed Precision", "text": ["Returns a bool indicating whether this instance is enabled."]}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "Automatic Mixed Precision", "text": ["Loads the scaler state. If this instance is disabled, load_state_dict() is a no-op.", "state_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict()."]}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "Automatic Mixed Precision", "text": ["Multiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.", "Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.", "outputs (Tensor or iterable of Tensors) \u2013 Outputs to scale."]}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "Automatic Mixed Precision", "text": ["new_scale (float) \u2013 Value to use as the new scale backoff factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "Automatic Mixed Precision", "text": ["new_scale (float) \u2013 Value to use as the new scale growth factor."]}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "Automatic Mixed Precision", "text": ["new_interval (int) \u2013 Value to use as the new growth interval."]}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "Automatic Mixed Precision", "text": ["Returns the state of the scaler as a dict. It contains five entries:", "If this instance is not enabled, returns an empty dict.", "Note", "If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update()."]}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "Automatic Mixed Precision", "text": ["step() carries out the following two operations:", "*args and **kwargs are forwarded to optimizer.step().", "Returns the return value of optimizer.step(*args, **kwargs).", "Warning", "Closure use is not currently supported."]}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "Automatic Mixed Precision", "text": ["Divides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.", "unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step().", "Simple example, using unscale_() to enable clipping of unscaled gradients:", "optimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.", "Note", "unscale_() does not incur a CPU-GPU sync.", "Warning", "unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.", "Warning", "unscale_() may unscale sparse gradients out of place, replacing the .grad attribute."]}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "Automatic Mixed Precision", "text": ["Updates the scale factor.", "If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it.", "Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)", "new_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.", "Warning", "update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.", "Warning", "For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges."]}, {"name": "torch.cuda.caching_allocator_alloc()", "path": "generated/torch.cuda.caching_allocator_alloc#torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": ["Performs a memory allocation using the CUDA memory allocator.", "Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.caching_allocator_delete()", "path": "generated/torch.cuda.caching_allocator_delete#torch.cuda.caching_allocator_delete", "type": "CUDA", "text": ["Deletes memory allocated using the CUDA memory allocator.", "Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.", "mem_ptr (int) \u2013 memory address to be freed by the allocator.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.can_device_access_peer()", "path": "generated/torch.cuda.can_device_access_peer#torch.cuda.can_device_access_peer", "type": "CUDA", "text": ["Checks if peer access between two devices is possible.", "bool"]}, {"name": "torch.cuda.change_current_allocator()", "path": "generated/torch.cuda.change_current_allocator#torch.cuda.change_current_allocator", "type": "CUDA", "text": ["Changes the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.", "allocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.clock_rate()", "path": "generated/torch.cuda.clock_rate#torch.cuda.clock_rate", "type": "CUDA", "text": ["Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.comm.broadcast()", "path": "generated/torch.cuda.comm.broadcast#torch.cuda.comm.broadcast", "type": "CUDA", "text": ["Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor."]}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "generated/torch.cuda.comm.broadcast_coalesced#torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": ["Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices."]}, {"name": "torch.cuda.comm.gather()", "path": "generated/torch.cuda.comm.gather#torch.cuda.comm.gather", "type": "CUDA", "text": ["Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim."]}, {"name": "torch.cuda.comm.reduce_add()", "path": "generated/torch.cuda.comm.reduce_add#torch.cuda.comm.reduce_add", "type": "CUDA", "text": ["Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device."]}, {"name": "torch.cuda.comm.scatter()", "path": "generated/torch.cuda.comm.scatter#torch.cuda.comm.scatter", "type": "CUDA", "text": ["Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor."]}, {"name": "torch.cuda.CUDA Stream Sanitizer", "path": "cuda._sanitizer", "type": "CUDA", "text": ["Note", "This is a prototype feature, which means it is at an early stage for feedback and testing, and its components are subject to change.", "This module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different streams. It stores information on accesses to tensors to determine if they are synchronized or not. When enabled in a python program and a possible data race is detected, a detailed warning will be printed and the program will exit.", "It can be enabled either by importing this module and calling enable_cuda_sanitizer() or by exporting the TORCH_CUDA_SANITIZER environment variable.", "Here is an example of a simple synchronization error in PyTorch:", "The a tensor is initialized on the default stream and, without any synchronization methods, modified on a new stream. The two kernels will run concurrently on the same tensor, which might cause the second kernel to read uninitialized data before the first one was able to write it, or the first kernel might overwrite part of the result of the second. When this script is run on the commandline with:", "the following output is printed by CSAN:", "This gives extensive insight into the origin of the error:", "The error message also displays the schemas of the invoked operators, along with a note showing which arguments of the operators correspond to the affected tensor.", "See also", "The list of supported torch operators and their schemas can be viewed here.", "The bug can be fixed by forcing the new stream to wait for the default stream:", "When the script is run again, there are no errors reported.", "Enables CUDA Sanitizer.", "The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program."]}, {"name": "torch.cuda.CUDAGraph", "path": "generated/torch.cuda.cudagraph", "type": "CUDA", "text": ["Wrapper around a CUDA graph.", "Warning", "This API is in beta and may change in future releases.", "Begins capturing CUDA work on the current stream.", "Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally.", "Ends CUDA graph capture on the current stream. After capture_end, replay may be called on this instance.", "Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally.", "debug_path (required) \u2013 Path to dump the graph to.", "Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode()", "Enables debugging mode for CUDAGraph.debug_dump.", "Returns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool.", "Replays the CUDA work captured by this graph.", "Deletes the graph currently held by this instance."]}, {"name": "torch.cuda.CUDAGraph.capture_begin()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_begin", "type": "CUDA", "text": ["Begins capturing CUDA work on the current stream.", "Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally."]}, {"name": "torch.cuda.CUDAGraph.capture_end()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_end", "type": "CUDA", "text": ["Ends CUDA graph capture on the current stream. After capture_end, replay may be called on this instance.", "Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally."]}, {"name": "torch.cuda.CUDAGraph.debug_dump()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.debug_dump", "type": "CUDA", "text": ["debug_path (required) \u2013 Path to dump the graph to.", "Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode()"]}, {"name": "torch.cuda.CUDAGraph.enable_debug_mode()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.enable_debug_mode", "type": "CUDA", "text": ["Enables debugging mode for CUDAGraph.debug_dump."]}, {"name": "torch.cuda.CUDAGraph.pool()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.pool", "type": "CUDA", "text": ["Returns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool."]}, {"name": "torch.cuda.CUDAGraph.replay()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.replay", "type": "CUDA", "text": ["Replays the CUDA work captured by this graph."]}, {"name": "torch.cuda.CUDAGraph.reset()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.reset", "type": "CUDA", "text": ["Deletes the graph currently held by this instance."]}, {"name": "torch.cuda.CUDAPluggableAllocator", "path": "generated/torch.cuda.cudapluggableallocator", "type": "CUDA", "text": ["CUDA memory allocator loaded from a so file.", "Memory allocators are compiled in .so files and loaded dynamically using ctypes. To change the active allocator use the torch.memory.cuda.change_current_allocator() function.", "Warning", "This is currently supported only in unix OSs", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.current_blas_handle()", "path": "generated/torch.cuda.current_blas_handle#torch.cuda.current_blas_handle", "type": "CUDA", "text": ["Returns cublasHandle_t pointer to current cuBLAS handle"]}, {"name": "torch.cuda.current_device()", "path": "generated/torch.cuda.current_device#torch.cuda.current_device", "type": "CUDA", "text": ["Returns the index of a currently selected device.", "int"]}, {"name": "torch.cuda.current_stream()", "path": "generated/torch.cuda.current_stream#torch.cuda.current_stream", "type": "CUDA", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.default_stream()", "path": "generated/torch.cuda.default_stream#torch.cuda.default_stream", "type": "CUDA", "text": ["Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.device", "path": "generated/torch.cuda.device", "type": "CUDA", "text": ["Context-manager that changes the selected device.", "device (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None."]}, {"name": "torch.cuda.device_count()", "path": "generated/torch.cuda.device_count#torch.cuda.device_count", "type": "CUDA", "text": ["Returns the number of GPUs available.", "int"]}, {"name": "torch.cuda.device_of", "path": "generated/torch.cuda.device_of", "type": "CUDA", "text": ["Context-manager that changes the current device to that of given object.", "You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.", "obj (Tensor or Storage) \u2013 object allocated on the selected device."]}, {"name": "torch.cuda.empty_cache()", "path": "generated/torch.cuda.empty_cache#torch.cuda.empty_cache", "type": "CUDA", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.Event", "path": "generated/torch.cuda.event", "type": "CUDA", "text": ["Wrapper around a CUDA event.", "CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams.", "The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.", "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.", "Reconstruct an event from an IPC handle on the given device.", "Returns an IPC handle of this event. If not recorded yet, the event will use the current device.", "Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed.", "Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device.", "Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.", "Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.Event.elapsed_time()", "path": "generated/torch.cuda.event#torch.cuda.Event.elapsed_time", "type": "CUDA", "text": ["Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded."]}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.from_ipc_handle", "type": "CUDA", "text": ["Reconstruct an event from an IPC handle on the given device."]}, {"name": "torch.cuda.Event.ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.ipc_handle", "type": "CUDA", "text": ["Returns an IPC handle of this event. If not recorded yet, the event will use the current device."]}, {"name": "torch.cuda.Event.query()", "path": "generated/torch.cuda.event#torch.cuda.Event.query", "type": "CUDA", "text": ["Checks if all work currently captured by event has completed.", "A boolean indicating if all work currently captured by event has completed."]}, {"name": "torch.cuda.Event.record()", "path": "generated/torch.cuda.event#torch.cuda.Event.record", "type": "CUDA", "text": ["Records the event in a given stream.", "Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device."]}, {"name": "torch.cuda.Event.synchronize()", "path": "generated/torch.cuda.event#torch.cuda.Event.synchronize", "type": "CUDA", "text": ["Waits for the event to complete.", "Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.", "Note", "This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.Event.wait()", "path": "generated/torch.cuda.event#torch.cuda.Event.wait", "type": "CUDA", "text": ["Makes all future work submitted to the given stream wait for this event.", "Use torch.cuda.current_stream() if no stream is specified.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info."]}, {"name": "torch.cuda.ExternalStream", "path": "generated/torch.cuda.externalstream", "type": "CUDA", "text": ["Wrapper around an externally allocated CUDA stream.", "This class is used to wrap streams allocated in other libraries in order to facilitate data exchange and multi-library interactions.", "Note", "This class doesn\u2019t manage the stream life-cycle, it is the user responsibility to keep the referenced stream alive while this class is being used.", "Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed.", "Records an event.", "event (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event.", "Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.", "Makes all future work submitted to the stream wait for an event.", "event (torch.cuda.Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected.", "Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.ExternalStream.query()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.query", "type": "CUDA", "text": ["Checks if all the work submitted has been completed.", "A boolean indicating if all kernels in this stream are completed."]}, {"name": "torch.cuda.ExternalStream.record_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.record_event", "type": "CUDA", "text": ["Records an event.", "event (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.", "Recorded event."]}, {"name": "torch.cuda.ExternalStream.synchronize()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.synchronize", "type": "CUDA", "text": ["Wait for all the kernels in this stream to complete.", "Note", "This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info."]}, {"name": "torch.cuda.ExternalStream.wait_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_event", "type": "CUDA", "text": ["Makes all future work submitted to the stream wait for an event.", "event (torch.cuda.Event) \u2013 an event to wait for.", "Note", "This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info.", "This function returns without waiting for event: only future operations are affected."]}, {"name": "torch.cuda.ExternalStream.wait_stream()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_stream", "type": "CUDA", "text": ["Synchronizes with another stream.", "All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.", "stream (Stream) \u2013 a stream to synchronize.", "Note", "This function returns without waiting for currently enqueued kernels in stream: only future operations are affected."]}, {"name": "torch.cuda.get_allocator_backend()", "path": "generated/torch.cuda.get_allocator_backend#torch.cuda.get_allocator_backend", "type": "CUDA", "text": ["Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).", "Note", "See Memory management for details on choosing the allocator backend.", "str"]}, {"name": "torch.cuda.get_arch_list()", "path": "generated/torch.cuda.get_arch_list#torch.cuda.get_arch_list", "type": "CUDA", "text": ["Returns list CUDA architectures this library was compiled for.", "List[str]"]}, {"name": "torch.cuda.get_device_capability()", "path": "generated/torch.cuda.get_device_capability#torch.cuda.get_device_capability", "type": "CUDA", "text": ["Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)"]}, {"name": "torch.cuda.get_device_name()", "path": "generated/torch.cuda.get_device_name#torch.cuda.get_device_name", "type": "CUDA", "text": ["Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str"]}, {"name": "torch.cuda.get_device_properties()", "path": "generated/torch.cuda.get_device_properties#torch.cuda.get_device_properties", "type": "CUDA", "text": ["Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties"]}, {"name": "torch.cuda.get_gencode_flags()", "path": "generated/torch.cuda.get_gencode_flags#torch.cuda.get_gencode_flags", "type": "CUDA", "text": ["Returns NVCC gencode flags this library was compiled with.", "str"]}, {"name": "torch.cuda.get_rng_state()", "path": "generated/torch.cuda.get_rng_state#torch.cuda.get_rng_state", "type": "CUDA", "text": ["Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Tensor", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.get_rng_state_all()", "path": "generated/torch.cuda.get_rng_state_all#torch.cuda.get_rng_state_all", "type": "CUDA", "text": ["Returns a list of ByteTensor representing the random number states of all devices.", "List[Tensor]"]}, {"name": "torch.cuda.get_sync_debug_mode()", "path": "generated/torch.cuda.get_sync_debug_mode#torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": ["Returns current value of debug mode for cuda synchronizing operations.", "int"]}, {"name": "torch.cuda.graph", "path": "generated/torch.cuda.graph", "type": "CUDA", "text": ["Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.", "See CUDA Graphs for a general introduction, detailed use, and constraints.", "Note", "For effective memory sharing, if you pass a pool used by a previous capture and the previous capture used an explicit stream argument, you should pass the same stream argument to this capture.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.graph_pool_handle()", "path": "generated/torch.cuda.graph_pool_handle#torch.cuda.graph_pool_handle", "type": "CUDA", "text": ["Returns an opaque token representing the id of a graph memory pool. See Graph memory management.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.init()", "path": "generated/torch.cuda.init#torch.cuda.init", "type": "CUDA", "text": ["Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized."]}, {"name": "torch.cuda.initial_seed()", "path": "generated/torch.cuda.initial_seed#torch.cuda.initial_seed", "type": "CUDA", "text": ["Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA.", "int"]}, {"name": "torch.cuda.ipc_collect()", "path": "generated/torch.cuda.ipc_collect#torch.cuda.ipc_collect", "type": "CUDA", "text": ["Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory."]}, {"name": "torch.cuda.is_available()", "path": "generated/torch.cuda.is_available#torch.cuda.is_available", "type": "CUDA", "text": ["Returns a bool indicating if CUDA is currently available.", "bool"]}, {"name": "torch.cuda.is_current_stream_capturing()", "path": "generated/torch.cuda.is_current_stream_capturing#torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": ["Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "If a CUDA context does not exist on the current device, returns False without initializing the context."]}, {"name": "torch.cuda.is_initialized()", "path": "generated/torch.cuda.is_initialized#torch.cuda.is_initialized", "type": "CUDA", "text": ["Returns whether PyTorch\u2019s CUDA state has been initialized."]}, {"name": "torch.cuda.jiterator._create_jit_fn()", "path": "generated/torch.cuda.jiterator._create_jit_fn#torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op.", "The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir.", "Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.", "Callable", "Example:", "code_string also allows multiple function definitions, and the last function will be treated as the entry function.", "Example:", "Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu.", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 1 output", "Warning", "All input tensors must live in CUDA device"]}, {"name": "torch.cuda.jiterator._create_multi_output_jit_fn()", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn#torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "Callable", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 8 outputs"]}, {"name": "torch.cuda.list_gpu_processes()", "path": "generated/torch.cuda.list_gpu_processes#torch.cuda.list_gpu_processes", "type": "CUDA", "text": ["Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).", "str"]}, {"name": "torch.cuda.make_graphed_callables()", "path": "generated/torch.cuda.make_graphed_callables#torch.cuda.make_graphed_callables", "type": "CUDA", "text": ["Accepts callables (functions or nn.Modules) and returns graphed versions.", "Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node.", "The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph.", "Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop.", "See Partial-network capture for detailed use and constraints.", "If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.", "Note", "The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.", "Warning", "This API is in beta and may change in future releases.", "Warning", "sample_args for each callable must contain only Tensors. Other types are not allowed.", "Warning", "Returned callables do not support higher order differentiation (e.g., double backward).", "Warning", "In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.", "Warning", "After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.", "Warning", "torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.", "Warning", "When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.", "Warning", "The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False."]}, {"name": "torch.cuda.manual_seed()", "path": "generated/torch.cuda.manual_seed#torch.cuda.manual_seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all()."]}, {"name": "torch.cuda.manual_seed_all()", "path": "generated/torch.cuda.manual_seed_all#torch.cuda.manual_seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.cuda.max_memory_allocated()", "path": "generated/torch.cuda.max_memory_allocated#torch.cuda.max_memory_allocated", "type": "CUDA", "text": ["Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.max_memory_cached()", "path": "generated/torch.cuda.max_memory_cached#torch.cuda.max_memory_cached", "type": "CUDA", "text": ["Deprecated; see max_memory_reserved().", "int"]}, {"name": "torch.cuda.max_memory_reserved()", "path": "generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved", "type": "CUDA", "text": ["Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.mem_get_info()", "path": "generated/torch.cuda.mem_get_info#torch.cuda.mem_get_info", "type": "CUDA", "text": ["Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Tuple[int, int]", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory._dump_snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._dump_snapshot", "type": "Miscellaneous", "text": ["Saves a pickled version of the torch.memory._snapshot() dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz", "filename (str, optional) \u2013 Name of the file to create. Defaults to \u201cdump_snapshot.pickle\u201d."]}, {"name": "torch.cuda.memory._record_memory_history()", "path": "torch_cuda_memory#torch.cuda.memory._record_memory_history", "type": "Miscellaneous", "text": ["Enables recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in torch.cuda.memory._snapshot().", "In addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events.", "Use torch.cuda.memory._snapshot() to retrieve this information, and the tools in _memory_viz.py to visualize snapshots.", "The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues.", "C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth."]}, {"name": "torch.cuda.memory._snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._snapshot", "type": "Miscellaneous", "text": ["Saves a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure.", "The Snapshot dictionary object"]}, {"name": "torch.cuda.memory_allocated()", "path": "generated/torch.cuda.memory_allocated#torch.cuda.memory_allocated", "type": "CUDA", "text": ["Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_cached()", "path": "generated/torch.cuda.memory_cached#torch.cuda.memory_cached", "type": "CUDA", "text": ["Deprecated; see memory_reserved().", "int"]}, {"name": "torch.cuda.memory_reserved()", "path": "generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved", "type": "CUDA", "text": ["Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_snapshot()", "path": "generated/torch.cuda.memory_snapshot#torch.cuda.memory_snapshot", "type": "CUDA", "text": ["Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_stats()", "path": "generated/torch.cuda.memory_stats#torch.cuda.memory_stats", "type": "CUDA", "text": ["Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:", "The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Dict[str, Any]", "Note", "See Memory management for more details about GPU memory management.", "Note", "With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero."]}, {"name": "torch.cuda.memory_summary()", "path": "generated/torch.cuda.memory_summary#torch.cuda.memory_summary", "type": "CUDA", "text": ["Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "str", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.memory_usage()", "path": "generated/torch.cuda.memory_usage#torch.cuda.memory_usage", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.nvtx.mark()", "path": "generated/torch.cuda.nvtx.mark#torch.cuda.nvtx.mark", "type": "CUDA", "text": ["Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event."]}, {"name": "torch.cuda.nvtx.range_pop()", "path": "generated/torch.cuda.nvtx.range_pop#torch.cuda.nvtx.range_pop", "type": "CUDA", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.nvtx.range_push()", "path": "generated/torch.cuda.nvtx.range_push#torch.cuda.nvtx.range_push", "type": "CUDA", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range"]}, {"name": "torch.cuda.power_draw()", "path": "generated/torch.cuda.power_draw#torch.cuda.power_draw", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "generated/torch.cuda.reset_max_memory_allocated#torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "generated/torch.cuda.reset_max_memory_cached#torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.reset_peak_memory_stats()", "path": "generated/torch.cuda.reset_peak_memory_stats#torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": ["Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.", "See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.seed()", "path": "generated/torch.cuda.seed#torch.cuda.seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all()."]}, {"name": "torch.cuda.seed_all()", "path": "generated/torch.cuda.seed_all#torch.cuda.seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored."]}, {"name": "torch.cuda.set_device()", "path": "generated/torch.cuda.set_device#torch.cuda.set_device", "type": "CUDA", "text": ["Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative."]}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "generated/torch.cuda.set_per_process_memory_fraction#torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": ["Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity."]}, {"name": "torch.cuda.set_rng_state()", "path": "generated/torch.cuda.set_rng_state#torch.cuda.set_rng_state", "type": "CUDA", "text": ["Sets the random number generator state of the specified GPU."]}, {"name": "torch.cuda.set_rng_state_all()", "path": "generated/torch.cuda.set_rng_state_all#torch.cuda.set_rng_state_all", "type": "CUDA", "text": ["Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device"]}, {"name": "torch.cuda.set_stream()", "path": "generated/torch.cuda.set_stream#torch.cuda.set_stream", "type": "CUDA", "text": ["Usage of this function is discouraged in favor of the stream context manager.", "stream (Stream) \u2013 selected stream. This function is a no-op if this argument is None."]}, {"name": "torch.cuda.set_sync_debug_mode()", "path": "generated/torch.cuda.set_sync_debug_mode#torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": ["Sets the debug mode for cuda synchronizing operations.", "debug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.", "Warning", "This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet."]}, {"name": "torch.cuda.stream()", "path": "generated/torch.cuda.stream#torch.cuda.stream", "type": "CUDA", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "StreamContext", "..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream."]}, {"name": "torch.cuda.StreamContext", "path": "generated/torch.cuda.streamcontext", "type": "CUDA", "text": ["Context-manager that selects a given stream.", "All CUDA kernels queued within its context will be enqueued on a selected stream.", "Stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "Note", "Streams are per-device."]}, {"name": "torch.cuda.synchronize()", "path": "generated/torch.cuda.synchronize#torch.cuda.synchronize", "type": "CUDA", "text": ["Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.temperature()", "path": "generated/torch.cuda.temperature#torch.cuda.temperature", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.caching_allocator_alloc", "path": "generated/torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": ["Performs a memory allocation using the CUDA memory allocator.", "Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.caching_allocator_delete", "path": "generated/torch.cuda.caching_allocator_delete", "type": "CUDA", "text": ["Deletes memory allocated using the CUDA memory allocator.", "Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.", "mem_ptr (int) \u2013 memory address to be freed by the allocator.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.can_device_access_peer", "path": "generated/torch.cuda.can_device_access_peer", "type": "CUDA", "text": ["Checks if peer access between two devices is possible.", "bool"]}, {"name": "torch.cuda.torch.cuda.change_current_allocator", "path": "generated/torch.cuda.change_current_allocator", "type": "CUDA", "text": ["Changes the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.", "allocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.", "Note", "See Memory management for details on creating and using a custom allocator"]}, {"name": "torch.cuda.torch.cuda.clock_rate", "path": "generated/torch.cuda.clock_rate", "type": "CUDA", "text": ["Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.comm.broadcast", "path": "generated/torch.cuda.comm.broadcast", "type": "CUDA", "text": ["Broadcasts a tensor to specified GPU devices.", "Note", "Exactly one of devices and out must be specified.", "a tuple containing copies of tensor, placed on devices.", "a tuple containing out tensors, each containing a copy of tensor."]}, {"name": "torch.cuda.torch.cuda.comm.broadcast_coalesced", "path": "generated/torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": ["Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.", "A tuple containing copies of tensor, placed on devices."]}, {"name": "torch.cuda.torch.cuda.comm.gather", "path": "generated/torch.cuda.comm.gather", "type": "CUDA", "text": ["Gathers tensors from multiple GPU devices.", "Note", "destination must not be specified when out is specified.", "a tensor located on destination device, that is a result of concatenating tensors along dim.", "the out tensor, now containing results of concatenating tensors along dim."]}, {"name": "torch.cuda.torch.cuda.comm.reduce_add", "path": "generated/torch.cuda.comm.reduce_add", "type": "CUDA", "text": ["Sums tensors from multiple GPUs.", "All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.", "A tensor containing an elementwise sum of all inputs, placed on the destination device."]}, {"name": "torch.cuda.torch.cuda.comm.scatter", "path": "generated/torch.cuda.comm.scatter", "type": "CUDA", "text": ["Scatters tensor across multiple GPUs.", "Note", "Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.", "a tuple containing chunks of tensor, placed on devices.", "a tuple containing out tensors, each containing a chunk of tensor."]}, {"name": "torch.cuda.torch.cuda.current_blas_handle", "path": "generated/torch.cuda.current_blas_handle", "type": "CUDA", "text": ["Returns cublasHandle_t pointer to current cuBLAS handle"]}, {"name": "torch.cuda.torch.cuda.current_device", "path": "generated/torch.cuda.current_device", "type": "CUDA", "text": ["Returns the index of a currently selected device.", "int"]}, {"name": "torch.cuda.torch.cuda.current_stream", "path": "generated/torch.cuda.current_stream", "type": "CUDA", "text": ["Returns the currently selected Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.torch.cuda.default_stream", "path": "generated/torch.cuda.default_stream", "type": "CUDA", "text": ["Returns the default Stream for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).", "Stream"]}, {"name": "torch.cuda.torch.cuda.device_count", "path": "generated/torch.cuda.device_count", "type": "CUDA", "text": ["Returns the number of GPUs available.", "int"]}, {"name": "torch.cuda.torch.cuda.empty_cache", "path": "generated/torch.cuda.empty_cache", "type": "CUDA", "text": ["Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.", "Note", "empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.get_allocator_backend", "path": "generated/torch.cuda.get_allocator_backend", "type": "CUDA", "text": ["Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).", "Note", "See Memory management for details on choosing the allocator backend.", "str"]}, {"name": "torch.cuda.torch.cuda.get_arch_list", "path": "generated/torch.cuda.get_arch_list", "type": "CUDA", "text": ["Returns list CUDA architectures this library was compiled for.", "List[str]"]}, {"name": "torch.cuda.torch.cuda.get_device_capability", "path": "generated/torch.cuda.get_device_capability", "type": "CUDA", "text": ["Gets the cuda capability of a device.", "device (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the major and minor cuda capability of the device", "tuple(int, int)"]}, {"name": "torch.cuda.torch.cuda.get_device_name", "path": "generated/torch.cuda.get_device_name", "type": "CUDA", "text": ["Gets the name of a device.", "device (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).", "the name of the device", "str"]}, {"name": "torch.cuda.torch.cuda.get_device_properties", "path": "generated/torch.cuda.get_device_properties", "type": "CUDA", "text": ["Gets the properties of a device.", "device (torch.device or int or str) \u2013 device for which to return the properties of the device.", "the properties of the device", "_CudaDeviceProperties"]}, {"name": "torch.cuda.torch.cuda.get_gencode_flags", "path": "generated/torch.cuda.get_gencode_flags", "type": "CUDA", "text": ["Returns NVCC gencode flags this library was compiled with.", "str"]}, {"name": "torch.cuda.torch.cuda.get_rng_state", "path": "generated/torch.cuda.get_rng_state", "type": "CUDA", "text": ["Returns the random number generator state of the specified GPU as a ByteTensor.", "device (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).", "Tensor", "Warning", "This function eagerly initializes CUDA."]}, {"name": "torch.cuda.torch.cuda.get_rng_state_all", "path": "generated/torch.cuda.get_rng_state_all", "type": "CUDA", "text": ["Returns a list of ByteTensor representing the random number states of all devices.", "List[Tensor]"]}, {"name": "torch.cuda.torch.cuda.get_sync_debug_mode", "path": "generated/torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": ["Returns current value of debug mode for cuda synchronizing operations.", "int"]}, {"name": "torch.cuda.torch.cuda.graph_pool_handle", "path": "generated/torch.cuda.graph_pool_handle", "type": "CUDA", "text": ["Returns an opaque token representing the id of a graph memory pool. See Graph memory management.", "Warning", "This API is in beta and may change in future releases."]}, {"name": "torch.cuda.torch.cuda.init", "path": "generated/torch.cuda.init", "type": "CUDA", "text": ["Initialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand.", "Does nothing if the CUDA state is already initialized."]}, {"name": "torch.cuda.torch.cuda.initial_seed", "path": "generated/torch.cuda.initial_seed", "type": "CUDA", "text": ["Returns the current random seed of the current GPU.", "Warning", "This function eagerly initializes CUDA.", "int"]}, {"name": "torch.cuda.torch.cuda.ipc_collect", "path": "generated/torch.cuda.ipc_collect", "type": "CUDA", "text": ["Force collects GPU memory after it has been released by CUDA IPC.", "Note", "Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory."]}, {"name": "torch.cuda.torch.cuda.is_available", "path": "generated/torch.cuda.is_available", "type": "CUDA", "text": ["Returns a bool indicating if CUDA is currently available.", "bool"]}, {"name": "torch.cuda.torch.cuda.is_current_stream_capturing", "path": "generated/torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": ["Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.", "If a CUDA context does not exist on the current device, returns False without initializing the context."]}, {"name": "torch.cuda.torch.cuda.is_initialized", "path": "generated/torch.cuda.is_initialized", "type": "CUDA", "text": ["Returns whether PyTorch\u2019s CUDA state has been initialized."]}, {"name": "torch.cuda.torch.cuda.jiterator._create_jit_fn", "path": "generated/torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op.", "The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir.", "Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.", "Callable", "Example:", "code_string also allows multiple function definitions, and the last function will be treated as the entry function.", "Example:", "Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu.", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 1 output", "Warning", "All input tensors must live in CUDA device"]}, {"name": "torch.cuda.torch.cuda.jiterator._create_multi_output_jit_fn", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": ["Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.", "Callable", "Example:", "Warning", "This API is in beta and may change in future releases.", "Warning", "This API only supports up to 8 inputs and 8 outputs"]}, {"name": "torch.cuda.torch.cuda.list_gpu_processes", "path": "generated/torch.cuda.list_gpu_processes", "type": "CUDA", "text": ["Returns a human-readable printout of the running processes and their GPU memory use for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "device (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).", "str"]}, {"name": "torch.cuda.torch.cuda.make_graphed_callables", "path": "generated/torch.cuda.make_graphed_callables", "type": "CUDA", "text": ["Accepts callables (functions or nn.Modules) and returns graphed versions.", "Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node.", "The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph.", "Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop.", "See Partial-network capture for detailed use and constraints.", "If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.", "Note", "The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.", "Warning", "This API is in beta and may change in future releases.", "Warning", "sample_args for each callable must contain only Tensors. Other types are not allowed.", "Warning", "Returned callables do not support higher order differentiation (e.g., double backward).", "Warning", "In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.", "Warning", "After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.", "Warning", "torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.", "Warning", "When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.", "Warning", "The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False."]}, {"name": "torch.cuda.torch.cuda.manual_seed", "path": "generated/torch.cuda.manual_seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed.", "Warning", "If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all()."]}, {"name": "torch.cuda.torch.cuda.manual_seed_all", "path": "generated/torch.cuda.manual_seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "seed (int) \u2013 The desired seed."]}, {"name": "torch.cuda.torch.cuda.max_memory_allocated", "path": "generated/torch.cuda.max_memory_allocated", "type": "CUDA", "text": ["Returns the maximum GPU memory occupied by tensors in bytes for a given device.", "By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.max_memory_cached", "path": "generated/torch.cuda.max_memory_cached", "type": "CUDA", "text": ["Deprecated; see max_memory_reserved().", "int"]}, {"name": "torch.cuda.torch.cuda.max_memory_reserved", "path": "generated/torch.cuda.max_memory_reserved", "type": "CUDA", "text": ["Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.", "By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.mem_get_info", "path": "generated/torch.cuda.mem_get_info", "type": "CUDA", "text": ["Returns the global free and total GPU memory for a given device using cudaMemGetInfo.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Tuple[int, int]", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_allocated", "path": "generated/torch.cuda.memory_allocated", "type": "CUDA", "text": ["Returns the current GPU memory occupied by tensors in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_cached", "path": "generated/torch.cuda.memory_cached", "type": "CUDA", "text": ["Deprecated; see memory_reserved().", "int"]}, {"name": "torch.cuda.torch.cuda.memory_reserved", "path": "generated/torch.cuda.memory_reserved", "type": "CUDA", "text": ["Returns the current GPU memory managed by the caching allocator in bytes for a given device.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_snapshot", "path": "generated/torch.cuda.memory_snapshot", "type": "CUDA", "text": ["Returns a snapshot of the CUDA memory allocator state across all devices.", "Interpreting the output of this function requires familiarity with the memory allocator internals.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_stats", "path": "generated/torch.cuda.memory_stats", "type": "CUDA", "text": ["Returns a dictionary of CUDA memory allocator statistics for a given device.", "The return value of this function is a dictionary of statistics, each of which is a non-negative integer.", "Core statistics:", "For these core statistics, values are broken down as follows.", "Pool type:", "Metric type:", "In addition to the core statistics, we also provide some simple event counters:", "The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:", "The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:", "device (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).", "Dict[str, Any]", "Note", "See Memory management for more details about GPU memory management.", "Note", "With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero."]}, {"name": "torch.cuda.torch.cuda.memory_summary", "path": "generated/torch.cuda.memory_summary", "type": "CUDA", "text": ["Returns a human-readable printout of the current memory allocator statistics for a given device.", "This can be useful to display periodically during training, or when handling out-of-memory exceptions.", "str", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.memory_usage", "path": "generated/torch.cuda.memory_usage", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.nvtx.mark", "path": "generated/torch.cuda.nvtx.mark", "type": "CUDA", "text": ["Describe an instantaneous event that occurred at some point.", "msg (str) \u2013 ASCII message to associate with the event."]}, {"name": "torch.cuda.torch.cuda.nvtx.range_pop", "path": "generated/torch.cuda.nvtx.range_pop", "type": "CUDA", "text": ["Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended."]}, {"name": "torch.cuda.torch.cuda.nvtx.range_push", "path": "generated/torch.cuda.nvtx.range_push", "type": "CUDA", "text": ["Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.", "msg (str) \u2013 ASCII message to associate with range"]}, {"name": "torch.cuda.torch.cuda.OutOfMemoryError", "path": "generated/torch.cuda.outofmemoryerror", "type": "CUDA", "text": ["Exception raised when CUDA is out of memory"]}, {"name": "torch.cuda.torch.cuda.power_draw", "path": "generated/torch.cuda.power_draw", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.reset_max_memory_allocated", "path": "generated/torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.", "See max_memory_allocated() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.reset_max_memory_cached", "path": "generated/torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": ["Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.", "See max_memory_cached() for details.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Warning", "This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.reset_peak_memory_stats", "path": "generated/torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": ["Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.", "See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "Note", "See Memory management for more details about GPU memory management."]}, {"name": "torch.cuda.torch.cuda.seed", "path": "generated/torch.cuda.seed", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.", "Warning", "If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all()."]}, {"name": "torch.cuda.torch.cuda.seed_all", "path": "generated/torch.cuda.seed_all", "type": "CUDA", "text": ["Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored."]}, {"name": "torch.cuda.torch.cuda.set_device", "path": "generated/torch.cuda.set_device", "type": "CUDA", "text": ["Sets the current device.", "Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.", "device (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative."]}, {"name": "torch.cuda.torch.cuda.set_per_process_memory_fraction", "path": "generated/torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": ["Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.", "Note", "In general, the total available free memory is less than the total capacity."]}, {"name": "torch.cuda.torch.cuda.set_rng_state", "path": "generated/torch.cuda.set_rng_state", "type": "CUDA", "text": ["Sets the random number generator state of the specified GPU."]}, {"name": "torch.cuda.torch.cuda.set_rng_state_all", "path": "generated/torch.cuda.set_rng_state_all", "type": "CUDA", "text": ["Sets the random number generator state of all devices.", "new_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device"]}, {"name": "torch.cuda.torch.cuda.set_stream", "path": "generated/torch.cuda.set_stream", "type": "CUDA", "text": ["Usage of this function is discouraged in favor of the stream context manager.", "stream (Stream) \u2013 selected stream. This function is a no-op if this argument is None."]}, {"name": "torch.cuda.torch.cuda.set_sync_debug_mode", "path": "generated/torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": ["Sets the debug mode for cuda synchronizing operations.", "debug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.", "Warning", "This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet."]}, {"name": "torch.cuda.torch.cuda.stream", "path": "generated/torch.cuda.stream", "type": "CUDA", "text": ["Wrapper around the Context-manager StreamContext that selects a given stream.", "stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.", "StreamContext", "..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream."]}, {"name": "torch.cuda.torch.cuda.synchronize", "path": "generated/torch.cuda.synchronize", "type": "CUDA", "text": ["Waits for all kernels in all streams on a CUDA device to complete.", "device (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default)."]}, {"name": "torch.cuda.torch.cuda.temperature", "path": "generated/torch.cuda.temperature", "type": "CUDA", "text": ["over the past sample period as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.torch.cuda.utilization", "path": "generated/torch.cuda.utilization", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cuda.utilization()", "path": "generated/torch.cuda.utilization#torch.cuda.utilization", "type": "CUDA", "text": ["Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.", "device (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).", "int", "Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried."]}, {"name": "torch.cummax", "path": "generated/torch.cummax", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cummin", "path": "generated/torch.cummin", "type": "Torch", "text": ["Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.", "out (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)", "Example:"]}, {"name": "torch.cumprod", "path": "generated/torch.cumprod", "type": "Torch", "text": ["Returns the cumulative product of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.cumsum", "path": "generated/torch.cumsum", "type": "Torch", "text": ["Returns the cumulative sum of elements of input in the dimension dim.", "For example, if input is a vector of size N, the result will also be a vector of size N, with elements.", "Example:"]}, {"name": "torch.cumulative_trapezoid", "path": "generated/torch.cumulative_trapezoid", "type": "Torch", "text": ["Cumulatively computes the trapezoidal rule along dim. By default the spacing between elements is assumed to be 1, but dx can be used to specify a different constant spacing, and x can be used to specify arbitrary spacing along dim.", "For more details, please read torch.trapezoid(). The difference between torch.trapezoid() and this function is that, torch.trapezoid() returns a value for each integration, where as this function returns a cumulative value for every spacing within the integration. This is analogous to how .sum returns a value and .cumsum returns a cumulative sum.", "Examples:"]}, {"name": "torch.deg2rad", "path": "generated/torch.deg2rad", "type": "Torch", "text": ["Returns a new tensor with each of the elements of input converted from angles in degrees to radians.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.dequantize", "path": "generated/torch.dequantize", "type": "Torch", "text": ["Returns an fp32 Tensor by dequantizing a quantized Tensor", "tensor (Tensor) \u2013 A quantized Tensor", "Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors", "tensors (sequence of Tensors) \u2013 A list of quantized Tensors"]}, {"name": "torch.det", "path": "generated/torch.det", "type": "Torch", "text": ["Alias for torch.linalg.det()"]}, {"name": "torch.device", "path": "tensor_attributes#torch.device", "type": "Miscellaneous", "text": []}, {"name": "torch.diag", "path": "generated/torch.diag", "type": "Torch", "text": ["The argument diagonal controls which diagonal to consider:", "out (Tensor, optional) \u2013 the output tensor.", "See also", "torch.diagonal() always returns the diagonal of its input.", "torch.diagflat() always constructs a tensor with diagonal elements specified by the input.", "Examples:", "Get the square matrix where the input vector is the diagonal:", "Get the k-th diagonal of a given matrix:"]}, {"name": "torch.diag_embed", "path": "generated/torch.diag_embed", "type": "Torch", "text": ["Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default.", "The argument offset controls which diagonal to consider:", "The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00, the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset.", "Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.", "Example:"]}, {"name": "torch.diagflat", "path": "generated/torch.diagflat", "type": "Torch", "text": ["The argument offset controls which diagonal to consider:", "Examples:"]}, {"name": "torch.diagonal", "path": "generated/torch.diagonal", "type": "Torch", "text": ["Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.", "The argument offset controls which diagonal to consider:", "Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.", "Note", "To take a batch diagonal, pass in dim1=-2, dim2=-1.", "Examples:"]}, {"name": "torch.diagonal_scatter", "path": "generated/torch.diagonal_scatter", "type": "Torch", "text": ["Embeds the values of the src tensor into input along the diagonal elements of input, with respect to dim1 and dim2.", "This function returns a tensor with fresh storage; it does not return a view.", "The argument offset controls which diagonal to consider:", "Note", "src must be of the proper size in order to be embedded into input. Specifically, it should have the same shape as torch.diagonal(input, offset, dim1, dim2)", "Examples:"]}, {"name": "torch.diff", "path": "generated/torch.diff", "type": "Torch", "text": ["Computes the n-th forward difference along the given dimension.", "The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.digamma", "path": "generated/torch.digamma", "type": "Torch", "text": ["Alias for torch.special.digamma()."]}, {"name": "torch.dist", "path": "generated/torch.dist", "type": "Torch", "text": ["Returns the p-norm of (input - other)", "The shapes of input and other must be broadcastable.", "Example:"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook returns a future that wraps the input, so it is a noop that does not incur any communication overheads.", "This hook should only be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook", "type": "DDP Communication Hooks", "text": ["Warning: This API is experimental, and it requires NCCL version later than 2.9.6.", "This DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision Brain floating point format (torch.bfloat16) and then divides it by the process group size. It allreduces those bfloat16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper", "type": "DDP Communication Hooks", "text": ["Warning: This API is experimental, and it requires NCCL version later than 2.9.6.", "This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_ (``torch.bfloat16`), and casts the resulting tensor of the given hook back to the input data type, such as float32.", "Therefore, bf16_compress_hook is equivalent to bf16_compress_wrapper(allreduce_hook).", "Callable[[Any, GradBucket], Future[Tensor]]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision floating-point format (torch.float16) and then divides it by the process group size. It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper", "type": "DDP Communication Hooks", "text": ["This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (torch.float16), and casts the resulting tensor of the given hook back to the input data type, such as float32.", "Therefore, fp16_compress_hook is equivalent to fp16_compress_wrapper(allreduce_hook).", "Callable[[Any, GradBucket], Future[Tensor]]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.", "Warning", "Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.", "Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": ["This DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:", "Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:", "1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.", "1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).", "Handles uncompressed tensors:", "2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;", "2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.", "Handles the tensors that should be compressed by PowerSGD compression:", "3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;", "3.2. Computes each P in Ps, which is equal to MQ;", "3.3. Allreduces Ps as a batch;", "3.4. Orthogonalizes each P in Ps;", "3.5. Computes each Q in Qs, which is approximately equal to M^TP;", "3.6. Allreduces Qs as a batch;", "3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.", "Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.", "Future handler of the communication, which updates the gradients in place.", "Future[Tensor]"]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": ["Stores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.", "matrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.", "1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.", "1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain matrix_approximation_rank threshold.", "To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.", "To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, start_powerSGD_iter typically should be no less than the number of warm-up steps.", "Compression statistics are logged every compression_stats_logging_frequency iterations once PowerSGD compression starts.", "Warning", "If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__", "type": "DDP Communication Hooks", "text": ["Returns a Dict[str, Any] which will be pickled and saved. process_group is not serializable and excluded from a returned state."]}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__", "type": "DDP Communication Hooks", "text": ["Takes a provided state and retrieves PowerSGDState. process_group is set to default."]}, {"name": "torch.distributed.algorithms.Join", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join", "type": "Miscellaneous", "text": ["This class defines the generic join context manager, which allows custom hooks to be called after a process joins. These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to JoinHook for details about the hook definition.", "Warning", "The context manager requires each participating Joinable to call the method notify_join_context() before its own per- iteration collective communications to ensure correctness.", "Warning", "The context manager requires that all process_group attributes in the JoinHook objects are the same. If there are multiple JoinHook objects, then the device of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if throw_on_early_termination is enabled, both of which using an all- reduce.", "Example:", "Notifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.", "This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel.", "Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.", "joinable (Joinable) \u2013 the Joinable object calling this method.", "An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise."]}, {"name": "torch.distributed.algorithms.Join.notify_join_context()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join.notify_join_context", "type": "Miscellaneous", "text": ["Notifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.", "This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel.", "Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.", "joinable (Joinable) \u2013 the Joinable object calling this method.", "An async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise."]}, {"name": "torch.distributed.algorithms.Joinable", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable", "type": "Miscellaneous", "text": ["This defines an abstract base class for joinable classes. A joinable class (inheriting from Joinable) should implement join_hook(), which returns a JoinHook instance, in addition to join_device() and join_process_group() that return device and process group information, respectively.", "Returns the device from which to perform collective communications needed by the join context manager implementation itself.", "Returns a JoinHook instance for the given Joinable.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "JoinHook", "Returns the process group for the collective communications needed by the join context manager itself."]}, {"name": "torch.distributed.algorithms.Joinable.join_device", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_device", "type": "Miscellaneous", "text": ["Returns the device from which to perform collective communications needed by the join context manager implementation itself."]}, {"name": "torch.distributed.algorithms.Joinable.join_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_hook", "type": "Miscellaneous", "text": ["Returns a JoinHook instance for the given Joinable.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "JoinHook"]}, {"name": "torch.distributed.algorithms.Joinable.join_process_group", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_process_group", "type": "Miscellaneous", "text": ["Returns the process group for the collective communications needed by the join context manager itself."]}, {"name": "torch.distributed.algorithms.JoinHook", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook", "type": "Miscellaneous", "text": ["This defines a join hook, which provides two entry points in the join context manager: a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.", "To implement a join hook for the generic join context manager, define a class that inherits from JoinHook and override main_hook() and post_hook() as appropriate.", "This hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step).", "This hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.", "is_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise."]}, {"name": "torch.distributed.algorithms.JoinHook.main_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.main_hook", "type": "Miscellaneous", "text": ["This hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step)."]}, {"name": "torch.distributed.algorithms.JoinHook.post_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.post_hook", "type": "Miscellaneous", "text": ["This hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.", "is_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise."]}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "Distributed Communication", "text": ["Gathers tensors from the whole group in a list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_into_tensor()", "path": "distributed#torch.distributed.all_gather_into_tensor", "type": "Distributed Communication", "text": ["Gather tensors from all ranks and put them in a single output tensor.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Warning", "The Gloo backend does not support this API."]}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "Distributed Communication", "text": ["Gathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU", "Only nccl backend is currently supported tensors should only be GPU tensors", "Complex tensors are supported.", "output_tensor_lists (List[List[Tensor]]) \u2013 ", "Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i].", "Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j]", "Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "Distributed Communication", "text": ["Gathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling all_gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using all_gather() instead."]}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines in such a way that all get the final result.", "After the call tensor is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU.", "After the call, all tensor in tensor_list is going to be bitwise identical in all processes.", "Complex tensors are supported.", "Only nccl and gloo backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "Distributed Communication", "text": ["Each process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all is experimental and subject to change."]}, {"name": "torch.distributed.all_to_all_single()", "path": "distributed#torch.distributed.all_to_all_single", "type": "Distributed Communication", "text": ["Each process splits input tensor and then scatters the split list to all processes in a group. Then concatenate the received tensors from all the processes in the group and return single output tensor.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "all_to_all_single is experimental and subject to change."]}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC", "text": ["Kicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.", "We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.", "We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API."]}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC", "text": ["Context object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass."]}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC", "text": ["Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.", "context_id (int) \u2013 The autograd context id for which we should retrieve the gradients.", "A map where the key is the Tensor and the value is the associated gradient for that Tensor."]}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "Distributed Communication", "text": ["An enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered backends.", "The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL.", "This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".", "Note", "The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.", "Registers a new backend with the given name and instantiating function.", "This class method is used by 3rd party ProcessGroup extension to register new backends.", "Note", "This support of 3rd party backend is experimental and subject to change."]}, {"name": "torch.distributed.Backend.register_backend()", "path": "distributed#torch.distributed.Backend.register_backend", "type": "Distributed Communication", "text": ["Registers a new backend with the given name and instantiating function.", "This class method is used by 3rd party ProcessGroup extension to register new backends.", "Note", "This support of 3rd party backend is experimental and subject to change."]}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "Distributed Communication", "text": ["Synchronizes all processes.", "This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.batch_isend_irecv()", "path": "distributed#torch.distributed.batch_isend_irecv", "type": "Distributed Communication", "text": ["Send or Receive a batch of tensors asynchronously and return a list of requests.", "Process each of the operations in p2p_op_list and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.", "p2p_op_list \u2013 A list of point-to-point operations(type of each operator is torch.distributed.P2POp). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.", "A list of distributed request objects returned by calling the corresponding op in the op_list.", "Note", "Note that when this API is used with the NCCL PG backend, users must set the current GPU device with torch.cuda.set_device, otherwise it will lead to unexpected hang issues.", "In addition, if this API is the first collective call in the group passed to dist.P2POp, all ranks of the group must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the group, batched P2P operations involving only a subset of ranks of the group are allowed."]}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "Distributed Communication", "text": ["Broadcasts the tensor to the whole group.", "tensor must have the same number of elements in all processes participating in the collective.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "Distributed Communication", "text": ["Broadcasts the tensor to the whole group with multiple GPU tensors per node.", "tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU", "Only nccl and gloo backend are currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "Distributed Communication", "text": ["Broadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.", "None. If rank is part of the group, object_list will contain the broadcasted objects from src rank.", "Note", "For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Note", "Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling broadcast_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using broadcast() instead."]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner", "type": "Distributed Checkpoint", "text": ["DefaultLoadPlanner that adds multiple features on top of LoadPlanner.", "In particular it adds the following:", "flatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode", "This is an extension from the planner interface to make it easy to extend the default planner", "Tensor", "This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Tensor"]}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Any", "This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.lookup_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner", "Any"]}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.transform_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.transform_object", "type": "Distributed Checkpoint", "text": ["This is an extension from the planner interface to make it easy to extend the default planner"]}, {"name": "torch.distributed.checkpoint.FileSystemReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemReader", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.FileSystemWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemWriter", "type": "Distributed Checkpoint", "text": ["Basic implementation of StorageWriter using file IO.", "This implementation makes the following assumptions and simplifications:", "The checkpoint consist of one file per write request plus a .metadata file with the serialized metadata."]}, {"name": "torch.distributed.checkpoint.load_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.load_state_dict", "type": "Distributed Checkpoint", "text": ["Loads a distributed state_dict in SPMD style.", "Each rank will try to read the least amount of data necessary to fullfill the requested state_dict. When loading ShardedTensor instances, each rank only reads data for their local shards.", "Warning", "All tensors in state_dict must be allocated on their destination device prior to calling this function.", "All non-tensor data is loaded using torch.load() and modified in place on state_dict.", "Warning", "Users must call load_state_dict on the root module to ensure load pos-processing and non-tensor data properly propagates.", "None.", "None", "Note", "load_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device()."]}, {"name": "torch.distributed.checkpoint.LoadPlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlan", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.LoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner", "type": "Distributed Checkpoint", "text": ["Abstract class defining the protocol used by load_state_dict to plan the load process.", "LoadPlanner are stateful objects that can be used to customize the whole load process.", "LoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.", "A planner subclass can expect the following sequence of calls during load_state_dict:", "Signals the start of loading a checkpoint.", "Process the state_dict and produces a LoadPlan that will be sent for global planning.", "Takes the LoadPlan from all ranks and make any global decision.", "This is called once per non-tensor value in state_dict.", "They are called in pair for each Tensor value in state_dict.", "Users are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method.", "There are two usual patterns of extension:", "Rewriting state_dict. This is the simplest way to extend the load process as it doesn\u2019t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place", "Modifying resolve_tensor and commit_tensor to handle load time transformation.", "This method is called once the StorageReader finished loading data into tensor.", "The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict.", "The contents of tensor will follow its device synchronization model.", "Compute the global load plan and return plans for each rank.", ". N.B. This is called on the coordinator rank only", "List[LoadPlan]", "Create a LoadPlan based on state_dict and metadata provided by set_up_planner.", ". N.B. This is called on every rank.", "LoadPlan", "Accept the plan from coordinator and return final LoadPlan.", "LoadPlan", "Load the item described by read_item``and ``value.", "This method is expected to modify in-place the underlying state_dict.", "The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded.", "Return the tensor described by read_item to be used by the StorageReader to load read_item.", "The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.", "Tensor", "Initialize this instance to load data into state_dict", ". N.B. This is called on every rank."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.commit_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.commit_tensor", "type": "Distributed Checkpoint", "text": ["This method is called once the StorageReader finished loading data into tensor.", "The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict.", "The contents of tensor will follow its device synchronization model."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_global_plan", "type": "Distributed Checkpoint", "text": ["Compute the global load plan and return plans for each rank.", ". N.B. This is called on the coordinator rank only", "List[LoadPlan]"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_local_plan", "type": "Distributed Checkpoint", "text": ["Create a LoadPlan based on state_dict and metadata provided by set_up_planner.", ". N.B. This is called on every rank.", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.finish_plan", "type": "Distributed Checkpoint", "text": ["Accept the plan from coordinator and return final LoadPlan.", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.load_bytes()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.load_bytes", "type": "Distributed Checkpoint", "text": ["Load the item described by read_item``and ``value.", "This method is expected to modify in-place the underlying state_dict.", "The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded."]}, {"name": "torch.distributed.checkpoint.LoadPlanner.resolve_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.resolve_tensor", "type": "Distributed Checkpoint", "text": ["Return the tensor described by read_item to be used by the StorageReader to load read_item.", "The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.", "Tensor"]}, {"name": "torch.distributed.checkpoint.LoadPlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.set_up_planner", "type": "Distributed Checkpoint", "text": ["Initialize this instance to load data into state_dict", ". N.B. This is called on every rank."]}, {"name": "torch.distributed.checkpoint.ReadItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.ReadItem", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.save_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.save_state_dict", "type": "Distributed Checkpoint", "text": ["Saves a distributed model in SPMD style.", "This function is different from torch.save() as it handles ShardedTensor by having each rank only save their local shards.", "Warning", "There is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.", "Warning", "If using the process_group argument, make sure that only its ranks call save_state_dict and that all data in state_dict belong to it.", "Note", "When saving checkpoint for FSDP\u2019s ShardingStrategy.HYBRID_SHARD, only one of the shard_group should be calling save_state_dict and the corresponding process group needs to be passed in.", "Note", "This function can be used to save a state_dict without having a process group initialized by passing no_dist=True.", "Metadata object for the saved checkpoint.", "Metadata", "Note", "save_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device()."]}, {"name": "torch.distributed.checkpoint.SavePlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlan", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.checkpoint.SavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner", "type": "Distributed Checkpoint", "text": ["Abstract class defining the protocol used by save_state_dict to plan the save process.", "SavePlanners are stateful objects that can be used to customize the whole save process.", "SavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.", "A planner subclass can expect the following sequence of calls during save_state_dict:", "Signals the start of a checkpoint save.", "Process the state_dict and produces a SavePlan that will be sent for global planning.", "Takes the SavePlan from all ranks and make any global decision.", "This gives each rank a chance to adjust to global planning decisions.", "Lookups a value on the state_dict for the storage layer to write.", "Users are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method.", "There are 3 usual patterns of extension:", "Rewriting state_dict. This is the simplest way to extend the save process as it doesn\u2019t requite understanding the intrincacies of how SavePlan works:", "Modifying local plan and lookup in tandem. This is useful when fine control of how data is persisted", "Using the global planning step to make central decisions that can\u2019t be made individually by each rank", "Finally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:", "Compute the global checkpoint plan and return the local plan of each rank.", "This is called on the coordinator rank only.", "Tuple[List[SavePlan], Metadata]", "Compute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.", "This is called on all ranks.", "SavePlan", "Merge the plan created by create_local_plan and the result of create_global_plan.", "This is called on all ranks.", "SavePlan", "Lookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it.", "Called on each rank multiple times, at least once per WriteItem in the final SavePlan.", "This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.", "Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.", "When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.", "Union[Tensor, BytesIO]", "Initialize this planner to save state_dict.", "Implementations should save those values as they won\u2019t be provided lated in the save process.", "This is called on all ranks."]}, {"name": "torch.distributed.checkpoint.SavePlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_global_plan", "type": "Distributed Checkpoint", "text": ["Compute the global checkpoint plan and return the local plan of each rank.", "This is called on the coordinator rank only.", "Tuple[List[SavePlan], Metadata]"]}, {"name": "torch.distributed.checkpoint.SavePlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_local_plan", "type": "Distributed Checkpoint", "text": ["Compute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.", "This is called on all ranks.", "SavePlan"]}, {"name": "torch.distributed.checkpoint.SavePlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.finish_plan", "type": "Distributed Checkpoint", "text": ["Merge the plan created by create_local_plan and the result of create_global_plan.", "This is called on all ranks.", "SavePlan"]}, {"name": "torch.distributed.checkpoint.SavePlanner.resolve_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.resolve_data", "type": "Distributed Checkpoint", "text": ["Lookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it.", "Called on each rank multiple times, at least once per WriteItem in the final SavePlan.", "This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.", "Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.", "When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.", "Union[Tensor, BytesIO]"]}, {"name": "torch.distributed.checkpoint.SavePlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.set_up_planner", "type": "Distributed Checkpoint", "text": ["Initialize this planner to save state_dict.", "Implementations should save those values as they won\u2019t be provided lated in the save process.", "This is called on all ranks."]}, {"name": "torch.distributed.checkpoint.StorageReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader", "type": "Distributed Checkpoint", "text": ["Interface used by load_state_dict to read from storage.", "One StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.", "A subclass should expected the following sequence of calls by load_state_dict:", "Perform centralized planning of storage loading.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.", "plans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.", "A list of transformed LoadPlan after storage global planning", "List[LoadPlan]", "Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.", "plan (LoadPlan) \u2013 The local plan from the LoadPlan in use.", "A transformed LoadPlan after storage local planning", "LoadPlan", "Reads all items from plan using planner to resolve the data.", "A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place.", "A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into.", "It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.", "A future that completes once all reads are finished.", "Future[None]", "Reads the checkpoint metadata.", "The metadata object associated with the checkpoint being loaded.", "Metadata", "Initialize this instance."]}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_global_plan", "type": "Distributed Checkpoint", "text": ["Perform centralized planning of storage loading.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.", "plans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.", "A list of transformed LoadPlan after storage global planning", "List[LoadPlan]"]}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_local_plan", "type": "Distributed Checkpoint", "text": ["Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.", "plan (LoadPlan) \u2013 The local plan from the LoadPlan in use.", "A transformed LoadPlan after storage local planning", "LoadPlan"]}, {"name": "torch.distributed.checkpoint.StorageReader.read_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_data", "type": "Distributed Checkpoint", "text": ["Reads all items from plan using planner to resolve the data.", "A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place.", "A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into.", "It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.", "A future that completes once all reads are finished.", "Future[None]"]}, {"name": "torch.distributed.checkpoint.StorageReader.read_metadata()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_metadata", "type": "Distributed Checkpoint", "text": ["Reads the checkpoint metadata.", "The metadata object associated with the checkpoint being loaded.", "Metadata"]}, {"name": "torch.distributed.checkpoint.StorageReader.set_up_storage_reader()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.set_up_storage_reader", "type": "Distributed Checkpoint", "text": ["Initialize this instance."]}, {"name": "torch.distributed.checkpoint.StorageWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter", "type": "Distributed Checkpoint", "text": ["Interface used by save_state_dict to write to storage.", "One StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.", "A subclass should expect the following sequence of calls.", "Writes the metadata and marks the current checkpoint as successful.", "The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.", "None", "None", "Perform centralized planning of storage.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.", "plans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.", "A list of transformed SavePlan after storage global planning", "List[SavePlan]", "Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.", "plan (SavePlan) \u2013 The local plan from the SavePlanner in use.", "A transformed SavePlan after storage local planning", "SavePlan", "Initialize this instance.", "is_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.", "Write all items from plan using planner to resolve the data.", "A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write.", "Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:", "A future that completes to a list of WriteResult", "Future[List[WriteResult]]"]}, {"name": "torch.distributed.checkpoint.StorageWriter.finish()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.finish", "type": "Distributed Checkpoint", "text": ["Writes the metadata and marks the current checkpoint as successful.", "The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.", "None", "None"]}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_global_plan", "type": "Distributed Checkpoint", "text": ["Perform centralized planning of storage.", "This method is only called on the coordinator instance.", "While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.", "plans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.", "A list of transformed SavePlan after storage global planning", "List[SavePlan]"]}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_local_plan", "type": "Distributed Checkpoint", "text": ["Perform storage-specific local planning.", "While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.", "plan (SavePlan) \u2013 The local plan from the SavePlanner in use.", "A transformed SavePlan after storage local planning", "SavePlan"]}, {"name": "torch.distributed.checkpoint.StorageWriter.set_up_storage_writer()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer", "type": "Distributed Checkpoint", "text": ["Initialize this instance.", "is_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint."]}, {"name": "torch.distributed.checkpoint.StorageWriter.write_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.write_data", "type": "Distributed Checkpoint", "text": ["Write all items from plan using planner to resolve the data.", "A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write.", "Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:", "A future that completes to a list of WriteResult", "Future[List[WriteResult]]"]}, {"name": "torch.distributed.checkpoint.WriteItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.WriteItem", "type": "Distributed Checkpoint", "text": []}, {"name": "torch.distributed.DistBackendError", "path": "distributed#torch.distributed.DistBackendError", "type": "Distributed Communication", "text": ["Exception raised when a backend error occurs in distributed"]}, {"name": "torch.distributed.elastic.agent.server.api.RunResult", "path": "elastic/agent#torch.distributed.elastic.agent.server.api.RunResult", "type": "Distributed Elastic", "text": ["Results returned by the worker executions. Run results follow an \u201call-or-nothing\u201d policy where the run is successful if and only if ALL local workers managed by this agent complete successfully.", "If the result is successful (e.g. is_failed() = False) then the return_values field contains the outputs (return values) of the workers managed by THIS agent mapped by their GLOBAL ranks. That is result.return_values[0] is the return value of global rank 0.", "Note", "return_values are only meaningful for when the worker entrypoint is a function. Workers specified as a binary entrypoint do not canonically have a return value and the return_values field is meaningless and may be empty.", "If is_failed() returns True then the failures field contains the failure information, again, mapped by the GLOBAL rank of the worker that failed.", "The keys in return_values and failures are mutually exclusive, that is, a worker\u2019s final state can only be one of: succeeded, failed. Workers intentionally terminated by the agent according to the agent\u2019s restart policy, are not represented in either return_values nor failures."]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent", "type": "Distributed Elastic", "text": ["Agent process responsible for managing one or more worker processes. The worker processes are assumed to be regular distributed PyTorch scripts. When the worker process is created by the agent, the agent provides the necessary information for the worker processes to properly initialize a torch process group.", "The exact deployment topology and ratio of agent-to-worker is dependent on the specific implementation of the agent and the user\u2019s job placement preferences. For instance, to run a distributed training job on GPU with 8 trainers (one per GPU) one can:", "Usage", "The WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.", "WorkerGroup", "Runs the agent, retrying the worker group on failures up to max_restarts.", "The result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.", "Exception - any other failures NOT related to worker process \u2013 ", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group", "type": "Distributed Elastic", "text": ["The WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.", "WorkerGroup"]}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.run()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.run", "type": "Distributed Elastic", "text": ["Runs the agent, retrying the worker group on failures up to max_restarts.", "The result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.", "Exception - any other failures NOT related to worker process \u2013 ", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "type": "Distributed Elastic", "text": ["An implementation of torchelastic.agent.server.ElasticAgent that handles host-local workers. This agent is deployed per host and is configured to spawn n workers. When using GPUs, n maps to the number of GPUs available on the host.", "The local agent does not communicate to other local agents deployed on other hosts, even if the workers may communicate inter-host. The worker id is interpreted to be a local process. The agent starts and stops all worker processes as a single unit.", "The worker function and argument passed to the worker function must be python multiprocessing compatible. To pass multiprocessing data structures to the workers you may create the data structure in the same multiprocessing context as the specified start_method and pass it as a function argument.", "The exit_barrier_timeout specifies the amount of time (in seconds) to wait for other agents to finish. This acts as a safety net to handle cases where workers finish at different times, to prevent agents from viewing workers that finished early as a scale-down event. It is strongly advised that the user code deal with ensuring that workers are terminated in a synchronous manner rather than relying on the exit_barrier_timeout.", "A named pipe based watchdog can be enabled in `LocalElasticAgent` if an environment variable TORCHELASTIC_ENABLE_FILE_TIMER with value 1 has been defined in the `LocalElasticAgent` process. Optionally, another environment variable `TORCHELASTIC_TIMER_FILE` can be set with a unique file name for the named pipe. If the environment variable `TORCHELASTIC_TIMER_FILE` is not set, `LocalElasticAgent` will internally create a unique file name and set it to the environment variable `TORCHELASTIC_TIMER_FILE`, and this environment variable will be propagated to the worker processes to allow them to connect to the same named pipe that `LocalElasticAgent` uses.", "Example launching function", "Example launching binary"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent", "type": "Distributed Elastic", "text": ["An ElasticAgent that manages workers (WorkerGroup) for a single WorkerSpec (e.g. one particular type of worker role).", "Determines proper ranks for worker processes. The rank assignment is done according to the following algorithm:", "List[Worker]", "Wait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish.", "Starts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers.", "The caller should first call _stop_workers() to stop running workers prior to calling this method.", "Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method", "Checks on the workers for the worker_group and returns the new state of the worker group.", "RunResult", "Runs rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group.", "Restarts (stops, rendezvous, starts) all local workers in the group.", "Cleans up any resources that were allocated during the agent\u2019s work.", "death_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default", "Starts worker_group.spec.local_world_size number of workers according to worker spec for the worker group .", "Returns a map of local_rank to worker id.", "Dict[int, Any]", "Stops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks", "type": "Distributed Elastic", "text": ["Determines proper ranks for worker processes. The rank assignment is done according to the following algorithm:", "List[Worker]"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier", "type": "Distributed Elastic", "text": ["Wait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers", "type": "Distributed Elastic", "text": ["Starts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers.", "The caller should first call _stop_workers() to stop running workers prior to calling this method.", "Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers", "type": "Distributed Elastic", "text": ["Checks on the workers for the worker_group and returns the new state of the worker group.", "RunResult"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous", "type": "Distributed Elastic", "text": ["Runs rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers", "type": "Distributed Elastic", "text": ["Restarts (stops, rendezvous, starts) all local workers in the group."]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown", "type": "Distributed Elastic", "text": ["Cleans up any resources that were allocated during the agent\u2019s work.", "death_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers", "type": "Distributed Elastic", "text": ["Starts worker_group.spec.local_world_size number of workers according to worker spec for the worker group .", "Returns a map of local_rank to worker id.", "Dict[int, Any]"]}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers", "type": "Distributed Elastic", "text": ["Stops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc."]}, {"name": "torch.distributed.elastic.agent.server.Worker", "path": "elastic/agent#torch.distributed.elastic.agent.server.Worker", "type": "Distributed Elastic", "text": ["Represents a worker instance. Contrast this with WorkerSpec that represents the specifications of a worker. A Worker is created from a WorkerSpec. A Worker is to a WorkerSpec as an object is to a class.", "The id of the worker is interpreted by the specific implementation of ElasticAgent. For a local agent, it could be the pid (int) of the worker, for a remote agent it could be encoded as host:port (string)."]}, {"name": "torch.distributed.elastic.agent.server.WorkerGroup", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerGroup", "type": "Distributed Elastic", "text": ["Represents the set of Worker instances for the given WorkerSpec managed by ElasticAgent. Whether the worker group contains cross instance workers or not depends on the implementation of the agent."]}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec", "type": "Distributed Elastic", "text": ["Contains blueprint information about a particular type of worker. For a given role, there must only exist a single worker spec. Worker spec is expected to be homogeneous across all nodes (machine), that is each node runs the same number of workers for a particular spec.", "If the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name."]}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name", "type": "Distributed Elastic", "text": ["If the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name."]}, {"name": "torch.distributed.elastic.agent.server.WorkerState", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState", "type": "Distributed Elastic", "text": ["State of the WorkerGroup. Workers in a worker group change state as a unit. If a single worker in a worker group fails the entire set is considered failed:", "A worker group starts from an initial INIT state, then progresses to HEALTHY or UNHEALTHY states, and finally reaches a terminal SUCCEEDED or FAILED state.", "Worker groups can be interrupted and temporarily put into STOPPED state by the agent. Workers in STOPPED state are scheduled to be restarted in the near future by the agent. Some examples of workers being put into STOPPED state are:", "When actions (start, stop, rdzv, retry, etc) on worker group fails and results in the action being partially applied to the worker group the state will be UNKNOWN. Typically this happens on uncaught/unhandled exceptions during state change events on the agent. The agent is not expected to recover worker groups in UNKNOWN state and is better off self terminating and allowing the job manager to retry the node.", "True if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).", "bool"]}, {"name": "torch.distributed.elastic.agent.server.WorkerState.is_running()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState.is_running", "type": "Distributed Elastic", "text": ["True if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).", "bool"]}, {"name": "torch.distributed.elastic.events.api.Event", "path": "elastic/events#torch.distributed.elastic.events.api.Event", "type": "Distributed Elastic", "text": ["The class represents the generic event that occurs during the torchelastic job execution. The event can be any kind of meaningful action."]}, {"name": "torch.distributed.elastic.events.api.EventMetadataValue", "path": "elastic/events#torch.distributed.elastic.events.api.EventMetadataValue", "type": "Distributed Elastic", "text": ["alias of Optional[Union[str, int, float, bool]]"]}, {"name": "torch.distributed.elastic.events.api.EventSource", "path": "elastic/events#torch.distributed.elastic.events.api.EventSource", "type": "Distributed Elastic", "text": ["Known identifiers of the event producers."]}, {"name": "torch.distributed.elastic.events.get_logging_handler()", "path": "elastic/events#torch.distributed.elastic.events.get_logging_handler", "type": "Distributed Elastic", "text": ["Handler"]}, {"name": "torch.distributed.elastic.events.record()", "path": "elastic/events#torch.distributed.elastic.events.record", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.MetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.MetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.api.NullMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.NullMetricHandler", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.configure()", "path": "elastic/metrics#torch.distributed.elastic.metrics.configure", "type": "Distributed Elastic", "text": []}, {"name": "torch.distributed.elastic.metrics.prof()", "path": "elastic/metrics#torch.distributed.elastic.metrics.prof", "type": "Distributed Elastic", "text": ["@profile decorator publishes duration.ms, count, success, failure metrics for the function that it decorates. The metric name defaults to the qualified name (class_name.def_name) of the function. If the function does not belong to a class, it uses the leaf module name instead.", "Usage"]}, {"name": "torch.distributed.elastic.metrics.put_metric()", "path": "elastic/metrics#torch.distributed.elastic.metrics.put_metric", "type": "Distributed Elastic", "text": ["Publishes a metric data point.", "Usage"]}, {"name": "torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "type": "Distributed Elastic", "text": ["PContext holding worker processes invoked as a function."]}, {"name": "torch.distributed.elastic.multiprocessing.api.PContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.PContext", "type": "Distributed Elastic", "text": ["The base class that standardizes operations over a set of processes that are launched via different mechanisms. The name PContext is intentional to disambiguate with torch.multiprocessing.ProcessContext.", "Warning", "stdouts and stderrs should ALWAYS be a superset of tee_stdouts and tee_stderrs (respectively) this is b/c tee is implemented as a redirect + tail -f <stdout/stderr.log>"]}, {"name": "torch.distributed.elastic.multiprocessing.api.RunProcsResult", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.RunProcsResult", "type": "Distributed Elastic", "text": ["Results of a completed run of processes started with start_processes(). Returned by PContext.", "Note the following:"]}, {"name": "torch.distributed.elastic.multiprocessing.api.SubprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.SubprocessContext", "type": "Distributed Elastic", "text": ["PContext holding worker processes invoked as a binary."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "type": "Distributed Elastic", "text": ["Special exception type that can be raised from a function annotated with the @record decorator to have the child process\u2019 (root exception) propagate up the stack as-is (e.g. without being wrapped in the parent\u2019s traceback).", "Useful in cases where the parent is a simple nanny process and the child (worker) processes are actually doing meaningful compute. In this case, errors typically occur on the child process as the parent is not doing anything non-trivial, and child errors should be propagated to the scheduler for accurate root cause diagnostics.", "Note", "The propagation relies on error files rather than exception handling to support both function and binary launches.", "Example:", "In the example above, trainer 1\u2019s failure (written into error.json) is the root cause and should be reported to the scheduler\u2019s init process. The torchelastic agent raises a ChildFailedError(\"trainer\", {1: \"trainer_1/error.json\"}) upon detecting trainer 1\u2019s failure which would propagate the contents of trainer 1\u2019s error file to the scheduler\u2019s init process."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "type": "Distributed Elastic", "text": ["Writes the provided exception object along with some other metadata about the error in a structured way in JSON format to an error file specified by the environment variable: TORCHELASTIC_ERROR_FILE. If this environment variable is not set, then simply logs the contents of what would have been written to the error file.", "This handler may be subclassed to customize the handling of the error. Subclasses should override initialize() and record_exception()."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "type": "Distributed Elastic", "text": ["Represents the failed process result. When the worker process fails, it may record failure root cause into the file. Tries to read the failure timestamp from the provided error_file, if the error_file does not exist, the timestamp is the current timestamp (seconds since epoch).", "The message field is a concise explanation of the failure. If the error file exists then the message is obtained from the error file. Otherwise one is generated based on the failure signature.", "Note", "It is assumed that the error_file is written by torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler. Otherwise the behavior is undefined."]}, {"name": "torch.distributed.elastic.multiprocessing.errors.record()", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.record", "type": "Distributed Elastic", "text": ["Syntactic sugar to record errors/exceptions that happened in the decorated function using the provided error_handler.", "Using this decorator is equivalent to:", "Important", "use this decorator once per process at the top level method, typically this is the main method.", "Example", "Callable[[\u2026], T]"]}, {"name": "torch.distributed.elastic.multiprocessing.start_processes()", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.start_processes", "type": "Distributed Elastic", "text": ["Starts n copies of entrypoint processes with the provided options. entrypoint is either a Callable (function) or a str (binary). The number of copies is determined by the number of entries for args and envs arguments, which need to have the same key set.", "args and env parameters are the arguments and environment variables to pass down to the entrypoint mapped by the replica index (local rank). All local ranks must be accounted for. That is, the keyset should be {0,1,...,(nprocs-1)}.", "Note", "When the entrypoint is a binary (str), args can only be strings. If any other type is given, then it is casted to a string representation (e.g. str(arg1)). Furthermore, a binary failure will only write an error.json error file if the main function is annotated with torch.distributed.elastic.multiprocessing.errors.record. For function launches, this is done by default and there is no need to manually annotate with the @record annotation.", "redirects and tee are bitmasks specifying which std stream(s) to redirect to a log file in the log_dir. Valid mask values are defined in Std. To redirect/tee only certain local ranks, pass redirects as a map with the key as the local rank to specify the redirect behavior for. Any missing local ranks will default to Std.NONE.", "tee acts like the unix \u201ctee\u201d command in that it redirects + prints to console. To avoid worker stdout/stderr from printing to console, use the redirects parameter.", "For each process, the log_dir will contain:", "Note", "It is expected that the log_dir exists, is empty, and is a directory.", "Example:", "PContext"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "type": "Distributed Elastic", "text": ["Represents a C10d-backed rendezvous backend.", "See base class.", "Optional[Tuple[bytes, Any]]", "See base class.", "See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "type": "Distributed Elastic", "text": ["See base class."]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": ["Creates a new C10dRendezvousBackend from the specified parameters.", "Parameter", "Description", "store_type", "The type of the C10d store. The currently supported types are \u201ctcp\u201d and \u201cfile\u201d which correspond to torch.distributed.TCPStore and torch.distributed.FileStore, respectively. Defaults to \u201ctcp\u201d.", "read_timeout", "The read timeout, in seconds, for store operations. Defaults to 60 seconds.", "Note this only applies to torch.distributed.TCPStore. It is not relevant to torch.distributed.FileStore which does not take in timeout as a parameter.", "is_host", "A boolean value indicating whether this backend instance will host the C10d store. If not specified it will be inferred heuristically by matching the hostname or the IP address of this machine against the specified rendezvous endpoint. Defaults to None.", "Note that this configuration option only applies to torch.distributed.TCPStore. In normal circumstances you can safely skip it; the only time when it is needed is if its value cannot be correctly determined (e.g. the rendezvous endpoint has a CNAME as the hostname or does not match the FQDN of the machine).", "Tuple[C10dRendezvousBackend, Store]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler", "type": "Distributed Elastic", "text": ["Creates a new DynamicRendezvousHandler from the specified parameters.", "DynamicRendezvousHandler", "Parameter", "Description", "join_timeout", "The total time, in seconds, within which the rendezvous is expected to complete. Defaults to 600 seconds.", "last_call_timeout", "An additional wait amount, in seconds, before completing the rendezvous once the minimum number of nodes has been reached. Defaults to 30 seconds.", "close_timeout", "The time, in seconds, within which the rendezvous is expected to close after a call to RendezvousHandler.set_closed() or RendezvousHandler.shutdown(). Defaults to 30 seconds."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "type": "Distributed Elastic", "text": ["Represents a handler that sets up a rendezvous among a set of nodes.", "Creates a new DynamicRendezvousHandler."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend", "type": "Distributed Elastic", "text": ["Creates a new DynamicRendezvousHandler."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "type": "Distributed Elastic", "text": ["Represents a backend that holds the rendezvous state.", "Gets the rendezvous state.", "A tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.", "Optional[Tuple[bytes, Any]]", "Gets the name of the backend.", "Sets the rendezvous state.", "The new rendezvous state is set conditionally:", "A tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["Gets the rendezvous state.", "A tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "type": "Distributed Elastic", "text": ["Gets the name of the backend."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["Sets the rendezvous state.", "The new rendezvous state is set conditionally:", "A tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "type": "Distributed Elastic", "text": ["Holds the timeout configuration of a rendezvous.", "Gets the close timeout.", "Gets the keep-alive heartbeat timeout.", "Gets the join timeout.", "Gets the last call timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "type": "Distributed Elastic", "text": ["Gets the close timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "type": "Distributed Elastic", "text": ["Gets the keep-alive heartbeat timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "type": "Distributed Elastic", "text": ["Gets the join timeout."]}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "type": "Distributed Elastic", "text": ["Gets the last call timeout."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "type": "Distributed Elastic", "text": ["Implements a torch.distributed.elastic.rendezvous.RendezvousHandler interface backed by torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous. EtcdRendezvousHandler uses a URL to configure the type of rendezvous to use and to pass implementation specific configurations to the rendezvous module. The basic etcd rendezvous configuration URL looks like the following", "The URL above is interpreted as follows:", "Below are a full list of the parameters that can be passed to etcd rendezvous:", "Parameter", "Description", "min_workers", "minimum number of workers for the rendezvous to be valid", "max_workers", "maximum number of workers to admit", "timeout", "total timeout within which next_rendezvous is expected to succeed (default 600s)", "last_call_timeout", "additional wait amount (\u201clast call\u201d) after min number of workers has been reached (defaults to 30s)", "etcd_prefix", "path prefix (from etcd root), inside which all etcd nodes will be created (defaults to /torchelastic/p2p)"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": ["Creates a new EtcdRendezvousBackend from the specified parameters.", "Parameter", "Description", "read_timeout", "The read timeout, in seconds, for etcd operations. Defaults to 60 seconds.", "protocol", "The protocol to use to communicate with etcd. Valid values are \u201chttp\u201d and \u201chttps\u201d. Defaults to \u201chttp\u201d.", "ssl_cert", "The path to the SSL client certificate to use along with HTTPS. Defaults to None.", "ssl_cert_key", "The path to the private key of the SSL client certificate to use along with HTTPS. Defaults to None.", "ca_cert", "The path to the rool SSL authority certificate. Defaults to None.", "Tuple[EtcdRendezvousBackend, Store]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "type": "Distributed Elastic", "text": ["Represents an etcd-based rendezvous backend.", "See base class.", "Optional[Tuple[bytes, Any]]", "See base class.", "See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "type": "Distributed Elastic", "text": ["See base class."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state", "type": "Distributed Elastic", "text": ["See base class.", "Optional[Tuple[bytes, Any, bool]]"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "type": "Distributed Elastic", "text": ["Note", "tested on etcd server v3.4.3", "Starts and stops a local standalone etcd server on a random free port. Useful for single node, multi-worker launches or testing, where a sidecar etcd server is more convenient than having to separately setup an etcd server.", "This class registers a termination handler to shutdown the etcd subprocess on exit. This termination handler is NOT a substitute for calling the stop() method.", "The following fallback mechanism is used to find the etcd binary:", "Usage", "etcd_binary_path \u2013 path of etcd server binary (see above for fallback path)"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "type": "Distributed Elastic", "text": ["Implements a c10 Store interface by piggybacking on the rendezvous etcd instance. This is the store object returned by EtcdRendezvous", "Atomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.", "the new (incremented) value", "int", "Check if all of the keys are immediately present (without waiting).", "bool", "Get a value by key, possibly doing a blocking wait.", "If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.", "value (bytes)", "LookupError - If key still not published after timeout \u2013 ", "bytes", "Write a key/value pair into EtcdStore. Both key and value may be either Python str or bytes.", "Waits until all of the keys are published, or until timeout.", "LookupError - if timeout occurs \u2013 "]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add", "type": "Distributed Elastic", "text": ["Atomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.", "the new (incremented) value", "int"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check", "type": "Distributed Elastic", "text": ["Check if all of the keys are immediately present (without waiting).", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get", "type": "Distributed Elastic", "text": ["Get a value by key, possibly doing a blocking wait.", "If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.", "value (bytes)", "LookupError - If key still not published after timeout \u2013 ", "bytes"]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set", "type": "Distributed Elastic", "text": ["Write a key/value pair into EtcdStore. Both key and value may be either Python str or bytes."]}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait", "type": "Distributed Elastic", "text": ["Waits until all of the keys are published, or until timeout.", "LookupError - if timeout occurs \u2013 "]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousClosedError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousClosedError", "type": "Distributed Elastic", "text": ["Raised when a rendezvous is closed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousConnectionError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousConnectionError", "type": "Distributed Elastic", "text": ["Raised when the connection to a rendezvous backend has failed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousError", "type": "Distributed Elastic", "text": ["Represents the base type for rendezvous errors."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler", "type": "Distributed Elastic", "text": ["Main rendezvous interface.", "Note", "Distributed Torch users normally do not need to implement their own RendezvousHandler. An implementation based on C10d Store is already provided, and is recommended for most users.", "Returns the name of the rendezvous backend.", "str", "Returns the run id of the rendezvous.", "The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.", "str", "Checks whether the rendezvous has been closed.", "A closed rendezvous means all future attempts to re-rendezvous within same job will fail.", "is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.", "bool", "Main entry-point into the rendezvous barrier.", "Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.", "A tuple of torch.distributed.Store, rank, and world size.", "Tuple[Store, int, int]", "Returns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group.", "Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).", "int", "Marks the rendezvous as closed.", "Closes all resources that were open for the rendezvous.", "Example:", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend", "type": "Distributed Elastic", "text": ["Returns the name of the rendezvous backend.", "str"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id", "type": "Distributed Elastic", "text": ["Returns the run id of the rendezvous.", "The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.", "str"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed", "type": "Distributed Elastic", "text": ["Checks whether the rendezvous has been closed.", "A closed rendezvous means all future attempts to re-rendezvous within same job will fail.", "is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous", "type": "Distributed Elastic", "text": ["Main entry-point into the rendezvous barrier.", "Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.", "A tuple of torch.distributed.Store, rank, and world size.", "Tuple[Store, int, int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting", "type": "Distributed Elastic", "text": ["Returns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group.", "Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).", "int"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed", "type": "Distributed Elastic", "text": ["Marks the rendezvous as closed."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown", "type": "Distributed Elastic", "text": ["Closes all resources that were open for the rendezvous.", "Example:", "bool"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "type": "Distributed Elastic", "text": ["Represents a registry of RendezvousHandler backends.", "Creates a new RendezvousHandler.", "RendezvousHandler", "Registers a new rendezvous backend."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler", "type": "Distributed Elastic", "text": ["Creates a new RendezvousHandler.", "RendezvousHandler"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register", "type": "Distributed Elastic", "text": ["Registers a new rendezvous backend."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters", "type": "Distributed Elastic", "text": ["Holds the parameters to construct a RendezvousHandler.", "Returns the value for key if key exists, else default.", "Any", "Returns the value for key as a bool.", "Optional[bool]", "Returns the value for key as an int.", "Optional[int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get", "type": "Distributed Elastic", "text": ["Returns the value for key if key exists, else default.", "Any"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool", "type": "Distributed Elastic", "text": ["Returns the value for key as a bool.", "Optional[bool]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int", "type": "Distributed Elastic", "text": ["Returns the value for key as an int.", "Optional[int]"]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousStateError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousStateError", "type": "Distributed Elastic", "text": ["Raised when the state of a rendezvous is corrupt."]}, {"name": "torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "type": "Distributed Elastic", "text": ["Raised when a rendezvous did not complete on time."]}, {"name": "torch.distributed.elastic.timer.configure()", "path": "elastic/timer#torch.distributed.elastic.timer.configure", "type": "Distributed Elastic", "text": ["Configures a timer client. Must be called before using expires."]}, {"name": "torch.distributed.elastic.timer.expires()", "path": "elastic/timer#torch.distributed.elastic.timer.expires", "type": "Distributed Elastic", "text": ["Acquires a countdown timer that expires in after seconds from now, unless the code-block that it wraps is finished within the timeframe. When the timer expires, this worker is eligible to be reaped. The exact meaning of \u201creaped\u201d depends on the client implementation. In most cases, reaping means to terminate the worker process. Note that the worker is NOT guaranteed to be reaped at exactly time.now() + after, but rather the worker is \u201celigible\u201d for being reaped and the TimerServer that the client talks to will ultimately make the decision when and how to reap the workers with expired timers.", "Usage:"]}, {"name": "torch.distributed.elastic.timer.FileTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerClient", "type": "Distributed Elastic", "text": ["Client side of FileTimerServer. This client is meant to be used on the same host that the FileTimerServer is running on and uses pid to uniquely identify a worker. This client uses a named_pipe to send timer requests to the FileTimerServer. This client is a producer while the FileTimerServer is a consumer. Multiple clients can work with the same FileTimerServer."]}, {"name": "torch.distributed.elastic.timer.FileTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerServer", "type": "Distributed Elastic", "text": ["Server that works with FileTimerClient. Clients are expected to be running on the same host as the process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host)."]}, {"name": "torch.distributed.elastic.timer.LocalTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerClient", "type": "Distributed Elastic", "text": ["Client side of LocalTimerServer. This client is meant to be used on the same host that the LocalTimerServer is running on and uses pid to uniquely identify a worker. This is particularly useful in situations where one spawns a subprocess (trainer) per GPU on a host with multiple GPU devices."]}, {"name": "torch.distributed.elastic.timer.LocalTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerServer", "type": "Distributed Elastic", "text": ["Server that works with LocalTimerClient. Clients are expected to be subprocesses to the parent process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host)."]}, {"name": "torch.distributed.elastic.timer.TimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient", "type": "Distributed Elastic", "text": ["Client library to acquire and release countdown timers by communicating with the TimerServer.", "Acquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer.", "Releases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect."]}, {"name": "torch.distributed.elastic.timer.TimerClient.acquire()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.acquire", "type": "Distributed Elastic", "text": ["Acquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer."]}, {"name": "torch.distributed.elastic.timer.TimerClient.release()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.release", "type": "Distributed Elastic", "text": ["Releases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect."]}, {"name": "torch.distributed.elastic.timer.TimerRequest", "path": "elastic/timer#torch.distributed.elastic.timer.TimerRequest", "type": "Distributed Elastic", "text": ["Data object representing a countdown timer acquisition and release that is used between the TimerClient and TimerServer. A negative expiration_time should be interpreted as a \u201crelease\u201d request.", "Note", "the type of worker_id is implementation specific. It is whatever the TimerServer and TimerClient implementations have on to uniquely identify a worker."]}, {"name": "torch.distributed.elastic.timer.TimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer", "type": "Distributed Elastic", "text": ["Entity that monitors active timers and expires them in a timely fashion. This server is responsible for reaping workers that have expired timers.", "Clears all timers for the given worker_ids.", "Returns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.", "Dict[str, List[TimerRequest]]", "Processes the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request."]}, {"name": "torch.distributed.elastic.timer.TimerServer.clear_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.clear_timers", "type": "Distributed Elastic", "text": ["Clears all timers for the given worker_ids."]}, {"name": "torch.distributed.elastic.timer.TimerServer.get_expired_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.get_expired_timers", "type": "Distributed Elastic", "text": ["Returns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.", "Dict[str, List[TimerRequest]]"]}, {"name": "torch.distributed.elastic.timer.TimerServer.register_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.register_timers", "type": "Distributed Elastic", "text": ["Processes the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request."]}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "Distributed Communication", "text": ["A store implementation that uses a file to store the underlying key-value pairs."]}, {"name": "torch.distributed.fsdp.BackwardPrefetch", "path": "fsdp#torch.distributed.fsdp.BackwardPrefetch", "type": "Fully Sharded Data Parallel", "text": ["This configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.", "For more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders."]}, {"name": "torch.distributed.fsdp.CPUOffload", "path": "fsdp#torch.distributed.fsdp.CPUOffload", "type": "Fully Sharded Data Parallel", "text": ["This configures CPU offloading.", "offload_params (bool) \u2013 This specifies whether to offload parameters to CPU when not involved in computation. If True, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU."]}, {"name": "torch.distributed.fsdp.FullOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["rank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)"]}, {"name": "torch.distributed.fsdp.FullStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["FullStateDictConfig is a config class meant to be used with StateDictType.FULL_STATE_DICT. We recommend enabling both offload_to_cpu=True and rank0_only=True when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the state_dict_type() context manager as follows:", "rank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel", "type": "Fully Sharded Data Parallel", "text": ["A wrapper for sharding module parameters across data parallel workers. This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. FullyShardedDataParallel is commonly shortened to FSDP.", "Example:", "Warning", "The optimizer must be initialized after the module has been wrapped with FSDP since FSDP will shard and transform the module\u2019s parameters in a way that may not preserve the original parameter variables. Thus, the previously initialized optimizer may have stale references to the parameters.", "Warning", "If the destination CUDA device has ID dev_id, either (1) module should already be placed on that device, (2) the device should be set using torch.cuda.set_device(dev_id), or (3) dev_id should be passed into the device_id constructor argument. This FSDP instance\u2019s compute device will be that destination device. For (1) and (3), the FSDP initialization always occurs on GPU. For (2), the FSDP initialization happens on module \u2018s current device, which may be CPU.", "Warning", "FSDP currently does not support gradient accumulation outside no_sync() when using CPU offloading. Trying to do so yields incorrect results since FSDP will use the newly-reduced gradient instead of accumulating with any existing gradient.", "Warning", "Changing the original parameter variable names after construction will lead to undefined behavior.", "Warning", "Passing in the sync_module_states=True flag requires module to be on GPU or to use the device_id argument to specify a CUDA device that FSDP will move module to in the FSDP constructor. This is because sync_module_states=True requires GPU communication.", "Warning", "As of PyTorch 1.12, FSDP only offers limited support for shared parameters (for example, setting one Linear layer\u2019s weight to another\u2019s). In particular, modules that share parameters must be wrapped as part of the same FSDP unit. If enhanced shared parameter support is needed for your use case, please ping https://github.com/pytorch/pytorch/issues/77724", "Warning", "FSDP has some constraints on freezing parameters (i.e. setting param.requires_grad=False). For use_orig_params=False, each FSDP instance must manage parameters that are all frozen or all non-frozen. For use_orig_params=True, FSDP supports mixing frozen and non-frozen, but we recommend not doing so since then the gradient memory usage will be higher than expected (namely, equivalent to not freezing those parameters). This means that ideally, frozen parameters should be isolated into their own nn.Module s and wrapped separately with FSDP.", "Note", "Attempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule\u2019s parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit.", "Note", "FSDP moves input tensors to the forward method to the GPU compute device, so the user does not need to manually move them from CPU.", "Warning", "The user should not modify the parameters between forward and backward without using the summon_full_params() context since the modifications may not persist. Moreover, for use_orig_params=False, accessing the original parameters between forward and backward may raise an illegal memory access.", "Warning", "For use_orig_params=True, ShardingStrategy.SHARD_GRAD_OP exposes the unsharded parameters, not the sharded parameters, after forward since it does not free the unsharded ones, unlike ShardingStrategy.FULL_SHARD. One caveat is that, since gradients are always sharded or None, ShardingStrategy.SHARD_GRAD_OP will not expose the sharded gradients with the unsharded parameters after forward. If you want to inspect the gradients, try summon_full_params() with with_grads=True.", "Warning", "FSDP replaces managed modules\u2019 parameters with torch.Tensor views during forward and backward computation for autograd-related reasons. If your module\u2019s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP\u2019s newly created views, and autograd will not work correctly.", "Note", "With limit_all_gathers=True, you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.", "Note", "When using sharding_strategy=ShardingStrategy.HYBRID_SHARD with the sharding process group being intra-node and the replication process group being inter-node, setting NCCL_CROSS_NIC=1 can help improve the all-reduce times over the replication process group for some cluster setups.", "auto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy]]) \u2013 ", "This specifies a policy to apply FSDP to submodules of module, which is needed for communication and computation overlap and thus affects performance. If None, then FSDP only applies to module, and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts ModuleWrapPolicy directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments module: nn.Module, recurse: bool, and nonwrapped_numel: int and should return a bool specifying whether the passed-in module should have FSDP applied if recurse=False or if the traversal should continue into the module\u2019s subtree if recurse=True. Users may add additional arguments to the callable. The size_based_auto_wrap_policy in torch.distributed.fsdp.wrap.py gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.", "Example:", "param_init_fn (Optional[Callable[[nn.Module], None]]) \u2013 ", "A Callable[torch.nn.Module] -> None that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via is_meta and either applies param_init_fn if specified or calls nn.Module.reset_parameters() otherwise. For both cases, the implementation should only initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX\u2019s (https://github.com/pytorch/torchdistX) deferred_init() API, where the deferred modules are initialized by calling param_init_fn if specified or torchdistX\u2019s default materialize_module() otherwise. If param_init_fn is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.", "Example:", "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.", "fn (Module -> None) \u2013 function to be applied to each submodule", "self", "Clips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.", "Total norm of the parameters (viewed as a single vector).", "Tensor", "Note", "If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().", "Note", "If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.", "Note", "The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.", "Warning", "This needs to be called on all ranks since it uses collective communications.", "The API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.", "Refer to shard_full_optim_state_dict().", "Dict[str, Any]", "Runs the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.", "Any", "Returns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.", "FSDP modules that are nested in the input module.", "List[FullyShardedDataParallel]", "Consolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.", "Warning", "This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.", "Warning", "Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.", "Note", "Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().", "A dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.", "Dict[str, Any]", "Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.", "A StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.", "StateDictSettings", "Returns the wrapped module (like DistributedDataParallel).", "Overrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Tensor]]", "Overrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Parameter]]", "A context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.", "Note", "This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.", "Note", "When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.", "Generator", "Transforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.", "For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM.", "For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory.", "For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).", "Example:", "A dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.", "Dict[str, Any]", "Given a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel.", "Dict[str, Any]", "Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.", "Warning", "FSDP communication hook should be registered before running an initial forward pass and only once.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.", "Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.", "To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model:", "To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:", "The optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.", "Dict[str, Any]", "Scatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]", "Set the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.", "Note", "This API should be called for only the top-level (root) module.", "Note", "This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:", "Example:", "A StateDictSettings that include the previous state_dict type and configuration for the module.", "StateDictSettings", "Shards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]", "The API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):.", "For the detailed usage, refer to full_optim_state_dict().", "Warning", "The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.", "Dict[str, Any]", "A context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail.", "Example:", "Generator", "A context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.", "Note", "This can be used on inner FSDPs.", "Note", "This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.", "Note", "Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.", "Note", "The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.", "Note", "This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.", "Warning", "Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.", "Warning", "Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.apply()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.apply", "type": "Fully Sharded Data Parallel", "text": ["Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).", "Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.", "fn (Module -> None) \u2013 function to be applied to each submodule", "self"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_", "type": "Fully Sharded Data Parallel", "text": ["Clips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.", "Total norm of the parameters (viewed as a single vector).", "Tensor", "Note", "If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().", "Note", "If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.", "Note", "The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.", "Warning", "This needs to be called on all ranks since it uses collective communications."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["The API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.", "Refer to shard_full_optim_state_dict().", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.forward()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.forward", "type": "Fully Sharded Data Parallel", "text": ["Runs the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.", "Any"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules", "type": "Fully Sharded Data Parallel", "text": ["Returns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.", "FSDP modules that are nested in the input module.", "List[FullyShardedDataParallel]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Consolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.", "Warning", "This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.", "Warning", "Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.", "Note", "Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().", "A dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["Get the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.", "A StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.", "StateDictSettings"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.module", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.module", "type": "Fully Sharded Data Parallel", "text": ["Returns the wrapped module (like DistributedDataParallel)."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_buffers()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers", "type": "Fully Sharded Data Parallel", "text": ["Overrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Tensor]]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_parameters()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters", "type": "Fully Sharded Data Parallel", "text": ["Overrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.", "Iterator[Tuple[str, Parameter]]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.no_sync()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.no_sync", "type": "Fully Sharded Data Parallel", "text": ["A context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.", "Note", "This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.", "Note", "When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Transforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.", "For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM.", "For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory.", "For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).", "Example:", "A dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load", "type": "Fully Sharded Data Parallel", "text": ["Given a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook", "type": "Fully Sharded Data Parallel", "text": ["Registers a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.", "Warning", "FSDP communication hook should be registered before running an initial forward pass and only once.", "state (object) \u2013 ", "Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker."]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Re-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.", "To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model:", "To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:", "The optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Scatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["Set the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.", "Note", "This API should be called for only the top-level (root) module.", "Note", "This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:", "Example:", "A StateDictSettings that include the previous state_dict type and configuration for the module.", "StateDictSettings"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["Shards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict().", "Example:", "Note", "Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.", "The full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": ["The API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):.", "For the detailed usage, refer to full_optim_state_dict().", "Warning", "The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.", "Dict[str, Any]"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type", "type": "Fully Sharded Data Parallel", "text": ["A context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail.", "Example:", "Generator"]}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params", "type": "Fully Sharded Data Parallel", "text": ["A context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.", "Note", "This can be used on inner FSDPs.", "Note", "This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.", "Note", "Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.", "Note", "The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.", "Note", "This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.", "Warning", "Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.", "Warning", "Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.", "Generator"]}, {"name": "torch.distributed.fsdp.LocalOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.LocalStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.MixedPrecision", "path": "fsdp#torch.distributed.fsdp.MixedPrecision", "type": "Fully Sharded Data Parallel", "text": ["This configures FSDP-native mixed precision training.", "Note", "This API is experimental and subject to change.", "Note", "Only floating point tensors are cast to their specified dtypes.", "Note", "In summon_full_params, parameters are forced to full precision, but buffers are not.", "Note", "Layer norm and batch norm accumulate in float32 even when their inputs are in a low precision like float16 or bfloat16. Disabling FSDP\u2019s mixed precision for those norm modules only means that the affine parameters are kept in float32. However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.", "Note", "By default, if the user passes a model with any _BatchNorm modules and specifies an auto_wrap_policy, then the batch norm modules will have FSDP applied to them separately with mixed precision disabled. See the _module_classes_to_ignore argument.", "Note", "MixedPrecision has cast_root_forward_inputs=True and cast_forward_inputs=False by default. For the root FSDP instance, its cast_root_forward_inputs takes precedence over its cast_forward_inputs. For non-root FSDP instances, their cast_root_forward_inputs values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same MixedPrecision configuration and only needs to cast inputs to the param_dtype at the beginning of the model\u2019s forward pass.", "Note", "For nested FSDP instances with different MixedPrecision configurations, we recommend setting individual cast_forward_inputs values to configure casting inputs or not before each instance\u2019s forward. In such a case, since the casts happen before each FSDP instance\u2019s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different MixedPrecision configuration.", "Example:", "The above shows a working example. On the other hand, if model[1] were replaced with model[0], meaning that the submodule using different MixedPrecision ran its forward first, then model[1] would incorrectly see float16 activations instead of bfloat16 ones."]}, {"name": "torch.distributed.fsdp.OptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.OptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["OptimStateDictConfig is the base class for all optim_state_dict configuration classes. Users should instantiate a child class (e.g. FullOptimStateDictConfig) in order to configure settings for the corresponding optim_state_dict type supported by FSDP."]}, {"name": "torch.distributed.fsdp.ShardedOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.ShardedOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.ShardedStateDictConfig", "path": "fsdp#torch.distributed.fsdp.ShardedStateDictConfig", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.fsdp.ShardingStrategy", "path": "fsdp#torch.distributed.fsdp.ShardingStrategy", "type": "Fully Sharded Data Parallel", "text": ["This specifies the sharding strategy to be used for distributed training by FullyShardedDataParallel."]}, {"name": "torch.distributed.fsdp.StateDictConfig", "path": "fsdp#torch.distributed.fsdp.StateDictConfig", "type": "Fully Sharded Data Parallel", "text": ["StateDictConfig is the base class for all state_dict configuration classes. Users should instantiate a child class (e.g. FullStateDictConfig) in order to configure settings for the corresponding state_dict type supported by FSDP."]}, {"name": "torch.distributed.fsdp.StateDictSettings", "path": "fsdp#torch.distributed.fsdp.StateDictSettings", "type": "Fully Sharded Data Parallel", "text": []}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "Distributed Communication", "text": ["Gathers a list of tensors in a single process.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "Distributed Communication", "text": ["Gathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.", "None. On the dst rank, object_gather_list will contain the output of the collective.", "Note", "Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.", "Note", "For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().", "Warning", "gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using gather() instead."]}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "Distributed Communication", "text": ["Returns the backend of the given process group.", "group (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.", "The backend of the given process group as a lower case string.", "str"]}, {"name": "torch.distributed.get_global_rank()", "path": "distributed#torch.distributed.get_global_rank", "type": "Distributed Communication", "text": ["Translate a group rank into a global rank.", "group_rank must be part of group otherwise this raises RuntimeError.", "Global rank of group_rank relative to group", "int", "N.B. calling this function on the default process group returns identity"]}, {"name": "torch.distributed.get_group_rank()", "path": "distributed#torch.distributed.get_group_rank", "type": "Distributed Communication", "text": ["Translate a global rank into a group rank.", "global_rank must be part of group otherwise this raises RuntimeError.", "Group rank of global_rank relative to group", "int", "N.B. calling this function on the default process group returns identity"]}, {"name": "torch.distributed.get_process_group_ranks()", "path": "distributed#torch.distributed.get_process_group_ranks", "type": "Distributed Communication", "text": ["Get all ranks associated with group.", "group (ProcessGroup) \u2013 ProcessGroup to get all ranks from.", "List of global ranks ordered by group rank."]}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "Distributed Communication", "text": ["Returns the rank of the current process in the provided group or the default group if none was provided.", "Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The rank of the process group -1, if not part of the group", "int"]}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "Distributed Communication", "text": ["Returns the number of processes in the current process group", "group (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.", "The world size of the process group -1, if not part of the group", "int"]}, {"name": "torch.distributed.GradBucket", "path": "ddp_comm_hooks#torch.distributed.GradBucket", "type": "DDP Communication Hooks", "text": ["This class mainly passes a flattened gradient tensor (returned by buffer()) to DDP communication hook. This tensor can be further decomposed into a list of per-parameter tensors within this bucket (returned by get_per_parameter_tensors()) to apply layer-wise operations."]}, {"name": "torch.distributed.GradBucket.buffer()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.buffer", "type": "DDP Communication Hooks", "text": ["A flattened 1D torch.Tensor buffer, which can be further decomposed into a list of per-parameter tensors within this bucket."]}, {"name": "torch.distributed.GradBucket.gradients()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.gradients", "type": "DDP Communication Hooks", "text": ["A list of torch.Tensor. Each tensor in the list corresponds to a gradient."]}, {"name": "torch.distributed.GradBucket.index()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.index", "type": "DDP Communication Hooks", "text": ["Warning", "Since the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training.", "The index of a bucket that stores gradients of a few contiguous layers. All the gradients are bucketized."]}, {"name": "torch.distributed.GradBucket.is_last()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.is_last", "type": "DDP Communication Hooks", "text": ["Whether this bucket is the last bucket to allreduce in an iteration. This also means that this bucket corresponds to the first few layers in the forward pass."]}, {"name": "torch.distributed.GradBucket.parameters()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.parameters", "type": "DDP Communication Hooks", "text": ["A list of torch.Tensor. Each tensor in the list corresponds to a model parameter."]}, {"name": "torch.distributed.GradBucket.set_buffer()", "path": "ddp_comm_hooks#torch.distributed.GradBucket.set_buffer", "type": "DDP Communication Hooks", "text": ["Replaces the tensor in the bucket with the input tensor buffer."]}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "Distributed Communication", "text": ["A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes."]}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "Distributed Communication", "text": ["Initializes the default distributed process group, and this will also initialize the distributed package.", "If neither is specified, init_method is assumed to be \u201cenv://\u201d.", "Note", "To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI.", "Note", "Support for multiple backends is experimental. Currently when no backend is specified, both gloo and nccl backends will be created. The gloo backend will be used for collectives with CPU tensors and the nccl backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format \u201c<device_type>:<backend_name>,<device_type>:<backend_name>\u201d, e.g. \u201ccpu:gloo,cuda:custom_backend\u201d."]}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "Distributed Communication", "text": ["Receives a tensor asynchronously.", "Warning", "tag is not supported with the NCCL backend.", "A distributed request object. None, if not part of the group", "Work"]}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "Distributed Communication", "text": ["Returns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS.", "bool"]}, {"name": "torch.distributed.is_gloo_available()", "path": "distributed#torch.distributed.is_gloo_available", "type": "Distributed Communication", "text": ["Checks if the Gloo backend is available.", "bool"]}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "Distributed Communication", "text": ["Checking if the default process group has been initialized", "bool"]}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "Distributed Communication", "text": ["Checks if the MPI backend is available.", "bool"]}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "Distributed Communication", "text": ["Checks if the NCCL backend is available.", "bool"]}, {"name": "torch.distributed.is_torchelastic_launched()", "path": "distributed#torch.distributed.is_torchelastic_launched", "type": "Distributed Communication", "text": ["Checks whether this process was launched with torch.distributed.elastic (aka torchelastic). The existence of TORCHELASTIC_RUN_ID environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since TORCHELASTIC_RUN_ID maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..", "bool"]}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "Distributed Communication", "text": ["Sends a tensor asynchronously.", "Warning", "Modifying tensor before the request completes causes undefined behavior.", "Warning", "tag is not supported with the NCCL backend.", "A distributed request object. None, if not part of the group", "Work"]}, {"name": "torch.distributed.monitored_barrier()", "path": "distributed#torch.distributed.monitored_barrier", "type": "Distributed Communication", "text": ["Synchronizes all processes similar to torch.distributed.barrier, but takes a configurable timeout and is able to report ranks that did not pass this barrier within that timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.", "This collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application\u2019s collective calls to check if any ranks are desynchronized.", "Note", "Note that this collective is only supported with the GLOO backend.", "None."]}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "Distributed Communication", "text": ["Creates a new distributed group.", "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.", "Warning", "Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.", "A handle of distributed group that can be given to collective calls or None if the rank is not part of ranks.", "N.B. use_local_synchronization doesn\u2019t work with MPI.", "N.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks don\u2019t join the group barrier().", "N.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order."]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule", "type": "Distributed RPC", "text": ["A RemoteModule instance can only be created after RPC initialization. It creates a user-specified module on a specified remote node. It behaves like a regular nn.Module except that the forward method is executed on the remote node. It takes care of autograd recording to ensure the backward pass propagates gradients back to the corresponding remote module.", "It generates two methods forward_async and forward based on the signature of the forward method of module_cls. forward_async runs asynchronously and returns a Future. The arguments of forward_async and forward are the same as the forward method of the module returned by the module_cls.", "For example, if module_cls returns an instance of nn.Linear, that has forward method signature: def forward(input: Tensor) -> Tensor:, the generated RemoteModule will have 2 methods with the signatures:", "module_cls (nn.Module) \u2013 ", "Class for the module to be created remotely. For example,", "A remote module instance which wraps the Module created by the user-provided module_cls, it has a blocking forward method and an asynchronous forward_async method that returns a future of the forward call on the user-provided module on the remote side.", "Run the following code in two different processes:", "Furthermore, a more practical example that is combined with DistributedDataParallel (DDP) can be found in this tutorial.", "Returns an RRef (RRef[nn.Module]) pointing to the remote module.", "RRef[Module]", "Returns a list of RRef pointing to the remote module\u2019s parameters. This can typically be used in conjunction with DistributedOptimizer.", "recurse (bool) \u2013 if True, then returns parameters of the remote module and all submodules of the remote module. Otherwise, returns only parameters that are direct members of the remote module.", "A list of RRef (List[RRef[nn.Parameter]]) to remote module\u2019s parameters.", "List[RRef[Parameter]]"]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref()", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule.get_module_rref", "type": "Distributed RPC", "text": ["Returns an RRef (RRef[nn.Module]) pointing to the remote module.", "RRef[Module]"]}, {"name": "torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters()", "path": "rpc#torch.distributed.nn.api.remote_module.RemoteModule.remote_parameters", "type": "Distributed RPC", "text": ["Returns a list of RRef pointing to the remote module\u2019s parameters. This can typically be used in conjunction with DistributedOptimizer.", "recurse (bool) \u2013 if True, then returns parameters of the remote module and all submodules of the remote module. Otherwise, returns only parameters that are direct members of the remote module.", "A list of RRef (List[RRef[nn.Parameter]]) to remote module\u2019s parameters.", "List[RRef[Parameter]]"]}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "distributed.optim#torch.distributed.optim.DistributedOptimizer", "type": "Distributed Optimizers", "text": ["DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.", "This class uses get_gradients() in order to retrieve the gradients for specific parameters.", "Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.", "DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow the recipe in PyTorch tutorials to enable TorchScript support for your own custom optimizers.", "Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimization step.", "This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.", "context_id \u2013 the autograd context id for which we should run the optimizer step."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer", "type": "Distributed Optimizers", "text": ["Wraps an arbitrary torch.optim.Optimizer and runs post-local SGD, This optimizer runs local optimizer at every step. After the warm-up stage, it averages parameters periodically afer the local optimizer is applied.", "Example:", "This is the same as torch.optim.Optimizer load_state_dict(), but also restores model averager\u2019s step value to the one saved in the provided state_dict.", "If there is no \"step\" entry in state_dict, it will raise a warning and initialize the model averager\u2019s step to 0.", "This is the same as torch.optim.Optimizer state_dict(), but adds an extra entry to record model averager\u2019s step to the checkpoint to ensure reload does not cause unnecessary warm up again.", "Performs a single optimization step (parameter update)."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.load_state_dict", "type": "Distributed Optimizers", "text": ["This is the same as torch.optim.Optimizer load_state_dict(), but also restores model averager\u2019s step value to the one saved in the provided state_dict.", "If there is no \"step\" entry in state_dict, it will raise a warning and initialize the model averager\u2019s step to 0."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.state_dict()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.state_dict", "type": "Distributed Optimizers", "text": ["This is the same as torch.optim.Optimizer state_dict(), but adds an extra entry to record model averager\u2019s step to the checkpoint to ensure reload does not cause unnecessary warm up again."]}, {"name": "torch.distributed.optim.PostLocalSGDOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.PostLocalSGDOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimization step (parameter update)."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer", "type": "Distributed Optimizers", "text": ["This class wraps an arbitrary optim.Optimizer and shards its states across ranks in the group as described by ZeRO. The local optimizer instance in each rank is only responsible for updating approximately 1 / world_size parameters and hence only needs to keep 1 / world_size optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. ZeroRedundancyOptimizer can be used in conjunction with torch.nn.parallel.DistributedDataParallel to reduce per-rank peak memory consumption.", "ZeroRedundancyOptimizer uses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.", "params (Iterable) \u2013 an Iterable of torch.Tensor s or dict s giving all parameters, which will be sharded across ranks.", "Example:", "Warning", "Currently, ZeroRedundancyOptimizer requires that all of the passed-in parameters are the same dense type.", "Warning", "If you pass overlap_with_ddp=True, be wary of the following: Given the way that overlapping DistributedDataParallel with ZeroRedundancyOptimizer is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if static_graph=False or static_graph=True, respectively. This is because it needs information about the gradient bucketing strategy used by DistributedDataParallel, which is not finalized until the second forward pass if static_graph=False or until the third forward pass if static_graph=True. To adjust for this, one option is to prepend dummy inputs.", "Warning", "ZeroRedundancyOptimizer is experimental and subject to change.", "Add a parameter group to the Optimizer \u2018s param_groups.", "This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 specifies the parameters to be optimized and group-specific optimization options.", "Warning", "This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.", "Consolidate a list of state_dict s (one per rank) on the target rank.", "to (int) \u2013 the rank that receives the optimizer states (default: 0).", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Warning", "This needs to be called on all ranks.", "Returns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.", "Gradients must be properly set before this hook is called.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "This hook does not support any keyword arguments; i.e. kwargs is unused.", "Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.", "state_dict (dict) \u2013 optimizer state; should be an object returned from a call to state_dict().", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Returns the last global optimizer state known to this rank.", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt; or if this method is called without a preceding call to consolidate_state_dict().", "Dict[str, Any]", "Performs a single optimizer step and syncs parameters across all ranks.", "closure (Callable) \u2013 a closure that re-evaluates the model and returns the loss; optional for most optimizers.", "Optional loss depending on the underlying local optimizer.", "Optional[float]"]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.add_param_group", "type": "Distributed Optimizers", "text": ["Add a parameter group to the Optimizer \u2018s param_groups.", "This can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the Optimizer as training progresses.", "param_group (dict) \u2013 specifies the parameters to be optimized and group-specific optimization options.", "Warning", "This method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.consolidate_state_dict", "type": "Distributed Optimizers", "text": ["Consolidate a list of state_dict s (one per rank) on the target rank.", "to (int) \u2013 the rank that receives the optimizer states (default: 0).", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt.", "Warning", "This needs to be called on all ranks."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.join_hook()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.join_hook", "type": "Distributed Optimizers", "text": ["Returns the ZeRO join hook, which enables training on uneven inputs by shadowing the collective communications in the optimizer step.", "Gradients must be properly set before this hook is called.", "kwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.", "This hook does not support any keyword arguments; i.e. kwargs is unused."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.load_state_dict", "type": "Distributed Optimizers", "text": ["Load the state pertaining to the given rank from the input state_dict, updating the local optimizer as needed.", "state_dict (dict) \u2013 optimizer state; should be an object returned from a call to state_dict().", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt."]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.state_dict()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.state_dict", "type": "Distributed Optimizers", "text": ["Returns the last global optimizer state known to this rank.", "RuntimeError \u2013 if overlap_with_ddp=True and this method is called before this ZeroRedundancyOptimizer instance has been fully initialized, which happens once DistributedDataParallel gradient buckets have been rebuilt; or if this method is called without a preceding call to consolidate_state_dict().", "Dict[str, Any]"]}, {"name": "torch.distributed.optim.ZeroRedundancyOptimizer.step()", "path": "distributed.optim#torch.distributed.optim.ZeroRedundancyOptimizer.step", "type": "Distributed Optimizers", "text": ["Performs a single optimizer step and syncs parameters across all ranks.", "closure (Callable) \u2013 a closure that re-evaluates the model and returns the loss; optional for most optimizers.", "Optional loss depending on the underlying local optimizer.", "Optional[float]"]}, {"name": "torch.distributed.P2POp", "path": "distributed#torch.distributed.P2POp", "type": "Distributed Communication", "text": ["A class to build point-to-point operations for batch_isend_irecv.", "This class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to batch_isend_irecv for point-to-point communications."]}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Miscellaneous", "text": ["Wraps an arbitrary nn.Sequential module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on a single GPU, pipeline parallelism is a useful technique to employ for training.", "The implementation is based on the torchgpipe paper.", "Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization.", "You should place all the modules on the appropriate devices and wrap them into an nn.Sequential module defining the desired order of execution. If a module does not contain any parameters/buffers, it is assumed this module should be executed on CPU and appropriate input tensors to the module are moved to CPU before execution. This behavior can be overridden by the WithDevice wrapper which can be used to explicitly specify which device a module should run on.", "Pipeline of two FC layers across GPUs 0 and 1.", "Note", "You can wrap a Pipe model with torch.nn.parallel.DistributedDataParallel only when the checkpoint parameter of Pipe is 'never'.", "Note", "Pipe only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an RRef to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelining you can use local_value() to retrieve the output locally.", "Warning", "Pipe is experimental and subject to change.", "Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to contain at least one tensor. This restriction is applied at partition boundaries too.", "The sequence of inputs are fed into the first stage of the pipeline as *inputs. As a result the positional args for this function should match the positional args for the first stage of the pipeline. The same condition applies for output of one stage of the pipeline which is the input for the next stage.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "Only tensors are split into multiple micro-batches, non-Tensor inputs are just replicated as-is in each micro-batch. For non-Tensor outputs in the last stage of the pipeline, they are aggregated as a List and returned the user. For example, if you have 2 micro-batches returning the integer 5, the user would receive the consolidated output of [5, 5]", "All the input tensors need to be on the same device as the first partition of the pipeline.", "If a tensor is wrapped with the NoChunk wrapper, the tensor is not split across micro-batches and is replicated as-is similar to non-tensors.", "inputs \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input doesn\u2019t contain at least one tensor", "RRef"]}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Miscellaneous", "text": ["Processes a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to contain at least one tensor. This restriction is applied at partition boundaries too.", "The sequence of inputs are fed into the first stage of the pipeline as *inputs. As a result the positional args for this function should match the positional args for the first stage of the pipeline. The same condition applies for output of one stage of the pipeline which is the input for the next stage.", "The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.", "Only tensors are split into multiple micro-batches, non-Tensor inputs are just replicated as-is in each micro-batch. For non-Tensor outputs in the last stage of the pipeline, they are aggregated as a List and returned the user. For example, if you have 2 micro-batches returning the integer 5, the user would receive the consolidated output of [5, 5]", "All the input tensors need to be on the same device as the first partition of the pipeline.", "If a tensor is wrapped with the NoChunk wrapper, the tensor is not split across micro-batches and is replicated as-is similar to non-tensors.", "inputs \u2013 input mini-batch", "RRef to the output of the mini-batch", "TypeError \u2013 input doesn\u2019t contain at least one tensor", "RRef"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Miscellaneous", "text": ["The command to pop a skip tensor.", "name (str) \u2013 name of skip tensor", "the skip tensor previously stashed by another layer under the same name", "None"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Miscellaneous", "text": ["The decorator to define a nn.Module with skip connections. Decorated modules are called \u201cskippable\u201d. This functionality works perfectly fine even when the module is not wrapped by Pipe.", "Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by stash and/or pop parameters. Skip tensors with pre-declared name can be stashed by yield stash(name, tensor) or popped by tensor = yield\npop(name).", "Here is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed and popped at the first and last layer, respectively:", "One skippable module can stash or pop multiple skip tensors:", "Every skip tensor must be associated with exactly one pair of stash and pop. Pipe checks this restriction automatically when wrapping a module. You can also check the restriction by verify_skippables() without Pipe.", "Callable[[Type[Module]], Type[Skippable]]"]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Miscellaneous", "text": ["The command to stash a skip tensor."]}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Miscellaneous", "text": ["Verifies if the underlying skippable modules satisfy integrity.", "Every skip tensor must have only one pair of stash and pop. If there are one or more unmatched pairs, it will raise TypeError with the detailed messages.", "Here are a few failure cases. verify_skippables() will report failure for these cases:", "To use the same name for multiple skip tensors, they must be isolated by different namespaces. See isolate().", "TypeError \u2013 one or more pairs of stash and pop are not matched."]}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "Distributed Communication", "text": ["A wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store."]}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "Distributed Communication", "text": ["Receives a tensor synchronously.", "Sender rank -1, if not part of the group", "int"]}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "Distributed Communication", "text": ["Reduces the tensor data across all machines.", "Only the process with rank dst is going to receive the final result.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group"]}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "Distributed Communication", "text": ["Reduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU", "Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result.", "Only nccl backend is currently supported tensors should only be GPU tensors", "Async work handle, if async_op is set to True. None, otherwise"]}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "Distributed Communication", "text": ["Deprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX.", "ReduceOp is recommended to use instead."]}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "Distributed Communication", "text": ["Reduces, then scatters a list of tensors to all processes in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "Distributed Communication", "text": ["Reduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported.", "Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.", "output_tensor_list (List[Tensor]) \u2013 ", "Output tensors (on different GPUs) to receive the result of the operation.", "Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.", "input_tensor_lists (List[List[Tensor]]) \u2013 ", "Input lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i].", "Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j]", "Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group."]}, {"name": "torch.distributed.reduce_scatter_tensor()", "path": "distributed#torch.distributed.reduce_scatter_tensor", "type": "Distributed Communication", "text": ["Reduces, then scatters a tensor to all ranks in a group.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group.", "Warning", "The Gloo backend does not support this API."]}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "Distributed Communication", "text": ["An enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, BXOR, and PREMUL_SUM.", "BAND, BOR, and BXOR reductions are not available when using the NCCL backend.", "AVG divides values by the world size before summing across ranks. AVG is only available with the NCCL backend, and only for NCCL versions 2.10 or later.", "PREMUL_SUM multiplies inputs by a given scalar locally before reduction. PREMUL_SUM is only available with the NCCL backend, and only available for NCCL versions 2.11 or later. Users are supposed to use torch.distributed._make_nccl_premul_sum.", "Additionally, MAX, MIN and PRODUCT are not supported for complex tensors.", "The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc.", "This class does not support __members__ property."]}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC", "text": ["An enum class of available backends.", "PyTorch ships with a builtin BackendType.TENSORPIPE backend. Additional ones can be registered using the register_backend() function."]}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC", "text": ["A decorator for a function indicating that the return value of the function is guaranteed to be a Future object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the Future returned by the wrapped function and installs subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. That also means the returned Future only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\u2019s (fn) execution needs to pause and resume due to, e.g., containing rpc_async() or waiting for other signals.", "Note", "To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a Future object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with @staticmethod or @classmethod, @rpc.functions.async_execution needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by @rpc.functions.async_execution.", "The returned Future object can come from rpc_async(), then(), or Future constructor. The example below shows directly using the Future returned by then().", "When combined with TorchScript decorators, this decorator must be the outmost one.", "When combined with static or class method, this decorator must be the inner one.", "This decorator also works with RRef helpers, i.e., . torch.distributed.rpc.RRef.rpc_sync(), torch.distributed.rpc.RRef.rpc_async(), and torch.distributed.rpc.RRef.remote()."]}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC", "text": ["Get WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.", "worker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)", "WorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None."]}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC", "text": ["Initializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs."]}, {"name": "torch.distributed.rpc.PyRRef", "path": "rpc#torch.distributed.rpc.PyRRef", "type": "Distributed RPC", "text": ["A class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker. A UserRRef will be deleted when 1) no references to it in both the application code and in the local RRef context, or 2) the application has called a graceful shutdown. Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation only offers best-effort error detection, and applications should not use UserRRefs after rpc.shutdown().", "Warning", "RRefs can only be serialized and deserialized by the RPC module. Serializing and deserializing RRefs without RPC (e.g., Python pickle, torch save() / load(), JIT save() / load(), etc.) will lead to errors.", "Following examples skip RPC initialization and shutdown code for simplicity. Refer to RPC docs for those details.", "Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.", "Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef.", "Returns whether or not the current node is the owner of this RRef.", "If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.", "Returns worker information of the node that owns this RRef.", "Returns worker name of the node that owns this RRef.", "Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.", "Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.", "Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.PyRRef.backward()", "path": "rpc#torch.distributed.rpc.PyRRef.backward", "type": "Distributed RPC", "text": ["Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor."]}, {"name": "torch.distributed.rpc.PyRRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.PyRRef.confirmed_by_owner", "type": "Distributed RPC", "text": ["Returns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef."]}, {"name": "torch.distributed.rpc.PyRRef.is_owner()", "path": "rpc#torch.distributed.rpc.PyRRef.is_owner", "type": "Distributed RPC", "text": ["Returns whether or not the current node is the owner of this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.local_value()", "path": "rpc#torch.distributed.rpc.PyRRef.local_value", "type": "Distributed RPC", "text": ["If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception."]}, {"name": "torch.distributed.rpc.PyRRef.owner()", "path": "rpc#torch.distributed.rpc.PyRRef.owner", "type": "Distributed RPC", "text": ["Returns worker information of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.owner_name()", "path": "rpc#torch.distributed.rpc.PyRRef.owner_name", "type": "Distributed RPC", "text": ["Returns worker name of the node that owns this RRef."]}, {"name": "torch.distributed.rpc.PyRRef.remote()", "path": "rpc#torch.distributed.rpc.PyRRef.remote", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef."]}, {"name": "torch.distributed.rpc.PyRRef.rpc_async()", "path": "rpc#torch.distributed.rpc.PyRRef.rpc_async", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.PyRRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.PyRRef.rpc_sync", "type": "Distributed RPC", "text": ["Create a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following:", "timeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used."]}, {"name": "torch.distributed.rpc.PyRRef.to_here()", "path": "rpc#torch.distributed.rpc.PyRRef.to_here", "type": "Distributed RPC", "text": ["Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.", "timeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used."]}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC", "text": ["Make a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.", "A user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.", "Warning", "The remote API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the torch.distributed.rpc.RRef.confirmed_by_owner() API.", "Warning", "Errors such as timeouts for the remote API are handled on a best-effort basis. This means that when remote calls initiated by remote fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as to_here or fork call), then future uses of the RRef will appropriately raise errors. However, it is possible that the user application will use the RRef before the errors are handled. In this case, errors may not be raised as they have not yet been handled.", "Example:"]}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC", "text": ["Make a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a Future that can be awaited on.", "Returns a Future object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the Future object.", "Warning", "Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.", "Warning", "The rpc_async API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned Future completes.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC", "text": ["Make a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.", "Returns the result of running func with args and kwargs.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:", "Below is an example of running a TorchScript function using RPC."]}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC", "text": ["An abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to init_rpc() in order to initialize RPC with specific configurations, such as the RPC timeout and init_method to be used.", "URL specifying how to initialize the process group. Default is env://", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC", "text": ["Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, this is a local shutdown, and it does not wait for other RPC processes to reach this method.", "Warning", "For Future objects returned by rpc_async(), future.wait() should not be called after shutdown().", "graceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for UserRRefs and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.", "Make sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example,", "export MASTER_ADDR=localhost export MASTER_PORT=5678", "Then run the following code in two different processes:"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC", "text": ["The backend options for TensorPipeAgent, derived from RpcBackendOptions.", "The device map locations.", "All devices used by the local agent.", "URL specifying how to initialize the process group. Default is env://", "The number of threads in the thread-pool used by TensorPipeAgent to execute requests.", "A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.", "Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.", "Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this List.", "devices (List of int, str, or torch.device) \u2013 local devices used by the TensorPipe RPC agent."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC", "text": ["The device map locations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.devices", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.devices", "type": "Distributed RPC", "text": ["All devices used by the local agent."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC", "text": ["URL specifying how to initialize the process group. Default is env://"]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC", "text": ["The number of threads in the thread-pool used by TensorPipeAgent to execute requests."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC", "text": ["A float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC", "text": ["Set device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations."]}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_devices", "type": "Distributed RPC", "text": ["Set local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this List.", "devices (List of int, str, or torch.device) \u2013 local devices used by the TensorPipe RPC agent."]}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC", "text": ["A structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through get_worker_info() and the result can be passed in to functions such as rpc_sync(), rpc_async(), remote() to avoid copying a string on every invocation.", "Globally unique id to identify the worker.", "The name of the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.id", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC", "text": ["Globally unique id to identify the worker."]}, {"name": "torch.distributed.rpc.WorkerInfo.name", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC", "text": ["The name of the worker."]}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "Distributed Communication", "text": ["Scatters a list of tensors to all processes in a group.", "Each process will receive exactly one tensor and store its data in the tensor argument.", "Complex tensors are supported.", "Async work handle, if async_op is set to True. None, if not async_op or if not part of the group", "Note", "Note that all Tensors in scatter_list must have the same size."]}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "Distributed Communication", "text": ["Scatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.", "None. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.", "Note", "Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.", "Warning", "scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.", "Warning", "Calling scatter_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using scatter() instead."]}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "Distributed Communication", "text": ["Sends a tensor synchronously."]}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "Distributed Communication", "text": ["Base class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore)."]}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "Distributed Communication", "text": ["The first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception."]}, {"name": "torch.distributed.Store.compare_set()", "path": "distributed#torch.distributed.Store.compare_set", "type": "Distributed Communication", "text": ["Inserts the key-value pair into the store based on the supplied key and performs comparison between expected_value and desired_value before inserting. desired_value will only be set if expected_value for the key already exists in the store or if expected_value is an empty string."]}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "Distributed Communication", "text": ["Deletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.", "Warning", "The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.", "key (str) \u2013 The key to be deleted from the store", "True if key was deleted, otherwise False."]}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "Distributed Communication", "text": ["Retrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.", "key (str) \u2013 The function will return the value associated with this key.", "Value associated with key if key is in the store."]}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "Distributed Communication", "text": ["Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.", "Warning", "When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.", "The number of keys present in the store."]}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "Distributed Communication", "text": ["Inserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value."]}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "Distributed Communication", "text": ["Sets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().", "timeout (timedelta) \u2013 timeout to be set in the store."]}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "Distributed Communication", "text": ["Overloaded function.", "Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.", "keys (list) \u2013 List of keys on which to wait until they are set in the store.", "Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout."]}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "Distributed Communication", "text": ["A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection."]}, {"name": "torch.distributed.tensor.parallel.ddp.pre_dp_module_transform()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.ddp.pre_dp_module_transform", "type": "Tensor Parallelism", "text": ["Enable the composability between Tensor Parallelism (TP) and Data Parallelism(DP) in PyTorch when using DDP. We need to convert Parameters which are DTensors to local tensors before wrapping with data parallelism API. We then register two hooks, one for converting local tensors back to DTensor preforward and one to convert DTensors back to tensors after Forward. By integrating this way, we avoid any special handling of DTensor parameters by DDP and get DTensor\u2019s gradients propagated back to DP, e.g. gradient buckets of DDP.", "For now, this API only works with DistributedDataParallel. It will later support other DP methods such as FSDP.", "module (nn.Module) \u2013 Module which has been applied TP on."]}, {"name": "torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.fsdp.enable_2d_with_fsdp", "type": "Tensor Parallelism", "text": ["The API registers the extension which is needed for Tensor Parallelism (TP) to work with FullyShardedDataParallel (FSDP). We first parallelize parameters within one module or sub_modules based on a parallelize_plan and will let FSDP reshard the local tensor of distributed parameter which is essentially a DTensor.", "A bool indicated whether extension registration succeeds or not.", "bool"]}, {"name": "torch.distributed.tensor.parallel.parallelize_module()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.parallelize_module", "type": "Tensor Parallelism", "text": ["The API to apply Tensor Parallelism (TP) in PyTorch. We parallelize module or sub_modules based on a parallelize_plan. The parallelize_plan contains ParallelStyle, which indicates how user wants the module or sub_module to be parallelized.", "User can also specify different parallel style per module fully qualified name (FQN). The API supports 2D parallelism natively by accepting an n-dimension device_mesh and users just need to specify the dimension where we perform tensor parallelism on.", "A nn.Module object parallelized.", "Warning", "PairwiseParallel comes with constraints for now. If you need finer granularity, you need to pass in a dict of module FQN and parallel style instead."]}, {"name": "torch.distributed.tensor.parallel.style.ColwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.ColwiseParallel", "type": "Tensor Parallelism", "text": ["Partitioning the column of a tensor or module. We assume the input to be a replicated DTensor and output to be a sharded torch.Tensor."]}, {"name": "torch.distributed.tensor.parallel.style.make_input_replicate_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_replicate_1d", "type": "Tensor Parallelism", "text": ["Replicate input tensor over an 1-D device mesh. This function will be used in ParallelStyle.", "A DTensor replicated over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_reshard_replicate()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_reshard_replicate", "type": "Tensor Parallelism", "text": ["To construct a Sharded DTensor from a tensor on different ranks and then convert to a replicate DTensor.", "and then converted to replicate.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_shard_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d", "type": "Tensor Parallelism", "text": ["Shard input tensor on dim over an 1-D device mesh. This function will be used in ParallelStyle.", "A DTensor sharded on dimension dim over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_input_shard_1d_last_dim", "type": "Tensor Parallelism", "text": ["Wrapper func of make_input_shard_1d with dim = -1.", "A DTensor sharded on the last dimension over device_mesh.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_replicate_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_replicate_1d", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a replicated DTensor. This will be used in ParallelStyle.", "A DTensor object made replicate.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_reshard_tensor()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_reshard_tensor", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a sharded DTensor and return the local tensor.", "A torch.Tensor object converted from output DTensor.", "Tensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_shard_1d()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_shard_1d", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a sharded DTensor. This will be used in ParallelStyle.", "A DTensor object sharded on the given dim.", "DTensor"]}, {"name": "torch.distributed.tensor.parallel.style.make_output_tensor()", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.make_output_tensor", "type": "Tensor Parallelism", "text": ["Convert Output DTensor to a replicated DTensor first and then convert it to Tensor.", "A torch.Tensor object converted from output DTensor.", "Tensor"]}, {"name": "torch.distributed.tensor.parallel.style.PairwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.PairwiseParallel", "type": "Tensor Parallelism", "text": ["PairwiseParallel concatenate colwise and rowwise styles as a fixed pair like what Megatron-LM(https://arxiv.org/abs/1909.08053) is doing. We assume both input and output need to be replicate DTensors.", "Warning", "PairwiseParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use PairwiseParallel only for even-number-layer MLP for now."]}, {"name": "torch.distributed.tensor.parallel.style.RowwiseParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.RowwiseParallel", "type": "Tensor Parallelism", "text": ["Partitioning the row of a module. We assume the input to be a sharded DTensor and output to be a torch.Tensor."]}, {"name": "torch.distributed.tensor.parallel.style.SequenceParallel", "path": "distributed.tensor.parallel#torch.distributed.tensor.parallel.style.SequenceParallel", "type": "Tensor Parallelism", "text": ["SequenceParallel concatenate colwise and rowwise styles as a fixed pair together with sequence parallel like what Megatron-LM Sequence parallel (https://arxiv.org/pdf/2205.05198.pdf) is doing. We assume both input and output need to be sharded DTensors.", "Warning", "SequenceParallel does not support nn.MultiheadAttention, nn.Transformer well at this moment. One workaround is to apply ColwiseParallel and RowwiseParallel to the components of transformer. We recommend to use SequenceParallel only for even-number-layer MLP for now."]}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Bernoulli distribution parameterized by probs or logits (but not both).", "Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p.", "Example:"]}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.mean", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.mode", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.bernoulli.Bernoulli.variance", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Beta distribution parameterized by concentration1 and concentration0.", "Example:"]}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration0", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.concentration1", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.mean", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.mode", "path": "distributions#torch.distributions.beta.Beta.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.beta.Beta.variance", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits.", "Example:"]}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.entropy()", "path": "distributions#torch.distributions.binomial.Binomial.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.mean", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.mode", "path": "distributions#torch.distributions.binomial.Binomial.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.param_shape", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.support", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.binomial.Binomial.variance", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a categorical distribution parameterized by either probs or logits (but not both).", "Note", "It is equivalent to the distribution that torch.multinomial() samples from.", "Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\} where K is probs.size(-1).", "If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index.", "If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "See also: torch.multinomial()", "Example:"]}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.mean", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.mode", "path": "distributions#torch.distributions.categorical.Categorical.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.param_shape", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.support", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.categorical.Categorical.variance", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "Probability Distributions", "text": ["Bases: Distribution", "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution.", "Example:"]}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.mean", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.mode", "path": "distributions#torch.distributions.cauchy.Cauchy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.cauchy.Cauchy.variance", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "Probability Distributions", "text": ["Bases: Gamma", "Creates a Chi-squared distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5)", "Example:", "df (float or Tensor) \u2013 shape parameter of the distribution"]}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.df", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "Probability Distributions", "text": ["Registry to link constraints to transforms.", "Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "Probability Distributions", "text": ["Registers a Constraint subclass in this registry. Usage:"]}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "Probability Distributions", "text": ["alias of _Cat"]}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "Probability Distributions", "text": ["Abstract base class for constraints.", "A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.", "Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "Probability Distributions", "text": ["Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint."]}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "Probability Distributions", "text": ["alias of _DependentProperty"]}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "Probability Distributions", "text": ["alias of _GreaterThan"]}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "Probability Distributions", "text": ["alias of _GreaterThanEq"]}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "Probability Distributions", "text": ["alias of _HalfOpenInterval"]}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "Probability Distributions", "text": ["alias of _IndependentConstraint"]}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "Probability Distributions", "text": ["alias of _IntegerInterval"]}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "Probability Distributions", "text": ["alias of _Interval"]}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "Probability Distributions", "text": ["alias of _LessThan"]}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "Probability Distributions", "text": ["alias of _Multinomial"]}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "Probability Distributions", "text": ["alias of _Stack"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both).", "The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.", "Example:", "[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845"]}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Dirichlet distribution parameterized by concentration concentration.", "Example:", "concentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)"]}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.mean", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.mode", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.dirichlet.Dirichlet.variance", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "Probability Distributions", "text": ["Bases: object", "Distribution is the abstract base class for probability distributions.", "Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.", "Returns the shape over which parameters are batched.", "Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor", "Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Tensor", "Returns the shape of a single sample (without batching).", "Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size.", "Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor", "Returns the mean of the distribution.", "Returns the mode of the distribution.", "Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Tensor", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Tensor", "Generates n samples or n batches of samples if the distribution parameters are batched.", "Tensor", "Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation.", "Returns the standard deviation of the distribution.", "Returns a Constraint object representing this distribution\u2019s support.", "Returns the variance of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.arg_constraints", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "Probability Distributions", "text": ["Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict."]}, {"name": "torch.distributions.distribution.Distribution.batch_shape", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "Probability Distributions", "text": ["Returns the shape over which parameters are batched."]}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "Probability Distributions", "text": ["Returns the cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "Probability Distributions", "text": ["Returns entropy of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "Probability Distributions", "text": ["Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions).", "Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ...", "To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).", "expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.event_shape", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "Probability Distributions", "text": ["Returns the shape of a single sample (without batching)."]}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "Probability Distributions", "text": ["Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.", "New distribution instance with batch dimensions expanded to batch_size."]}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "Probability Distributions", "text": ["Returns the inverse cumulative density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "Probability Distributions", "text": ["Returns the log of the probability density/mass function evaluated at value.", "value (Tensor) \u2013 ", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.mean", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "Probability Distributions", "text": ["Returns the mean of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.mode", "path": "distributions#torch.distributions.distribution.Distribution.mode", "type": "Probability Distributions", "text": ["Returns the mode of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "Probability Distributions", "text": ["Returns perplexity of distribution, batched over batch_shape.", "Tensor of shape batch_shape.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "Probability Distributions", "text": ["Generates n samples or n batches of samples if the distribution parameters are batched.", "Tensor"]}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "Probability Distributions", "text": ["Sets whether validation is enabled or disabled.", "The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.", "value (bool) \u2013 Whether to enable validation."]}, {"name": "torch.distributions.distribution.Distribution.stddev", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "Probability Distributions", "text": ["Returns the standard deviation of the distribution."]}, {"name": "torch.distributions.distribution.Distribution.support", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "Probability Distributions", "text": ["Returns a Constraint object representing this distribution\u2019s support."]}, {"name": "torch.distributions.distribution.Distribution.variance", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "Probability Distributions", "text": ["Returns the variance of the distribution."]}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "Probability Distributions", "text": ["Bases: Distribution", "ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below", "where \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient statistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and k(x)k(x) is the carrier measure.", "Note", "This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).", "Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "Probability Distributions", "text": ["Method to compute the entropy using Bregman divergence of the log normalizer."]}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Exponential distribution parameterized by rate.", "Example:", "rate (float or Tensor) \u2013 rate = 1 / scale of the distribution"]}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.mean", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.mode", "path": "distributions#torch.distributions.exponential.Exponential.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.stddev", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.exponential.Exponential.variance", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Fisher-Snedecor distribution parameterized by df1 and df2.", "Example:"]}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mode", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Gamma distribution parameterized by shape concentration and rate.", "Example:"]}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.cdf()", "path": "distributions#torch.distributions.gamma.Gamma.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.mean", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.mode", "path": "distributions#torch.distributions.gamma.Gamma.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gamma.Gamma.variance", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a success.", "Samples are non-negative integers [0, inf\u2061\\inf).", "Example:"]}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.mean", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.mode", "path": "distributions#torch.distributions.geometric.Geometric.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.geometric.Geometric.variance", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Gumbel Distribution.", "Examples:"]}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.mean", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.mode", "path": "distributions#torch.distributions.gumbel.Gumbel.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.stddev", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.gumbel.Gumbel.variance", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a half-Cauchy distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Cauchy distribution"]}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mode", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a half-normal distribution parameterized by scale where:", "Example:", "scale (float or Tensor) \u2013 scale of the full Normal distribution"]}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.mean", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.mode", "path": "distributions#torch.distributions.half_normal.HalfNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.scale", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.half_normal.HalfNormal.variance", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "Probability Distributions", "text": ["Bases: Distribution", "Reinterprets some of the batch dims of a distribution as event dims.", "This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:"]}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_enumerate_support", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.has_rsample", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.mean", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.mode", "path": "distributions#torch.distributions.independent.Independent.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.support", "path": "distributions#torch.distributions.independent.Independent.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.independent.Independent.variance", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "Probability Distributions", "text": ["Compute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two distributions.", "A batch of KL divergences of shape batch_shape.", "Tensor", "NotImplementedError \u2013 If the distribution types have not been registered via register_kl()."]}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "Probability Distributions", "text": ["Decorator to register a pairwise function with kl_divergence(). Usage:", "Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation:", "you should register a third most-specific implementation, e.g.:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Kumaraswamy distribution.", "Example:"]}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mode", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Laplace distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.mean", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.mode", "path": "distributions#torch.distributions.laplace.Laplace.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.stddev", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.laplace.Laplace.variance", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "Probability Distributions", "text": ["Bases: Distribution", "LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta to make the probability of the correlation matrix MM generated from a Cholesky factor proportional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}. Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices:", "Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3.", "Example:", "References", "[1] Generating random correlation matrices based on vines and extended onion method (2009), Daniel Lewandowski, Dorota Kurowicka, Harry Joe. Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008"]}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a log-normal distribution parameterized by loc and scale where:", "Example:"]}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.loc", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.mean", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.mode", "path": "distributions#torch.distributions.log_normal.LogNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.scale", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.log_normal.LogNormal.variance", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag:", "Note", "The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:"]}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "Probability Distributions", "text": ["Bases: Distribution", "The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component.", "Examples:"]}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches.", "Note that total_count need not be specified if only log_prob() is called (see example below)", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "Example:"]}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.entropy()", "path": "distributions#torch.distributions.multinomial.Multinomial.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.logits", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.mean", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.param_shape", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.probs", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.support", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multinomial.Multinomial.variance", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.", "The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix L\\mathbf{L} with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top. This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified.", "Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition."]}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mode", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of success of each Bernoulli trial is probs."]}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mode", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a normal (also called Gaussian) distribution parameterized by loc and scale.", "Example:"]}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.mean", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.mode", "path": "distributions#torch.distributions.normal.Normal.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.stddev", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.normal.Normal.variance", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a one-hot categorical distribution parameterized by probs or logits.", "Samples are one-hot coded vectors of size probs.size(-1).", "Note", "The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. logits will return this normalized value.", "See also: torch.distributions.Categorical() for specifications of probs and logits.", "Example:"]}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mode", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a Pareto Type 1 distribution.", "Example:"]}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.mean", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.mode", "path": "distributions#torch.distributions.pareto.Pareto.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.support", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.pareto.Pareto.variance", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Poisson distribution parameterized by rate, the rate parameter.", "Samples are nonnegative integers, with a pmf given by", "Example:", "rate (Number, Tensor) \u2013 the rate parameter"]}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.mean", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.mode", "path": "distributions#torch.distributions.poisson.Poisson.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.poisson.Poisson.variance", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution.", "Samples are logits of values in (0, 1). See [1] for more details.", "[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)", "[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)"]}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples.", "Example:"]}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable.", "Example:"]}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "Probability Distributions", "text": ["Bases: Distribution", "Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale.", "Example:"]}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.mean", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.mode", "path": "distributions#torch.distributions.studentT.StudentT.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.studentT.StudentT.variance", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "Probability Distributions", "text": ["Bases: Distribution", "Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:", "Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.", "An example for the usage of TransformedDistribution would be:", "For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical", "Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.", "Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.", "Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.", "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.", "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "Probability Distributions", "text": ["Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "Probability Distributions", "text": ["Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "Probability Distributions", "text": ["Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "Probability Distributions", "text": ["Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list."]}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=\u2223x\u2223y = |x|."]}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "Probability Distributions", "text": ["Transform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x."]}, {"name": "torch.distributions.transforms.CatTransform", "path": "distributions#torch.distributions.transforms.CatTransform", "type": "Probability Distributions", "text": ["Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim, of length lengths[dim], in a way compatible with torch.cat().", "Example:"]}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "Probability Distributions", "text": ["Composes multiple transforms in a chain. The transforms being composed are responsible for caching."]}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "Probability Distributions", "text": ["Transforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:"]}, {"name": "torch.distributions.transforms.CumulativeDistributionTransform", "path": "distributions#torch.distributions.transforms.CumulativeDistributionTransform", "type": "Probability Distributions", "text": ["Transform via the cumulative distribution function of a probability distribution.", "distribution (Distribution) \u2013 Distribution whose cumulative distribution function to use for the transformation.", "Example:"]}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=exp\u2061(x)y = \\exp(x)."]}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "Probability Distributions", "text": ["Wrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian()."]}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.", "This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization."]}, {"name": "torch.distributions.transforms.PositiveDefiniteTransform", "path": "distributions#torch.distributions.transforms.PositiveDefiniteTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained matrices to positive-definite matrices."]}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=xexponenty = x^{\\text{exponent}}."]}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "Probability Distributions", "text": ["Unit Jacobian transform to reshape the rightmost part of a tensor.", "Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape()."]}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and x=logit(y)x = \\text{logit}(y)."]}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x) then normalizing.", "This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms."]}, {"name": "torch.distributions.transforms.SoftplusTransform", "path": "distributions#torch.distributions.transforms.SoftplusTransform", "type": "Probability Distributions", "text": ["Transform via the mapping Softplus(x)=log\u2061(1+exp\u2061(x))\\text{Softplus}(x) = \\log(1 + \\exp(x)). The implementation reverts to the linear function when x>20x > 20."]}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "Probability Distributions", "text": ["Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().", "Example:"]}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "Probability Distributions", "text": ["Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.", "This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.", "This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization."]}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "Probability Distributions", "text": ["Transform via the mapping y=tanh\u2061(x)y = \\tanh(x).", "It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead.", "Note that one should use cache_size=1 when it comes to NaN/Inf values."]}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "Probability Distributions", "text": ["Abstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution.", "Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:", "However the following will error when caching due to dependency reversal:", "Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().", "cache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.", "Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t.", "Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.", "Computes the log det jacobian log |dy/dx| given input and output.", "Infers the shape of the forward computation, given the input shape. Defaults to preserving shape.", "Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "Probability Distributions", "text": ["Infers the shape of the forward computation, given the input shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.inv", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "Probability Distributions", "text": ["Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t."]}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "Probability Distributions", "text": ["Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape."]}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "Probability Distributions", "text": ["Computes the log det jacobian log |dy/dx| given input and output."]}, {"name": "torch.distributions.transforms.Transform.sign", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "Probability Distributions", "text": ["Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms."]}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "Probability Distributions", "text": ["Bases: Distribution", "Generates uniformly distributed random samples from the half-open interval [low, high).", "Example:"]}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.mean", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.mode", "path": "distributions#torch.distributions.uniform.Uniform.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.stddev", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.support", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.uniform.Uniform.variance", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "Probability Distributions", "text": ["Bases: Distribution", "A circular von Mises distribution.", "This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.", "The provided mean is the circular one.", "The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157.", "The provided variance is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.mean", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "Probability Distributions", "text": ["The provided mean is the circular one."]}, {"name": "torch.distributions.von_mises.VonMises.mode", "path": "distributions#torch.distributions.von_mises.VonMises.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "Probability Distributions", "text": ["The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157."]}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "Probability Distributions", "text": ["The provided variance is the circular one."]}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "Probability Distributions", "text": ["Bases: TransformedDistribution", "Samples from a two-parameter Weibull distribution."]}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.mean", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.mode", "path": "distributions#torch.distributions.weibull.Weibull.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.weibull.Weibull.variance", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart", "path": "distributions#torch.distributions.wishart.Wishart", "type": "Probability Distributions", "text": ["Bases: ExponentialFamily", "Creates a Wishart distribution parameterized by a symmetric positive definite matrix \u03a3\\Sigma, or its Cholesky decomposition \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top", "Note", "Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. \u2018torch.distributions.LKJCholesky\u2019 is a restricted Wishart distribution.[1]", "References", "[1] Wang, Z., Wu, Y. and Chu, H., 2018. On equivalence of the LKJ distribution and the restricted Wishart distribution. [2] Sawyer, S., 2007. Wishart Distributions and Inverse-Wishart Sampling. [3] Anderson, T. W., 2003. An Introduction to Multivariate Statistical Analysis (3rd ed.). [4] Odell, P. L. & Feiveson, A. H., 1966. A Numerical Procedure to Generate a SampleCovariance Matrix. JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.", "Warning", "In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly."]}, {"name": "torch.distributions.wishart.Wishart.arg_constraints", "path": "distributions#torch.distributions.wishart.Wishart.arg_constraints", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.covariance_matrix", "path": "distributions#torch.distributions.wishart.Wishart.covariance_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.entropy()", "path": "distributions#torch.distributions.wishart.Wishart.entropy", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.expand()", "path": "distributions#torch.distributions.wishart.Wishart.expand", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.has_rsample", "path": "distributions#torch.distributions.wishart.Wishart.has_rsample", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.log_prob()", "path": "distributions#torch.distributions.wishart.Wishart.log_prob", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.mean", "path": "distributions#torch.distributions.wishart.Wishart.mean", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.mode", "path": "distributions#torch.distributions.wishart.Wishart.mode", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.precision_matrix", "path": "distributions#torch.distributions.wishart.Wishart.precision_matrix", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.rsample()", "path": "distributions#torch.distributions.wishart.Wishart.rsample", "type": "Probability Distributions", "text": ["Warning", "In some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return -inf values in .log_prob(). In those cases, the user should validate the samples and either fix the value of df or adjust max_try_correction value for argument in .rsample accordingly."]}, {"name": "torch.distributions.wishart.Wishart.scale_tril", "path": "distributions#torch.distributions.wishart.Wishart.scale_tril", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.support", "path": "distributions#torch.distributions.wishart.Wishart.support", "type": "Probability Distributions", "text": []}, {"name": "torch.distributions.wishart.Wishart.variance", "path": "distributions#torch.distributions.wishart.Wishart.variance", "type": "Probability Distributions", "text": []}, {"name": "torch.div", "path": "generated/torch.div", "type": "Torch", "text": ["Divides each element of the input input by the corresponding element of other.", "Note", "By default, this performs a \u201ctrue\u201d division like Python 3. See the rounding_mode argument for floor division.", "Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.", "rounding_mode (str, optional) \u2013 ", "Type of rounding applied to the result:", "Examples:"]}, {"name": "torch.divide", "path": "generated/torch.divide", "type": "Torch", "text": ["Alias for torch.div()."]}, {"name": "torch.dot", "path": "generated/torch.dot", "type": "Torch", "text": ["Computes the dot product of two 1D tensors.", "Note", "Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.DoubleStorage", "path": "storage#torch.DoubleStorage", "type": "Storage", "text": []}, {"name": "torch.DoubleStorage.dtype", "path": "storage#torch.DoubleStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.dsplit", "path": "generated/torch.dsplit", "type": "Torch", "text": ["Splits input, a tensor with three or more dimensions, into multiple tensors depthwise according to indices_or_sections. Each split is a view of input.", "This is equivalent to calling torch.tensor_split(input, indices_or_sections, dim=2) (the split dimension is 2), except that if indices_or_sections is an integer it must evenly divide the split dimension or a runtime error will be thrown.", "This function is based on NumPy\u2019s numpy.dsplit()."]}, {"name": "torch.dstack", "path": "generated/torch.dstack", "type": "Torch", "text": ["Stack tensors in sequence depthwise (along third axis).", "This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d().", "tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.dtype", "path": "tensor_attributes#torch.dtype", "type": "Miscellaneous", "text": []}, {"name": "torch.einsum", "path": "generated/torch.einsum", "type": "Torch", "text": ["Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.", "Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation. The details of this format are described below, but the general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output. The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output. For example, matrix multiplication can be computed using einsum as torch.einsum(\u201cij,jk->ik\u201d, A, B). Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why).", "Equation:", "The equation string specifies the subscripts (letters in [a-zA-Z]) for each dimension of the input operands in the same order as the dimensions, separating subscripts for each operand by a comma (\u2018,\u2019), e.g. \u2018ij,jk\u2019 specify subscripts for two 2D operands. The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts, and then summing out the dimensions whose subscripts are not part of the output.", "Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation followed by the subscripts for the output. For instance, the following equation computes the transpose of a matrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and at most once for the output.", "Ellipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis. Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts, e.g. for an input operand with 5 dimensions, the ellipsis in the equation \u2018ab\u2026c\u2019 cover the third and fourth dimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the \u2018shape\u2019 of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis will come first in the output (left-most dimensions), before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements batch matrix multiplication \u2018\u2026ij,\u2026jk\u2019.", "A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis, arrow and comma) but something like \u2018\u2026\u2019 is not valid. An empty string \u2018\u2019 is valid for scalar operands.", "Note", "torch.einsum handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.", "Note", "This function uses opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) to speed up computation or to consume less memory by optimizing contraction order. This optimization occurs when there are at least three inputs, since the order does not matter otherwise. Note that finding _the_ optimal path is an NP-hard problem, thus, opt_einsum relies on different heuristics to achieve near-optimal results. If opt_einsum is not available, the default order is to contract from left to right.", "To bypass this default behavior, add the following line to disable the usage of opt_einsum and skip path calculation: torch.backends.opt_einsum.enabled = False", "To specify which strategy you\u2019d like for opt_einsum to compute the contraction path, add the following line: torch.backends.opt_einsum.strategy = \u2018auto\u2019. The default strategy is \u2018auto\u2019, and we also support \u2018greedy\u2019 and \u2018optimal\u2019. Disclaimer that the runtime of \u2018optimal\u2019 is factorial in the number of inputs! See more details in the opt_einsum documentation (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html).", "Note", "As of PyTorch 1.10 torch.einsum() also supports the sublist format (see examples below). In this format, subscripts for each operand are specified by sublists, list of integers in the range [0, 52). These sublists follow their operands, and an extra sublist can appear at the end of the input to specify the output\u2019s subscripts., e.g. torch.einsum(op1, sublist1, op2, sublist2, \u2026, [subslist_out]). Python\u2019s Ellipsis object may be provided in a sublist to enable broadcasting as described in the Equation section above.", "Tensor", "Examples:"]}, {"name": "torch.empty", "path": "generated/torch.empty", "type": "Torch", "text": ["Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "size (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.", "Example:"]}, {"name": "torch.empty_like", "path": "generated/torch.empty_like", "type": "Torch", "text": ["Returns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "input (Tensor) \u2013 the size of input will determine size of the output tensor.", "Example:"]}, {"name": "torch.empty_strided", "path": "generated/torch.empty_strided", "type": "Torch", "text": ["Creates a tensor with the specified size and stride and filled with undefined data.", "Warning", "If the constructed tensor is \u201coverlapped\u201d (with multiple indices referring to the same element in memory) its behavior is undefined.", "Note", "If torch.use_deterministic_algorithms() is set to True, the output tensor is initialized to prevent any possible nondeterministic behavior from using the data as an input to an operation. Floating point and complex tensors are filled with NaN, and integer tensors are filled with the maximum value.", "Example:"]}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "Torch", "text": ["Context-manager that enables gradient calculation.", "Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled.", "This context manager is thread local; it will not affect computation in other threads.", "Also functions as a decorator.", "Note", "enable_grad is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.", "Note", "This API does not apply to forward-mode AD."]}, {"name": "torch.eq", "path": "generated/torch.eq", "type": "Torch", "text": ["Computes element-wise equality", "The second argument can be a number or a tensor whose shape is broadcastable with the first argument.", "out (Tensor, optional) \u2013 the output tensor.", "A boolean tensor that is True where input is equal to other and False elsewhere", "Example:"]}, {"name": "torch.equal", "path": "generated/torch.equal", "type": "Torch", "text": ["True if two tensors have the same size and elements, False otherwise.", "Example:"]}, {"name": "torch.erf", "path": "generated/torch.erf", "type": "Torch", "text": ["Alias for torch.special.erf()."]}, {"name": "torch.erfc", "path": "generated/torch.erfc", "type": "Torch", "text": ["Alias for torch.special.erfc()."]}, {"name": "torch.erfinv", "path": "generated/torch.erfinv", "type": "Torch", "text": ["Alias for torch.special.erfinv()."]}, {"name": "torch.exp", "path": "generated/torch.exp", "type": "Torch", "text": ["Returns a new tensor with the exponential of the elements of the input tensor input.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.exp2", "path": "generated/torch.exp2", "type": "Torch", "text": ["Alias for torch.special.exp2()."]}, {"name": "torch.expm1", "path": "generated/torch.expm1", "type": "Torch", "text": ["Alias for torch.special.expm1()."]}, {"name": "torch.export", "path": "export", "type": "Traced Graph Export", "text": ["Warning", "This feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.", "torch.export.export() takes an arbitrary Python callable (a torch.nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.", "torch.export produces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found here (coming soon!).", "Under the hood, torch.export leverages the following latest technologies:", "torch.compile() also utilizes the same PT2 stack as torch.export, but is slightly different:", "Compared to torch.fx.symbolic_trace(), torch.export traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, torch.export keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, torch.export is expected to work on more user programs, and produce lower-level graphs (at the torch.ops.aten operator level). Note that users can still use torch.fx.symbolic_trace() as a preprocessing step before torch.export.", "Compared to torch.jit.script(), torch.export does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).", "Compared to torch.jit.trace(), torch.export is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.", "The main entrypoint is through torch.export.export(), which takes a callable (torch.nn.Module, function, or method) and sample inputs, and captures the computation graph into an torch.export.ExportedProgram. An example:", "Inspecting the ExportedProgram, we can note the following:", "By default torch.export will trace the program assuming all input shapes are static, and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be marked dynamic using the torch.export.dynamic_dim() API, and passed into torch.export.export() through the constraints argument. An example:", "Some additional things to note:", "To save the ExportedProgram, users can use the torch.export.save() and torch.export.load() APIs. A convention is to save the ExportedProgram using a .pt2 file extension.", "An example:", "As mentioned before, by default, torch.export will trace the program specializing on the input tensors\u2019 shapes, unless a dimension is specified as dynamic via the torch.export.dynamic_dim() API. This means that if there exists shape-dependent control flow, torch.export will specialize on the branch that is being taken with the given sample inputs. For example:", "The conditional of (x.shape[0] > 5) does not appear in the ExportedProgram because the example inputs have the static shape of (10, 2). Since torch.export specializes on the inputs\u2019 static shapes, the else branch (x - 1) will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, torch.export.dynamic_dim() will need to be used to specify the dimension of the input tensor (x.shape[0]) to be dynamic, and the source code will need to be rewritten.", "torch.export also specializes the traced graph based on the values of inputs that are not torch.Tensor, such as int, float, bool, and str. However, we will likely change this in the near future to not specialize on inputs of primitive types.", "For example:", "Because integers are specialized, the torch.ops.aten.add.Tensor operations are all computed with the inlined constant 1, rather than arg1_1. Additionally, the times iterator used in the for loop is also \u201cinlined\u201d in the graph through the 3 repeated torch.ops.aten.add.Tensor calls, and the input arg2_1 is never used.", "As torch.export is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of torch.compile, an unsupported operation will cause a \u201cgraph break\u201d and the unsupported operation will be run with default Python evaluation. In contrast, torch.export will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.", "When a graph break is encountered, ExportDB is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.", "Graph breaks can also be encountered on data-dependent control flow (if\nx.shape[0] > 2) when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators (coming soon!).", "Data dependent behavior such as using the value inside of a tensor to construct another tensor, or using the value of a tensor to slice into another tensor, is also something the tracer cannot fully determine. Users will need to rewrite their code using the inline constraint APIs torch.export.constrain_as_size() and torch.export.constrain_as_value().", "When tracing, a META implementation (or \u201cmeta kernel\u201d) is required for all operators. This is used to reason about the input/output shapes for this operator.", "Note that the official API for registering custom meta kernels for custom ops is currently undergoing development. While the final API is being refined, you can refer to the documentation here.", "In the unfortunate case where your model uses an ATen operator that is does not have a meta kernel implementation yet, please file an issue.", "Additional Links for Export Users", "Deep Dive for PyTorch Developers", "export() takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional Core ATen Operator Set and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.", "Soundness Guarantee", "While tracing, export() takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output ExportedProgram is considered valid only when these assumptions hold true.", "There are 2 types of assumptions made during tracing", "All assumptions must be validated at graph capture time for export() to succeed. Specifically:", "If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example export() would suggest following code for input constraints:", "This example means the program requires the dim 0 of input x to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into constraints argument.", "An ExportedProgram containing the traced callable.", "ExportedProgram", "Acceptable input/output types", "Acceptable types of inputs (for args and kwargs) and outputs include:", "dynamic_dim() constructs a Constraint object that describes the dynamism of a dimension index of tensor t. Constraint objects should be passed to constraints argument of export().", "A Constraint object that describes shape dynamism. It can be passed to export() so that export() does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.", "Specifically dynamic_dim() can be used to express following types of dynamism.", "Size of a dimension is dynamic and unbounded:", "Size of a dimension is dynamic with a lower bound:", "Size of a dimension is dynamic with an upper bound:", "Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:", "Hint export() about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.", "None", "For example, following program can not be traced soundly wihout using constrain_as_size() to give export() a hint about shape ranges:", "export() would give following error:", "Assuming the actual range of d can be between [3, 10], you can add a call to constrain_as_size() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed", "Hint export() about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.", "Warning", "(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call constrain_as_size() instead.)", "None", "For example, following program can not be traced soundly:", "v is a data-dependent value, which is assumed to have a range of (-inf, inf). export() a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus export() would give following error:", "Assuming the actual range of v can be between [10, 200], you can add a call to constrain_as_value() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Saves an ExportedProgram to a file-like object. It can then be loaded using the Python API torch.export.load.", "Example:", "Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Loads an ExportedProgram previously saved with torch.export.save.", "An ExportedProgram object", "ExportedProgram", "Example:", "Warning", "Do not construct Constraint directly, use dynamic_dim() instead.", "This represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range.", "Package of a program from export(). It contains an torch.fx.Graph that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.", "You can call an ExportedProgram like the original callable traced by export() with the same calling convention.", "To perform transformations on the graph, use .module property to access an torch.fx.GraphModule. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use export() again to construct a correct ExportedProgram.", "Returns a self contained GraphModule with all the parameters/buffers inlined.", "ExportGraphSignature models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.", "Export Graph is functional and does not access \u201cstates\u201d like parameters or buffers within the graph via getattr nodes. Instead, export() gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.", "The ordering of all inputs and outputs are:", "e.g. If following module is exported:", "Resulting Graph would be:", "Resulting ExportGraphSignature would be:", "An enumeration."]}, {"name": "torch.export.ArgumentKind", "path": "export#torch.export.ArgumentKind", "type": "Traced Graph Export", "text": ["An enumeration."]}, {"name": "torch.export.ArgumentSpec", "path": "export#torch.export.ArgumentSpec", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.constrain_as_size()", "path": "export#torch.export.constrain_as_size", "type": "Traced Graph Export", "text": ["Hint export() about the constraint of an intermediate scalar value that represents shape of a tensor so that subsequent tensor constructors can be traced correctly because many operators need to make assumption about range of sizes.", "None", "For example, following program can not be traced soundly wihout using constrain_as_size() to give export() a hint about shape ranges:", "export() would give following error:", "Assuming the actual range of d can be between [3, 10], you can add a call to constrain_as_size() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:", "Warning", "if your size is intended to be dynamic, do NOT test if sizes are equal to 0 or 1, these will SILENTLY report false and be bypassed"]}, {"name": "torch.export.constrain_as_value()", "path": "export#torch.export.constrain_as_value", "type": "Traced Graph Export", "text": ["Hint export() about the constraint of an intermediate scalar value so that subsequent branching behaviors that check on the range of aforementioned scalar value can be soundly traced.", "Warning", "(Note that if the intermediate scalar value will be used like a size, including being passed as size arg to a tensor factory or view, call constrain_as_size() instead.)", "None", "For example, following program can not be traced soundly:", "v is a data-dependent value, which is assumed to have a range of (-inf, inf). export() a hint about which branch to take would not be able to determine if the traced branching decision is correct or not. Thus export() would give following error:", "Assuming the actual range of v can be between [10, 200], you can add a call to constrain_as_value() in the source code like this:", "With the additional hint, export() would be able to trace the program correctly by taking the else branch, resulting in following graph:"]}, {"name": "torch.export.Constraint", "path": "export#torch.export.Constraint", "type": "Traced Graph Export", "text": ["Warning", "Do not construct Constraint directly, use dynamic_dim() instead.", "This represents constraints on input tensor dimensions, e.g., requiring them to be fully polymorphic or within some range."]}, {"name": "torch.export.Dynamic shapes", "path": "torch.compiler_dynamic_shapes", "type": "Traced Graph Export", "text": ["Code: symbolic_shapes.py", "See also: The dynamic shapes manual", "Deep learning compilers commonly only work for static shapes, that is to say, they produced compiled programs which only work for a single specific configuration of input shapes, and must recompile if any input shape changes. This assumption works great for the majority of commonly run deep learning models today, but there are a few situations where it is insufficient:", "In supporting dynamic shapes, we chose not to support dynamic rank programs, e.g., programs whose inputs tensors change in dimensionality, as this pattern rarely occurs in real-world deep learning programs, and it avoids the need to reason inductively over symbolic lists of shapes.", "The default dynamic behavior in PyTorch 2.1 is:", "When considering how to add support for dynamic shapes to TorchDynamo and TorchInductor, we made a major design decision: in order to reuse decompositions and other preexisting code written in Python/C++ targeting the PyTorch API, we must be able to trace through dynamic shapes. Unlike a fully symbolic system which might capture both branches of a conditional, we always pick one branch and specialize our trace under the assumption that we only use this trace when we would have made the same choice for that branch in the future. To do this, we maintain a \u201chint\u201d for every symbolic size saying what its concrete value is at compile time (as TorchDynamo is a just-in-time compiler, it always knows what the actual input sizes are.) When we perform a condition on a tensor, we simply consult the hint to find out which branch to take.", "This greatly simplifies the symbolic shape formulas we produce, but means we have a much more involved system for managing guards. Consider, for example, the following program:", "The final IR we will compile with TorchInductor will either be torch.cat([x, y]).add(2) or torch.cat([x, y]).mul(2) (with the condition flattened away), but to determine which branch we are in, we would need to know the size of z, an intermediate. Because TorchDynamo must know upfront if a compiled trace is valid (we do not support bailouts, like some JIT compilers), we must be able to reduce z.size(0) as an expression in terms of the inputs, x.size(0) + y.size(0). This is done by writing meta functions for all operators in PyTorch which can propagate size information to the output of a tensor without actually performing computation on the node.", "Symbolic shapes workflow:", "Important files:", "Understanding the Python class hierarchy:", "C++ is fairly similar:", "When you write code that is traceable with make_fx, it must be able to deal with SymInt/SymFloat/SymBool flowing through it. The dynamic shapes manual gives some guidance for how to do this.", "Symbolic reasoning:", "To resolve control flow, we check the hint, aka actual value, of a symbolic integer to determine which branch to go. However, in some cases, we may not have a hint: so-called unbacked symbolic integers arise when a size variable emerges from a data-dependent operation like .nonzero() or .item(). It is illegal to perform control flow on these symbolic integers, so we must graph break on these operations.", "Naively implemented, this is too restrictive: most PyTorch programs will immediately fail if you try to do anything with unbacked symbolic integers. Here are the most important enhancements to make this actually work:", "In future versions of PT2 (beyond PT2.1), we will extend our reasoning system to infer that an unbacked symbolic integer is size-like based on usage. For example, if you pass the result of an .item() call to a factory function like torch.empty, we will automatically infer that the result is a size (because if it was not, it would fail.) This assumption would get validated at runtime, raising an error if it was not fulfilled."]}, {"name": "torch.export.dynamic_dim()", "path": "export#torch.export.dynamic_dim", "type": "Traced Graph Export", "text": ["dynamic_dim() constructs a Constraint object that describes the dynamism of a dimension index of tensor t. Constraint objects should be passed to constraints argument of export().", "A Constraint object that describes shape dynamism. It can be passed to export() so that export() does not assume static size of specified tensor, i.e. keeping it dynamic as a symbolic size rather than specializing according to size of example tracing input.", "Specifically dynamic_dim() can be used to express following types of dynamism.", "Size of a dimension is dynamic and unbounded:", "Size of a dimension is dynamic with a lower bound:", "Size of a dimension is dynamic with an upper bound:", "Size of a dimension is dynamic and it is always equal to size of another dynamic dimension:"]}, {"name": "torch.export.export()", "path": "export#torch.export.export", "type": "Traced Graph Export", "text": ["export() takes an arbitrary Python callable (an nn.Module, a function or a method) and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized. The traced graph (1) produces a normalized operator set consisting only of functional Core ATen Operator Set and user specified custom operators, (2) has eliminated all Python control flow and data structures (except for certain conditions), and (3) has the set of shape constraints needed to show that this normalization and control flow elimination is sound for a future input.", "Soundness Guarantee", "While tracing, export() takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output ExportedProgram is considered valid only when these assumptions hold true.", "There are 2 types of assumptions made during tracing", "All assumptions must be validated at graph capture time for export() to succeed. Specifically:", "If any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested code needed to construct necessary constraints to validate the assumptions, for example export() would suggest following code for input constraints:", "This example means the program requires the dim 0 of input x to be less than or equal to 5 to be valid. You can inspect the constraints needed and then copy this exact function into your code to generated needed constraints to be passed into constraints argument.", "An ExportedProgram containing the traced callable.", "ExportedProgram", "Acceptable input/output types", "Acceptable types of inputs (for args and kwargs) and outputs include:"]}, {"name": "torch.export.ExportBackwardSignature", "path": "export#torch.export.ExportBackwardSignature", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.ExportDB", "path": "generated/exportdb/index", "type": "Traced Graph Export", "text": ["ExportDB is a centralized dataset of supported and unsupported export cases. It is targeted towards users who want to understand specifically what types of code are supported, the subtleties of export, and how to modify their existing code to be compatible with export. Note that this is not an exhaustive set of everything that is supported by exportdb, but it covers the most common and confusing use cases that users will run into.", "If you have a feature that you think needs a stronger guarantee from us to support in export please create an issue in the pytorch/pytorch repo wih a module:export tag.", "Tags", "Note", "Tags: torch.escape-hatch", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.assert", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.context-manager", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags:", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "You can rewrite the example above to something like the following:"]}, {"name": "torch.export.ExportDB.python.assert", "path": "generated/exportdb/python.assert", "type": "Traced Graph Export", "text": ["Note", "Tags: python.assert", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.builtin", "path": "generated/exportdb/python.builtin", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "You can rewrite the example above to something like the following:"]}, {"name": "torch.export.ExportDB.python.closure", "path": "generated/exportdb/python.closure", "type": "Traced Graph Export", "text": ["Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.context-manager", "path": "generated/exportdb/python.context-manager", "type": "Traced Graph Export", "text": ["Note", "Tags: python.context-manager", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.control-flow", "path": "generated/exportdb/python.control-flow", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.python.data-structure", "path": "generated/exportdb/python.data-structure", "type": "Traced Graph Export", "text": ["Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.data-structure, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.cond", "path": "generated/exportdb/torch.cond", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: python.closure, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.dynamic-shape", "path": "generated/exportdb/torch.dynamic-shape", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, torch.cond", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.control-flow", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.builtin", "Support Level: NOT_SUPPORTED_YET", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape, python.assert, python.data-structure", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.dynamic-value", "path": "generated/exportdb/torch.dynamic-value", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.escape-hatch", "path": "generated/exportdb/torch.escape-hatch", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.escape-hatch", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:", "Note", "Tags: torch.escape-hatch, torch.dynamic-value", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportDB.torch.map", "path": "generated/exportdb/torch.map", "type": "Traced Graph Export", "text": ["Note", "Tags: torch.map, torch.dynamic-shape", "Support Level: SUPPORTED", "Original source code:", "Result:"]}, {"name": "torch.export.ExportedProgram", "path": "export#torch.export.ExportedProgram", "type": "Traced Graph Export", "text": ["Package of a program from export(). It contains an torch.fx.Graph that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.", "You can call an ExportedProgram like the original callable traced by export() with the same calling convention.", "To perform transformations on the graph, use .module property to access an torch.fx.GraphModule. You can then use FX transformation to rewrite the graph. Afterwards, you can simply use export() again to construct a correct ExportedProgram.", "Returns a self contained GraphModule with all the parameters/buffers inlined."]}, {"name": "torch.export.ExportedProgram.module()", "path": "export#torch.export.ExportedProgram.module", "type": "Traced Graph Export", "text": ["Returns a self contained GraphModule with all the parameters/buffers inlined."]}, {"name": "torch.export.ExportGraphSignature", "path": "export#torch.export.ExportGraphSignature", "type": "Traced Graph Export", "text": ["ExportGraphSignature models the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.", "Export Graph is functional and does not access \u201cstates\u201d like parameters or buffers within the graph via getattr nodes. Instead, export() gurantees that parameters and buffers are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.", "The ordering of all inputs and outputs are:", "e.g. If following module is exported:", "Resulting Graph would be:", "Resulting ExportGraphSignature would be:"]}, {"name": "torch.export.Fake tensor", "path": "torch.compiler_fake_tensor", "type": "Traced Graph Export", "text": ["Code: fake_tensor.py", "When doing Dynamo symbolic evaluation and compiler passes, we often want to be able to run tensor operations to understand what output sizes/dtypes/devices are, without actually running those operations (or trashing preexisting tensors), which would be slower (if you\u2019re doing a lot of compute) and take a lot of memory (it\u2019s bad if your compiler needs to use GPU memory while you are compiling the program). A fake tensor is like a real tensor in all respects, except that it doesn\u2019t actually have any data. For example, when we do Dynamo tracing, we need to trace through user Tensor code and answer questions about intermediates (e.g., if a user does a conditional on an intermediate tensor). Without fake tensor, we would not have accurate information for these queries.", "Similarly, suppose you want to store metadata for a tensor, e.g., on an FX IR node (meta[\u2018val\u2019]). You can instead store a fake tensor directly on the node, which will give you all the metadata you need for the tensor, including subtle stuff that you probably wouldn\u2019t have handled (e.g., aliasing relationships).", "All fake tensors are associated with a FakeTensorMode. Because fake tensor\u2019s primary use case is to do analysis on real tensors, the general workflow is you have a bunch of real tensors, you allocate a FakeTensorMode, and then you use from_real_tensor to convert all those real tensors into fake tensors, and then you do things to the fake tensors. In particular, the FakeTensorMode maintains a memo table persistently mapping tensors (and storages) to the same storages. If you fakeify the same tensor multiple times, you will get the same fake tensor; if you fakeify two tensors which alias each other, you will get two fake tensors which alias the same fake storage. FakeTensors are tensor subclasses, so if you do operations on them, you\u2019ll automatically get a fake tensor, but in general you will want to do operations on fake tensors (e.g., if you\u2019re running an FX pass) with the FakeTensorMode active; what a tensor operation will do is automatically turn on the fake tensor mode and try again.", "A fake tensor is represented as a __torch_dispatch__ tensor subclass of a meta tensor. This means under the hood, fake tensors are meta device tensors; they then use extra extensibility hooks, specifically dispatch_device, to lie about what the actual device of the tensor is. This was one of the more error-prone parts of fake tensors in the early days: sometimes, fake tensors were too good at lying about being CPU/CUDA whatever, and you\u2019d end up with a CPU kernel getting called with a fake tensor trying to dereference the data pointer, which obviously won\u2019t work. If you are segfaulting in fake tensor code, this is the first thing you should check: is the C++ backtrace in a CPU kernel (unexpected!) or a meta kernel (expected!) A meta kernel is like a real kernel, but all it does is allocate the outputs, it doesn\u2019t do any data compute.", "A tensor subclass has to define how to implement various operations. Here is the general fake tensor recipe:", "Non-PT2 usage (check out test/test_fake_tensor.py for more examples):", "Q: Why do you have real tensors as inputs?", "A: In a PT2 context, this is because you typically are compiling just-in-time, so for all the inputs to a graph you\u2019re compiling, you already have the \u201creal\u201d inputs, because you\u2019re compiling while you\u2019re executing the program.", "PT2 pre-AOTAutograd usage (this is unusual, you probably don\u2019t want to do this):", "detect_fake_mode will search a number of locations to try to find \u201cthe\u201d fake tensor mode associated with the lifecycle. Typically it will be pulled off of the tracing context.", "PT2 post-AOTAutograd usage:", "# Fake mode is enabled! example_inputs is typically fake already # TODO: we probably want to change this # Still do this to access fake mode fake_mode = detect_fake_mode(example_inputs) # But in general you don\u2019t have to turn it on", "Other useful stuff:", "When might you want to disable fake tensor mode? Usually you don\u2019t want to do this. One niche case where we\u2019ve found it useful is to implement constant propagation on fake tensors: in this case, we need to do some actual tensor computation even though we\u2019re in a fake tensor mode.", "Auto-convert or not? Originally, FakeTensorMode would not automatically fakeify real tensors if you tried to do compute on them inside a FakeTensorMode region. The motivation behind this was to prevent the following footgun:", "What should this code do? It would be surprising if we actually modified the metadata on the real tensor. But at the same time, there isn\u2019t any obvious opportunity to create a FakeTensor. So we conservatively decided to make this raise an error: \u201cInvoking operators with non-Fake Tensor inputs in FakeTensorMode is not yet supported. Please convert all Tensors to FakeTensors first.\u201d", "This error is pretty annoying in practice. For example, suppose you have a real nn.Module and you want to feed fake tensors through it. You need to somehow fakeify the nn.Module. This motivated FakeCopyMode.", "Eventually, we gave up and added automatic fakeification. However, this is still not yet enabled by default in many uses of FakeTensorMode.", "Metadata mutation on fake tensor If you have a fake tensor, and you t_() it, the metadata on the fake tensor changes. This is reasonable on its face, but sometimes you want to also store fake tensors as metadata on FX nodes; mutating a fake tensor is bad because this will invalidate old metadata!", "In fact, there is a fundamental tension here, which is that fake tensors maintain extremely accurate metadata about tensors, up to and including object identity. If object metadata changes over time in an FX graph, there is not actually any way to represent this change over time. Most of the time, our serious FX analyses are done on functionalized graphs, which don\u2019t have this, but occasionally you need to do an analysis on a non-functionalized graph. Maybe it was a mistake to put fake tensor in meta[\u2018val\u2019]", "Fake tensor uses both a subclass and a mode tensor subclass pattern, where FakeTensor.__torch_dispatch__ enables the FakeTensorMode associated with the fake tensor, and then redispatches (relying on FakeTensorMode to do the heavy lifting). If fake tensor operations get a subclass argument it doesn\u2019t recognize, it will return NotImplemented, giving the other subclass a chance to run first (hopefully desugaring into plain tensor operations), before it tries again. This can cause infinite loops.", "Unfortunately, there is a pretty complicated set of places where any given operator may be implemented. Some important cases to know about:", "Because fake tensors are used in situations that are very sensitive to the exact properties of a tensor, fake tensors do conversion very carefully, preserving leaf-ness, requires_grad\u2019ness, aliasing, and a whole host of other properties. The bulk of the heavy lifting is in MetaConverter.", "You would think fake tensors are fast because they don\u2019t do any tensor compute. But at small tensor sizes we are actually entirely overhead bound, and, well, fake tensor is in Python, and we often do a LOT of work to do a single tensor operation (because they are implemented as decompositions). So fake tensors are actually pretty slow in practice, especially when symbolic shapes are involved. There are two important fastpaths we currently have in fake tensor that make a big difference in practice:", "There is interest in sending fake tensors as user inputs into the PT2 stack, which would imply we would need to be able to create a fake tensor of a fake tensor. This isn\u2019t really supported right now, but maybe it would not be too difficult to do.", "Every FakeTensorMode contains a ShapeEnv, which tracks all symbolic shapes information. Their lifetimes are typically tied: they live and die together.", "Because FakeTensorMode has a ShapeEnv (but meta implementations do not), meta functions that are data-dependent and require allocating an unbacked SymInt live in fake tensor. Fake tensor also takes care of memoizing unbacked SymInts, so that, e.g., if you call nonzero() on the same fake tensor twice, you get the same symbolic size.", "Colab Tutorial On Using FakeTensor To Determine Max Batch Size"]}, {"name": "torch.export.IRs", "path": "torch.compiler_ir", "type": "Traced Graph Export", "text": ["PyTorch 2.0 offers two set of IRs for backends to interface with: Core Aten IR and Prims IR.", "Core aten ops is the core subset of aten operators that can be used to compose other operators. Core aten IR is fully functional, and there is no inplace or _out variants in this opset. In contrast to Prims IR, core aten ops reuses the existing aten ops in \u201cnative_functions.yaml\u201d, and it doesn\u2019t further decompose ops into explicit type promotion and broadcasting ops. This opset is designed to serve as the functional IR to interface with backends.", "Warning", "This opset is still under active development, more ops will be added in the future.", "Operator", "Schema", "aten._adaptive_avg_pool2d", "_adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor", "aten._adaptive_avg_pool2d_backward", "_adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor", "aten._adaptive_avg_pool3d", "_adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor", "aten._cdist_forward", "_cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor", "aten._embedding_bag", "_embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)", "aten._local_scalar_dense", "_local_scalar_dense(Tensor self) -> Scalar", "aten._log_softmax", "_log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "aten._native_batch_norm_legit", "_native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._native_batch_norm_legit.no_stats", "_native_batch_norm_legit.no_stats(Tensor input, Tensor? weight, Tensor? bias, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._native_batch_norm_legit_no_training", "_native_batch_norm_legit_no_training(Tensor input, Tensor? weight, Tensor? bias, Tensor running_mean, Tensor running_var, float momentum, float eps) -> (Tensor, Tensor, Tensor)", "aten._pdist_forward", "_pdist_forward(Tensor self, float p=2) -> Tensor", "aten._softmax", "_softmax(Tensor self, int dim, bool half_to_float) -> Tensor", "aten._to_copy", "_to_copy(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, bool non_blocking=False, MemoryFormat? memory_format=None) -> Tensor", "aten.abs", "abs(Tensor self) -> Tensor", "aten.acos", "acos(Tensor self) -> Tensor", "aten.acosh", "acosh(Tensor self) -> Tensor", "aten.adaptive_avg_pool1d", "adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor", "aten.add.Scalar", "add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", "aten.add.Tensor", "add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", "aten.addmm", "addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor", "aten.alias", "alias(Tensor(a) self) -> Tensor(a)", "aten.amax", "amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor", "aten.amin", "amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor", "aten.any", "any(Tensor self) -> Tensor", "aten.any.dim", "any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor", "aten.arange.start_step", "arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.argmax", "argmax(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", "aten.argmin", "argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor", "aten.as_strided", "as_strided(Tensor(a) self, SymInt[] size, SymInt[] stride, SymInt? storage_offset=None) -> Tensor(a)", "aten.asin", "asin(Tensor self) -> Tensor", "aten.asinh", "asinh(Tensor self) -> Tensor", "aten.atan", "atan(Tensor self) -> Tensor", "aten.atanh", "atanh(Tensor self) -> Tensor", "aten.avg_pool1d", "avg_pool1d(Tensor self, int[1] kernel_size, int[1] stride=[], int[1] padding=0, bool ceil_mode=False, bool count_include_pad=True) -> Tensor", "aten.avg_pool2d", "avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor", "aten.avg_pool2d_backward", "avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor", "aten.avg_pool3d", "avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor", "aten.bitwise_and.Scalar", "bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_and.Tensor", "bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bitwise_not", "bitwise_not(Tensor self) -> Tensor", "aten.bitwise_or.Scalar", "bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_or.Tensor", "bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bitwise_xor.Scalar", "bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor", "aten.bitwise_xor.Tensor", "bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor", "aten.bmm", "bmm(Tensor self, Tensor mat2) -> Tensor", "aten.cat", "cat(Tensor[] tensors, int dim=0) -> Tensor", "aten.ceil", "ceil(Tensor self) -> Tensor", "aten.clamp", "clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor", "aten.clamp.Tensor", "clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor", "aten.clone", "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor", "aten.col2im", "col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor", "aten.constant_pad_nd", "constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor", "aten.convolution", "convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor", "aten.convolution_backward", "convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.cos", "cos(Tensor self) -> Tensor", "aten.cosh", "cosh(Tensor self) -> Tensor", "aten.cumsum", "cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor", "aten.div.Scalar", "div.Scalar(Tensor self, Scalar other) -> Tensor", "aten.div.Tensor", "div.Tensor(Tensor self, Tensor other) -> Tensor", "aten.embedding", "embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor", "aten.embedding_dense_backward", "embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor", "aten.empty.memory_format", "empty.memory_format(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor", "aten.empty_strided", "empty_strided(SymInt[] size, SymInt[] stride, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.eq.Scalar", "eq.Scalar(Tensor self, Scalar other) -> Tensor", "aten.eq.Tensor", "eq.Tensor(Tensor self, Tensor other) -> Tensor", "aten.erf", "erf(Tensor self) -> Tensor", "aten.exp", "exp(Tensor self) -> Tensor", "aten.expand", "expand(Tensor(a) self, SymInt[] size, *, bool implicit=False) -> Tensor(a)", "aten.fill.Scalar", "fill.Scalar(Tensor self, Scalar value) -> Tensor", "aten.flip", "flip(Tensor self, int[] dims) -> Tensor", "aten.floor", "floor(Tensor self) -> Tensor", "aten.fmod.Scalar", "fmod.Scalar(Tensor self, Scalar other) -> Tensor", "aten.fmod.Tensor", "fmod.Tensor(Tensor self, Tensor other) -> Tensor", "aten.full", "full(SymInt[] size, Scalar fill_value, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.gather", "gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor", "aten.ge.Scalar", "ge.Scalar(Tensor self, Scalar other) -> Tensor", "aten.ge.Tensor", "ge.Tensor(Tensor self, Tensor other) -> Tensor", "aten.gelu", "gelu(Tensor self, *, str approximate=\u2019none\u2019) -> Tensor", "aten.grid_sampler_2d", "grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor", "aten.gt.Scalar", "gt.Scalar(Tensor self, Scalar other) -> Tensor", "aten.gt.Tensor", "gt.Tensor(Tensor self, Tensor other) -> Tensor", "aten.hardtanh", "hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor", "aten.index.Tensor", "index.Tensor(Tensor self, Tensor?[] indices) -> Tensor", "aten.index_put", "index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor", "aten.index_select", "index_select(Tensor self, int dim, Tensor index) -> Tensor", "aten.isinf", "isinf(Tensor self) -> Tensor", "aten.isnan", "isnan(Tensor self) -> Tensor", "aten.le.Scalar", "le.Scalar(Tensor self, Scalar other) -> Tensor", "aten.le.Tensor", "le.Tensor(Tensor self, Tensor other) -> Tensor", "aten.leaky_relu", "leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor", "aten.log", "log(Tensor self) -> Tensor", "aten.logical_and", "logical_and(Tensor self, Tensor other) -> Tensor", "aten.logical_not", "logical_not(Tensor self) -> Tensor", "aten.logical_or", "logical_or(Tensor self, Tensor other) -> Tensor", "aten.logical_xor", "logical_xor(Tensor self, Tensor other) -> Tensor", "aten.lt.Scalar", "lt.Scalar(Tensor self, Scalar other) -> Tensor", "aten.lt.Tensor", "lt.Tensor(Tensor self, Tensor other) -> Tensor", "aten.max.dim", "max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", "aten.max_pool2d_with_indices", "max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", "aten.max_pool2d_with_indices_backward", "max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor", "aten.max_pool3d_with_indices", "max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)", "aten.maximum", "maximum(Tensor self, Tensor other) -> Tensor", "aten.mean", "mean(Tensor self, *, ScalarType? dtype=None) -> Tensor", "aten.mean.dim", "mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.min.dim", "min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)", "aten.minimum", "minimum(Tensor self, Tensor other) -> Tensor", "aten.mm", "mm(Tensor self, Tensor mat2) -> Tensor", "aten.mul.Scalar", "mul.Scalar(Tensor self, Scalar other) -> Tensor", "aten.mul.Tensor", "mul.Tensor(Tensor self, Tensor other) -> Tensor", "aten.native_dropout", "native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)", "aten.native_group_norm", "native_group_norm(Tensor input, Tensor? weight, Tensor? bias, SymInt N, SymInt C, SymInt HxW, int group, float eps) -> (Tensor, Tensor, Tensor)", "aten.native_group_norm_backward", "native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, SymInt N, SymInt C, SymInt HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.native_layer_norm", "native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)", "aten.native_layer_norm_backward", "native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)", "aten.ne.Scalar", "ne.Scalar(Tensor self, Scalar other) -> Tensor", "aten.ne.Tensor", "ne.Tensor(Tensor self, Tensor other) -> Tensor", "aten.neg", "neg(Tensor self) -> Tensor", "aten.nonzero", "nonzero(Tensor self) -> Tensor", "aten.permute", "permute(Tensor(a) self, int[] dims) -> Tensor(a)", "aten.pixel_shuffle", "pixel_shuffle(Tensor self, int upscale_factor) -> Tensor", "aten.pow.Tensor_Scalar", "pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor", "aten.pow.Tensor_Tensor", "pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor", "aten.prod", "prod(Tensor self, *, ScalarType? dtype=None) -> Tensor", "aten.prod.dim_int", "prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.rand", "rand(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.randn", "randn(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.randperm", "randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.reciprocal", "reciprocal(Tensor self) -> Tensor", "aten.reflection_pad1d", "reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor", "aten.reflection_pad2d", "reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor", "aten.reflection_pad3d", "reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor", "aten.relu", "relu(Tensor self) -> Tensor", "aten.remainder.Scalar", "remainder.Scalar(Tensor self, Scalar other) -> Tensor", "aten.remainder.Tensor", "remainder.Tensor(Tensor self, Tensor other) -> Tensor", "aten.repeat", "repeat(Tensor self, SymInt[] repeats) -> Tensor", "aten.replication_pad2d", "replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor", "aten.replication_pad3d", "replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor", "aten.roll", "roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor", "aten.round", "round(Tensor self) -> Tensor", "aten.rsqrt", "rsqrt(Tensor self) -> Tensor", "aten.scalar_tensor", "scalar_tensor(Scalar s, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor", "aten.scatter.src", "scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", "aten.scatter.value", "scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor", "aten.scatter_add", "scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor", "aten.scatter_reduce.two", "scatter_reduce.two(Tensor self, int dim, Tensor index, Tensor src, str reduce, *, bool include_self=True) -> Tensor", "aten.select.int", "select.int(Tensor(a) self, int dim, SymInt index) -> Tensor(a)", "aten.select_scatter", "select_scatter(Tensor self, Tensor src, int dim, SymInt index) -> Tensor", "aten.sigmoid", "sigmoid(Tensor self) -> Tensor", "aten.sign", "sign(Tensor self) -> Tensor", "aten.sin", "sin(Tensor self) -> Tensor", "aten.sinh", "sinh(Tensor self) -> Tensor", "aten.slice.Tensor", "slice.Tensor(Tensor(a) self, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor(a)", "aten.slice_scatter", "slice_scatter(Tensor self, Tensor src, int dim=0, SymInt? start=None, SymInt? end=None, SymInt step=1) -> Tensor", "aten.sort", "sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)", "aten.split_with_sizes", "split_with_sizes(Tensor(a -> *) self, SymInt[] split_sizes, int dim=0) -> Tensor(a)[]", "aten.sqrt", "sqrt(Tensor self) -> Tensor", "aten.squeeze.dim", "squeeze.dim(Tensor(a) self, int dim) -> Tensor(a)", "aten.squeeze.dims", "squeeze.dims(Tensor(a) self, int[] dim) -> Tensor(a)", "aten.sub.Scalar", "sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor", "aten.sub.Tensor", "sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor", "aten.sum.dim_IntList", "sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor", "aten.sym_numel", "sym_numel(Tensor self) -> SymInt", "aten.sym_size.int", "sym_size.int(Tensor self, int dim) -> SymInt", "aten.sym_storage_offset", "sym_storage_offset(Tensor self) -> SymInt", "aten.sym_stride.int", "sym_stride.int(Tensor self, int dim) -> SymInt", "aten.tan", "tan(Tensor self) -> Tensor", "aten.tanh", "tanh(Tensor self) -> Tensor", "aten.topk", "topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)", "aten.unsqueeze", "unsqueeze(Tensor(a) self, int dim) -> Tensor(a)", "aten.upsample_bilinear2d.vec", "upsample_bilinear2d.vec(Tensor input, SymInt[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor", "aten.upsample_nearest2d.vec", "upsample_nearest2d.vec(Tensor input, SymInt[]? output_size, float[]? scale_factors) -> Tensor", "aten.var.correction", "var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor", "aten.var.dim", "var.dim(Tensor self, int[1]? dim, bool unbiased=True, bool keepdim=False) -> Tensor", "aten.view", "view(Tensor(a) self, SymInt[] size) -> Tensor(a)", "aten.where.self", "where.self(Tensor condition, Tensor self, Tensor other) -> Tensor", "Prims IR is a set of primitive operators that can be used to compose other operators. Prims IR is a lower level opset than core aten IR, and it further decomposes ops into explicit type promotion and broadcasting ops: prims.convert_element_type and prims.broadcast_in_dim. This opset is designed to interface with compiler backends.", "Warning", "This opset is still under active development, more ops will be added in the future.", "Operator", "Schema", "prims.abs", "abs(Tensor self) -> Tensor", "prims.acos", "acos(Tensor self) -> Tensor", "prims.acosh", "acosh(Tensor self) -> Tensor", "prims.asin", "asin(Tensor self) -> Tensor", "prims.asinh", "asinh(Tensor self) -> Tensor", "prims.atan", "atan(Tensor self) -> Tensor", "prims.atanh", "atanh(Tensor self) -> Tensor", "prims.cos", "cos(Tensor self) -> Tensor", "prims.cosh", "cosh(Tensor self) -> Tensor", "prims.bessel_i0", "bessel_i0(Tensor self) -> Tensor", "prims.bessel_i0e", "bessel_i0e(Tensor self) -> Tensor", "prims.bessel_i1", "bessel_i1(Tensor self) -> Tensor", "prims.bessel_i1e", "bessel_i1e(Tensor self) -> Tensor", "prims.bessel_j0", "bessel_j0(Tensor self) -> Tensor", "prims.bessel_j1", "bessel_j1(Tensor self) -> Tensor", "prims.bitwise_not", "bitwise_not(Tensor self) -> Tensor", "prims.cbrt", "cbrt(Tensor self) -> Tensor", "prims.ceil", "ceil(Tensor self) -> Tensor", "prims.conj_physical", "conj_physical(Tensor self) -> Tensor", "prims.digamma", "digamma(Tensor self) -> Tensor", "prims.erf", "erf(Tensor self) -> Tensor", "prims.erf_inv", "erf_inv(Tensor self) -> Tensor", "prims.erfc", "erfc(Tensor self) -> Tensor", "prims.erfcx", "erfcx(Tensor self) -> Tensor", "prims.exp", "exp(Tensor self) -> Tensor", "prims.expm1", "expm1(Tensor self) -> Tensor", "prims.exp2", "exp2(Tensor self) -> Tensor", "prims.fill", "fill(Tensor self, Scalar value) -> Tensor", "prims.floor", "floor(Tensor self) -> Tensor", "prims.imag", "imag(Tensor self) -> Tensor", "prims.isfinite", "isfinite(Tensor self) -> Tensor", "prims.lgamma", "lgamma(Tensor self) -> Tensor", "prims.log", "log(Tensor self) -> Tensor", "prims.log1p", "log1p(Tensor self) -> Tensor", "prims.log2", "log2(Tensor self) -> Tensor", "prims.log10", "log10(Tensor self) -> Tensor", "prims.ndtri", "ndtri(Tensor self) -> Tensor", "prims.neg", "neg(Tensor self) -> Tensor", "prims.real", "real(Tensor self) -> Tensor", "prims.reciprocal", "reciprocal(Tensor self) -> Tensor", "prims.round", "round(Tensor self) -> Tensor", "prims.sign", "sign(Tensor self) -> Tensor", "prims.signbit", "signbit(Tensor self) -> Tensor", "prims.sin", "sin(Tensor self) -> Tensor", "prims.sinh", "sinh(Tensor self) -> Tensor", "prims.spherical_bessel_j0", "spherical_bessel_j0(Tensor self) -> Tensor", "prims.sqrt", "sqrt(Tensor self) -> Tensor", "prims.tan", "tan(Tensor self) -> Tensor", "prims.tanh", "tanh(Tensor self) -> Tensor", "prims.trunc", "trunc(Tensor self) -> Tensor", "prims.add", "add(Tensor self, Tensor other) -> Tensor", "prims.atan2", "atan2(Tensor self, Tensor other) -> Tensor", "prims.bitwise_and", "bitwise_and(Tensor self, Tensor other) -> Tensor", "prims.bitwise_or", "bitwise_or(Tensor self, Tensor other) -> Tensor", "prims.bitwise_xor", "bitwise_xor(Tensor self, Tensor other) -> Tensor", "prims.div", "div(Tensor self, Tensor other) -> Tensor", "prims.eq", "eq(Tensor self, Tensor other) -> Tensor", "prims.fmax", "fmax(Tensor self, Tensor other) -> Tensor", "prims.fmin", "fmin(Tensor self, Tensor other) -> Tensor", "prims.fmod", "fmod(Tensor self, Tensor other) -> Tensor", "prims.gcd", "gcd(Tensor self, Tensor other) -> Tensor", "prims.ge", "ge(Tensor self, Tensor other) -> Tensor", "prims.gt", "gt(Tensor self, Tensor other) -> Tensor", "prims.hypot", "hypot(Tensor self, Tensor other) -> Tensor", "prims.igamma", "igamma(Tensor self, Tensor other) -> Tensor", "prims.igammac", "igammac(Tensor self, Tensor other) -> Tensor", "prims.le", "le(Tensor self, Tensor other) -> Tensor", "prims.lt", "lt(Tensor self, Tensor other) -> Tensor", "prims.maximum", "maximum(Tensor self, Tensor other) -> Tensor", "prims.minimum", "minimum(Tensor self, Tensor other) -> Tensor", "prims.mul", "mul(Tensor self, Tensor other) -> Tensor", "prims.ne", "ne(Tensor self, Tensor other) -> Tensor", "prims.nextafter", "nextafter(Tensor self, Tensor other) -> Tensor", "prims.pow", "pow(Tensor self, Tensor other) -> Tensor", "prims.remainder", "remainder(Tensor self, Tensor other) -> Tensor", "prims.rsqrt", "rsqrt(Tensor self) -> Tensor", "prims.shift_left", "shift_left(Tensor self, Tensor other) -> Tensor", "prims.shift_right_arithmetic", "shift_right_arithmetic(Tensor self, Tensor other) -> Tensor", "prims.sub", "sub(Tensor self, Tensor other) -> Tensor", "prims.zeta", "zeta(Tensor self, Tensor other) -> Tensor", "prims.as_strided", "as_strided(Tensor(a!) a, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor(a!)", "prims.broadcast_in_dim", "broadcast_in_dim(Tensor(a) a, SymInt[] shape, int[] broadcast_dimensions) -> Tensor(a)", "prims.collapse_view", "collapse_view(Tensor(a) a, int start, int end) -> Tensor(a)", "prims.conj", "conj(Tensor(a) a) -> Tensor(a)", "prims.slice", "slice(Tensor(a) a, SymInt[] start_indices, SymInt[] limit_indices, SymInt[]? strides=None) -> Tensor(a)", "prims.slice_in_dim", "slice_in_dim(Tensor(a) a, SymInt start_index, SymInt limit_index, int stride=1, int axis=0) -> Tensor(a)", "prims.split_dim", "split_dim(Tensor(a) a, int dim, SymInt outer_length) -> Tensor(a)", "prims.squeeze", "squeeze(Tensor(a) a, int[] dimensions) -> Tensor(a)", "prims.transpose", "transpose(Tensor(a) a, int[] permutation) -> Tensor(a)", "prims.view_of", "view_of(Tensor(a) a) -> Tensor", "prims.as_strided_scatter", "as_strided_scatter(Tensor self, Tensor src, SymInt[] size, SymInt[] stride, SymInt storage_offset) -> Tensor", "prims.collapse", "collapse(Tensor a, int start, int end) -> Tensor", "prims.cat", "cat(Tensor[] tensors, int dim) -> Tensor", "prims.reshape", "reshape(Tensor a, SymInt[] shape) -> Tensor", "prims.rev", "rev(Tensor a, int[] dims) -> Tensor", "prims.where", "where(Tensor pred, Tensor a, Tensor b) -> Tensor", "prims.clone", "clone(Tensor self, *, MemoryFormat? memory_format=None) -> Tensor", "prims.convert_element_type", "convert_element_type(Tensor a, ScalarType dtype) -> Tensor", "prims.device_put", "device_put(Tensor a, Device device) -> Tensor", "prims.item", "item(Tensor a) -> Scalar", "prims.maximum_value", "maximum_value(ScalarType dtype) -> Scalar", "prims.minimum_value", "minimum_value(ScalarType dtype) -> Scalar", "prims.copy_strided", "copy_strided(Tensor a, SymInt[] stride) -> Tensor", "prims.copy_to", "copy_to(Tensor(a!) a, Tensor b) -> Tensor(a!)", "prims.resize", "resize(Tensor(a!) a, SymInt[] shape) -> Tensor(a!)", "prims.amax", "amax(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.amin", "amin(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.prod", "prod(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.sum", "sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.xor_sum", "xor_sum(Tensor inp, int[]? dims, *, ScalarType? output_dtype=None) -> Tensor", "prims.var", "var(Tensor inp, int[]? dims, *, float correction, ScalarType? output_dtype=None) -> Tensor", "prims.empty_strided", "empty_strided(SymInt[] shape, SymInt[] strides, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.empty_permuted", "empty_permuted(SymInt[] shape, int[] physical_layout, *, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.scalar_tensor", "scalar_tensor(Scalar s, *, ScalarType? dtype=None, Device? device=None) -> Tensor", "prims.iota", "iota(SymInt length, *, SymInt start, SymInt step, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.svd", "svd(Tensor A, *, bool full_matrices) -> (Tensor U, Tensor S, Tensor Vh)", "prims.normal", "normal(SymInt[] shape, *, Scalar mean, Scalar std, ScalarType dtype, Device device, bool requires_grad) -> Tensor", "prims.uniform", "uniform(SymInt[] shape, *, Scalar low, Scalar high, ScalarType dtype, Device device) -> Tensor", "prims.fft_r2c", "fft_r2c(Tensor self, *, int[] dim, bool onesided) -> Tensor", "prims.fft_c2c", "fft_c2c(Tensor self, *, int[] dim, bool forward) -> Tensor", "prims.fft_c2r", "fft_c2r(Tensor self, *, int[] dim, SymInt last_dim_size) -> Tensor"]}, {"name": "torch.export.load()", "path": "export#torch.export.load", "type": "Traced Graph Export", "text": ["Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Loads an ExportedProgram previously saved with torch.export.save.", "An ExportedProgram object", "ExportedProgram", "Example:"]}, {"name": "torch.export.ModuleCallEntry", "path": "export#torch.export.ModuleCallEntry", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.ModuleCallSignature", "path": "export#torch.export.ModuleCallSignature", "type": "Traced Graph Export", "text": []}, {"name": "torch.export.save()", "path": "export#torch.export.save", "type": "Traced Graph Export", "text": ["Warning", "Under active development, saved files may not be usable in newer versions of PyTorch.", "Saves an ExportedProgram to a file-like object. It can then be loaded using the Python API torch.export.load.", "Example:"]}, {"name": "torch.export.TorchDynamo Deep Dive", "path": "torch.compiler_deepdive", "type": "Traced Graph Export", "text": ["Before you read this section, read torch.compiler.", "TorchDynamo is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation API in CPython (PEP 523) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an FX Graph which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds \u2014 usability and performance.", "TorchDynamo makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator torch._dynamo.optimize() which is wrapped for convenience by torch.compile()", "The following diagram demonstrates how PyTorch works with torch.compile and without it:", "TorchInductor is one of the backends supported by TorchDynamo Graph into Triton for GPUs or C++/OpenMP for CPUs. We have a training performance dashboard that provides performance comparison for different training backends. You can read more in the TorchInductor post on PyTorch dev-discuss.", "For an in-depth overview, read the sections below, watch the deep-dive video, and check out the dev-discuss topics.", "Author: Jason Ansel and Kaichao You", "This section will go over some of the TorchDynamo internals and will demonstrate how TorchDynamo works under the hood.", "TorchDynamo operates just-in-time and specializes graphs based on dynamic properties. Below is a basic example of how to use TorchDynamo. One can decorate a function or a method using torchdynamo.optimize to enable TorchDynamo optimization:", "For example, the first graph above has the following guards:", "If any of those guards fail, the graph will be recaptured and recompiled. The interesting guard type there is TENSOR_MATCH, which checks the following torch.Tensor properties:", "The full specialization mode allows the backend compiler to assume an entirely static graph. Unfortunately, most backends require this. Operators which return dynamic shapes will trigger a graph break when not in dynamic shape mode.", "If you want to understand better what TorchDynamo is doing, you can set:", "This code triggers useful (but spammy) printouts.", "For example, the printouts for the first graph in the toy_example are:", "At the top you can see the FX graph. Next, you see the original bytecode of the function, followed by the modified bytecode generated by TorchDynamo. Finally, you see the guards which we covered above.", "In the modified bytecode, __compiled_fn_0 is the return value of my_compiler() (the compiled graph). __resume_at_30_1 and __resume_at_38_2 are both generated continuation functions that pick up execution after a graph break (at bytecode offsets 30 and 38). Each of these functions take the form:", "By generating this resume_at function, we force the remainder of the function to be executed in a new Python frame which recursively triggers TorchDynamo to restart its capture once execution reaches that point for the first time.", "To inspect the artifacts generated by TorchDynamo, there is an API torch._dynamo.eval_frame._debug_get_cache_entry_list that retrieves compiled code and guards out of a function\u2019s __code__ object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and a types.CodeType object to keep the code to be executed if the guarding conditions are satisfied.", "The compiled bytecode, printed by dis.dis(code), will call the result of the backend compiler function which is stored inside a global variable such as __compiled_fn_0 in the module containing the original function.", "The generated bytecodes are roughly equivalent to the following Python (converted manually for illustration purposes).", "Note that we pass a simple my_compiler function as the backend compiler, therefore the subgraph code __resume_at_38_2, __resume_at_30_1, and __compiled_fn_0._torchdynamo_orig_callable remain python code. However, if we use other backends like the built-in inductor, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU."]}, {"name": "torch.export.Writing Graph Transformations on ATen IR", "path": "torch.compiler_transformations", "type": "Traced Graph Export", "text": ["Since the ATen IR sits at the FX Graph/GraphModule level, any transformations written for FX Graphs can be easily applied onto the ATen IR. If you\u2019re familiar with writing FX graph transformations, then this will be the same.", "The most direct way of writing transformations is by looping through the given graph and directly manipulating the nodes within the graph.", "For example, let\u2019s say we want to replace torch.ops.aten.add.Tensor() calls with torch.ops.aten.mul.Tensor() calls:", "We can also delete and append new nodes through FX utility functions that can be found in the Graph documentation. For example, if we want to insert a torch.ops.aten.relu.default() after the add call:", "In general, transformations can be roughly categorized into a couple of axis:", "Axis A: 1. Creating one-to-X mapping (eg. decomposition) 2. Creating many-to-one mapping (eg. fusion)", "Axis B: 1. Doing forwards iteration (eg. shape propagation) 2. Doing backwards iteration (eg. dead code elimination)", "Axis C: 1. Dependent on local node information (eg. out-variant conversion) 2. Dependent on global graph information (eg. memory planning)", "Our projection on the frequency of these use cases are: 1. A.1, B.1, C.1 2. A.2 3. B.2, C.2", "Although we can make all graph transformations through directly manipulating the graph, we also provide some helper utilities for some ease of use for the level 1 and 2 use-cases.", "For level 1 uses cases (creating one-to-X mappings, doing forwards iterations, and looking at local node information), we can utilize the Transformer class to execute each node and recreate a graph, except with the transformations specified.", "An example for one-to-one mappings, if we wanted to replace an op A with another op B, we can run the GraphModule, and very time we see op A, return op B.", "An example is:", "The super().call_function(target, args, kwargs, meta) call creates a call_function FX node, and returns the result of running the operator with the given arguments.", "If we wanted to do one-to-X mappings, like replacing op A with 2 other ops B and C, we would then make 2 calls to super().call_function to create 2 FX nodes, one with op B and another with op C, and return the result of running op C.", "For example:", "If we wanted to remove an op, we can just return the value passed into the function:", "An example of utilizing local node information is, if we wanted to convert all the scalars within the graph to tensors, we can run the given fx.GraphModule, and for every argument that contains a scalar, we convert it to a tensor. It might look something like:", "For creating many-to-one mappings, we can utilize FX\u2019s subgraph rewriter. Given a pattern, it creates a subgraph of operators matching to the pattern, and then replaces each matched subgraph with the replacement.", "Note:", "The pattern and replacement inputs must be callable functions or GraphModules containing the same operators that are used within the graph (ATen ops) so that the subgraph rewriter can find the correct pattern in the graph. Inputs to the pattern/replacement callables will be treated as wildcards when matching.", "An example:", "The subgraph rewriter returns a list of ReplacedPatterns:", "Note:", "The `PassManager <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/pass_manager.py>`__ is a class used to run multiple passes on a given graph module. When initializing a PassManager instance, we pass in a list of passes that we want to run and set a couple of flags. To run the collection of passes on a graph module, we can pass the graph module directly to the PassManager instance.", "An example:", "To add a common set of checks that are run after each pass, we can call the function set_checks(check: Callable) which takes in a callable function as input. If the run_checks_after_each_pass flag is set, the check will be called after each pass is run on the graph module.", "An example:", "There are a couple of common FX graph based partitioners we can use to partition the graph.", "For finding subgraphs within a graph that match a specific pattern, we can utilize FX\u2019s `SubgraphMatcher <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/utils/matcher_utils.py>`__.", "Class Attributes:", "An example:", "The match function returns a list of InternalMatch:", "To find the largest subgraphs of nodes that support a specific invariant, we can utilize FX\u2019s `CapabilityBasedPartitioner <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/infra/partitioner.py#L34>`__.", "Class Attributes", "The `OperatorSupportBase <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#LL28C1-L28C1>`__ class is used by the partitioner to determine if a specific node in the graph belongs in the partition. This is done by overriding the is_node_supported function. You can chain multiple OperatorSuppportBase by using `chain <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L150>`__(which returns False if any of the OperatorSupportBase return False) and `any_chain <https://github.com/pytorch/pytorch/blob/main/torch/fx/passes/operator_support.py#L164>`__ (which returns True if any of the OperatorSupportBase returns True).", "An example:"]}, {"name": "torch.eye", "path": "generated/torch.eye", "type": "Torch", "text": ["Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.", "A 2-D tensor with ones on the diagonal and zeros elsewhere", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_channel_affine", "path": "generated/torch.fake_quantize_per_channel_affine", "type": "Torch", "text": ["Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.", "A newly fake_quantized per channel torch.float32 tensor", "Tensor", "Example:"]}, {"name": "torch.fake_quantize_per_tensor_affine", "path": "generated/torch.fake_quantize_per_tensor_affine", "type": "Torch", "text": ["Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.", "A newly fake_quantized torch.float32 tensor", "Tensor", "Example:"]}, {"name": "torch.fft", "path": "fft", "type": "Discrete Fourier Transforms", "text": ["Discrete Fourier transforms and related functions.", "Computes the one dimensional discrete Fourier transform of input.", "Computes the one dimensional inverse discrete Fourier transform of input.", "Computes the 2 dimensional discrete Fourier transform of input.", "Computes the 2 dimensional inverse discrete Fourier transform of input.", "Computes the N dimensional discrete Fourier transform of input.", "Computes the N dimensional inverse discrete Fourier transform of input.", "Computes the one dimensional Fourier transform of real-valued input.", "Computes the inverse of rfft().", "Computes the 2-dimensional discrete Fourier transform of real input.", "Computes the inverse of rfft2().", "Computes the N-dimensional discrete Fourier transform of real input.", "Computes the inverse of rfftn().", "Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the inverse of hfft().", "Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the 2-dimensional inverse discrete Fourier transform of real input.", "Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Computes the N-dimensional inverse discrete Fourier transform of real input.", "Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Computes the sample frequencies for rfft() with a signal of size n.", "Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "Inverse of fftshift()."]}, {"name": "torch.fft.fft()", "path": "generated/torch.fft.fft#torch.fft.fft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.fft2()", "path": "generated/torch.fft.fft2#torch.fft.fft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftfreq()", "path": "generated/torch.fft.fftfreq#torch.fft.fftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:"]}, {"name": "torch.fft.fftn()", "path": "generated/torch.fft.fftn#torch.fft.fftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.fftshift()", "path": "generated/torch.fft.fftshift#torch.fft.fftshift", "type": "Discrete Fourier Transforms", "text": ["Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:"]}, {"name": "torch.fft.hfft()", "path": "generated/torch.fft.hfft#torch.fft.hfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:"]}, {"name": "torch.fft.hfft2()", "path": "generated/torch.fft.hfft2#torch.fft.hfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal. Equivalent to hfftn() but only transforms the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfft2(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.hfftn()", "path": "generated/torch.fft.hfftn#torch.fft.hfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "hfftn()/ihfftn() are analogous to rfftn()/irfftn(). The real FFT expects a real signal in the time-domain and gives Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the shape argument s, in the same way as with irfftn().", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. It is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfftn(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.ifft()", "path": "generated/torch.fft.ifft#torch.fft.ifft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.ifft2()", "path": "generated/torch.fft.ifft2#torch.fft.ifft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftn()", "path": "generated/torch.fft.ifftn#torch.fft.ifftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.ifftshift()", "path": "generated/torch.fft.ifftshift#torch.fft.ifftshift", "type": "Discrete Fourier Transforms", "text": ["Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.ihfft()", "path": "generated/torch.fft.ihfft#torch.fft.ihfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from ifft():"]}, {"name": "torch.fft.ihfft2()", "path": "generated/torch.fft.ihfft2#torch.fft.ihfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional inverse discrete Fourier transform of real input. Equivalent to ihfftn() but transforms only the two last dimensions by default.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifft2(), the Hermitian time-space signal takes up only half the space.", "The discrete Fourier transform is separable, so ihfft2() here is equivalent to a combination of ifft() and ihfft():"]}, {"name": "torch.fft.ihfftn()", "path": "generated/torch.fft.ihfftn#torch.fft.ihfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional inverse discrete Fourier transform of real input.", "input must be a real-valued signal, interpreted in the Fourier domain. The n-dimensional IFFT of a real signal is Hermitian-symmetric, X[i, j, ...] = conj(X[-i, -j, ...]). ihfftn() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included in the last signal dimension. To compute the full output, use ifftn().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so ihfftn() here is equivalent to a combination of ihfft() and ifft():"]}, {"name": "torch.fft.irfft()", "path": "generated/torch.fft.irfft#torch.fft.irfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:"]}, {"name": "torch.fft.irfft2()", "path": "generated/torch.fft.irfft2#torch.fft.irfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.irfftn()", "path": "generated/torch.fft.irfftn#torch.fft.irfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.rfft()", "path": "generated/torch.fft.rfft#torch.fft.rfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued."]}, {"name": "torch.fft.rfft2()", "path": "generated/torch.fft.rfft2#torch.fft.rfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.rfftfreq()", "path": "generated/torch.fft.rfftfreq#torch.fft.rfftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])"]}, {"name": "torch.fft.rfftn()", "path": "generated/torch.fft.rfftn#torch.fft.rfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.torch.fft.fft", "path": "generated/torch.fft.fft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft()), these correspond to:", "Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.torch.fft.fft2", "path": "generated/torch.fft.fft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.torch.fft.fftfreq", "path": "generated/torch.fft.fftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the discrete Fourier Transform sample frequencies for a signal of size n.", "Note", "By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.", "For even input, we can see the Nyquist frequency at f[2] is given as negative:"]}, {"name": "torch.fft.torch.fft.fftn", "path": "generated/torch.fft.fftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional discrete Fourier transform of input.", "Note", "The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (fftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls:"]}, {"name": "torch.fft.torch.fft.fftshift", "path": "generated/torch.fft.fftshift", "type": "Discrete Fourier Transforms", "text": ["Reorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first.", "This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.", "Note", "By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2 in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().", "Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor.", "This also works for multi-dimensional transforms:", "fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift().", "Similarly, we can convert the frequency domain components to centered convention by applying fftshift().", "The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order:"]}, {"name": "torch.fft.torch.fft.hfft", "path": "generated/torch.fft.hfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "Note", "hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().", "Note", "Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft()), these correspond to:", "Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output:", "Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies:", "Like with irfft(), the output length must be given in order to recover an even length output:"]}, {"name": "torch.fft.torch.fft.hfft2", "path": "generated/torch.fft.hfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of a Hermitian symmetric input signal. Equivalent to hfftn() but only transforms the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfft2(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.hfftn", "path": "generated/torch.fft.hfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the n-dimensional discrete Fourier transform of a Hermitian symmetric input signal.", "input is interpreted as a one-sided Hermitian signal in the time domain. By the Hermitian property, the Fourier transform will be real-valued.", "Note", "hfftn()/ihfftn() are analogous to rfftn()/irfftn(). The real FFT expects a real signal in the time-domain and gives Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the shape argument s, in the same way as with irfftn().", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. It is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (hfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (ihfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Starting from a real frequency-space signal, we can generate a Hermitian-symmetric time-domain signal: >>> T = torch.rand(10, 9) >>> t = torch.fft.ihfftn(T)", "Without specifying the output length to hfftn(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.ifft", "path": "generated/torch.fft.ifft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft()), these correspond to:", "Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor."]}, {"name": "torch.fft.torch.fft.ifft2", "path": "generated/torch.fft.ifft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.torch.fft.ifftn", "path": "generated/torch.fft.ifftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N dimensional inverse discrete Fourier transform of input.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ifftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls:"]}, {"name": "torch.fft.torch.fft.ifftshift", "path": "generated/torch.fft.ifftshift", "type": "Discrete Fourier Transforms", "text": ["Inverse of fftshift().", "A round-trip through fftshift() and ifftshift() gives the same result:"]}, {"name": "torch.fft.torch.fft.ihfft", "path": "generated/torch.fft.ihfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of hfft().", "input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft()), these correspond to:", "Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from ifft():"]}, {"name": "torch.fft.torch.fft.ihfft2", "path": "generated/torch.fft.ihfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional inverse discrete Fourier transform of real input. Equivalent to ihfftn() but transforms only the two last dimensions by default.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifft2(), the Hermitian time-space signal takes up only half the space.", "The discrete Fourier transform is separable, so ihfft2() here is equivalent to a combination of ifft() and ihfft():"]}, {"name": "torch.fft.torch.fft.ihfftn", "path": "generated/torch.fft.ihfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional inverse discrete Fourier transform of real input.", "input must be a real-valued signal, interpreted in the Fourier domain. The n-dimensional IFFT of a real signal is Hermitian-symmetric, X[i, j, ...] = conj(X[-i, -j, ...]). ihfftn() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included in the last signal dimension. To compute the full output, use ifftn().", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (ihfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (hfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from ifftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so ihfftn() here is equivalent to a combination of ihfft() and ifft():"]}, {"name": "torch.fft.torch.fft.irfft", "path": "generated/torch.fft.irfft", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension. With default arguments, size of the transformed dimension should be (2^n + 1) as argument n defaults to even output size = 2 * (transformed_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft()), these correspond to:", "Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length:", "So, it is recommended to always pass the signal length n:"]}, {"name": "torch.fft.torch.fft.irfft2", "path": "generated/torch.fft.irfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default.", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfft2()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.irfftn", "path": "generated/torch.fft.irfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the inverse of rfftn().", "input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.", "Note", "Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.", "Note", "The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.", "Note", "Supports torch.half and torch.chalf on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions. With default arguments, the size of last dimension should be (2^n + 1) as argument s defaults to even output size = 2 * (last_dim_size - 1)", "norm (str, optional) \u2013 ", "Normalization mode. For the backward transform (irfftn()), these correspond to:", "Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (normalize by 1/n).", "out (Tensor, optional) \u2013 the output tensor.", "Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension:", "So, it is recommended to always pass the signal shape s."]}, {"name": "torch.fft.torch.fft.rfft", "path": "generated/torch.fft.rfft", "type": "Discrete Fourier Transforms", "text": ["Computes the one dimensional Fourier transform of real-valued input.", "The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimension.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft()), these correspond to:", "Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compare against the full output from fft():", "Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued."]}, {"name": "torch.fft.torch.fft.rfft2", "path": "generated/torch.fft.rfft2", "type": "Discrete Fourier Transforms", "text": ["Computes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default.", "The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfft2()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fft2(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fft.torch.fft.rfftfreq", "path": "generated/torch.fft.rfftfreq", "type": "Discrete Fourier Transforms", "text": ["Computes the sample frequencies for rfft() with a signal of size n.", "Note", "rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are:", "Note", "For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.", "Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500])"]}, {"name": "torch.fft.torch.fft.rfftn", "path": "generated/torch.fft.rfftn", "type": "Discrete Fourier Transforms", "text": ["Computes the N-dimensional discrete Fourier transform of real input.", "The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.", "Note", "Supports torch.half on CUDA with GPU Architecture SM53 or greater. However it only supports powers of 2 signal length in every transformed dimensions.", "norm (str, optional) \u2013 ", "Normalization mode. For the forward transform (rfftn()), these correspond to:", "Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse.", "Default is \"backward\" (no normalization).", "out (Tensor, optional) \u2013 the output tensor.", "Compared against the full output from fftn(), we have all elements up to the Nyquist frequency.", "The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft():"]}, {"name": "torch.fix", "path": "generated/torch.fix", "type": "Torch", "text": ["Alias for torch.trunc()"]}, {"name": "torch.flatten", "path": "generated/torch.flatten", "type": "Torch", "text": ["Flattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged.", "Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned.", "Note", "Flattening a zero-dimensional tensor will return a one-dimensional view.", "Example:"]}, {"name": "torch.flip", "path": "generated/torch.flip", "type": "Torch", "text": ["Reverse the order of an n-D tensor along given axis in dims.", "Note", "torch.flip makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flip, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flip is expected to be slower than np.flip.", "Example:"]}, {"name": "torch.fliplr", "path": "generated/torch.fliplr", "type": "Torch", "text": ["Flip tensor in the left/right direction, returning a new tensor.", "Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 2-D.", "Note", "torch.fliplr makes a copy of input\u2019s data. This is different from NumPy\u2019s np.fliplr, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.fliplr is expected to be slower than np.fliplr.", "input (Tensor) \u2013 Must be at least 2-dimensional.", "Example:"]}, {"name": "torch.flipud", "path": "generated/torch.flipud", "type": "Torch", "text": ["Flip tensor in the up/down direction, returning a new tensor.", "Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.", "Note", "Requires the tensor to be at least 1-D.", "Note", "torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.", "input (Tensor) \u2013 Must be at least 1-dimensional.", "Example:"]}, {"name": "torch.float_power", "path": "generated/torch.float_power", "type": "Torch", "text": ["Raises input to the power of exponent, elementwise, in double precision. If neither input is complex returns a torch.float64 tensor, and if one or more inputs is complex returns a torch.complex128 tensor.", "Note", "This function always computes in double precision, unlike torch.pow(), which implements more typical type promotion. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "Storage", "text": []}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "Storage", "text": []}, {"name": "torch.floor", "path": "generated/torch.floor", "type": "Torch", "text": ["Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.", "For integer inputs, follows the array-api convention of returning a copy of the input tensor.", "input (Tensor) \u2013 the input tensor.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.floor_divide", "path": "generated/torch.floor_divide", "type": "Torch", "text": ["Note", "Before PyTorch 1.13 torch.floor_divide() incorrectly performed truncation division. To restore the previous behavior use torch.div() with rounding_mode='trunc'.", "Computes input divided by other, elementwise, and floors the result.", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmax", "path": "generated/torch.fmax", "type": "Torch", "text": ["Computes the element-wise maximum of input and other.", "This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmin", "path": "generated/torch.fmin", "type": "Torch", "text": ["Computes the element-wise minimum of input and other.", "This is like torch.minimum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. Only if both elements are NaN is NaN propagated.", "This function is a wrapper around C++\u2019s std::fmin and is similar to NumPy\u2019s fmin function.", "Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.fmod", "path": "generated/torch.fmod", "type": "Torch", "text": ["Applies C++\u2019s std::fmod entrywise. The result has the same sign as the dividend input and its absolute value is less than that of other.", "This function may be defined in terms of torch.div() as", "Supports broadcasting to a common shape, type promotion, and integer and float inputs.", "Note", "When the divisor is zero, returns NaN for floating point dtypes on both CPU and GPU; raises RuntimeError for integer division by zero on CPU; Integer division by zero on GPU may return any value.", "Note", "Complex inputs are not supported. In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers.", "See also", "torch.remainder() which implements Python\u2019s modulus operator. This one is defined using division rounding down the result.", "out (Tensor, optional) \u2013 the output tensor.", "Example:"]}, {"name": "torch.frac", "path": "generated/torch.frac", "type": "Torch", "text": ["Computes the fractional portion of each element in input.", "Example:"]}, {"name": "torch.frexp", "path": "generated/torch.frexp", "type": "Torch", "text": ["Decomposes input into mantissa and exponent tensors such that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}.", "The range of mantissa is the open interval (-1, 1).", "Supports float inputs.", "input (Tensor) \u2013 the input tensor", "out (tuple, optional) \u2013 the output tensors", "Example:"]}, {"name": "torch.from_dlpack", "path": "generated/torch.from_dlpack", "type": "Torch", "text": ["Converts a tensor from an external library into a torch.Tensor.", "The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.", "ext_tensor (object with __dlpack__ attribute, or a DLPack capsule) \u2013 ", "The tensor or DLPack capsule to convert.", "If ext_tensor is a tensor (or ndarray) object, it must support the __dlpack__ protocol (i.e., have a ext_tensor.__dlpack__ method). Otherwise ext_tensor may be a DLPack capsule, which is an opaque PyCapsule instance, typically produced by a to_dlpack function or method.", "Tensor", "Examples:"]}, {"name": "torch.from_numpy", "path": "generated/torch.from_numpy", "type": "Torch", "text": ["Creates a Tensor from a numpy.ndarray.", "The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable.", "It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.complex64, numpy.complex128, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool.", "Warning", "Writing to a tensor created from a read-only NumPy array is not supported and will result in undefined behavior.", "Example:"]}, {"name": "torch.frombuffer", "path": "generated/torch.frombuffer", "type": "Torch", "text": ["Creates a 1-dimensional Tensor from an object that implements the Python buffer protocol.", "Skips the first offset bytes in the buffer, and interprets the rest of the raw bytes as a 1-dimensional tensor of type dtype with count elements.", "Note that either of the following must be true:", "1. count is a positive non-zero number, and the total number of bytes in the buffer is less than offset plus count times the size (in bytes) of dtype.", "2. count is negative, and the length (number of bytes) of the buffer subtracted by the offset is a multiple of the size (in bytes) of dtype.", "The returned tensor and buffer share the same memory. Modifications to the tensor will be reflected in the buffer and vice versa. The returned tensor is not resizable.", "Note", "This function increments the reference count for the object that owns the shared memory. Therefore, such memory will not be deallocated before the returned tensor goes out of scope.", "Warning", "This function\u2019s behavior is undefined when passed an object implementing the buffer protocol whose data is not on the CPU. Doing so is likely to cause a segmentation fault.", "Warning", "This function does not try to infer the dtype (hence, it is not optional). Passing a different dtype than its source may result in unexpected behavior.", "buffer (object) \u2013 a Python object that exposes the buffer interface.", "Example:"]}, {"name": "torch.full", "path": "generated/torch.full", "type": "Torch", "text": ["Creates a tensor of size size filled with fill_value. The tensor\u2019s dtype is inferred from fill_value.", "Example:"]}, {"name": "torch.full_like", "path": "generated/torch.full_like", "type": "Torch", "text": ["Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device)."]}, {"name": "torch.func", "path": "func", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as \u201cfunctorch\u201d, is JAX-like composable function transforms for PyTorch.", "Note", "This library is currently in beta. What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don\u2019t have full coverage over PyTorch operations.", "If you have suggestions on the API or use-cases you\u2019d like to be covered, please open an GitHub issue or reach out. We\u2019d love to hear about how you\u2019re using the library.", "There are a number of use cases that are tricky to do in PyTorch today:", "Composing vmap(), grad(), and vjp() transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the JAX framework."]}, {"name": "torch.func.functional_call()", "path": "generated/torch.func.functional_call#torch.func.functional_call", "type": "JAX-like Function Transforms", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Note", "If the module has active parametrizations, passing a value in the parameter_and_buffer_dicts argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameter_and_buffer_dicts input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "An example of passing mutliple dictionaries", "And here is an example of applying the grad transform over the parameters of a model.", "Note", "If the user does not need grad tracking outside of grad transforms, they can detach all of the parameters for better performance and memory usage", "Example:", "This means that the user cannot call grad_weight.backward(). However, if they don\u2019t need autograd tracking outside of the transforms, this will result in less memory usage and faster speeds.", "the result of calling module.", "Any"]}, {"name": "torch.func.functionalize()", "path": "generated/torch.func.functionalize#torch.func.functionalize", "type": "JAX-like Function Transforms", "text": ["functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function\u2019s semantics.", "functionalize(func) returns a new function with the same semantics as func, but with all intermediate mutations removed. Every inplace operation performed on an intermediate tensor: intermediate.foo_() gets replaced by its out-of-place equivalent: intermediate_updated = intermediate.foo().", "functionalize is useful for shipping a pytorch program off to backends or compilers that aren\u2019t able to easily represent mutations or aliasing operators.", "Returns a new \u201cfunctionalized\u201d function. It takes the same inputs as func, and has the same behavior, but any mutations (and optionally aliasing) performed on intermeidate tensors in the function will be removed.", "Callable", "functionalize will also remove mutations (and views) that were performed on function inputs. However to preserve semantics, functionalize will \u201cfix up\u201d the mutations after the transform has finished running, by detecting if any tensor inputs \u201cshould have\u201d been mutated, and copying the new data back to the inputs if necessary.", "Example:", "Finally, a helpful mental model for understanding functionalization is that most user pytorch programs are writting with the public torch API. When executed, torch operators are generally decomposed into our internal C++ \u201cATen\u201d API. The logic for functionalization happens entirely at the level of ATen. Functionalization knows how to take every aliasing operator in ATen, and map it to its non-aliasing equivalent (e.g. tensor.view({-1}) -> at::view_copy(tensor, {-1})), and how to take every mutating operator in ATen, and map it to its non-mutating equivalent (e.g. tensor.add_(1) -> at::add(tensor, -1)), while tracking aliases and mutations out-of-line to know when to fix things up. Information about which ATen operators are aliasing or mutating all comes from https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml."]}, {"name": "torch.func.grad()", "path": "generated/torch.func.grad#torch.func.grad", "type": "JAX-like Function Transforms", "text": ["grad operator helps computing gradients of func with respect to the input(s) specified by argnums. This operator can be nested to compute higher-order gradients.", "Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified has_aux equals True, tuple of gradients and output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of output gradients with respect to each argnums value is returned.", "Callable", "Example of using grad:", "When composed with vmap, grad can be used to compute per-sample-gradients:", "Example of using grad with has_aux and argnums:", "Note", "Using PyTorch torch.no_grad together with grad.", "Case 1: Using torch.no_grad inside a function:", "In this case, grad(f)(x) will respect the inner torch.no_grad.", "Case 2: Using grad inside torch.no_grad context manager:", "In this case, grad will respect the inner torch.no_grad, but not the outer one. This is because grad is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.grad_and_value()", "path": "generated/torch.func.grad_and_value#torch.func.grad_and_value", "type": "JAX-like Function Transforms", "text": ["Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Function to compute a tuple of gradients with respect to its inputs and the forward computation. By default, the output of the function is a tuple of the gradient tensor(s) with respect to the first argument and the primal computation. If specified has_aux equals True, tuple of gradients and tuple of the forward computation with output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of a tuple of the output gradients with respect to each argnums value and the forward computation is returned.", "Callable", "See grad() for examples"]}, {"name": "torch.func.hessian()", "path": "generated/torch.func.hessian#torch.func.hessian", "type": "JAX-like Function Transforms", "text": ["Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "The forward-over-reverse strategy (composing jacfwd(jacrev(func))) is a good default for good performance. It is possible to compute Hessians through other compositions of jacfwd() and jacrev() like jacfwd(jacfwd(func)) or jacrev(jacrev(func)).", "Returns a function that takes in the same inputs as func and returns the Hessian of func with respect to the arg(s) at argnums.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(jacrev(func)), which has better operator coverage.", "A basic usage with a R^N -> R^1 function gives a N x N Hessian:"]}, {"name": "torch.func.jacfwd()", "path": "generated/torch.func.jacfwd#torch.func.jacfwd", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(), which has better operator coverage.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "jacfwd() can be composed with vmap to produce batched Jacobians:", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "Additionally, jacrev() can be composed with itself or jacrev() to produce Hessians", "By default, jacfwd() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments"]}, {"name": "torch.func.jacrev()", "path": "generated/torch.func.jacrev#torch.func.jacrev", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Note", "Using chunk_size=1 is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of vmap() are not applicable.", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "jacrev() can be composed with vmap to produce batched Jacobians:", "Additionally, jacrev() can be composed with itself to produce Hessians", "By default, jacrev() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments", "Note", "Using PyTorch torch.no_grad together with jacrev. Case 1: Using torch.no_grad inside a function:", "In this case, jacrev(f)(x) will respect the inner torch.no_grad.", "Case 2: Using jacrev inside torch.no_grad context manager:", "In this case, jacrev will respect the inner torch.no_grad, but not the outer one. This is because jacrev is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.jvp()", "path": "generated/torch.func.jvp#torch.func.jvp", "type": "JAX-like Function Transforms", "text": ["Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \u201cJacobian of func evaluated at primals\u201d times tangents. This is also known as forward-mode autodiff.", "Returns a (output, jvp_out) tuple containing the output of func evaluated at primals and the Jacobian-vector product. If has_aux is True, then instead returns a (output, jvp_out, aux) tuple.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it.", "jvp is useful when you wish to compute gradients of a function R^1 -> R^N", "jvp() can support functions with multiple inputs by passing in the tangents for each of the inputs"]}, {"name": "torch.func.linearize()", "path": "generated/torch.func.linearize#torch.func.linearize", "type": "JAX-like Function Transforms", "text": ["Returns the value of func at primals and linear approximation at primals.", "Returns a (output, jvp_fn) tuple containing the output of func applied to primals and a function that computes the jvp of func evaluated at primals.", "Tuple[Any, Callable]", "linearize is useful if jvp is to be computed multiple times at primals. However, to achieve this, linearize saves intermediate computation and has higher memory requrements than directly applying jvp. So, if all the tangents are known, it maybe more efficient to compute vmap(jvp) instead of using linearize.", "Note", "linearize evaluates func twice. Please file an issue for an implementation with a single evaluation."]}, {"name": "torch.func.Migrating from functorch to torch.func", "path": "func.migrating", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as \u201cfunctorch\u201d, is JAX-like composable function transforms for PyTorch.", "functorch started as an out-of-tree library over at the pytorch/functorch repository. Our goal has always been to upstream functorch directly into PyTorch and provide it as a core PyTorch library.", "As the final step of the upstream, we\u2019ve decided to migrate from being a top level package (functorch) to being a part of PyTorch to reflect how the function transforms are integrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating import functorch and ask that users migrate to the newest APIs, which we will maintain going forward. import functorch will be kept around to maintain backwards compatibility for a couple of releases.", "The following APIs are a drop-in replacement for the following functorch APIs. They are fully backwards compatible.", "functorch API", "PyTorch API (as of PyTorch 2.0)", "functorch.vmap", "torch.vmap() or torch.func.vmap()", "functorch.grad", "torch.func.grad()", "functorch.vjp", "torch.func.vjp()", "functorch.jvp", "torch.func.jvp()", "functorch.jacrev", "torch.func.jacrev()", "functorch.jacfwd", "torch.func.jacfwd()", "functorch.hessian", "torch.func.hessian()", "functorch.functionalize", "torch.func.functionalize()", "Furthermore, if you are using torch.autograd.functional APIs, please try out the torch.func equivalents instead. torch.func function transforms are more composable and more performant in many cases.", "torch.autograd.functional API", "torch.func API (as of PyTorch 2.0)", "torch.autograd.functional.vjp()", "torch.func.grad() or torch.func.vjp()", "torch.autograd.functional.jvp()", "torch.func.jvp()", "torch.autograd.functional.jacobian()", "torch.func.jacrev() or torch.func.jacfwd()", "torch.autograd.functional.hessian()", "torch.func.hessian()", "We\u2019ve changed the APIs to apply function transforms over NN modules to make them fit better into the PyTorch design philosophy. The new API is different, so please read this section carefully.", "torch.func.functional_call() is the replacement for functorch.make_functional and functorch.make_functional_with_buffers. However, it is not a drop-in replacement.", "If you\u2019re in a hurry, you can use helper functions in this gist that emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers. We recommend using torch.func.functional_call() directly because it is a more explicit and flexible API.", "Concretely, functorch.make_functional returns a functional module and parameters. The functional module accepts parameters and inputs to the model as arguments. torch.func.functional_call() allows one to call the forward pass of an existing module using new parameters and buffers and inputs.", "Here\u2019s an example of how to compute gradients of parameters of a model using functorch vs torch.func:", "And here\u2019s an example of how to compute jacobians of model parameters:", "Note that it is important for memory consumption that you should only carry around a single copy of your parameters. model.named_parameters() does not copy the parameters. If in your model training you update the parameters of the model in-place, then the nn.Module that is your model has the single copy of the parameters and everything is OK.", "However, if you want to carry your parameters around in a dictionary and update them out-of-place, then there are two copies of parameters: the one in the dictionary and the one in the model. In this case, you should change model to not hold memory by converting it to the meta device via model.to('meta').", "Please use torch.func.stack_module_state() instead of functorch.combine_state_for_ensemble torch.func.stack_module_state() returns two dictionaries, one of stacked parameters, and one of stacked buffers, that can then be used with torch.vmap() and torch.func.functional_call() for ensembling.", "For example, here is an example of how to ensemble over a very simple model:", "We are no longer supporting functorch.compile (also known as AOTAutograd) as a frontend for compilation in PyTorch; we have integrated AOTAutograd into PyTorch\u2019s compilation story. If you are a user, please use torch.compile() instead."]}, {"name": "torch.func.replace_all_batch_norm_modules_()", "path": "generated/torch.func.replace_all_batch_norm_modules_#torch.func.replace_all_batch_norm_modules_", "type": "JAX-like Function Transforms", "text": ["In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "Module"]}, {"name": "torch.func.stack_module_state()", "path": "generated/torch.func.stack_module_state#torch.func.stack_module_state", "type": "JAX-like Function Transforms", "text": ["Prepares a list of torch.nn.Modules for ensembling with vmap().", "Given a list of M nn.Modules of the same class, returns two dictionaries that stack all of their parameters and buffers together, indexed by name. The stacked parameters are optimizable (i.e. they are new leaf nodes in the autograd history that are unrelated to the original parameters and can be passed directly to an optimizer).", "Here\u2019s an example of how to ensemble over a very simple model:", "When there\u2019s submodules, this follows state dict naming conventions", "Warning", "All of the modules being stacked together must be the same (except for the values of their parameters/buffers). For example, they should be in the same mode (training vs eval).", "Tuple[Dict[str, Any], Dict[str, Any]]"]}, {"name": "torch.func.torch.func API Reference", "path": "func.api", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs.", "grad operator helps computing gradients of func with respect to the input(s) specified by argnums.", "Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \"Jacobian of func evaluated at primals\" times tangents.", "Returns the value of func at primals and linear approximation at primals.", "Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function's semantics.", "In general, you can transform over a function that calls a torch.nn.Module. For example, the following is an example of computing a jacobian of a function that takes three values and returns three values:", "However, if you want to do something like compute a jacobian over the parameters of the model, then there needs to be a way to construct a function where the parameters are the inputs to the function. That\u2019s what functional_call() is for: it accepts an nn.Module, the transformed parameters, and the inputs to the Module\u2019s forward pass. It returns the value of running the Module\u2019s forward pass with the replaced parameters.", "Here\u2019s how we would compute the Jacobian over the parameters", "Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Prepares a list of torch.nn.Modules for ensembling with vmap().", "In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "If you\u2019re looking for information on fixing Batch Norm modules, please follow the guidance here"]}, {"name": "torch.func.torch.func API Reference.Patching Batch Norm", "path": "func.batch_norm", "type": "JAX-like Function Transforms", "text": ["Batch Norm requires in-place updates to running_mean and running_var of the same size as the input. Functorch does not support inplace update to a regular tensor that takes in a batched tensor (i.e. regular.add_(batched) is not allowed). So when vmapping over a batch of inputs to a single module, we end up with this error", "One of the best supported ways is to switch BatchNorm for GroupNorm. Options 1 and 2 support this", "All of these options assume that you don\u2019t need running stats. If you\u2019re using a module this means that it\u2019s assumed you won\u2019t use batch norm in evaluation mode. If you have a use case that involves running batch norm with vmap in evaluation mode, please file an issue", "If you want to change for GroupNorm, anywhere that you have BatchNorm, replace it with:", "Here C is the same C as in the original BatchNorm. G is the number of groups to break C into. As such, C % G == 0 and as a fallback, you can set C == G, meaning each channel will be treated separately.", "If you must use BatchNorm and you\u2019ve built the module yourself, you can change the module to not use running stats. In other words, anywhere that there\u2019s a BatchNorm module, set the track_running_stats flag to be False", "Some torchvision models, like resnet and regnet, can take in a norm_layer parameter. These are often defaulted to be BatchNorm2d if they\u2019ve been defaulted.", "Instead you can set it to be GroupNorm.", "Here, once again, c % g == 0 so as a fallback, set g = c.", "If you are attached to BatchNorm, be sure to use a version that doesn\u2019t use running stats", "functorch has added some functionality to allow for quick, in-place patching of the module to not use running stats. Changing the norm layer is more fragile, so we have not offered that. If you have a net where you want the BatchNorm to not use running stats, you can run replace_all_batch_norm_modules_ to update the module in-place to not use running stats", "When run under eval mode, the running_mean and running_var will not be updated. Therefore, vmap can support this mode"]}, {"name": "torch.func.torch.func API Reference.torch.func.functional_call", "path": "generated/torch.func.functional_call", "type": "JAX-like Function Transforms", "text": ["Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.", "Note", "If the module has active parametrizations, passing a value in the parameter_and_buffer_dicts argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as {submodule_name}.parametrizations.{parameter_name}.original.", "Note", "If the module performs in-place operations on parameters/buffers, these will be reflected in the parameter_and_buffer_dicts input.", "Example:", "Note", "If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.", "Example:", "An example of passing mutliple dictionaries", "And here is an example of applying the grad transform over the parameters of a model.", "Note", "If the user does not need grad tracking outside of grad transforms, they can detach all of the parameters for better performance and memory usage", "Example:", "This means that the user cannot call grad_weight.backward(). However, if they don\u2019t need autograd tracking outside of the transforms, this will result in less memory usage and faster speeds.", "the result of calling module.", "Any"]}, {"name": "torch.func.torch.func API Reference.torch.func.functionalize", "path": "generated/torch.func.functionalize", "type": "JAX-like Function Transforms", "text": ["functionalize is a transform that can be used to remove (intermediate) mutations and aliasing from a function, while preserving the function\u2019s semantics.", "functionalize(func) returns a new function with the same semantics as func, but with all intermediate mutations removed. Every inplace operation performed on an intermediate tensor: intermediate.foo_() gets replaced by its out-of-place equivalent: intermediate_updated = intermediate.foo().", "functionalize is useful for shipping a pytorch program off to backends or compilers that aren\u2019t able to easily represent mutations or aliasing operators.", "Returns a new \u201cfunctionalized\u201d function. It takes the same inputs as func, and has the same behavior, but any mutations (and optionally aliasing) performed on intermeidate tensors in the function will be removed.", "Callable", "functionalize will also remove mutations (and views) that were performed on function inputs. However to preserve semantics, functionalize will \u201cfix up\u201d the mutations after the transform has finished running, by detecting if any tensor inputs \u201cshould have\u201d been mutated, and copying the new data back to the inputs if necessary.", "Example:", "Finally, a helpful mental model for understanding functionalization is that most user pytorch programs are writting with the public torch API. When executed, torch operators are generally decomposed into our internal C++ \u201cATen\u201d API. The logic for functionalization happens entirely at the level of ATen. Functionalization knows how to take every aliasing operator in ATen, and map it to its non-aliasing equivalent (e.g. tensor.view({-1}) -> at::view_copy(tensor, {-1})), and how to take every mutating operator in ATen, and map it to its non-mutating equivalent (e.g. tensor.add_(1) -> at::add(tensor, -1)), while tracking aliases and mutations out-of-line to know when to fix things up. Information about which ATen operators are aliasing or mutating all comes from https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml."]}, {"name": "torch.func.torch.func API Reference.torch.func.grad", "path": "generated/torch.func.grad", "type": "JAX-like Function Transforms", "text": ["grad operator helps computing gradients of func with respect to the input(s) specified by argnums. This operator can be nested to compute higher-order gradients.", "Function to compute gradients with respect to its inputs. By default, the output of the function is the gradient tensor(s) with respect to the first argument. If specified has_aux equals True, tuple of gradients and output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of output gradients with respect to each argnums value is returned.", "Callable", "Example of using grad:", "When composed with vmap, grad can be used to compute per-sample-gradients:", "Example of using grad with has_aux and argnums:", "Note", "Using PyTorch torch.no_grad together with grad.", "Case 1: Using torch.no_grad inside a function:", "In this case, grad(f)(x) will respect the inner torch.no_grad.", "Case 2: Using grad inside torch.no_grad context manager:", "In this case, grad will respect the inner torch.no_grad, but not the outer one. This is because grad is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.grad_and_value", "path": "generated/torch.func.grad_and_value", "type": "JAX-like Function Transforms", "text": ["Returns a function to compute a tuple of the gradient and primal, or forward, computation.", "Function to compute a tuple of gradients with respect to its inputs and the forward computation. By default, the output of the function is a tuple of the gradient tensor(s) with respect to the first argument and the primal computation. If specified has_aux equals True, tuple of gradients and tuple of the forward computation with output auxiliary objects is returned. If argnums is a tuple of integers, a tuple of a tuple of the output gradients with respect to each argnums value and the forward computation is returned.", "Callable", "See grad() for examples"]}, {"name": "torch.func.torch.func API Reference.torch.func.hessian", "path": "generated/torch.func.hessian", "type": "JAX-like Function Transforms", "text": ["Computes the Hessian of func with respect to the arg(s) at index argnum via a forward-over-reverse strategy.", "The forward-over-reverse strategy (composing jacfwd(jacrev(func))) is a good default for good performance. It is possible to compute Hessians through other compositions of jacfwd() and jacrev() like jacfwd(jacfwd(func)) or jacrev(jacrev(func)).", "Returns a function that takes in the same inputs as func and returns the Hessian of func with respect to the arg(s) at argnums.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(jacrev(func)), which has better operator coverage.", "A basic usage with a R^N -> R^1 function gives a N x N Hessian:"]}, {"name": "torch.func.torch.func API Reference.torch.func.jacfwd", "path": "generated/torch.func.jacfwd", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using forward-mode autodiff", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it. An alternative is to use jacrev(), which has better operator coverage.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "jacfwd() can be composed with vmap to produce batched Jacobians:", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "Additionally, jacrev() can be composed with itself or jacrev() to produce Hessians", "By default, jacfwd() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments"]}, {"name": "torch.func.torch.func API Reference.torch.func.jacrev", "path": "generated/torch.func.jacrev", "type": "JAX-like Function Transforms", "text": ["Computes the Jacobian of func with respect to the arg(s) at index argnum using reverse mode autodiff", "Note", "Using chunk_size=1 is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of vmap() are not applicable.", "Returns a function that takes in the same inputs as func and returns the Jacobian of func with respect to the arg(s) at argnums. If has_aux is True, then the returned function instead returns a (jacobian, aux) tuple where jacobian is the Jacobian and aux is auxiliary objects returned by func.", "A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian", "If you would like to compute the output of the function as well as the jacobian of the function, use the has_aux flag to return the output as an auxiliary object:", "jacrev() can be composed with vmap to produce batched Jacobians:", "Additionally, jacrev() can be composed with itself to produce Hessians", "By default, jacrev() computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using argnums:", "Additionally, passing a tuple to argnums will compute the Jacobian with respect to multiple arguments", "Note", "Using PyTorch torch.no_grad together with jacrev. Case 1: Using torch.no_grad inside a function:", "In this case, jacrev(f)(x) will respect the inner torch.no_grad.", "Case 2: Using jacrev inside torch.no_grad context manager:", "In this case, jacrev will respect the inner torch.no_grad, but not the outer one. This is because jacrev is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.jvp", "path": "generated/torch.func.jvp", "type": "JAX-like Function Transforms", "text": ["Standing for the Jacobian-vector product, returns a tuple containing the output of func(*primals) and the \u201cJacobian of func evaluated at primals\u201d times tangents. This is also known as forward-mode autodiff.", "Returns a (output, jvp_out) tuple containing the output of func evaluated at primals and the Jacobian-vector product. If has_aux is True, then instead returns a (output, jvp_out, aux) tuple.", "Note", "You may see this API error out with \u201cforward-mode AD not implemented for operator X\u201d. If so, please file a bug report and we will prioritize it.", "jvp is useful when you wish to compute gradients of a function R^1 -> R^N", "jvp() can support functions with multiple inputs by passing in the tangents for each of the inputs"]}, {"name": "torch.func.torch.func API Reference.torch.func.linearize", "path": "generated/torch.func.linearize", "type": "JAX-like Function Transforms", "text": ["Returns the value of func at primals and linear approximation at primals.", "Returns a (output, jvp_fn) tuple containing the output of func applied to primals and a function that computes the jvp of func evaluated at primals.", "Tuple[Any, Callable]", "linearize is useful if jvp is to be computed multiple times at primals. However, to achieve this, linearize saves intermediate computation and has higher memory requrements than directly applying jvp. So, if all the tangents are known, it maybe more efficient to compute vmap(jvp) instead of using linearize.", "Note", "linearize evaluates func twice. Please file an issue for an implementation with a single evaluation."]}, {"name": "torch.func.torch.func API Reference.torch.func.replace_all_batch_norm_modules_", "path": "generated/torch.func.replace_all_batch_norm_modules_", "type": "JAX-like Function Transforms", "text": ["In place updates root by setting the running_mean and running_var to be None and setting track_running_stats to be False for any nn.BatchNorm module in root", "Module"]}, {"name": "torch.func.torch.func API Reference.torch.func.stack_module_state", "path": "generated/torch.func.stack_module_state", "type": "JAX-like Function Transforms", "text": ["Prepares a list of torch.nn.Modules for ensembling with vmap().", "Given a list of M nn.Modules of the same class, returns two dictionaries that stack all of their parameters and buffers together, indexed by name. The stacked parameters are optimizable (i.e. they are new leaf nodes in the autograd history that are unrelated to the original parameters and can be passed directly to an optimizer).", "Here\u2019s an example of how to ensemble over a very simple model:", "When there\u2019s submodules, this follows state dict naming conventions", "Warning", "All of the modules being stacked together must be the same (except for the values of their parameters/buffers). For example, they should be in the same mode (training vs eval).", "Tuple[Dict[str, Any], Dict[str, Any]]"]}, {"name": "torch.func.torch.func API Reference.torch.func.vjp", "path": "generated/torch.func.vjp", "type": "JAX-like Function Transforms", "text": ["Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Returns a (output, vjp_fn) tuple containing the output of func applied to primals and a function that computes the vjp of func with respect to all primals using the cotangents passed to the returned function. If has_aux is True, then instead returns a (output, vjp_fn, aux) tuple. The returned vjp_fn function will return a tuple of each VJP.", "When used in simple cases, vjp() behaves the same as grad()", "However, vjp() can support functions with multiple outputs by passing in the cotangents for each of the outputs", "vjp() can even support outputs being Python structs", "The function returned by vjp() will compute the partials with respect to each of the primals", "primals are the positional arguments for f. All kwargs use their default value", "Note", "Using PyTorch torch.no_grad together with vjp. Case 1: Using torch.no_grad inside a function:", "In this case, vjp(f)(x) will respect the inner torch.no_grad.", "Case 2: Using vjp inside torch.no_grad context manager:", "In this case, vjp will respect the inner torch.no_grad, but not the outer one. This is because vjp is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.torch.func API Reference.torch.func.vmap", "path": "generated/torch.func.vmap", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by func, effectively vectorizing those operations.", "vmap is useful for handling batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func). vmap can also be used to compute batched gradients when composed with autograd.", "Note", "torch.vmap() is aliased to torch.func.vmap() for convenience. Use whichever one you\u2019d like.", "Returns a new \u201cbatched\u201d function. It takes the same inputs as func, except each input has an extra dimension at the index specified by in_dims. It takes returns the same outputs as func, except each output has an extra dimension at the index specified by out_dims.", "Callable", "One example of using vmap() is to compute batched dot products. PyTorch doesn\u2019t provide a batched torch.dot API; instead of unsuccessfully rummaging through docs, use vmap() to construct a new function.", "vmap() can be helpful in hiding batch dimensions, leading to a simpler model authoring experience.", "vmap() can also help vectorize computations that were previously difficult or impossible to batch. One example is higher-order gradient computation. The PyTorch autograd engine computes vjps (vector-Jacobian products). Computing a full Jacobian matrix for some function f: R^N -> R^N usually requires N calls to autograd.grad, one per Jacobian row. Using vmap(), we can vectorize the whole computation, computing the Jacobian in a single call to autograd.grad.", "vmap() can also be nested, producing an output with multiple batched dimensions", "If the inputs are not batched along the first dimension, in_dims specifies the dimension that each inputs are batched along as", "If there are multiple inputs each of which is batched along different dimensions, in_dims must be a tuple with the batch dimension for each input as", "If the input is a Python struct, in_dims must be a tuple containing a struct matching the shape of the input:", "By default, the output is batched along the first dimension. However, it can be batched along any dimension by using out_dims", "For any function that uses kwargs, the returned function will not batch the kwargs but will accept kwargs", "Note", "vmap does not provide general autobatching or handle variable-length sequences out of the box."]}, {"name": "torch.func.torch.func Whirlwind Tour", "path": "func.whirlwind_tour", "type": "JAX-like Function Transforms", "text": ["torch.func, previously known as functorch, is a library for JAX-like composable function transforms in PyTorch.", "There are a number of use cases that are tricky to do in PyTorch today: - computing per-sample-gradients (or other per-sample quantities)", "Composing vmap(), grad(), vjp(), and jvp() transforms allows us to express the above without designing a separate subsystem for each.", "grad(func) is our gradient computation transform. It returns a new function that computes the gradients of func. It assumes func returns a single-element Tensor and by default it computes the gradients of the output of func w.r.t. to the first input.", "Note: vmap() imposes restrictions on the code that it can be used on. For more details, please see UX Limitations.", "vmap(func)(*inputs) is a transform that adds a dimension to all Tensor operations in func. vmap(func) returns a new function that maps func over some dimension (default: 0) of each Tensor in inputs.", "vmap is useful for hiding batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func), leading to a simpler modeling experience:", "When composed with grad(), vmap() can be used to compute per-sample-gradients:", "The vjp() transform applies func to inputs and returns a new function that computes the vector-Jacobian product (vjp) given some cotangents Tensors.", "The jvp() transforms computes Jacobian-vector-products and is also known as \u201cforward-mode AD\u201d. It is not a higher-order function unlike most other transforms, but it returns the outputs of func(inputs) as well as the jvps.", "The jacrev() transform returns a new function that takes in x and returns the Jacobian of the function with respect to x using reverse-mode AD.", "jacrev() can be composed with vmap() to produce batched jacobians:", "jacfwd() is a drop-in replacement for jacrev that computes Jacobians using forward-mode AD:", "Composing jacrev() with itself or jacfwd() can produce hessians:", "hessian() is a convenience function that combines jacfwd and jacrev:"]}, {"name": "torch.func.UX Limitations", "path": "func.ux_limitations", "type": "JAX-like Function Transforms", "text": ["torch.func, like JAX, has restrictions around what can be transformed. In general, JAX\u2019s limitations are that transforms only work with pure functions: that is, functions where the output is completely determined by the input and that do not involve side effects (like mutation).", "We have a similar guarantee: our transforms work well with pure functions. However, we do support certain in-place operations. On one hand, writing code compatible with function transforms may involve changing how you write PyTorch code, on the other hand, you may find that our transforms let you express things that were previously difficult to express in PyTorch.", "All torch.func transforms share a limitation in that a function should not assign to global variables. Instead, all outputs to a function must be returned from the function. This restriction comes from how torch.func is implemented: each transform wraps Tensor inputs in special torch.func Tensor subclasses that facilitate the transform.", "So, instead of the following:", "Please rewrite f to return intermediate:", "If you are trying to use a torch.autograd API like torch.autograd.grad or torch.autograd.backward inside of a function being transformed by vmap() or one of torch.func\u2019s AD transforms (vjp(), jvp(), jacrev(), jacfwd()), the transform may not be able to transform over it. If it is unable to do so, you\u2019ll receive an error message.", "This is a fundamental design limitation in how PyTorch\u2019s AD support is implemented and the reason why we designed the torch.func library. Please instead use the torch.func equivalents of the torch.autograd APIs: - torch.autograd.grad, Tensor.backward -> torch.func.vjp or torch.func.grad - torch.autograd.functional.jvp -> torch.func.jvp - torch.autograd.functional.jacobian -> torch.func.jacrev or torch.func.jacfwd - torch.autograd.functional.hessian -> torch.func.hessian", "Note", "vmap() is our most restrictive transform. The grad-related transforms (grad(), vjp(), jvp()) do not have these limitations. jacfwd() (and hessian(), which is implemented with jacfwd()) is a composition of vmap() and jvp() so it also has these limitations.", "vmap(func) is a transform that returns a function that maps func over some new dimension of each input Tensor. The mental model for vmap is that it is like running a for-loop: for pure functions (i.e. in the absence of side effects), vmap(f)(x) is equivalent to:", "In the presence of side effects, vmap() no longer acts like it is running a for-loop. For example, the following function:", "will print \u201chello!\u201d once and pop only one element from lst.", "vmap() executes f a single time, so all side effects only happen once.", "This is a consequence of how vmap is implemented. torch.func has a special, internal BatchedTensor class. vmap(f)(*inputs) takes all Tensor inputs, turns them into BatchedTensors, and calls f(*batched_tensor_inputs). BatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized) behavior for each PyTorch operator.", "You might be here due to receiving an error about vmap-incompatible in-place operations. vmap() will raise an error if it encounters an unsupported PyTorch in-place operation and it will succeed otherwise. Unsupported operations are those that would cause a Tensor with more elements to be written to a Tensor with fewer elements. Here\u2019s an example of how this can occur:", "x is a Tensor with one element, y is a Tensor with three elements. x + y has three elements (due to broadcasting), but attempting to write three elements back into x, which only has one element, raises an error due to attempting to write three elements into a Tensor with a single element.", "There is no problem if the Tensor being written to is batched under vmap() (i.e. it is being vmapped over).", "One common fix for this is to replace calls to factory functions with their \u201cnew_*\u201d equivalent. For example:", "To see why this helps, consider the following.", "Inside of vmap(), result is a Tensor of shape [3, 3]. However, although vec looks like it has shape [3], vec actually has underlying shape [2, 3]. It is not possible to copy vec into result.diagonal(), which has shape [3], because it has too many elements.", "Replacing torch.zeros() with Tensor.new_zeros() makes it so that result has an underlying Tensor of shape [2, 3, 3], so it is now possible to copy vec, which has underlying shape [2, 3], into result.diagonal().", "vmap() doesn\u2019t support the out= keyword argument in PyTorch operations. It will error out gracefully if it encounters that in your code.", "This is not a fundamental limitation; we could theoretically support this in the future but we have chosen not to for now.", "We don\u2019t yet support vmap over data-dependent control flow. Data-dependent control flow is when the condition of an if-statement, while-loop, or for-loop is a Tensor that is being vmap\u2019ed over. For example, the following will raise an error message:", "However, any control flow that is not dependent on the values in vmap\u2019ed tensors will work:", "JAX supports transforming over data-dependent control flow using special control flow operators (e.g. jax.lax.cond, jax.lax.while_loop). We\u2019re investigating adding equivalents of those to PyTorch.", "We do not (and will not) support vmap over a user-defined function that calls .item() on a Tensor. For example, the following will raise an error message:", "Please try to rewrite your code to not use .item() calls.", "You may also encounter an error message about using .item() but you might not have used it. In those cases, it is possible that PyTorch internally is calling .item() \u2013 please file an issue on GitHub and we\u2019ll fix PyTorch internals.", "vmap(f) requires that f applied to every \u201cexample\u201d in your input returns a Tensor with the same shape. Operations such as torch.nonzero, torch.is_nonzero are not supported and will error as a result.", "To see why, consider the following example:", "torch.nonzero(xs[0]) returns a Tensor of shape 2; but torch.nonzero(xs[1]) returns a Tensor of shape 1. We are unable to construct a single Tensor as an output; the output would need to be a ragged Tensor (and PyTorch does not yet have the concept of a ragged Tensor).", "The user\u2019s intention when calling a random operation can be unclear. Specifically, some users may want the random behavior to be the same across batches while others may want it to differ across batches. To address this, vmap takes a randomness flag.", "The flag can only be passed to vmap and can take on 3 values, \u201cerror,\u201d \u201cdifferent,\u201d or \u201csame,\u201d defaulting to error. Under \u201cerror\u201d mode, any call to a random function will produce an error asking the user to use one of the other two flags based on their use case.", "Under \u201cdifferent\u201d randomness, elements in a batch produce different random values. For instance,", "Under \u201csame\u201d randomness, elements in a batch produce same random values. For instance,", "Warning", "Our system only determine the randomness behavior of PyTorch operators and cannot control the behavior of other libraries, like numpy. This is similar to JAX\u2019s limitations with their solutions", "Note", "Multiple vmap calls using either type of supported randomness will not produce the same results. Like with standard PyTorch, a user can get randomness reproducibility through either using torch.manual_seed() outside of vmap or by using generators.", "Note", "Finally, our randomness differs from JAX because we aren\u2019t using a stateless PRNG, in part because PyTorch doesn\u2019t have full support for a stateless PRNG. Instead, we\u2019ve introduced a flag system to allow for the most common forms of randomness that we see. If your use case does not fit these forms of randomness, please file an issue."]}, {"name": "torch.func.vjp()", "path": "generated/torch.func.vjp#torch.func.vjp", "type": "JAX-like Function Transforms", "text": ["Standing for the vector-Jacobian product, returns a tuple containing the results of func applied to primals and a function that, when given cotangents, computes the reverse-mode Jacobian of func with respect to primals times cotangents.", "Returns a (output, vjp_fn) tuple containing the output of func applied to primals and a function that computes the vjp of func with respect to all primals using the cotangents passed to the returned function. If has_aux is True, then instead returns a (output, vjp_fn, aux) tuple. The returned vjp_fn function will return a tuple of each VJP.", "When used in simple cases, vjp() behaves the same as grad()", "However, vjp() can support functions with multiple outputs by passing in the cotangents for each of the outputs", "vjp() can even support outputs being Python structs", "The function returned by vjp() will compute the partials with respect to each of the primals", "primals are the positional arguments for f. All kwargs use their default value", "Note", "Using PyTorch torch.no_grad together with vjp. Case 1: Using torch.no_grad inside a function:", "In this case, vjp(f)(x) will respect the inner torch.no_grad.", "Case 2: Using vjp inside torch.no_grad context manager:", "In this case, vjp will respect the inner torch.no_grad, but not the outer one. This is because vjp is a \u201cfunction transform\u201d: its result should not depend on the result of a context manager outside of f."]}, {"name": "torch.func.vmap()", "path": "generated/torch.func.vmap#torch.func.vmap", "type": "JAX-like Function Transforms", "text": ["vmap is the vectorizing map; vmap(func) returns a new function that maps func over some dimension of the inputs. Semantically, vmap pushes the map into PyTorch operations called by func, effectively vectorizing those operations.", "vmap is useful for handling batch dimensions: one can write a function func that runs on examples and then lift it to a function that can take batches of examples with vmap(func). vmap can also be used to compute batched gradients when composed with autograd.", "Note", "torch.vmap() is aliased to torch.func.vmap() for convenience. Use whichever one you\u2019d like.", "Returns a new \u201cbatched\u201d function. It takes the same inputs as func, except each input has an extra dimension at the index specified by in_dims. It takes returns the same outputs as func, except each output has an extra dimension at the index specified by out_dims.", "Callable", "One example of using vmap() is to compute batched dot products. PyTorch doesn\u2019t provide a batched torch.dot API; instead of unsuccessfully rummaging through docs, use vmap() to construct a new function.", "vmap() can be helpful in hiding batch dimensions, leading to a simpler model authoring experience.", "vmap() can also help vectorize computations that were previously difficult or impossible to batch. One example is higher-order gradient computation. The PyTorch autograd engine computes vjps (vector-Jacobian products). Computing a full Jacobian matrix for some function f: R^N -> R^N usually requires N calls to autograd.grad, one per Jacobian row. Using vmap(), we can vectorize the whole computation, computing the Jacobian in a single call to autograd.grad.", "vmap() can also be nested, producing an output with multiple batched dimensions", "If the inputs are not batched along the first dimension, in_dims specifies the dimension that each inputs are batched along as", "If there are multiple inputs each of which is batched along different dimensions, in_dims must be a tuple with the batch dimension for each input as", "If the input is a Python struct, in_dims must be a tuple containing a struct matching the shape of the input:", "By default, the output is batched along the first dimension. However, it can be batched along any dimension by using out_dims", "For any function that uses kwargs, the returned function will not batch the kwargs but will accept kwargs", "Note", "vmap does not provide general autobatching or handle variable-length sequences out of the box."]}, {"name": "torch.futures", "path": "futures", "type": "Miscellaneous", "text": ["This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.", "Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Warning", "GPU support is a beta feature, subject to changes.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "If the Future\u2019s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn\u2019t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().", "Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn\u2019t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Future[S]", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Obtain the value of an already-completed future.", "This method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.", "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it\u2019s already being taken care of by then()).", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.", "T", "Block until the value of this Future is ready.", "If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn\u2019t change streams.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "T", "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures.", "Future[List[Future]]", "Waits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.", "futures (list) \u2013 a list of Future object.", "A list of the completed Future results. This method will throw an error if wait on any Future throws.", "List"]}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "Miscellaneous", "text": ["Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.", "futures (list) \u2013 a list of Future objects.", "Returns a Future object to a list of the passed in Futures.", "Future[List[Future]]"]}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "Miscellaneous", "text": ["Wrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.", "Warning", "GPU support is a beta feature, subject to changes.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool", "Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future.", "Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future.", "Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: fut.then(cb1).then(cb2)). The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.", "If the Future\u2019s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn\u2019t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of wait().", "Similarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn\u2019t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.", "callback (Callable) \u2013 a Callable that takes this Future as the only argument.", "A new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.", "Future[S]", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, the future returned by then will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.", "Obtain the value of an already-completed future.", "This method should only be called after a call to wait() has completed, or inside a callback function passed to then(). In other cases this Future may not yet hold a value and calling value() could fail.", "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done beforehand, separately, through a call to wait() (except within callbacks, for which it\u2019s already being taken care of by then()).", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this value() method will also throw an error.", "T", "Block until the value of this Future is ready.", "If the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that wait() will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, wait() will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn\u2019t change streams.", "The value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.", "T"]}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "Miscellaneous", "text": ["Append the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this Future. The callback function can use the value() method to get the value. Note that if this Future is already completed, the given callback will be run inline.", "We recommend that you use the then() method as it provides a way to synchronize after your callback has completed. add_done_callback can be cheaper if your callback does not return anything. But both then() and add_done_callback use the same callback registration API under the hood.", "With respect to GPU tensors, this method behaves in the same way as then().", "callback (Future) \u2013 a Callable that takes in one argument, which is the reference to this Future.", "Note", "Note that if the callback function throws, either through the original future being completed with an exception and calling fut.wait(), or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently."]}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "Miscellaneous", "text": ["Return True if this Future is done. A Future is done if it has a result or an exception.", "If the value contains tensors that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see wait()).", "bool"]}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "Miscellaneous", "text": ["Set an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.", "result (BaseException) \u2013 the exception for this Future."]}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "Miscellaneous", "text": ["Set the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.", "If the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it\u2019s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn\u2019t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this Future.", "result (object) \u2013 the result object of this Future."]}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "Miscellaneous", "text": ["Append the given callback function to this Futu