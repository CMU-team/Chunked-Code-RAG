[{"name": "tf.AggregationMethod", "path": "aggregationmethod", "type": "tf", "text": "\nA class listing aggregation methods used to combine gradients.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.AggregationMethod`\n\nComputing partial derivatives can require aggregating gradient contributions.\nThis class lists the various methods that can be used to combine gradients in\nthe graph.\n\nThe following aggregation methods are part of the stable API for aggregating\ngradients:\n\nThe following aggregation methods are experimental and may not be supported in\nfuture releases:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.argsort", "path": "argsort", "type": "tf", "text": "\nReturns the indices of a tensor that give its sorted order along an axis.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.argsort`\n\nFor a 1D tensor, `tf.gather(values, tf.argsort(values))` is equivalent to\n`tf.sort(values)`. For higher dimensions, the output has the same shape as\n`values`, but along the given axis, values represent the index of the sorted\nelement in that slice of the tensor at the given position.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.audio", "path": "audio", "type": "tf.audio", "text": "\nPublic API for tf.audio namespace.\n\n`decode_wav(...)`: Decode a 16-bit PCM WAV file to a float tensor.\n\n`encode_wav(...)`: Encode audio data using the WAV file format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.audio.decode_wav", "path": "audio/decode_wav", "type": "tf.audio", "text": "\nDecode a 16-bit PCM WAV file to a float tensor.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.audio.decode_wav`\n\nThe -32768 to 32767 signed 16-bit values will be scaled to -1.0 to 1.0 in\nfloat.\n\nWhen desired_channels is set, if the input contains fewer channels than this\nthen the last channel will be duplicated to give the requested number, else if\nthe input has more channels than requested then the additional channels will\nbe ignored.\n\nIf desired_samples is set, then the audio will be cropped or padded with\nzeroes to the requested length.\n\nThe first output contains a Tensor with the content of the audio samples. The\nlowest dimension will be the number of channels, and the second will be the\nnumber of samples. For example, a ten-sample-long stereo WAV file should give\nan output shape of [10, 2].\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.audio.encode_wav", "path": "audio/encode_wav", "type": "tf.audio", "text": "\nEncode audio data using the WAV file format.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.audio.encode_wav`\n\nThis operation will generate a string suitable to be saved out to create a\n.wav audio file. It will be encoded in the 16-bit PCM format. It takes in\nfloat values in the range -1.0f to 1.0f, and any outside that value will be\nclamped to that range.\n\n`audio` is a 2-D float Tensor of shape `[length, channels]`. `sample_rate` is\na scalar Tensor holding the rate to use (e.g. 44100).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autodiff", "path": "autodiff", "type": "tf.autodiff", "text": "\nPublic API for tf.autodiff namespace.\n\n`class ForwardAccumulator`: Computes Jacobian-vector products (\"JVP\"s) using\nforward-mode autodiff.\n\n`class GradientTape`: Record operations for automatic differentiation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autodiff.ForwardAccumulator", "path": "autodiff/forwardaccumulator", "type": "tf.autodiff", "text": "\nComputes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff.\n\nCompare to `tf.GradientTape` which computes vector-Jacobian products (\"VJP\"s)\nusing reverse-mode autodiff (backprop). Reverse mode is more attractive when\ncomputing gradients of a scalar-valued function with respect to many inputs\n(e.g. a neural network with many parameters and a scalar loss). Forward mode\nworks best on functions with many outputs and few inputs. Since it does not\nhold on to intermediate activations, it is much more memory efficient than\nbackprop where it is applicable.\n\nConsider a simple linear regression:\n\nThe example has two variables containing parameters, `dense.kernel` (2\nparameters) and `dense.bias` (1 parameter). Considering the training data `x`\nas a constant, this means the Jacobian matrix for the function mapping from\nparameters to loss has one row and three columns.\n\nWith forwardprop, we specify a length-three vector in advance which multiplies\nthe Jacobian. The `primals` constructor argument is the parameter (a\n`tf.Tensor` or `tf.Variable`) we're specifying a vector for, and the\n`tangents` argument is the \"vector\" in Jacobian-vector product. If our goal is\nto compute the entire Jacobian matrix, forwardprop computes one column at a\ntime while backprop computes one row at a time. Since the Jacobian in the\nlinear regression example has only one row, backprop requires fewer\ninvocations:\n\nImplicit in the `tape.gradient` call is a length-one vector which left-\nmultiplies the Jacobian, a vector-Jacobian product.\n\n`ForwardAccumulator` maintains JVPs corresponding primal tensors it is\nwatching, derived from the original `primals` specified in the constructor. As\nsoon as a primal tensor is deleted, `ForwardAccumulator` deletes the\ncorresponding JVP.\n\n`acc.jvp(x)` retrieves `acc`'s JVP corresponding to the primal tensor `x`. It\ndoes not perform any computation. `acc.jvp` calls can be repeated as long as\n`acc` is accessible, whether the context manager is active or not. New JVPs\nare only computed while the context manager is active.\n\nNote that `ForwardAccumulator`s are always applied in the order their context\nmanagers were entered, so inner accumulators will not see JVP computation from\nouter accumulators. Take higher-order JVPs from outer accumulators:\n\nReversing the collection in the last line to instead retrieve\n`inner.jvp(outer.jvp(primal_out))` will not work.\n\nStrict nesting also applies to combinations of `ForwardAccumulator` and\n`tf.GradientTape`. More deeply nested `GradientTape` objects will ignore the\nproducts of outer `ForwardAccumulator` objects. This allows (for example)\nmemory-efficient forward-over-backward computation of Hessian-vector products,\nwhere the inner `GradientTape` would otherwise hold on to all intermediate\nJVPs:\n\nView source\n\nFetches the Jacobian-vector product computed for `primals`.\n\nNote that this method performs no computation, and simply looks up a JVP that\nwas already computed (unlike backprop using a `tf.GradientTape`, where the\ncomputation happens on the call to `tape.gradient`).\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph", "path": "autograph", "type": "tf.autograph", "text": "\nConversion of plain Python into TensorFlow graph code.\n\nFor more information, see the AutoGraph guide.\n\nBy equivalent graph code we mean code that generates a TensorFlow graph when\nrun. The generated graph has the same effects as the original code when\nexecuted (for example with `tf.function` or `tf.compat.v1.Session.run`). In\nother words, using AutoGraph can be thought of as running Python in\nTensorFlow.\n\n`experimental` module: Public API for tf.autograph.experimental namespace.\n\n`set_verbosity(...)`: Sets the AutoGraph verbosity level.\n\n`to_code(...)`: Returns the source code generated by AutoGraph, as a string.\n\n`to_graph(...)`: Converts a Python entity into a TensorFlow graph.\n\n`trace(...)`: Traces argument information at compilation time.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.experimental", "path": "autograph/experimental", "type": "tf.autograph", "text": "\nPublic API for tf.autograph.experimental namespace.\n\n`class Feature`: This enumeration represents optional conversion options.\n\n`do_not_convert(...)`: Decorator that suppresses the conversion of a function.\n\n`set_loop_options(...)`: Specifies additional arguments to be passed to the\nenclosing while_loop.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.experimental.do_not_convert", "path": "autograph/experimental/do_not_convert", "type": "tf.autograph", "text": "\nDecorator that suppresses the conversion of a function.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.autograph.experimental.do_not_convert`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.experimental.Feature", "path": "autograph/experimental/feature", "type": "tf.autograph", "text": "\nThis enumeration represents optional conversion options.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.autograph.experimental.Feature`\n\nThese conversion options are experimental. They are subject to change without\nnotice and offer no guarantees.\n\nExample Usage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.experimental.set_loop_options", "path": "autograph/experimental/set_loop_options", "type": "tf.autograph", "text": "\nSpecifies additional arguments to be passed to the enclosing while_loop.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.autograph.experimental.set_loop_options`\n\nThe parameters apply to and only to the immediately enclosing loop. It only\nhas effect if the loop is staged as a TF while_loop; otherwise the parameters\nhave no effect.\n\nAlso see tf.while_loop.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.set_verbosity", "path": "autograph/set_verbosity", "type": "tf.autograph", "text": "\nSets the AutoGraph verbosity level.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.autograph.set_verbosity`\n\nDebug logging in AutoGraph\n\nMore verbose logging is useful to enable when filing bug reports or doing more\nin-depth debugging.\n\nThere are two means to control the logging verbosity:\n\nThe `set_verbosity` function\n\nThe `AUTOGRAPH_VERBOSITY` environment variable\n\n`set_verbosity` takes precedence over the environment variable.\n\nLogs entries are output to absl's default output, with `INFO` level. Logs can\nbe mirrored to stdout by using the `alsologtostdout` argument. Mirroring is\nenabled by default when Python runs in interactive mode.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.to_code", "path": "autograph/to_code", "type": "tf.autograph", "text": "\nReturns the source code generated by AutoGraph, as a string.\n\nAlso see: `tf.autograph.to_graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.to_graph", "path": "autograph/to_graph", "type": "tf.autograph", "text": "\nConverts a Python entity into a TensorFlow graph.\n\nAlso see: `tf.autograph.to_code`, `tf.function`.\n\nUnlike `tf.function`, `to_graph` is a low-level transpiler that converts\nPython code to TensorFlow graph code. It does not implement any caching,\nvariable management or create any actual ops, and is best used where greater\ncontrol over the generated TensorFlow graph is desired. Another difference\nfrom `tf.function` is that `to_graph` will not wrap the graph into a\nTensorFlow function or a Python callable. Internally, `tf.function` uses\n`to_graph`.\n\nSupported Python entities include:\n\nFunctions are converted into new functions with converted code.\n\nClasses are converted by generating a new class whose methods use converted\ncode.\n\nMethods are converted into unbound function that have an additional first\nargument called `self`.\n\nFor a tutorial, see the tf.function and AutoGraph guide. For more detailed\ninformation, see the AutoGraph reference documentation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.autograph.trace", "path": "autograph/trace", "type": "tf.autograph", "text": "\nTraces argument information at compilation time.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.autograph.trace`\n\n`trace` is useful when debugging, and it always executes during the tracing\nphase, that is, when the TF graph is constructed.\n\nExample usage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.batch_to_space", "path": "batch_to_space", "type": "tf", "text": "\nBatchToSpace for N-D tensors of type T.\n\nThis operation reshapes the \"batch\" dimension 0 into `M + 1` dimensions of\nshape `block_shape + [batch]`, interleaves these blocks back into the grid\ndefined by the spatial dimensions `[1, ..., M]`, to obtain a result with the\nsame rank as the input. The spatial dimensions of this intermediate result are\nthen optionally cropped according to `crops` to produce the output. This is\nthe reverse of SpaceToBatch (see `tf.space_to_batch`).\n\n(1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [0, 0]]`:\n\nThe output tensor has shape `[1, 2, 2, 1]` and value:\n\n(2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [0, 0]]`:\n\nThe output tensor has shape `[1, 2, 2, 3]` and value:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitcast", "path": "bitcast", "type": "tf", "text": "\nBitcasts a tensor from one type to another without copying data.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitcast`\n\nGiven a tensor `input`, this operation returns a tensor that has the same\nbuffer data as `input` with datatype `type`.\n\nIf the input datatype `T` is larger than the output datatype `type` then the\nshape changes from [...] to [..., sizeof(`T`)/sizeof(`type`)].\n\nIf `T` is smaller than `type`, the operator requires that the rightmost\ndimension be equal to sizeof(`type`)/sizeof(`T`). The shape then goes from\n[..., sizeof(`type`)/sizeof(`T`)] to [...].\n\ntf.bitcast() and tf.cast() work differently when real dtype is casted as a\ncomplex dtype (e.g. tf.complex64 or tf.complex128) as tf.cast() make imaginary\npart 0 while tf.bitcast() gives module error. For example,\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise", "path": "bitwise", "type": "tf.bitwise", "text": "\nOperations for manipulating the binary representations of integers.\n\n`bitwise_and(...)`: Elementwise computes the bitwise AND of `x` and `y`.\n\n`bitwise_or(...)`: Elementwise computes the bitwise OR of `x` and `y`.\n\n`bitwise_xor(...)`: Elementwise computes the bitwise XOR of `x` and `y`.\n\n`invert(...)`: Invert (flip) each bit of supported types; for example, type\n`uint8` value 01010101 becomes 10101010.\n\n`left_shift(...)`: Elementwise computes the bitwise left-shift of `x` and `y`.\n\n`right_shift(...)`: Elementwise computes the bitwise right-shift of `x` and\n`y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.bitwise_and", "path": "bitwise/bitwise_and", "type": "tf.bitwise", "text": "\nElementwise computes the bitwise AND of `x` and `y`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.bitwise_and`\n\nThe result will have those bits set, that are set in both `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.bitwise_or", "path": "bitwise/bitwise_or", "type": "tf.bitwise", "text": "\nElementwise computes the bitwise OR of `x` and `y`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.bitwise_or`\n\nThe result will have those bits set, that are set in `x`, `y` or both. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.bitwise_xor", "path": "bitwise/bitwise_xor", "type": "tf.bitwise", "text": "\nElementwise computes the bitwise XOR of `x` and `y`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.bitwise_xor`\n\nThe result will have those bits set, that are different in `x` and `y`. The\ncomputation is performed on the underlying representations of `x` and `y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.invert", "path": "bitwise/invert", "type": "tf.bitwise", "text": "\nInvert (flip) each bit of supported types; for example, type `uint8` value\n01010101 becomes 10101010.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.invert`\n\nFlip each bit of supported types. For example, type `int8` (decimal 2) binary\n00000010 becomes (decimal -3) binary 11111101. This operation is performed on\neach element of the tensor argument `x`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.left_shift", "path": "bitwise/left_shift", "type": "tf.bitwise", "text": "\nElementwise computes the bitwise left-shift of `x` and `y`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.left_shift`\n\nIf `y` is negative, or greater than or equal to the width of `x` in bits the\nresult is implementation defined.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.bitwise.right_shift", "path": "bitwise/right_shift", "type": "tf.bitwise", "text": "\nElementwise computes the bitwise right-shift of `x` and `y`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.bitwise.right_shift`\n\nPerforms a logical shift for unsigned integer types, and an arithmetic shift\nfor signed integer types.\n\nIf `y` is negative, or greater than or equal to than the width of `x` in bits\nthe result is implementation defined.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.boolean_mask", "path": "boolean_mask", "type": "tf", "text": "\nApply boolean mask to tensor.\n\nNumpy equivalent is `tensor[mask]`.\n\nIn general, `0 < dim(mask) = K <= dim(tensor)`, and `mask`'s shape must match\nthe first K dimensions of `tensor`'s shape. We then have:\n`boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]` where\n`(i1,...,iK)` is the ith `True` entry of `mask` (row-major order). The `axis`\ncould be used with `mask` to indicate the axis to mask from. In that case,\n`axis + dim(mask) <= dim(tensor)` and `mask`'s shape must match the first\n`axis + dim(mask)` dimensions of `tensor`'s shape.\n\nSee also: `tf.ragged.boolean_mask`, which can be applied to both dense and\nragged tensors, and can be used if you need to preserve the masked dimensions\nof `tensor` (rather than flattening them, as `tf.boolean_mask` does).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.broadcast_dynamic_shape", "path": "broadcast_dynamic_shape", "type": "tf", "text": "\nComputes the shape of a broadcast given symbolic shapes.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.broadcast_dynamic_shape`\n\nWhen `shape_x` and `shape_y` are Tensors representing shapes (i.e. the result\nof calling tf.shape on another Tensor) this computes a Tensor which is the\nshape of the result of a broadcasting op applied in tensors of shapes\n`shape_x` and `shape_y`.\n\nThis is useful when validating the result of a broadcasting operation when the\ntensors do not have statically known shapes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.broadcast_static_shape", "path": "broadcast_static_shape", "type": "tf", "text": "\nComputes the shape of a broadcast given known shapes.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.broadcast_static_shape`\n\nWhen `shape_x` and `shape_y` are fully known `TensorShape`s this computes a\n`TensorShape` which is the shape of the result of a broadcasting op applied in\ntensors of shapes `shape_x` and `shape_y`.\n\nFor example, if shape_x is `TensorShape([1, 2, 3])` and shape_y is\n`TensorShape([5, 1, 3])`, the result is a TensorShape whose value is\n`TensorShape([5, 2, 3])`.\n\nThis is useful when validating the result of a broadcasting operation when the\ntensors have statically known shapes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.broadcast_to", "path": "broadcast_to", "type": "tf", "text": "\nBroadcast an array for a compatible shape.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.broadcast_to`\n\nBroadcasting is the process of making arrays to have compatible shapes for\narithmetic operations. Two shapes are compatible if for each dimension pair\nthey are either equal or one of them is one. When trying to broadcast a Tensor\nto a shape, it starts with the trailing dimensions, and works its way forward.\n\nFor example,\n\nIn the above example, the input Tensor with the shape of `[1, 3]` is\nbroadcasted to output Tensor with shape of `[3, 3]`.\n\nWhen doing broadcasted operations such as multiplying a tensor by a scalar,\nbroadcasting (usually) confers some time or space benefit, as the broadcasted\ntensor is never materialized.\n\nHowever, `broadcast_to` does not carry with it any such benefits. The newly-\ncreated tensor takes the full memory of the broadcasted shape. (In a graph\ncontext, `broadcast_to` might be fused to subsequent operation and then be\noptimized away, however.)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.case", "path": "case", "type": "tf", "text": "\nCreate a case operation.\n\nSee also `tf.switch_case`.\n\nThe `pred_fn_pairs` parameter is a list of pairs of size N. Each pair contains\na boolean scalar tensor and a python callable that creates the tensors to be\nreturned if the boolean evaluates to True. `default` is a callable generating\na list of tensors. All the callables in `pred_fn_pairs` as well as `default`\n(if provided) should return the same number and types of tensors.\n\nIf `exclusive==True`, all predicates are evaluated, and an exception is thrown\nif more than one of the predicates evaluates to `True`. If `exclusive==False`,\nexecution stops at the first predicate which evaluates to True, and the\ntensors generated by the corresponding function are returned immediately. If\nnone of the predicates evaluate to True, this operation returns the tensors\ngenerated by `default`.\n\n`tf.case` supports nested structures as implemented in\n`tf.contrib.framework.nest`. All of the callables must return the same\n(possibly nested) value structure of lists, tuples, and/or named tuples.\nSingleton lists and tuples form the only exceptions to this: when returned by\na callable, they are implicitly unpacked to single values. This behavior is\ndisabled by passing `strict=True`.\n\nExample 1:\n\nExample 2:\n\n`pred_fn_pairs` could be a dictionary in v1. However, tf.Tensor and\ntf.Variable are no longer hashable in v2, so cannot be used as a key for a\ndictionary. Please use a list or a tuple instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.cast", "path": "cast", "type": "tf", "text": "\nCasts a tensor to a new type.\n\nMain aliases\n\n`tf.dtypes.cast`\n\nSee Migration guide for more details.\n\n`tf.compat.v1.cast`, `tf.compat.v1.dtypes.cast`\n\nThe operation casts `x` (in case of `Tensor`) or `x.values` (in case of\n`SparseTensor` or `IndexedSlices`) to `dtype`.\n\nThe operation supports data types (for `x` and `dtype`) of `uint8`, `uint16`,\n`uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`, `float16`, `float32`,\n`float64`, `complex64`, `complex128`, `bfloat16`. In case of casting from\ncomplex types (`complex64`, `complex128`) to real types, only the real part of\n`x` is returned. In case of casting from real types to complex types\n(`complex64`, `complex128`), the imaginary part of the returned value is set\nto `0`. The handling of complex types here matches the behavior of numpy.\n\nNote casting nan and inf values to integral types has undefined behavior.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.clip_by_global_norm", "path": "clip_by_global_norm", "type": "tf", "text": "\nClips values of multiple tensors by the ratio of the sum of their norms.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.clip_by_global_norm`\n\nGiven a tuple or list of tensors `t_list`, and a clipping ratio `clip_norm`,\nthis operation returns a list of clipped tensors `list_clipped` and the global\nnorm (`global_norm`) of all tensors in `t_list`. Optionally, if you've already\ncomputed the global norm for `t_list`, you can specify the global norm with\n`use_norm`.\n\nTo perform the clipping, the values `t_list[i]` are set to:\n\nwhere:\n\nIf `clip_norm > global_norm` then the entries in `t_list` remain as they are,\notherwise they're all shrunk by the global ratio.\n\nIf `global_norm == infinity` then the entries in `t_list` are all set to `NaN`\nto signal that an error occurred.\n\nAny of the entries of `t_list` that are of type `None` are ignored.\n\nThis is the correct way to perform gradient clipping (Pascanu et al., 2012).\n\nHowever, it is slower than `clip_by_norm()` because all the parameters must be\nready before the clipping operation can be performed.\n\nOn the difficulty of training Recurrent Neural Networks: Pascanu et al., 2012\n(pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.clip_by_norm", "path": "clip_by_norm", "type": "tf", "text": "\nClips tensor values to a maximum L2-norm.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.clip_by_norm`\n\nGiven a tensor `t`, and a maximum clip value `clip_norm`, this operation\nnormalizes `t` so that its L2-norm is less than or equal to `clip_norm`, along\nthe dimensions given in `axes`. Specifically, in the default case where all\ndimensions are used for calculation, if the L2-norm of `t` is already less\nthan or equal to `clip_norm`, then `t` is not modified. If the L2-norm is\ngreater than `clip_norm`, then this operation returns a tensor of the same\ntype and shape as `t` with its values set to:\n\n`t * clip_norm / l2norm(t)`\n\nIn this case, the L2-norm of the output tensor is `clip_norm`.\n\nAs another example, if `t` is a matrix and `axes == [1]`, then each row of the\noutput will have L2-norm less than or equal to `clip_norm`. If `axes == [0]`\ninstead, each column of the output will be clipped.\n\nThis operation is typically used to clip gradients before applying them with\nan optimizer. Most gradient data is a collection of different shaped tensors\nfor different parts of the model. Thus, this is a common usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.clip_by_value", "path": "clip_by_value", "type": "tf", "text": "\nClips tensor values to a specified min and max.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.clip_by_value`\n\nGiven a tensor `t`, this operation returns a tensor of the same type and shape\nas `t` with its values clipped to `clip_value_min` and `clip_value_max`. Any\nvalues less than `clip_value_min` are set to `clip_value_min`. Any values\ngreater than `clip_value_max` are set to `clip_value_max`.\n\nBasic usage passes a scalar as the min and max value.\n\nThe min and max can be the same size as `t`, or broadcastable to that size.\n\nBroadcasting fails, intentionally, if you would expand the dimensions of `t`\n\nIt throws a `TypeError` if you try to clip an `int` to a `float` value\n(`tf.cast` the input to `float` first).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat", "path": "compat", "type": "tf.compat", "text": "\nCompatibility functions.\n\nThe `tf.compat` module contains two sets of compatibility functions.\n\nThe `compat.v1` and `compat.v2` submodules provide a complete copy of both the\n`v1` and `v2` APIs for backwards and forwards compatibility across TensorFlow\nversions 1.x and 2.x. See the migration guide for details.\n\nAside from the `compat.v1` and `compat.v2` submodules, `tf.compat` also\ncontains a set of helper functions for writing code that works in both:\n\nThe compatibility module also provides the following aliases for common sets\nof python types:\n\n`v1` module: Bring in all of the public TensorFlow interface into this module.\n\n`as_bytes(...)`: Converts `bytearray`, `bytes`, or unicode python input types\nto `bytes`.\n\n`as_str(...)`\n\n`as_str_any(...)`: Converts input to `str` type.\n\n`as_text(...)`: Converts any string-like python input types to unicode.\n\n`dimension_at_index(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`dimension_value(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`forward_compatibility_horizon(...)`: Context manager for testing forward\ncompatibility of generated graphs.\n\n`forward_compatible(...)`: Return true if the forward compatibility window has\nexpired.\n\n`path_to_str(...)`: Converts input which is a `PathLike` object to `str` type.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.as_bytes", "path": "compat/as_bytes", "type": "tf.compat", "text": "\nConverts `bytearray`, `bytes`, or unicode python input types to `bytes`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.as_bytes`\n\nUses utf-8 encoding for text by default.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.as_str", "path": "compat/as_str", "type": "tf.compat", "text": "\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.as_str`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.as_str_any", "path": "compat/as_str_any", "type": "tf.compat", "text": "\nConverts input to `str` type.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.as_str_any`\n\nUses `str(value)`, except for `bytes` typed inputs, which are converted using\n`as_str`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.as_text", "path": "compat/as_text", "type": "tf.compat", "text": "\nConverts any string-like python input types to unicode.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.as_text`\n\nReturns the input as a unicode string. Uses utf-8 encoding for text by\ndefault.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.dimension_at_index", "path": "compat/dimension_at_index", "type": "tf.compat", "text": "\nCompatibility utility required to allow for both V1 and V2 behavior in TF.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.dimension_at_index`, `tf.compat.v1.dimension_at_index`\n\nUntil the release of TF 2.0, we need the legacy behavior of `TensorShape` to\ncoexist with the new behavior. This utility is a bridge between the two.\n\nIf you want to retrieve the Dimension instance corresponding to a certain\nindex in a TensorShape instance, use this utility, like this:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.dimension_value", "path": "compat/dimension_value", "type": "tf.compat", "text": "\nCompatibility utility required to allow for both V1 and V2 behavior in TF.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.dimension_value`, `tf.compat.v1.dimension_value`\n\nUntil the release of TF 2.0, we need the legacy behavior of `TensorShape` to\ncoexist with the new behavior. This utility is a bridge between the two.\n\nWhen accessing the value of a TensorShape dimension, use this utility, like\nthis:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.forward_compatibility_horizon", "path": "compat/forward_compatibility_horizon", "type": "tf.compat", "text": "\nContext manager for testing forward compatibility of generated graphs.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.forward_compatibility_horizon`\n\nSee Version compatibility.\n\nTo ensure forward compatibility of generated graphs (see `forward_compatible`)\nwith older binaries, new features can be gated with:\n\nHowever, when adding new features, one may want to unittest it before the\nforward compatibility window expires. This context manager enables such tests.\nFor example:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.forward_compatible", "path": "compat/forward_compatible", "type": "tf.compat", "text": "\nReturn true if the forward compatibility window has expired.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.forward_compatible`\n\nSee Version compatibility.\n\nForward-compatibility refers to scenarios where the producer of a TensorFlow\nmodel (a GraphDef or SavedModel) is compiled against a version of the\nTensorFlow library newer than what the consumer was compiled against. The\n\"producer\" is typically a Python program that constructs and trains a model\nwhile the \"consumer\" is typically another program that loads and serves the\nmodel.\n\nTensorFlow has been supporting a 3 week forward-compatibility window for\nprograms compiled from source at HEAD.\n\nFor example, consider the case where a new operation `MyNewAwesomeAdd` is\ncreated with the intent of replacing the implementation of an existing Python\nwrapper - `tf.add`. The Python wrapper implementation should change from\nsomething like:\n\nto:\n\nWhere `year`, `month`, and `day` specify the date beyond which binaries that\nconsume a model are expected to have been updated to include the new\noperations. This date is typically at least 3 weeks beyond the date the code\nthat adds the new operation is committed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.path_to_str", "path": "compat/path_to_str", "type": "tf.compat", "text": "\nConverts input which is a `PathLike` object to `str` type.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.compat.path_to_str`\n\nConverts from any python constant representation of a `PathLike` object to a\nstring. If the input is not a `PathLike` object, simply returns the input.\n\nIn case a simplified `str` version of the path is needed from an `os.PathLike`\nobject\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1", "path": "compat/v1", "type": "tf.compat", "text": "\nBring in all of the public TensorFlow interface into this module.\n\n`app` module: Generic entry point script.\n\n`audio` module: Public API for tf.audio namespace.\n\n`autograph` module: Conversion of plain Python into TensorFlow graph code.\n\n`bitwise` module: Operations for manipulating the binary representations of\nintegers.\n\n`compat` module: Compatibility functions.\n\n`config` module: Public API for tf.config namespace.\n\n`data` module: `tf.data.Dataset` API for input pipelines.\n\n`debugging` module: Public API for tf.debugging namespace.\n\n`distribute` module: Library for running a computation across multiple\ndevices.\n\n`distributions` module: Core module for TensorFlow distribution objects and\nhelpers.\n\n`dtypes` module: Public API for tf.dtypes namespace.\n\n`errors` module: Exception types for TensorFlow errors.\n\n`estimator` module: Estimator: High level tools for working with models.\n\n`experimental` module: Public API for tf.experimental namespace.\n\n`feature_column` module: Public API for tf.feature_column namespace.\n\n`flags` module: Import router for absl.flags. See\nhttps://github.com/abseil/abseil-py\n\n`gfile` module: Import router for file_io.\n\n`graph_util` module: Helpers to manipulate a tensor graph in python.\n\n`image` module: Image ops.\n\n`initializers` module: Public API for tf.initializers namespace.\n\n`io` module: Public API for tf.io namespace.\n\n`keras` module: Implementation of the Keras API meant to be a high-level API\nfor TensorFlow.\n\n`layers` module: Public API for tf.layers namespace.\n\n`linalg` module: Operations for linear algebra.\n\n`lite` module: Public API for tf.lite namespace.\n\n`logging` module: Logging and Summary Operations.\n\n`lookup` module: Public API for tf.lookup namespace.\n\n`losses` module: Loss operations for use in neural networks.\n\n`manip` module: Operators for manipulating tensors.\n\n`math` module: Math Operations.\n\n`metrics` module: Evaluation-related metrics.\n\n`mixed_precision` module: Public API for tf.mixed_precision namespace.\n\n`mlir` module: Public API for tf.mlir namespace.\n\n`nest` module: Public API for tf.nest namespace.\n\n`nn` module: Wrappers for primitive Neural Net (NN) Operations.\n\n`profiler` module: Public API for tf.profiler namespace.\n\n`python_io` module: Python functions for directly manipulating TFRecord-\nformatted files.\n\n`quantization` module: Public API for tf.quantization namespace.\n\n`queue` module: Public API for tf.queue namespace.\n\n`ragged` module: Ragged Tensors.\n\n`random` module: Public API for tf.random namespace.\n\n`raw_ops` module: Public API for tf.raw_ops namespace.\n\n`resource_loader` module: Resource management library.\n\n`saved_model` module: Public API for tf.saved_model namespace.\n\n`sets` module: Tensorflow set operations.\n\n`signal` module: Signal processing operations.\n\n`sparse` module: Sparse Tensor Representation.\n\n`spectral` module: Public API for tf.spectral namespace.\n\n`strings` module: Operations for working with string Tensors.\n\n`summary` module: Operations for writing summary data, for use in analysis and\nvisualization.\n\n`sysconfig` module: System configuration library.\n\n`test` module: Testing.\n\n`tpu` module: Ops related to Tensor Processing Units.\n\n`train` module: Support for training models.\n\n`types` module: Public TensorFlow type definitions.\n\n`user_ops` module: Public API for tf.user_ops namespace.\n\n`version` module: Public API for tf.version namespace.\n\n`xla` module: Public API for tf.xla namespace.\n\n`class AggregationMethod`: A class listing aggregation methods used to combine\ngradients.\n\n`class AttrValue`: A ProtocolMessage\n\n`class ConditionalAccumulator`: A conditional accumulator for aggregating\ngradients.\n\n`class ConditionalAccumulatorBase`: A conditional accumulator for aggregating\ngradients.\n\n`class ConfigProto`: A ProtocolMessage\n\n`class CriticalSection`: Critical section.\n\n`class DType`: Represents the type of the elements in a `Tensor`.\n\n`class DeviceSpec`: Represents a (possibly partial) specification for a\nTensorFlow device.\n\n`class Dimension`: Represents the value of one dimension in a TensorShape.\n\n`class Event`: A ProtocolMessage\n\n`class FIFOQueue`: A queue implementation that dequeues elements in first-in\nfirst-out order.\n\n`class FixedLenFeature`: Configuration for parsing a fixed-length input\nfeature.\n\n`class FixedLenSequenceFeature`: Configuration for parsing a variable-length\ninput feature into a `Tensor`.\n\n`class FixedLengthRecordReader`: A Reader that outputs fixed-length records\nfrom a file.\n\n`class GPUOptions`: A ProtocolMessage\n\n`class GradientTape`: Record operations for automatic differentiation.\n\n`class Graph`: A TensorFlow computation, represented as a dataflow graph.\n\n`class GraphDef`: A ProtocolMessage\n\n`class GraphKeys`: Standard names to use for graph collections.\n\n`class GraphOptions`: A ProtocolMessage\n\n`class HistogramProto`: A ProtocolMessage\n\n`class IdentityReader`: A Reader that outputs the queued work as both the key\nand value.\n\n`class IndexedSlices`: A sparse representation of a set of tensor slices at\ngiven indices.\n\n`class IndexedSlicesSpec`: Type specification for a `tf.IndexedSlices`.\n\n`class InteractiveSession`: A TensorFlow `Session` for use in interactive\ncontexts, such as a shell.\n\n`class LMDBReader`: A Reader that outputs the records from a LMDB file.\n\n`class LogMessage`: A ProtocolMessage\n\n`class MetaGraphDef`: A ProtocolMessage\n\n`class Module`: Base neural network module class.\n\n`class NameAttrList`: A ProtocolMessage\n\n`class NodeDef`: A ProtocolMessage\n\n`class OpError`: A generic error that is raised when TensorFlow execution\nfails.\n\n`class Operation`: Represents a graph node that performs computation on\ntensors.\n\n`class OptimizerOptions`: A ProtocolMessage\n\n`class OptionalSpec`: Type specification for `tf.experimental.Optional`.\n\n`class PaddingFIFOQueue`: A FIFOQueue that supports batching variable-sized\ntensors by padding.\n\n`class PriorityQueue`: A queue implementation that dequeues elements in\nprioritized order.\n\n`class QueueBase`: Base class for queue implementations.\n\n`class RaggedTensor`: Represents a ragged tensor.\n\n`class RaggedTensorSpec`: Type specification for a `tf.RaggedTensor`.\n\n`class RandomShuffleQueue`: A queue implementation that dequeues elements in a\nrandom order.\n\n`class ReaderBase`: Base class for different Reader types, that produce a\nrecord every step.\n\n`class RegisterGradient`: A decorator for registering the gradient function\nfor an op type.\n\n`class RunMetadata`: A ProtocolMessage\n\n`class RunOptions`: A ProtocolMessage\n\n`class Session`: A class for running TensorFlow operations.\n\n`class SessionLog`: A ProtocolMessage\n\n`class SparseConditionalAccumulator`: A conditional accumulator for\naggregating sparse gradients.\n\n`class SparseFeature`: Configuration for parsing a sparse input feature from\nan `Example`.\n\n`class SparseTensor`: Represents a sparse tensor.\n\n`class SparseTensorSpec`: Type specification for a `tf.sparse.SparseTensor`.\n\n`class SparseTensorValue`: SparseTensorValue(indices, values, dense_shape)\n\n`class Summary`: A ProtocolMessage\n\n`class SummaryMetadata`: A ProtocolMessage\n\n`class TFRecordReader`: A Reader that outputs the records from a TFRecords\nfile.\n\n`class Tensor`: A tensor is a multidimensional array of elements represented\nby a\n\n`class TensorArray`: Class wrapping dynamic-sized, per-time-step, write-once\nTensor arrays.\n\n`class TensorArraySpec`: Type specification for a `tf.TensorArray`.\n\n`class TensorInfo`: A ProtocolMessage\n\n`class TensorShape`: Represents the shape of a `Tensor`.\n\n`class TensorSpec`: Describes a tf.Tensor.\n\n`class TextLineReader`: A Reader that outputs the lines of a file delimited by\nnewlines.\n\n`class TypeSpec`: Specifies a TensorFlow value type.\n\n`class UnconnectedGradients`: Controls how gradient computation behaves when y\ndoes not depend on x.\n\n`class VarLenFeature`: Configuration for parsing a variable-length input\nfeature.\n\n`class Variable`: See the Variables Guide.\n\n`class VariableAggregation`: Indicates how a distributed variable will be\naggregated.\n\n`class VariableScope`: Variable scope object to carry defaults to provide to\n`get_variable`.\n\n`class VariableSynchronization`: Indicates when a distributed variable will be\nsynced.\n\n`class WholeFileReader`: A Reader that outputs the entire contents of a file\nas a value.\n\n`class constant_initializer`: Initializer that generates tensors with constant\nvalues.\n\n`class glorot_normal_initializer`: The Glorot normal initializer, also called\nXavier normal initializer.\n\n`class glorot_uniform_initializer`: The Glorot uniform initializer, also\ncalled Xavier uniform initializer.\n\n`class name_scope`: A context manager for use when defining a Python op.\n\n`class ones_initializer`: Initializer that generates tensors initialized to 1.\n\n`class orthogonal_initializer`: Initializer that generates an orthogonal\nmatrix.\n\n`class random_normal_initializer`: Initializer that generates tensors with a\nnormal distribution.\n\n`class random_uniform_initializer`: Initializer that generates tensors with a\nuniform distribution.\n\n`class truncated_normal_initializer`: Initializer that generates a truncated\nnormal distribution.\n\n`class uniform_unit_scaling_initializer`: Initializer that generates tensors\nwithout scaling variance.\n\n`class variable_scope`: A context manager for defining ops that creates\nvariables (layers).\n\n`class variance_scaling_initializer`: Initializer capable of adapting its\nscale to the shape of weights tensors.\n\n`class zeros_initializer`: Initializer that generates tensors initialized to\n0.\n\n`Assert(...)`: Asserts that the given condition is true.\n\n`NoGradient(...)`: Specifies that ops of type `op_type` is not differentiable.\n\n`NotDifferentiable(...)`: Specifies that ops of type `op_type` is not\ndifferentiable.\n\n`Print(...)`: Prints a list of tensors. (deprecated)\n\n`abs(...)`: Computes the absolute value of a tensor.\n\n`accumulate_n(...)`: Returns the element-wise sum of a list of tensors.\n\n`acos(...)`: Computes acos of x element-wise.\n\n`acosh(...)`: Computes inverse hyperbolic cosine of x element-wise.\n\n`add(...)`: Returns x + y element-wise.\n\n`add_check_numerics_ops(...)`: Connect a `tf.debugging.check_numerics` to\nevery floating point tensor.\n\n`add_n(...)`: Adds all input tensors element-wise.\n\n`add_to_collection(...)`: Wrapper for `Graph.add_to_collection()` using the\ndefault graph.\n\n`add_to_collections(...)`: Wrapper for `Graph.add_to_collections()` using the\ndefault graph.\n\n`all_variables(...)`: Use `tf.compat.v1.global_variables` instead.\n(deprecated)\n\n`angle(...)`: Returns the element-wise argument of a complex (or real) tensor.\n\n`arg_max(...)`: Returns the index with the largest value across dimensions of\na tensor.\n\n`arg_min(...)`: Returns the index with the smallest value across dimensions of\na tensor.\n\n`argmax(...)`: Returns the index with the largest value across axes of a\ntensor. (deprecated arguments)\n\n`argmin(...)`: Returns the index with the smallest value across axes of a\ntensor. (deprecated arguments)\n\n`argsort(...)`: Returns the indices of a tensor that give its sorted order\nalong an axis.\n\n`as_dtype(...)`: Converts the given `type_value` to a `DType`.\n\n`as_string(...)`: Converts each entry in the given tensor to strings.\n\n`asin(...)`: Computes the trignometric inverse sine of x element-wise.\n\n`asinh(...)`: Computes inverse hyperbolic sine of x element-wise.\n\n`assert_equal(...)`: Assert the condition `x == y` holds element-wise.\n\n`assert_greater(...)`: Assert the condition `x > y` holds element-wise.\n\n`assert_greater_equal(...)`: Assert the condition `x >= y` holds element-wise.\n\n`assert_integer(...)`: Assert that `x` is of integer dtype.\n\n`assert_less(...)`: Assert the condition `x < y` holds element-wise.\n\n`assert_less_equal(...)`: Assert the condition `x <= y` holds element-wise.\n\n`assert_near(...)`: Assert the condition `x` and `y` are close element-wise.\n\n`assert_negative(...)`: Assert the condition `x < 0` holds element-wise.\n\n`assert_non_negative(...)`: Assert the condition `x >= 0` holds element-wise.\n\n`assert_non_positive(...)`: Assert the condition `x <= 0` holds element-wise.\n\n`assert_none_equal(...)`: Assert the condition `x != y` holds element-wise.\n\n`assert_positive(...)`: Assert the condition `x > 0` holds element-wise.\n\n`assert_proper_iterable(...)`: Static assert that values is a \"proper\"\niterable.\n\n`assert_rank(...)`: Assert `x` has rank equal to `rank`.\n\n`assert_rank_at_least(...)`: Assert `x` has rank equal to `rank` or higher.\n\n`assert_rank_in(...)`: Assert `x` has rank in `ranks`.\n\n`assert_same_float_dtype(...)`: Validate and return float type based on\n`tensors` and `dtype`.\n\n`assert_scalar(...)`: Asserts that the given `tensor` is a scalar (i.e. zero-\ndimensional).\n\n`assert_type(...)`: Statically asserts that the given `Tensor` is of the\nspecified type.\n\n`assert_variables_initialized(...)`: Returns an Op to check if variables are\ninitialized.\n\n`assign(...)`: Update `ref` by assigning `value` to it.\n\n`assign_add(...)`: Update `ref` by adding `value` to it.\n\n`assign_sub(...)`: Update `ref` by subtracting `value` from it.\n\n`atan(...)`: Computes the trignometric inverse tangent of x element-wise.\n\n`atan2(...)`: Computes arctangent of `y/x` element-wise, respecting signs of\nthe arguments.\n\n`atanh(...)`: Computes inverse hyperbolic tangent of x element-wise.\n\n`batch_gather(...)`: Gather slices from params according to indices with\nleading batch dims. (deprecated)\n\n`batch_scatter_update(...)`: Generalization of `tf.compat.v1.scatter_update`\nto axis different than 0. (deprecated)\n\n`batch_to_space(...)`: BatchToSpace for 4-D tensors of type T.\n\n`batch_to_space_nd(...)`: BatchToSpace for N-D tensors of type T.\n\n`betainc(...)`: Compute the regularized incomplete beta integral \\\\(I_x(a,\nb)\\\\).\n\n`bincount(...)`: Counts the number of occurrences of each value in an integer\narray.\n\n`bitcast(...)`: Bitcasts a tensor from one type to another without copying\ndata.\n\n`boolean_mask(...)`: Apply boolean mask to tensor.\n\n`broadcast_dynamic_shape(...)`: Computes the shape of a broadcast given\nsymbolic shapes.\n\n`broadcast_static_shape(...)`: Computes the shape of a broadcast given known\nshapes.\n\n`broadcast_to(...)`: Broadcast an array for a compatible shape.\n\n`case(...)`: Create a case operation.\n\n`cast(...)`: Casts a tensor to a new type.\n\n`ceil(...)`: Return the ceiling of the input, element-wise.\n\n`check_numerics(...)`: Checks a tensor for NaN and Inf values.\n\n`cholesky(...)`: Computes the Cholesky decomposition of one or more square\nmatrices.\n\n`cholesky_solve(...)`: Solves systems of linear eqns `A X = RHS`, given\nCholesky factorizations.\n\n`clip_by_average_norm(...)`: Clips tensor values to a maximum average L2-norm.\n(deprecated)\n\n`clip_by_global_norm(...)`: Clips values of multiple tensors by the ratio of\nthe sum of their norms.\n\n`clip_by_norm(...)`: Clips tensor values to a maximum L2-norm.\n\n`clip_by_value(...)`: Clips tensor values to a specified min and max.\n\n`colocate_with(...)`: DEPRECATED FUNCTION\n\n`complex(...)`: Converts two real numbers to a complex number.\n\n`concat(...)`: Concatenates tensors along one dimension.\n\n`cond(...)`: Return `true_fn()` if the predicate `pred` is true else\n`false_fn()`. (deprecated arguments)\n\n`confusion_matrix(...)`: Computes the confusion matrix from predictions and\nlabels.\n\n`conj(...)`: Returns the complex conjugate of a complex number.\n\n`constant(...)`: Creates a constant tensor.\n\n`container(...)`: Wrapper for `Graph.container()` using the default graph.\n\n`control_dependencies(...)`: Wrapper for `Graph.control_dependencies()` using\nthe default graph.\n\n`control_flow_v2_enabled(...)`: Returns `True` if v2 control flow is enabled.\n\n`convert_to_tensor(...)`: Converts the given `value` to a `Tensor`.\n\n`convert_to_tensor_or_indexed_slices(...)`: Converts the given object to a\n`Tensor` or an `IndexedSlices`.\n\n`convert_to_tensor_or_sparse_tensor(...)`: Converts value to a `SparseTensor`\nor `Tensor`.\n\n`cos(...)`: Computes cos of x element-wise.\n\n`cosh(...)`: Computes hyperbolic cosine of x element-wise.\n\n`count_nonzero(...)`: Computes number of nonzero elements across dimensions of\na tensor. (deprecated arguments) (deprecated arguments)\n\n`count_up_to(...)`: Increments 'ref' until it reaches 'limit'. (deprecated)\n\n`create_partitioned_variables(...)`: Create a list of partitioned variables\naccording to the given `slicing`. (deprecated)\n\n`cross(...)`: Compute the pairwise cross product.\n\n`cumprod(...)`: Compute the cumulative product of the tensor `x` along `axis`.\n\n`cumsum(...)`: Compute the cumulative sum of the tensor `x` along `axis`.\n\n`custom_gradient(...)`: Decorator to define a function with a custom gradient.\n\n`decode_base64(...)`: Decode web-safe base64-encoded strings.\n\n`decode_compressed(...)`: Decompress strings.\n\n`decode_csv(...)`: Convert CSV records to tensors. Each column maps to one\ntensor.\n\n`decode_json_example(...)`: Convert JSON-encoded Example records to binary\nprotocol buffer strings.\n\n`decode_raw(...)`: Convert raw byte strings into tensors. (deprecated\narguments)\n\n`delete_session_tensor(...)`: Delete the tensor for the given tensor handle.\n\n`depth_to_space(...)`: DepthToSpace for tensors of type T.\n\n`dequantize(...)`: Dequantize the 'input' tensor into a float or bfloat16\nTensor.\n\n`deserialize_many_sparse(...)`: Deserialize and concatenate `SparseTensors`\nfrom a serialized minibatch.\n\n`device(...)`: Wrapper for `Graph.device()` using the default graph.\n\n`diag(...)`: Returns a diagonal tensor with a given diagonal values.\n\n`diag_part(...)`: Returns the diagonal part of the tensor.\n\n`digamma(...)`: Computes Psi, the derivative of Lgamma (the log of the\nabsolute value of\n\n`dimension_at_index(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`dimension_value(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`disable_control_flow_v2(...)`: Opts out of control flow v2.\n\n`disable_eager_execution(...)`: Disables eager execution.\n\n`disable_resource_variables(...)`: Opts out of resource variables.\n(deprecated)\n\n`disable_tensor_equality(...)`: Compare Tensors by their id and be hashable.\n\n`disable_v2_behavior(...)`: Disables TensorFlow 2.x behaviors.\n\n`disable_v2_tensorshape(...)`: Disables the V2 TensorShape behavior and\nreverts to V1 behavior.\n\n`div(...)`: Divides x / y elementwise (using Python 2 division operator\nsemantics). (deprecated)\n\n`div_no_nan(...)`: Computes a safe divide which returns 0 if the y is zero.\n\n`divide(...)`: Computes Python style division of `x` by `y`.\n\n`dynamic_partition(...)`: Partitions `data` into `num_partitions` tensors\nusing indices from `partitions`.\n\n`dynamic_stitch(...)`: Interleave the values from the `data` tensors into a\nsingle tensor.\n\n`edit_distance(...)`: Computes the Levenshtein distance between sequences.\n\n`einsum(...)`: Tensor contraction over specified indices and outer product.\n\n`enable_control_flow_v2(...)`: Use control flow v2.\n\n`enable_eager_execution(...)`: Enables eager execution for the lifetime of\nthis program.\n\n`enable_resource_variables(...)`: Creates resource variables by default.\n\n`enable_tensor_equality(...)`: Compare Tensors with element-wise comparison\nand thus be unhashable.\n\n`enable_v2_behavior(...)`: Enables TensorFlow 2.x behaviors.\n\n`enable_v2_tensorshape(...)`: In TensorFlow 2.0, iterating over a TensorShape\ninstance returns values.\n\n`encode_base64(...)`: Encode strings into web-safe base64 format.\n\n`ensure_shape(...)`: Updates the shape of a tensor and checks at runtime that\nthe shape holds.\n\n`equal(...)`: Returns the truth value of (x == y) element-wise.\n\n`erf(...)`: Computes the Gauss error function of `x` element-wise.\n\n`erfc(...)`: Computes the complementary error function of `x` element-wise.\n\n`executing_eagerly(...)`: Checks whether the current thread has eager\nexecution enabled.\n\n`executing_eagerly_outside_functions(...)`: Returns True if executing eagerly,\neven if inside a graph function.\n\n`exp(...)`: Computes exponential of x element-wise. \\\\(y = e^x\\\\).\n\n`expand_dims(...)`: Returns a tensor with a length 1 axis inserted at index\n`axis`. (deprecated arguments)\n\n`expm1(...)`: Computes `exp(x) - 1` element-wise.\n\n`extract_image_patches(...)`: Extract `patches` from `images` and put them in\nthe \"depth\" output dimension.\n\n`extract_volume_patches(...)`: Extract `patches` from `input` and put them in\nthe `\"depth\"` output dimension. 3D extension of `extract_image_patches`.\n\n`eye(...)`: Construct an identity matrix, or a batch of matrices.\n\n`fake_quant_with_min_max_args(...)`: Fake-quantize the 'inputs' tensor, type\nfloat to 'outputs' tensor of same type.\n\n`fake_quant_with_min_max_args_gradient(...)`: Compute gradients for a\nFakeQuantWithMinMaxArgs operation.\n\n`fake_quant_with_min_max_vars(...)`: Fake-quantize the 'inputs' tensor of type\nfloat via global float scalars\n\n`fake_quant_with_min_max_vars_gradient(...)`: Compute gradients for a\nFakeQuantWithMinMaxVars operation.\n\n`fake_quant_with_min_max_vars_per_channel(...)`: Fake-quantize the 'inputs'\ntensor of type float via per-channel floats\n\n`fake_quant_with_min_max_vars_per_channel_gradient(...)`: Compute gradients\nfor a FakeQuantWithMinMaxVarsPerChannel operation.\n\n`fft(...)`: Fast Fourier transform.\n\n`fft2d(...)`: 2D fast Fourier transform.\n\n`fft3d(...)`: 3D fast Fourier transform.\n\n`fill(...)`: Creates a tensor filled with a scalar value.\n\n`fingerprint(...)`: Generates fingerprint values.\n\n`fixed_size_partitioner(...)`: Partitioner to specify a fixed number of shards\nalong given axis.\n\n`floor(...)`: Returns element-wise largest integer not greater than x.\n\n`floor_div(...)`: Returns x // y element-wise.\n\n`floordiv(...)`: Divides `x / y` elementwise, rounding toward the most\nnegative integer.\n\n`floormod(...)`: Returns element-wise remainder of division. When `x < 0` xor\n`y < 0` is\n\n`foldl(...)`: foldl on the list of tensors unpacked from `elems` on dimension\n0.\n\n`foldr(...)`: foldr on the list of tensors unpacked from `elems` on dimension\n0.\n\n`function(...)`: Compiles a function into a callable TensorFlow graph.\n\n`gather(...)`: Gather slices from params axis `axis` according to indices.\n\n`gather_nd(...)`: Gather slices from `params` into a Tensor with shape\nspecified by `indices`.\n\n`get_collection(...)`: Wrapper for `Graph.get_collection()` using the default\ngraph.\n\n`get_collection_ref(...)`: Wrapper for `Graph.get_collection_ref()` using the\ndefault graph.\n\n`get_default_graph(...)`: Returns the default graph for the current thread.\n\n`get_default_session(...)`: Returns the default session for the current\nthread.\n\n`get_local_variable(...)`: Gets an existing local variable or creates a new\none.\n\n`get_logger(...)`: Return TF logger instance.\n\n`get_seed(...)`: Returns the local seeds an operation should use given an op-\nspecific seed.\n\n`get_session_handle(...)`: Return the handle of `data`.\n\n`get_session_tensor(...)`: Get the tensor of type `dtype` by feeding a tensor\nhandle.\n\n`get_static_value(...)`: Returns the constant value of the given tensor, if\nefficiently calculable.\n\n`get_variable(...)`: Gets an existing variable with these parameters or create\na new one.\n\n`get_variable_scope(...)`: Returns the current variable scope.\n\n`global_norm(...)`: Computes the global norm of multiple tensors.\n\n`global_variables(...)`: Returns global variables.\n\n`global_variables_initializer(...)`: Returns an Op that initializes global\nvariables.\n\n`grad_pass_through(...)`: Creates a grad-pass-through op with the forward\nbehavior provided in f.\n\n`gradients(...)`: Constructs symbolic derivatives of sum of `ys` w.r.t. x in\n`xs`.\n\n`greater(...)`: Returns the truth value of (x > y) element-wise.\n\n`greater_equal(...)`: Returns the truth value of (x >= y) element-wise.\n\n`group(...)`: Create an op that groups multiple operations.\n\n`guarantee_const(...)`: Gives a guarantee to the TF runtime that the input\ntensor is a constant.\n\n`hessians(...)`: Constructs the Hessian of sum of `ys` with respect to `x` in\n`xs`.\n\n`histogram_fixed_width(...)`: Return histogram of values.\n\n`histogram_fixed_width_bins(...)`: Bins the given values for use in a\nhistogram.\n\n`identity(...)`: Return a Tensor with the same shape and contents as input.\n\n`identity_n(...)`: Returns a list of tensors with the same shapes and contents\nas the input\n\n`ifft(...)`: Inverse fast Fourier transform.\n\n`ifft2d(...)`: Inverse 2D fast Fourier transform.\n\n`ifft3d(...)`: Inverse 3D fast Fourier transform.\n\n`igamma(...)`: Compute the lower regularized incomplete Gamma function `P(a,\nx)`.\n\n`igammac(...)`: Compute the upper regularized incomplete Gamma function `Q(a,\nx)`.\n\n`imag(...)`: Returns the imaginary part of a complex (or real) tensor.\n\n`import_graph_def(...)`: Imports the graph from `graph_def` into the current\ndefault `Graph`. (deprecated arguments)\n\n`init_scope(...)`: A context manager that lifts ops out of control-flow scopes\nand function-building graphs.\n\n`initialize_all_tables(...)`: Returns an Op that initializes all tables of the\ndefault graph. (deprecated)\n\n`initialize_all_variables(...)`: See\n`tf.compat.v1.global_variables_initializer`. (deprecated)\n\n`initialize_local_variables(...)`: See\n`tf.compat.v1.local_variables_initializer`. (deprecated)\n\n`initialize_variables(...)`: See `tf.compat.v1.variables_initializer`.\n(deprecated)\n\n`invert_permutation(...)`: Computes the inverse permutation of a tensor.\n\n`is_finite(...)`: Returns which elements of x are finite.\n\n`is_inf(...)`: Returns which elements of x are Inf.\n\n`is_nan(...)`: Returns which elements of x are NaN.\n\n`is_non_decreasing(...)`: Returns `True` if `x` is non-decreasing.\n\n`is_numeric_tensor(...)`: Returns `True` if the elements of `tensor` are\nnumbers.\n\n`is_strictly_increasing(...)`: Returns `True` if `x` is strictly increasing.\n\n`is_tensor(...)`: Checks whether `x` is a TF-native type that can be passed to\nmany TF ops.\n\n`is_variable_initialized(...)`: Tests if a variable has been initialized.\n\n`lbeta(...)`: Computes \\\\(ln(|Beta(x)|)\\\\), reducing along the last dimension.\n\n`less(...)`: Returns the truth value of (x < y) element-wise.\n\n`less_equal(...)`: Returns the truth value of (x <= y) element-wise.\n\n`lgamma(...)`: Computes the log of the absolute value of `Gamma(x)` element-\nwise.\n\n`lin_space(...)`: Generates evenly-spaced values in an interval along a given\naxis.\n\n`linspace(...)`: Generates evenly-spaced values in an interval along a given\naxis.\n\n`load_file_system_library(...)`: Loads a TensorFlow plugin, containing file\nsystem implementation. (deprecated)\n\n`load_library(...)`: Loads a TensorFlow plugin.\n\n`load_op_library(...)`: Loads a TensorFlow plugin, containing custom ops and\nkernels.\n\n`local_variables(...)`: Returns local variables.\n\n`local_variables_initializer(...)`: Returns an Op that initializes all local\nvariables.\n\n`log(...)`: Computes natural logarithm of x element-wise.\n\n`log1p(...)`: Computes natural logarithm of (1 + x) element-wise.\n\n`log_sigmoid(...)`: Computes log sigmoid of `x` element-wise.\n\n`logical_and(...)`: Logical AND function.\n\n`logical_not(...)`: Returns the truth value of `NOT x` element-wise.\n\n`logical_or(...)`: Returns the truth value of x OR y element-wise.\n\n`logical_xor(...)`: Logical XOR function.\n\n`make_ndarray(...)`: Create a numpy ndarray from a tensor.\n\n`make_template(...)`: Given an arbitrary function, wrap it so that it does\nvariable sharing.\n\n`make_tensor_proto(...)`: Create a TensorProto.\n\n`map_fn(...)`: Transforms `elems` by applying `fn` to each element unstacked\non axis 0. (deprecated arguments)\n\n`matching_files(...)`: Returns the set of files matching one or more glob\npatterns.\n\n`matmul(...)`: Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n\n`matrix_band_part(...)`: Copy a tensor setting everything outside a central\nband in each innermost matrix to zero.\n\n`matrix_determinant(...)`: Computes the determinant of one or more square\nmatrices.\n\n`matrix_diag(...)`: Returns a batched diagonal tensor with given batched\ndiagonal values.\n\n`matrix_diag_part(...)`: Returns the batched diagonal part of a batched\ntensor.\n\n`matrix_inverse(...)`: Computes the inverse of one or more square invertible\nmatrices or their adjoints (conjugate transposes).\n\n`matrix_set_diag(...)`: Returns a batched matrix tensor with new batched\ndiagonal values.\n\n`matrix_solve(...)`: Solves systems of linear equations.\n\n`matrix_solve_ls(...)`: Solves one or more linear least-squares problems.\n\n`matrix_square_root(...)`: Computes the matrix square root of one or more\nsquare matrices:\n\n`matrix_transpose(...)`: Transposes last two dimensions of tensor `a`.\n\n`matrix_triangular_solve(...)`: Solve systems of linear equations with upper\nor lower triangular matrices.\n\n`maximum(...)`: Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n\n`meshgrid(...)`: Broadcasts parameters for evaluation on an N-D grid.\n\n`min_max_variable_partitioner(...)`: Partitioner to allocate minimum size per\nslice.\n\n`minimum(...)`: Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n\n`mod(...)`: Returns element-wise remainder of division. When `x < 0` xor `y <\n0` is\n\n`model_variables(...)`: Returns all variables in the MODEL_VARIABLES\ncollection.\n\n`moving_average_variables(...)`: Returns all variables that maintain their\nmoving averages.\n\n`multinomial(...)`: Draws samples from a multinomial distribution.\n(deprecated)\n\n`multiply(...)`: Returns an element-wise x * y.\n\n`negative(...)`: Computes numerical negative value element-wise.\n\n`no_gradient(...)`: Specifies that ops of type `op_type` is not\ndifferentiable.\n\n`no_op(...)`: Does nothing. Only useful as a placeholder for control edges.\n\n`no_regularizer(...)`: Use this function to prevent regularization of\nvariables.\n\n`nondifferentiable_batch_function(...)`: Batches the computation done by the\ndecorated function.\n\n`norm(...)`: Computes the norm of vectors, matrices, and tensors. (deprecated\narguments)\n\n`not_equal(...)`: Returns the truth value of (x != y) element-wise.\n\n`numpy_function(...)`: Wraps a python function and uses it as a TensorFlow op.\n\n`one_hot(...)`: Returns a one-hot tensor.\n\n`ones(...)`: Creates a tensor with all elements set to one (1).\n\n`ones_like(...)`: Creates a tensor with all elements set to 1.\n\n`op_scope(...)`: DEPRECATED. Same as name_scope above, just different argument\norder.\n\n`pad(...)`: Pads a tensor.\n\n`parallel_stack(...)`: Stacks a list of rank-`R` tensors into one rank-`(R+1)`\ntensor in parallel.\n\n`parse_example(...)`: Parses `Example` protos into a `dict` of tensors.\n\n`parse_single_example(...)`: Parses a single `Example` proto.\n\n`parse_single_sequence_example(...)`: Parses a single `SequenceExample` proto.\n\n`parse_tensor(...)`: Transforms a serialized tensorflow.TensorProto proto into\na Tensor.\n\n`placeholder(...)`: Inserts a placeholder for a tensor that will be always\nfed.\n\n`placeholder_with_default(...)`: A placeholder op that passes through `input`\nwhen its output is not fed.\n\n`polygamma(...)`: Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).\n\n`pow(...)`: Computes the power of one value to another.\n\n`print(...)`: Print the specified inputs.\n\n`py_func(...)`: Wraps a python function and uses it as a TensorFlow op.\n\n`py_function(...)`: Wraps a python function into a TensorFlow op that executes\nit eagerly.\n\n`qr(...)`: Computes the QR decompositions of one or more matrices.\n\n`quantize(...)`: Quantize the 'input' tensor of type float to 'output' tensor\nof type 'T'.\n\n`quantize_and_dequantize_v4(...)`: Returns the gradient of\n`QuantizeAndDequantizeV4`.\n\n`quantize_v2(...)`: Please use `tf.quantization.quantize` instead.\n\n`quantized_concat(...)`: Concatenates quantized tensors along one dimension.\n\n`random_crop(...)`: Randomly crops a tensor to a given size.\n\n`random_gamma(...)`: Draws `shape` samples from each of the given Gamma\ndistribution(s).\n\n`random_normal(...)`: Outputs random values from a normal distribution.\n\n`random_poisson(...)`: Draws `shape` samples from each of the given Poisson\ndistribution(s).\n\n`random_shuffle(...)`: Randomly shuffles a tensor along its first dimension.\n\n`random_uniform(...)`: Outputs random values from a uniform distribution.\n\n`range(...)`: Creates a sequence of numbers.\n\n`rank(...)`: Returns the rank of a tensor.\n\n`read_file(...)`: Reads and outputs the entire contents of the input filename.\n\n`real(...)`: Returns the real part of a complex (or real) tensor.\n\n`realdiv(...)`: Returns x / y element-wise for real types.\n\n`reciprocal(...)`: Computes the reciprocal of x element-wise.\n\n`recompute_grad(...)`: An eager-compatible version of recompute_grad.\n\n`reduce_all(...)`: Computes the \"logical and\" of elements across dimensions of\na tensor. (deprecated arguments)\n\n`reduce_any(...)`: Computes the \"logical or\" of elements across dimensions of\na tensor. (deprecated arguments)\n\n`reduce_join(...)`: Joins all strings into a single string, or joins along an\naxis.\n\n`reduce_logsumexp(...)`: Computes log(sum(exp(elements across dimensions of a\ntensor))). (deprecated arguments)\n\n`reduce_max(...)`: Computes the maximum of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_mean(...)`: Computes the mean of elements across dimensions of a\ntensor.\n\n`reduce_min(...)`: Computes the minimum of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_prod(...)`: Computes the product of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_sum(...)`: Computes the sum of elements across dimensions of a tensor.\n(deprecated arguments)\n\n`regex_replace(...)`: Replace elements of `input` matching regex `pattern`\nwith `rewrite`.\n\n`register_tensor_conversion_function(...)`: Registers a function for\nconverting objects of `base_type` to `Tensor`.\n\n`repeat(...)`: Repeat elements of `input`.\n\n`report_uninitialized_variables(...)`: Adds ops to list the names of\nuninitialized variables.\n\n`required_space_to_batch_paddings(...)`: Calculate padding required to make\nblock_shape divide input_shape.\n\n`reset_default_graph(...)`: Clears the default graph stack and resets the\nglobal default graph.\n\n`reshape(...)`: Reshapes a tensor.\n\n`resource_variables_enabled(...)`: Returns `True` if resource variables are\nenabled.\n\n`reverse(...)`: Reverses specific dimensions of a tensor.\n\n`reverse_sequence(...)`: Reverses variable length slices. (deprecated\narguments) (deprecated arguments)\n\n`reverse_v2(...)`: Reverses specific dimensions of a tensor.\n\n`rint(...)`: Returns element-wise integer closest to x.\n\n`roll(...)`: Rolls the elements of a tensor along an axis.\n\n`round(...)`: Rounds the values of a tensor to the nearest integer, element-\nwise.\n\n`rsqrt(...)`: Computes reciprocal of square root of x element-wise.\n\n`saturate_cast(...)`: Performs a safe saturating cast of `value` to `dtype`.\n\n`scalar_mul(...)`: Multiplies a scalar times a `Tensor` or `IndexedSlices`\nobject.\n\n`scan(...)`: scan on the list of tensors unpacked from `elems` on dimension 0.\n\n`scatter_add(...)`: Adds sparse updates to the variable referenced by\n`resource`.\n\n`scatter_div(...)`: Divides a variable reference by sparse updates.\n\n`scatter_max(...)`: Reduces sparse updates into a variable reference using the\n`max` operation.\n\n`scatter_min(...)`: Reduces sparse updates into a variable reference using the\n`min` operation.\n\n`scatter_mul(...)`: Multiplies sparse updates into a variable reference.\n\n`scatter_nd(...)`: Scatter `updates` into a new tensor according to `indices`.\n\n`scatter_nd_add(...)`: Applies sparse addition to individual values or slices\nin a Variable.\n\n`scatter_nd_sub(...)`: Applies sparse subtraction to individual values or\nslices in a Variable.\n\n`scatter_nd_update(...)`: Applies sparse `updates` to individual values or\nslices in a Variable.\n\n`scatter_sub(...)`: Subtracts sparse updates to a variable reference.\n\n`scatter_update(...)`: Applies sparse updates to a variable reference.\n\n`searchsorted(...)`: Searches input tensor for values on the innermost\ndimension.\n\n`segment_max(...)`: Computes the maximum along segments of a tensor.\n\n`segment_mean(...)`: Computes the mean along segments of a tensor.\n\n`segment_min(...)`: Computes the minimum along segments of a tensor.\n\n`segment_prod(...)`: Computes the product along segments of a tensor.\n\n`segment_sum(...)`: Computes the sum along segments of a tensor.\n\n`self_adjoint_eig(...)`: Computes the eigen decomposition of a batch of self-\nadjoint matrices.\n\n`self_adjoint_eigvals(...)`: Computes the eigenvalues of one or more self-\nadjoint matrices.\n\n`sequence_mask(...)`: Returns a mask tensor representing the first N positions\nof each cell.\n\n`serialize_many_sparse(...)`: Serialize `N`-minibatch `SparseTensor` into an\n`[N, 3]` `Tensor`.\n\n`serialize_sparse(...)`: Serialize a `SparseTensor` into a 3-vector (1-D\n`Tensor`) object.\n\n`serialize_tensor(...)`: Transforms a Tensor into a serialized TensorProto\nproto.\n\n`set_random_seed(...)`: Sets the graph-level random seed for the default\ngraph.\n\n`setdiff1d(...)`: Computes the difference between two lists of numbers or\nstrings.\n\n`shape(...)`: Returns the shape of a tensor.\n\n`shape_n(...)`: Returns shape of tensors.\n\n`sigmoid(...)`: Computes sigmoid of `x` element-wise.\n\n`sign(...)`: Returns an element-wise indication of the sign of a number.\n\n`sin(...)`: Computes sine of x element-wise.\n\n`sinh(...)`: Computes hyperbolic sine of x element-wise.\n\n`size(...)`: Returns the size of a tensor.\n\n`slice(...)`: Extracts a slice from a tensor.\n\n`sort(...)`: Sorts a tensor.\n\n`space_to_batch(...)`: SpaceToBatch for 4-D tensors of type T.\n\n`space_to_batch_nd(...)`: SpaceToBatch for N-D tensors of type T.\n\n`space_to_depth(...)`: SpaceToDepth for tensors of type T.\n\n`sparse_add(...)`: Adds two tensors, at least one of each is a `SparseTensor`.\n(deprecated arguments)\n\n`sparse_concat(...)`: Concatenates a list of `SparseTensor` along the\nspecified dimension. (deprecated arguments)\n\n`sparse_fill_empty_rows(...)`: Fills empty rows in the input 2-D\n`SparseTensor` with a default value.\n\n`sparse_mask(...)`: Masks elements of `IndexedSlices`.\n\n`sparse_matmul(...)`: Multiply matrix \"a\" by matrix \"b\".\n\n`sparse_maximum(...)`: Returns the element-wise max of two SparseTensors.\n\n`sparse_merge(...)`: Combines a batch of feature ids and values into a single\n`SparseTensor`. (deprecated)\n\n`sparse_minimum(...)`: Returns the element-wise min of two SparseTensors.\n\n`sparse_placeholder(...)`: Inserts a placeholder for a sparse tensor that will\nbe always fed.\n\n`sparse_reduce_max(...)`: Computes the max of elements across dimensions of a\nSparseTensor. (deprecated arguments) (deprecated arguments)\n\n`sparse_reduce_max_sparse(...)`: Computes the max of elements across\ndimensions of a SparseTensor. (deprecated arguments)\n\n`sparse_reduce_sum(...)`: Computes the sum of elements across dimensions of a\nSparseTensor. (deprecated arguments) (deprecated arguments)\n\n`sparse_reduce_sum_sparse(...)`: Computes the sum of elements across\ndimensions of a SparseTensor. (deprecated arguments)\n\n`sparse_reorder(...)`: Reorders a `SparseTensor` into the canonical, row-major\nordering.\n\n`sparse_reset_shape(...)`: Resets the shape of a `SparseTensor` with indices\nand values unchanged.\n\n`sparse_reshape(...)`: Reshapes a `SparseTensor` to represent values in a new\ndense shape.\n\n`sparse_retain(...)`: Retains specified non-empty values within a\n`SparseTensor`.\n\n`sparse_segment_mean(...)`: Computes the mean along sparse segments of a\ntensor.\n\n`sparse_segment_sqrt_n(...)`: Computes the sum along sparse segments of a\ntensor divided by the sqrt(N).\n\n`sparse_segment_sum(...)`: Computes the sum along sparse segments of a tensor.\n\n`sparse_slice(...)`: Slice a `SparseTensor` based on the `start` and `size.\n\n`sparse_softmax(...)`: Applies softmax to a batched N-D `SparseTensor`.\n\n`sparse_split(...)`: Split a `SparseTensor` into `num_split` tensors along\n`axis`. (deprecated arguments)\n\n`sparse_tensor_dense_matmul(...)`: Multiply SparseTensor (or dense Matrix) (of\nrank 2) \"A\" by dense matrix\n\n`sparse_tensor_to_dense(...)`: Converts a `SparseTensor` into a dense tensor.\n\n`sparse_to_dense(...)`: Converts a sparse representation into a dense tensor.\n(deprecated)\n\n`sparse_to_indicator(...)`: Converts a `SparseTensor` of ids into a dense bool\nindicator tensor.\n\n`sparse_transpose(...)`: Transposes a `SparseTensor`\n\n`split(...)`: Splits a tensor `value` into a list of sub tensors.\n\n`sqrt(...)`: Computes element-wise square root of the input tensor.\n\n`square(...)`: Computes square of x element-wise.\n\n`squared_difference(...)`: Returns conj(x - y)(x - y) element-wise.\n\n`squeeze(...)`: Removes dimensions of size 1 from the shape of a tensor.\n(deprecated arguments)\n\n`stack(...)`: Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n\n`stop_gradient(...)`: Stops gradient computation.\n\n`strided_slice(...)`: Extracts a strided slice of a tensor (generalized Python\narray indexing).\n\n`string_join(...)`: Perform element-wise concatenation of a list of string\ntensors.\n\n`string_split(...)`: Split elements of `source` based on `delimiter`.\n(deprecated arguments)\n\n`string_strip(...)`: Strip leading and trailing whitespaces from the Tensor.\n\n`string_to_hash_bucket(...)`: Converts each string in the input Tensor to its\nhash mod by a number of buckets.\n\n`string_to_hash_bucket_fast(...)`: Converts each string in the input Tensor to\nits hash mod by a number of buckets.\n\n`string_to_hash_bucket_strong(...)`: Converts each string in the input Tensor\nto its hash mod by a number of buckets.\n\n`string_to_number(...)`: Converts each string in the input Tensor to the\nspecified numeric type.\n\n`substr(...)`: Return substrings from `Tensor` of strings.\n\n`subtract(...)`: Returns x - y element-wise.\n\n`svd(...)`: Computes the singular value decompositions of one or more\nmatrices.\n\n`switch_case(...)`: Create a switch/case operation, i.e. an integer-indexed\nconditional.\n\n`tables_initializer(...)`: Returns an Op that initializes all tables of the\ndefault graph.\n\n`tan(...)`: Computes tan of x element-wise.\n\n`tanh(...)`: Computes hyperbolic tangent of `x` element-wise.\n\n`tensor_scatter_add(...)`: Adds sparse `updates` to an existing tensor\naccording to `indices`.\n\n`tensor_scatter_nd_add(...)`: Adds sparse `updates` to an existing tensor\naccording to `indices`.\n\n`tensor_scatter_nd_max(...)`\n\n`tensor_scatter_nd_min(...)`\n\n`tensor_scatter_nd_sub(...)`: Subtracts sparse `updates` from an existing\ntensor according to `indices`.\n\n`tensor_scatter_nd_update(...)`: \"Scatter `updates` into an existing tensor\naccording to `indices`.\n\n`tensor_scatter_sub(...)`: Subtracts sparse `updates` from an existing tensor\naccording to `indices`.\n\n`tensor_scatter_update(...)`: \"Scatter `updates` into an existing tensor\naccording to `indices`.\n\n`tensordot(...)`: Tensor contraction of a and b along specified axes and outer\nproduct.\n\n`tile(...)`: Constructs a tensor by tiling a given tensor.\n\n`timestamp(...)`: Provides the time since epoch in seconds.\n\n`to_bfloat16(...)`: Casts a tensor to type `bfloat16`. (deprecated)\n\n`to_complex128(...)`: Casts a tensor to type `complex128`. (deprecated)\n\n`to_complex64(...)`: Casts a tensor to type `complex64`. (deprecated)\n\n`to_double(...)`: Casts a tensor to type `float64`. (deprecated)\n\n`to_float(...)`: Casts a tensor to type `float32`. (deprecated)\n\n`to_int32(...)`: Casts a tensor to type `int32`. (deprecated)\n\n`to_int64(...)`: Casts a tensor to type `int64`. (deprecated)\n\n`trace(...)`: Compute the trace of a tensor `x`.\n\n`trainable_variables(...)`: Returns all variables created with\n`trainable=True`.\n\n`transpose(...)`: Transposes `a`.\n\n`truediv(...)`: Divides x / y elementwise (using Python 3 division operator\nsemantics).\n\n`truncated_normal(...)`: Outputs random values from a truncated normal\ndistribution.\n\n`truncatediv(...)`: Returns x / y element-wise for integer types.\n\n`truncatemod(...)`: Returns element-wise remainder of division. This emulates\nC semantics in that\n\n`tuple(...)`: Group tensors together.\n\n`type_spec_from_value(...)`: Returns a `tf.TypeSpec` that represents the given\n`value`.\n\n`unique(...)`: Finds unique elements in a 1-D tensor.\n\n`unique_with_counts(...)`: Finds unique elements in a 1-D tensor.\n\n`unravel_index(...)`: Converts an array of flat indices into a tuple of\ncoordinate arrays.\n\n`unsorted_segment_max(...)`: Computes the maximum along segments of a tensor.\n\n`unsorted_segment_mean(...)`: Computes the mean along segments of a tensor.\n\n`unsorted_segment_min(...)`: Computes the minimum along segments of a tensor.\n\n`unsorted_segment_prod(...)`: Computes the product along segments of a tensor.\n\n`unsorted_segment_sqrt_n(...)`: Computes the sum along segments of a tensor\ndivided by the sqrt(N).\n\n`unsorted_segment_sum(...)`: Computes the sum along segments of a tensor.\n\n`unstack(...)`: Unpacks the given dimension of a rank-`R` tensor into\nrank-`(R-1)` tensors.\n\n`variable_axis_size_partitioner(...)`: Get a partitioner for VariableScope to\nkeep shards below `max_shard_bytes`.\n\n`variable_creator_scope(...)`: Scope which defines a variable creation\nfunction to be used by variable().\n\n`variable_op_scope(...)`: Deprecated: context manager for defining an op that\ncreates variables.\n\n`variables_initializer(...)`: Returns an Op that initializes a list of\nvariables.\n\n`vectorized_map(...)`: Parallel map on the list of tensors unpacked from\n`elems` on dimension 0.\n\n`verify_tensor_all_finite(...)`: Assert that the tensor does not contain any\nNaN's or Inf's.\n\n`where(...)`: Return the elements, either from `x` or `y`, depending on the\n`condition`.\n\n`where_v2(...)`: Return the elements where `condition` is `True` (multiplexing\n`x` and `y`).\n\n`while_loop(...)`: Repeat `body` while the condition `cond` is true.\n\n`wrap_function(...)`: Wraps the TF 1.x function fn into a graph function.\n\n`write_file(...)`: Writes contents to the file at input filename. Creates file\nand recursively\n\n`zeros(...)`: Creates a tensor with all elements set to zero.\n\n`zeros_like(...)`: Creates a tensor with all elements set to zero.\n\n`zeta(...)`: Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.add_check_numerics_ops", "path": "compat/v1/add_check_numerics_ops", "type": "tf.compat", "text": "\nConnect a `tf.debugging.check_numerics` to every floating point tensor.\n\n`check_numerics` operations themselves are added for each `half`, `float`, or\n`double` tensor in the current default graph. For all ops in the graph, the\n`check_numerics` op for all of its (`half`, `float`, or `double`) inputs is\nguaranteed to run before the `check_numerics` op on any of its outputs.\n\nNot compatible with eager execution. To check for `Inf`s and `NaN`s under\neager execution, call `tf.debugging.enable_check_numerics()` once before\nexecuting the checked operations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.add_to_collection", "path": "compat/v1/add_to_collection", "type": "tf.compat", "text": "\nWrapper for `Graph.add_to_collection()` using the default graph.\n\nSee `tf.Graph.add_to_collection` for more details.\n\nCollections are only supported in eager when variables are created inside an\nEagerVariableStore (e.g. as part of a layer or template).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.add_to_collections", "path": "compat/v1/add_to_collections", "type": "tf.compat", "text": "\nWrapper for `Graph.add_to_collections()` using the default graph.\n\nSee `tf.Graph.add_to_collections` for more details.\n\nCollections are only supported in eager when variables are created inside an\nEagerVariableStore (e.g. as part of a layer or template).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.all_variables", "path": "compat/v1/all_variables", "type": "tf.compat", "text": "\nUse `tf.compat.v1.global_variables` instead. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.app", "path": "compat/v1/app", "type": "tf.compat", "text": "\nGeneric entry point script.\n\n`flags` module: Import router for absl.flags. See\nhttps://github.com/abseil/abseil-py\n\n`run(...)`: Runs the program with an optional 'main' function and 'argv' list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.app.run", "path": "compat/v1/app/run", "type": "tf.compat", "text": "\nRuns the program with an optional 'main' function and 'argv' list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.argmax", "path": "compat/v1/argmax", "type": "tf.compat", "text": "\nReturns the index with the largest value across axes of a tensor. (deprecated\narguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.argmax`\n\nNote that in case of ties the identity of the return value is not guaranteed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.argmin", "path": "compat/v1/argmin", "type": "tf.compat", "text": "\nReturns the index with the smallest value across axes of a tensor. (deprecated\narguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.argmin`\n\nNote that in case of ties the identity of the return value is not guaranteed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.arg_max", "path": "compat/v1/arg_max", "type": "tf.compat", "text": "\nReturns the index with the largest value across dimensions of a tensor.\n\nNote that in case of ties the identity of the return value is not guaranteed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.arg_min", "path": "compat/v1/arg_min", "type": "tf.compat", "text": "\nReturns the index with the smallest value across dimensions of a tensor.\n\nNote that in case of ties the identity of the return value is not guaranteed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_equal", "path": "compat/v1/assert_equal", "type": "tf.compat", "text": "\nAssert the condition `x == y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_equal`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] == y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_greater", "path": "compat/v1/assert_greater", "type": "tf.compat", "text": "\nAssert the condition `x > y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_greater`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] > y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_greater_equal", "path": "compat/v1/assert_greater_equal", "type": "tf.compat", "text": "\nAssert the condition `x >= y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_greater_equal`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] >= y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_integer", "path": "compat/v1/assert_integer", "type": "tf.compat", "text": "\nAssert that `x` is of integer dtype.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_integer`\n\nExample of adding a dependency to an operation:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_less", "path": "compat/v1/assert_less", "type": "tf.compat", "text": "\nAssert the condition `x < y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_less`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] < y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_less_equal", "path": "compat/v1/assert_less_equal", "type": "tf.compat", "text": "\nAssert the condition `x <= y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_less_equal`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] <= y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_near", "path": "compat/v1/assert_near", "type": "tf.compat", "text": "\nAssert the condition `x` and `y` are close element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_near`\n\nExample of adding a dependency to an operation:\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have\n\n`tf.abs(x[i] - y[i]) <= atol + rtol * tf.abs(y[i])`.\n\nIf both `x` and `y` are empty, this is trivially satisfied.\n\nThe default `atol` and `rtol` is `10 * eps`, where `eps` is the smallest\nrepresentable positive number such that `1 + eps != 1`. This is about `1.2e-6`\nin `32bit`, `2.22e-15` in `64bit`, and `0.00977` in `16bit`. See\n`numpy.finfo`.\n\nSimilar to `numpy.testing.assert_allclose`, except tolerance depends on data\ntype. This is due to the fact that `TensorFlow` is often used with `32bit`,\n`64bit`, and even `16bit` data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_negative", "path": "compat/v1/assert_negative", "type": "tf.compat", "text": "\nAssert the condition `x < 0` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_negative`\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nNegative means, for every element `x[i]` of `x`, we have `x[i] < 0`. If `x` is\nempty this is trivially satisfied.\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_none_equal", "path": "compat/v1/assert_none_equal", "type": "tf.compat", "text": "\nAssert the condition `x != y` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_none_equal`\n\nThis condition holds if for every pair of (possibly broadcast) elements\n`x[i]`, `y[i]`, we have `x[i] != y[i]`. If both `x` and `y` are empty, this is\ntrivially satisfied.\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_non_negative", "path": "compat/v1/assert_non_negative", "type": "tf.compat", "text": "\nAssert the condition `x >= 0` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_non_negative`\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nNon-negative means, for every element `x[i]` of `x`, we have `x[i] >= 0`. If\n`x` is empty this is trivially satisfied.\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_non_positive", "path": "compat/v1/assert_non_positive", "type": "tf.compat", "text": "\nAssert the condition `x <= 0` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_non_positive`\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nNon-positive means, for every element `x[i]` of `x`, we have `x[i] <= 0`. If\n`x` is empty this is trivially satisfied.\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_positive", "path": "compat/v1/assert_positive", "type": "tf.compat", "text": "\nAssert the condition `x > 0` holds element-wise.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_positive`\n\nWhen running in graph mode, you should add a dependency on this operation to\nensure that it runs. Example of adding a dependency to an operation:\n\nPositive means, for every element `x[i]` of `x`, we have `x[i] > 0`. If `x` is\nempty this is trivially satisfied.\n\nreturns None\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_rank", "path": "compat/v1/assert_rank", "type": "tf.compat", "text": "\nAssert `x` has rank equal to `rank`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_rank`\n\nExample of adding a dependency to an operation:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_rank_at_least", "path": "compat/v1/assert_rank_at_least", "type": "tf.compat", "text": "\nAssert `x` has rank equal to `rank` or higher.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_rank_at_least`\n\nExample of adding a dependency to an operation:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_rank_in", "path": "compat/v1/assert_rank_in", "type": "tf.compat", "text": "\nAssert `x` has rank in `ranks`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_rank_in`\n\nExample of adding a dependency to an operation:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_scalar", "path": "compat/v1/assert_scalar", "type": "tf.compat", "text": "\nAsserts that the given `tensor` is a scalar (i.e. zero-dimensional).\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_scalar`\n\nThis function raises `ValueError` unless it can be certain that the given\n`tensor` is a scalar. `ValueError` is also raised if the shape of `tensor` is\nunknown.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_type", "path": "compat/v1/assert_type", "type": "tf.compat", "text": "\nStatically asserts that the given `Tensor` is of the specified type.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.debugging.assert_type`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assert_variables_initialized", "path": "compat/v1/assert_variables_initialized", "type": "tf.compat", "text": "\nReturns an Op to check if variables are initialized.\n\nWhen run, the returned Op will raise the exception `FailedPreconditionError`\nif any of the variables has not yet been initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assign", "path": "compat/v1/assign", "type": "tf.compat", "text": "\nUpdate `ref` by assigning `value` to it.\n\nThis operation outputs a Tensor that holds the new value of `ref` after the\nvalue has been assigned. This makes it easier to chain operations that need to\nuse the reset value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assign_add", "path": "compat/v1/assign_add", "type": "tf.compat", "text": "\nUpdate `ref` by adding `value` to it.\n\nThis operation outputs \"ref\" after the update is done. This makes it easier to\nchain operations that need to use the reset value. Unlike `tf.math.add`, this\nop does not broadcast. `ref` and `value` must have the same shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.assign_sub", "path": "compat/v1/assign_sub", "type": "tf.compat", "text": "\nUpdate `ref` by subtracting `value` from it.\n\nThis operation outputs `ref` after the update is done. This makes it easier to\nchain operations that need to use the reset value. Unlike `tf.math.subtract`,\nthis op does not broadcast. `ref` and `value` must have the same shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.AttrValue", "path": "compat/v1/attrvalue", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class ListValue`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.AttrValue.ListValue", "path": "compat/v1/attrvalue/listvalue", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.audio", "path": "compat/v1/audio", "type": "tf.compat", "text": "\nPublic API for tf.audio namespace.\n\n`decode_wav(...)`: Decode a 16-bit PCM WAV file to a float tensor.\n\n`encode_wav(...)`: Encode audio data using the WAV file format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.autograph", "path": "compat/v1/autograph", "type": "tf.compat", "text": "\nConversion of plain Python into TensorFlow graph code.\n\nFor more information, see the AutoGraph guide.\n\nBy equivalent graph code we mean code that generates a TensorFlow graph when\nrun. The generated graph has the same effects as the original code when\nexecuted (for example with `tf.function` or `tf.compat.v1.Session.run`). In\nother words, using AutoGraph can be thought of as running Python in\nTensorFlow.\n\n`experimental` module: Public API for tf.autograph.experimental namespace.\n\n`set_verbosity(...)`: Sets the AutoGraph verbosity level.\n\n`to_code(...)`: Returns the source code generated by AutoGraph, as a string.\n\n`to_graph(...)`: Converts a Python entity into a TensorFlow graph.\n\n`trace(...)`: Traces argument information at compilation time.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.autograph.experimental", "path": "compat/v1/autograph/experimental", "type": "tf.compat", "text": "\nPublic API for tf.autograph.experimental namespace.\n\n`class Feature`: This enumeration represents optional conversion options.\n\n`do_not_convert(...)`: Decorator that suppresses the conversion of a function.\n\n`set_loop_options(...)`: Specifies additional arguments to be passed to the\nenclosing while_loop.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.autograph.to_code", "path": "compat/v1/autograph/to_code", "type": "tf.compat", "text": "\nReturns the source code generated by AutoGraph, as a string.\n\nAlso see: `tf.autograph.to_graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.autograph.to_graph", "path": "compat/v1/autograph/to_graph", "type": "tf.compat", "text": "\nConverts a Python entity into a TensorFlow graph.\n\nAlso see: `tf.autograph.to_code`, `tf.function`.\n\nUnlike `tf.function`, `to_graph` is a low-level transpiler that converts\nPython code to TensorFlow graph code. It does not implement any caching,\nvariable management or create any actual ops, and is best used where greater\ncontrol over the generated TensorFlow graph is desired. Another difference\nfrom `tf.function` is that `to_graph` will not wrap the graph into a\nTensorFlow function or a Python callable. Internally, `tf.function` uses\n`to_graph`.\n\nExample Usage\n\nSupported Python entities include:\n\nFunctions are converted into new functions with converted code.\n\nClasses are converted by generating a new class whose methods use converted\ncode.\n\nMethods are converted into unbound function that have an additional first\nargument called `self`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.batch_gather", "path": "compat/v1/batch_gather", "type": "tf.compat", "text": "\nGather slices from params according to indices with leading batch dims.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.batch_scatter_update", "path": "compat/v1/batch_scatter_update", "type": "tf.compat", "text": "\nGeneralization of `tf.compat.v1.scatter_update` to axis different than 0.\n(deprecated)\n\nAnalogous to `batch_gather`. This assumes that `ref`, `indices` and `updates`\nhave a series of leading dimensions that are the same for all of them, and the\nupdates are performed on the last dimension of indices. In other words, the\ndimensions should be the following:\n\n`num_prefix_dims = indices.ndims - 1` `batch_dim = num_prefix_dims + 1`\n`updates.shape = indices.shape + var.shape[batch_dim:]`\n\nwhere\n\n`updates.shape[:num_prefix_dims]` `== indices.shape[:num_prefix_dims]` `==\nvar.shape[:num_prefix_dims]`\n\nAnd the operation performed can be expressed as:\n\n`var[i_1, ..., i_n, indices[i_1, ..., i_n, j]] = updates[i_1, ..., i_n, j]`\n\nWhen indices is a 1D tensor, this operation is equivalent to\n`tf.compat.v1.scatter_update`.\n\nTo avoid this operation there would be 2 alternatives:\n\n1) Reshaping the variable by merging the first `ndims` dimensions. However,\nthis is not possible because `tf.reshape` returns a Tensor, which we cannot\nuse `tf.compat.v1.scatter_update` on. 2) Looping over the first `ndims` of the\nvariable and using `tf.compat.v1.scatter_update` on the subtensors that result\nof slicing the first dimension. This is a valid option for `ndims = 1`, but\nless efficient than this implementation.\n\nSee also `tf.compat.v1.scatter_update` and `tf.compat.v1.scatter_nd_update`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.batch_to_space", "path": "compat/v1/batch_to_space", "type": "tf.compat", "text": "\nBatchToSpace for 4-D tensors of type T.\n\nThis is a legacy version of the more general BatchToSpaceND.\n\nRearranges (permutes) data from batch into blocks of spatial data, followed by\ncropping. This is the reverse transformation of SpaceToBatch. More\nspecifically, this op outputs a copy of the input tensor where values from the\n`batch` dimension are moved in spatial blocks to the `height` and `width`\ndimensions, followed by cropping along the `height` and `width` dimensions.\n\ncrops = [[crop_top, crop_bottom], [crop_left, crop_right]]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.batch_to_space_nd", "path": "compat/v1/batch_to_space_nd", "type": "tf.compat", "text": "\nBatchToSpace for N-D tensors of type T.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.manip.batch_to_space_nd`\n\nThis operation reshapes the \"batch\" dimension 0 into `M + 1` dimensions of\nshape `block_shape + [batch]`, interleaves these blocks back into the grid\ndefined by the spatial dimensions `[1, ..., M]`, to obtain a result with the\nsame rank as the input. The spatial dimensions of this intermediate result are\nthen optionally cropped according to `crops` to produce the output. This is\nthe reverse of SpaceToBatch. See below for a precise description.\n\nThis operation is equivalent to the following steps:\n\nReshape `input` to `reshaped` of shape: [block_shape[0], ...,\nblock_shape[M-1], batch / prod(block_shape), input_shape[1], ...,\ninput_shape[N-1]]\n\nPermute dimensions of `reshaped` to produce `permuted` of shape [batch /\nprod(block_shape),\n\ninput_shape[1], block_shape[0], ..., input_shape[M], block_shape[M-1],\n\ninput_shape[M+1], ..., input_shape[N-1]]\n\ninput_shape[1] * block_shape[0], ..., input_shape[M] * block_shape[M-1],\n\ninput_shape[M+1], ..., input_shape[N-1]]\n\ninput_shape[1] * block_shape[0] - crops[0,0] - crops[0,1], ..., input_shape[M]\n* block_shape[M-1] - crops[M-1,0] - crops[M-1,1],\n\ninput_shape[M+1], ..., input_shape[N-1]]\n\nSome examples:\n\n(1) For the following input of shape `[4, 1, 1, 1]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [0, 0]]`:\n\nThe output tensor has shape `[1, 2, 2, 1]` and value:\n\n(2) For the following input of shape `[4, 1, 1, 3]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [0, 0]]`:\n\nThe output tensor has shape `[1, 2, 2, 3]` and value:\n\n(3) For the following input of shape `[4, 2, 2, 1]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [0, 0]]`:\n\nThe output tensor has shape `[1, 4, 4, 1]` and value:\n\n(4) For the following input of shape `[8, 1, 3, 1]`, `block_shape = [2, 2]`,\nand `crops = [[0, 0], [2, 0]]`:\n\nThe output tensor has shape `[2, 2, 4, 1]` and value:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.bincount", "path": "compat/v1/bincount", "type": "tf.compat", "text": "\nCounts the number of occurrences of each value in an integer array.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.bincount`\n\nIf `minlength` and `maxlength` are not given, returns a vector with length\n`tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise. If\n`weights` are non-None, then index `i` of the output stores the sum of the\nvalue in `weights` at each index where the corresponding value in `arr` is\n`i`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.bitwise", "path": "compat/v1/bitwise", "type": "tf.compat", "text": "\nOperations for manipulating the binary representations of integers.\n\n`bitwise_and(...)`: Elementwise computes the bitwise AND of `x` and `y`.\n\n`bitwise_or(...)`: Elementwise computes the bitwise OR of `x` and `y`.\n\n`bitwise_xor(...)`: Elementwise computes the bitwise XOR of `x` and `y`.\n\n`invert(...)`: Invert (flip) each bit of supported types; for example, type\n`uint8` value 01010101 becomes 10101010.\n\n`left_shift(...)`: Elementwise computes the bitwise left-shift of `x` and `y`.\n\n`right_shift(...)`: Elementwise computes the bitwise right-shift of `x` and\n`y`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.boolean_mask", "path": "compat/v1/boolean_mask", "type": "tf.compat", "text": "\nApply boolean mask to tensor.\n\nNumpy equivalent is `tensor[mask]`.\n\nIn general, `0 < dim(mask) = K <= dim(tensor)`, and `mask`'s shape must match\nthe first K dimensions of `tensor`'s shape. We then have:\n`boolean_mask(tensor, mask)[i, j1,...,jd] = tensor[i1,...,iK,j1,...,jd]` where\n`(i1,...,iK)` is the ith `True` entry of `mask` (row-major order). The `axis`\ncould be used with `mask` to indicate the axis to mask from. In that case,\n`axis + dim(mask) <= dim(tensor)` and `mask`'s shape must match the first\n`axis + dim(mask)` dimensions of `tensor`'s shape.\n\nSee also: `tf.ragged.boolean_mask`, which can be applied to both dense and\nragged tensors, and can be used if you need to preserve the masked dimensions\nof `tensor` (rather than flattening them, as `tf.boolean_mask` does).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.case", "path": "compat/v1/case", "type": "tf.compat", "text": "\nCreate a case operation.\n\nSee also `tf.switch_case`.\n\nThe `pred_fn_pairs` parameter is a dict or list of pairs of size N. Each pair\ncontains a boolean scalar tensor and a python callable that creates the\ntensors to be returned if the boolean evaluates to True. `default` is a\ncallable generating a list of tensors. All the callables in `pred_fn_pairs` as\nwell as `default` (if provided) should return the same number and types of\ntensors.\n\nIf `exclusive==True`, all predicates are evaluated, and an exception is thrown\nif more than one of the predicates evaluates to `True`. If `exclusive==False`,\nexecution stops at the first predicate which evaluates to True, and the\ntensors generated by the corresponding function are returned immediately. If\nnone of the predicates evaluate to True, this operation returns the tensors\ngenerated by `default`.\n\n`tf.case` supports nested structures as implemented in\n`tf.contrib.framework.nest`. All of the callables must return the same\n(possibly nested) value structure of lists, tuples, and/or named tuples.\nSingleton lists and tuples form the only exceptions to this: when returned by\na callable, they are implicitly unpacked to single values. This behavior is\ndisabled by passing `strict=True`.\n\nIf an unordered dictionary is used for `pred_fn_pairs`, the order of the\nconditional tests is not guaranteed. However, the order is guaranteed to be\ndeterministic, so that variables created in conditional branches are created\nin fixed order across runs.\n\nExample 1:\n\nExample 2:\n\nUnordered dictionaries are not supported in eager mode when `exclusive=False`.\nUse a list of tuples instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.clip_by_average_norm", "path": "compat/v1/clip_by_average_norm", "type": "tf.compat", "text": "\nClips tensor values to a maximum average L2-norm. (deprecated)\n\nGiven a tensor `t`, and a maximum clip value `clip_norm`, this operation\nnormalizes `t` so that its average L2-norm is less than or equal to\n`clip_norm`. Specifically, if the average L2-norm is already less than or\nequal to `clip_norm`, then `t` is not modified. If the average L2-norm is\ngreater than `clip_norm`, then this operation returns a tensor of the same\ntype and shape as `t` with its values set to:\n\n`t * clip_norm / l2norm_avg(t)`\n\nIn this case, the average L2-norm of the output tensor is `clip_norm`.\n\nThis operation is typically used to clip gradients before applying them with\nan optimizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.colocate_with", "path": "compat/v1/colocate_with", "type": "tf.compat", "text": "\nDEPRECATED FUNCTION\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.compat", "path": "compat/v1/compat", "type": "tf.compat", "text": "\nCompatibility functions.\n\nThe `tf.compat` module contains two sets of compatibility functions.\n\nThe `compat.v1` and `compat.v2` submodules provide a complete copy of both the\n`v1` and `v2` APIs for backwards and forwards compatibility across TensorFlow\nversions 1.x and 2.x. See the migration guide for details.\n\nAside from the `compat.v1` and `compat.v2` submodules, `tf.compat` also\ncontains a set of helper functions for writing code that works in both:\n\nThe compatibility module also provides the following aliases for common sets\nof python types:\n\n`as_bytes(...)`: Converts `bytearray`, `bytes`, or unicode python input types\nto `bytes`.\n\n`as_str(...)`\n\n`as_str_any(...)`: Converts input to `str` type.\n\n`as_text(...)`: Converts any string-like python input types to unicode.\n\n`dimension_at_index(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`dimension_value(...)`: Compatibility utility required to allow for both V1\nand V2 behavior in TF.\n\n`forward_compatibility_horizon(...)`: Context manager for testing forward\ncompatibility of generated graphs.\n\n`forward_compatible(...)`: Return true if the forward compatibility window has\nexpired.\n\n`path_to_str(...)`: Converts input which is a `PathLike` object to `str` type.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.cond", "path": "compat/v1/cond", "type": "tf.compat", "text": "\nReturn `true_fn()` if the predicate `pred` is true else `false_fn()`.\n(deprecated arguments)\n\n`true_fn` and `false_fn` both return lists of output tensors. `true_fn` and\n`false_fn` must have the same non-zero number and type of outputs.\n\nAlthough this behavior is consistent with the dataflow model of TensorFlow, it\nhas frequently surprised users who expected a lazier semantics. Consider the\nfollowing simple program:\n\nIf `x < y`, the `tf.add` operation will be executed and `tf.square` operation\nwill not be executed. Since `z` is needed for at least one branch of the\n`cond`, the `tf.multiply` operation is always executed, unconditionally.\n\nNote that `cond` calls `true_fn` and `false_fn` exactly once (inside the call\nto `cond`, and not at all during `Session.run()`). `cond` stitches together\nthe graph fragments created during the `true_fn` and `false_fn` calls with\nsome additional graph nodes to ensure that the right branch gets executed\ndepending on the value of `pred`.\n\n`tf.cond` supports nested structures as implemented in\n`tensorflow.python.util.nest`. Both `true_fn` and `false_fn` must return the\nsame (possibly nested) value structure of lists, tuples, and/or named tuples.\nSingleton lists and tuples form the only exceptions to this: when returned by\n`true_fn` and/or `false_fn`, they are implicitly unpacked to single values.\nThis behavior is disabled by passing `strict=True`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ConditionalAccumulator", "path": "compat/v1/conditionalaccumulator", "type": "tf.compat", "text": "\nA conditional accumulator for aggregating gradients.\n\nInherits From: `ConditionalAccumulatorBase`\n\nUp-to-date gradients (i.e., time step at which gradient was computed is equal\nto the accumulator's time step) are added to the accumulator.\n\nExtraction of the average gradient is blocked until the required number of\ngradients has been accumulated.\n\nView source\n\nAttempts to apply a gradient to the accumulator.\n\nThe attempt is silently dropped if the gradient is stale, i.e., local_step is\nless than the accumulator's global time step.\n\nView source\n\nNumber of gradients that have currently been aggregated in accumulator.\n\nView source\n\nSets the global time step of the accumulator.\n\nThe operation logs a warning if we attempt to set to a time step that is lower\nthan the accumulator's own time step.\n\nView source\n\nAttempts to extract the average gradient from the accumulator.\n\nThe operation blocks until sufficient number of gradients have been\nsuccessfully applied to the accumulator.\n\nOnce successful, the following actions are also triggered:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ConditionalAccumulatorBase", "path": "compat/v1/conditionalaccumulatorbase", "type": "tf.compat", "text": "\nA conditional accumulator for aggregating gradients.\n\nUp-to-date gradients (i.e., time step at which gradient was computed is equal\nto the accumulator's time step) are added to the accumulator.\n\nExtraction of the average gradient is blocked until the required number of\ngradients has been accumulated.\n\nView source\n\nNumber of gradients that have currently been aggregated in accumulator.\n\nView source\n\nSets the global time step of the accumulator.\n\nThe operation logs a warning if we attempt to set to a time step that is lower\nthan the accumulator's own time step.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.config", "path": "compat/v1/config", "type": "tf.compat", "text": "\nPublic API for tf.config namespace.\n\n`experimental` module: Public API for tf.config.experimental namespace.\n\n`optimizer` module: Public API for tf.config.optimizer namespace.\n\n`threading` module: Public API for tf.config.threading namespace.\n\n`class LogicalDevice`: Abstraction for a logical device initialized by the\nruntime.\n\n`class LogicalDeviceConfiguration`: Configuration class for a logical devices.\n\n`class PhysicalDevice`: Abstraction for a locally visible physical device.\n\n`experimental_connect_to_cluster(...)`: Connects to the given cluster.\n\n`experimental_connect_to_host(...)`: Connects to a single machine to enable\nremote execution on it.\n\n`experimental_functions_run_eagerly(...)`: Returns the value of the\n`experimental_run_functions_eagerly` setting. (deprecated)\n\n`experimental_run_functions_eagerly(...)`: Enables / disables eager execution\nof `tf.function`s. (deprecated)\n\n`functions_run_eagerly(...)`: Returns the value of the `run_functions_eagerly`\nsetting.\n\n`get_logical_device_configuration(...)`: Get the virtual device configuration\nfor a `tf.config.PhysicalDevice`.\n\n`get_soft_device_placement(...)`: Get if soft device placement is enabled.\n\n`get_visible_devices(...)`: Get the list of visible physical devices.\n\n`list_logical_devices(...)`: Return a list of logical devices created by\nruntime.\n\n`list_physical_devices(...)`: Return a list of physical devices visible to the\nhost runtime.\n\n`run_functions_eagerly(...)`: Enables / disables eager execution of\n`tf.function`s.\n\n`set_logical_device_configuration(...)`: Set the logical device configuration\nfor a `tf.config.PhysicalDevice`.\n\n`set_soft_device_placement(...)`: Set if soft device placement is enabled.\n\n`set_visible_devices(...)`: Set the list of visible devices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.config.experimental", "path": "compat/v1/config/experimental", "type": "tf.compat", "text": "\nPublic API for tf.config.experimental namespace.\n\n`class ClusterDeviceFilters`: Represent a collection of device filters for the\nremote workers in cluster.\n\n`class VirtualDeviceConfiguration`: Configuration class for a logical devices.\n\n`disable_mlir_bridge(...)`: Disables experimental MLIR-Based TensorFlow\nCompiler Bridge.\n\n`disable_mlir_graph_optimization(...)`: Disables experimental MLIR-Based\nTensorFlow Compiler Optimizations.\n\n`enable_mlir_bridge(...)`: Enables experimental MLIR-Based TensorFlow Compiler\nBridge.\n\n`enable_mlir_graph_optimization(...)`: Enables experimental MLIR-Based\nTensorFlow Compiler Optimizations.\n\n`enable_tensor_float_32_execution(...)`: Enable or disable the use of\nTensorFloat-32 on supported hardware.\n\n`get_device_details(...)`: Returns details about a physical devices.\n\n`get_device_policy(...)`: Gets the current device policy.\n\n`get_memory_growth(...)`: Get if memory growth is enabled for a\n`PhysicalDevice`.\n\n`get_memory_usage(...)`: Get the memory usage, in bytes, for the chosen\ndevice.\n\n`get_synchronous_execution(...)`: Gets whether operations are executed\nsynchronously or asynchronously.\n\n`get_virtual_device_configuration(...)`: Get the virtual device configuration\nfor a `tf.config.PhysicalDevice`.\n\n`get_visible_devices(...)`: Get the list of visible physical devices.\n\n`list_logical_devices(...)`: Return a list of logical devices created by\nruntime.\n\n`list_physical_devices(...)`: Return a list of physical devices visible to the\nhost runtime.\n\n`set_device_policy(...)`: Sets the current thread device policy.\n\n`set_memory_growth(...)`: Set if memory growth should be enabled for a\n`PhysicalDevice`.\n\n`set_synchronous_execution(...)`: Specifies whether operations are executed\nsynchronously or asynchronously.\n\n`set_virtual_device_configuration(...)`: Set the logical device configuration\nfor a `tf.config.PhysicalDevice`.\n\n`set_visible_devices(...)`: Set the list of visible devices.\n\n`tensor_float_32_execution_enabled(...)`: Returns whether TensorFloat-32 is\nenabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.config.optimizer", "path": "compat/v1/config/optimizer", "type": "tf.compat", "text": "\nPublic API for tf.config.optimizer namespace.\n\n`get_experimental_options(...)`: Get experimental optimizer options.\n\n`get_jit(...)`: Get if JIT compilation is enabled.\n\n`set_experimental_options(...)`: Set experimental optimizer options.\n\n`set_jit(...)`: Set if JIT compilation is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.config.threading", "path": "compat/v1/config/threading", "type": "tf.compat", "text": "\nPublic API for tf.config.threading namespace.\n\n`get_inter_op_parallelism_threads(...)`: Get number of threads used for\nparallelism between independent operations.\n\n`get_intra_op_parallelism_threads(...)`: Get number of threads used within an\nindividual op for parallelism.\n\n`set_inter_op_parallelism_threads(...)`: Set number of threads used for\nparallelism between independent operations.\n\n`set_intra_op_parallelism_threads(...)`: Set number of threads used within an\nindividual op for parallelism.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ConfigProto", "path": "compat/v1/configproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class DeviceCountEntry`\n\n`class Experimental`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ConfigProto.DeviceCountEntry", "path": "compat/v1/configproto/devicecountentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ConfigProto.Experimental", "path": "compat/v1/configproto/experimental", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.confusion_matrix", "path": "compat/v1/confusion_matrix", "type": "tf.compat", "text": "\nComputes the confusion matrix from predictions and labels.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.confusion_matrix`\n\nThe matrix columns represent the prediction labels and the rows represent the\nreal labels. The confusion matrix is always a 2-D array of shape `[n, n]`,\nwhere `n` is the number of valid labels for a given classification task. Both\nprediction and labels must be 1-D arrays of the same shape in order for this\nfunction to work.\n\nIf `num_classes` is `None`, then `num_classes` will be set to one plus the\nmaximum value in either predictions or labels. Class labels are expected to\nstart at 0. For example, if `num_classes` is 3, then the possible labels would\nbe `[0, 1, 2]`.\n\nIf `weights` is not `None`, then each prediction contributes its corresponding\nweight to the total value of the confusion matrix cell.\n\nNote that the possible labels are assumed to be `[0, 1, 2, 3, 4]`, resulting\nin a 5x5 confusion matrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.constant", "path": "compat/v1/constant", "type": "tf.compat", "text": "\nCreates a constant tensor.\n\nThe resulting tensor is populated with values of type `dtype`, as specified by\narguments `value` and (optionally) `shape` (see examples below).\n\nThe argument `value` can be a constant value, or a list of values of type\n`dtype`. If `value` is a list, then the length of the list must be less than\nor equal to the number of elements implied by the `shape` argument (if\nspecified). In the case where the list length is less than the number of\nelements specified by `shape`, the last element in the list will be used to\nfill the remaining entries.\n\nThe argument `shape` is optional. If present, it specifies the dimensions of\nthe resulting tensor. If not present, the shape of `value` is used.\n\nIf the argument `dtype` is not specified, then the type is inferred from the\ntype of `value`.\n\n`tf.constant` differs from `tf.fill` in a few ways:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.container", "path": "compat/v1/container", "type": "tf.compat", "text": "\nWrapper for `Graph.container()` using the default graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.control_flow_v2_enabled", "path": "compat/v1/control_flow_v2_enabled", "type": "tf.compat", "text": "\nReturns `True` if v2 control flow is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.convert_to_tensor", "path": "compat/v1/convert_to_tensor", "type": "tf.compat", "text": "\nConverts the given `value` to a `Tensor`.\n\nThis function converts Python objects of various types to `Tensor` objects. It\naccepts `Tensor` objects, numpy arrays, Python lists, and Python scalars. For\nexample:\n\nThis function can be useful when composing a new operation in Python (such as\n`my_func` in the example above). All standard Python op constructors apply\nthis function to each of their Tensor-valued inputs, which allows those ops to\naccept numpy arrays, Python lists, and scalars in addition to `Tensor`\nobjects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.convert_to_tensor_or_indexed_slices", "path": "compat/v1/convert_to_tensor_or_indexed_slices", "type": "tf.compat", "text": "\nConverts the given object to a `Tensor` or an `IndexedSlices`.\n\nIf `value` is an `IndexedSlices` or `SparseTensor` it is returned unmodified.\nOtherwise, it is converted to a `Tensor` using `convert_to_tensor()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.convert_to_tensor_or_sparse_tensor", "path": "compat/v1/convert_to_tensor_or_sparse_tensor", "type": "tf.compat", "text": "\nConverts value to a `SparseTensor` or `Tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.count_nonzero", "path": "compat/v1/count_nonzero", "type": "tf.compat", "text": "\nComputes number of nonzero elements across dimensions of a tensor. (deprecated\narguments) (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.count_nonzero`\n\nReduces `input_tensor` along the dimensions given in `axis`. Unless `keepdims`\nis true, the rank of the tensor is reduced by 1 for each entry in `axis`. If\n`keepdims` is true, the reduced dimensions are retained with length 1.\n\nIf `axis` has no entries, all dimensions are reduced, and a tensor with a\nsingle element is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.count_up_to", "path": "compat/v1/count_up_to", "type": "tf.compat", "text": "\nIncrements 'ref' until it reaches 'limit'. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.create_partitioned_variables", "path": "compat/v1/create_partitioned_variables", "type": "tf.compat", "text": "\nCreate a list of partitioned variables according to the given `slicing`.\n(deprecated)\n\nCurrently only one dimension of the full variable can be sliced, and the full\nvariable can be reconstructed by the concatenation of the returned list along\nthat dimension.\n\nFor convenience, The requested number of partitions does not have to divide\nthe corresponding dimension evenly. If it does not, the shapes of the\npartitions are incremented by 1 starting from partition 0 until all slack is\nabsorbed. The adjustment rules may change in the future, but as you can\nsave/restore these variables with different slicing specifications this should\nnot be a problem.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data", "path": "compat/v1/data", "type": "tf.compat", "text": "\n`tf.data.Dataset` API for input pipelines.\n\nSee Importing Data for an overview.\n\n`experimental` module: Experimental API for building input pipelines.\n\n`class Dataset`: Represents a potentially large set of elements.\n\n`class DatasetSpec`: Type specification for `tf.data.Dataset`.\n\n`class FixedLengthRecordDataset`: A `Dataset` of fixed-length records from one\nor more binary files.\n\n`class Iterator`: Represents the state of iterating through a `Dataset`.\n\n`class Options`: Represents options for `tf.data.Dataset`.\n\n`class TFRecordDataset`: A `Dataset` comprising records from one or more\nTFRecord files.\n\n`class TextLineDataset`: A `Dataset` comprising lines from one or more text\nfiles.\n\n`get_output_classes(...)`: Returns the output classes for elements of the\ninput dataset / iterator.\n\n`get_output_shapes(...)`: Returns the output shapes for elements of the input\ndataset / iterator.\n\n`get_output_types(...)`: Returns the output shapes for elements of the input\ndataset / iterator.\n\n`make_initializable_iterator(...)`: Creates an iterator for elements of\n`dataset`.\n\n`make_one_shot_iterator(...)`: Creates an iterator for elements of `dataset`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.Dataset", "path": "compat/v1/data/dataset", "type": "tf.compat", "text": "\nRepresents a potentially large set of elements.\n\nInherits From: `Dataset`\n\nA `Dataset` can be used to represent an input pipeline as a collection of\nelements and a \"logical plan\" of transformations that act on those elements.\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental", "path": "compat/v1/data/experimental", "type": "tf.compat", "text": "\nExperimental API for building input pipelines.\n\nThis module contains experimental `Dataset` sources and transformations that\ncan be used in conjunction with the `tf.data.Dataset` API. Note that the\n`tf.data.experimental` API is not subject to the same backwards compatibility\nguarantees as `tf.data`, but we will provide deprecation advice in advance of\nremoving existing functionality.\n\nSee Importing Data for an overview.\n\n`service` module: API for using the tf.data service.\n\n`class AutoShardPolicy`: Represents the type of auto-sharding we enable.\n\n`class CheckpointInputPipelineHook`: Checkpoints input pipeline state every N\nsteps or seconds.\n\n`class CsvDataset`: A Dataset comprising lines from one or more CSV files.\n\n`class DatasetStructure`: Type specification for `tf.data.Dataset`.\n\n`class DistributeOptions`: Represents options for distributed data processing.\n\n`class MapVectorizationOptions`: Represents options for the MapVectorization\noptimization.\n\n`class OptimizationOptions`: Represents options for dataset optimizations.\n\n`class Optional`: Represents a value that may or may not be present.\n\n`class OptionalStructure`: Type specification for `tf.experimental.Optional`.\n\n`class RandomDataset`: A `Dataset` of pseudorandom values.\n\n`class Reducer`: A reducer is used for reducing a set of elements.\n\n`class SqlDataset`: A `Dataset` consisting of the results from a SQL query.\n\n`class StatsAggregator`: A stateful resource that aggregates statistics from\none or more iterators.\n\n`class StatsOptions`: Represents options for collecting dataset stats using\n`StatsAggregator`.\n\n`class Structure`: Specifies a TensorFlow value type.\n\n`class TFRecordWriter`: Writes a dataset to a TFRecord file.\n\n`class ThreadingOptions`: Represents options for dataset threading.\n\n`Counter(...)`: Creates a `Dataset` that counts from `start` in steps of size\n`step`.\n\n`RaggedTensorStructure(...)`: DEPRECATED FUNCTION\n\n`SparseTensorStructure(...)`: DEPRECATED FUNCTION\n\n`TensorArrayStructure(...)`: DEPRECATED FUNCTION\n\n`TensorStructure(...)`: DEPRECATED FUNCTION\n\n`assert_cardinality(...)`: Asserts the cardinality of the input dataset.\n\n`bucket_by_sequence_length(...)`: A transformation that buckets elements in a\n`Dataset` by length.\n\n`bytes_produced_stats(...)`: Records the number of bytes produced by each\nelement of the input dataset.\n\n`cardinality(...)`: Returns the cardinality of `dataset`, if known.\n\n`choose_from_datasets(...)`: Creates a dataset that deterministically chooses\nelements from `datasets`.\n\n`copy_to_device(...)`: A transformation that copies dataset elements to the\ngiven `target_device`.\n\n`dense_to_ragged_batch(...)`: A transformation that batches ragged elements\ninto `tf.RaggedTensor`s.\n\n`dense_to_sparse_batch(...)`: A transformation that batches ragged elements\ninto `tf.sparse.SparseTensor`s.\n\n`enumerate_dataset(...)`: A transformation that enumerates the elements of a\ndataset. (deprecated)\n\n`from_variant(...)`: Constructs a dataset from the given variant and\nstructure.\n\n`get_next_as_optional(...)`: Returns a `tf.experimental.Optional` with the\nnext element of the iterator. (deprecated)\n\n`get_single_element(...)`: Returns the single element in `dataset` as a nested\nstructure of tensors.\n\n`get_structure(...)`: Returns the type signature for elements of the input\ndataset / iterator.\n\n`group_by_reducer(...)`: A transformation that groups elements and performs a\nreduction.\n\n`group_by_window(...)`: A transformation that groups windows of elements by\nkey and reduces them.\n\n`ignore_errors(...)`: Creates a `Dataset` from another `Dataset` and silently\nignores any errors.\n\n`latency_stats(...)`: Records the latency of producing each element of the\ninput dataset.\n\n`make_batched_features_dataset(...)`: Returns a `Dataset` of feature\ndictionaries from `Example` protos.\n\n`make_csv_dataset(...)`: Reads CSV files into a dataset.\n\n`make_saveable_from_iterator(...)`: Returns a SaveableObject for\nsaving/restoring iterator state using Saver. (deprecated)\n\n`map_and_batch(...)`: Fused implementation of `map` and `batch`. (deprecated)\n\n`map_and_batch_with_legacy_function(...)`: Fused implementation of `map` and\n`batch`. (deprecated)\n\n`parallel_interleave(...)`: A parallel version of the `Dataset.interleave()`\ntransformation. (deprecated)\n\n`parse_example_dataset(...)`: A transformation that parses `Example` protos\ninto a `dict` of tensors.\n\n`prefetch_to_device(...)`: A transformation that prefetches dataset values to\nthe given `device`.\n\n`rejection_resample(...)`: A transformation that resamples a dataset to\nachieve a target distribution.\n\n`sample_from_datasets(...)`: Samples elements at random from the datasets in\n`datasets`.\n\n`scan(...)`: A transformation that scans a function across an input dataset.\n\n`shuffle_and_repeat(...)`: Shuffles and repeats a Dataset, reshuffling with\neach repetition. (deprecated)\n\n`snapshot(...)`: API to persist the output of the input dataset.\n\n`take_while(...)`: A transformation that stops dataset iteration based on a\n`predicate`.\n\n`to_variant(...)`: Returns a variant representing the given dataset.\n\n`unbatch(...)`: Splits elements of a dataset into multiple elements on the\nbatch dimension. (deprecated)\n\n`unique(...)`: Creates a `Dataset` from another `Dataset`, discarding\nduplicates.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.choose_from_datasets", "path": "compat/v1/data/experimental/choose_from_datasets", "type": "tf.compat", "text": "\nCreates a dataset that deterministically chooses elements from `datasets`.\n\nFor example, given the following datasets:\n\nThe elements of `result` will be:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.Counter", "path": "compat/v1/data/experimental/counter", "type": "tf.compat", "text": "\nCreates a `Dataset` that counts from `start` in steps of size `step`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.CsvDataset", "path": "compat/v1/data/experimental/csvdataset", "type": "tf.compat", "text": "\nA Dataset comprising lines from one or more CSV files.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.make_batched_features_dataset", "path": "compat/v1/data/experimental/make_batched_features_dataset", "type": "tf.compat", "text": "\nReturns a `Dataset` of feature dictionaries from `Example` protos.\n\nIf label_key argument is provided, returns a `Dataset` of tuple comprising of\nfeature dictionaries and label.\n\nAnd the expected output is:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.make_csv_dataset", "path": "compat/v1/data/experimental/make_csv_dataset", "type": "tf.compat", "text": "\nReads CSV files into a dataset.\n\nReads CSV files into a dataset, where each element is a (features, labels)\ntuple that corresponds to a batch of CSV rows. The features dictionary maps\nfeature column names to `Tensor`s containing the corresponding feature data,\nand labels is a `Tensor` containing the batch's label data.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.map_and_batch_with_legacy_function", "path": "compat/v1/data/experimental/map_and_batch_with_legacy_function", "type": "tf.compat", "text": "\nFused implementation of `map` and `batch`. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.RaggedTensorStructure", "path": "compat/v1/data/experimental/raggedtensorstructure", "type": "tf.compat", "text": "\nDEPRECATED FUNCTION\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.RandomDataset", "path": "compat/v1/data/experimental/randomdataset", "type": "tf.compat", "text": "\nA `Dataset` of pseudorandom values.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.sample_from_datasets", "path": "compat/v1/data/experimental/sample_from_datasets", "type": "tf.compat", "text": "\nSamples elements at random from the datasets in `datasets`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.service", "path": "compat/v1/data/experimental/service", "type": "tf.compat", "text": "\nAPI for using the tf.data service.\n\nThe tf.data service offers a way to improve training speed when the host\nattached to a training device can't keep up with the data consumption of the\nmodel. For example, suppose a host can generate 100 examples/second, but the\nmodel can process 200 examples/second. Training speed could be doubled by\nusing the tf.data service to generate 200 examples/second.\n\nThere are a few things to do before using the tf.data service to speed up\ntraining.\n\nThe tf.data service uses a cluster of workers to prepare data for training\nyour model. The `processing_mode` argument to\n`tf.data.experimental.service.distribute` describes how to leverage multiple\nworkers to process the input dataset. Currently, there are two processing\nmodes to choose from: \"distributed_epoch\" and \"parallel_epochs\".\n\n\"distributed_epoch\" means that the dataset will be split across all tf.data\nservice workers. The dispatcher produces \"splits\" for the dataset and sends\nthem to workers for further processing. For example, if a dataset begins with\na list of filenames, the dispatcher will iterate through the filenames and\nsend the filenames to tf.data workers, which will perform the rest of the\ndataset transformations on those files. \"distributed_epoch\" is useful when\nyour model needs to see each element of the dataset exactly once, or if it\nneeds to see the data in a generally-sequential order. \"distributed_epoch\"\nonly works for datasets with splittable sources, such as\n`Dataset.from_tensor_slices`, `Dataset.list_files`, or `Dataset.range`.\n\n\"parallel_epochs\" means that the entire input dataset will be processed\nindependently by each of the tf.data service workers. For this reason, it is\nimportant to shuffle data (e.g. filenames) non-deterministically, so that each\nworker will process the elements of the dataset in a different order.\n\"parallel_epochs\" can be used to distribute datasets that aren't splittable.\n\nBefore using the tf.data service, it is useful to first measure the potential\nperformance improvement. To do this, add\n\nat the end of your dataset, and see how it affects your model's step time.\n`take(1).cache().repeat()` will cache the first element of your dataset and\nproduce it repeatedly. This should make the dataset very fast, so that the\nmodel becomes the bottleneck and you can identify the ideal model speed. With\nenough workers, the tf.data service should be able to achieve similar speed.\n\ntf.data servers should be brought up alongside your training jobs, and brought\ndown when the jobs are finished. The tf.data service uses one `DispatchServer`\nand any number of `WorkerServers`. See\nhttps://github.com/tensorflow/ecosystem/tree/master/data_service for an\nexample of using Google Kubernetes Engine (GKE) to manage the tf.data service.\nThe server implementation in tf_std_data_server.py is not GKE-specific, and\ncan be used to run the tf.data service in other contexts.\n\nBy default, the tf.data dispatch server stores its state in-memory, making it\na single point of failure during training. To avoid this, pass\n`fault_tolerant_mode=True` when creating your `DispatchServer`. Dispatcher\nfault tolerance requires `work_dir` to be configured and accessible from the\ndispatcher both before and after restart (e.g. a GCS path). With fault\ntolerant mode enabled, the dispatcher will journal its state to the work\ndirectory so that no state is lost when the dispatcher is restarted.\n\nWorkerServers may be freely restarted, added, or removed during training. At\nstartup, workers will register with the dispatcher and begin processing all\noutstanding jobs from the beginning.\n\nOnce you have a tf.data service cluster running, take note of the dispatcher\nIP address and port. To connect to the service, you will use a string in the\nformat \"grpc://:\".\n\nBelow is a toy example that you can run yourself.\n\nSee the documentation of `tf.data.experimental.service.distribute` for more\ndetails about using the `distribute` transformation.\n\n`class DispatcherConfig`: Configuration class for tf.data service dispatchers.\n\n`class WorkerConfig`: Configuration class for tf.data service dispatchers.\n\n`distribute(...)`: A transformation that moves dataset processing to the\ntf.data service.\n\n`from_dataset_id(...)`: Creates a dataset which reads data from the tf.data\nservice.\n\n`register_dataset(...)`: Registers a dataset with the tf.data service.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.SparseTensorStructure", "path": "compat/v1/data/experimental/sparsetensorstructure", "type": "tf.compat", "text": "\nDEPRECATED FUNCTION\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.SqlDataset", "path": "compat/v1/data/experimental/sqldataset", "type": "tf.compat", "text": "\nA `Dataset` consisting of the results from a SQL query.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.StatsAggregator", "path": "compat/v1/data/experimental/statsaggregator", "type": "tf.compat", "text": "\nA stateful resource that aggregates statistics from one or more iterators.\n\nTo record statistics, use one of the custom transformation functions defined\nin this module when defining your `tf.data.Dataset`. All statistics will be\naggregated by the `StatsAggregator` that is associated with a particular\niterator (see below). For example, to record the latency of producing each\nelement by iterating over a dataset:\n\nTo associate a `StatsAggregator` with a `tf.data.Dataset` object, use the\nfollowing pattern:\n\nTo get a protocol buffer summary of the currently aggregated statistics, use\nthe `StatsAggregator.get_summary()` tensor. The easiest way to do this is to\nadd the returned tensor to the `tf.GraphKeys.SUMMARIES` collection, so that\nthe summaries will be included with any existing summaries.\n\nView source\n\nReturns a string `tf.Tensor` that summarizes the aggregated statistics.\n\nThe returned tensor will contain a serialized `tf.compat.v1.summary.Summary`\nprotocol buffer, which can be used with the standard TensorBoard logging\nfacilities.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.TensorArrayStructure", "path": "compat/v1/data/experimental/tensorarraystructure", "type": "tf.compat", "text": "\nDEPRECATED FUNCTION\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.experimental.TensorStructure", "path": "compat/v1/data/experimental/tensorstructure", "type": "tf.compat", "text": "\nDEPRECATED FUNCTION\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.FixedLengthRecordDataset", "path": "compat/v1/data/fixedlengthrecorddataset", "type": "tf.compat", "text": "\nA `Dataset` of fixed-length records from one or more binary files.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.get_output_classes", "path": "compat/v1/data/get_output_classes", "type": "tf.compat", "text": "\nReturns the output classes for elements of the input dataset / iterator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.get_output_shapes", "path": "compat/v1/data/get_output_shapes", "type": "tf.compat", "text": "\nReturns the output shapes for elements of the input dataset / iterator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.get_output_types", "path": "compat/v1/data/get_output_types", "type": "tf.compat", "text": "\nReturns the output shapes for elements of the input dataset / iterator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.Iterator", "path": "compat/v1/data/iterator", "type": "tf.compat", "text": "\nRepresents the state of iterating through a `Dataset`.\n\nThe expected values are `tf.Tensor` and `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a new, uninitialized `Iterator` based on the given handle.\n\nThis method allows you to define a \"feedable\" iterator where you can choose\nbetween concrete iterators by feeding a value in a `tf.Session.run` call. In\nthat case, `string_handle` would be a `tf.compat.v1.placeholder`, and you\nwould feed it with the value of `tf.data.Iterator.string_handle` in each step.\n\nFor example, if you had two iterators that marked the current position in a\ntraining dataset and a test dataset, you could choose which to use in each\nstep as follows:\n\nView source\n\nCreates a new, uninitialized `Iterator` with the given structure.\n\nThis iterator-constructing method can be used to create an iterator that is\nreusable with many different datasets.\n\nThe returned iterator is not bound to a particular dataset, and it has no\n`initializer`. To initialize the iterator, run the operation returned by\n`Iterator.make_initializer(dataset)`.\n\nThe following is an example\n\nView source\n\nReturns a nested structure of `tf.Tensor`s representing the next element.\n\nIn graph mode, you should typically call this method once and use its result\nas the input to another computation. A typical loop will then call\n`tf.Session.run` on the result of that computation. The loop will terminate\nwhen the `Iterator.get_next()` operation raises `tf.errors.OutOfRangeError`.\nThe following skeleton shows how to use this method when building a training\nloop:\n\nView source\n\nView source\n\nReturns a `tf.Operation` that initializes this iterator on `dataset`.\n\nView source\n\nReturns a string-valued `tf.Tensor` that represents this iterator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.make_initializable_iterator", "path": "compat/v1/data/make_initializable_iterator", "type": "tf.compat", "text": "\nCreates an iterator for elements of `dataset`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.make_one_shot_iterator", "path": "compat/v1/data/make_one_shot_iterator", "type": "tf.compat", "text": "\nCreates an iterator for elements of `dataset`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.TextLineDataset", "path": "compat/v1/data/textlinedataset", "type": "tf.compat", "text": "\nA `Dataset` comprising lines from one or more text files.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.data.TFRecordDataset", "path": "compat/v1/data/tfrecorddataset", "type": "tf.compat", "text": "\nA `Dataset` comprising records from one or more TFRecord files.\n\nInherits From: `Dataset`, `Dataset`\n\nView source\n\nApplies a transformation function to this dataset.\n\n`apply` enables chaining of custom `Dataset` transformations, which are\nrepresented as functions that take one `Dataset` argument and return a\ntransformed `Dataset`.\n\nView source\n\nReturns an iterator which converts all elements of the dataset to numpy.\n\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element\nshapes and types, print dataset elements directly instead of using\n`as_numpy_iterator`.\n\nThis method requires that you are running in eager mode and the dataset's\nelement_spec contains only `TensorSpec` components.\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n\nView source\n\nCombines consecutive elements of this dataset into batches.\n\nThe components of the resulting element will have an additional outer\ndimension, which will be `batch_size` (or `N % batch_size` for the last\nelement if `batch_size` does not divide the number of input elements `N`\nevenly and `drop_remainder` is `False`). If your program depends on the\nbatches having the same outer dimension, you should set the `drop_remainder`\nargument to `True` to prevent the smaller batch from being produced.\n\nView source\n\nCaches the elements in this dataset.\n\nThe first time the dataset is iterated over, its elements will be cached\neither in the specified file or in memory. Subsequent iterations will use the\ncached data.\n\nWhen caching to a file, the cached data will persist across runs. Even the\nfirst iteration through the data will read from the cache file. Changing the\ninput pipeline before the call to `.cache()` will have no effect until the\ncache file is removed or the filename is changed.\n\nView source\n\nReturns the cardinality of the dataset, if known.\n\n`cardinality` may return `tf.data.INFINITE_CARDINALITY` if the dataset\ncontains an infinite number of elements or `tf.data.UNKNOWN_CARDINALITY` if\nthe analysis fails to determine the number of elements in the dataset (e.g.\nwhen the dataset source is a file).\n\nView source\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n\nView source\n\nEnumerates the elements of this dataset.\n\nIt is similar to python's `enumerate`.\n\nView source\n\nFilters this dataset according to `predicate`.\n\nView source\n\nFilters this dataset according to `predicate`. (deprecated)\n\nView source\n\nMaps `map_func` across this dataset and flattens the result.\n\nUse `flat_map` if you want to make sure that the order of your dataset stays\nthe same. For example, to flatten a dataset of batches into a dataset of their\nelements:\n\n`tf.data.Dataset.interleave()` is a generalization of `flat_map`, since\n`flat_map` produces the same output as\n`tf.data.Dataset.interleave(cycle_length=1)`\n\nView source\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated\narguments)\n\nThe `generator` argument must be a callable object that returns an object that\nsupports the `iter()` protocol (e.g. a generator function).\n\nThe elements generated by `generator` must be compatible with either the given\n`output_signature` argument or with the given `output_types` and (optionally)\n`output_shapes` arguments, whichiver was specified.\n\nThe recommended way to call `from_generator` is to use the `output_signature`\nargument. In this case the output will be assumed to consist of objects with\nthe classes, shapes and types defined by `tf.TypeSpec` objects from\n`output_signature` argument:\n\nThere is also a deprecated way to call `from_generator` by either with\n`output_types` argument alone or together with `output_shapes` argument. In\nthis case the output of the function will be assumed to consist of `tf.Tensor`\nobjects with with the types defined by `output_types` and with the shapes\nwhich are either unknown or defined by `output_shapes`.\n\nView source\n\nSplits each rank-N `tf.sparse.SparseTensor` in this dataset row-wise.\n(deprecated)\n\nView source\n\nCreates a `Dataset` whose elements are slices of the given tensors.\n\nThe given tensors are sliced along their first dimension. This operation\npreserves the structure of the input tensors, removing the first dimension of\neach tensor and using it as the dataset dimension. All input tensors must have\nthe same size in their first dimensions.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n\n`from_tensors` produces a dataset containing only a single element. To slice\nthe input tensor into multiple elements, use `from_tensor_slices` instead.\n\nNote that if `tensors` contains a NumPy array, and eager execution is not\nenabled, the values will be embedded in the graph as one or more `tf.constant`\noperations. For large datasets (> 1 GB), this can waste memory and run into\nbyte limits of graph serialization. If `tensors` contains one or more large\nNumPy arrays, consider the alternative described in this guide.\n\nView source\n\nMaps `map_func` across this dataset, and interleaves the results.\n\nFor example, you can use `Dataset.interleave()` to process many input files\nconcurrently:\n\nThe `cycle_length` and `block_length` arguments control the order in which\nelements are produced. `cycle_length` controls the number of input elements\nthat are processed concurrently. If you set `cycle_length` to 1, this\ntransformation will handle one input element at a time, and will produce\nidentical results to `tf.data.Dataset.flat_map`. In general, this\ntransformation will apply `map_func` to `cycle_length` input elements, open\niterators on the returned `Dataset` objects, and cycle through them producing\n`block_length` consecutive elements from each iterator, and consuming the next\ninput element each time it reaches the end of an iterator.\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`interleave` will use multiple threads to fetch elements. If determinism isn't\nrequired, it can also improve performance to set `deterministic=False`.\n\nView source\n\nA dataset of all files matching one or more glob patterns.\n\nThe `file_pattern` argument should be a small number of glob patterns. If your\nfilenames have already been globbed, use\n`Dataset.from_tensor_slices(filenames)` instead, as re-globbing every filename\nwith `list_files` may result in poor performance with remote storage systems.\n\nIf we had the following files on our filesystem:\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nCreates an iterator for elements of this dataset. (deprecated)\n\nView source\n\nMaps `map_func` across the elements of this dataset.\n\nThis transformation applies `map_func` to each element of this dataset, and\nreturns a new dataset containing the transformed elements, in the same order\nas they appeared in the input. `map_func` can be used to change both the\nvalues and the structure of a dataset's elements. For example, adding 1 to\neach element, or projecting a subset of element components.\n\nThe input signature of `map_func` is determined by the structure of each\nelement in this dataset.\n\nThe value or values returned by `map_func` determine the structure of each\nelement in the returned dataset.\n\n`map_func` can accept as arguments and return any type of dataset element.\n\nNote that irrespective of the context in which `map_func` is defined (eager\nvs. graph), tf.data traces the function and executes it as a graph. To use\nPython code inside of the function you have a few options:\n\n1) Rely on AutoGraph to convert Python code into an equivalent graph\ncomputation. The downside of this approach is that AutoGraph can convert some\nbut not all Python code.\n\n2) Use `tf.py_function`, which allows you to write arbitrary Python code but\nwill generally result in worse performance than 1). For example:\n\n3) Use `tf.numpy_function`, which also allows you to write arbitrary Python\ncode. Note that `tf.py_function` accepts `tf.Tensor` whereas\n`tf.numpy_function` accepts numpy arrays and returns only numpy arrays. For\nexample:\n\nNote that the use of `tf.numpy_function` and `tf.py_function` in general\nprecludes the possibility of executing user-defined transformations in\nparallel (because of Python GIL).\n\nPerformance can often be improved by setting `num_parallel_calls` so that\n`map` will use multiple threads to process elements. If deterministic order\nisn't required, it can also improve performance to set `deterministic=False`.\n\nView source\n\nMaps `map_func` across the elements of this dataset. (deprecated)\n\nView source\n\nReturns the options for this dataset and its inputs.\n\nView source\n\nCombines consecutive elements of this dataset into padded batches.\n\nThis transformation combines multiple consecutive elements of the input\ndataset into a single element.\n\nLike `tf.data.Dataset.batch`, the components of the resulting element will\nhave an additional outer dimension, which will be `batch_size` (or `N %\nbatch_size` for the last element if `batch_size` does not divide the number of\ninput elements `N` evenly and `drop_remainder` is `False`). If your program\ndepends on the batches having the same outer dimension, you should set the\n`drop_remainder` argument to `True` to prevent the smaller batch from being\nproduced.\n\nUnlike `tf.data.Dataset.batch`, the input elements to be batched may have\ndifferent shapes, and this transformation will pad each component to the\nrespective shape in `padded_shapes`. The `padded_shapes` argument determines\nthe resulting shape for each dimension of each component in an output element:\n\nSee also `tf.data.experimental.dense_to_sparse_batch`, which combines elements\nthat may have different shapes into a `tf.sparse.SparseTensor`.\n\nView source\n\nCreates a `Dataset` that prefetches elements from this dataset.\n\nMost dataset input pipelines should end with a call to `prefetch`. This allows\nlater elements to be prepared while the current element is being processed.\nThis often improves latency and throughput, at the cost of using additional\nmemory to store prefetched elements.\n\nView source\n\nCreates a `Dataset` of a step-separated range of values.\n\nView source\n\nReduces the input dataset to a single element.\n\nThe transformation calls `reduce_func` successively on every element of the\ninput dataset until the dataset is exhausted, aggregating information in its\ninternal state. The `initial_state` argument is used for the initial state and\nthe final state is returned as the result.\n\nView source\n\nRepeats this dataset so each original value is seen `count` times.\n\nView source\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain\nall elements of A whose index mod n = i.\n\nThis dataset operator is very useful when running distributed training, as it\nallows each worker to read a unique subset.\n\nWhen reading a single input file, you can shard elements as follows:\n\nView source\n\nRandomly shuffles the elements of this dataset.\n\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples\nelements from this buffer, replacing the selected elements with new elements.\nFor perfect shuffling, a buffer size greater than or equal to the full size of\nthe dataset is required.\n\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is\nset to 1,000, then `shuffle` will initially select a random element from only\nthe first 1,000 elements in the buffer. Once an element is selected, its space\nin the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the\n1,000 element buffer.\n\n`reshuffle_each_iteration` controls whether the shuffle order should be\ndifferent for each epoch. In TF 1.X, the idiomatic way to create epochs was\nthrough the `repeat` transformation:\n\nIn TF 2.0, `tf.data.Dataset` objects are Python iterables which makes it\npossible to also create epochs through Python iteration:\n\nView source\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n\nView source\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n\nView source\n\nSplits elements of a dataset into multiple elements.\n\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where\n`B` may vary for each input element, then for each element in the dataset, the\nunbatched dataset will contain `B` consecutive elements of shape `[a0, a1,\n...]`.\n\nView source\n\nCombines (nests of) input elements into a dataset of (nests of) windows.\n\nA \"window\" is a finite dataset of flat elements of size `size` (or possibly\nfewer if there are not enough input elements to fill the window and\n`drop_remainder` evaluates to `False`).\n\nThe `shift` argument determines the number of input elements by which the\nwindow moves on each iteration. If windows and elements are both numbered\nstarting at 0, the first element in window `k` will be element `k * shift` of\nthe input dataset. In particular, the first element of the first window will\nalways be the first element of the input dataset.\n\nThe `stride` argument determines the stride of the input elements, and the\n`shift` argument determines the shift of the window.\n\nNote that when the `window` transformation is applied to a dataset of nested\nelements, it produces a dataset of nested windows.\n\nView source\n\nReturns a new `tf.data.Dataset` with the given options set.\n\nThe options are \"global\" in the sense they apply to the entire dataset. If\noptions are set multiple times, they are merged as long as different options\ndo not use different non-default values.\n\nView source\n\nCreates a `Dataset` by zipping together the given datasets.\n\nThis method has similar semantics to the built-in `zip()` function in Python,\nwith the main difference being that the `datasets` argument can be an\narbitrary nested structure of `Dataset` objects.\n\nView source\n\nView source\n\nCreates an iterator for elements of this dataset.\n\nThe returned iterator implements the Python Iterator protocol.\n\nView source\n\nReturns the length of the dataset if it is known and finite.\n\nThis method requires that you are running in eager mode, and that the length\nof the dataset is known and non-infinite. When the length may be unknown or\ninfinite, or if you are running in graph mode, use\n`tf.data.Dataset.cardinality` instead.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.debugging", "path": "compat/v1/debugging", "type": "tf.compat", "text": "\nPublic API for tf.debugging namespace.\n\n`experimental` module: Public API for tf.debugging.experimental namespace.\n\n`Assert(...)`: Asserts that the given condition is true.\n\n`assert_all_finite(...)`: Assert that the tensor does not contain any NaN's or\nInf's.\n\n`assert_equal(...)`: Assert the condition `x == y` holds element-wise.\n\n`assert_greater(...)`: Assert the condition `x > y` holds element-wise.\n\n`assert_greater_equal(...)`: Assert the condition `x >= y` holds element-wise.\n\n`assert_integer(...)`: Assert that `x` is of integer dtype.\n\n`assert_less(...)`: Assert the condition `x < y` holds element-wise.\n\n`assert_less_equal(...)`: Assert the condition `x <= y` holds element-wise.\n\n`assert_near(...)`: Assert the condition `x` and `y` are close element-wise.\n\n`assert_negative(...)`: Assert the condition `x < 0` holds element-wise.\n\n`assert_non_negative(...)`: Assert the condition `x >= 0` holds element-wise.\n\n`assert_non_positive(...)`: Assert the condition `x <= 0` holds element-wise.\n\n`assert_none_equal(...)`: Assert the condition `x != y` holds element-wise.\n\n`assert_positive(...)`: Assert the condition `x > 0` holds element-wise.\n\n`assert_proper_iterable(...)`: Static assert that values is a \"proper\"\niterable.\n\n`assert_rank(...)`: Assert `x` has rank equal to `rank`.\n\n`assert_rank_at_least(...)`: Assert `x` has rank equal to `rank` or higher.\n\n`assert_rank_in(...)`: Assert `x` has rank in `ranks`.\n\n`assert_same_float_dtype(...)`: Validate and return float type based on\n`tensors` and `dtype`.\n\n`assert_scalar(...)`: Asserts that the given `tensor` is a scalar (i.e. zero-\ndimensional).\n\n`assert_shapes(...)`: Assert tensor shapes and dimension size relationships\nbetween tensors.\n\n`assert_type(...)`: Statically asserts that the given `Tensor` is of the\nspecified type.\n\n`check_numerics(...)`: Checks a tensor for NaN and Inf values.\n\n`disable_check_numerics(...)`: Disable the eager/graph unified numerics\nchecking mechanism.\n\n`enable_check_numerics(...)`: Enable tensor numerics checking in an\neager/graph unified fashion.\n\n`get_log_device_placement(...)`: Get if device placements are logged.\n\n`is_finite(...)`: Returns which elements of x are finite.\n\n`is_inf(...)`: Returns which elements of x are Inf.\n\n`is_nan(...)`: Returns which elements of x are NaN.\n\n`is_non_decreasing(...)`: Returns `True` if `x` is non-decreasing.\n\n`is_numeric_tensor(...)`: Returns `True` if the elements of `tensor` are\nnumbers.\n\n`is_strictly_increasing(...)`: Returns `True` if `x` is strictly increasing.\n\n`set_log_device_placement(...)`: Set if device placements should be logged.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.debugging.assert_shapes", "path": "compat/v1/debugging/assert_shapes", "type": "tf.compat", "text": "\nAssert tensor shapes and dimension size relationships between tensors.\n\nThis Op checks that a collection of tensors shape relationships satisfies\ngiven constraints.\n\nExample of adding a dependency to an operation:\n\nIf `x`, `y`, `param` or `scalar` does not have a shape that satisfies all\nspecified constraints, `message`, as well as the first `summarize` entries of\nthe first encountered violating tensor are printed, and `InvalidArgumentError`\nis raised.\n\nSize entries in the specified shapes are checked against other entries by\ntheir hash, except:\n\nIf the first entry of a shape is `...` (type `Ellipsis`) or '*' that indicates\na variable number of outer dimensions of unspecified size, i.e. the constraint\napplies to the inner-most dimensions only.\n\nScalar tensors and specified shapes of length zero (excluding the 'inner-most'\nprefix) are both treated as having a single dimension of size one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.debugging.experimental", "path": "compat/v1/debugging/experimental", "type": "tf.compat", "text": "\nPublic API for tf.debugging.experimental namespace.\n\n`disable_dump_debug_info(...)`: Disable the currently-enabled debugging\ndumping.\n\n`enable_dump_debug_info(...)`: Enable dumping debugging information from a\nTensorFlow program.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.decode_csv", "path": "compat/v1/decode_csv", "type": "tf.compat", "text": "\nConvert CSV records to tensors. Each column maps to one tensor.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.io.decode_csv`\n\nRFC 4180 format is expected for the CSV records.\n(https://tools.ietf.org/html/rfc4180) Note that we allow leading and trailing\nspaces with int or float field.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.decode_raw", "path": "compat/v1/decode_raw", "type": "tf.compat", "text": "\nConvert raw byte strings into tensors. (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.io.decode_raw`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.delete_session_tensor", "path": "compat/v1/delete_session_tensor", "type": "tf.compat", "text": "\nDelete the tensor for the given tensor handle.\n\nThis is EXPERIMENTAL and subject to change.\n\nDelete the tensor of a given tensor handle. The tensor is produced in a\nprevious run() and stored in the state of the session.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.depth_to_space", "path": "compat/v1/depth_to_space", "type": "tf.compat", "text": "\nDepthToSpace for tensors of type T.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.depth_to_space`\n\nRearranges data from depth into blocks of spatial data. This is the reverse\ntransformation of SpaceToDepth. More specifically, this op outputs a copy of\nthe input tensor where values from the `depth` dimension are moved in spatial\nblocks to the `height` and `width` dimensions. The attr `block_size` indicates\nthe input block size and how the data is moved.\n\nThe `data_format` attr specifies the layout of the input and output tensors\nwith the following options: \"NHWC\": `[ batch, height, width, channels ]`\n\"NCHW\": `[ batch, channels, height, width ]` \"NCHW_VECT_C\": `qint8 [ batch,\nchannels / 4, height, width, 4 ]`\n\nIt is useful to consider the operation as transforming a 6-D Tensor. e.g. for\ndata_format = NHWC, Each element in the input tensor can be specified via 6\ncoordinates, ordered by decreasing memory layout significance as:\nn,iY,iX,bY,bX,oC (where n=batch index, iX, iY means X or Y coordinates within\nthe input image, bX, bY means coordinates within the output block, oC means\noutput channels). The output would be the input transposed to the following\nlayout: n,iY,bY,iX,bX,oC\n\nThis operation is useful for resizing the activations between convolutions\n(but keeping all data), e.g. instead of pooling. It is also useful for\ntraining purely convolutional models.\n\nFor example, given an input of shape `[1, 1, 1, 4]`, data_format = \"NHWC\" and\nblock_size = 2:\n\nThis operation will output a tensor of shape `[1, 2, 2, 1]`:\n\nHere, the input has a batch of 1 and each batch element has shape `[1, 1, 4]`,\nthe corresponding output will have 2x2 elements and will have a depth of 1\nchannel (1 = `4 / (block_size * block_size)`). The output element shape is\n`[2, 2, 1]`.\n\nFor an input tensor with larger depth, here of shape `[1, 1, 1, 12]`, e.g.\n\nThis operation, for block size of 2, will return the following tensor of shape\n`[1, 2, 2, 3]`\n\nSimilarly, for the following input of shape `[1 2 2 4]`, and a block size of\n2:\n\nthe operator will return the following tensor of shape `[1 4 4 1]`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.device", "path": "compat/v1/device", "type": "tf.compat", "text": "\nWrapper for `Graph.device()` using the default graph.\n\nSee `tf.Graph.device` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.DeviceSpec", "path": "compat/v1/devicespec", "type": "tf.compat", "text": "\nRepresents a (possibly partial) specification for a TensorFlow device.\n\nInherits From: `DeviceSpec`\n\n`DeviceSpec`s are used throughout TensorFlow to describe where state is stored\nand computations occur. Using `DeviceSpec` allows you to parse device spec\nstrings to verify their validity, merge them or compose them programmatically.\n\nWith eager execution disabled (by default in TensorFlow 1.x and by calling\ndisable_eager_execution() in TensorFlow 2.x), the following syntax can be\nused:\n\nA `DeviceSpec` consists of 5 components -- each of which is optionally\nspecified:\n\nView source\n\nConstruct a `DeviceSpec` from a string.\n\nView source\n\nReturns a new DeviceSpec which incorporates `dev`.\n\nWhen combining specs, `dev` will take precedence over the current spec. So for\ninstance:\n\nis equivalent to:\n\nView source\n\nMerge the properties of \"dev\" into this `DeviceSpec`.\n\nView source\n\nParse a `DeviceSpec` name into its components.\n\n2.x behavior change: In TensorFlow 1.x, this function mutates its own state\nand returns itself. In 2.x, DeviceSpecs are immutable, and this function will\nreturn a DeviceSpec which contains the spec.\n\nRecommended:\n\nWill work in 1.x and 2.x (though deprecated in 2.x):\n\nWill NOT work in 2.x:\n\nIn general, `DeviceSpec.from_string` should completely replace\n`DeviceSpec.parse_from_string`, and `DeviceSpec.replace` should completely\nreplace setting attributes directly.\n\nView source\n\nConvenience method for making a new DeviceSpec by overriding fields.\n\nView source\n\nReturn a string representation of this `DeviceSpec`.\n\nView source\n\nChecks if the `other` DeviceSpec is same as the current instance, eg have\n\nsame value for all the internal fields.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.Dimension", "path": "compat/v1/dimension", "type": "tf.compat", "text": "\nRepresents the value of one dimension in a TensorShape.\n\nView source\n\nRaises an exception if `other` is not compatible with this Dimension.\n\nView source\n\nReturns true if `other` is compatible with this Dimension.\n\nTwo known Dimensions are compatible if they have the same value. An unknown\nDimension is compatible with all other Dimensions.\n\nView source\n\nReturns a Dimension that combines the information in `self` and `other`.\n\nDimensions are combined as follows:\n\nView source\n\nReturns the sum of `self` and `other`.\n\nDimensions are summed as follows:\n\nView source\n\nDEPRECATED: Use `__floordiv__` via `x // y` instead.\n\nThis function exists only for backwards compatibility purposes; new code\nshould use `__floordiv__` via the syntax `x // y`. Using `x // y` communicates\nclearly that the result rounds down, and is forward compatible to Python 3.\n\nView source\n\nReturns true if `other` has the same known value as this Dimension.\n\nView source\n\nReturns the quotient of `self` and `other` rounded down.\n\nDimensions are divided as follows:\n\nView source\n\nReturns True if `self` is known to be greater than or equal to `other`.\n\nDimensions are compared as follows:\n\nView source\n\nReturns True if `self` is known to be greater than `other`.\n\nDimensions are compared as follows:\n\nView source\n\nReturns True if `self` is known to be less than or equal to `other`.\n\nDimensions are compared as follows:\n\nView source\n\nReturns True if `self` is known to be less than `other`.\n\nDimensions are compared as follows:\n\nView source\n\nReturns `self` modulo `other`.\n\nDimension modulo are computed as follows:\n\nView source\n\nReturns the product of `self` and `other`.\n\nDimensions are summed as follows:\n\nView source\n\nReturns true if `other` has a different known value from `self`.\n\nView source\n\nReturns the sum of `other` and `self`.\n\nView source\n\nUse `__floordiv__` via `x // y` instead.\n\nThis function exists only to have a better error message. Instead of:\n`TypeError: unsupported operand type(s) for /: 'int' and 'Dimension'`, this\nfunction will explicitly call for usage of `//` instead.\n\nView source\n\nReturns the quotient of `other` and `self` rounded down.\n\nView source\n\nReturns `other` modulo `self`.\n\nView source\n\nReturns the product of `self` and `other`.\n\nView source\n\nReturns the subtraction of `self` from `other`.\n\nView source\n\nUse `__floordiv__` via `x // y` instead.\n\nThis function exists only to have a better error message. Instead of:\n`TypeError: unsupported operand type(s) for /: 'int' and 'Dimension'`, this\nfunction will explicitly call for usage of `//` instead.\n\nView source\n\nReturns the subtraction of `other` from `self`.\n\nDimensions are subtracted as follows:\n\nView source\n\nUse `__floordiv__` via `x // y` instead.\n\nThis function exists only to have a better error message. Instead of:\n`TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'`, this\nfunction will explicitly call for usage of `//` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_control_flow_v2", "path": "compat/v1/disable_control_flow_v2", "type": "tf.compat", "text": "\nOpts out of control flow v2.\n\nIf your code needs tf.disable_control_flow_v2() to be called to work properly\nplease file a bug.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_eager_execution", "path": "compat/v1/disable_eager_execution", "type": "tf.compat", "text": "\nDisables eager execution.\n\nThis function can only be called before any Graphs, Ops, or Tensors have been\ncreated. It can be used at the beginning of the program for complex migration\nprojects from TensorFlow 1.x to 2.x.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_resource_variables", "path": "compat/v1/disable_resource_variables", "type": "tf.compat", "text": "\nOpts out of resource variables. (deprecated)\n\nIf your code needs tf.disable_resource_variables() to be called to work\nproperly please file a bug.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_tensor_equality", "path": "compat/v1/disable_tensor_equality", "type": "tf.compat", "text": "\nCompare Tensors by their id and be hashable.\n\nThis is a legacy behaviour of TensorFlow and is highly discouraged.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_v2_behavior", "path": "compat/v1/disable_v2_behavior", "type": "tf.compat", "text": "\nDisables TensorFlow 2.x behaviors.\n\nThis function can be called at the beginning of the program (before `Tensors`,\n`Graphs` or other structures have been created, and before devices have been\ninitialized. It switches all global behaviors that are different between\nTensorFlow 1.x and 2.x to behave as intended for 1.x.\n\nUser can call this function to disable 2.x behavior during complex migrations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.disable_v2_tensorshape", "path": "compat/v1/disable_v2_tensorshape", "type": "tf.compat", "text": "\nDisables the V2 TensorShape behavior and reverts to V1 behavior.\n\nSee docstring for `enable_v2_tensorshape` for details about the new behavior.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute", "path": "compat/v1/distribute", "type": "tf.compat", "text": "\nLibrary for running a computation across multiple devices.\n\nThe intent of this library is that you can write an algorithm in a stylized\nway and it will be usable with a variety of different `tf.distribute.Strategy`\nimplementations. Each descendant will implement a different strategy for\ndistributing the algorithm across multiple devices/machines. Furthermore,\nthese changes can be hidden inside the specific layers and other library\nclasses that need special treatment to run in a distributed setting, so that\nmost users' model definition code can run unchanged. The\n`tf.distribute.Strategy` API works the same way with eager and graph\nexecution.\n\nGuides\n\nTutorials\n\nDistributed Training Tutorials\n\nThe tutorials cover how to use `tf.distribute.Strategy` to do distributed\ntraining with native Keras APIs, custom training loops, and Esitmator APIs.\nThey also cover how to save/load model when using `tf.distribute.Strategy`.\n\nGlossary\n\nParameter servers: These are machines that hold a single copy of\nparameters/variables, used by some strategies (right now just\n`tf.distribute.experimental.ParameterServerStrategy`). All replicas that want\nto operate on a variable retrieve it at the beginning of a step and send an\nupdate to be applied at the end of the step. These can in priniciple support\neither sync or async training, but right now we only have support for async\ntraining with parameter servers. Compare to\n`tf.distribute.experimental.CentralStorageStrategy`, which puts all variables\non a single device on the same machine (and does sync training), and\n`tf.distribute.MirroredStrategy`, which mirrors variables to multiple devices\n(see below).\n\nReplica context vs. Cross-replica context vs Update context\n\nA replica context applies when you execute the computation function that was\ncalled with `strategy.run`. Conceptually, you're in replica context when\nexecuting the computation function that is being replicated.\n\nAn update context is entered in a `tf.distribute.StrategyExtended.update`\ncall.\n\nAn cross-replica context is entered when you enter a `strategy.scope`. This is\nuseful for calling `tf.distribute.Strategy` methods which operate across the\nreplicas (like `reduce_to()`). By default you start in a replica context (the\n\"default single replica context\") and then some methods can switch you back\nand forth.\n\nDistributed value: Distributed value is represented by the base class\n`tf.distribute.DistributedValues`. `tf.distribute.DistributedValues` is useful\nto represent values on multiple devices, and it contains a map from replica id\nto values. Two representative kinds of `tf.distribute.DistributedValues` are\n\"PerReplica\" and \"Mirrored\" values.\n\n\"PerReplica\" values exist on the worker devices, with a different value for\neach replica. They are produced by iterating through a distributed dataset\nreturned by `tf.distribute.Strategy.experimental_distribute_dataset` and\n`tf.distribute.Strategy.distribute_datasets_from_function`. They are also the\ntypical result returned by `tf.distribute.Strategy.run`.\n\n\"Mirrored\" values are like \"PerReplica\" values, except we know that the value\non all replicas are the same. We can safely read a \"Mirrored\" value in a\ncross-replica context by using the value on any replica.\n\nUnwrapping and merging: Consider calling a function `fn` on multiple replicas,\nlike `strategy.run(fn, args=[w])` with an argument `w` that is a\n`tf.distribute.DistributedValues`. This means `w` will have a map taking\nreplica id `0` to `w0`, replica id `1` to `w1`, etc. `strategy.run()` unwraps\n`w` before calling `fn`, so it calls `fn(w0)` on device `d0`, `fn(w1)` on\ndevice `d1`, etc. It then merges the return values from `fn()`, which leads to\none common object if the returned values are the same object from every\nreplica, or a `DistributedValues` object otherwise.\n\nReductions and all-reduce: A reduction is a method of aggregating multiple\nvalues into one value, like \"sum\" or \"mean\". If a strategy is doing sync\ntraining, we will perform a reduction on the gradients to a parameter from all\nreplicas before applying the update. All-reduce is an algorithm for performing\na reduction on values from multiple devices and making the result available on\nall of those devices.\n\nMirrored variables: These are variables that are created on multiple devices,\nwhere we keep the variables in sync by applying the same updates to every\ncopy. Mirrored variables are created with\n`tf.Variable(...synchronization=tf.VariableSynchronization.ON_WRITE...)`.\nNormally they are only used in synchronous training.\n\nSyncOnRead variables\n\nSyncOnRead variables are created by\n`tf.Variable(...synchronization=tf.VariableSynchronization.ON_READ...)`, and\nthey are created on multiple devices. In replica context, each component\nvariable on the local replica can perform reads and writes without\nsynchronization with each other. When the SyncOnRead variable is read in\ncross-replica context, the values from component variables are aggregated and\nreturned.\n\nSyncOnRead variables bring a lot of custom configuration difficulty to the\nunderlying logic, so we do not encourage users to instantiate and use\nSyncOnRead variable on their own. We have mainly used SyncOnRead variables for\nuse cases such as batch norm and metrics. For performance reasons, we often\ndon't need to keep these statistics in sync every step and they can be\naccumulated on each replica independently. The only time we want to sync them\nis reporting or checkpointing, which typically happens in cross-replica\ncontext. SyncOnRead variables are also often used by advanced users who want\nto control when variable values are aggregated. For example, users sometimes\nwant to maintain gradients independently on each replica for a couple of steps\nwithout aggregation.\n\nDistribute-aware layers\n\nLayers are generally called in a replica context, except when defining a Keras\nfunctional model. `tf.distribute.in_cross_replica_context` will let you\ndetermine which case you are in. If in a replica context, the\n`tf.distribute.get_replica_context` function will return the default replica\ncontext outside a strategy scope, `None` within a strategy scope, and a\n`tf.distribute.ReplicaContext` object inside a strategy scope and within a\n`tf.distribute.Strategy.run` function. The `ReplicaContext` object has an\n`all_reduce` method for aggregating across all replicas.\n\nNote that we provide a default version of `tf.distribute.Strategy` that is\nused when no other strategy is in scope, that provides the same API with\nreasonable default behavior.\n\n`cluster_resolver` module: Library imports for ClusterResolvers.\n\n`experimental` module: Public API for tf.distribute.experimental namespace.\n\n`class CrossDeviceOps`: Base class for cross-device reduction and broadcasting\nalgorithms.\n\n`class HierarchicalCopyAllReduce`: Hierarchical copy all-reduce implementation\nof CrossDeviceOps.\n\n`class InputContext`: A class wrapping information needed by an input\nfunction.\n\n`class InputReplicationMode`: Replication mode for input function.\n\n`class MirroredStrategy`: Synchronous training across multiple replicas on one\nmachine.\n\n`class NcclAllReduce`: NCCL all-reduce implementation of CrossDeviceOps.\n\n`class OneDeviceStrategy`: A distribution strategy for running on a single\ndevice.\n\n`class ReduceOp`: Indicates how a set of values should be reduced.\n\n`class ReductionToOneDevice`: A CrossDeviceOps implementation that copies\nvalues to one device to reduce.\n\n`class ReplicaContext`: A class with a collection of APIs that can be called\nin a replica context.\n\n`class RunOptions`: Run options for `strategy.run`.\n\n`class Server`: An in-process TensorFlow server, for use in distributed\ntraining.\n\n`class Strategy`: A list of devices with a state & compute distribution\npolicy.\n\n`class StrategyExtended`: Additional APIs for algorithms that need to be\ndistribution-aware.\n\n`experimental_set_strategy(...)`: Set a `tf.distribute.Strategy` as current\nwithout `with strategy.scope()`.\n\n`get_loss_reduction(...)`: `tf.distribute.ReduceOp` corresponding to the last\nloss reduction.\n\n`get_replica_context(...)`: Returns the current `tf.distribute.ReplicaContext`\nor `None`.\n\n`get_strategy(...)`: Returns the current `tf.distribute.Strategy` object.\n\n`has_strategy(...)`: Return if there is a current non-default\n`tf.distribute.Strategy`.\n\n`in_cross_replica_context(...)`: Returns `True` if in a cross-replica context.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.cluster_resolver", "path": "compat/v1/distribute/cluster_resolver", "type": "tf.compat", "text": "\nLibrary imports for ClusterResolvers.\n\nThis library contains all implementations of ClusterResolvers.\nClusterResolvers are a way of specifying cluster information for distributed\nexecution. Built on top of existing `ClusterSpec` framework, ClusterResolvers\nare a way for TensorFlow to communicate with various cluster management\nsystems (e.g. GCE, AWS, etc...).\n\n`class ClusterResolver`: Abstract class for all implementations of\nClusterResolvers.\n\n`class GCEClusterResolver`: ClusterResolver for Google Compute Engine.\n\n`class KubernetesClusterResolver`: ClusterResolver for Kubernetes.\n\n`class SimpleClusterResolver`: Simple implementation of ClusterResolver that\naccepts all attributes.\n\n`class SlurmClusterResolver`: ClusterResolver for system with Slurm workload\nmanager.\n\n`class TFConfigClusterResolver`: Implementation of a ClusterResolver which\nreads the TF_CONFIG EnvVar.\n\n`class TPUClusterResolver`: Cluster Resolver for Google Cloud TPUs.\n\n`class UnionResolver`: Performs a union on underlying ClusterResolvers.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.experimental", "path": "compat/v1/distribute/experimental", "type": "tf.compat", "text": "\nPublic API for tf.distribute.experimental namespace.\n\n`class CentralStorageStrategy`: A one-machine strategy that puts all variables\non a single device.\n\n`class CollectiveCommunication`: Cross device communication implementation.\n\n`class CollectiveHints`: Hints for collective operations like AllReduce.\n\n`class CommunicationImplementation`: Cross device communication\nimplementation.\n\n`class CommunicationOptions`: Options for cross device communications like\nAll-reduce.\n\n`class MultiWorkerMirroredStrategy`: A distribution strategy for synchronous\ntraining on multiple workers.\n\n`class ParameterServerStrategy`: An asynchronous multi-worker parameter server\ntf.distribute strategy.\n\n`class TPUStrategy`: TPU distribution strategy implementation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.experimental.CentralStorageStrategy", "path": "compat/v1/distribute/experimental/centralstoragestrategy", "type": "tf.compat", "text": "\nA one-machine strategy that puts all variables on a single device.\n\nInherits From: `Strategy`\n\nVariables are assigned to local CPU or the only GPU. If there is more than one\nGPU, compute operations (other than variable update operations) will be\nreplicated across all GPUs.\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy", "path": "compat/v1/distribute/experimental/multiworkermirroredstrategy", "type": "tf.compat", "text": "\nA distribution strategy for synchronous training on multiple workers.\n\nInherits From: `Strategy`\n\nThis strategy implements synchronous distributed training across multiple\nworkers, each with potentially multiple GPUs. Similar to\n`tf.distribute.MirroredStrategy`, it replicates all variables and computations\nto each local device. The difference is that it uses a distributed collective\nimplementation (e.g. all-reduce), so that multiple workers can work together.\n\nYou need to launch your program on each worker and configure\n`cluster_resolver` correctly. For example, if you are using\n`tf.distribute.cluster_resolver.TFConfigClusterResolver`, each worker needs to\nhave its corresponding `task_type` and `task_id` set in the `TF_CONFIG`\nenvironment variable. An example TF_CONFIG on worker-0 of a two worker cluster\nis:\n\nYour program runs on each worker as-is. Note that collectives require each\nworker to participate. All `tf.distribute` and non `tf.distribute` API may use\ncollectives internally, e.g. checkpointing and saving since reading a\n`tf.Variable` with `tf.VariableSynchronization.ON_READ` all-reduces the value.\nTherefore it's recommended to run exactly the same program on each worker.\nDispatching based on `task_type` or `task_id` of the worker is error-prone.\n\n`cluster_resolver.num_accelerators()` determines the number of GPUs the\nstrategy uses. If it's zero, the strategy uses the CPU. All workers need to\nuse the same number of devices, otherwise the behavior is undefined.\n\nThis strategy is not intended for TPU. Use `tf.distribute.TPUStrategy`\ninstead.\n\nAfter setting up TF_CONFIG, using this strategy is similar to using\n`tf.distribute.MirroredStrategy` and `tf.distribute.TPUStrategy`.\n\nYou can also write your own training loop:\n\nSee Multi-worker training with Keras for a detailed tutorial.\n\nSaving\n\nYou need to save and checkpoint on all workers instead of just one. This is\nbecause variables whose synchronization=ON_READ triggers aggregation during\nsaving. It's recommended to save to a different path on each worker to avoid\nrace conditions. Each worker saves the same thing. See Multi-worker training\nwith Keras tutorial for examples.\n\nKnown Issues\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.experimental.ParameterServerStrategy", "path": "compat/v1/distribute/experimental/parameterserverstrategy", "type": "tf.compat", "text": "\nAn asynchronous multi-worker parameter server tf.distribute strategy.\n\nInherits From: `Strategy`\n\nThis strategy requires two roles: workers and parameter servers. Variables and\nupdates to those variables will be assigned to parameter servers and other\noperations are assigned to workers.\n\nWhen each worker has more than one GPU, operations will be replicated on all\nGPUs. Even though operations may be replicated, variables are not and each\nworker shares a common view for which parameter server a variable is assigned\nto.\n\nBy default it uses `TFConfigClusterResolver` to detect configurations for\nmulti-worker training. This requires a 'TF_CONFIG' environment variable and\nthe 'TF_CONFIG' must have a cluster spec.\n\nThis class assumes each worker is running the same code independently, but\nparameter servers are running a standard server. This means that while each\nworker will synchronously compute a single gradient update across all GPUs,\nupdates between workers proceed asynchronously. Operations that occur only on\nthe first replica (such as incrementing the global step), will occur on the\nfirst replica of every worker.\n\nIt is expected to call `call_for_each_replica(fn, ...)` for any operations\nwhich potentially can be replicated across replicas (i.e. multiple GPUs) even\nif there is only CPU or one GPU. When defining the `fn`, extra caution needs\nto be taken:\n\n1) It is generally not recommended to open a device scope under the strategy's\nscope. A device scope (i.e. calling `tf.device`) will be merged with or\noverride the device for operations but will not change the device for\nvariables.\n\n2) It is also not recommended to open a colocation scope (i.e. calling\n`tf.compat.v1.colocate_with`) under the strategy's scope. For colocating\nvariables, use `strategy.extended.colocate_vars_with` instead. Colocation of\nops will possibly create device assignment conflicts.\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.experimental.TPUStrategy", "path": "compat/v1/distribute/experimental/tpustrategy", "type": "tf.compat", "text": "\nTPU distribution strategy implementation.\n\nInherits From: `Strategy`\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nRun `fn` on each replica, with the given arguments.\n\nExecutes ops specified by `fn` on each replica. If `args` or `kwargs` have\n\"per-replica\" values, such as those produced by a \"distributed `Dataset`\",\nwhen `fn` is executed on a particular replica, it will be executed with the\ncomponent of those \"per-replica\" values that correspond to that replica.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`all_reduce`.\n\nAll arguments in `args` or `kwargs` should either be nest of tensors or per-\nreplica objects containing tensors or composite tensors.\n\nUsers can pass strategy specific options to `options` argument. An example to\nenable bucketizing dynamic shapes in `TPUStrategy.run` is:\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.get_loss_reduction", "path": "compat/v1/distribute/get_loss_reduction", "type": "tf.compat", "text": "\n`tf.distribute.ReduceOp` corresponding to the last loss reduction.\n\nThis is used to decide whether loss should be scaled in optimizer (used only\nfor estimator + v1 optimizer use case).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.MirroredStrategy", "path": "compat/v1/distribute/mirroredstrategy", "type": "tf.compat", "text": "\nSynchronous training across multiple replicas on one machine.\n\nInherits From: `Strategy`\n\nThis strategy is typically used for training on one machine with multiple\nGPUs. For TPUs, use `tf.distribute.TPUStrategy`. To use `MirroredStrategy`\nwith multiple workers, please refer to\n`tf.distribute.experimental.MultiWorkerMirroredStrategy`.\n\nFor example, a variable created under a `MirroredStrategy` is a\n`MirroredVariable`. If no devices are specified in the constructor argument of\nthe strategy then it will use all the available GPUs. If no GPUs are found, it\nwill use the available CPUs. Note that TensorFlow treats all CPUs on a machine\nas a single device, and uses threads internally for parallelism.\n\nWhile using distribution strategies, all the variable creation should be done\nwithin the strategy's scope. This will replicate the variables across all the\nreplicas and keep them in sync using an all-reduce algorithm.\n\nVariables created inside a `MirroredStrategy` which is wrapped with a\n`tf.function` are still `MirroredVariables`.\n\n`experimental_distribute_dataset` can be used to distribute the dataset across\nthe replicas when writing your own training loop. If you are using `.fit` and\n`.compile` methods available in `tf.keras`, then `tf.keras` will handle the\ndistribution for you.\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.OneDeviceStrategy", "path": "compat/v1/distribute/onedevicestrategy", "type": "tf.compat", "text": "\nA distribution strategy for running on a single device.\n\nInherits From: `Strategy`\n\nUsing this strategy will place any variables created in its scope on the\nspecified device. Input distributed through this strategy will be prefetched\nto the specified device. Moreover, any functions called via `strategy.run`\nwill also be placed on the specified device as well.\n\nTypical usage of this strategy could be testing your code with the\ntf.distribute.Strategy API before switching to other strategies which actually\ndistribute to multiple devices/machines.\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.ReplicaContext", "path": "compat/v1/distribute/replicacontext", "type": "tf.compat", "text": "\nA class with a collection of APIs that can be called in a replica context.\n\nYou can use `tf.distribute.get_replica_context` to get an instance of\n`ReplicaContext`, which can only be called inside the function passed to\n`tf.distribute.Strategy.run`.\n\nThis identifies the replica among all replicas that are kept in sync. The\nvalue of the replica id can range from 0 to\n`tf.distribute.ReplicaContext.num_replicas_in_sync` \\- 1.\n\nView source\n\nAll-reduces `value` across all replicas.\n\nIt supports batched operations. You can pass a list of values and it attempts\nto batch them when possible. You can also specify `options` to indicate the\ndesired batching behavior, e.g. batch the values into multiple packs so that\nthey can better overlap with computations.\n\nNote that all replicas need to participate in the all-reduce, otherwise this\noperation hangs. Note that if there're multiple all-reduces, they need to\nexecute in the same order on all replicas. Dispatching all-reduce based on\nconditions is usually error-prone.\n\nThis API currently can only be called in the replica context. Other variants\nto reduce values across replicas are:\n\nView source\n\nMerge args across replicas and run `merge_fn` in a cross-replica context.\n\nThis allows communication and coordination when there are multiple calls to\nthe step_fn triggered by a call to `strategy.run(step_fn, ...)`.\n\nSee `tf.distribute.Strategy.run` for an explanation.\n\nIf not inside a distributed scope, this is equivalent to:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.Strategy", "path": "compat/v1/distribute/strategy", "type": "tf.compat", "text": "\nA list of devices with a state & compute distribution policy.\n\nSee the guide for overview and examples.\n\nIn general, when using a multi-worker `tf.distribute` strategy such as\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy()`, there is a\n`tf.distribute.cluster_resolver.ClusterResolver` associated with the strategy\nused, and such an instance is returned by this property.\n\nStrategies that intend to have an associated\n`tf.distribute.cluster_resolver.ClusterResolver` must set the relevant\nattribute, or override this property; otherwise, `None` is returned by\ndefault. Those strategies should also provide information regarding what is\nreturned by this property.\n\nSingle-worker strategies usually do not have a\n`tf.distribute.cluster_resolver.ClusterResolver`, and in those cases this\nproperty will return `None`.\n\nThe `tf.distribute.cluster_resolver.ClusterResolver` may be useful when the\nuser needs to access information such as the cluster spec, task type or task\nid. For example,\n\nFor more information, please see\n`tf.distribute.cluster_resolver.ClusterResolver`'s API docstring.\n\nView source\n\nDistributes `tf.data.Dataset` instances created by calls to `dataset_fn`.\n\nThe argument `dataset_fn` that users pass in is an input function that has a\n`tf.distribute.InputContext` argument and returns a `tf.data.Dataset`\ninstance. It is expected that the returned dataset from `dataset_fn` is\nalready batched by per-replica batch size (i.e. global batch size divided by\nthe number of replicas in sync) and sharded.\n`tf.distribute.Strategy.distribute_datasets_from_function` does not batch or\nshard the `tf.data.Dataset` instance returned from the input function.\n`dataset_fn` will be called on the CPU device of each of the workers and each\ngenerates a dataset where every replica on that worker will dequeue one batch\nof inputs (i.e. if a worker has two replicas, two batches will be dequeued\nfrom the `Dataset` every step).\n\nThis method can be used for several purposes. First, it allows you to specify\nyour own batching and sharding logic. (In contrast,\n`tf.distribute.experimental_distribute_dataset` does batching and sharding for\nyou.) For example, where `experimental_distribute_dataset` is unable to shard\nthe input files, this method might be used to manually shard the dataset\n(avoiding the slow fallback behavior in `experimental_distribute_dataset`). In\ncases where the dataset is infinite, this sharding can be done by creating\ndataset replicas that differ only in their random seed.\n\nThe `dataset_fn` should take an `tf.distribute.InputContext` instance where\ninformation about batching and input replication can be accessed.\n\nYou can use `element_spec` property of the `tf.distribute.DistributedDataset`\nreturned by this API to query the `tf.TypeSpec` of the elements returned by\nthe iterator. This can be used to set the `input_signature` property of a\n`tf.function`. Follow `tf.distribute.DistributedDataset.element_spec` to see\nan example.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input). If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nCreates `tf.distribute.DistributedDataset` from `tf.data.Dataset`.\n\nThe returned `tf.distribute.DistributedDataset` can be iterated over similar\nto regular datasets. NOTE: The user cannot add any more transformations to a\n`tf.distribute.DistributedDataset`. You can only create an iterator or examine\nthe `tf.TypeSpec` of the data generated by it. See API docs of\n`tf.distribute.DistributedDataset` to learn more.\n\nThe following is an example:\n\nThree key actions happending under the hood of this method are batching,\nsharding, and prefetching.\n\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and\ncalling `experimental_distribute_dataset` on it rebatches `dataset` to a new\nbatch size that is equal to the global batch size divided by the number of\nreplicas in sync. We iterate through it using a Pythonic for loop. `x` is a\n`tf.distribute.DistributedValues` containing data for all replicas, and each\nreplica gets data of the new batch size. `tf.distribute.Strategy.run` will\ntake care of feeding the right per-replica data in `x` to the right\n`replica_fn` executed on each replica.\n\nSharding contains autosharding across multiple workers and within every\nworker. First, in multi-worker distributed training (i.e. when you use\n`tf.distribute.experimental.MultiWorkerMirroredStrategy` or\n`tf.distribute.TPUStrategy`), autosharding a dataset over a set of workers\nmeans that each worker is assigned a subset of the entire dataset (if the\nright `tf.data.experimental.AutoShardPolicy` is set). This is to ensure that\nat each step, a global batch size of non-overlapping dataset elements will be\nprocessed by each worker. Autosharding has a couple of different options that\ncan be specified using `tf.data.experimental.DistributeOptions`. Then,\nsharding within each worker means the method will split the data among all the\nworker devices (if more than one a present). This will happen regardless of\nmulti-worker autosharding.\n\nBy default, this method adds a prefetch transformation at the end of the user\nprovided `tf.data.Dataset` instance. The argument to the prefetch\ntransformation which is `buffer_size` is equal to the number of replicas in\nsync.\n\nIf the above batch splitting and dataset sharding logic is undesirable, please\nuse `tf.distribute.Strategy.distribute_datasets_from_function` instead, which\ndoes not do any automatic batching or sharding for you.\n\nFor a tutorial on more usage and properties of this method, refer to the\ntutorial on distributed input. If you are interested in last partial batch\nhandling, read this section.\n\nView source\n\nReturns the list of all local per-replica values contained in `value`.\n\nView source\n\nMakes a tf.data.Dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nNote that you will likely need to use\ntf.distribute.Strategy.experimental_distribute_dataset with the returned\ndataset to further distribute it with the strategy.\n\nView source\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`.\n\nDEPRECATED: This method is not available in TF 2.x. Please switch to using\n`run` instead.\n\nWhen eager execution is enabled, executes ops specified by `fn` on each\nreplica. Otherwise, builds a graph to execute the ops on each replica.\n\nEach replica will take a single, different input from the inputs provided by\none `get_next` call on the input iterator.\n\n`fn` may call `tf.distribute.get_replica_context()` to access members such as\n`replica_id_in_sync_group`.\n\nView source\n\nMakes an iterator for input provided via `dataset`.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nData from the given dataset will be distributed evenly across all the compute\nreplicas. We will assume that the input dataset is batched by the global batch\nsize. With this assumption, we will make a best effort to divide each batch\nacross all the replicas (one or more workers). If this effort fails, an error\nwill be thrown, and the user should instead use `make_input_fn_iterator` which\nprovides more control to the user, and does not try to divide a batch across\nreplicas.\n\nThe user could also use `make_input_fn_iterator` if they want to customize\nwhich input is fed to which replica/worker etc.\n\nView source\n\nReturns an iterator split across replicas created from an input function.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe `input_fn` should take an `tf.distribute.InputContext` object where\ninformation about batching and input sharding can be accessed:\n\nThe `tf.data.Dataset` returned by `input_fn` should have a per-replica batch\nsize, which may be computed using `input_context.get_per_replica_batch_size`.\n\nView source\n\nReduce `value` across replicas and return result on current device.\n\nTo see how this would look with multiple replicas, consider the same example\nwith MirroredStrategy with 2 GPUs:\n\nThis API is typically used for aggregating the results returned from different\nreplicas, for reporting etc. For example, loss computed from different\nreplicas can be averaged using this API before printing.\n\nThere are a number of different tf.distribute APIs for reducing values across\nreplicas:\n\nWhat should axis be?\n\nGiven a per-replica value returned by `run`, say a per-example loss, the batch\nwill be divided across all the replicas. This function allows you to aggregate\nacross replicas and optionally also across batch elements by specifying the\naxis parameter accordingly.\n\nFor example, if you have a global batch size of 8 and 2 replicas, values for\nexamples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on\nreplica 1. With `axis=None`, `reduce` will aggregate only across replicas,\nreturning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is\ncomputing a scalar or some other value that doesn't have a \"batch\" dimension\n(like a gradient or loss).\n\nSometimes, you will want to aggregate across both the global batch and all\nreplicas. You can get this behavior by specifying the batch dimension as the\n`axis`, typically `axis=0`. In this case it would return a scalar\n`0+1+2+3+4+5+6+7`.\n\nIf there is a last partial batch, you will need to specify an axis so that the\nresulting shape is consistent across replicas. So if the last batch has size 6\nand it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch\nunless you specify `axis=0`. If you specify `tf.distribute.ReduceOp.MEAN`,\nusing `axis=0` will use the correct denominator of 6. Contrast this with\ncomputing `reduce_mean` to get a scalar value on each replica and this\nfunction to average those means, which will weigh some values `1/8` and others\n`1/4`.\n\nView source\n\nInvokes `fn` on each replica, with the given arguments.\n\nThis method is the primary way to distribute your computation with a\ntf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs`\nhave `tf.distribute.DistributedValues`, such as those produced by a\n`tf.distribute.DistributedDataset` from\n`tf.distribute.Strategy.experimental_distribute_dataset` or\n`tf.distribute.Strategy.distribute_datasets_from_function`, when `fn` is\nexecuted on a particular replica, it will be executed with the component of\n`tf.distribute.DistributedValues` that correspond to that replica.\n\n`fn` is invoked under a replica context. `fn` may call\n`tf.distribute.get_replica_context()` to access members such as `all_reduce`.\nPlease see the module-level docstring of tf.distribute for the concept of\nreplica context.\n\nAll arguments in `args` or `kwargs` should either be Python values of a nested\nstructure of tensors, e.g. a list of tensors, in which case `args` and\n`kwargs` will be passed to the `fn` invoked on each replica. Or `args` or\n`kwargs` can be `tf.distribute.DistributedValues` containing tensors or\ncomposite tensors, i.e. `tf.compat.v1.TensorInfo.CompositeTensor`, in which\ncase each `fn` call will get the component of a\n`tf.distribute.DistributedValues` corresponding to its replica.\n\nView source\n\nContext manager to make the strategy current and distribute variables.\n\nThis method returns a context manager, and is used as follows:\n\nWhat happens when Strategy.scope is entered?\n\nWhat should be in scope and what should be outside?\n\nThere are a number of requirements on what needs to happen inside the scope.\nHowever, in places where we have information about which strategy is in use,\nwe often enter the scope for the user, so they don't have to do it explicitly\n(i.e. calling those either inside or outside the scope is OK).\n\nView source\n\nReturns a copy of `config_proto` modified for use with this strategy.\n\nDEPRECATED: This method is not available in TF 2.x.\n\nThe updated config has something needed to run a strategy, e.g. configuration\nto run collective ops, or device filters to improve distributed training\nperformance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distribute.StrategyExtended", "path": "compat/v1/distribute/strategyextended", "type": "tf.compat", "text": "\nAdditional APIs for algorithms that need to be distribution-aware.\n\nInherits From: `StrategyExtended`\n\nSome common use cases of functions on this page:\n\n`tf.distribute.DistributedValues` can have the same locality as a distributed\nvariable, which leads to a mirrored value residing on the same devices as the\nvariable (as opposed to the compute devices). Such values may be passed to a\ncall to `tf.distribute.StrategyExtended.update` to update the value of a\nvariable. You may use `tf.distribute.StrategyExtended.colocate_vars_with` to\ngive a variable the same locality as another variable. You may convert a\n\"PerReplica\" value to a variable's locality by using\n`tf.distribute.StrategyExtended.reduce_to` or\n`tf.distribute.StrategyExtended.batch_reduce_to`.\n\nA distributed variable is variables created on multiple devices. As discussed\nin the glossary, mirrored variable and SyncOnRead variable are two examples.\nThe standard pattern for updating distributed variables is to:\n\nSteps 2 through 4 are done automatically by class\n`tf.keras.optimizers.Optimizer` if you call its\n`tf.keras.optimizers.Optimizer.apply_gradients` method in a replica context.\n\nIn fact, a higher-level solution to update a distributed variable is by\ncalling `assign` on the variable as you would do to a regular `tf.Variable`.\nYou can call the method in both replica context and cross-replica context. For\na mirrored variable, calling `assign` in replica context requires you to\nspecify the `aggregation` type in the variable constructor. In that case, the\ncontext switching and sync described in steps 2 through 4 are handled for you.\nIf you call `assign` on mirrored variable in cross-replica context, you can\nonly assign a single value or assign values from another mirrored variable or\na mirrored `tf.distribute.DistributedValues`. For a SyncOnRead variable, in\nreplica context, you can simply call `assign` on it and no aggregation happens\nunder the hood. In cross-replica context, you can only assign a single value\nto a SyncOnRead variable. One example case is restoring from a checkpoint: if\nthe `aggregation` type of the variable is `tf.VariableAggregation.SUM`, it is\nassumed that replica values were added before checkpointing, so at the time of\nrestoring, the value is divided by the number of replicas and then assigned to\neach replica; if the `aggregation` type is `tf.VariableAggregation.MEAN`, the\nvalue is assigned to each replica directly.\n\nThis is expected to return a constant value that will not be changed\nthroughout its life cycle.\n\nView source\n\nCombine multiple `reduce_to` calls into one for faster execution.\n\nSimilar to `reduce_to`, but accepts a list of (value, destinations) pairs.\nIt's more efficient than reduce each value separately.\n\nThis API currently can only be called in cross-replica context. Other variants\nto reduce values across replicas are:\n\nSee `reduce_to` for more information.\n\nView source\n\nMirror a tensor on one device to all worker devices.\n\nView source\n\nRun `fn` once per replica.\n\n`fn` may call `tf.get_replica_context()` to access methods such as\n`replica_id_in_sync_group` and `merge_call()`.\n\n`merge_call()` is used to communicate between the replicas and re-enter the\ncross-replica context. All replicas pause their execution having encountered a\n`merge_call()` call. After that the `merge_fn`-function is executed. Its\nresults are then unwrapped and given back to each replica call. After that\nexecution resumes until `fn` is complete or encounters another `merge_call()`.\nExample:\n\nView source\n\nScope that controls which devices variables will be created on.\n\nNo operations should be added to the graph inside this scope, it should only\nbe used when creating variables (some implementations work by changing\nvariable creation, others work by using a tf.compat.v1.colocate_with() scope).\n\nThis may only be used inside `self.scope()`.\n\nView source\n\nMakes a dataset for input provided via a numpy array.\n\nThis avoids adding `numpy_input` as a large constant in the graph, and copies\nthe data to the machine or machines that will be processing the input.\n\nView source\n\nDEPRECATED: please use `run` instead.\n\nRun `fn` with input from `iterator` for `iterations` times.\n\nThis method can be used to run a step function for training a number of times\nusing input from a dataset.\n\nView source\n\nDevice(s) for non-slot variables.\n\nDEPRECATED: TF 1.x ONLY.\n\nThis method returns non-slot devices where non-slot variables are placed.\nUsers can create non-slot variables on these devices by using a block:\n\nView source\n\nReads the value of a variable.\n\nReturns the aggregate value of a replica-local variable, or the (read-only)\nvalue of any other variable.\n\nView source\n\nCombine (via e.g. sum or mean) values across replicas.\n\n`reduce_to` aggregates `tf.distribute.DistributedValues` and distributed\nvariables. It supports both dense values and `tf.IndexedSlices`.\n\nThis API currently can only be called in cross-replica context. Other variants\nto reduce values across replicas are:\n\n`destinations` specifies where to reduce the value to, e.g. \"GPU:0\". You can\nalso pass in a `Tensor`, and the destinations will be the device of that\ntensor. For all-reduce, pass the same to `value` and `destinations`.\n\nIt can be used in `tf.distribute.ReplicaContext.merge_call` to write code that\nworks for all `tf.distribute.Strategy`.\n\nView source\n\nRun `fn` to update `var` using inputs mirrored to the same devices.\n\n`tf.distribute.StrategyExtended.update` takes a distributed variable `var` to\nbe updated, an update function `fn`, and `args` and `kwargs` for `fn`. It\napplies `fn` to each component variable of `var` and passes corresponding\nvalues from `args` and `kwargs`. Neither `args` nor `kwargs` may contain per-\nreplica values. If they contain mirrored values, they will be unwrapped before\ncalling `fn`. For example, `fn` can be `assign_add` and `args` can be a\nmirrored DistributedValues where each component contains the value to be added\nto this mirrored variable `var`. Calling `update` will call `assign_add` on\neach component variable of `var` with the corresponding tensor value on that\ndevice.\n\nIf `var` is mirrored across multiple devices, then this method implements\nlogic as following:\n\nOtherwise, this method returns `fn(var, *args, **kwargs)` colocated with\n`var`.\n\nView source\n\nRuns `fn(*args, **kwargs)` on `colocate_with` devices.\n\nUsed to update non-slot variables.\n\nDEPRECATED: TF 1.x ONLY.\n\nView source\n\nReturns the container that this per-replica `value` belongs to.\n\nView source\n\nTests whether `v` was created while this strategy scope was active.\n\nVariables created inside the strategy scope are \"owned\" by it:\n\nVariables created outside the strategy are not owned by it:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions", "path": "compat/v1/distributions", "type": "tf.compat", "text": "\nCore module for TensorFlow distribution objects and helpers.\n\n`class Bernoulli`: Bernoulli distribution.\n\n`class Beta`: Beta distribution.\n\n`class Categorical`: Categorical distribution.\n\n`class Dirichlet`: Dirichlet distribution.\n\n`class DirichletMultinomial`: Dirichlet-Multinomial compound distribution.\n\n`class Distribution`: A generic probability distribution base class.\n\n`class Exponential`: Exponential distribution.\n\n`class Gamma`: Gamma distribution.\n\n`class Laplace`: The Laplace distribution with location `loc` and `scale`\nparameters.\n\n`class Multinomial`: Multinomial distribution.\n\n`class Normal`: The Normal distribution with location `loc` and `scale`\nparameters.\n\n`class RegisterKL`: Decorator to register a KL divergence implementation\nfunction.\n\n`class ReparameterizationType`: Instances of this class represent how sampling\nis reparameterized.\n\n`class StudentT`: Student's t-distribution.\n\n`class Uniform`: Uniform distribution with `low` and `high` parameters.\n\n`kl_divergence(...)`: Get the KL-divergence KL(distribution_a ||\ndistribution_b). (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Bernoulli", "path": "compat/v1/distributions/bernoulli", "type": "tf.compat", "text": "\nBernoulli distribution.\n\nInherits From: `Distribution`\n\nThe Bernoulli distribution with `probs` parameter, i.e., the probability of a\n`1` outcome (vs a `0` outcome).\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nAdditional documentation from `Bernoulli`:\n\nReturns `1` if `prob > 0.5` and `0` otherwise.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Beta", "path": "compat/v1/distributions/beta", "type": "tf.compat", "text": "\nBeta distribution.\n\nInherits From: `Distribution`\n\nThe Beta distribution is defined over the `(0, 1)` interval using parameters\n`concentration1` (aka \"alpha\") and `concentration0` (aka \"beta\").\n\nThe probability density function (pdf) is,\n\nwhere:\n\nThe concentration parameters represent mean total counts of a `1` or a `0`,\ni.e.,\n\nwhere `mean` in `(0, 1)` and `total_concentration` is a positive real number\nrepresenting a mean `total_count = concentration1 + concentration0`.\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\nSamples of this distribution are reparameterized (pathwise differentiable).\nThe derivatives are computed using the approach described in (Figurnov et al.,\n2018).\n\nCompute the gradients of samples w.r.t. the parameters:\n\nImplicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nAdditional documentation from `Beta`:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nAdditional documentation from `Beta`:\n\nView source\n\nLog probability density/mass function.\n\nAdditional documentation from `Beta`:\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nAdditional documentation from `Beta`:\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nAdditional documentation from `Beta`:\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Categorical", "path": "compat/v1/distributions/categorical", "type": "tf.compat", "text": "\nCategorical distribution.\n\nInherits From: `Distribution`\n\nThe Categorical distribution is parameterized by either probabilities or log-\nprobabilities of a set of `K` classes. It is defined over the integers `{0, 1,\n..., K}`.\n\nThe Categorical distribution is closely related to the `OneHotCategorical` and\n`Multinomial` distributions. The Categorical distribution can be intuited as\ngenerating samples according to `argmax{ OneHotCategorical(probs) }` itself\nbeing identical to `argmax{ Multinomial(probs, total_count=1) }`.\n\nThe probability mass function (pmf) is,\n\nThe number of classes, `K`, must not exceed:\n\nIn other words,\n\nCreates a 3-class distribution with the 2nd class being most likely.\n\nCreates a 3-class distribution with the 2nd class being most likely.\nParameterized by logits rather than probabilities.\n\nCreates a 3-class distribution with the 3rd class being most likely. The\ndistribution functions can be evaluated on counts.\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Dirichlet", "path": "compat/v1/distributions/dirichlet", "type": "tf.compat", "text": "\nDirichlet distribution.\n\nInherits From: `Distribution`\n\nThe Dirichlet distribution is defined over the `(k-1)`-simplex using a\npositive, length-`k` vector `concentration` (`k > 1`). The Dirichlet is\nidentically the Beta distribution when `k = 2`.\n\nThe Dirichlet is a distribution over the open `(k-1)`-simplex, i.e.,\n\nThe probability density function (pdf) is,\n\nwhere:\n\nThe `concentration` represents mean total counts of class occurrence, i.e.,\n\nwhere `mean` in `S^{k-1}` and `total_concentration` is a positive real number\nrepresenting a mean total count.\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\nSamples of this distribution are reparameterized (pathwise differentiable).\nThe derivatives are computed using the approach described in (Figurnov et al.,\n2018).\n\nCompute the gradients of samples w.r.t. the parameters:\n\nImplicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nAdditional documentation from `Dirichlet`:\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nAdditional documentation from `Dirichlet`:\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nAdditional documentation from `Dirichlet`:\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.DirichletMultinomial", "path": "compat/v1/distributions/dirichletmultinomial", "type": "tf.compat", "text": "\nDirichlet-Multinomial compound distribution.\n\nInherits From: `Distribution`\n\nThe Dirichlet-Multinomial distribution is parameterized by a (batch of)\nlength-`K` `concentration` vectors (`K > 1`) and a `total_count` number of\ntrials, i.e., the number of trials per draw from the DirichletMultinomial. It\nis defined over a (batch of) length-`K` vector `counts` such that\n`tf.reduce_sum(counts, -1) = total_count`. The Dirichlet-Multinomial is\nidentically the Beta-Binomial distribution when `K = 2`.\n\nThe Dirichlet-Multinomial is a distribution over `K`-class counts, i.e., a\nlength-`K` vector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\nThe probability mass function (pmf) is,\n\nwhere:\n\nDirichlet-Multinomial is a compound distribution, i.e., its samples are\ngenerated as follows.\n\nThe last `concentration` dimension parametrizes a single Dirichlet-Multinomial\ndistribution. When calling distribution functions (e.g., `dist.prob(counts)`),\n`concentration`, `total_count` and `counts` are broadcast to the same shape.\nThe last dimension of `counts` corresponds single Dirichlet-Multinomial\ndistributions.\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\nThe number of classes, `K`, must not exceed:\n\nIn other words,\n\nCreates a 3-class distribution, with the 3rd class is most likely to be drawn.\nThe distribution functions can be evaluated on counts.\n\nCreates a 2-batch of 3-class distributions.\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nAdditional documentation from `DirichletMultinomial`:\n\nThe covariance for each batch member is defined as the following:\n\nwhere `concentration = alpha` and `total_concentration = alpha_0 = sum_j\nalpha_j`.\n\nThe covariance between elements in a batch is defined as:\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nAdditional documentation from `DirichletMultinomial`:\n\nFor each batch of counts, `value = [n_0, ..., n_{K-1}]`, `P[value]` is the\nprobability that after sampling `self.total_count` draws from this Dirichlet-\nMultinomial distribution, the number of draws falling in class `j` is `n_j`.\nSince this definition is exchangeable; different sequences have the same\ncounts so the probability includes a combinatorial coefficient.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nAdditional documentation from `DirichletMultinomial`:\n\nFor each batch of counts, `value = [n_0, ..., n_{K-1}]`, `P[value]` is the\nprobability that after sampling `self.total_count` draws from this Dirichlet-\nMultinomial distribution, the number of draws falling in class `j` is `n_j`.\nSince this definition is exchangeable; different sequences have the same\ncounts so the probability includes a combinatorial coefficient.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Distribution", "path": "compat/v1/distributions/distribution", "type": "tf.compat", "text": "\nA generic probability distribution base class.\n\n`Distribution` is a base class for constructing and organizing properties\n(e.g., mean, variance) of random variables (e.g, Bernoulli, Gaussian).\n\nSubclasses are expected to implement a leading-underscore version of the same-\nnamed function. The argument signature should be identical except for the\nomission of `name=\"...\"`. For example, to enable `log_prob(value,\nname=\"log_prob\")` a subclass should implement `_log_prob(value)`.\n\nSubclasses can append to public-level docstrings by providing docstrings for\ntheir method specializations. For example:\n\nwould add the string \"Some other details.\" to the `log_prob` function\ndocstring. This is implemented as a simple decorator to avoid python linter\ncomplaining about missing Args/Returns/Raises sections in the partial\ndocstrings.\n\nAll distributions support batches of independent distributions of that type.\nThe batch shape is determined by broadcasting together the parameters.\n\nThe shape of arguments to `__init__`, `cdf`, `log_cdf`, `prob`, and `log_prob`\nreflect this broadcasting, as does the return value of `sample` and\n`sample_n`.\n\n`sample_n_shape = [n] + batch_shape + event_shape`, where `sample_n_shape` is\nthe shape of the `Tensor` returned from `sample_n`, `n` is the number of\nsamples, `batch_shape` defines how many independent distributions there are,\nand `event_shape` defines the shape of samples from each of those independent\ndistributions. Samples are independent along the `batch_shape` dimensions, but\nnot necessarily so along the `event_shape` dimensions (depending on the\nparticulars of the underlying distribution).\n\nUsing the `Uniform` distribution as an example:\n\nThere are three important concepts associated with TensorFlow Distributions\nshapes:\n\nThe event shape and the batch shape are properties of a Distribution object,\nwhereas the sample shape is associated with a specific call to `sample` or\n`log_prob`.\n\nFor detailed usage examples of TensorFlow Distributions shapes, see this\ntutorial\n\nSome distributions do not have well-defined statistics for all initialization\nparameter values. For example, the beta distribution is parameterized by\npositive real numbers `concentration1` and `concentration0`, and does not have\nwell-defined mode if `concentration1 < 1` or `concentration0 < 1`.\n\nThe user is given the option of raising an exception or returning `NaN`.\n\nIn all cases, an exception is raised if invalid parameters are passed, e.g.\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Exponential", "path": "compat/v1/distributions/exponential", "type": "tf.compat", "text": "\nExponential distribution.\n\nInherits From: `Gamma`, `Distribution`\n\nThe Exponential distribution is parameterized by an event `rate` parameter.\n\nThe probability density function (pdf) is,\n\nwhere `rate = lambda` and `Z` is the normalizaing constant.\n\nThe Exponential distribution is a special case of the Gamma distribution,\ni.e.,\n\nThe Exponential distribution uses a `rate` parameter, or \"inverse scale\",\nwhich can be intuited as,\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nAdditional documentation from `Gamma`:\n\nThe mode of a gamma distribution is `(shape - 1) / rate` when `shape > 1`, and\n`NaN` otherwise. If `self.allow_nan_stats` is `False`, an exception will be\nraised rather than returning `NaN`.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Gamma", "path": "compat/v1/distributions/gamma", "type": "tf.compat", "text": "\nGamma distribution.\n\nInherits From: `Distribution`\n\nThe Gamma distribution is defined over positive real numbers using parameters\n`concentration` (aka \"alpha\") and `rate` (aka \"beta\").\n\nThe probability density function (pdf) is,\n\nwhere:\n\nThe cumulative density function (cdf) is,\n\nwhere `GammaInc` is the lower incomplete Gamma function.\n\nThe parameters can be intuited via their relationship to mean and stddev,\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\nSamples of this distribution are reparameterized (pathwise differentiable).\nThe derivatives are computed using the approach described in (Figurnov et al.,\n2018).\n\nCompute the gradients of samples w.r.t. the parameters:\n\nImplicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nAdditional documentation from `Gamma`:\n\nThe mode of a gamma distribution is `(shape - 1) / rate` when `shape > 1`, and\n`NaN` otherwise. If `self.allow_nan_stats` is `False`, an exception will be\nraised rather than returning `NaN`.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.kl_divergence", "path": "compat/v1/distributions/kl_divergence", "type": "tf.compat", "text": "\nGet the KL-divergence KL(distribution_a || distribution_b). (deprecated)\n\nIf there is no KL method registered specifically for `type(distribution_a)`\nand `type(distribution_b)`, then the class hierarchies of these types are\nsearched.\n\nIf one KL method is registered between any pairs of classes in these two\nparent hierarchies, it is used.\n\nIf more than one such registered method exists, the method whose registered\nclasses have the shortest sum MRO paths to the input types is used.\n\nIf more than one such shortest path exists, the first method identified in the\nsearch is used (favoring a shorter MRO distance to `type(distribution_a)`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Laplace", "path": "compat/v1/distributions/laplace", "type": "tf.compat", "text": "\nThe Laplace distribution with location `loc` and `scale` parameters.\n\nInherits From: `Distribution`\n\nThe probability density function (pdf) of this distribution is,\n\nwhere `loc = mu`, `scale = sigma`, and `Z` is the normalization constant.\n\nNote that the Laplace distribution can be thought of two exponential\ndistributions spliced together \"back-to-back.\"\n\nThe Lpalce distribution is a member of the location-scale family, i.e., it can\nbe constructed as,\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Multinomial", "path": "compat/v1/distributions/multinomial", "type": "tf.compat", "text": "\nMultinomial distribution.\n\nInherits From: `Distribution`\n\nThis Multinomial distribution is parameterized by `probs`, a (batch of)\nlength-`K` `prob` (probability) vectors (`K > 1`) such that\n`tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\nthe number of trials per draw from the Multinomial. It is defined over a\n(batch of) length-`K` vector `counts` such that `tf.reduce_sum(counts, -1) =\ntotal_count`. The Multinomial is identically the Binomial distribution when `K\n= 2`.\n\nThe Multinomial is a distribution over `K`-class counts, i.e., a length-`K`\nvector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\nThe probability mass function (pmf) is,\n\nwhere:\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\nThe number of classes, `K`, must not exceed:\n\nIn other words,\n\nCreate a 3-class distribution, with the 3rd class is most likely to be drawn,\nusing logits.\n\nCreate a 3-class distribution, with the 3rd class is most likely to be drawn.\n\nThe distribution functions can be evaluated on counts.\n\nCreate a 2-batch of 3-class distributions.\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nAdditional documentation from `Multinomial`:\n\nFor each batch of counts, `value = [n_0, ... ,n_{k-1}]`, `P[value]` is the\nprobability that after sampling `self.total_count` draws from this Multinomial\ndistribution, the number of draws falling in class `j` is `n_j`. Since this\ndefinition is exchangeable; different sequences have the same counts so the\nprobability includes a combinatorial coefficient.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Normal", "path": "compat/v1/distributions/normal", "type": "tf.compat", "text": "\nThe Normal distribution with location `loc` and `scale` parameters.\n\nInherits From: `Distribution`\n\nThe probability density function (pdf) is,\n\nwhere `loc = mu` is the mean, `scale = sigma` is the std. deviation, and, `Z`\nis the normalization constant.\n\nThe Normal distribution is a member of the location-scale family, i.e., it can\nbe constructed as,\n\nExamples of initialization of one or a batch of distributions.\n\nArguments are broadcast when possible.\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.RegisterKL", "path": "compat/v1/distributions/registerkl", "type": "tf.compat", "text": "\nDecorator to register a KL divergence implementation function.\n\n@distributions.RegisterKL(distributions.Normal, distributions.Normal) def\n_kl_normal_mvn(norm_a, norm_b): # Return KL(norm_a || norm_b)\n\nView source\n\nPerform the KL registration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.ReparameterizationType", "path": "compat/v1/distributions/reparameterizationtype", "type": "tf.compat", "text": "\nInstances of this class represent how sampling is reparameterized.\n\nTwo static instances exist in the distributions library, signifying one of two\npossible properties for samples from a distribution:\n\n`FULLY_REPARAMETERIZED`: Samples from the distribution are fully\nreparameterized, and straight-through gradients are supported.\n\n`NOT_REPARAMETERIZED`: Samples from the distribution are not fully\nreparameterized, and straight-through gradients are either partially\nunsupported or are not supported at all. In this case, for purposes of e.g. RL\nor variational inference, it is generally safest to wrap the sample results in\na `stop_gradients` call and use policy gradients / surrogate loss instead.\n\nView source\n\nDetermine if this `ReparameterizationType` is equal to another.\n\nSince ReparameterizationType instances are constant static global instances,\nequality checks if two instances' id() values are equal.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.StudentT", "path": "compat/v1/distributions/studentt", "type": "tf.compat", "text": "\nStudent's t-distribution.\n\nInherits From: `Distribution`\n\nThis distribution has parameters: degree of freedom `df`, location `loc`, and\n`scale`.\n\nThe probability density function (pdf) is,\n\nwhere:\n\nThe StudentT distribution is a member of the location-scale family, i.e., it\ncan be constructed as,\n\nNotice that `scale` has semantics more similar to standard deviation than\nvariance. However it is not actually the std. deviation; the Student's\nt-distribution std. dev. is `scale sqrt(df / (df - 2))` when `df > 2`.\n\nSamples of this distribution are reparameterized (pathwise differentiable).\nThe derivatives are computed using the approach described in (Figurnov et al.,\n2018).\n\nExamples of initialization of one or a batch of distributions.\n\nArguments are broadcast when possible.\n\nCompute the gradients of samples w.r.t. the parameters:\n\nImplicit Reparameterization Gradients: Figurnov et al., 2018 (pdf)\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nAdditional documentation from `StudentT`:\n\nThe mean of Student's T equals `loc` if `df > 1`, otherwise it is `NaN`. If\n`self.allow_nan_stats=True`, then an exception will be raised rather than\nreturning `NaN`.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\nAdditional documentation from `StudentT`:\n\nThe variance for Student's T equals\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.distributions.Uniform", "path": "compat/v1/distributions/uniform", "type": "tf.compat", "text": "\nUniform distribution with `low` and `high` parameters.\n\nInherits From: `Distribution`\n\nThe probability density function (pdf) is,\n\nwhere\n\nThe parameters `low` and `high` must be shaped in a way that supports\nbroadcasting (e.g., `high - low` is a valid operation).\n\nStats return +/- infinity when it makes sense. E.g., the variance of a Cauchy\ndistribution is infinity. However, sometimes the statistic is undefined, e.g.,\nif a distribution's pdf does not achieve a maximum within the support of the\ndistribution, the mode is undefined. If the mean is undefined, then by\ndefinition the variance is undefined. E.g. the mean for Student's T for df = 1\nis undefined (no clear way to say it is either + or - infinity), so the\nvariance = E[(X - mean)**2] is also undefined.\n\nMay be partially defined or unknown.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nMay be partially defined or unknown.\n\nCurrently this is one of the static instances\n`distributions.FULLY_REPARAMETERIZED` or `distributions.NOT_REPARAMETERIZED`.\n\nView source\n\nShape of a single sample from a single event index as a 1-D `Tensor`.\n\nThe batch dimensions are indexes into independent, non-identical\nparameterizations of this distribution.\n\nView source\n\nCumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nView source\n\nCreates a deep copy of the distribution.\n\nView source\n\nCovariance.\n\nCovariance is (possibly) defined only for non-scalar-event distributions.\n\nFor example, for a length-`k`, vector-valued distribution, it is calculated\nas,\n\nwhere `Cov` is a (batch of) `k x k` matrix, `0 <= (i, j) < k`, and `E` denotes\nexpectation.\n\nAlternatively, for non-vector, multivariate distributions (e.g., matrix-\nvalued, Wishart), `Covariance` shall return a (batch of) matrices under some\nvectorization of the events, i.e.,\n\nwhere `Cov` is a (batch of) `k' x k'` matrices, `0 <= (i, j) < k' =\nreduce_prod(event_shape)`, and `Vec` is some function mapping indices of this\ndistribution's event dimensions to indices of a length-`k'` vector.\n\nView source\n\nComputes the (Shannon) cross entropy.\n\nDenote this distribution (`self`) by `P` and the `other` distribution by `Q`.\nAssuming `P, Q` are absolutely continuous with respect to one another and\npermit densities `p(x) dr(x)` and `q(x) dr(x)`, (Shanon) cross entropy is\ndefined as:\n\nwhere `F` denotes the support of the random variable `X ~ P`.\n\nView source\n\nShannon entropy in nats.\n\nView source\n\nShape of a single sample from a single batch as a 1-D int32 `Tensor`.\n\nView source\n\nIndicates that `batch_shape == []`.\n\nView source\n\nIndicates that `event_shape == []`.\n\nView source\n\nComputes the Kullback--Leibler divergence.\n\nDenote this distribution (`self`) by `p` and the `other` distribution by `q`.\nAssuming `p, q` are absolutely continuous with respect to reference measure\n`r`, the KL divergence is defined as:\n\nwhere `F` denotes the support of the random variable `X ~ p`, `H[., .]`\ndenotes (Shanon) cross entropy, and `H[.]` denotes (Shanon) entropy.\n\nView source\n\nLog cumulative distribution function.\n\nGiven random variable `X`, the cumulative distribution function `cdf` is:\n\nOften, a numerical approximation can be used for `log_cdf(x)` that yields a\nmore accurate answer than simply taking the logarithm of the `cdf` when `x <<\n-1`.\n\nView source\n\nLog probability density/mass function.\n\nView source\n\nLog survival function.\n\nGiven random variable `X`, the survival function is defined:\n\nTypically, different numerical approximations can be used for the log survival\nfunction, which are more accurate than `1 - cdf(x)` when `x >> 1`.\n\nView source\n\nMean.\n\nView source\n\nMode.\n\nView source\n\nShapes of parameters given the desired shape of a call to `sample()`.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`.\n\nSubclasses should override class method `_param_shapes`.\n\nView source\n\nparam_shapes with static (i.e. `TensorShape`) shapes.\n\nThis is a class method that describes what key/value arguments are required to\ninstantiate the given `Distribution` so that a particular shape is returned\nfor that instance's call to `sample()`. Assumes that the sample's shape is\nknown statically.\n\nSubclasses should override class method `_param_shapes` to return constant-\nvalued tensors when constant values are fed.\n\nView source\n\nProbability density/mass function.\n\nView source\n\nQuantile function. Aka \"inverse cdf\" or \"percent point function\".\n\nGiven random variable `X` and `p in [0, 1]`, the `quantile` is:\n\nView source\n\n`high - low`.\n\nView source\n\nGenerate samples of the specified shape.\n\nNote that a call to `sample()` without arguments will generate a single\nsample.\n\nView source\n\nStandard deviation.\n\nStandard deviation is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `stddev.shape = batch_shape + event_shape`.\n\nView source\n\nSurvival function.\n\nGiven random variable `X`, the survival function is defined:\n\nView source\n\nVariance.\n\nVariance is defined as,\n\nwhere `X` is the random variable associated with this distribution, `E`\ndenotes expectation, and `Var.shape = batch_shape + event_shape`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.dtypes", "path": "compat/v1/dtypes", "type": "tf.compat", "text": "\nPublic API for tf.dtypes namespace.\n\n`class DType`: Represents the type of the elements in a `Tensor`.\n\n`as_dtype(...)`: Converts the given `type_value` to a `DType`.\n\n`as_string(...)`: Converts each entry in the given tensor to strings.\n\n`cast(...)`: Casts a tensor to a new type.\n\n`complex(...)`: Converts two real numbers to a complex number.\n\n`saturate_cast(...)`: Performs a safe saturating cast of `value` to `dtype`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_control_flow_v2", "path": "compat/v1/enable_control_flow_v2", "type": "tf.compat", "text": "\nUse control flow v2.\n\ncontrol flow v2 (cfv2) is an improved version of control flow in TensorFlow\nwith support for higher order derivatives. Enabling cfv2 will change the\ngraph/function representation of control flow, e.g., `tf.while_loop` and\n`tf.cond` will generate functional `While` and `If` ops instead of low-level\n`Switch`, `Merge` etc. ops. Note: Importing and running graphs exported with\nold control flow will still be supported.\n\nCalling tf.enable_control_flow_v2() lets you opt-in to this TensorFlow 2.0\nfeature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_eager_execution", "path": "compat/v1/enable_eager_execution", "type": "tf.compat", "text": "\nEnables eager execution for the lifetime of this program.\n\nEager execution provides an imperative interface to TensorFlow. With eager\nexecution enabled, TensorFlow functions execute operations immediately (as\nopposed to adding to a graph to be executed later in a `tf.compat.v1.Session`)\nand return concrete values (as opposed to symbolic references to a node in a\ncomputational graph).\n\nEager execution cannot be enabled after TensorFlow APIs have been used to\ncreate or execute graphs. It is typically recommended to invoke this function\nat program startup and not in a library (as most libraries should be usable\nboth with and without eager execution).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_resource_variables", "path": "compat/v1/enable_resource_variables", "type": "tf.compat", "text": "\nCreates resource variables by default.\n\nResource variables are improved versions of TensorFlow variables with a well-\ndefined memory model. Accessing a resource variable reads its value, and all\nops which access a specific read value of the variable are guaranteed to see\nthe same value for that tensor. Writes which happen after a read (by having a\ncontrol or data dependency on the read) are guaranteed not to affect the value\nof the read tensor, and similarly writes which happen before a read are\nguaranteed to affect the value. No guarantees are made about unordered\nread/write pairs.\n\nCalling tf.enable_resource_variables() lets you opt-in to this TensorFlow 2.0\nfeature.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_tensor_equality", "path": "compat/v1/enable_tensor_equality", "type": "tf.compat", "text": "\nCompare Tensors with element-wise comparison and thus be unhashable.\n\nComparing tensors with element-wise allows comparisons such as\ntf.Variable(1.0) == 1.0. Element-wise equality implies that tensors are\nunhashable. Thus tensors can no longer be directly used in sets or as a key in\na dictionary.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_v2_behavior", "path": "compat/v1/enable_v2_behavior", "type": "tf.compat", "text": "\nEnables TensorFlow 2.x behaviors.\n\nThis function can be called at the beginning of the program (before `Tensors`,\n`Graphs` or other structures have been created, and before devices have been\ninitialized. It switches all global behaviors that are different between\nTensorFlow 1.x and 2.x to behave as intended for 2.x.\n\nThis function is called in the main TensorFlow `__init__.py` file, user should\nnot need to call it, except during complex migrations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.enable_v2_tensorshape", "path": "compat/v1/enable_v2_tensorshape", "type": "tf.compat", "text": "\nIn TensorFlow 2.0, iterating over a TensorShape instance returns values.\n\nThis enables the new behavior.\n\nConcretely, `tensor_shape[i]` returned a Dimension instance in V1, but it V2\nit returns either an integer, or None.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.errors", "path": "compat/v1/errors", "type": "tf.compat", "text": "\nException types for TensorFlow errors.\n\n`class AbortedError`: The operation was aborted, typically due to a concurrent\naction.\n\n`class AlreadyExistsError`: Raised when an entity that we attempted to create\nalready exists.\n\n`class CancelledError`: Raised when an operation or step is cancelled.\n\n`class DataLossError`: Raised when unrecoverable data loss or corruption is\nencountered.\n\n`class DeadlineExceededError`: Raised when a deadline expires before an\noperation could complete.\n\n`class FailedPreconditionError`: Operation was rejected because the system is\nnot in a state to execute it.\n\n`class InternalError`: Raised when the system experiences an internal error.\n\n`class InvalidArgumentError`: Raised when an operation receives an invalid\nargument.\n\n`class NotFoundError`: Raised when a requested entity (e.g., a file or\ndirectory) was not found.\n\n`class OpError`: A generic error that is raised when TensorFlow execution\nfails.\n\n`class OutOfRangeError`: Raised when an operation iterates past the valid\ninput range.\n\n`class PermissionDeniedError`: Raised when the caller does not have permission\nto run an operation.\n\n`class ResourceExhaustedError`: Some resource has been exhausted.\n\n`class UnauthenticatedError`: The request does not have valid authentication\ncredentials.\n\n`class UnavailableError`: Raised when the runtime is currently unavailable.\n\n`class UnimplementedError`: Raised when an operation has not been implemented.\n\n`class UnknownError`: Unknown error.\n\n`class raise_exception_on_not_ok_status`: Context manager to check for C API\nstatus.\n\n`error_code_from_exception_type(...)`\n\n`exception_type_from_error_code(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.errors.error_code_from_exception_type", "path": "compat/v1/errors/error_code_from_exception_type", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.errors.exception_type_from_error_code", "path": "compat/v1/errors/exception_type_from_error_code", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.errors.raise_exception_on_not_ok_status", "path": "compat/v1/errors/raise_exception_on_not_ok_status", "type": "tf.compat", "text": "\nContext manager to check for C API status.\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator", "path": "compat/v1/estimator", "type": "tf.compat", "text": "\nEstimator: High level tools for working with models.\n\n`experimental` module: Public API for tf.estimator.experimental namespace.\n\n`export` module: All public utility methods for exporting Estimator to\nSavedModel.\n\n`inputs` module: Utility methods to create simple input_fns.\n\n`tpu` module: Public API for tf.estimator.tpu namespace.\n\n`class BaselineClassifier`: A classifier that can establish a simple baseline.\n\n`class BaselineEstimator`: An estimator that can establish a simple baseline.\n\n`class BaselineRegressor`: A regressor that can establish a simple baseline.\n\n`class BestExporter`: This class exports the serving graph and checkpoints of\nthe best models.\n\n`class BinaryClassHead`: Creates a `Head` for single label binary\nclassification.\n\n`class BoostedTreesClassifier`: A Classifier for Tensorflow Boosted Trees\nmodels.\n\n`class BoostedTreesEstimator`: An Estimator for Tensorflow Boosted Trees\nmodels.\n\n`class BoostedTreesRegressor`: A Regressor for Tensorflow Boosted Trees\nmodels.\n\n`class CheckpointSaverHook`: Saves checkpoints every N steps or seconds.\n\n`class CheckpointSaverListener`: Interface for listeners that take action\nbefore or after checkpoint save.\n\n`class DNNClassifier`: A classifier for TensorFlow DNN models.\n\n`class DNNEstimator`: An estimator for TensorFlow DNN models with user-\nspecified head.\n\n`class DNNLinearCombinedClassifier`: An estimator for TensorFlow Linear and\nDNN joined classification models.\n\n`class DNNLinearCombinedEstimator`: An estimator for TensorFlow Linear and DNN\njoined models with custom head.\n\n`class DNNLinearCombinedRegressor`: An estimator for TensorFlow Linear and DNN\njoined models for regression.\n\n`class DNNRegressor`: A regressor for TensorFlow DNN models.\n\n`class Estimator`: Estimator class to train and evaluate TensorFlow models.\n\n`class EstimatorSpec`: Ops and objects returned from a `model_fn` and passed\nto an `Estimator`.\n\n`class EvalSpec`: Configuration for the \"eval\" part for the\n`train_and_evaluate` call.\n\n`class Exporter`: A class representing a type of model export.\n\n`class FeedFnHook`: Runs `feed_fn` and sets the `feed_dict` accordingly.\n\n`class FinalExporter`: This class exports the serving graph and checkpoints at\nthe end.\n\n`class FinalOpsHook`: A hook which evaluates `Tensors` at the end of a\nsession.\n\n`class GlobalStepWaiterHook`: Delays execution until global step reaches\n`wait_until_step`.\n\n`class Head`: Interface for the head/top of a model.\n\n`class LatestExporter`: This class regularly exports the serving graph and\ncheckpoints.\n\n`class LinearClassifier`: Linear classifier model.\n\n`class LinearEstimator`: An estimator for TensorFlow linear models with user-\nspecified head.\n\n`class LinearRegressor`: An estimator for TensorFlow Linear regression\nproblems.\n\n`class LoggingTensorHook`: Prints the given tensors every N local steps, every\nN seconds, or at end.\n\n`class LogisticRegressionHead`: Creates a `Head` for logistic regression.\n\n`class ModeKeys`: Standard names for Estimator model modes.\n\n`class MultiClassHead`: Creates a `Head` for multi class classification.\n\n`class MultiHead`: Creates a `Head` for multi-objective learning.\n\n`class MultiLabelHead`: Creates a `Head` for multi-label classification.\n\n`class NanLossDuringTrainingError`: Unspecified run-time error.\n\n`class NanTensorHook`: Monitors the loss tensor and stops training if loss is\nNaN.\n\n`class PoissonRegressionHead`: Creates a `Head` for poisson regression using\n`tf.nn.log_poisson_loss`.\n\n`class ProfilerHook`: Captures CPU/GPU profiling information every N steps or\nseconds.\n\n`class RegressionHead`: Creates a `Head` for regression using the\n`mean_squared_error` loss.\n\n`class RunConfig`: This class specifies the configurations for an `Estimator`\nrun.\n\n`class SecondOrStepTimer`: Timer that triggers at most once every N seconds or\nonce every N steps.\n\n`class SessionRunArgs`: Represents arguments to be added to a `Session.run()`\ncall.\n\n`class SessionRunContext`: Provides information about the `session.run()` call\nbeing made.\n\n`class SessionRunHook`: Hook to extend calls to MonitoredSession.run().\n\n`class SessionRunValues`: Contains the results of `Session.run()`.\n\n`class StepCounterHook`: Hook that counts steps per second.\n\n`class StopAtStepHook`: Hook that requests stop at a specified step.\n\n`class SummarySaverHook`: Saves summaries every N steps.\n\n`class TrainSpec`: Configuration for the \"train\" part for the\n`train_and_evaluate` call.\n\n`class VocabInfo`: Vocabulary information for warm-starting.\n\n`class WarmStartSettings`: Settings for warm-starting in\n`tf.estimator.Estimators`.\n\n`add_metrics(...)`: Creates a new `tf.estimator.Estimator` which has given\nmetrics.\n\n`classifier_parse_example_spec(...)`: Generates parsing spec for\ntf.parse_example to be used with classifiers.\n\n`regressor_parse_example_spec(...)`: Generates parsing spec for\ntf.parse_example to be used with regressors.\n\n`train_and_evaluate(...)`: Train and evaluate the `estimator`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.BaselineClassifier", "path": "compat/v1/estimator/baselineclassifier", "type": "tf.compat", "text": "\nA classifier that can establish a simple baseline.\n\nInherits From: `Estimator`\n\nThis classifier ignores feature values and will learn to predict the average\nvalue of each label. For single-label problems, this will predict the\nprobability distribution of the classes as seen in the labels. For multi-label\nproblems, this will predict the fraction of examples that are positive for\neach class.\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.BaselineEstimator", "path": "compat/v1/estimator/baselineestimator", "type": "tf.compat", "text": "\nAn estimator that can establish a simple baseline.\n\nInherits From: `Estimator`\n\nThe estimator uses a user-specified head.\n\nThis estimator ignores feature values and will learn to predict the average\nvalue of each label. E.g. for single-label classification problems, this will\npredict the probability distribution of the classes as seen in the labels. For\nmulti-label classification problems, it will predict the ratio of examples\nthat contain each class.\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.BaselineRegressor", "path": "compat/v1/estimator/baselineregressor", "type": "tf.compat", "text": "\nA regressor that can establish a simple baseline.\n\nInherits From: `Estimator`\n\nThis regressor ignores feature values and will learn to predict the average\nvalue of each label.\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.classifier_parse_example_spec", "path": "compat/v1/estimator/classifier_parse_example_spec", "type": "tf.compat", "text": "\nGenerates parsing spec for tf.parse_example to be used with classifiers.\n\nIf users keep data in tf.Example format, they need to call tf.parse_example\nwith a proper feature spec. There are two main things that this utility helps:\n\nExample output of parsing spec:\n\nExample usage with a classifier:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNClassifier", "path": "compat/v1/estimator/dnnclassifier", "type": "tf.compat", "text": "\nA classifier for TensorFlow DNN models.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using softmax cross entropy.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNEstimator", "path": "compat/v1/estimator/dnnestimator", "type": "tf.compat", "text": "\nAn estimator for TensorFlow DNN models with user-specified head.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss and predicted output are determined by the specified head.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedClassifier", "path": "compat/v1/estimator/dnnlinearcombinedclassifier", "type": "tf.compat", "text": "\nAn estimator for TensorFlow Linear and DNN joined classification models.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using softmax cross entropy.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedEstimator", "path": "compat/v1/estimator/dnnlinearcombinedestimator", "type": "tf.compat", "text": "\nAn estimator for TensorFlow Linear and DNN joined models with custom head.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using mean squared error.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNLinearCombinedRegressor", "path": "compat/v1/estimator/dnnlinearcombinedregressor", "type": "tf.compat", "text": "\nAn estimator for TensorFlow Linear and DNN joined models for regression.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using mean squared error.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.DNNRegressor", "path": "compat/v1/estimator/dnnregressor", "type": "tf.compat", "text": "\nA regressor for TensorFlow DNN models.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using mean squared error.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.Estimator", "path": "compat/v1/estimator/estimator", "type": "tf.compat", "text": "\nEstimator class to train and evaluate TensorFlow models.\n\nThe `Estimator` object wraps a model which is specified by a `model_fn`,\nwhich, given inputs and a number of other parameters, returns the ops\nnecessary to perform training, evaluation, or predictions.\n\nAll outputs (checkpoints, event files, etc.) are written to `model_dir`, or a\nsubdirectory thereof. If `model_dir` is not set, a temporary directory is\nused.\n\nThe `config` argument can be passed `tf.estimator.RunConfig` object containing\ninformation about the execution environment. It is passed on to the\n`model_fn`, if the `model_fn` has a parameter named \"config\" (and input\nfunctions in the same manner). If the `config` parameter is not passed, it is\ninstantiated by the `Estimator`. Not passing config means that defaults useful\nfor local execution are used. `Estimator` makes config available to the model\n(for instance, to allow specialization based on the number of workers\navailable), and also uses some of its fields to control internals, especially\nregarding checkpointing.\n\nThe `params` argument contains hyperparameters. It is passed to the\n`model_fn`, if the `model_fn` has a parameter named \"params\", and to the input\nfunctions in the same manner. `Estimator` only passes params along, it does\nnot inspect it. The structure of `params` is therefore entirely up to the\ndeveloper.\n\nNone of `Estimator`'s methods can be overridden in subclasses (its constructor\nenforces this). Subclasses should use `model_fn` to configure the base class,\nand may add methods implementing specialized functionality.\n\nSee estimators for more information.\n\nTo warm-start an `Estimator`:\n\nFor more details on warm-start configuration, see\n`tf.estimator.WarmStartSettings`.\n\nCalling methods of `Estimator` will work while eager execution is enabled.\nHowever, the `model_fn` and `input_fn` is not executed eagerly, `Estimator`\nwill switch to graph mode before calling all user-provided functions (incl.\nhooks), so their code has to be compatible with graph mode execution. Note\nthat `input_fn` code using `tf.data` generally works in both graph and eager\nmodes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.experimental", "path": "compat/v1/estimator/experimental", "type": "tf.compat", "text": "\nPublic API for tf.estimator.experimental namespace.\n\n`class InMemoryEvaluatorHook`: Hook to run evaluation in training without a\ncheckpoint.\n\n`class KMeans`: An Estimator for K-Means clustering.\n\n`class LinearSDCA`: Stochastic Dual Coordinate Ascent helper for linear\nestimators.\n\n`build_raw_supervised_input_receiver_fn(...)`: Build a\nsupervised_input_receiver_fn for raw features and labels.\n\n`call_logit_fn(...)`: Calls logit_fn (experimental).\n\n`dnn_logit_fn_builder(...)`: Function builder for a dnn logit_fn.\n\n`linear_logit_fn_builder(...)`: Function builder for a linear logit_fn.\n\n`make_early_stopping_hook(...)`: Creates early-stopping hook.\n\n`make_stop_at_checkpoint_step_hook(...)`: Creates a proper\nStopAtCheckpointStepHook based on chief status.\n\n`stop_if_higher_hook(...)`: Creates hook to stop if the given metric is higher\nthan the threshold.\n\n`stop_if_lower_hook(...)`: Creates hook to stop if the given metric is lower\nthan the threshold.\n\n`stop_if_no_decrease_hook(...)`: Creates hook to stop if metric does not\ndecrease within given max steps.\n\n`stop_if_no_increase_hook(...)`: Creates hook to stop if metric does not\nincrease within given max steps.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.experimental.dnn_logit_fn_builder", "path": "compat/v1/estimator/experimental/dnn_logit_fn_builder", "type": "tf.compat", "text": "\nFunction builder for a dnn logit_fn.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.experimental.KMeans", "path": "compat/v1/estimator/experimental/kmeans", "type": "tf.compat", "text": "\nAn Estimator for K-Means clustering.\n\nInherits From: `Estimator`\n\nThe `SavedModel` saved by the `export_saved_model` method does not include the\ncluster centers. However, the cluster centers may be retrieved by the latest\ncheckpoint saved during training. Specifically,\n\nis equivalent to\n\nView source\n\nReturns the cluster centers.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nFinds the index of the closest cluster center to each input point.\n\nView source\n\nReturns the sum of squared distances to nearest clusters.\n\nNote that this function is different from the corresponding one in sklearn\nwhich returns the negative sum.\n\nView source\n\nTrains a model given training data `input_fn`.\n\nView source\n\nTransforms each input point to its distances to all cluster centers.\n\nNote that if `distance_metric=KMeansClustering.SQUARED_EUCLIDEAN_DISTANCE`,\nthis function returns the squared Euclidean distance while the corresponding\nsklearn function returns the Euclidean distance.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.experimental.linear_logit_fn_builder", "path": "compat/v1/estimator/experimental/linear_logit_fn_builder", "type": "tf.compat", "text": "\nFunction builder for a linear logit_fn.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.export", "path": "compat/v1/estimator/export", "type": "tf.compat", "text": "\nAll public utility methods for exporting Estimator to SavedModel.\n\nThis file includes functions and constants from core (model_utils) and\nexport.py\n\n`class ClassificationOutput`: Represents the output of a classification head.\n\n`class ExportOutput`: Represents an output of a model that can be served.\n\n`class PredictOutput`: Represents the output of a generic prediction head.\n\n`class RegressionOutput`: Represents the output of a regression head.\n\n`class ServingInputReceiver`: A return type for a serving_input_receiver_fn.\n\n`class TensorServingInputReceiver`: A return type for a\nserving_input_receiver_fn.\n\n`build_parsing_serving_input_receiver_fn(...)`: Build a\nserving_input_receiver_fn expecting fed tf.Examples.\n\n`build_raw_serving_input_receiver_fn(...)`: Build a serving_input_receiver_fn\nexpecting feature Tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.inputs", "path": "compat/v1/estimator/inputs", "type": "tf.compat", "text": "\nUtility methods to create simple input_fns.\n\n`numpy_input_fn(...)`: Returns input function that would feed dict of numpy\narrays into the model.\n\n`pandas_input_fn(...)`: Returns input function that would feed Pandas\nDataFrame into the model.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.inputs.numpy_input_fn", "path": "compat/v1/estimator/inputs/numpy_input_fn", "type": "tf.compat", "text": "\nReturns input function that would feed dict of numpy arrays into the model.\n\nThis returns a function outputting `features` and `targets` based on the dict\nof numpy arrays. The dict `features` has the same keys as the `x`. The dict\n`targets` has the same keys as the `y` if `y` is a dict.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.inputs.pandas_input_fn", "path": "compat/v1/estimator/inputs/pandas_input_fn", "type": "tf.compat", "text": "\nReturns input function that would feed Pandas DataFrame into the model.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.LinearClassifier", "path": "compat/v1/estimator/linearclassifier", "type": "tf.compat", "text": "\nLinear classifier model.\n\nInherits From: `Estimator`\n\nTrain a linear model to classify instances into one of multiple possible\nclasses. When number of possible classes is 2, this is binary classification.\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss is calculated by using softmax cross entropy.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.LinearEstimator", "path": "compat/v1/estimator/linearestimator", "type": "tf.compat", "text": "\nAn estimator for TensorFlow linear models with user-specified head.\n\nInherits From: `Estimator`\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a `KeyError`:\n\nLoss and predicted output are determined by the specified head.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.LinearRegressor", "path": "compat/v1/estimator/linearregressor", "type": "tf.compat", "text": "\nAn estimator for TensorFlow Linear regression problems.\n\nInherits From: `Estimator`\n\nTrain a linear regression model to predict label value given observation of\nfeature values.\n\nInput of `train` and `evaluate` should have following features, otherwise\nthere will be a KeyError:\n\nLoss is calculated by using mean squared error.\n\nEstimators can be used while eager execution is enabled. Note that `input_fn`\nand all hooks are executed inside a graph context, so they have to be written\nto be compatible with graph mode. Note that `input_fn` code using `tf.data`\ngenerally works in both graph and eager modes.\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.regressor_parse_example_spec", "path": "compat/v1/estimator/regressor_parse_example_spec", "type": "tf.compat", "text": "\nGenerates parsing spec for tf.parse_example to be used with regressors.\n\nIf users keep data in tf.Example format, they need to call tf.parse_example\nwith a proper feature spec. There are two main things that this utility helps:\n\nExample output of parsing spec:\n\nExample usage with a regressor:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu", "path": "compat/v1/estimator/tpu", "type": "tf.compat", "text": "\nPublic API for tf.estimator.tpu namespace.\n\n`experimental` module: Public API for tf.estimator.tpu.experimental namespace.\n\n`class InputPipelineConfig`: Please see the definition of these values in\nTPUConfig.\n\n`class RunConfig`: RunConfig with TPU support.\n\n`class TPUConfig`: TPU related configuration required by `TPUEstimator`.\n\n`class TPUEstimator`: Estimator with TPU support.\n\n`class TPUEstimatorSpec`: Ops and objects returned from a `model_fn` and\npassed to `TPUEstimator`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.experimental", "path": "compat/v1/estimator/tpu/experimental", "type": "tf.compat", "text": "\nPublic API for tf.estimator.tpu.experimental namespace.\n\n`class EmbeddingConfigSpec`: Class to keep track of the specification for TPU\nembeddings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec", "path": "compat/v1/estimator/tpu/experimental/embeddingconfigspec", "type": "tf.compat", "text": "\nClass to keep track of the specification for TPU embeddings.\n\nPass this class to `tf.estimator.tpu.TPUEstimator` via the\n`embedding_config_spec` parameter. At minimum you need to specify\n`feature_columns` and `optimization_parameters`. The feature columns passed\nshould be created with some combination of\n`tf.tpu.experimental.embedding_column` and\n`tf.tpu.experimental.shared_embedding_columns`.\n\nTPU embeddings do not support arbitrary Tensorflow optimizers and the main\noptimizer you use for your model will be ignored for the embedding table\nvariables. Instead TPU embeddigns support a fixed set of predefined optimizers\nthat you can select from and set the parameters of. These include adagrad,\nadam and stochastic gradient descent. Each supported optimizer has a\n`Parameters` class in the `tf.tpu.experimental` namespace.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.InputPipelineConfig", "path": "compat/v1/estimator/tpu/inputpipelineconfig", "type": "tf.compat", "text": "\nPlease see the definition of these values in TPUConfig.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.RunConfig", "path": "compat/v1/estimator/tpu/runconfig", "type": "tf.compat", "text": "\nRunConfig with TPU support.\n\nInherits From: `RunConfig`\n\nIf device_fn is not `None`, it overrides the default device function used in\n`Estimator`. Otherwise the default one is used.\n\nAll global ids in the training cluster are assigned from an increasing\nsequence of consecutive integers. The first id is 0.\n\nNodes with task type `worker` can have id 0, 1, 2. Nodes with task type `ps`\ncan have id, 0, 1. So, `task_id` is not unique, but the pair (`task_type`,\n`task_id`) can uniquely determine a node in the cluster.\n\nGlobal id, i.e., this field, is tracking the index of the node among ALL nodes\nin the cluster. It is uniquely assigned. For example, for the cluster spec\ngiven above, the global ids are assigned as:\n\nView source\n\nReturns a new instance of `RunConfig` replacing specified properties.\n\nOnly the properties in the following list are allowed to be replaced:\n\nIn addition, either `save_checkpoints_steps` or `save_checkpoints_secs` can be\nset (should not be both).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.TPUConfig", "path": "compat/v1/estimator/tpu/tpuconfig", "type": "tf.compat", "text": "\nTPU related configuration required by `TPUEstimator`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.TPUEstimator", "path": "compat/v1/estimator/tpu/tpuestimator", "type": "tf.compat", "text": "\nEstimator with TPU support.\n\nInherits From: `Estimator`\n\nTPUEstimator also supports training on CPU and GPU. You don't need to define a\nseparate `tf.estimator.Estimator`.\n\nTPUEstimator handles many of the details of running on TPU devices, such as\nreplicating inputs and models for each core, and returning to host\nperiodically to run hooks.\n\nTPUEstimator transforms a global batch size in params to a per-shard batch\nsize when calling the `input_fn` and `model_fn`. Users should specify global\nbatch size in constructor, and then get the batch size for each shard in\n`input_fn` and `model_fn` by `params['batch_size']`.\n\nFor training, `model_fn` gets per-core batch size; `input_fn` may get per-core\nor per-host batch size depending on `per_host_input_for_training` in\n`TPUConfig` (See docstring for TPUConfig for details).\n\nFor evaluation and prediction, `model_fn` gets per-core batch size and\n`input_fn` get per-host batch size.\n\n`model_fn` should return `TPUEstimatorSpec`, which expects the `eval_metrics`\nfor TPU evaluation. If eval_on_tpu is False, the evaluation will execute on\nCPU or GPU; in this case the following discussion on TPU evaluation does not\napply.\n\n`TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`, where\n`tensors` could be a list of any nested structure of `Tensor`s (See\n`TPUEstimatorSpec` for details). `metric_fn` takes the `tensors` and returns a\ndict from metric string name to the result of calling a metric function,\nnamely a `(metric_tensor, update_op)` tuple.\n\nOne can set `use_tpu` to `False` for testing. All training, evaluation, and\npredict will be executed on CPU. `input_fn` and `model_fn` will receive\n`train_batch_size` or `eval_batch_size` unmodified as `params['batch_size']`.\n\nTPU evaluation only works on a single host (one TPU worker) except BROADCAST\nmode.\n\n`input_fn` for evaluation should NOT raise an end-of-input exception\n(`OutOfRangeError` or `StopIteration`). And all evaluation steps and all\nbatches should have the same size.\n\nPrediction on TPU is an experimental feature to support large batch inference.\nIt is not designed for latency-critical system. In addition, due to some\nusability issues, for prediction with small dataset, CPU `.predict`, i.e.,\ncreating a new `TPUEstimator` instance with `use_tpu=False`, might be more\nconvenient.\n\nTPU prediction only works on a single host (one TPU worker).\n\n`input_fn` must return a `Dataset` instance rather than `features`. In fact,\n.train() and .evaluate() also support Dataset as return value.\n\n`export_saved_model` exports 2 metagraphs, one with `saved_model.SERVING`, and\nanother with `saved_model.SERVING` and `saved_model.TPU` tags. At serving\ntime, these tags are used to select the appropriate metagraph to load.\n\nBefore running the graph on TPU, the TPU system needs to be initialized. If\nTensorFlow Serving model-server is used, this is done automatically. If not,\nplease use `session.run(tpu.initialize_system())`.\n\nThere are two versions of the API: 1 or 2.\n\nIn V1, the exported CPU graph is `model_fn` as it is. The exported TPU graph\nwraps `tpu.rewrite()` and `TPUPartitionedCallOp` around `model_fn` so\n`model_fn` is on TPU by default. To place ops on CPU,\n`tpu.outside_compilation(host_call, logits)` can be used.\n\nIn V2, `export_saved_model()` sets up `params['use_tpu']` flag to let the user\nknow if the code is exporting to TPU (or not). When `params['use_tpu']` is\n`True`, users need to call `tpu.rewrite()`, `TPUPartitionedCallOp` and/or\n`batch_function()`. Alternatively use `inference_on_tpu()` which is a\nconvenience wrapper of the three.\n\nTIP: V2 is recommended as it is more flexible (eg: batching, etc).\n\nView source\n\nShows the directory name where evaluation metrics are dumped.\n\nView source\n\nEvaluates the model given evaluation data `input_fn`.\n\nFor each step, calls `input_fn`, which returns one batch of data. Evaluates\nuntil:\n\nView source\n\nExports a `SavedModel` with `tf.MetaGraphDefs` for each requested mode.\n\nFor each mode passed in via the `input_receiver_fn_map`, this method builds a\nnew graph by calling the `input_receiver_fn` to obtain feature and label\n`Tensor`s. Next, this method calls the `Estimator`'s `model_fn` in the passed\nmode to generate the model graph based on those features and labels, and\nrestores the given checkpoint (or, lacking that, the most recent checkpoint)\ninto the graph. Only one of the modes is used for saving variables to the\n`SavedModel` (order of preference: `tf.estimator.ModeKeys.TRAIN`,\n`tf.estimator.ModeKeys.EVAL`, then `tf.estimator.ModeKeys.PREDICT`), such that\nup to three `tf.MetaGraphDefs` are saved with a single set of variables in a\nsingle `SavedModel` directory.\n\nFor the variables and `tf.MetaGraphDefs`, a timestamped export directory below\n`export_dir_base`, and writes a `SavedModel` into it containing the\n`tf.MetaGraphDef` for the given mode and its associated signatures.\n\nFor prediction, the exported `MetaGraphDef` will provide one `SignatureDef`\nfor each element of the `export_outputs` dict returned from the `model_fn`,\nnamed using the same keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nFor training and evaluation, the `train_op` is stored in an extra collection,\nand loss, metrics, and predictions are included in a `SignatureDef` for the\nmode in question.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir.\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nThe experimental_mode parameter can be used to export a single\ntrain/eval/predict graph as a `SavedModel`. See\n`experimental_export_all_saved_models` for full docs.\n\nView source\n\nExports inference graph as a `SavedModel` into the given dir. (deprecated)\n\nFor a detailed guide, see SavedModel from Estimators.\n\nThis method builds a new graph by first calling the\n`serving_input_receiver_fn` to obtain feature `Tensor`s, and then calling this\n`Estimator`'s `model_fn` to generate the model graph based on those features.\nIt restores the given checkpoint (or, lacking that, the most recent\ncheckpoint) into this graph in a fresh session. Finally it creates a\ntimestamped export directory below the given `export_dir_base`, and writes a\n`SavedModel` into it containing a single `tf.MetaGraphDef` saved from this\nsession.\n\nThe exported `MetaGraphDef` will provide one `SignatureDef` for each element\nof the `export_outputs` dict returned from the `model_fn`, named using the\nsame keys. One of these keys is always\n`tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY`,\nindicating which signature will be served when a serving request does not\nspecify one. For each signature, the outputs are provided by the corresponding\n`tf.estimator.export.ExportOutput`s, and the inputs are always the input\nreceivers provided by the `serving_input_receiver_fn`.\n\nExtra assets may be written into the `SavedModel` via the `assets_extra`\nargument. This should be a dict, where each key gives a destination path\n(including the filename) relative to the assets.extra directory. The\ncorresponding value gives the full path of the source file to be copied. For\nexample, the simple case of copying a single file without renaming it is\nspecified as `{'my_asset_file.txt': '/path/to/my_asset_file.txt'}`.\n\nView source\n\nReturns list of all variable names in this model.\n\nView source\n\nReturns value of the variable given by name.\n\nView source\n\nFinds the filename of the latest saved checkpoint file in `model_dir`.\n\nView source\n\nYields predictions for given features.\n\nPlease note that interleaving two predict outputs does not work. See:\nissue/20506\n\nView source\n\nTrains a model given training data `input_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.estimator.tpu.TPUEstimatorSpec", "path": "compat/v1/estimator/tpu/tpuestimatorspec", "type": "tf.compat", "text": "\nOps and objects returned from a `model_fn` and passed to `TPUEstimator`.\n\nSee `EstimatorSpec` for `mode`, `predictions`, `loss`, `train_op`, and\n`export_outputs`.\n\nFor evaluation, `eval_metrics`is a tuple of `metric_fn` and `tensors`, where\n`metric_fn` runs on CPU to generate metrics and `tensors` represents the\n`Tensor`s transferred from TPU system to CPU host and passed to `metric_fn`.\nTo be precise, TPU evaluation expects a slightly different signature from the\n`tf.estimator.Estimator`. While `EstimatorSpec.eval_metric_ops` expects a\ndict, `TPUEstimatorSpec.eval_metrics` is a tuple of `metric_fn` and `tensors`.\nThe `tensors` could be a list of `Tensor`s or dict of names to `Tensor`s. The\n`tensors` usually specify the model logits, which are transferred back from\nTPU system to CPU host. All tensors must have be batch-major, i.e., the batch\nsize is the first dimension. Once all tensors are available at CPU host from\nall shards, they are concatenated (on CPU) and passed as positional arguments\nto the `metric_fn` if `tensors` is list or keyword arguments if `tensors` is a\ndict. `metric_fn` takes the `tensors` and returns a dict from metric string\nname to the result of calling a metric function, namely a `(metric_tensor,\nupdate_op)` tuple. See `TPUEstimator` for MNIST example how to specify the\n`eval_metrics`.\n\n`scaffold_fn` is a function running on CPU to generate the `Scaffold`. This\nfunction should not capture any Tensors in `model_fn`.\n\n`host_call` is a tuple of a `function` and a list or dictionary of `tensors`\nto pass to that function and returns a list of Tensors. `host_call` currently\nworks for train() and evaluate(). The Tensors returned by the function is\nexecuted on the CPU on every step, so there is communication overhead when\nsending tensors from TPU to CPU. To reduce the overhead, try reducing the size\nof the tensors. The `tensors` are concatenated along their major (batch)\ndimension, and so must be >= rank 1. The `host_call` is useful for writing\nsummaries with `tf.contrib.summary.create_file_writer`.\n\nView source\n\nCreates an equivalent `EstimatorSpec` used by CPU train/eval.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.Event", "path": "compat/v1/event", "type": "tf.compat", "text": "\nA ProtocolMessage\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.summary.Event`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.executing_eagerly", "path": "compat/v1/executing_eagerly", "type": "tf.compat", "text": "\nChecks whether the current thread has eager execution enabled.\n\nEager execution is typically enabled via\n`tf.compat.v1.enable_eager_execution`, but may also be enabled within the\ncontext of a Python function via tf.contrib.eager.py_func.\n\nWhen eager execution is enabled, returns `True` in most cases. However, this\nAPI might return `False` in the following use cases.\n\nInside `tf.function`:\n\nInside `tf.function` after `tf.config.run_functions_eagerly(True)` is called:\n\nInside a transformation function for `tf.dataset`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.executing_eagerly_outside_functions", "path": "compat/v1/executing_eagerly_outside_functions", "type": "tf.compat", "text": "\nReturns True if executing eagerly, even if inside a graph function.\n\nThis function will check the outermost context for the program and see if it\nis in eager mode. It is useful comparing to `tf.executing_eagerly()`, which\nchecks the current context and will return `False` within a `tf.function`\nbody. It can be used to build library that behave differently in eager runtime\nand v1 session runtime (deprecated).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.expand_dims", "path": "compat/v1/expand_dims", "type": "tf.compat", "text": "\nReturns a tensor with a length 1 axis inserted at index `axis`. (deprecated\narguments)\n\nGiven a tensor `input`, this operation inserts a dimension of length 1 at the\ndimension index `axis` of `input`'s shape. The dimension index follows Python\nindexing rules: It's zero-based, a negative index it is counted backward from\nthe end.\n\nThis operation is useful to:\n\nIf you have a single image of shape `[height, width, channels]`:\n\nYou can add an outer `batch` axis by passing `axis=0`:\n\nThe new axis location matches Python `list.insert(axis, 1)`:\n\nFollowing standard Python indexing rules, a negative `axis` counts from the\nend so `axis=-1` adds an inner most dimension:\n\nThis operation requires that `axis` is a valid index for `input.shape`,\nfollowing Python indexing rules:\n\nThis operation is related to:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.experimental", "path": "compat/v1/experimental", "type": "tf.compat", "text": "\nPublic API for tf.experimental namespace.\n\n`class Optional`: Represents a value that may or may not be present.\n\n`async_clear_error(...)`: Clear pending operations and error statuses in async\nexecution.\n\n`async_scope(...)`: Context manager for grouping async operations.\n\n`function_executor_type(...)`: Context manager for setting the executor of\neager defined functions.\n\n`output_all_intermediates(...)`: Whether to output all intermediates from\nfunctional control flow ops.\n\n`register_filesystem_plugin(...)`: Loads a TensorFlow FileSystem plugin.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.experimental.output_all_intermediates", "path": "compat/v1/experimental/output_all_intermediates", "type": "tf.compat", "text": "\nWhether to output all intermediates from functional control flow ops.\n\nThe \"default\" behavior to is to output all intermediates when using v2 control\nflow inside Keras models in graph mode (possibly inside Estimators). This is\nneeded to support taking gradients of v2 control flow. In graph mode, Keras\ncan sometimes freeze the forward graph before the gradient computation which\ndoes not work for v2 control flow since it requires updating the forward ops\nto output the needed intermediates. We work around this by proactively\noutputting the needed intermediates when building the forward pass itself.\nIdeally any such extra tensors should be pruned out at runtime. However, if\nfor any reason this doesn't work for you or if you have an inference-only\nmodel you can turn this behavior off using\n`tf.compat.v1.experimental.output_all_intermediates(False)`.\n\nIf with the default behavior you are still seeing errors of the form\n\"Connecting to invalid output X of source node Y which has Z outputs\" try\nsetting `tf.compat.v1.experimental.output_all_intermediates(True)` and please\nfile an issue at https://github.com/tensorflow/tensorflow/issues.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.extract_image_patches", "path": "compat/v1/extract_image_patches", "type": "tf.compat", "text": "\nExtract `patches` from `images` and put them in the \"depth\" output dimension.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.image.extract_image_patches`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column", "path": "compat/v1/feature_column", "type": "tf.compat", "text": "\nPublic API for tf.feature_column namespace.\n\n`bucketized_column(...)`: Represents discretized dense input bucketed by\n`boundaries`.\n\n`categorical_column_with_hash_bucket(...)`: Represents sparse feature where\nids are set by hashing.\n\n`categorical_column_with_identity(...)`: A `CategoricalColumn` that returns\nidentity values.\n\n`categorical_column_with_vocabulary_file(...)`: A `CategoricalColumn` with a\nvocabulary file.\n\n`categorical_column_with_vocabulary_list(...)`: A `CategoricalColumn` with in-\nmemory vocabulary.\n\n`crossed_column(...)`: Returns a column for performing crosses of categorical\nfeatures.\n\n`embedding_column(...)`: `DenseColumn` that converts from sparse, categorical\ninput.\n\n`indicator_column(...)`: Represents multi-hot representation of given\ncategorical column.\n\n`input_layer(...)`: Returns a dense `Tensor` as input layer based on given\n`feature_columns`.\n\n`linear_model(...)`: Returns a linear prediction `Tensor` based on given\n`feature_columns`.\n\n`make_parse_example_spec(...)`: Creates parsing spec dictionary from input\nfeature_columns.\n\n`numeric_column(...)`: Represents real valued or numerical features.\n\n`sequence_categorical_column_with_hash_bucket(...)`: A sequence of categorical\nterms where ids are set by hashing.\n\n`sequence_categorical_column_with_identity(...)`: Returns a feature column\nthat represents sequences of integers.\n\n`sequence_categorical_column_with_vocabulary_file(...)`: A sequence of\ncategorical terms where ids use a vocabulary file.\n\n`sequence_categorical_column_with_vocabulary_list(...)`: A sequence of\ncategorical terms where ids use an in-memory list.\n\n`sequence_numeric_column(...)`: Returns a feature column that represents\nsequences of numeric data.\n\n`shared_embedding_columns(...)`: List of dense columns that convert from\nsparse, categorical input.\n\n`weighted_categorical_column(...)`: Applies weight values to a\n`CategoricalColumn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column.categorical_column_with_vocabulary_file", "path": "compat/v1/feature_column/categorical_column_with_vocabulary_file", "type": "tf.compat", "text": "\nA `CategoricalColumn` with a vocabulary file.\n\nUse this when your inputs are in string or integer format, and you have a\nvocabulary file that maps each value to an integer ID. By default, out-of-\nvocabulary values are ignored. Use either (but not both) of `num_oov_buckets`\nand `default_value` to specify how to include out-of-vocabulary values.\n\nFor input dictionary `features`, `features[key]` is either `Tensor` or\n`SparseTensor`. If `Tensor`, missing values can be represented by `-1` for int\nand `''` for string, which will be dropped by this feature column.\n\nExample with `num_oov_buckets`: File '/us/states.txt' contains 50 lines, each\nwith a 2-character U.S. state abbreviation. All inputs with values in that\nfile are assigned an ID 0-49, corresponding to its line number. All other\nvalues are hashed and assigned an ID 50-54.\n\nExample with `default_value`: File '/us/states.txt' contains 51 lines - the\nfirst line is 'XX', and the other 50 each have a 2-character U.S. state\nabbreviation. Both a literal 'XX' in input, and other values missing from the\nfile, will be assigned ID 0. All others are assigned the corresponding line\nnumber 1-50.\n\nAnd to make an embedding with either:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column.input_layer", "path": "compat/v1/feature_column/input_layer", "type": "tf.compat", "text": "\nReturns a dense `Tensor` as input layer based on given `feature_columns`.\n\nGenerally a single example in training data is described with FeatureColumns.\nAt the first layer of the model, this column oriented data should be converted\nto a single `Tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column.linear_model", "path": "compat/v1/feature_column/linear_model", "type": "tf.compat", "text": "\nReturns a linear prediction `Tensor` based on given `feature_columns`.\n\nThis function generates a weighted sum based on output dimension `units`.\nWeighted sum refers to logits in classification problems. It refers to the\nprediction itself for linear regression problems.\n\nNote on supported columns: `linear_model` treats categorical columns as\n`indicator_column`s. To be specific, assume the input as `SparseTensor` looks\nlike:\n\n`linear_model` assigns weights for the presence of \"a\", \"b\", \"c' implicitly,\njust like `indicator_column`, while `input_layer` explicitly requires wrapping\neach of categorical columns with an `embedding_column` or an\n`indicator_column`.\n\nThe `sparse_combiner` argument works as follows For example, for two features\nrepresented as the categorical columns:\n\nwith `sparse_combiner` as \"mean\", the linear model outputs consequently are:\n\nwhere `y_i` is the output, `b` is the bias, and `w_x` is the weight assigned\nto the presence of `x` in the input features.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column.make_parse_example_spec", "path": "compat/v1/feature_column/make_parse_example_spec", "type": "tf.compat", "text": "\nCreates parsing spec dictionary from input feature_columns.\n\nThe returned dictionary can be used as arg 'features' in\n`tf.io.parse_example`.\n\nFor the above example, make_parse_example_spec would return the dict:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.feature_column.shared_embedding_columns", "path": "compat/v1/feature_column/shared_embedding_columns", "type": "tf.compat", "text": "\nList of dense columns that convert from sparse, categorical input.\n\nThis is similar to `embedding_column`, except that it produces a list of\nembedding columns that share the same embedding weights.\n\nUse this when your inputs are sparse and of the same type (e.g. watched and\nimpression video IDs that share the same vocabulary), and you want to convert\nthem to a dense representation (e.g., to feed to a DNN).\n\nInputs must be a list of categorical columns created by any of the\n`categorical_column_*` function. They must all be of the same type and have\nthe same arguments except `key`. E.g. they can be\ncategorical_column_with_vocabulary_file with the same vocabulary_file. Some or\nall columns could also be weighted_categorical_column.\n\nHere is an example embedding of two features for a DNNClassifier model:\n\nHere is an example using `shared_embedding_columns` with model_fn:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.FixedLengthRecordReader", "path": "compat/v1/fixedlengthrecordreader", "type": "tf.compat", "text": "\nA Reader that outputs fixed-length records from a file.\n\nInherits From: `ReaderBase`\n\nSee ReaderBase for supported methods.\n\nReaders are not compatible with eager execution. Instead, please use `tf.data`\nto get data into your model.\n\nView source\n\nReturns the number of records this reader has produced.\n\nThis is the same as the number of Read executions that have succeeded.\n\nView source\n\nReturns the number of work units this reader has finished processing.\n\nView source\n\nReturns the next record (key, value) pair produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g. when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile).\n\nView source\n\nReturns up to num_records (key, value) pairs produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g., when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile). It may return less than num_records even before the last batch.\n\nView source\n\nRestore a reader to its initial clean state.\n\nView source\n\nRestore a reader to a previously saved state.\n\nNot all Readers support being restored, so this can produce an Unimplemented\nerror.\n\nView source\n\nProduce a string tensor that encodes the state of a reader.\n\nNot all Readers support being serialized, so this can produce an Unimplemented\nerror.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.fixed_size_partitioner", "path": "compat/v1/fixed_size_partitioner", "type": "tf.compat", "text": "\nPartitioner to specify a fixed number of shards along given axis.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags", "path": "compat/v1/flags", "type": "tf.compat", "text": "\nImport router for absl.flags. See https://github.com/abseil/abseil-py\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags`\n\n`tf_decorator` module: Base TFDecorator class and utility functions for\nworking with decorators.\n\n`class ArgumentParser`: Base class used to parse and convert arguments.\n\n`class ArgumentSerializer`: Base class for generating string representations\nof a flag value.\n\n`class BaseListParser`: Base class for a parser of lists of strings.\n\n`class BooleanFlag`: Basic boolean flag.\n\n`class BooleanParser`: Parser of boolean values.\n\n`class CantOpenFlagFileError`: Raised when flagfile fails to open.\n\n`class CsvListSerializer`: Base class for generating string representations of\na flag value.\n\n`class DuplicateFlagError`: Raised if there is a flag naming conflict.\n\n`class EnumClassFlag`: Basic enum flag; its value is an enum class's member.\n\n`class EnumClassParser`: Parser of an Enum class member.\n\n`class EnumFlag`: Basic enum flag; its value can be any string from list of\nenum_values.\n\n`class EnumParser`: Parser of a string enum value (a string value from a given\nset).\n\n`class Error`: The base class for all flags errors.\n\n`class Flag`: Information about a command-line flag.\n\n`class FlagHolder`: Holds a defined flag.\n\n`class FlagNameConflictsWithMethodError`: Raised when a flag name conflicts\nwith FlagValues methods.\n\n`class FlagValues`: Registry of 'Flag' objects.\n\n`class FloatParser`: Parser of floating point values.\n\n`class IllegalFlagValueError`: Raised when the flag command line argument is\nillegal.\n\n`class IntegerParser`: Parser of an integer value.\n\n`class ListParser`: Parser for a comma-separated list of strings.\n\n`class ListSerializer`: Base class for generating string representations of a\nflag value.\n\n`class MultiEnumClassFlag`: A multi_enum_class flag.\n\n`class MultiFlag`: A flag that can appear multiple time on the command-line.\n\n`class UnparsedFlagAccessError`: Raised when accessing the flag value from\nunparsed FlagValues.\n\n`class UnrecognizedFlagError`: Raised when a flag is unrecognized.\n\n`class ValidationError`: Raised when flag validator constraint is not\nsatisfied.\n\n`class WhitespaceSeparatedListParser`: Parser for a whitespace-separated list\nof strings.\n\n`DEFINE(...)`: Registers a generic Flag object.\n\n`DEFINE_alias(...)`: Defines an alias flag for an existing one.\n\n`DEFINE_bool(...)`: Registers a boolean flag.\n\n`DEFINE_boolean(...)`: Registers a boolean flag.\n\n`DEFINE_enum(...)`: Registers a flag whose value can be any string from\nenum_values.\n\n`DEFINE_enum_class(...)`: Registers a flag whose value can be the name of enum\nmembers.\n\n`DEFINE_flag(...)`: Registers a 'Flag' object with a 'FlagValues' object.\n\n`DEFINE_float(...)`: Registers a flag whose value must be a float.\n\n`DEFINE_integer(...)`: Registers a flag whose value must be an integer.\n\n`DEFINE_list(...)`: Registers a flag whose value is a comma-separated list of\nstrings.\n\n`DEFINE_multi(...)`: Registers a generic MultiFlag that parses its args with a\ngiven parser.\n\n`DEFINE_multi_enum(...)`: Registers a flag whose value can be a list strings\nfrom enum_values.\n\n`DEFINE_multi_enum_class(...)`: Registers a flag whose value can be a list of\nenum members.\n\n`DEFINE_multi_float(...)`: Registers a flag whose value can be a list of\narbitrary floats.\n\n`DEFINE_multi_integer(...)`: Registers a flag whose value can be a list of\narbitrary integers.\n\n`DEFINE_multi_string(...)`: Registers a flag whose value can be a list of any\nstrings.\n\n`DEFINE_spaceseplist(...)`: Registers a flag whose value is a whitespace-\nseparated list of strings.\n\n`DEFINE_string(...)`: Registers a flag whose value can be any string.\n\n`FLAGS(...)`: Registry of 'Flag' objects.\n\n`adopt_module_key_flags(...)`: Declares that all flags key to a module are key\nto the current module.\n\n`declare_key_flag(...)`: Declares one flag as key to the current module.\n\n`disclaim_key_flags(...)`: Declares that the current module will not define\nany more key flags.\n\n`doc_to_help(...)`: Takes a doc string and reformats it as help.\n\n`flag_dict_to_args(...)`: Convert a dict of values into process call\nparameters.\n\n`get_help_width(...)`: Returns the integer width of help lines that is used in\nTextWrap.\n\n`mark_bool_flags_as_mutual_exclusive(...)`: Ensures that only one flag among\nflag_names is True.\n\n`mark_flag_as_required(...)`: Ensures that flag is not None during program\nexecution.\n\n`mark_flags_as_mutual_exclusive(...)`: Ensures that only one flag among\nflag_names is not None.\n\n`mark_flags_as_required(...)`: Ensures that flags are not None during program\nexecution.\n\n`multi_flags_validator(...)`: A function decorator for defining a multi-flag\nvalidator.\n\n`register_multi_flags_validator(...)`: Adds a constraint to multiple flags.\n\n`register_validator(...)`: Adds a constraint, which will be enforced during\nprogram execution.\n\n`text_wrap(...)`: Wraps a given text to a maximum line length and returns it.\n\n`validator(...)`: A function decorator for defining a flag validator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.adopt_module_key_flags", "path": "compat/v1/flags/adopt_module_key_flags", "type": "tf.compat", "text": "\nDeclares that all flags key to a module are key to the current module.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.adopt_module_key_flags`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.ArgumentParser", "path": "compat/v1/flags/argumentparser", "type": "tf.compat", "text": "\nBase class used to parse and convert arguments.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.ArgumentParser`\n\nThe parse() method checks to make sure that the string argument is a legal\nvalue and convert it to a native type. If the value cannot be converted, it\nshould throw a 'ValueError' exception with a human readable explanation of why\nthe value is illegal.\n\nSubclasses should also define a syntactic_help string which may be presented\nto the user to describe the form of the legal values.\n\nArgument parser classes must be stateless, since instances are cached and\nshared between flags. Initializer arguments are allowed, but all member\nvariables must be derived from initializer arguments only.\n\nReturns a string representing the type of the flag.\n\nParses the string argument and returns the native value.\n\nBy default it returns its argument unmodified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.ArgumentSerializer", "path": "compat/v1/flags/argumentserializer", "type": "tf.compat", "text": "\nBase class for generating string representations of a flag value.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.ArgumentSerializer`\n\nReturns a serialized string of the value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.BaseListParser", "path": "compat/v1/flags/baselistparser", "type": "tf.compat", "text": "\nBase class for a parser of lists of strings.\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.BaseListParser`\n\nTo extend, inherit from this class; from the subclass init, call\n\nwhere token is a character used to tokenize, and name is a description of the\nseparator.\n\nSee base class.\n\nSee base class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.BooleanFlag", "path": "compat/v1/flags/booleanflag", "type": "tf.compat", "text": "\nBasic boolean flag.\n\nInherits From: `Flag`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.BooleanFlag`\n\nBoolean flags do not take any arguments, and their value is either True (1) or\nFalse (0). The false value is specified on the command line by prepending the\nword 'no' to either the long or the short flag name.\n\nFor example, if a Boolean flag was created whose long name was 'update' and\nwhose short name was 'x', then this flag could be explicitly unset through\neither --noupdate or --nox.\n\nReturns a str that describes the type of the flag.\n\nParses string and sets flag value.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.BooleanParser", "path": "compat/v1/flags/booleanparser", "type": "tf.compat", "text": "\nParser of boolean values.\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.BooleanParser`\n\nSee base class.\n\nSee base class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.CantOpenFlagFileError", "path": "compat/v1/flags/cantopenflagfileerror", "type": "tf.compat", "text": "\nRaised when flagfile fails to open.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.CantOpenFlagFileError`\n\nE.g. the file doesn't exist, or has wrong permissions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.CsvListSerializer", "path": "compat/v1/flags/csvlistserializer", "type": "tf.compat", "text": "\nBase class for generating string representations of a flag value.\n\nInherits From: `ArgumentSerializer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.CsvListSerializer`\n\nSerializes a list as a CSV string or unicode.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.declare_key_flag", "path": "compat/v1/flags/declare_key_flag", "type": "tf.compat", "text": "\nDeclares one flag as key to the current module.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.declare_key_flag`\n\nKey flags are flags that are deemed really important for a module. They are\nimportant when listing help messages; e.g., if the --helpshort command-line\nflag is used, then only the key flags of the main module are listed (instead\nof all flags, as in the case of --helpfull).\n\nflags.declare_key_flag('flag_1')\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE", "path": "compat/v1/flags/define", "type": "tf.compat", "text": "\nRegisters a generic Flag object.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE`\n\nAuxiliary function: clients should use the specialized DEFINE_ function\ninstead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_alias", "path": "compat/v1/flags/define_alias", "type": "tf.compat", "text": "\nDefines an alias flag for an existing one.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_alias`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_bool", "path": "compat/v1/flags/define_bool", "type": "tf.compat", "text": "\nRegisters a boolean flag.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_bool`, `tf.compat.v1.app.flags.DEFINE_boolean`,\n`tf.compat.v1.flags.DEFINE_boolean`\n\nSuch a boolean flag does not take an argument. If a user wants to specify a\nfalse value explicitly, the long option beginning with 'no' must be used: i.e.\n--noflag\n\nThis flag will have a value of None, True or False. None is possible if\ndefault=None and the user does not specify the flag on the command line.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_enum", "path": "compat/v1/flags/define_enum", "type": "tf.compat", "text": "\nRegisters a flag whose value can be any string from enum_values.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_enum`\n\nInstead of a string enum, prefer `DEFINE_enum_class`, which allows defining\nenums from an `enum.Enum` class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_enum_class", "path": "compat/v1/flags/define_enum_class", "type": "tf.compat", "text": "\nRegisters a flag whose value can be the name of enum members.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_enum_class`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_flag", "path": "compat/v1/flags/define_flag", "type": "tf.compat", "text": "\nRegisters a 'Flag' object with a 'FlagValues' object.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_flag`\n\nBy default, the global FLAGS 'FlagValue' object is used.\n\nTypical users will use one of the more specialized DEFINE_xxx functions, such\nas DEFINE_string or DEFINE_integer. But developers who need to create Flag\nobjects themselves should use this function to register their flags.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_float", "path": "compat/v1/flags/define_float", "type": "tf.compat", "text": "\nRegisters a flag whose value must be a float.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_float`\n\nIf lower_bound or upper_bound are set, then this flag must be within the given\nrange.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_integer", "path": "compat/v1/flags/define_integer", "type": "tf.compat", "text": "\nRegisters a flag whose value must be an integer.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_integer`\n\nIf lower_bound, or upper_bound are set, then this flag must be within the\ngiven range.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_list", "path": "compat/v1/flags/define_list", "type": "tf.compat", "text": "\nRegisters a flag whose value is a comma-separated list of strings.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_list`\n\nThe flag value is parsed with a CSV parser.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi", "path": "compat/v1/flags/define_multi", "type": "tf.compat", "text": "\nRegisters a generic MultiFlag that parses its args with a given parser.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi`\n\nAuxiliary function. Normal users should NOT use it directly.\n\nDevelopers who need to create their own 'Parser' classes for options which can\nappear multiple times can call this module function to register their flags.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi_enum", "path": "compat/v1/flags/define_multi_enum", "type": "tf.compat", "text": "\nRegisters a flag whose value can be a list strings from enum_values.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi_enum`\n\nUse the flag on the command line multiple times to place multiple enum values\ninto the list. The 'default' may be a single string (which will be converted\ninto a single-element list) or a list of strings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi_enum_class", "path": "compat/v1/flags/define_multi_enum_class", "type": "tf.compat", "text": "\nRegisters a flag whose value can be a list of enum members.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi_enum_class`\n\nUse the flag on the command line multiple times to place multiple enum values\ninto the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi_float", "path": "compat/v1/flags/define_multi_float", "type": "tf.compat", "text": "\nRegisters a flag whose value can be a list of arbitrary floats.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi_float`\n\nUse the flag on the command line multiple times to place multiple float values\ninto the list. The 'default' may be a single float (which will be converted\ninto a single-element list) or a list of floats.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi_integer", "path": "compat/v1/flags/define_multi_integer", "type": "tf.compat", "text": "\nRegisters a flag whose value can be a list of arbitrary integers.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi_integer`\n\nUse the flag on the command line multiple times to place multiple integer\nvalues into the list. The 'default' may be a single integer (which will be\nconverted into a single-element list) or a list of integers.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_multi_string", "path": "compat/v1/flags/define_multi_string", "type": "tf.compat", "text": "\nRegisters a flag whose value can be a list of any strings.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_multi_string`\n\nUse the flag on the command line multiple times to place multiple string\nvalues into the list. The 'default' may be a single string (which will be\nconverted into a single-element list) or a list of strings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_spaceseplist", "path": "compat/v1/flags/define_spaceseplist", "type": "tf.compat", "text": "\nRegisters a flag whose value is a whitespace-separated list of strings.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_spaceseplist`\n\nAny whitespace can be used as a separator.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DEFINE_string", "path": "compat/v1/flags/define_string", "type": "tf.compat", "text": "\nRegisters a flag whose value can be any string.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DEFINE_string`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.disclaim_key_flags", "path": "compat/v1/flags/disclaim_key_flags", "type": "tf.compat", "text": "\nDeclares that the current module will not define any more key flags.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.disclaim_key_flags`\n\nNormally, the module that calls the DEFINE_xxx functions claims the flag to be\nits key flag. This is undesirable for modules that define additional\nDEFINE_yyy functions with its own flag parsers and serializers, since that\nmodule will accidentally claim flags defined by DEFINE_yyy as its key flags.\nAfter calling this function, the module disclaims flag definitions thereafter,\nso the key flags will be correctly attributed to the caller of DEFINE_yyy.\n\nAfter calling this function, the module will not be able to define any more\nflags. This function will affect all FlagValues objects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.doc_to_help", "path": "compat/v1/flags/doc_to_help", "type": "tf.compat", "text": "\nTakes a doc string and reformats it as help.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.doc_to_help`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.DuplicateFlagError", "path": "compat/v1/flags/duplicateflagerror", "type": "tf.compat", "text": "\nRaised if there is a flag naming conflict.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.DuplicateFlagError`\n\nCreates a DuplicateFlagError by providing flag name and values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.EnumClassFlag", "path": "compat/v1/flags/enumclassflag", "type": "tf.compat", "text": "\nBasic enum flag; its value is an enum class's member.\n\nInherits From: `Flag`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.EnumClassFlag`\n\nReturns a str that describes the type of the flag.\n\nParses string and sets flag value.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.EnumClassParser", "path": "compat/v1/flags/enumclassparser", "type": "tf.compat", "text": "\nParser of an Enum class member.\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.EnumClassParser`\n\nSee base class.\n\nDetermines validity of argument and returns the correct element of enum.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.EnumFlag", "path": "compat/v1/flags/enumflag", "type": "tf.compat", "text": "\nBasic enum flag; its value can be any string from list of enum_values.\n\nInherits From: `Flag`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.EnumFlag`\n\nReturns a str that describes the type of the flag.\n\nParses string and sets flag value.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.EnumParser", "path": "compat/v1/flags/enumparser", "type": "tf.compat", "text": "\nParser of a string enum value (a string value from a given set).\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.EnumParser`\n\nSee base class.\n\nDetermines validity of argument and returns the correct element of enum.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.Error", "path": "compat/v1/flags/error", "type": "tf.compat", "text": "\nThe base class for all flags errors.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.Error`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.Flag", "path": "compat/v1/flags/flag", "type": "tf.compat", "text": "\nInformation about a command-line flag.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.Flag`\n\n'Flag' objects define the following fields: .name - the name for this flag;\n.default - the default value for this flag; .default_unparsed - the unparsed\ndefault value for this flag. .default_as_str - default value as repr'd string,\ne.g., \"'true'\" (or None); .value - the most recent parsed value of this flag;\nset by parse(); .help - a help string or None if no help is available;\n.short_name - the single letter alias for this flag (or None); .boolean - if\n'true', this flag does not accept arguments; .present - true if this flag was\nparsed from command line flags; .parser - an ArgumentParser object;\n.serializer - an ArgumentSerializer object; .allow_override - the flag may be\nredefined without raising an error, and newly defined flag overrides the old\none. .allow_override_cpp - use the flag from C++ if available; the flag\ndefinition is replaced by the C++ flag after init; .allow_hide_cpp - use the\nPython flag despite having a C++ flag with the same name (ignore the C++\nflag); .using_default_value - the flag value has not been set by user;\n.allow_overwrite - the flag may be parsed more than once without raising an\nerror, the last set value will be used; .allow_using_method_names - whether\nthis flag can be defined even if it has a name that conflicts with a\nFlagValues method.\n\nThe only public method of a 'Flag' object is parse(), but it is typically only\ncalled by a 'FlagValues' object. The parse() method is a thin wrapper around\nthe 'ArgumentParser' parse() method. The parsed value is saved in .value, and\nthe .present attribute is updated. If this flag was already present, an Error\nis raised.\n\nparse() is also called during init to parse the default value and initialize\nthe .value attribute. This enables other python modules to safely use flags\neven if the main module neglects to parse the command line arguments. The\n.present attribute is cleared after init parsing. If the default value is set\nto None, then the init parsing step is skipped and the .value attribute is\ninitialized to None.\n\nReturns a str that describes the type of the flag.\n\nParses string and sets flag value.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.FlagHolder", "path": "compat/v1/flags/flagholder", "type": "tf.compat", "text": "\nHolds a defined flag.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.FlagHolder`\n\nThis facilitates a cleaner api around global state. Instead of\n\nit encourages code like\n\nsince the name of the flag appears only once in the source code.\n\nIf _ensure_non_none_value is True, then return value is not None.\n\nReturn self==value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.FlagNameConflictsWithMethodError", "path": "compat/v1/flags/flagnameconflictswithmethoderror", "type": "tf.compat", "text": "\nRaised when a flag name conflicts with FlagValues methods.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.FlagNameConflictsWithMethodError`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.FLAGS", "path": "compat/v1/flags/flags", "type": "tf.compat", "text": "\nRegistry of 'Flag' objects.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.FLAGS`\n\nA 'FlagValues' can then scan command line arguments, passing flag arguments\nthrough to the 'Flag' objects that it owns. It also provides easy access to\nthe flag values. Typically only one 'FlagValues' object is needed by an\napplication: flags.FLAGS\n\nThis class is heavily overloaded:\n\n'Flag' objects are registered via setitem: FLAGS['longname'] = x # register a\nnew flag\n\nThe .value attribute of the registered 'Flag' objects can be accessed as\nattributes of this 'FlagValues' object, through getattr. Both the long and\nshort name of the original 'Flag' objects can be used to access its value:\nFLAGS.longname # parsed flag value FLAGS.x # parsed flag value (short name)\n\nCommand line arguments are scanned and passed to the registered 'Flag' objects\nthrough the call method. Unparsed arguments, including argv0 are returned.\nargv = FLAGS(sys.argv) # scan command line arguments\n\nThe original registered Flag objects can be retrieved through the use of the\ndictionary-like operator, getitem: x = FLAGS['longname'] # access the\nregistered Flag object\n\nThe str() operator of a 'FlagValues' object provides help for all of the\nregistered 'Flag' objects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.FlagValues", "path": "compat/v1/flags/flagvalues", "type": "tf.compat", "text": "\nRegistry of 'Flag' objects.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.FlagValues`\n\nA 'FlagValues' can then scan command line arguments, passing flag arguments\nthrough to the 'Flag' objects that it owns. It also provides easy access to\nthe flag values. Typically only one 'FlagValues' object is needed by an\napplication: flags.FLAGS\n\nThis class is heavily overloaded:\n\n'Flag' objects are registered via setitem: FLAGS['longname'] = x # register a\nnew flag\n\nThe .value attribute of the registered 'Flag' objects can be accessed as\nattributes of this 'FlagValues' object, through getattr. Both the long and\nshort name of the original 'Flag' objects can be used to access its value:\nFLAGS.longname # parsed flag value FLAGS.x # parsed flag value (short name)\n\nCommand line arguments are scanned and passed to the registered 'Flag' objects\nthrough the call method. Unparsed arguments, including argv0 are returned.\nargv = FLAGS(sys.argv) # scan command line arguments\n\nThe original registered Flag objects can be retrieved through the use of the\ndictionary-like operator, getitem: x = FLAGS['longname'] # access the\nregistered Flag object\n\nThe str() operator of a 'FlagValues' object provides help for all of the\nregistered 'Flag' objects.\n\nAppends flags registered in another FlagValues instance.\n\nAppends all flags assignments from this FlagInfo object to a file.\n\nOutput will be in the format of a flagfile.\n\nReturn the name of the module defining this flag, or default.\n\nReturn the ID of the module defining this flag, or default.\n\nReturns a dictionary that maps flag names to flag values.\n\nReturns the dictionary of module_name -> list of defined flags.\n\nReturns the dictionary of module_id -> list of defined flags.\n\nReturns a string with the flags assignments from this FlagValues object.\n\nThis function ignores flags whose value is None. Each flag assignment is\nseparated by a newline.\n\nReturns the value of a flag (if not None) or a default value.\n\nReturns the list of flags defined by a module.\n\nReturns a help string for all known flags.\n\nReturns the list of key flags for a module.\n\nReturns whether flags were parsed.\n\nReturns the dictionary of module_name -> list of key flags.\n\nDescribes the key flags of the main module.\n\nExplicitly marks flags as parsed.\n\nUse this when the caller knows that this FlagValues has been parsed as if a\ncall() invocation has happened. This is only a public method for use by things\nlike appcommands which do additional command like parsing.\n\nDescribes the key flags of a module.\n\nProcesses command line args, but also allow args to be read from file.\n\nThis function is called by FLAGS(argv). It scans the input list for a flag\nthat looks like: --flagfile=. Then it opens , reads all valid key and value\npairs and inserts them into the input list in exactly the place where the\n--flagfile arg is found.\n\nNote that your application's flags are still defined the usual way using\nabsl.flags DEFINE_flag() type functions.\n\nNotes (assuming we're getting a commandline of some sort as our input): -->\nFor duplicate flags, the last one we hit should \"win\". --> Since flags that\nappear later win, a flagfile's settings can be \"weak\" if the --flagfile comes\nat the beginning of the argument sequence, and it can be \"strong\" if the\n--flagfile comes at the end. --> A further \"--flagfile=\" CAN be nested in a\nflagfile. It will be expanded in exactly the spot where it is found. --> In a\nflagfile, a line beginning with # or // is a comment. --> Entirely blank lines\nshould be ignored.\n\nRecords the module that defines a specific flag.\n\nWe keep track of which flag is defined by which module so that we can later\nsort the flags by module.\n\nRecords the module that defines a specific flag.\n\nSpecifies that a flag is a key flag for a module.\n\nRemove flags that were previously appended from another FlagValues.\n\nChanges the default value of the named flag object.\n\nThe flag's current value is also updated if the flag is currently using the\ndefault value, i.e. not specified in the command line, and not set by\nFLAGS.name = value.\n\nSets whether or not to use GNU style scanning.\n\nGNU style allows mixing of flag and non-flag arguments. See\nhttp://docs.python.org/library/getopt.html#getopt.gnu_getopt\n\nUnparses all flags to the point before any FLAGS(argv) was called.\n\nVerifies whether all flags pass validation.\n\nOutputs flag documentation in XML format.\n\nParses flags from argv; stores parsed flags into this FlagValues object.\n\nAll unparsed arguments are returned.\n\nReturns True if name is a value (flag) in the dict.\n\nReturns the Flag object for the flag --name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.flag_dict_to_args", "path": "compat/v1/flags/flag_dict_to_args", "type": "tf.compat", "text": "\nConvert a dict of values into process call parameters.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.flag_dict_to_args`\n\nThis method is used to convert a dictionary into a sequence of parameters for\na binary that parses arguments using this module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.FloatParser", "path": "compat/v1/flags/floatparser", "type": "tf.compat", "text": "\nParser of floating point values.\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.FloatParser`\n\nParsed value may be bounded to a given upper and lower bound.\n\nReturns the float value of argument.\n\nSee base class.\n\nReturns whether the value is outside the bounds or not.\n\nSee base class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.get_help_width", "path": "compat/v1/flags/get_help_width", "type": "tf.compat", "text": "\nReturns the integer width of help lines that is used in TextWrap.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.get_help_width`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.IllegalFlagValueError", "path": "compat/v1/flags/illegalflagvalueerror", "type": "tf.compat", "text": "\nRaised when the flag command line argument is illegal.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.IllegalFlagValueError`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.IntegerParser", "path": "compat/v1/flags/integerparser", "type": "tf.compat", "text": "\nParser of an integer value.\n\nInherits From: `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.IntegerParser`\n\nParsed value may be bounded to a given upper and lower bound.\n\nReturns the int value of argument.\n\nSee base class.\n\nReturns whether the value is outside the bounds or not.\n\nSee base class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.ListParser", "path": "compat/v1/flags/listparser", "type": "tf.compat", "text": "\nParser for a comma-separated list of strings.\n\nInherits From: `BaseListParser`, `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.ListParser`\n\nSee base class.\n\nParses argument as comma-separated list of strings.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.ListSerializer", "path": "compat/v1/flags/listserializer", "type": "tf.compat", "text": "\nBase class for generating string representations of a flag value.\n\nInherits From: `ArgumentSerializer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.ListSerializer`\n\nSee base class.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.mark_bool_flags_as_mutual_exclusive", "path": "compat/v1/flags/mark_bool_flags_as_mutual_exclusive", "type": "tf.compat", "text": "\nEnsures that only one flag among flag_names is True.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.mark_bool_flags_as_mutual_exclusive`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.mark_flags_as_mutual_exclusive", "path": "compat/v1/flags/mark_flags_as_mutual_exclusive", "type": "tf.compat", "text": "\nEnsures that only one flag among flag_names is not None.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.mark_flags_as_mutual_exclusive`\n\nImportant note: This validator checks if flag values are None, and it does not\ndistinguish between default and explicit values. Therefore, this validator\ndoes not make sense when applied to flags with default values other than None,\nincluding other false values (e.g. False, 0, '', []). That includes multi\nflags with a default value of [] instead of None.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.mark_flags_as_required", "path": "compat/v1/flags/mark_flags_as_required", "type": "tf.compat", "text": "\nEnsures that flags are not None during program execution.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.mark_flags_as_required`\n\nIf your module might be imported by others, and you only wish to make the flag\nrequired when the module is directly executed, call this method like this:\n\nif name == 'main': flags.mark_flags_as_required(['flag1', 'flag2', 'flag3'])\napp.run()\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.mark_flag_as_required", "path": "compat/v1/flags/mark_flag_as_required", "type": "tf.compat", "text": "\nEnsures that flag is not None during program execution.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.mark_flag_as_required`\n\nRegisters a flag validator, which will follow usual validator rules. Important\nnote: validator will pass for any non-None value, such as False, 0 (zero), ''\n(empty string) and so on.\n\nIf your module might be imported by others, and you only wish to make the flag\nrequired when the module is directly executed, call this method like this:\n\nif name == 'main': flags.mark_flag_as_required('your_flag_name') app.run()\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.MultiEnumClassFlag", "path": "compat/v1/flags/multienumclassflag", "type": "tf.compat", "text": "\nA multi_enum_class flag.\n\nInherits From: `MultiFlag`, `Flag`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.MultiEnumClassFlag`\n\nSee the doc for MultiFlag for most behaviors of this class. In addition, this\nclass knows how to handle enum.Enum instances as values for this flag type.\n\nSee base class.\n\nParses one or more arguments with the installed parser.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.MultiFlag", "path": "compat/v1/flags/multiflag", "type": "tf.compat", "text": "\nA flag that can appear multiple time on the command-line.\n\nInherits From: `Flag`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.MultiFlag`\n\nThe value of such a flag is a list that contains the individual values from\nall the appearances of that flag on the command-line.\n\nSee the doc for Flag for most behavior of this class. Only differences in\nbehavior are described here:\n\nThe default value may be either a single value or an iterable of values. A\nsingle value is transformed into a single-item list of that value.\n\nThe value of the flag is always a list, even if the option was only supplied\nonce, and even if the default value is a single value\n\nSee base class.\n\nParses one or more arguments with the installed parser.\n\nSerializes the flag.\n\nReturn self==value.\n\nReturn a >= b. Computed by @total_ordering from (not a < b).\n\nReturn a > b. Computed by @total_ordering from (not a < b) and (a != b).\n\nReturn a <= b. Computed by @total_ordering from (a < b) or (a == b).\n\nReturn self<value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.multi_flags_validator", "path": "compat/v1/flags/multi_flags_validator", "type": "tf.compat", "text": "\nA function decorator for defining a multi-flag validator.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.multi_flags_validator`\n\nRegisters the decorated function as a validator for flag_names, e.g.\n\n@flags.multi_flags_validator(['foo', 'bar']) def _CheckFooBar(flags_dict): ...\n\nSee register_multi_flags_validator() for the specification of checker\nfunction.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.register_multi_flags_validator", "path": "compat/v1/flags/register_multi_flags_validator", "type": "tf.compat", "text": "\nAdds a constraint to multiple flags.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.register_multi_flags_validator`\n\nThe constraint is validated when flags are initially parsed, and after each\nchange of the corresponding flag's value.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.register_validator", "path": "compat/v1/flags/register_validator", "type": "tf.compat", "text": "\nAdds a constraint, which will be enforced during program execution.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.register_validator`\n\nThe constraint is validated when flags are initially parsed, and after each\nchange of the corresponding flag's value. Args: flag_name: str, name of the\nflag to be checked. checker: callable, a function to validate the flag. input\n- A single positional argument: The value of the corresponding flag (string,\nboolean, etc. This value will be passed to checker by the library). output -\nbool, True if validator constraint is satisfied. If constraint is not\nsatisfied, it should either return False or raise\nflags.ValidationError(desired_error_message). message: str, error text to be\nshown to the user if checker returns False. If checker raises\nflags.ValidationError, message from the raised error will be shown.\nflag_values: flags.FlagValues, optional FlagValues instance to validate\nagainst. Raises: AttributeError: Raised when flag_name is not registered as a\nvalid flag name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.text_wrap", "path": "compat/v1/flags/text_wrap", "type": "tf.compat", "text": "\nWraps a given text to a maximum line length and returns it.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.text_wrap`\n\nIt turns lines that only contain whitespace into empty lines, keeps new lines,\nand expands tabs using 4 spaces.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator", "path": "compat/v1/flags/tf_decorator", "type": "tf.compat", "text": "\nBase TFDecorator class and utility functions for working with decorators.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator`\n\nThere are two ways to create decorators that TensorFlow can introspect into.\nThis is important for documentation generation purposes, so that function\nsignatures aren't obscured by the (*args, **kwds) signature that decorators\noften provide.\n\ndef print_hello_before_calling(target): def wrapper(*args, *kwargs):\nprint('hello') return target(args, **kwargs) return\ntf_decorator.make_decorator(target, wrapper)\n\nclass CallCounter(tf_decorator.TFDecorator): def init(self, target):\nsuper(CallCounter, self).init('count_calls', target) self.call_count = 0\n\ndef call(self, *args, *kwargs): self.call_count += 1 return super(CallCounter,\nself).decorated_target(args, **kwargs)\n\ndef count_calls(target): return CallCounter(target)\n\n`tf_stack` module: Functions used to extract and analyze stacks. Faster than\nPython libs.\n\n`class TFDecorator`: Base class for all TensorFlow decorators.\n\n`make_decorator(...)`: Make a decorator from a wrapper and a target.\n\n`rewrap(...)`: Injects a new target into a function built by make_decorator.\n\n`unwrap(...)`: Unwraps an object into a list of TFDecorators and a final\ntarget.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.make_decorator", "path": "compat/v1/flags/tf_decorator/make_decorator", "type": "tf.compat", "text": "\nMake a decorator from a wrapper and a target.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.make_decorator`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.rewrap", "path": "compat/v1/flags/tf_decorator/rewrap", "type": "tf.compat", "text": "\nInjects a new target into a function built by make_decorator.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.rewrap`\n\nThis function allows replacing a function wrapped by `decorator_func`,\nassuming the decorator that wraps the function is written as described below.\n\nThe decorator function must use `<decorator name>.__wrapped__` instead of the\nwrapped function that is normally used:\n\ndef simple_parametrized_wrapper(*args, *kwds): return wrapped_fn(args, **kwds)\n\ntf_decorator.make_decorator(simple_parametrized_wrapper, wrapped_fn)\n\ndef simple_parametrized_wrapper(*args, *kwds): return\nsimple_parametrizedwrapper.wrapped_(args, **kwds)\n\ntf_decorator.make_decorator(simple_parametrized_wrapper, wrapped_fn)\n\nNote that this process modifies decorator_func.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.TFDecorator", "path": "compat/v1/flags/tf_decorator/tfdecorator", "type": "tf.compat", "text": "\nBase class for all TensorFlow decorators.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.TFDecorator`\n\nTFDecorator captures and exposes the wrapped target, and provides details\nabout the current decorator.\n\nView source\n\nCall self as a function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack", "path": "compat/v1/flags/tf_decorator/tf_stack", "type": "tf.compat", "text": "\nFunctions used to extract and analyze stacks. Faster than Python libs.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack`\n\n`class CurrentModuleFilter`: Filters stack frames from the module where this\nis used (best effort).\n\n`class FrameSummary`\n\n`class StackSummary`\n\n`class StackTraceFilter`: Allows filtering traceback information by removing\nsuperfluous frames.\n\n`class StackTraceMapper`: Allows remapping traceback information to different\nsource code.\n\n`class StackTraceTransform`: Base class for stack trace transformation\nfunctions.\n\n`extract_stack(...)`: A lightweight, extensible re-implementation of\ntraceback.extract_stack.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.CurrentModuleFilter", "path": "compat/v1/flags/tf_decorator/tf_stack/currentmodulefilter", "type": "tf.compat", "text": "\nFilters stack frames from the module where this is used (best effort).\n\nInherits From: `StackTraceFilter`, `StackTraceTransform`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.CurrentModuleFilter`\n\nView source\n\nView source\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.extract_stack", "path": "compat/v1/flags/tf_decorator/tf_stack/extract_stack", "type": "tf.compat", "text": "\nA lightweight, extensible re-implementation of traceback.extract_stack.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.extract_stack`\n\nNOTE(mrry): traceback.extract_stack eagerly retrieves the line of code for\neach stack frame using linecache, which results in an abundance of stat()\ncalls. This implementation does not retrieve the code, and any consumer should\napply _convert_stack to the result to obtain a traceback that can be formatted\netc. using traceback methods.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.FrameSummary", "path": "compat/v1/flags/tf_decorator/tf_stack/framesummary", "type": "tf.compat", "text": "\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.FrameSummary`\n\neq(self: tensorflow.python._tf_stack.FrameSummary, arg0:\ntensorflow.python._tf_stack.FrameSummary) -> bool\n\ngetitem(self: tensorflow.python._tf_stack.FrameSummary, arg0: object) ->\nobject\n\niter(self: tensorflow.python._tf_stack.FrameSummary) -> iterator\n\nlen(self: tensorflow.python._tf_stack.FrameSummary) -> int\n\nne(self: tensorflow.python._tf_stack.FrameSummary, arg0:\ntensorflow.python._tf_stack.FrameSummary) -> bool\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackSummary", "path": "compat/v1/flags/tf_decorator/tf_stack/stacksummary", "type": "tf.compat", "text": "\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.StackSummary`\n\nappend(self: tensorflow.python._tf_stack.StackSummary, x:\ntensorflow.python._tf_stack.FrameSummary) -> None\n\nAdd an item to the end of the list\n\ncount(self: tensorflow.python._tf_stack.StackSummary, x:\ntensorflow.python._tf_stack.FrameSummary) -> int\n\nReturn the number of times `x` appears in the list\n\nextend(*args, **kwargs) Overloaded function.\n\nExtend the list by appending all the items in the given list\n\nExtend the list by appending all the items in the given list\n\ninsert(self: tensorflow.python._tf_stack.StackSummary, i: int, x:\ntensorflow.python._tf_stack.FrameSummary) -> None\n\nInsert an item at a given position.\n\npop(*args, **kwargs) Overloaded function.\n\nRemove and return the last item\n\nRemove and return the item at index `i`\n\nremove(self: tensorflow.python._tf_stack.StackSummary, x:\ntensorflow.python._tf_stack.FrameSummary) -> None\n\nRemove the first item from the list whose value is x. It is an error if there\nis no such item.\n\nbool(self: tensorflow.python._tf_stack.StackSummary) -> bool\n\nCheck whether the list is nonempty\n\ncontains(self: tensorflow.python._tf_stack.StackSummary, x:\ntensorflow.python._tf_stack.FrameSummary) -> bool\n\nReturn true the container contains `x`\n\neq(self: tensorflow.python._tf_stack.StackSummary, arg0:\ntensorflow.python._tf_stack.StackSummary) -> bool\n\ngetitem(*args, **kwargs) Overloaded function.\n\nRetrieve list elements using a slice object\n\ngetitem(self: tensorflow.python._tf_stack.StackSummary, arg0: int) ->\ntensorflow.python._tf_stack.FrameSummary\n\ngetitem(self: tensorflow.python._tf_stack.StackSummary, arg0: int) ->\ntensorflow.python._tf_stack.FrameSummary\n\niter(self: tensorflow.python._tf_stack.StackSummary) -> iterator\n\nlen(self: tensorflow.python._tf_stack.StackSummary) -> int\n\nne(self: tensorflow.python._tf_stack.StackSummary, arg0:\ntensorflow.python._tf_stack.StackSummary) -> bool\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceFilter", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracefilter", "type": "tf.compat", "text": "\nAllows filtering traceback information by removing superfluous frames.\n\nInherits From: `StackTraceTransform`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceFilter`\n\nView source\n\nView source\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceMapper", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracemapper", "type": "tf.compat", "text": "\nAllows remapping traceback information to different source code.\n\nInherits From: `StackTraceTransform`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceMapper`\n\nView source\n\nReturns a map (filename, lineno) -> (filename, lineno, function_name).\n\nView source\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.tf_stack.StackTraceTransform", "path": "compat/v1/flags/tf_decorator/tf_stack/stacktracetransform", "type": "tf.compat", "text": "\nBase class for stack trace transformation functions.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.tf_stack.StackTraceTransform`\n\nView source\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.tf_decorator.unwrap", "path": "compat/v1/flags/tf_decorator/unwrap", "type": "tf.compat", "text": "\nUnwraps an object into a list of TFDecorators and a final target.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.tf_decorator.unwrap`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.UnparsedFlagAccessError", "path": "compat/v1/flags/unparsedflagaccesserror", "type": "tf.compat", "text": "\nRaised when accessing the flag value from unparsed FlagValues.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.UnparsedFlagAccessError`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.UnrecognizedFlagError", "path": "compat/v1/flags/unrecognizedflagerror", "type": "tf.compat", "text": "\nRaised when a flag is unrecognized.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.UnrecognizedFlagError`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.ValidationError", "path": "compat/v1/flags/validationerror", "type": "tf.compat", "text": "\nRaised when flag validator constraint is not satisfied.\n\nInherits From: `Error`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.ValidationError`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.validator", "path": "compat/v1/flags/validator", "type": "tf.compat", "text": "\nA function decorator for defining a flag validator.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.validator`\n\nRegisters the decorated function as a validator for flag_name, e.g.\n\n@flags.validator('foo') def _CheckFoo(foo): ...\n\nSee register_validator() for the specification of checker function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.flags.WhitespaceSeparatedListParser", "path": "compat/v1/flags/whitespaceseparatedlistparser", "type": "tf.compat", "text": "\nParser for a whitespace-separated list of strings.\n\nInherits From: `BaseListParser`, `ArgumentParser`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.app.flags.WhitespaceSeparatedListParser`\n\nSee base class.\n\nParses argument as whitespace-separated list of strings.\n\nIt also parses argument as comma-separated list of strings if requested.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.floor_div", "path": "compat/v1/floor_div", "type": "tf.compat", "text": "\nReturns x // y element-wise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.foldl", "path": "compat/v1/foldl", "type": "tf.compat", "text": "\nfoldl on the list of tensors unpacked from `elems` on dimension 0.\n\nThis foldl operator repeatedly applies the callable `fn` to a sequence of\nelements from first to last. The elements are made of the tensors unpacked\nfrom `elems` on dimension 0. The callable fn takes two tensors as arguments.\nThe first argument is the accumulated value computed from the preceding\ninvocation of fn, and the second is the value at the current position of\n`elems`. If `initializer` is None, `elems` must contain at least one element,\nand its first element is used as the initializer.\n\nSuppose that `elems` is unpacked into `values`, a list of tensors. The shape\nof the result tensor is fn(initializer, values[0]).shape`.\n\nThis method also allows multi-arity `elems` and output of `fn`. If `elems` is\na (possibly nested) list or tuple of tensors, then each of these tensors must\nhave a matching first (unpack) dimension. The signature of `fn` may match the\nstructure of `elems`. That is, if `elems` is `(t1, [t2, t3, [t4, t5]])`, then\nan appropriate signature for `fn` is: `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.foldr", "path": "compat/v1/foldr", "type": "tf.compat", "text": "\nfoldr on the list of tensors unpacked from `elems` on dimension 0.\n\nThis foldr operator repeatedly applies the callable `fn` to a sequence of\nelements from last to first. The elements are made of the tensors unpacked\nfrom `elems`. The callable fn takes two tensors as arguments. The first\nargument is the accumulated value computed from the preceding invocation of\nfn, and the second is the value at the current position of `elems`. If\n`initializer` is None, `elems` must contain at least one element, and its\nfirst element is used as the initializer.\n\nSuppose that `elems` is unpacked into `values`, a list of tensors. The shape\nof the result tensor is `fn(initializer, values[0]).shape`.\n\nThis method also allows multi-arity `elems` and output of `fn`. If `elems` is\na (possibly nested) list or tuple of tensors, then each of these tensors must\nhave a matching first (unpack) dimension. The signature of `fn` may match the\nstructure of `elems`. That is, if `elems` is `(t1, [t2, t3, [t4, t5]])`, then\nan appropriate signature for `fn` is: `fn = lambda (t1, [t2, t3, [t4, t5]]):`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gather", "path": "compat/v1/gather", "type": "tf.compat", "text": "\nGather slices from params axis `axis` according to indices.\n\nGather slices from params axis `axis` according to `indices`. `indices` must\nbe an integer tensor of any dimension (usually 0-D or 1-D).\n\nFor 0-D (scalar) `indices`:\n\nWhere N = `ndims(params)`.\n\nFor 1-D (vector) `indices` with `batch_dims=0`:\n\nIn the general case, produces an output tensor where:\n\nWhere N = `ndims(params)`, M = `ndims(indices)`, and B = `batch_dims`. Note\nthat `params.shape[:batch_dims]` must be identical to\n`indices.shape[:batch_dims]`.\n\nThe shape of the output tensor is:\n\n`output.shape = params.shape[:axis] + indices.shape[batch_dims:] +\nparams.shape[axis + 1:]`.\n\nNote that on CPU, if an out of bound index is found, an error is returned. On\nGPU, if an out of bound index is found, a 0 is stored in the corresponding\noutput value.\n\nSee also `tf.gather_nd`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gather_nd", "path": "compat/v1/gather_nd", "type": "tf.compat", "text": "\nGather slices from `params` into a Tensor with shape specified by `indices`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.manip.gather_nd`\n\n`indices` is an K-dimensional integer tensor, best thought of as a\n(K-1)-dimensional tensor of indices into `params`, where each element defines\na slice of `params`:\n\nWhereas in `tf.gather` `indices` defines slices into the first dimension of\n`params`, in `tf.gather_nd`, `indices` defines slices into the first `N`\ndimensions of `params`, where `N = indices.shape[-1]`.\n\nThe last dimension of `indices` can be at most the rank of `params`:\n\nThe last dimension of `indices` corresponds to elements (if `indices.shape[-1]\n== params.rank`) or slices (if `indices.shape[-1] < params.rank`) along\ndimension `indices.shape[-1]` of `params`. The output tensor has shape\n\nAdditionally both 'params' and 'indices' can have M leading batch dimensions\nthat exactly match. In this case 'batch_dims' must be M.\n\nNote that on CPU, if an out of bound index is found, an error is returned. On\nGPU, if an out of bound index is found, a 0 is stored in the corresponding\noutput value.\n\nSome examples below.\n\nSimple indexing into a matrix:\n\nSlice indexing into a matrix:\n\nIndexing into a 3-tensor:\n\nThe examples below are for the case when only indices have leading extra\ndimensions. If both 'params' and 'indices' have leading batch dimensions, use\nthe 'batch_dims' parameter to run gather_nd in batch mode.\n\nBatched indexing into a matrix:\n\nBatched slice indexing into a matrix:\n\nBatched indexing into a 3-tensor:\n\nExamples with batched 'params' and 'indices':\n\nSee also `tf.gather`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_collection", "path": "compat/v1/get_collection", "type": "tf.compat", "text": "\nWrapper for `Graph.get_collection()` using the default graph.\n\nSee `tf.Graph.get_collection` for more details.\n\nCollections are not supported when eager execution is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_collection_ref", "path": "compat/v1/get_collection_ref", "type": "tf.compat", "text": "\nWrapper for `Graph.get_collection_ref()` using the default graph.\n\nSee `tf.Graph.get_collection_ref` for more details.\n\nCollections are not supported when eager execution is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_default_graph", "path": "compat/v1/get_default_graph", "type": "tf.compat", "text": "\nReturns the default graph for the current thread.\n\nThe returned graph will be the innermost graph on which a `Graph.as_default()`\ncontext has been entered, or a global default graph if none has been\nexplicitly created.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_default_session", "path": "compat/v1/get_default_session", "type": "tf.compat", "text": "\nReturns the default session for the current thread.\n\nThe returned `Session` will be the innermost session on which a `Session` or\n`Session.as_default()` context has been entered.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_local_variable", "path": "compat/v1/get_local_variable", "type": "tf.compat", "text": "\nGets an existing local variable or creates a new one.\n\nBehavior is the same as in `get_variable`, except that variables are added to\nthe `LOCAL_VARIABLES` collection and `trainable` is set to `False`. This\nfunction prefixes the name with the current variable scope and performs reuse\nchecks. See the Variable Scope How To for an extensive description of how\nreusing works. Here is a basic example:\n\nIf initializer is `None` (the default), the default initializer passed in the\nvariable scope will be used. If that one is `None` too, a\n`glorot_uniform_initializer` will be used. The initializer can also be a\nTensor, in which case the variable is initialized to this value and shape.\n\nSimilarly, if the regularizer is `None` (the default), the default regularizer\npassed in the variable scope will be used (if that is `None` too, then by\ndefault no regularization is performed).\n\nIf a partitioner is provided, a `PartitionedVariable` is returned. Accessing\nthis object as a `Tensor` returns the shards concatenated along the partition\naxis.\n\nSome useful partitioners are available. See, e.g.,\n`variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_seed", "path": "compat/v1/get_seed", "type": "tf.compat", "text": "\nReturns the local seeds an operation should use given an op-specific seed.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.random.get_seed`\n\nGiven operation-specific seed, `op_seed`, this helper function returns two\nseeds derived from graph-level and op-level seeds. Many random operations\ninternally use the two seeds to allow user to change the seed globally for a\ngraph, or for only specific operations.\n\nFor details on how the graph-level seed interacts with op seeds, see\n`tf.compat.v1.random.set_random_seed`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_session_handle", "path": "compat/v1/get_session_handle", "type": "tf.compat", "text": "\nReturn the handle of `data`.\n\nThis is EXPERIMENTAL and subject to change.\n\nKeep `data` \"in-place\" in the runtime and create a handle that can be used to\nretrieve `data` in a subsequent run().\n\nCombined with `get_session_tensor`, we can keep a tensor produced in one run\ncall in place, and use it as the input in a future run call.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_session_tensor", "path": "compat/v1/get_session_tensor", "type": "tf.compat", "text": "\nGet the tensor of type `dtype` by feeding a tensor handle.\n\nThis is EXPERIMENTAL and subject to change.\n\nGet the value of the tensor from a tensor handle. The tensor is produced in a\nprevious run() and stored in the state of the session.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_variable", "path": "compat/v1/get_variable", "type": "tf.compat", "text": "\nGets an existing variable with these parameters or create a new one.\n\nThis function prefixes the name with the current variable scope and performs\nreuse checks. See the Variable Scope How To for an extensive description of\nhow reusing works. Here is a basic example:\n\nIf initializer is `None` (the default), the default initializer passed in the\nvariable scope will be used. If that one is `None` too, a\n`glorot_uniform_initializer` will be used. The initializer can also be a\nTensor, in which case the variable is initialized to this value and shape.\n\nSimilarly, if the regularizer is `None` (the default), the default regularizer\npassed in the variable scope will be used (if that is `None` too, then by\ndefault no regularization is performed).\n\nIf a partitioner is provided, a `PartitionedVariable` is returned. Accessing\nthis object as a `Tensor` returns the shards concatenated along the partition\naxis.\n\nSome useful partitioners are available. See, e.g.,\n`variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.get_variable_scope", "path": "compat/v1/get_variable_scope", "type": "tf.compat", "text": "\nReturns the current variable scope.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile", "path": "compat/v1/gfile", "type": "tf.compat", "text": "\nImport router for file_io.\n\n`class FastGFile`: File I/O wrappers without thread locking.\n\n`class GFile`: File I/O wrappers without thread locking.\n\n`class Open`: File I/O wrappers without thread locking.\n\n`Copy(...)`: Copies data from `oldpath` to `newpath`.\n\n`DeleteRecursively(...)`: Deletes everything under dirname recursively.\n\n`Exists(...)`: Determines whether a path exists or not.\n\n`Glob(...)`: Returns a list of files that match the given pattern(s).\n\n`IsDirectory(...)`: Returns whether the path is a directory or not.\n\n`ListDirectory(...)`: Returns a list of entries contained within a directory.\n\n`MakeDirs(...)`: Creates a directory and all parent/intermediate directories.\n\n`MkDir(...)`: Creates a directory with the name `dirname`.\n\n`Remove(...)`: Deletes the file located at 'filename'.\n\n`Rename(...)`: Rename or move a file / directory.\n\n`Stat(...)`: Returns file statistics for a given path.\n\n`Walk(...)`: Recursive directory tree generator for directories.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Copy", "path": "compat/v1/gfile/copy", "type": "tf.compat", "text": "\nCopies data from `oldpath` to `newpath`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.DeleteRecursively", "path": "compat/v1/gfile/deleterecursively", "type": "tf.compat", "text": "\nDeletes everything under dirname recursively.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Exists", "path": "compat/v1/gfile/exists", "type": "tf.compat", "text": "\nDetermines whether a path exists or not.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.FastGFile", "path": "compat/v1/gfile/fastgfile", "type": "tf.compat", "text": "\nFile I/O wrappers without thread locking.\n\nNote, that this is somewhat like builtin Python file I/O, but there are\nsemantic differences to make it more efficient for some backing filesystems.\nFor example, a write mode file will not be opened until the first write call\n(to minimize RPC invocations in network filesystems).\n\nView source\n\nCloses FileIO. Should be called for the WritableFile to be flushed.\n\nView source\n\nFlushes the Writable file.\n\nThis only ensures that the data has made its way out of the process without\nany guarantees on whether it's written to disk. This means that the data would\nsurvive an application crash but not necessarily an OS crash.\n\nView source\n\nView source\n\nReturns the contents of a file as a string.\n\nStarts reading from current position in file.\n\nView source\n\nReads the next line, keeping \\n. At EOF, returns ''.\n\nView source\n\nReturns all lines from the file in a list.\n\nView source\n\nSeeks to the offset in the file. (deprecated arguments)\n\nView source\n\nReturns True as FileIO supports random access ops of seek()/tell()\n\nView source\n\nReturns the size of the file.\n\nView source\n\nReturns the current position in the file.\n\nView source\n\nWrites file_content to the file. Appends to the end of the file.\n\nView source\n\nMake usable with \"with\" statement.\n\nView source\n\nMake usable with \"with\" statement.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Glob", "path": "compat/v1/gfile/glob", "type": "tf.compat", "text": "\nReturns a list of files that match the given pattern(s).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.IsDirectory", "path": "compat/v1/gfile/isdirectory", "type": "tf.compat", "text": "\nReturns whether the path is a directory or not.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.ListDirectory", "path": "compat/v1/gfile/listdirectory", "type": "tf.compat", "text": "\nReturns a list of entries contained within a directory.\n\nThe list is in arbitrary order. It does not contain the special entries \".\"\nand \"..\".\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.MakeDirs", "path": "compat/v1/gfile/makedirs", "type": "tf.compat", "text": "\nCreates a directory and all parent/intermediate directories.\n\nIt succeeds if dirname already exists and is writable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.MkDir", "path": "compat/v1/gfile/mkdir", "type": "tf.compat", "text": "\nCreates a directory with the name `dirname`.\n\nNotes: The parent directories need to exist. Use `tf.io.gfile.makedirs`\ninstead if there is the possibility that the parent dirs don't exist.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Remove", "path": "compat/v1/gfile/remove", "type": "tf.compat", "text": "\nDeletes the file located at 'filename'.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Rename", "path": "compat/v1/gfile/rename", "type": "tf.compat", "text": "\nRename or move a file / directory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Stat", "path": "compat/v1/gfile/stat", "type": "tf.compat", "text": "\nReturns file statistics for a given path.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gfile.Walk", "path": "compat/v1/gfile/walk", "type": "tf.compat", "text": "\nRecursive directory tree generator for directories.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.global_variables", "path": "compat/v1/global_variables", "type": "tf.compat", "text": "\nReturns global variables.\n\nGlobal variables are variables that are shared across machines in a\ndistributed environment. The `Variable()` constructor or `get_variable()`\nautomatically adds new variables to the graph collection\n`GraphKeys.GLOBAL_VARIABLES`. This convenience function returns the contents\nof that collection.\n\nAn alternative to global variables are local variables. See\n`tf.compat.v1.local_variables`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.global_variables_initializer", "path": "compat/v1/global_variables_initializer", "type": "tf.compat", "text": "\nReturns an Op that initializes global variables.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.global_variables`\n\nThis is just a shortcut for `variables_initializer(global_variables())`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GPUOptions", "path": "compat/v1/gpuoptions", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class Experimental`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GPUOptions.Experimental", "path": "compat/v1/gpuoptions/experimental", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class VirtualDevices`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GPUOptions.Experimental.VirtualDevices", "path": "compat/v1/gpuoptions/experimental/virtualdevices", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.gradients", "path": "compat/v1/gradients", "type": "tf.compat", "text": "\nConstructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n\n`ys` and `xs` are each a `Tensor` or a list of tensors. `grad_ys` is a list of\n`Tensor`, holding the gradients received by the `ys`. The list must be the\nsame length as `ys`.\n\n`gradients()` adds ops to the graph to output the derivatives of `ys` with\nrespect to `xs`. It returns a list of `Tensor` of length `len(xs)` where each\ntensor is the `sum(dy/dx)` for y in `ys` and for x in `xs`.\n\n`grad_ys` is a list of tensors of the same length as `ys` that holds the\ninitial gradients for each y in `ys`. When `grad_ys` is None, we fill in a\ntensor of '1's of the shape of y for each y in `ys`. A user can provide their\nown initial `grad_ys` to compute the derivatives using a different initial\ngradient for each y (e.g., if one wanted to weight the gradient differently\nfor each value in each y).\n\n`stop_gradients` is a `Tensor` or a list of tensors to be considered constant\nwith respect to all `xs`. These tensors will not be backpropagated through, as\nthough they had been explicitly disconnected using `stop_gradient`. Among\nother things, this allows computation of partial derivatives as opposed to\ntotal derivatives. For example:\n\nHere the partial derivatives `g` evaluate to `[1.0, 1.0]`, compared to the\ntotal derivatives `tf.gradients(a + b, [a, b])`, which take into account the\ninfluence of `a` on `b` and evaluate to `[3.0, 1.0]`. Note that the above is\nequivalent to:\n\n`stop_gradients` provides a way of stopping gradient after the graph has\nalready been constructed, as compared to `tf.stop_gradient` which is used\nduring graph construction. When the two approaches are combined,\nbackpropagation stops at both `tf.stop_gradient` nodes and nodes in\n`stop_gradients`, whichever is encountered first.\n\nAll integer tensors are considered constant with respect to all `xs`, as if\nthey were included in `stop_gradients`.\n\n`unconnected_gradients` determines the value returned for each x in xs if it\nis unconnected in the graph to ys. By default this is None to safeguard\nagainst errors. Mathematically these gradients are zero which can be requested\nusing the `'zero'` option. `tf.UnconnectedGradients` provides the following\noptions and behaviors:\n\nLet us take one practical example which comes during the back propogation\nphase. This function is used to evaluate the derivatives of the cost function\nwith respect to Weights `Ws` and Biases `bs`. Below sample implementation\nprovides the exaplantion of what it is actually used for :\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GraphDef", "path": "compat/v1/graphdef", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GraphKeys", "path": "compat/v1/graphkeys", "type": "tf.compat", "text": "\nStandard names to use for graph collections.\n\nThe standard library uses various well-known names to collect and retrieve\nvalues associated with a graph. For example, the `tf.Optimizer` subclasses\ndefault to optimizing the variables collected under\n`tf.GraphKeys.TRAINABLE_VARIABLES` if none is specified, but it is also\npossible to pass an explicit list of variables.\n\nThe following standard keys are defined:\n\nThe following standard keys are defined, but their collections are not\nautomatically populated as many of the others are:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.GraphOptions", "path": "compat/v1/graphoptions", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util", "path": "compat/v1/graph_util", "type": "tf.compat", "text": "\nHelpers to manipulate a tensor graph in python.\n\n`convert_variables_to_constants(...)`: Replaces all the variables in a graph\nwith constants of the same values. (deprecated)\n\n`extract_sub_graph(...)`: Extract the subgraph that can reach any of the nodes\nin 'dest_nodes'. (deprecated)\n\n`import_graph_def(...)`: Imports the graph from `graph_def` into the current\ndefault `Graph`. (deprecated arguments)\n\n`must_run_on_cpu(...)`: Returns True if the given node_def must run on CPU,\notherwise False. (deprecated)\n\n`remove_training_nodes(...)`: Prunes out nodes that aren't needed for\ninference. (deprecated)\n\n`tensor_shape_from_node_def_name(...)`: Convenience function to get a shape\nfrom a NodeDef's input string. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util.convert_variables_to_constants", "path": "compat/v1/graph_util/convert_variables_to_constants", "type": "tf.compat", "text": "\nReplaces all the variables in a graph with constants of the same values.\n(deprecated)\n\nIf you have a trained graph containing Variable ops, it can be convenient to\nconvert them all to Const ops holding the same values. This makes it possible\nto describe the network fully with a single GraphDef file, and allows the\nremoval of a lot of ops related to loading and saving the variables.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util.extract_sub_graph", "path": "compat/v1/graph_util/extract_sub_graph", "type": "tf.compat", "text": "\nExtract the subgraph that can reach any of the nodes in 'dest_nodes'.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util.must_run_on_cpu", "path": "compat/v1/graph_util/must_run_on_cpu", "type": "tf.compat", "text": "\nReturns True if the given node_def must run on CPU, otherwise False.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util.remove_training_nodes", "path": "compat/v1/graph_util/remove_training_nodes", "type": "tf.compat", "text": "\nPrunes out nodes that aren't needed for inference. (deprecated)\n\nThere are nodes like Identity and CheckNumerics that are only useful during\ntraining, and can be removed in graphs that will be used for nothing but\ninference. Here we identify and remove them, returning an equivalent graph. To\nbe specific, CheckNumerics nodes are always removed, and Identity nodes that\naren't involved in control edges are spliced out so that their input and\noutputs are directly connected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.graph_util.tensor_shape_from_node_def_name", "path": "compat/v1/graph_util/tensor_shape_from_node_def_name", "type": "tf.compat", "text": "\nConvenience function to get a shape from a NodeDef's input string.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.hessians", "path": "compat/v1/hessians", "type": "tf.compat", "text": "\nConstructs the Hessian of sum of `ys` with respect to `x` in `xs`.\n\n`hessians()` adds ops to the graph to output the Hessian matrix of `ys` with\nrespect to `xs`. It returns a list of `Tensor` of length `len(xs)` where each\ntensor is the Hessian of `sum(ys)`.\n\nThe Hessian is a matrix of second-order partial derivatives of a scalar tensor\n(see https://en.wikipedia.org/wiki/Hessian_matrix for more details).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.HistogramProto", "path": "compat/v1/histogramproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.IdentityReader", "path": "compat/v1/identityreader", "type": "tf.compat", "text": "\nA Reader that outputs the queued work as both the key and value.\n\nInherits From: `ReaderBase`\n\nTo use, enqueue strings in a Queue. Read will take the front work string and\noutput (work, work).\n\nSee ReaderBase for supported methods.\n\nReaders are not compatible with eager execution. Instead, please use `tf.data`\nto get data into your model.\n\nView source\n\nReturns the number of records this reader has produced.\n\nThis is the same as the number of Read executions that have succeeded.\n\nView source\n\nReturns the number of work units this reader has finished processing.\n\nView source\n\nReturns the next record (key, value) pair produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g. when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile).\n\nView source\n\nReturns up to num_records (key, value) pairs produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g., when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile). It may return less than num_records even before the last batch.\n\nView source\n\nRestore a reader to its initial clean state.\n\nView source\n\nRestore a reader to a previously saved state.\n\nNot all Readers support being restored, so this can produce an Unimplemented\nerror.\n\nView source\n\nProduce a string tensor that encodes the state of a reader.\n\nNot all Readers support being serialized, so this can produce an Unimplemented\nerror.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image", "path": "compat/v1/image", "type": "tf.compat", "text": "\nImage ops.\n\nThe `tf.image` module contains various functions for image processing and\ndecoding-encoding Ops.\n\nMany of the encoding/decoding functions are also available in the core `tf.io`\nmodule.\n\nThe resizing Ops accept input images as tensors of several types. They always\noutput resized images as float32 tensors.\n\nThe convenience function `tf.image.resize` supports both 4-D and 3-D tensors\nas input and output. 4-D tensors are for batches of images, 3-D tensors for\nindividual images.\n\nResized images will be distorted if their original aspect ratio is not the\nsame as size. To avoid distortions see tf.image.resize_with_pad.\n\nThe Class `tf.image.ResizeMethod` provides various resize methods like\n`bilinear`, `nearest_neighbor`.\n\nImage ops work either on individual images or on batches of images, depending\non the shape of their input Tensor.\n\nIf 3-D, the shape is `[height, width, channels]`, and the Tensor represents\none image. If 4-D, the shape is `[batch_size, height, width, channels]`, and\nthe Tensor represents `batch_size` images.\n\nCurrently, `channels` can usefully be 1, 2, 3, or 4. Single-channel images are\ngrayscale, images with 3 channels are encoded as either RGB or HSV. Images\nwith 2 or 4 channels include an alpha channel, which has to be stripped from\nthe image before passing the image to most image processing functions (and can\nbe re-attached later).\n\nInternally, images are either stored in as one `float32` per channel per pixel\n(implicitly, values are assumed to lie in `[0,1)`) or one `uint8` per channel\nper pixel (values are assumed to lie in `[0,255]`).\n\nTensorFlow can convert between images in RGB or HSV or YIQ.\n\nTensorFlow provides functions to adjust images in various ways: brightness,\ncontrast, hue, and saturation. Each adjustment can be done with predefined\nparameters or with random parameters picked from predefined intervals. Random\nadjustments are often useful to expand a training set and reduce overfitting.\n\nIf several adjustments are chained it is advisable to minimize the number of\nredundant conversions by first converting the images to the most natural data\ntype and representation.\n\nTensorFlow provides Ops to decode and encode JPEG and PNG formats. Encoded\nimages are represented by scalar string Tensors, decoded images by 3-D uint8\ntensors of shape `[height, width, channels]`. (PNG also supports uint16.)\n\nThe encode and decode Ops apply to one image at a time. Their input and output\nare all of variable size. If you need fixed size images, pass the output of\nthe decode Ops to one of the cropping and resizing Ops.\n\n`class ResizeMethod`: See `v1.image.resize` for details.\n\n`adjust_brightness(...)`: Adjust the brightness of RGB or Grayscale images.\n\n`adjust_contrast(...)`: Adjust contrast of RGB or grayscale images.\n\n`adjust_gamma(...)`: Performs Gamma Correction.\n\n`adjust_hue(...)`: Adjust hue of RGB images.\n\n`adjust_jpeg_quality(...)`: Adjust jpeg encoding quality of an image.\n\n`adjust_saturation(...)`: Adjust saturation of RGB images.\n\n`central_crop(...)`: Crop the central region of the image(s).\n\n`combined_non_max_suppression(...)`: Greedily selects a subset of bounding\nboxes in descending order of score.\n\n`convert_image_dtype(...)`: Convert `image` to `dtype`, scaling its values if\nneeded.\n\n`crop_and_resize(...)`: Extracts crops from the input image tensor and resizes\nthem.\n\n`crop_to_bounding_box(...)`: Crops an image to a specified bounding box.\n\n`decode_and_crop_jpeg(...)`: Decode and Crop a JPEG-encoded image to a uint8\ntensor.\n\n`decode_bmp(...)`: Decode the first frame of a BMP-encoded image to a uint8\ntensor.\n\n`decode_gif(...)`: Decode the frame(s) of a GIF-encoded image to a uint8\ntensor.\n\n`decode_image(...)`: Function for `decode_bmp`, `decode_gif`, `decode_jpeg`,\nand `decode_png`.\n\n`decode_jpeg(...)`: Decode a JPEG-encoded image to a uint8 tensor.\n\n`decode_png(...)`: Decode a PNG-encoded image to a uint8 or uint16 tensor.\n\n`draw_bounding_boxes(...)`: Draw bounding boxes on a batch of images.\n\n`encode_jpeg(...)`: JPEG-encode an image.\n\n`encode_png(...)`: PNG-encode an image.\n\n`extract_glimpse(...)`: Extracts a glimpse from the input tensor.\n\n`extract_image_patches(...)`: Extract `patches` from `images` and put them in\nthe \"depth\" output dimension.\n\n`extract_jpeg_shape(...)`: Extract the shape information of a JPEG-encoded\nimage.\n\n`extract_patches(...)`: Extract `patches` from `images`.\n\n`flip_left_right(...)`: Flip an image horizontally (left to right).\n\n`flip_up_down(...)`: Flip an image vertically (upside down).\n\n`generate_bounding_box_proposals(...)`: Generate bounding box proposals from\nencoded bounding boxes.\n\n`grayscale_to_rgb(...)`: Converts one or more images from Grayscale to RGB.\n\n`hsv_to_rgb(...)`: Convert one or more images from HSV to RGB.\n\n`image_gradients(...)`: Returns image gradients (dy, dx) for each color\nchannel.\n\n`is_jpeg(...)`: Convenience function to check if the 'contents' encodes a JPEG\nimage.\n\n`non_max_suppression(...)`: Greedily selects a subset of bounding boxes in\ndescending order of score.\n\n`non_max_suppression_overlaps(...)`: Greedily selects a subset of bounding\nboxes in descending order of score.\n\n`non_max_suppression_padded(...)`: Greedily selects a subset of bounding boxes\nin descending order of score.\n\n`non_max_suppression_with_scores(...)`: Greedily selects a subset of bounding\nboxes in descending order of score.\n\n`pad_to_bounding_box(...)`: Pad `image` with zeros to the specified `height`\nand `width`.\n\n`per_image_standardization(...)`: Linearly scales each image in `image` to\nhave mean 0 and variance 1.\n\n`psnr(...)`: Returns the Peak Signal-to-Noise Ratio between a and b.\n\n`random_brightness(...)`: Adjust the brightness of images by a random factor.\n\n`random_contrast(...)`: Adjust the contrast of an image or images by a random\nfactor.\n\n`random_crop(...)`: Randomly crops a tensor to a given size.\n\n`random_flip_left_right(...)`: Randomly flip an image horizontally (left to\nright).\n\n`random_flip_up_down(...)`: Randomly flips an image vertically (upside down).\n\n`random_hue(...)`: Adjust the hue of RGB images by a random factor.\n\n`random_jpeg_quality(...)`: Randomly changes jpeg encoding quality for\ninducing jpeg noise.\n\n`random_saturation(...)`: Adjust the saturation of RGB images by a random\nfactor.\n\n`resize(...)`: Resize `images` to `size` using the specified `method`.\n\n`resize_area(...)`: Resize `images` to `size` using area interpolation.\n\n`resize_bicubic(...)`\n\n`resize_bilinear(...)`\n\n`resize_image_with_crop_or_pad(...)`: Crops and/or pads an image to a target\nwidth and height.\n\n`resize_image_with_pad(...)`: Resizes and pads an image to a target width and\nheight.\n\n`resize_images(...)`: Resize `images` to `size` using the specified `method`.\n\n`resize_nearest_neighbor(...)`\n\n`resize_with_crop_or_pad(...)`: Crops and/or pads an image to a target width\nand height.\n\n`rgb_to_grayscale(...)`: Converts one or more images from RGB to Grayscale.\n\n`rgb_to_hsv(...)`: Converts one or more images from RGB to HSV.\n\n`rgb_to_yiq(...)`: Converts one or more images from RGB to YIQ.\n\n`rgb_to_yuv(...)`: Converts one or more images from RGB to YUV.\n\n`rot90(...)`: Rotate image(s) counter-clockwise by 90 degrees.\n\n`sample_distorted_bounding_box(...)`: Generate a single randomly distorted\nbounding box for an image. (deprecated)\n\n`sobel_edges(...)`: Returns a tensor holding Sobel edge maps.\n\n`ssim(...)`: Computes SSIM index between img1 and img2.\n\n`ssim_multiscale(...)`: Computes the MS-SSIM between img1 and img2.\n\n`total_variation(...)`: Calculate and return the total variation for one or\nmore images.\n\n`transpose(...)`: Transpose image(s) by swapping the height and width\ndimension.\n\n`transpose_image(...)`: Transpose image(s) by swapping the height and width\ndimension.\n\n`yiq_to_rgb(...)`: Converts one or more images from YIQ to RGB.\n\n`yuv_to_rgb(...)`: Converts one or more images from YUV to RGB.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.crop_and_resize", "path": "compat/v1/image/crop_and_resize", "type": "tf.compat", "text": "\nExtracts crops from the input image tensor and resizes them.\n\nExtracts crops from the input image tensor and resizes them using bilinear\nsampling or nearest neighbor sampling (possibly with aspect ratio change) to a\ncommon output size specified by `crop_size`. This is more general than the\n`crop_to_bounding_box` op which extracts a fixed size slice from the input\nimage and does not allow resizing or aspect ratio change.\n\nReturns a tensor with `crops` from the input `image` at positions defined at\nthe bounding box locations in `boxes`. The cropped boxes are all resized (with\nbilinear or nearest neighbor interpolation) to a fixed `size = [crop_height,\ncrop_width]`. The result is a 4-D tensor `[num_boxes, crop_height, crop_width,\ndepth]`. The resizing is corner aligned. In particular, if `boxes = [[0, 0, 1,\n1]]`, the method will give identical results to using\n`tf.image.resize_bilinear()` or `tf.image.resize_nearest_neighbor()`(depends\non the `method` argument) with `align_corners=True`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.draw_bounding_boxes", "path": "compat/v1/image/draw_bounding_boxes", "type": "tf.compat", "text": "\nDraw bounding boxes on a batch of images.\n\nOutputs a copy of `images` but draws on top of the pixels zero or more\nbounding boxes specified by the locations in `boxes`. The coordinates of the\neach bounding box in `boxes` are encoded as `[y_min, x_min, y_max, x_max]`.\nThe bounding box coordinates are floats in `[0.0, 1.0]` relative to the width\nand the height of the underlying image.\n\nFor example, if an image is 100 x 200 pixels (height x width) and the bounding\nbox is `[0.1, 0.2, 0.5, 0.9]`, the upper-left and bottom-right coordinates of\nthe bounding box will be `(40, 10)` to `(180, 50)` (in (x,y) coordinates).\n\nParts of the bounding box may fall outside the image.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.extract_glimpse", "path": "compat/v1/image/extract_glimpse", "type": "tf.compat", "text": "\nExtracts a glimpse from the input tensor.\n\nReturns a set of windows called glimpses extracted at location `offsets` from\nthe input tensor. If the windows only partially overlaps the inputs, the non-\noverlapping areas will be filled with random noise.\n\nThe result is a 4-D tensor of shape `[batch_size, glimpse_height,\nglimpse_width, channels]`. The channels and batch dimensions are the same as\nthat of the input tensor. The height and width of the output windows are\nspecified in the `size` parameter.\n\nThe argument `normalized` and `centered` controls how the windows are built:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize", "path": "compat/v1/image/resize", "type": "tf.compat", "text": "\nResize `images` to `size` using the specified `method`.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.image.resize_images`\n\nResized images will be distorted if their original aspect ratio is not the\nsame as `size`. To avoid distortions see `tf.image.resize_with_pad` or\n`tf.image.resize_with_crop_or_pad`.\n\nThe `method` can be one of:\n\nThe return value has the same type as `images` if `method` is\n`tf.image.ResizeMethod.NEAREST_NEIGHBOR`. It will also have the same type as\n`images` if the size of `images` can be statically determined to be the same\nas `size`, because `images` is returned in this case. Otherwise, the return\nvalue has type `float32`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.ResizeMethod", "path": "compat/v1/image/resizemethod", "type": "tf.compat", "text": "\nSee `v1.image.resize` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize_area", "path": "compat/v1/image/resize_area", "type": "tf.compat", "text": "\nResize `images` to `size` using area interpolation.\n\nInput images can be of different types but output images are always float.\n\nThe range of pixel values for the output image might be slightly different\nfrom the range for the input image because of limited numerical precision. To\nguarantee an output range, for example `[0.0, 1.0]`, apply `tf.clip_by_value`\nto the output.\n\nEach output pixel is computed by first transforming the pixel's footprint into\nthe input tensor and then averaging the pixels that intersect the footprint.\nAn input pixel's contribution to the average is weighted by the fraction of\nits area that intersects the footprint. This is the same as OpenCV's\nINTER_AREA.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize_bicubic", "path": "compat/v1/image/resize_bicubic", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize_bilinear", "path": "compat/v1/image/resize_bilinear", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize_image_with_pad", "path": "compat/v1/image/resize_image_with_pad", "type": "tf.compat", "text": "\nResizes and pads an image to a target width and height.\n\nResizes an image to a target width and height by keeping the aspect ratio the\nsame without distortion. If the target dimensions don't match the image\ndimensions, the image is resized and then padded with zeroes to match\nrequested dimensions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.resize_nearest_neighbor", "path": "compat/v1/image/resize_nearest_neighbor", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.image.sample_distorted_bounding_box", "path": "compat/v1/image/sample_distorted_bounding_box", "type": "tf.compat", "text": "\nGenerate a single randomly distorted bounding box for an image. (deprecated)\n\nBounding box annotations are often supplied in addition to ground-truth labels\nin image recognition or object localization tasks. A common technique for\ntraining such a system is to randomly distort an image while preserving its\ncontent, i.e. data augmentation. This Op outputs a randomly distorted\nlocalization of an object, i.e. bounding box, given an `image_size`,\n`bounding_boxes` and a series of constraints.\n\nThe output of this Op is a single bounding box that may be used to crop the\noriginal image. The output is returned as 3 tensors: `begin`, `size` and\n`bboxes`. The first 2 tensors can be fed directly into `tf.slice` to crop the\nimage. The latter may be supplied to `tf.image.draw_bounding_boxes` to\nvisualize what the bounding box looks like.\n\nBounding boxes are supplied and returned as `[y_min, x_min, y_max, x_max]`.\nThe bounding box coordinates are floats in `[0.0, 1.0]` relative to the width\nand height of the underlying image.\n\nFor example,\n\nNote that if no bounding box information is available, setting\n`use_image_if_no_bounding_boxes = True` will assume there is a single implicit\nbounding box covering the whole image. If `use_image_if_no_bounding_boxes` is\nfalse and no bounding boxes are supplied, an error is raised.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initializers", "path": "compat/v1/initializers", "type": "tf.compat", "text": "\nPublic API for tf.initializers namespace.\n\n`class constant`: Initializer that generates tensors with constant values.\n\n`class glorot_normal`: The Glorot normal initializer, also called Xavier\nnormal initializer.\n\n`class glorot_uniform`: The Glorot uniform initializer, also called Xavier\nuniform initializer.\n\n`class identity`: Initializer that generates the identity matrix.\n\n`class ones`: Initializer that generates tensors initialized to 1.\n\n`class orthogonal`: Initializer that generates an orthogonal matrix.\n\n`class random_normal`: Initializer that generates tensors with a normal\ndistribution.\n\n`class random_uniform`: Initializer that generates tensors with a uniform\ndistribution.\n\n`class truncated_normal`: Initializer that generates a truncated normal\ndistribution.\n\n`class uniform_unit_scaling`: Initializer that generates tensors without\nscaling variance.\n\n`class variance_scaling`: Initializer capable of adapting its scale to the\nshape of weights tensors.\n\n`class zeros`: Initializer that generates tensors initialized to 0.\n\n`global_variables(...)`: Returns an Op that initializes global variables.\n\n`he_normal(...)`: He normal initializer.\n\n`he_uniform(...)`: He uniform variance scaling initializer.\n\n`lecun_normal(...)`: LeCun normal initializer.\n\n`lecun_uniform(...)`: LeCun uniform initializer.\n\n`local_variables(...)`: Returns an Op that initializes all local variables.\n\n`tables_initializer(...)`: Returns an Op that initializes all tables of the\ndefault graph.\n\n`variables(...)`: Returns an Op that initializes a list of variables.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initializers.he_normal", "path": "compat/v1/initializers/he_normal", "type": "tf.compat", "text": "\nHe normal initializer.\n\nIt draws samples from a truncated normal distribution centered on 0 with\nstandard deviation (after truncation) given by `stddev = sqrt(2 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\nHe et al., 2015\n\n(pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initializers.he_uniform", "path": "compat/v1/initializers/he_uniform", "type": "tf.compat", "text": "\nHe uniform variance scaling initializer.\n\nIt draws samples from a uniform distribution within [-limit, limit] where\n`limit` is `sqrt(6 / fan_in)` where `fan_in` is the number of input units in\nthe weight tensor.\n\nHe et al., 2015\n\n(pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initializers.lecun_normal", "path": "compat/v1/initializers/lecun_normal", "type": "tf.compat", "text": "\nLeCun normal initializer.\n\nIt draws samples from a truncated normal distribution centered on 0 with\nstandard deviation (after truncation) given by `stddev = sqrt(1 / fan_in)`\nwhere `fan_in` is the number of input units in the weight tensor.\n\n(pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initializers.lecun_uniform", "path": "compat/v1/initializers/lecun_uniform", "type": "tf.compat", "text": "\nLeCun uniform initializer.\n\nIt draws samples from a uniform distribution within [-limit, limit] where\n`limit` is `sqrt(3 / fan_in)` where `fan_in` is the number of input units in\nthe weight tensor.\n\n(pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initialize_all_tables", "path": "compat/v1/initialize_all_tables", "type": "tf.compat", "text": "\nReturns an Op that initializes all tables of the default graph. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initialize_all_variables", "path": "compat/v1/initialize_all_variables", "type": "tf.compat", "text": "\nSee `tf.compat.v1.global_variables_initializer`. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initialize_local_variables", "path": "compat/v1/initialize_local_variables", "type": "tf.compat", "text": "\nSee `tf.compat.v1.local_variables_initializer`. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.initialize_variables", "path": "compat/v1/initialize_variables", "type": "tf.compat", "text": "\nSee `tf.compat.v1.variables_initializer`. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.InteractiveSession", "path": "compat/v1/interactivesession", "type": "tf.compat", "text": "\nA TensorFlow `Session` for use in interactive contexts, such as a shell.\n\nThe only difference with a regular `Session` is that an `InteractiveSession`\ninstalls itself as the default session on construction. The methods\n`tf.Tensor.eval` and `tf.Operation.run` will use that session to run ops.\n\nThis is convenient in interactive shells and IPython notebooks, as it avoids\nhaving to pass an explicit `Session` object to run ops.\n\nNote that a regular session installs itself as the default session when it is\ncreated in a `with` statement. The common usage in non-interactive programs is\nto follow that pattern:\n\nView source\n\nReturns a context manager that makes this object the default session.\n\nUse with the `with` keyword to specify that calls to `tf.Operation.run` or\n`tf.Tensor.eval` should be executed in this session.\n\nTo get the current default session, use `tf.compat.v1.get_default_session`.\n\nAlternatively, you can use `with tf.compat.v1.Session():` to create a session\nthat is automatically closed on exiting the context, including when an\nuncaught exception is raised.\n\nView source\n\nCloses an `InteractiveSession`.\n\nView source\n\nLists available devices in this session.\n\nEach element in the list has the following properties\n\nView source\n\nReturns a Python callable that runs a particular step.\n\nThe returned callable will take `len(feed_list)` arguments whose types must be\ncompatible feed values for the respective elements of `feed_list`. For\nexample, if element `i` of `feed_list` is a `tf.Tensor`, the `i`th argument to\nthe returned callable must be a numpy ndarray (or something convertible to an\nndarray) with matching element type and shape. See `tf.Session.run` for\ndetails of the allowable feed key and value types.\n\nThe returned callable will have the same return type as\n`tf.Session.run(fetches, ...)`. For example, if `fetches` is a `tf.Tensor`,\nthe callable will return a numpy ndarray; if `fetches` is a `tf.Operation`, it\nwill return `None`.\n\nView source\n\nContinues the execution with more feeds and fetches.\n\nThis is EXPERIMENTAL and subject to change.\n\nTo use partial execution, a user first calls `partial_run_setup()` and then a\nsequence of `partial_run()`. `partial_run_setup` specifies the list of feeds\nand fetches that will be used in the subsequent `partial_run` calls.\n\nThe optional `feed_dict` argument allows the caller to override the value of\ntensors in the graph. See run() for more information.\n\nBelow is a simple example:\n\nView source\n\nSets up a graph with feeds and fetches for partial run.\n\nThis is EXPERIMENTAL and subject to change.\n\nNote that contrary to `run`, `feeds` only specifies the graph elements. The\ntensors will be supplied by the subsequent `partial_run` calls.\n\nView source\n\nRuns operations and evaluates tensors in `fetches`.\n\nThis method runs one \"step\" of TensorFlow computation, by running the\nnecessary graph fragment to execute every `Operation` and evaluate every\n`Tensor` in `fetches`, substituting the values in `feed_dict` for the\ncorresponding input values.\n\nThe `fetches` argument may be a single graph element, or an arbitrarily nested\nlist, tuple, namedtuple, dict, or OrderedDict containing graph elements at its\nleaves. A graph element can be one of the following types:\n\nThe value returned by `run()` has the same shape as the `fetches` argument,\nwhere the leaves are replaced by the corresponding values returned by\nTensorFlow.\n\nThe optional `feed_dict` argument allows the caller to override the value of\ntensors in the graph. Each key in `feed_dict` can be one of the following\ntypes:\n\nEach value in `feed_dict` must be convertible to a numpy array of the dtype of\nthe corresponding key.\n\nThe optional `options` argument expects a [`RunOptions`] proto. The options\nallow controlling the behavior of this particular step (e.g. turning tracing\non).\n\nThe optional `run_metadata` argument expects a [`RunMetadata`] proto. When\nappropriate, the non-Tensor output of this step will be collected there. For\nexample, when users turn on tracing in `options`, the profiled info will be\ncollected into this argument and passed back.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.io", "path": "compat/v1/io", "type": "tf.compat", "text": "\nPublic API for tf.io namespace.\n\n`gfile` module: Public API for tf.io.gfile namespace.\n\n`class FixedLenFeature`: Configuration for parsing a fixed-length input\nfeature.\n\n`class FixedLenSequenceFeature`: Configuration for parsing a variable-length\ninput feature into a `Tensor`.\n\n`class PaddingFIFOQueue`: A FIFOQueue that supports batching variable-sized\ntensors by padding.\n\n`class PriorityQueue`: A queue implementation that dequeues elements in\nprioritized order.\n\n`class QueueBase`: Base class for queue implementations.\n\n`class RaggedFeature`: Configuration for passing a RaggedTensor input feature.\n\n`class RandomShuffleQueue`: A queue implementation that dequeues elements in a\nrandom order.\n\n`class SparseFeature`: Configuration for parsing a sparse input feature from\nan `Example`.\n\n`class TFRecordCompressionType`: The type of compression for the record.\n\n`class TFRecordOptions`: Options used for manipulating TFRecord files.\n\n`class TFRecordWriter`: A class to write records to a TFRecords file.\n\n`class VarLenFeature`: Configuration for parsing a variable-length input\nfeature.\n\n`decode_and_crop_jpeg(...)`: Decode and Crop a JPEG-encoded image to a uint8\ntensor.\n\n`decode_base64(...)`: Decode web-safe base64-encoded strings.\n\n`decode_bmp(...)`: Decode the first frame of a BMP-encoded image to a uint8\ntensor.\n\n`decode_compressed(...)`: Decompress strings.\n\n`decode_csv(...)`: Convert CSV records to tensors. Each column maps to one\ntensor.\n\n`decode_gif(...)`: Decode the frame(s) of a GIF-encoded image to a uint8\ntensor.\n\n`decode_image(...)`: Function for `decode_bmp`, `decode_gif`, `decode_jpeg`,\nand `decode_png`.\n\n`decode_jpeg(...)`: Decode a JPEG-encoded image to a uint8 tensor.\n\n`decode_json_example(...)`: Convert JSON-encoded Example records to binary\nprotocol buffer strings.\n\n`decode_png(...)`: Decode a PNG-encoded image to a uint8 or uint16 tensor.\n\n`decode_proto(...)`: The op extracts fields from a serialized protocol buffers\nmessage into tensors.\n\n`decode_raw(...)`: Convert raw byte strings into tensors. (deprecated\narguments)\n\n`deserialize_many_sparse(...)`: Deserialize and concatenate `SparseTensors`\nfrom a serialized minibatch.\n\n`encode_base64(...)`: Encode strings into web-safe base64 format.\n\n`encode_jpeg(...)`: JPEG-encode an image.\n\n`encode_png(...)`: PNG-encode an image.\n\n`encode_proto(...)`: The op serializes protobuf messages provided in the input\ntensors.\n\n`extract_jpeg_shape(...)`: Extract the shape information of a JPEG-encoded\nimage.\n\n`is_jpeg(...)`: Convenience function to check if the 'contents' encodes a JPEG\nimage.\n\n`match_filenames_once(...)`: Save the list of files matching pattern, so it is\nonly computed once.\n\n`matching_files(...)`: Returns the set of files matching one or more glob\npatterns.\n\n`parse_example(...)`: Parses `Example` protos into a `dict` of tensors.\n\n`parse_sequence_example(...)`: Parses a batch of `SequenceExample` protos.\n\n`parse_single_example(...)`: Parses a single `Example` proto.\n\n`parse_single_sequence_example(...)`: Parses a single `SequenceExample` proto.\n\n`parse_tensor(...)`: Transforms a serialized tensorflow.TensorProto proto into\na Tensor.\n\n`read_file(...)`: Reads and outputs the entire contents of the input filename.\n\n`serialize_many_sparse(...)`: Serialize `N`-minibatch `SparseTensor` into an\n`[N, 3]` `Tensor`.\n\n`serialize_sparse(...)`: Serialize a `SparseTensor` into a 3-vector (1-D\n`Tensor`) object.\n\n`serialize_tensor(...)`: Transforms a Tensor into a serialized TensorProto\nproto.\n\n`tf_record_iterator(...)`: An iterator that read the records from a TFRecords\nfile. (deprecated)\n\n`write_file(...)`: Writes contents to the file at input filename. Creates file\nand recursively\n\n`write_graph(...)`: Writes a graph proto to a file.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.io.gfile", "path": "compat/v1/io/gfile", "type": "tf.compat", "text": "\nPublic API for tf.io.gfile namespace.\n\n`class GFile`: File I/O wrappers without thread locking.\n\n`copy(...)`: Copies data from `src` to `dst`.\n\n`exists(...)`: Determines whether a path exists or not.\n\n`glob(...)`: Returns a list of files that match the given pattern(s).\n\n`isdir(...)`: Returns whether the path is a directory or not.\n\n`listdir(...)`: Returns a list of entries contained within a directory.\n\n`makedirs(...)`: Creates a directory and all parent/intermediate directories.\n\n`mkdir(...)`: Creates a directory with the name given by `path`.\n\n`remove(...)`: Deletes the path located at 'path'.\n\n`rename(...)`: Rename or move a file / directory.\n\n`rmtree(...)`: Deletes everything under path recursively.\n\n`stat(...)`: Returns file statistics for a given path.\n\n`walk(...)`: Recursive directory tree generator for directories.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.io.TFRecordCompressionType", "path": "compat/v1/io/tfrecordcompressiontype", "type": "tf.compat", "text": "\nThe type of compression for the record.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.python_io.TFRecordCompressionType`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.io.tf_record_iterator", "path": "compat/v1/io/tf_record_iterator", "type": "tf.compat", "text": "\nAn iterator that read the records from a TFRecords file. (deprecated)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.python_io.tf_record_iterator`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.is_variable_initialized", "path": "compat/v1/is_variable_initialized", "type": "tf.compat", "text": "\nTests if a variable has been initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras", "path": "compat/v1/keras", "type": "tf.compat", "text": "\nImplementation of the Keras API meant to be a high-level API for TensorFlow.\n\nDetailed documentation and user guides are available at tensorflow.org.\n\n`activations` module: Built-in activation functions.\n\n`applications` module: Keras Applications are canned architectures with pre-\ntrained weights.\n\n`backend` module: Keras backend API.\n\n`callbacks` module: Callbacks: utilities called at certain points during model\ntraining.\n\n`constraints` module: Constraints: functions that impose constraints on weight\nvalues.\n\n`datasets` module: Public API for tf.keras.datasets namespace.\n\n`estimator` module: Keras estimator API.\n\n`experimental` module: Public API for tf.keras.experimental namespace.\n\n`initializers` module: Keras initializer serialization / deserialization.\n\n`layers` module: Keras layers API.\n\n`losses` module: Built-in loss functions.\n\n`metrics` module: Built-in metrics.\n\n`mixed_precision` module: Keras mixed precision API.\n\n`models` module: Code for model cloning, plus model-related API entries.\n\n`optimizers` module: Built-in optimizer classes.\n\n`preprocessing` module: Keras data preprocessing utils.\n\n`regularizers` module: Built-in regularizers.\n\n`utils` module: Public API for tf.keras.utils namespace.\n\n`wrappers` module: Public API for tf.keras.wrappers namespace.\n\n`class Model`: `Model` groups layers into an object with training and\ninference features.\n\n`class Sequential`: `Sequential` groups a linear stack of layers into a\n`tf.keras.Model`.\n\n`Input(...)`: `Input()` is used to instantiate a Keras tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.activations", "path": "compat/v1/keras/activations", "type": "tf.compat", "text": "\nBuilt-in activation functions.\n\n`deserialize(...)`: Returns activation function given a string identifier.\n\n`elu(...)`: Exponential Linear Unit.\n\n`exponential(...)`: Exponential activation function.\n\n`get(...)`: Returns function.\n\n`hard_sigmoid(...)`: Hard sigmoid activation function.\n\n`linear(...)`: Linear activation function (pass-through).\n\n`relu(...)`: Applies the rectified linear unit activation function.\n\n`selu(...)`: Scaled Exponential Linear Unit (SELU).\n\n`serialize(...)`: Returns the string identifier of an activation function.\n\n`sigmoid(...)`: Sigmoid activation function, `sigmoid(x) = 1 / (1 + exp(-x))`.\n\n`softmax(...)`: Softmax converts a real vector to a vector of categorical\nprobabilities.\n\n`softplus(...)`: Softplus activation function, `softplus(x) = log(exp(x) +\n1)`.\n\n`softsign(...)`: Softsign activation function, `softsign(x) = x / (abs(x) +\n1)`.\n\n`swish(...)`: Swish activation function, `swish(x) = x * sigmoid(x)`.\n\n`tanh(...)`: Hyperbolic tangent activation function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications", "path": "compat/v1/keras/applications", "type": "tf.compat", "text": "\nKeras Applications are canned architectures with pre-trained weights.\n\n`densenet` module: DenseNet models for Keras.\n\n`efficientnet` module: EfficientNet models for Keras.\n\n`imagenet_utils` module: Utilities for ImageNet data preprocessing &\nprediction decoding.\n\n`inception_resnet_v2` module: Inception-ResNet V2 model for Keras.\n\n`inception_v3` module: Inception V3 model for Keras.\n\n`mobilenet` module: MobileNet v1 models for Keras.\n\n`mobilenet_v2` module: MobileNet v2 models for Keras.\n\n`mobilenet_v3` module: MobileNet v3 models for Keras.\n\n`nasnet` module: NASNet-A models for Keras.\n\n`resnet` module: ResNet models for Keras.\n\n`resnet50` module: Public API for tf.keras.applications.resnet50 namespace.\n\n`resnet_v2` module: ResNet v2 models for Keras.\n\n`vgg16` module: VGG16 model for Keras.\n\n`vgg19` module: VGG19 model for Keras.\n\n`xception` module: Xception V1 model for Keras.\n\n`DenseNet121(...)`: Instantiates the Densenet121 architecture.\n\n`DenseNet169(...)`: Instantiates the Densenet169 architecture.\n\n`DenseNet201(...)`: Instantiates the Densenet201 architecture.\n\n`EfficientNetB0(...)`: Instantiates the EfficientNetB0 architecture.\n\n`EfficientNetB1(...)`: Instantiates the EfficientNetB1 architecture.\n\n`EfficientNetB2(...)`: Instantiates the EfficientNetB2 architecture.\n\n`EfficientNetB3(...)`: Instantiates the EfficientNetB3 architecture.\n\n`EfficientNetB4(...)`: Instantiates the EfficientNetB4 architecture.\n\n`EfficientNetB5(...)`: Instantiates the EfficientNetB5 architecture.\n\n`EfficientNetB6(...)`: Instantiates the EfficientNetB6 architecture.\n\n`EfficientNetB7(...)`: Instantiates the EfficientNetB7 architecture.\n\n`InceptionResNetV2(...)`: Instantiates the Inception-ResNet v2 architecture.\n\n`InceptionV3(...)`: Instantiates the Inception v3 architecture.\n\n`MobileNet(...)`: Instantiates the MobileNet architecture.\n\n`MobileNetV2(...)`: Instantiates the MobileNetV2 architecture.\n\n`MobileNetV3Large(...)`: Instantiates the MobileNetV3Large architecture.\n\n`MobileNetV3Small(...)`: Instantiates the MobileNetV3Small architecture.\n\n`NASNetLarge(...)`: Instantiates a NASNet model in ImageNet mode.\n\n`NASNetMobile(...)`: Instantiates a Mobile NASNet model in ImageNet mode.\n\n`ResNet101(...)`: Instantiates the ResNet101 architecture.\n\n`ResNet101V2(...)`: Instantiates the ResNet101V2 architecture.\n\n`ResNet152(...)`: Instantiates the ResNet152 architecture.\n\n`ResNet152V2(...)`: Instantiates the ResNet152V2 architecture.\n\n`ResNet50(...)`: Instantiates the ResNet50 architecture.\n\n`ResNet50V2(...)`: Instantiates the ResNet50V2 architecture.\n\n`VGG16(...)`: Instantiates the VGG16 model.\n\n`VGG19(...)`: Instantiates the VGG19 architecture.\n\n`Xception(...)`: Instantiates the Xception architecture.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.densenet", "path": "compat/v1/keras/applications/densenet", "type": "tf.compat", "text": "\nDenseNet models for Keras.\n\n`DenseNet121(...)`: Instantiates the Densenet121 architecture.\n\n`DenseNet169(...)`: Instantiates the Densenet169 architecture.\n\n`DenseNet201(...)`: Instantiates the Densenet201 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.efficientnet", "path": "compat/v1/keras/applications/efficientnet", "type": "tf.compat", "text": "\nEfficientNet models for Keras.\n\n`EfficientNetB0(...)`: Instantiates the EfficientNetB0 architecture.\n\n`EfficientNetB1(...)`: Instantiates the EfficientNetB1 architecture.\n\n`EfficientNetB2(...)`: Instantiates the EfficientNetB2 architecture.\n\n`EfficientNetB3(...)`: Instantiates the EfficientNetB3 architecture.\n\n`EfficientNetB4(...)`: Instantiates the EfficientNetB4 architecture.\n\n`EfficientNetB5(...)`: Instantiates the EfficientNetB5 architecture.\n\n`EfficientNetB6(...)`: Instantiates the EfficientNetB6 architecture.\n\n`EfficientNetB7(...)`: Instantiates the EfficientNetB7 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.imagenet_utils", "path": "compat/v1/keras/applications/imagenet_utils", "type": "tf.compat", "text": "\nUtilities for ImageNet data preprocessing & prediction decoding.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.inception_resnet_v2", "path": "compat/v1/keras/applications/inception_resnet_v2", "type": "tf.compat", "text": "\nInception-ResNet V2 model for Keras.\n\n`InceptionResNetV2(...)`: Instantiates the Inception-ResNet v2 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.inception_v3", "path": "compat/v1/keras/applications/inception_v3", "type": "tf.compat", "text": "\nInception V3 model for Keras.\n\n`InceptionV3(...)`: Instantiates the Inception v3 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.mobilenet", "path": "compat/v1/keras/applications/mobilenet", "type": "tf.compat", "text": "\nMobileNet v1 models for Keras.\n\nMobileNet is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nwidth factors. This allows different width models to reduce the number of\nmultiply-adds and thereby reduce inference cost on mobile devices.\n\nMobileNets support any input size greater than 32 x 32, with larger image\nsizes offering better performance. The number of parameters and number of\nmultiply-adds can be modified by using the `alpha` parameter, which\nincreases/decreases the number of filters in each layer. By altering the image\nsize and `alpha` parameter, all 16 models from the paper can be built, with\nImageNet weights provided.\n\nThe paper demonstrates the performance of MobileNets using `alpha` values of\n1.0 (also called 100 % MobileNet), 0.75, 0.5 and 0.25. For each of these\n`alpha` values, weights for 4 different input image sizes are provided (224,\n192, 160, 128).\n\nThe following table describes the size and accuracy of the 100% MobileNet\n\n| 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | | 0.75 MobileNet-224 | 68.4 % | 325\n| 2.6 | | 0.50 MobileNet-224 | 63.7 % | 149 | 1.3 |\n\nThe following table describes the performance of\n\n| 1.0 MobileNet-224 | 70.6 % | 529 | 4.2 | | 1.0 MobileNet-192 | 69.1 % | 529\n| 4.2 | | 1.0 MobileNet-160 | 67.2 % | 529 | 4.2 |\n\n`MobileNet(...)`: Instantiates the MobileNet architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.mobilenet_v2", "path": "compat/v1/keras/applications/mobilenet_v2", "type": "tf.compat", "text": "\nMobileNet v2 models for Keras.\n\nMobileNetV2 is a general architecture and can be used for multiple use cases.\nDepending on the use case, it can use different input layer size and different\nwidth factors. This allows different width models to reduce the number of\nmultiply-adds and thereby reduce inference cost on mobile devices.\n\nMobileNetV2 is very similar to the original MobileNet, except that it uses\ninverted residual blocks with bottlenecking features. It has a drastically\nlower parameter count than the original MobileNet. MobileNets support any\ninput size greater than 32 x 32, with larger image sizes offering better\nperformance.\n\nThe number of parameters and number of multiply-adds can be modified by using\nthe `alpha` parameter, which increases/decreases the number of filters in each\nlayer. By altering the image size and `alpha` parameter, all 22 models from\nthe paper can be built, with ImageNet weights provided.\n\nThe paper demonstrates the performance of MobileNets using `alpha` values of\n1.0 (also called 100 % MobileNet), 0.35, 0.5, 0.75, 1.0, 1.3, and 1.4 For each\nof these `alpha` values, weights for 5 different input image sizes are\nprovided (224, 192, 160, 128, and 96).\n\nThe following table describes the performance of\n\nMACs stands for Multiply Adds Classification Checkpoint|MACs (M)|Parameters\n(M)|Top 1 Accuracy|Top 5 Accuracy\n--------------------------|------------|---------------|---------|----|---------\n| [mobilenet_v2_1.4_224] | 582 | 6.06 | 75.0 | 92.5 | | [mobilenet_v2_1.3_224]\n| 509 | 5.34 | 74.4 | 92.1 | | [mobilenet_v2_1.0_224] | 300 | 3.47 | 71.8 |\n91.0 | | [mobilenet_v2_1.0_192] | 221 | 3.47 | 70.7 | 90.1 | |\n[mobilenet_v2_1.0_160] | 154 | 3.47 | 68.8 | 89.0 | | [mobilenet_v2_1.0_128] |\n99 | 3.47 | 65.3 | 86.9 | | [mobilenet_v2_1.0_96] | 56 | 3.47 | 60.3 | 83.2 |\n| [mobilenet_v2_0.75_224] | 209 | 2.61 | 69.8 | 89.6 | |\n[mobilenet_v2_0.75_192] | 153 | 2.61 | 68.7 | 88.9 | | [mobilenet_v2_0.75_160]\n| 107 | 2.61 | 66.4 | 87.3 | | [mobilenet_v2_0.75_128] | 69 | 2.61 | 63.2 |\n85.3 | | [mobilenet_v2_0.75_96] | 39 | 2.61 | 58.8 | 81.6 | |\n[mobilenet_v2_0.5_224] | 97 | 1.95 | 65.4 | 86.4 | | [mobilenet_v2_0.5_192] |\n71 | 1.95 | 63.9 | 85.4 | | [mobilenet_v2_0.5_160] | 50 | 1.95 | 61.0 | 83.2 |\n| [mobilenet_v2_0.5_128] | 32 | 1.95 | 57.7 | 80.8 | | [mobilenet_v2_0.5_96] |\n18 | 1.95 | 51.2 | 75.8 | | [mobilenet_v2_0.35_224] | 59 | 1.66 | 60.3 | 82.9\n| | [mobilenet_v2_0.35_192] | 43 | 1.66 | 58.2 | 81.2 | |\n[mobilenet_v2_0.35_160] | 30 | 1.66 | 55.7 | 79.1 | | [mobilenet_v2_0.35_128]\n| 20 | 1.66 | 50.8 | 75.0 | | [mobilenet_v2_0.35_96] | 11 | 1.66 | 45.5 | 70.4\n|\n\nReference:\n\n`MobileNetV2(...)`: Instantiates the MobileNetV2 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.mobilenet_v3", "path": "compat/v1/keras/applications/mobilenet_v3", "type": "tf.compat", "text": "\nMobileNet v3 models for Keras.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.nasnet", "path": "compat/v1/keras/applications/nasnet", "type": "tf.compat", "text": "\nNASNet-A models for Keras.\n\nNASNet refers to Neural Architecture Search Network, a family of models that\nwere designed automatically by learning the model architectures directly on\nthe dataset of interest.\n\nHere we consider NASNet-A, the highest performance model that was found for\nthe CIFAR-10 dataset, and then extended to ImageNet 2012 dataset, obtaining\nstate of the art performance on CIFAR-10 and ImageNet 2012. Only the NASNet-A\nmodels, and their respective weights, which are suited for ImageNet 2012 are\nprovided.\n\n| NASNet-A (4 @ 1056) | 74.0 % | 91.6 % | 564 M | 5.3 |\n\n`NASNetLarge(...)`: Instantiates a NASNet model in ImageNet mode.\n\n`NASNetMobile(...)`: Instantiates a Mobile NASNet model in ImageNet mode.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.resnet", "path": "compat/v1/keras/applications/resnet", "type": "tf.compat", "text": "\nResNet models for Keras.\n\n`ResNet101(...)`: Instantiates the ResNet101 architecture.\n\n`ResNet152(...)`: Instantiates the ResNet152 architecture.\n\n`ResNet50(...)`: Instantiates the ResNet50 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.resnet50", "path": "compat/v1/keras/applications/resnet50", "type": "tf.compat", "text": "\nPublic API for tf.keras.applications.resnet50 namespace.\n\n`ResNet50(...)`: Instantiates the ResNet50 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.resnet_v2", "path": "compat/v1/keras/applications/resnet_v2", "type": "tf.compat", "text": "\nResNet v2 models for Keras.\n\n`ResNet101V2(...)`: Instantiates the ResNet101V2 architecture.\n\n`ResNet152V2(...)`: Instantiates the ResNet152V2 architecture.\n\n`ResNet50V2(...)`: Instantiates the ResNet50V2 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.vgg16", "path": "compat/v1/keras/applications/vgg16", "type": "tf.compat", "text": "\nVGG16 model for Keras.\n\n`VGG16(...)`: Instantiates the VGG16 model.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.vgg19", "path": "compat/v1/keras/applications/vgg19", "type": "tf.compat", "text": "\nVGG19 model for Keras.\n\n`VGG19(...)`: Instantiates the VGG19 architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.applications.xception", "path": "compat/v1/keras/applications/xception", "type": "tf.compat", "text": "\nXception V1 model for Keras.\n\nOn ImageNet, this model gets to a top-1 validation accuracy of 0.790 and a\ntop-5 validation accuracy of 0.945.\n\n`Xception(...)`: Instantiates the Xception architecture.\n\n`decode_predictions(...)`: Decodes the prediction of an ImageNet model.\n\n`preprocess_input(...)`: Preprocesses a tensor or Numpy array encoding a batch\nof images.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.backend", "path": "compat/v1/keras/backend", "type": "tf.compat", "text": "\nKeras backend API.\n\n`class name_scope`: A context manager for use when defining a Python op.\n\n`clear_session(...)`: Resets all state generated by Keras.\n\n`epsilon(...)`: Returns the value of the fuzz factor used in numeric\nexpressions.\n\n`floatx(...)`: Returns the default float type, as a string.\n\n`get_session(...)`: Returns the TF session to be used by the backend.\n\n`get_uid(...)`: Associates a string prefix with an integer counter in a\nTensorFlow graph.\n\n`image_data_format(...)`: Returns the default image data format convention.\n\n`is_keras_tensor(...)`: Returns whether `x` is a Keras tensor.\n\n`reset_uids(...)`: Resets graph identifiers.\n\n`rnn(...)`: Iterates over the time dimension of a tensor.\n\n`set_epsilon(...)`: Sets the value of the fuzz factor used in numeric\nexpressions.\n\n`set_floatx(...)`: Sets the default float type.\n\n`set_image_data_format(...)`: Sets the value of the image data format\nconvention.\n\n`set_session(...)`: Sets the global TensorFlow session.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.backend.get_session", "path": "compat/v1/keras/backend/get_session", "type": "tf.compat", "text": "\nReturns the TF session to be used by the backend.\n\nIf a default TensorFlow session is available, we will return it.\n\nElse, we will return the global Keras session assuming it matches the current\ngraph.\n\nIf no global Keras session exists at this point: we will create a new global\nsession.\n\nNote that you can manually set the global session via `K.set_session(sess)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.backend.name_scope", "path": "compat/v1/keras/backend/name_scope", "type": "tf.compat", "text": "\nA context manager for use when defining a Python op.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.name_scope`\n\nThis context manager validates that the given `values` are from the same\ngraph, makes that graph the default graph, and pushes a name scope in that\ngraph (see `tf.Graph.name_scope` for more details on that).\n\nFor example, to define a new Python op called `my_op`:\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.backend.set_session", "path": "compat/v1/keras/backend/set_session", "type": "tf.compat", "text": "\nSets the global TensorFlow session.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.callbacks", "path": "compat/v1/keras/callbacks", "type": "tf.compat", "text": "\nCallbacks: utilities called at certain points during model training.\n\n`class BaseLogger`: Callback that accumulates epoch averages of metrics.\n\n`class CSVLogger`: Callback that streams epoch results to a CSV file.\n\n`class Callback`: Abstract base class used to build new callbacks.\n\n`class CallbackList`: Container abstracting a list of callbacks.\n\n`class EarlyStopping`: Stop training when a monitored metric has stopped\nimproving.\n\n`class History`: Callback that records events into a `History` object.\n\n`class LambdaCallback`: Callback for creating simple, custom callbacks on-the-\nfly.\n\n`class LearningRateScheduler`: Learning rate scheduler.\n\n`class ModelCheckpoint`: Callback to save the Keras model or model weights at\nsome frequency.\n\n`class ProgbarLogger`: Callback that prints metrics to stdout.\n\n`class ReduceLROnPlateau`: Reduce learning rate when a metric has stopped\nimproving.\n\n`class RemoteMonitor`: Callback used to stream events to a server.\n\n`class TensorBoard`: Enable visualizations for TensorBoard.\n\n`class TerminateOnNaN`: Callback that terminates training when a NaN loss is\nencountered.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.callbacks.TensorBoard", "path": "compat/v1/keras/callbacks/tensorboard", "type": "tf.compat", "text": "\nEnable visualizations for TensorBoard.\n\nInherits From: `TensorBoard`, `Callback`\n\nTensorBoard is a visualization tool provided with TensorFlow.\n\nThis callback logs events for TensorBoard, including:\n\nIf you have installed TensorFlow with pip, you should be able to launch\nTensorBoard from the command line:\n\nYou can find more information about TensorBoard here.\n\nUsing the `TensorBoard` callback will work when eager execution is enabled,\nwith the restriction that outputting histogram summaries of weights and\ngradients is not supported. Consequently, `histogram_freq` will be ignored.\n\nView source\n\nSets Keras model and creates summary ops.\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.constraints", "path": "compat/v1/keras/constraints", "type": "tf.compat", "text": "\nConstraints: functions that impose constraints on weight values.\n\n`class Constraint`\n\n`class MaxNorm`: MaxNorm weight constraint.\n\n`class MinMaxNorm`: MinMaxNorm weight constraint.\n\n`class NonNeg`: Constrains the weights to be non-negative.\n\n`class RadialConstraint`: Constrains `Conv2D` kernel weights to be the same\nfor each radius.\n\n`class UnitNorm`: Constrains the weights incident to each hidden unit to have\nunit norm.\n\n`class max_norm`: MaxNorm weight constraint.\n\n`class min_max_norm`: MinMaxNorm weight constraint.\n\n`class non_neg`: Constrains the weights to be non-negative.\n\n`class radial_constraint`: Constrains `Conv2D` kernel weights to be the same\nfor each radius.\n\n`class unit_norm`: Constrains the weights incident to each hidden unit to have\nunit norm.\n\n`deserialize(...)`\n\n`get(...)`\n\n`serialize(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets", "path": "compat/v1/keras/datasets", "type": "tf.compat", "text": "\nPublic API for tf.keras.datasets namespace.\n\n`boston_housing` module: Boston housing price regression dataset.\n\n`cifar10` module: CIFAR10 small images classification dataset.\n\n`cifar100` module: CIFAR100 small images classification dataset.\n\n`fashion_mnist` module: Fashion-MNIST dataset.\n\n`imdb` module: IMDB sentiment classification dataset.\n\n`mnist` module: MNIST handwritten digits dataset.\n\n`reuters` module: Reuters topic classification dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.boston_housing", "path": "compat/v1/keras/datasets/boston_housing", "type": "tf.compat", "text": "\nBoston housing price regression dataset.\n\n`load_data(...)`: Loads the Boston Housing dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.cifar10", "path": "compat/v1/keras/datasets/cifar10", "type": "tf.compat", "text": "\nCIFAR10 small images classification dataset.\n\n`load_data(...)`: Loads CIFAR10 dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.cifar100", "path": "compat/v1/keras/datasets/cifar100", "type": "tf.compat", "text": "\nCIFAR100 small images classification dataset.\n\n`load_data(...)`: Loads CIFAR100 dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.fashion_mnist", "path": "compat/v1/keras/datasets/fashion_mnist", "type": "tf.compat", "text": "\nFashion-MNIST dataset.\n\n`load_data(...)`: Loads the Fashion-MNIST dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.imdb", "path": "compat/v1/keras/datasets/imdb", "type": "tf.compat", "text": "\nIMDB sentiment classification dataset.\n\n`get_word_index(...)`: Retrieves a dict mapping words to their index in the\nIMDB dataset.\n\n`load_data(...)`: Loads the IMDB dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.mnist", "path": "compat/v1/keras/datasets/mnist", "type": "tf.compat", "text": "\nMNIST handwritten digits dataset.\n\n`load_data(...)`: Loads the MNIST dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.datasets.reuters", "path": "compat/v1/keras/datasets/reuters", "type": "tf.compat", "text": "\nReuters topic classification dataset.\n\n`get_word_index(...)`: Retrieves a dict mapping words to their index in the\nReuters dataset.\n\n`load_data(...)`: Loads the Reuters newswire classification dataset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.estimator", "path": "compat/v1/keras/estimator", "type": "tf.compat", "text": "\nKeras estimator API.\n\n`model_to_estimator(...)`: Constructs an `Estimator` instance from given keras\nmodel.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.estimator.model_to_estimator", "path": "compat/v1/keras/estimator/model_to_estimator", "type": "tf.compat", "text": "\nConstructs an `Estimator` instance from given keras model.\n\nIf you use infrastructure or other tooling that relies on Estimators, you can\nstill build a Keras model and use model_to_estimator to convert the Keras\nmodel to an Estimator for use with downstream systems.\n\nFor usage example, please see: Creating estimators from Keras Models.\n\nEstimators returned by `model_to_estimator` are configured so that they can\nhandle sample weights (similar to `keras_model.fit(x, y, sample_weights)`).\n\nTo pass sample weights when training or evaluating the Estimator, the first\nitem returned by the input function should be a dictionary with keys\n`features` and `sample_weights`. Example below:\n\nExample with customized export signature:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.experimental", "path": "compat/v1/keras/experimental", "type": "tf.compat", "text": "\nPublic API for tf.keras.experimental namespace.\n\n`class CosineDecay`: A LearningRateSchedule that uses a cosine decay schedule.\n\n`class CosineDecayRestarts`: A LearningRateSchedule that uses a cosine decay\nschedule with restarts.\n\n`class LinearCosineDecay`: A LearningRateSchedule that uses a linear cosine\ndecay schedule.\n\n`class LinearModel`: Linear Model for regression and classification problems.\n\n`class NoisyLinearCosineDecay`: A LearningRateSchedule that uses a noisy\nlinear cosine decay schedule.\n\n`class PeepholeLSTMCell`: Equivalent to LSTMCell class but adds peephole\nconnections.\n\n`class SequenceFeatures`: A layer for sequence input.\n\n`class WideDeepModel`: Wide & Deep Model for regression and classification\nproblems.\n\n`export_saved_model(...)`: Exports a `tf.keras.Model` as a Tensorflow\nSavedModel.\n\n`load_from_saved_model(...)`: Loads a keras Model from a SavedModel created by\n`export_saved_model()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.experimental.export_saved_model", "path": "compat/v1/keras/experimental/export_saved_model", "type": "tf.compat", "text": "\nExports a `tf.keras.Model` as a Tensorflow SavedModel.\n\nNote that at this time, subclassed models can only be saved using\n`serving_only=True`.\n\nThe exported `SavedModel` is a standalone serialization of Tensorflow objects,\nand is supported by TF language APIs and the Tensorflow Serving system. To\nload the model, use the function\n`tf.keras.experimental.load_from_saved_model`.\n\nThe `SavedModel` contains:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.experimental.load_from_saved_model", "path": "compat/v1/keras/experimental/load_from_saved_model", "type": "tf.compat", "text": "\nLoads a keras Model from a SavedModel created by `export_saved_model()`.\n\nThis function reinstantiates model state by:\n\n1) loading model topology from json (this will eventually come from\nmetagraph). 2) loading model weights from checkpoint.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers", "path": "compat/v1/keras/initializers", "type": "tf.compat", "text": "\nKeras initializer serialization / deserialization.\n\n`class Constant`: Initializer that generates tensors with constant values.\n\n`class Identity`: Initializer that generates the identity matrix.\n\n`class Initializer`: Initializer base class: all Keras initializers inherit\nfrom this class.\n\n`class Ones`: Initializer that generates tensors initialized to 1.\n\n`class Orthogonal`: Initializer that generates an orthogonal matrix.\n\n`class RandomNormal`: Initializer that generates tensors with a normal\ndistribution.\n\n`class RandomUniform`: Initializer that generates tensors with a uniform\ndistribution.\n\n`class TruncatedNormal`: Initializer that generates a truncated normal\ndistribution.\n\n`class VarianceScaling`: Initializer capable of adapting its scale to the\nshape of weights tensors.\n\n`class Zeros`: Initializer that generates tensors initialized to 0.\n\n`class constant`: Initializer that generates tensors with constant values.\n\n`class glorot_normal`: The Glorot normal initializer, also called Xavier\nnormal initializer.\n\n`class glorot_uniform`: The Glorot uniform initializer, also called Xavier\nuniform initializer.\n\n`class he_normal`: Initializer capable of adapting its scale to the shape of\nweights tensors.\n\n`class he_uniform`: Initializer capable of adapting its scale to the shape of\nweights tensors.\n\n`class identity`: Initializer that generates the identity matrix.\n\n`class lecun_normal`: Initializer capable of adapting its scale to the shape\nof weights tensors.\n\n`class lecun_uniform`: Initializer capable of adapting its scale to the shape\nof weights tensors.\n\n`class normal`: Initializer that generates tensors with a normal distribution.\n\n`class ones`: Initializer that generates tensors initialized to 1.\n\n`class orthogonal`: Initializer that generates an orthogonal matrix.\n\n`class random_normal`: Initializer that generates tensors with a normal\ndistribution.\n\n`class random_uniform`: Initializer that generates tensors with a uniform\ndistribution.\n\n`class truncated_normal`: Initializer that generates a truncated normal\ndistribution.\n\n`class uniform`: Initializer that generates tensors with a uniform\ndistribution.\n\n`class zeros`: Initializer that generates tensors initialized to 0.\n\n`deserialize(...)`: Return an `Initializer` object from its config.\n\n`get(...)`\n\n`serialize(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.Constant", "path": "compat/v1/keras/initializers/constant", "type": "tf.compat", "text": "\nInitializer that generates tensors with constant values.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.constant_initializer`, `tf.compat.v1.initializers.constant`,\n`tf.compat.v1.keras.initializers.constant`\n\nThe resulting tensor is populated with values of type `dtype`, as specified by\narguments `value` following the desired `shape` of the new tensor (see\nexamples below).\n\nThe argument `value` can be a constant value, or a list of values of type\n`dtype`. If `value` is a list, then the length of the list must be less than\nor equal to the number of elements implied by the desired shape of the tensor.\nIn the case where the total number of elements in `value` is less than the\nnumber of elements required by the tensor shape, the last element in `value`\nwill be used to fill the remaining entries. If the total number of elements in\n`value` is greater than the number of elements required by the tensor shape,\nthe initializer will raise a `ValueError`.\n\nThe following example can be rewritten using a numpy.ndarray instead of the\n`value` list, even reshaped, as shown in the two commented lines below the\n`value` list initialization.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.glorot_normal", "path": "compat/v1/keras/initializers/glorot_normal", "type": "tf.compat", "text": "\nThe Glorot normal initializer, also called Xavier normal initializer.\n\nInherits From: `VarianceScaling`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.glorot_normal_initializer`,\n`tf.compat.v1.initializers.glorot_normal`\n\nIt draws samples from a truncated normal distribution centered on 0 with\nstandard deviation (after truncation) given by `stddev = sqrt(2 / (fan_in +\nfan_out))` where `fan_in` is the number of input units in the weight tensor\nand `fan_out` is the number of output units in the weight tensor.\n\nGlorot et al., 2010 (pdf)\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.glorot_uniform", "path": "compat/v1/keras/initializers/glorot_uniform", "type": "tf.compat", "text": "\nThe Glorot uniform initializer, also called Xavier uniform initializer.\n\nInherits From: `VarianceScaling`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.glorot_uniform_initializer`,\n`tf.compat.v1.initializers.glorot_uniform`\n\nIt draws samples from a uniform distribution within [-limit, limit] where\n`limit` is `sqrt(6 / (fan_in + fan_out))` where `fan_in` is the number of\ninput units in the weight tensor and `fan_out` is the number of output units\nin the weight tensor.\n\nGlorot et al., 2010 (pdf)\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.he_normal", "path": "compat/v1/keras/initializers/he_normal", "type": "tf.compat", "text": "\nInitializer capable of adapting its scale to the shape of weights tensors.\n\nInherits From: `VarianceScaling`\n\nWith `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\ndrawn from a truncated/untruncated normal distribution with a mean of zero and\na standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`\nwhere n is:\n\nWith `distribution=\"uniform\"`, samples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.he_uniform", "path": "compat/v1/keras/initializers/he_uniform", "type": "tf.compat", "text": "\nInitializer capable of adapting its scale to the shape of weights tensors.\n\nInherits From: `VarianceScaling`\n\nWith `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\ndrawn from a truncated/untruncated normal distribution with a mean of zero and\na standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`\nwhere n is:\n\nWith `distribution=\"uniform\"`, samples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.Identity", "path": "compat/v1/keras/initializers/identity", "type": "tf.compat", "text": "\nInitializer that generates the identity matrix.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.identity`,\n`tf.compat.v1.keras.initializers.identity`\n\nOnly use for 2D matrices.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.lecun_normal", "path": "compat/v1/keras/initializers/lecun_normal", "type": "tf.compat", "text": "\nInitializer capable of adapting its scale to the shape of weights tensors.\n\nInherits From: `VarianceScaling`\n\nWith `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\ndrawn from a truncated/untruncated normal distribution with a mean of zero and\na standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`\nwhere n is:\n\nWith `distribution=\"uniform\"`, samples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.lecun_uniform", "path": "compat/v1/keras/initializers/lecun_uniform", "type": "tf.compat", "text": "\nInitializer capable of adapting its scale to the shape of weights tensors.\n\nInherits From: `VarianceScaling`\n\nWith `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\ndrawn from a truncated/untruncated normal distribution with a mean of zero and\na standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`\nwhere n is:\n\nWith `distribution=\"uniform\"`, samples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.Ones", "path": "compat/v1/keras/initializers/ones", "type": "tf.compat", "text": "\nInitializer that generates tensors initialized to 1.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.ones`, `tf.compat.v1.keras.initializers.ones`,\n`tf.compat.v1.ones_initializer`\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.Orthogonal", "path": "compat/v1/keras/initializers/orthogonal", "type": "tf.compat", "text": "\nInitializer that generates an orthogonal matrix.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.orthogonal`,\n`tf.compat.v1.keras.initializers.orthogonal`,\n`tf.compat.v1.orthogonal_initializer`\n\nIf the shape of the tensor to initialize is two-dimensional, it is initialized\nwith an orthogonal matrix obtained from the QR decomposition of a matrix of\nrandom numbers drawn from a normal distribution. If the matrix has fewer rows\nthan columns then the output will have orthogonal rows. Otherwise, the output\nwill have orthogonal columns.\n\nIf the shape of the tensor to initialize is more than two-dimensional, a\nmatrix of shape `(shape[0] * ... * shape[n - 2], shape[n - 1])` is\ninitialized, where `n` is the length of the shape vector. The matrix is\nsubsequently reshaped to give a tensor of the desired shape.\n\nSaxe et al., 2014 (pdf)\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.RandomNormal", "path": "compat/v1/keras/initializers/randomnormal", "type": "tf.compat", "text": "\nInitializer that generates tensors with a normal distribution.\n\nInherits From: `random_normal_initializer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.keras.initializers.normal`,\n`tf.compat.v1.keras.initializers.random_normal`\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.RandomUniform", "path": "compat/v1/keras/initializers/randomuniform", "type": "tf.compat", "text": "\nInitializer that generates tensors with a uniform distribution.\n\nInherits From: `random_uniform_initializer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.keras.initializers.random_uniform`,\n`tf.compat.v1.keras.initializers.uniform`\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.TruncatedNormal", "path": "compat/v1/keras/initializers/truncatednormal", "type": "tf.compat", "text": "\nInitializer that generates a truncated normal distribution.\n\nInherits From: `truncated_normal_initializer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.keras.initializers.truncated_normal`\n\nThese values are similar to values from a `random_normal_initializer` except\nthat values more than two standard deviations from the mean are discarded and\nre-drawn. This is the recommended initializer for neural network weights and\nfilters.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.VarianceScaling", "path": "compat/v1/keras/initializers/variancescaling", "type": "tf.compat", "text": "\nInitializer capable of adapting its scale to the shape of weights tensors.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.variance_scaling`,\n`tf.compat.v1.variance_scaling_initializer`\n\nWith `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\ndrawn from a truncated/untruncated normal distribution with a mean of zero and\na standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`\nwhere n is:\n\nWith `distribution=\"uniform\"`, samples are drawn from a uniform distribution\nwithin [-limit, limit], with `limit = sqrt(3 * scale / n)`.\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.initializers.Zeros", "path": "compat/v1/keras/initializers/zeros", "type": "tf.compat", "text": "\nInitializer that generates tensors initialized to 0.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.zeros`, `tf.compat.v1.keras.initializers.zeros`,\n`tf.compat.v1.zeros_initializer`\n\nView source\n\nInstantiates an initializer from a configuration dictionary.\n\nView source\n\nReturns the configuration of the initializer as a JSON-serializable dict.\n\nView source\n\nReturns a tensor object initialized as specified by the initializer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers", "path": "compat/v1/keras/layers", "type": "tf.compat", "text": "\nKeras layers API.\n\n`experimental` module: Public API for tf.keras.layers.experimental namespace.\n\n`class AbstractRNNCell`: Abstract object representing an RNN cell.\n\n`class Activation`: Applies an activation function to an output.\n\n`class ActivityRegularization`: Layer that applies an update to the cost\nfunction based input activity.\n\n`class Add`: Layer that adds a list of inputs.\n\n`class AdditiveAttention`: Additive attention layer, a.k.a. Bahdanau-style\nattention.\n\n`class AlphaDropout`: Applies Alpha Dropout to the input.\n\n`class Attention`: Dot-product attention layer, a.k.a. Luong-style attention.\n\n`class Average`: Layer that averages a list of inputs element-wise.\n\n`class AveragePooling1D`: Average pooling for temporal data.\n\n`class AveragePooling2D`: Average pooling operation for spatial data.\n\n`class AveragePooling3D`: Average pooling operation for 3D data (spatial or\nspatio-temporal).\n\n`class AvgPool1D`: Average pooling for temporal data.\n\n`class AvgPool2D`: Average pooling operation for spatial data.\n\n`class AvgPool3D`: Average pooling operation for 3D data (spatial or spatio-\ntemporal).\n\n`class BatchNormalization`: Layer that normalizes its inputs.\n\n`class Bidirectional`: Bidirectional wrapper for RNNs.\n\n`class Concatenate`: Layer that concatenates a list of inputs.\n\n`class Conv1D`: 1D convolution layer (e.g. temporal convolution).\n\n`class Conv1DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class Conv2D`: 2D convolution layer (e.g. spatial convolution over images).\n\n`class Conv2DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class Conv3D`: 3D convolution layer (e.g. spatial convolution over volumes).\n\n`class Conv3DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class ConvLSTM2D`: Convolutional LSTM.\n\n`class Convolution1D`: 1D convolution layer (e.g. temporal convolution).\n\n`class Convolution1DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class Convolution2D`: 2D convolution layer (e.g. spatial convolution over\nimages).\n\n`class Convolution2DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class Convolution3D`: 3D convolution layer (e.g. spatial convolution over\nvolumes).\n\n`class Convolution3DTranspose`: Transposed convolution layer (sometimes called\nDeconvolution).\n\n`class Cropping1D`: Cropping layer for 1D input (e.g. temporal sequence).\n\n`class Cropping2D`: Cropping layer for 2D input (e.g. picture).\n\n`class Cropping3D`: Cropping layer for 3D data (e.g. spatial or spatio-\ntemporal).\n\n`class CuDNNGRU`: Fast GRU implementation backed by cuDNN.\n\n`class CuDNNLSTM`: Fast LSTM implementation backed by cuDNN.\n\n`class Dense`: Just your regular densely-connected NN layer.\n\n`class DenseFeatures`: A layer that produces a dense `Tensor` based on given\n`feature_columns`.\n\n`class DepthwiseConv2D`: Depthwise separable 2D convolution.\n\n`class Dot`: Layer that computes a dot product between samples in two tensors.\n\n`class Dropout`: Applies Dropout to the input.\n\n`class ELU`: Exponential Linear Unit.\n\n`class Embedding`: Turns positive integers (indexes) into dense vectors of\nfixed size.\n\n`class Flatten`: Flattens the input. Does not affect the batch size.\n\n`class GRU`: Gated Recurrent Unit - Cho et al. 2014.\n\n`class GRUCell`: Cell class for the GRU layer.\n\n`class GaussianDropout`: Apply multiplicative 1-centered Gaussian noise.\n\n`class GaussianNoise`: Apply additive zero-centered Gaussian noise.\n\n`class GlobalAveragePooling1D`: Global average pooling operation for temporal\ndata.\n\n`class GlobalAveragePooling2D`: Global average pooling operation for spatial\ndata.\n\n`class GlobalAveragePooling3D`: Global Average pooling operation for 3D data.\n\n`class GlobalAvgPool1D`: Global average pooling operation for temporal data.\n\n`class GlobalAvgPool2D`: Global average pooling operation for spatial data.\n\n`class GlobalAvgPool3D`: Global Average pooling operation for 3D data.\n\n`class GlobalMaxPool1D`: Global max pooling operation for 1D temporal data.\n\n`class GlobalMaxPool2D`: Global max pooling operation for spatial data.\n\n`class GlobalMaxPool3D`: Global Max pooling operation for 3D data.\n\n`class GlobalMaxPooling1D`: Global max pooling operation for 1D temporal data.\n\n`class GlobalMaxPooling2D`: Global max pooling operation for spatial data.\n\n`class GlobalMaxPooling3D`: Global Max pooling operation for 3D data.\n\n`class InputLayer`: Layer to be used as an entry point into a Network (a graph\nof layers).\n\n`class InputSpec`: Specifies the rank, dtype and shape of every input to a\nlayer.\n\n`class LSTM`: Long Short-Term Memory layer - Hochreiter 1997.\n\n`class LSTMCell`: Cell class for the LSTM layer.\n\n`class Lambda`: Wraps arbitrary expressions as a `Layer` object.\n\n`class Layer`: This is the class from which all layers inherit.\n\n`class LayerNormalization`: Layer normalization layer (Ba et al., 2016).\n\n`class LeakyReLU`: Leaky version of a Rectified Linear Unit.\n\n`class LocallyConnected1D`: Locally-connected layer for 1D inputs.\n\n`class LocallyConnected2D`: Locally-connected layer for 2D inputs.\n\n`class Masking`: Masks a sequence by using a mask value to skip timesteps.\n\n`class MaxPool1D`: Max pooling operation for 1D temporal data.\n\n`class MaxPool2D`: Max pooling operation for 2D spatial data.\n\n`class MaxPool3D`: Max pooling operation for 3D data (spatial or spatio-\ntemporal).\n\n`class MaxPooling1D`: Max pooling operation for 1D temporal data.\n\n`class MaxPooling2D`: Max pooling operation for 2D spatial data.\n\n`class MaxPooling3D`: Max pooling operation for 3D data (spatial or spatio-\ntemporal).\n\n`class Maximum`: Layer that computes the maximum (element-wise) a list of\ninputs.\n\n`class Minimum`: Layer that computes the minimum (element-wise) a list of\ninputs.\n\n`class MultiHeadAttention`: MultiHeadAttention layer.\n\n`class Multiply`: Layer that multiplies (element-wise) a list of inputs.\n\n`class PReLU`: Parametric Rectified Linear Unit.\n\n`class Permute`: Permutes the dimensions of the input according to a given\npattern.\n\n`class RNN`: Base class for recurrent layers.\n\n`class ReLU`: Rectified Linear Unit activation function.\n\n`class RepeatVector`: Repeats the input n times.\n\n`class Reshape`: Layer that reshapes inputs into the given shape.\n\n`class SeparableConv1D`: Depthwise separable 1D convolution.\n\n`class SeparableConv2D`: Depthwise separable 2D convolution.\n\n`class SeparableConvolution1D`: Depthwise separable 1D convolution.\n\n`class SeparableConvolution2D`: Depthwise separable 2D convolution.\n\n`class SimpleRNN`: Fully-connected RNN where the output is to be fed back to\ninput.\n\n`class SimpleRNNCell`: Cell class for SimpleRNN.\n\n`class Softmax`: Softmax activation function.\n\n`class SpatialDropout1D`: Spatial 1D version of Dropout.\n\n`class SpatialDropout2D`: Spatial 2D version of Dropout.\n\n`class SpatialDropout3D`: Spatial 3D version of Dropout.\n\n`class StackedRNNCells`: Wrapper allowing a stack of RNN cells to behave as a\nsingle cell.\n\n`class Subtract`: Layer that subtracts two inputs.\n\n`class ThresholdedReLU`: Thresholded Rectified Linear Unit.\n\n`class TimeDistributed`: This wrapper allows to apply a layer to every\ntemporal slice of an input.\n\n`class UpSampling1D`: Upsampling layer for 1D inputs.\n\n`class UpSampling2D`: Upsampling layer for 2D inputs.\n\n`class UpSampling3D`: Upsampling layer for 3D inputs.\n\n`class Wrapper`: Abstract wrapper base class.\n\n`class ZeroPadding1D`: Zero-padding layer for 1D input (e.g. temporal\nsequence).\n\n`class ZeroPadding2D`: Zero-padding layer for 2D input (e.g. picture).\n\n`class ZeroPadding3D`: Zero-padding layer for 3D data (spatial or spatio-\ntemporal).\n\n`Input(...)`: `Input()` is used to instantiate a Keras tensor.\n\n`add(...)`: Functional interface to the `tf.keras.layers.Add` layer.\n\n`average(...)`: Functional interface to the `tf.keras.layers.Average` layer.\n\n`concatenate(...)`: Functional interface to the `Concatenate` layer.\n\n`deserialize(...)`: Instantiates a layer from a config dictionary.\n\n`disable_v2_dtype_behavior(...)`: Disables the V2 dtype behavior for Keras\nlayers.\n\n`dot(...)`: Functional interface to the `Dot` layer.\n\n`enable_v2_dtype_behavior(...)`: Enable the V2 dtype behavior for Keras\nlayers.\n\n`maximum(...)`: Functional interface to compute maximum (element-wise) list of\n`inputs`.\n\n`minimum(...)`: Functional interface to the `Minimum` layer.\n\n`multiply(...)`: Functional interface to the `Multiply` layer.\n\n`serialize(...)`\n\n`subtract(...)`: Functional interface to the `Subtract` layer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.BatchNormalization", "path": "compat/v1/keras/layers/batchnormalization", "type": "tf.compat", "text": "\nLayer that normalizes its inputs.\n\nInherits From: `Layer`, `Module`\n\nBatch normalization applies a transformation that maintains the mean output\nclose to 0 and the output standard deviation close to 1.\n\nImportantly, batch normalization works differently during training and during\ninference.\n\nDuring training (i.e. when using `fit()` or when calling the layer/model with\nthe argument `training=True`), the layer normalizes its output using the mean\nand standard deviation of the current batch of inputs. That is to say, for\neach channel being normalized, the layer returns `(batch - mean(batch)) /\n(var(batch) + epsilon) * gamma + beta`, where:\n\nDuring inference (i.e. when using `evaluate()` or `predict()` or when calling\nthe layer/model with the argument `training=False` (which is the default), the\nlayer normalizes its output using a moving average of the mean and standard\ndeviation of the batches it has seen during training. That is to say, it\nreturns `(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma +\nbeta`.\n\n`self.moving_mean` and `self.moving_var` are non-trainable variables that are\nupdated each time the layer in called in training mode, as such:\n\nAs such, the layer will only normalize its inputs during inference after\nhaving been trained on data that has similar statistics as the inference data.\n\nInput shape: Arbitrary. Use the keyword argument `input_shape` (tuple of\nintegers, does not include the samples axis) when using this layer as the\nfirst layer in a model.\n\nOutput shape: Same shape as input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.CuDNNGRU", "path": "compat/v1/keras/layers/cudnngru", "type": "tf.compat", "text": "\nFast GRU implementation backed by cuDNN.\n\nInherits From: `RNN`, `Layer`, `Module`\n\nMore information about cuDNN can be found on the NVIDIA developer website. Can\nonly be run on GPU.\n\nView source\n\nReset the recorded states for the stateful RNN layer.\n\nCan only be used when RNN layer is constructed with `stateful` = `True`. Args:\nstates: Numpy arrays that contains the value for the initial state, which will\nbe feed to cell at the first time step. When the value is None, zero filled\nnumpy array will be created based on the cell state size.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.CuDNNLSTM", "path": "compat/v1/keras/layers/cudnnlstm", "type": "tf.compat", "text": "\nFast LSTM implementation backed by cuDNN.\n\nInherits From: `RNN`, `Layer`, `Module`\n\nMore information about cuDNN can be found on the NVIDIA developer website. Can\nonly be run on GPU.\n\nView source\n\nReset the recorded states for the stateful RNN layer.\n\nCan only be used when RNN layer is constructed with `stateful` = `True`. Args:\nstates: Numpy arrays that contains the value for the initial state, which will\nbe feed to cell at the first time step. When the value is None, zero filled\nnumpy array will be created based on the cell state size.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.DenseFeatures", "path": "compat/v1/keras/layers/densefeatures", "type": "tf.compat", "text": "\nA layer that produces a dense `Tensor` based on given `feature_columns`.\n\nInherits From: `Layer`, `Module`\n\nGenerally a single example in training data is described with FeatureColumns.\nAt the first layer of the model, this column-oriented data should be converted\nto a single `Tensor`.\n\nThis layer can be called multiple times with different features.\n\nThis is the V1 version of this layer that uses variable_scope's or partitioner\nto create variables which works well with PartitionedVariables. Variable\nscopes are deprecated in V2, so the V2 version uses name_scopes instead. But\ncurrently that lacks support for partitioned variables. Use this if you need\npartitioned variables. Use the partitioner argument if you have a Keras model\nand uses `tf.compat.v1.keras.estimator.model_to_estimator` for training.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.disable_v2_dtype_behavior", "path": "compat/v1/keras/layers/disable_v2_dtype_behavior", "type": "tf.compat", "text": "\nDisables the V2 dtype behavior for Keras layers.\n\nSee `tf.compat.v1.keras.layers.enable_v2_dtype_behavior`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.enable_v2_dtype_behavior", "path": "compat/v1/keras/layers/enable_v2_dtype_behavior", "type": "tf.compat", "text": "\nEnable the V2 dtype behavior for Keras layers.\n\nBy default, the V2 dtype behavior is enabled in TensorFlow 2, so this function\nis only useful if `tf.compat.v1.disable_v2_behavior` has been called. Since\nmixed precision requires V2 dtype behavior to be enabled, this function allows\nyou to use mixed precision in Keras layers if `disable_v2_behavior` has been\ncalled.\n\nWhen enabled, the dtype of Keras layers defaults to floatx (which is typically\nfloat32) instead of None. In addition, layers will automatically cast\nfloating-point inputs to the layer's dtype.\n\nA layer author can opt-out their layer from the automatic input casting by\npassing `autocast=False` to the base Layer's constructor. This disables the\nautocasting part of the V2 behavior for that layer, but not the defaulting to\nfloatx part of the V2 behavior.\n\nWhen a global `tf.keras.mixed_precision.Policy` is set, a Keras layer's dtype\nwill default to the global policy instead of floatx. Layers will automatically\ncast inputs to the policy's compute_dtype.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental", "path": "compat/v1/keras/layers/experimental", "type": "tf.compat", "text": "\nPublic API for tf.keras.layers.experimental namespace.\n\n`preprocessing` module: Public API for\ntf.keras.layers.experimental.preprocessing namespace.\n\n`class EinsumDense`: A layer that uses tf.einsum as the backing computation.\n\n`class RandomFourierFeatures`: Layer that projects its inputs into a random\nfeature space.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing", "path": "compat/v1/keras/layers/experimental/preprocessing", "type": "tf.compat", "text": "\nPublic API for tf.keras.layers.experimental.preprocessing namespace.\n\n`class CategoryCrossing`: Category crossing layer.\n\n`class CategoryEncoding`: CategoryEncoding layer.\n\n`class CenterCrop`: Crop the central portion of the images to target height\nand width.\n\n`class Discretization`: Buckets data into discrete ranges.\n\n`class Hashing`: Implements categorical feature hashing, also known as\n\"hashing trick\".\n\n`class IntegerLookup`: Maps integers from a vocabulary to integer indices.\n\n`class Normalization`: Feature-wise normalization of the data.\n\n`class PreprocessingLayer`: Base class for PreprocessingLayers.\n\n`class RandomContrast`: Adjust the contrast of an image or images by a random\nfactor.\n\n`class RandomCrop`: Randomly crop the images to target height and width.\n\n`class RandomFlip`: Randomly flip each image horizontally and vertically.\n\n`class RandomHeight`: Randomly vary the height of a batch of images during\ntraining.\n\n`class RandomRotation`: Randomly rotate each image.\n\n`class RandomTranslation`: Randomly translate each image during training.\n\n`class RandomWidth`: Randomly vary the width of a batch of images during\ntraining.\n\n`class RandomZoom`: Randomly zoom each image during training.\n\n`class Rescaling`: Multiply inputs by `scale` and adds `offset`.\n\n`class Resizing`: Image resizing layer.\n\n`class StringLookup`: Maps strings from a vocabulary to integer indices.\n\n`class TextVectorization`: Text vectorization layer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.CategoryEncoding", "path": "compat/v1/keras/layers/experimental/preprocessing/categoryencoding", "type": "tf.compat", "text": "\nCategoryEncoding layer.\n\nInherits From: `CategoryEncoding`, `PreprocessingLayer`, `Layer`, `Module`\n\nThis layer provides options for condensing input data into denser\nrepresentations. It accepts either integer values or strings as inputs, allows\nusers to map those inputs into a contiguous integer space, and outputs either\nthose integer values (one sample = 1D tensor of integer token indices) or a\ndense representation (one sample = 1D tensor of float values representing data\nabout the sample's tokens).\n\nIf desired, the user can call this layer's adapt() method on a dataset. When\nthis layer is adapted, it will analyze the dataset, determine the frequency of\nindividual integer or string values, and create a 'vocabulary' from them. This\nvocabulary can have unlimited size or be capped, depending on the\nconfiguration options for this layer; if there are more unique values in the\ninput than the maximum vocabulary size, the most frequent terms will be used\nto create the vocabulary.\n\nView source\n\nFits the state of the preprocessing layer to the dataset.\n\nOverrides the default adapt method to apply relevant preprocessing to the\ninputs before passing to the combiner.\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.IntegerLookup", "path": "compat/v1/keras/layers/experimental/preprocessing/integerlookup", "type": "tf.compat", "text": "\nMaps integers from a vocabulary to integer indices.\n\nInherits From: `IntegerLookup`, `PreprocessingLayer`, `Layer`, `Module`\n\nView source\n\nFits the state of the preprocessing layer to the dataset.\n\nOverrides the default adapt method to apply relevant preprocessing to the\ninputs before passing to the combiner.\n\nView source\n\nView source\n\nSets vocabulary data for this layer with inverse=False.\n\nThis method sets the vocabulary for this layer directly, instead of analyzing\na dataset through 'adapt'. It should be used whenever the vocab information is\nalready known. If vocabulary data is already present in the layer, this method\nwill either replace it\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.Normalization", "path": "compat/v1/keras/layers/experimental/preprocessing/normalization", "type": "tf.compat", "text": "\nFeature-wise normalization of the data.\n\nInherits From: `Normalization`, `PreprocessingLayer`, `Layer`, `Module`\n\nThis layer will coerce its inputs into a distribution centered around 0 with\nstandard deviation 1. It accomplishes this by precomputing the mean and\nvariance of the data, and calling (input-mean)/sqrt(var) at runtime.\n\nWhat happens in `adapt`: Compute mean and variance of the data and store them\nas the layer's weights. `adapt` should be called before `fit`, `evaluate`, or\n`predict`.\n\nCalculate the mean and variance by analyzing the dataset in `adapt`.\n\nPass the mean and variance directly.\n\nView source\n\nFits the state of the preprocessing layer to the data being passed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.StringLookup", "path": "compat/v1/keras/layers/experimental/preprocessing/stringlookup", "type": "tf.compat", "text": "\nMaps strings from a vocabulary to integer indices.\n\nInherits From: `StringLookup`, `PreprocessingLayer`, `Layer`, `Module`\n\nView source\n\nFits the state of the preprocessing layer to the dataset.\n\nOverrides the default adapt method to apply relevant preprocessing to the\ninputs before passing to the combiner.\n\nView source\n\nView source\n\nSets vocabulary data for this layer with inverse=False.\n\nThis method sets the vocabulary for this layer directly, instead of analyzing\na dataset through 'adapt'. It should be used whenever the vocab information is\nalready known. If vocabulary data is already present in the layer, this method\nwill either replace it\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.experimental.preprocessing.TextVectorization", "path": "compat/v1/keras/layers/experimental/preprocessing/textvectorization", "type": "tf.compat", "text": "\nText vectorization layer.\n\nInherits From: `TextVectorization`, `PreprocessingLayer`, `Layer`, `Module`\n\nThis layer has basic options for managing text in a Keras model. It transforms\na batch of strings (one sample = one string) into either a list of token\nindices (one sample = 1D tensor of integer token indices) or a dense\nrepresentation (one sample = 1D tensor of float values representing data about\nthe sample's tokens).\n\nThe processing of each sample contains the following steps:\n\n1) standardize each sample (usually lowercasing + punctuation stripping) 2)\nsplit each sample into substrings (usually words) 3) recombine substrings into\ntokens (usually ngrams) 4) index tokens (associate a unique int value with\neach token) 5) transform each sample using this index, either into a vector of\nints or a dense float vector.\n\nView source\n\nFits the state of the preprocessing layer to the dataset.\n\nOverrides the default adapt method to apply relevant preprocessing to the\ninputs before passing to the combiner.\n\nView source\n\nView source\n\nSets vocabulary (and optionally document frequency) data for this layer.\n\nThis method sets the vocabulary and DF data for this layer directly, instead\nof analyzing a dataset through 'adapt'. It should be used whenever the vocab\n(and optionally document frequency) information is already known. If\nvocabulary data is already present in the layer, this method will replace it.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.GRU", "path": "compat/v1/keras/layers/gru", "type": "tf.compat", "text": "\nGated Recurrent Unit - Cho et al. 2014.\n\nInherits From: `RNN`, `Layer`, `Module`\n\nThere are two variants. The default one is based on 1406.1078v3 and has reset\ngate applied to hidden state before matrix multiplication. The other one is\nbased on original 1406.1078v1 and has the order reversed.\n\nThe second variant is compatible with CuDNNGRU (GPU-only) and allows inference\non CPU. Thus it has separate biases for `kernel` and `recurrent_kernel`. Use\n`'reset_after'=True` and `recurrent_activation='sigmoid'`.\n\nView source\n\nReset the recorded states for the stateful RNN layer.\n\nCan only be used when RNN layer is constructed with `stateful` = `True`. Args:\nstates: Numpy arrays that contains the value for the initial state, which will\nbe feed to cell at the first time step. When the value is None, zero filled\nnumpy array will be created based on the cell state size.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.GRUCell", "path": "compat/v1/keras/layers/grucell", "type": "tf.compat", "text": "\nCell class for the GRU layer.\n\nInherits From: `Layer`, `Module`\n\nView source\n\nGet the dropout mask for RNN cell's input.\n\nIt will create mask based on context if there isn't any existing cached mask.\nIf a new mask is generated, it will update the cache in the cell.\n\nView source\n\nView source\n\nGet the recurrent dropout mask for RNN cell.\n\nIt will create mask based on context if there isn't any existing cached mask.\nIf a new mask is generated, it will update the cache in the cell.\n\nView source\n\nReset the cached dropout masks if any.\n\nThis is important for the RNN layer to invoke this in it `call()` method so\nthat the cached mask is cleared before calling the `cell.call()`. The mask\nshould be cached across the timestep within the same batch, but shouldn't be\ncached between batches. Otherwise it will introduce unreasonable bias against\ncertain index of data within the batch.\n\nView source\n\nReset the cached recurrent dropout masks if any.\n\nThis is important for the RNN layer to invoke this in it call() method so that\nthe cached mask is cleared before calling the cell.call(). The mask should be\ncached across the timestep within the same batch, but shouldn't be cached\nbetween batches. Otherwise it will introduce unreasonable bias against certain\nindex of data within the batch.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.LSTM", "path": "compat/v1/keras/layers/lstm", "type": "tf.compat", "text": "\nLong Short-Term Memory layer - Hochreiter 1997.\n\nInherits From: `RNN`, `Layer`, `Module`\n\nNote that this cell is not optimized for performance on GPU. Please use\n`tf.compat.v1.keras.layers.CuDNNLSTM` for better performance on GPU.\n\nView source\n\nReset the recorded states for the stateful RNN layer.\n\nCan only be used when RNN layer is constructed with `stateful` = `True`. Args:\nstates: Numpy arrays that contains the value for the initial state, which will\nbe feed to cell at the first time step. When the value is None, zero filled\nnumpy array will be created based on the cell state size.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.layers.LSTMCell", "path": "compat/v1/keras/layers/lstmcell", "type": "tf.compat", "text": "\nCell class for the LSTM layer.\n\nInherits From: `Layer`, `Module`\n\nView source\n\nGet the dropout mask for RNN cell's input.\n\nIt will create mask based on context if there isn't any existing cached mask.\nIf a new mask is generated, it will update the cache in the cell.\n\nView source\n\nView source\n\nGet the recurrent dropout mask for RNN cell.\n\nIt will create mask based on context if there isn't any existing cached mask.\nIf a new mask is generated, it will update the cache in the cell.\n\nView source\n\nReset the cached dropout masks if any.\n\nThis is important for the RNN layer to invoke this in it `call()` method so\nthat the cached mask is cleared before calling the `cell.call()`. The mask\nshould be cached across the timestep within the same batch, but shouldn't be\ncached between batches. Otherwise it will introduce unreasonable bias against\ncertain index of data within the batch.\n\nView source\n\nReset the cached recurrent dropout masks if any.\n\nThis is important for the RNN layer to invoke this in it call() method so that\nthe cached mask is cleared before calling the cell.call(). The mask should be\ncached across the timestep within the same batch, but shouldn't be cached\nbetween batches. Otherwise it will introduce unreasonable bias against certain\nindex of data within the batch.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.losses", "path": "compat/v1/keras/losses", "type": "tf.compat", "text": "\nBuilt-in loss functions.\n\n`class BinaryCrossentropy`: Computes the cross-entropy loss between true\nlabels and predicted labels.\n\n`class CategoricalCrossentropy`: Computes the crossentropy loss between the\nlabels and predictions.\n\n`class CategoricalHinge`: Computes the categorical hinge loss between `y_true`\nand `y_pred`.\n\n`class CosineSimilarity`: Computes the cosine similarity between labels and\npredictions.\n\n`class Hinge`: Computes the hinge loss between `y_true` and `y_pred`.\n\n`class Huber`: Computes the Huber loss between `y_true` and `y_pred`.\n\n`class KLDivergence`: Computes Kullback-Leibler divergence loss between\n`y_true` and `y_pred`.\n\n`class LogCosh`: Computes the logarithm of the hyperbolic cosine of the\nprediction error.\n\n`class Loss`: Loss base class.\n\n`class MeanAbsoluteError`: Computes the mean of absolute difference between\nlabels and predictions.\n\n`class MeanAbsolutePercentageError`: Computes the mean absolute percentage\nerror between `y_true` and `y_pred`.\n\n`class MeanSquaredError`: Computes the mean of squares of errors between\nlabels and predictions.\n\n`class MeanSquaredLogarithmicError`: Computes the mean squared logarithmic\nerror between `y_true` and `y_pred`.\n\n`class Poisson`: Computes the Poisson loss between `y_true` and `y_pred`.\n\n`class SparseCategoricalCrossentropy`: Computes the crossentropy loss between\nthe labels and predictions.\n\n`class SquaredHinge`: Computes the squared hinge loss between `y_true` and\n`y_pred`.\n\n`KLD(...)`: Computes Kullback-Leibler divergence loss between `y_true` and\n`y_pred`.\n\n`MAE(...)`: Computes the mean absolute error between labels and predictions.\n\n`MAPE(...)`: Computes the mean absolute percentage error between `y_true` and\n`y_pred`.\n\n`MSE(...)`: Computes the mean squared error between labels and predictions.\n\n`MSLE(...)`: Computes the mean squared logarithmic error between `y_true` and\n`y_pred`.\n\n`binary_crossentropy(...)`: Computes the binary crossentropy loss.\n\n`categorical_crossentropy(...)`: Computes the categorical crossentropy loss.\n\n`categorical_hinge(...)`: Computes the categorical hinge loss between `y_true`\nand `y_pred`.\n\n`cosine(...)`: Computes the cosine similarity between labels and predictions.\n\n`cosine_proximity(...)`: Computes the cosine similarity between labels and\npredictions.\n\n`cosine_similarity(...)`: Computes the cosine similarity between labels and\npredictions.\n\n`deserialize(...)`: Deserializes a serialized loss class/function instance.\n\n`get(...)`: Retrieves a Keras loss as a `function`/`Loss` class instance.\n\n`hinge(...)`: Computes the hinge loss between `y_true` and `y_pred`.\n\n`kl_divergence(...)`: Computes Kullback-Leibler divergence loss between\n`y_true` and `y_pred`.\n\n`kld(...)`: Computes Kullback-Leibler divergence loss between `y_true` and\n`y_pred`.\n\n`kullback_leibler_divergence(...)`: Computes Kullback-Leibler divergence loss\nbetween `y_true` and `y_pred`.\n\n`log_cosh(...)`: Logarithm of the hyperbolic cosine of the prediction error.\n\n`logcosh(...)`: Logarithm of the hyperbolic cosine of the prediction error.\n\n`mae(...)`: Computes the mean absolute error between labels and predictions.\n\n`mape(...)`: Computes the mean absolute percentage error between `y_true` and\n`y_pred`.\n\n`mean_absolute_error(...)`: Computes the mean absolute error between labels\nand predictions.\n\n`mean_absolute_percentage_error(...)`: Computes the mean absolute percentage\nerror between `y_true` and `y_pred`.\n\n`mean_squared_error(...)`: Computes the mean squared error between labels and\npredictions.\n\n`mean_squared_logarithmic_error(...)`: Computes the mean squared logarithmic\nerror between `y_true` and `y_pred`.\n\n`mse(...)`: Computes the mean squared error between labels and predictions.\n\n`msle(...)`: Computes the mean squared logarithmic error between `y_true` and\n`y_pred`.\n\n`poisson(...)`: Computes the Poisson loss between y_true and y_pred.\n\n`serialize(...)`: Serializes loss function or `Loss` instance.\n\n`sparse_categorical_crossentropy(...)`: Computes the sparse categorical\ncrossentropy loss.\n\n`squared_hinge(...)`: Computes the squared hinge loss between `y_true` and\n`y_pred`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.metrics", "path": "compat/v1/keras/metrics", "type": "tf.compat", "text": "\nBuilt-in metrics.\n\n`class AUC`: Computes the approximate AUC (Area under the curve) via a Riemann\nsum.\n\n`class Accuracy`: Calculates how often predictions equal labels.\n\n`class BinaryAccuracy`: Calculates how often predictions match binary labels.\n\n`class BinaryCrossentropy`: Computes the crossentropy metric between the\nlabels and predictions.\n\n`class CategoricalAccuracy`: Calculates how often predictions matches one-hot\nlabels.\n\n`class CategoricalCrossentropy`: Computes the crossentropy metric between the\nlabels and predictions.\n\n`class CategoricalHinge`: Computes the categorical hinge metric between\n`y_true` and `y_pred`.\n\n`class CosineSimilarity`: Computes the cosine similarity between the labels\nand predictions.\n\n`class FalseNegatives`: Calculates the number of false negatives.\n\n`class FalsePositives`: Calculates the number of false positives.\n\n`class Hinge`: Computes the hinge metric between `y_true` and `y_pred`.\n\n`class KLDivergence`: Computes Kullback-Leibler divergence metric between\n`y_true` and `y_pred`.\n\n`class LogCoshError`: Computes the logarithm of the hyperbolic cosine of the\nprediction error.\n\n`class Mean`: Computes the (weighted) mean of the given values.\n\n`class MeanAbsoluteError`: Computes the mean absolute error between the labels\nand predictions.\n\n`class MeanAbsolutePercentageError`: Computes the mean absolute percentage\nerror between `y_true` and `y_pred`.\n\n`class MeanIoU`: Computes the mean Intersection-Over-Union metric.\n\n`class MeanRelativeError`: Computes the mean relative error by normalizing\nwith the given values.\n\n`class MeanSquaredError`: Computes the mean squared error between `y_true` and\n`y_pred`.\n\n`class MeanSquaredLogarithmicError`: Computes the mean squared logarithmic\nerror between `y_true` and `y_pred`.\n\n`class MeanTensor`: Computes the element-wise (weighted) mean of the given\ntensors.\n\n`class Metric`: Encapsulates metric logic and state.\n\n`class Poisson`: Computes the Poisson metric between `y_true` and `y_pred`.\n\n`class Precision`: Computes the precision of the predictions with respect to\nthe labels.\n\n`class PrecisionAtRecall`: Computes best precision where recall is >=\nspecified value.\n\n`class Recall`: Computes the recall of the predictions with respect to the\nlabels.\n\n`class RecallAtPrecision`: Computes best recall where precision is >=\nspecified value.\n\n`class RootMeanSquaredError`: Computes root mean squared error metric between\n`y_true` and `y_pred`.\n\n`class SensitivityAtSpecificity`: Computes best sensitivity where specificity\nis >= specified value.\n\n`class SparseCategoricalAccuracy`: Calculates how often predictions matches\ninteger labels.\n\n`class SparseCategoricalCrossentropy`: Computes the crossentropy metric\nbetween the labels and predictions.\n\n`class SparseTopKCategoricalAccuracy`: Computes how often integer targets are\nin the top `K` predictions.\n\n`class SpecificityAtSensitivity`: Computes best specificity where sensitivity\nis >= specified value.\n\n`class SquaredHinge`: Computes the squared hinge metric between `y_true` and\n`y_pred`.\n\n`class Sum`: Computes the (weighted) sum of the given values.\n\n`class TopKCategoricalAccuracy`: Computes how often targets are in the top `K`\npredictions.\n\n`class TrueNegatives`: Calculates the number of true negatives.\n\n`class TruePositives`: Calculates the number of true positives.\n\n`KLD(...)`: Computes Kullback-Leibler divergence loss between `y_true` and\n`y_pred`.\n\n`MAE(...)`: Computes the mean absolute error between labels and predictions.\n\n`MAPE(...)`: Computes the mean absolute percentage error between `y_true` and\n`y_pred`.\n\n`MSE(...)`: Computes the mean squared error between labels and predictions.\n\n`MSLE(...)`: Computes the mean squared logarithmic error between `y_true` and\n`y_pred`.\n\n`binary_accuracy(...)`: Calculates how often predictions matches binary\nlabels.\n\n`binary_crossentropy(...)`: Computes the binary crossentropy loss.\n\n`categorical_accuracy(...)`: Calculates how often predictions matches one-hot\nlabels.\n\n`categorical_crossentropy(...)`: Computes the categorical crossentropy loss.\n\n`cosine(...)`: Computes the cosine similarity between labels and predictions.\n\n`cosine_proximity(...)`: Computes the cosine similarity between labels and\npredictions.\n\n`deserialize(...)`: Deserializes a serialized metric class/function instance.\n\n`get(...)`: Retrieves a Keras metric as a `function`/`Metric` class instance.\n\n`hinge(...)`: Computes the hinge loss between `y_true` and `y_pred`.\n\n`kl_divergence(...)`: Computes Kullback-Leibler divergence loss between\n`y_true` and `y_pred`.\n\n`kld(...)`: Computes Kullback-Leibler divergence loss between `y_true` and\n`y_pred`.\n\n`kullback_leibler_divergence(...)`: Computes Kullback-Leibler divergence loss\nbetween `y_true` and `y_pred`.\n\n`log_cosh(...)`: Logarithm of the hyperbolic cosine of the prediction error.\n\n`logcosh(...)`: Logarithm of the hyperbolic cosine of the prediction error.\n\n`mae(...)`: Computes the mean absolute error between labels and predictions.\n\n`mape(...)`: Computes the mean absolute percentage error between `y_true` and\n`y_pred`.\n\n`mean_absolute_error(...)`: Computes the mean absolute error between labels\nand predictions.\n\n`mean_absolute_percentage_error(...)`: Computes the mean absolute percentage\nerror between `y_true` and `y_pred`.\n\n`mean_squared_error(...)`: Computes the mean squared error between labels and\npredictions.\n\n`mean_squared_logarithmic_error(...)`: Computes the mean squared logarithmic\nerror between `y_true` and `y_pred`.\n\n`mse(...)`: Computes the mean squared error between labels and predictions.\n\n`msle(...)`: Computes the mean squared logarithmic error between `y_true` and\n`y_pred`.\n\n`poisson(...)`: Computes the Poisson loss between y_true and y_pred.\n\n`serialize(...)`: Serializes metric function or `Metric` instance.\n\n`sparse_categorical_accuracy(...)`: Calculates how often predictions matches\ninteger labels.\n\n`sparse_categorical_crossentropy(...)`: Computes the sparse categorical\ncrossentropy loss.\n\n`sparse_top_k_categorical_accuracy(...)`: Computes how often integer targets\nare in the top `K` predictions.\n\n`squared_hinge(...)`: Computes the squared hinge loss between `y_true` and\n`y_pred`.\n\n`top_k_categorical_accuracy(...)`: Computes how often targets are in the top\n`K` predictions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.mixed_precision", "path": "compat/v1/keras/mixed_precision", "type": "tf.compat", "text": "\nKeras mixed precision API.\n\nSee the mixed precision guide to learn how to use the API.\n\n`experimental` module: Public API for tf.keras.mixed_precision.experimental\nnamespace.\n\n`class LossScaleOptimizer`: An optimizer that applies loss scaling to prevent\nnumeric underflow.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.mixed_precision.experimental", "path": "compat/v1/keras/mixed_precision/experimental", "type": "tf.compat", "text": "\nPublic API for tf.keras.mixed_precision.experimental namespace.\n\n`class LossScaleOptimizer`: An deprecated optimizer that applies loss scaling.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.models", "path": "compat/v1/keras/models", "type": "tf.compat", "text": "\nCode for model cloning, plus model-related API entries.\n\n`class Model`: `Model` groups layers into an object with training and\ninference features.\n\n`class Sequential`: `Sequential` groups a linear stack of layers into a\n`tf.keras.Model`.\n\n`clone_model(...)`: Clone any `Model` instance.\n\n`load_model(...)`: Loads a model saved via `model.save()`.\n\n`model_from_config(...)`: Instantiates a Keras model from its config.\n\n`model_from_json(...)`: Parses a JSON model configuration string and returns a\nmodel instance.\n\n`model_from_yaml(...)`: Parses a yaml model configuration file and returns a\nmodel instance.\n\n`save_model(...)`: Saves a model as a TensorFlow SavedModel or HDF5 file.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.optimizers", "path": "compat/v1/keras/optimizers", "type": "tf.compat", "text": "\nBuilt-in optimizer classes.\n\nFor more examples see the base class `tf.keras.optimizers.Optimizer`.\n\n`schedules` module: Public API for tf.keras.optimizers.schedules namespace.\n\n`class Adadelta`: Optimizer that implements the Adadelta algorithm.\n\n`class Adagrad`: Optimizer that implements the Adagrad algorithm.\n\n`class Adam`: Optimizer that implements the Adam algorithm.\n\n`class Adamax`: Optimizer that implements the Adamax algorithm.\n\n`class Ftrl`: Optimizer that implements the FTRL algorithm.\n\n`class Nadam`: Optimizer that implements the NAdam algorithm.\n\n`class Optimizer`: Base class for Keras optimizers.\n\n`class RMSprop`: Optimizer that implements the RMSprop algorithm.\n\n`class SGD`: Gradient descent (with momentum) optimizer.\n\n`deserialize(...)`: Inverse of the `serialize` function.\n\n`get(...)`: Retrieves a Keras Optimizer instance.\n\n`serialize(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.optimizers.schedules", "path": "compat/v1/keras/optimizers/schedules", "type": "tf.compat", "text": "\nPublic API for tf.keras.optimizers.schedules namespace.\n\n`class ExponentialDecay`: A LearningRateSchedule that uses an exponential\ndecay schedule.\n\n`class InverseTimeDecay`: A LearningRateSchedule that uses an inverse time\ndecay schedule.\n\n`class LearningRateSchedule`: A serializable learning rate decay schedule.\n\n`class PiecewiseConstantDecay`: A LearningRateSchedule that uses a piecewise\nconstant decay schedule.\n\n`class PolynomialDecay`: A LearningRateSchedule that uses a polynomial decay\nschedule.\n\n`deserialize(...)`\n\n`serialize(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.preprocessing", "path": "compat/v1/keras/preprocessing", "type": "tf.compat", "text": "\nKeras data preprocessing utils.\n\n`image` module: Set of tools for real-time data augmentation on image data.\n\n`sequence` module: Utilities for preprocessing sequence data.\n\n`text` module: Utilities for text input preprocessing.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.preprocessing.image", "path": "compat/v1/keras/preprocessing/image", "type": "tf.compat", "text": "\nSet of tools for real-time data augmentation on image data.\n\n`class DirectoryIterator`: Iterator capable of reading images from a directory\non disk.\n\n`class ImageDataGenerator`: Generate batches of tensor image data with real-\ntime data augmentation.\n\n`class Iterator`: Base class for image data iterators.\n\n`class NumpyArrayIterator`: Iterator yielding data from a Numpy array.\n\n`apply_affine_transform(...)`: Applies an affine transformation specified by\nthe parameters given.\n\n`apply_brightness_shift(...)`: Performs a brightness shift.\n\n`apply_channel_shift(...)`: Performs a channel shift.\n\n`array_to_img(...)`: Converts a 3D Numpy array to a PIL Image instance.\n\n`img_to_array(...)`: Converts a PIL Image instance to a Numpy array.\n\n`load_img(...)`: Loads an image into PIL format.\n\n`random_brightness(...)`: Performs a random brightness shift.\n\n`random_channel_shift(...)`: Performs a random channel shift.\n\n`random_rotation(...)`: Performs a random rotation of a Numpy image tensor.\n\n`random_shear(...)`: Performs a random spatial shear of a Numpy image tensor.\n\n`random_shift(...)`: Performs a random spatial shift of a Numpy image tensor.\n\n`random_zoom(...)`: Performs a random spatial zoom of a Numpy image tensor.\n\n`save_img(...)`: Saves an image stored as a Numpy array to a path or file\nobject.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.preprocessing.sequence", "path": "compat/v1/keras/preprocessing/sequence", "type": "tf.compat", "text": "\nUtilities for preprocessing sequence data.\n\n`class TimeseriesGenerator`: Utility class for generating batches of temporal\ndata.\n\n`make_sampling_table(...)`: Generates a word rank-based probabilistic sampling\ntable.\n\n`pad_sequences(...)`: Pads sequences to the same length.\n\n`skipgrams(...)`: Generates skipgram word pairs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.preprocessing.text", "path": "compat/v1/keras/preprocessing/text", "type": "tf.compat", "text": "\nUtilities for text input preprocessing.\n\n`class Tokenizer`: Text tokenization utility class.\n\n`hashing_trick(...)`: Converts a text to a sequence of indexes in a fixed-size\nhashing space.\n\n`one_hot(...)`: One-hot encodes a text into a list of word indexes of size\n`n`.\n\n`text_to_word_sequence(...)`: Converts a text to a sequence of words (or\ntokens).\n\n`tokenizer_from_json(...)`: Parses a JSON tokenizer configuration file and\nreturns a\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.regularizers", "path": "compat/v1/keras/regularizers", "type": "tf.compat", "text": "\nBuilt-in regularizers.\n\n`class L1`: A regularizer that applies a L1 regularization penalty.\n\n`class L1L2`: A regularizer that applies both L1 and L2 regularization\npenalties.\n\n`class L2`: A regularizer that applies a L2 regularization penalty.\n\n`class Regularizer`: Regularizer base class.\n\n`class l1`: A regularizer that applies a L1 regularization penalty.\n\n`class l2`: A regularizer that applies a L2 regularization penalty.\n\n`deserialize(...)`\n\n`get(...)`: Retrieve a regularizer instance from a config or identifier.\n\n`l1_l2(...)`: Create a regularizer that applies both L1 and L2 penalties.\n\n`serialize(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.utils", "path": "compat/v1/keras/utils", "type": "tf.compat", "text": "\nPublic API for tf.keras.utils namespace.\n\n`class CustomObjectScope`: Exposes custom classes/functions to Keras\ndeserialization internals.\n\n`class GeneratorEnqueuer`: Builds a queue out of a data generator.\n\n`class OrderedEnqueuer`: Builds a Enqueuer from a Sequence.\n\n`class Progbar`: Displays a progress bar.\n\n`class Sequence`: Base object for fitting to a sequence of data, such as a\ndataset.\n\n`class SequenceEnqueuer`: Base class to enqueue inputs.\n\n`class custom_object_scope`: Exposes custom classes/functions to Keras\ndeserialization internals.\n\n`deserialize_keras_object(...)`: Turns the serialized form of a Keras object\nback into an actual object.\n\n`get_custom_objects(...)`: Retrieves a live reference to the global dictionary\nof custom objects.\n\n`get_file(...)`: Downloads a file from a URL if it not already in the cache.\n\n`get_registered_name(...)`: Returns the name registered to an object within\nthe Keras framework.\n\n`get_registered_object(...)`: Returns the class associated with `name` if it\nis registered with Keras.\n\n`get_source_inputs(...)`: Returns the list of input tensors necessary to\ncompute `tensor`.\n\n`model_to_dot(...)`: Convert a Keras model to dot format.\n\n`normalize(...)`: Normalizes a Numpy array.\n\n`plot_model(...)`: Converts a Keras model to dot format and save to a file.\n\n`register_keras_serializable(...)`: Registers an object with the Keras\nserialization framework.\n\n`serialize_keras_object(...)`: Serialize a Keras object into a JSON-compatible\nrepresentation.\n\n`to_categorical(...)`: Converts a class vector (integers) to binary class\nmatrix.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.wrappers", "path": "compat/v1/keras/wrappers", "type": "tf.compat", "text": "\nPublic API for tf.keras.wrappers namespace.\n\n`scikit_learn` module: Wrapper for using the Scikit-Learn API with Keras\nmodels.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.keras.wrappers.scikit_learn", "path": "compat/v1/keras/wrappers/scikit_learn", "type": "tf.compat", "text": "\nWrapper for using the Scikit-Learn API with Keras models.\n\n`class KerasClassifier`: Implementation of the scikit-learn classifier API for\nKeras.\n\n`class KerasRegressor`: Implementation of the scikit-learn regressor API for\nKeras.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers", "path": "compat/v1/layers", "type": "tf.compat", "text": "\nPublic API for tf.layers namespace.\n\n`experimental` module: Public API for tf.layers.experimental namespace.\n\n`class AveragePooling1D`: Average Pooling layer for 1D inputs.\n\n`class AveragePooling2D`: Average pooling layer for 2D inputs (e.g. images).\n\n`class AveragePooling3D`: Average pooling layer for 3D inputs (e.g. volumes).\n\n`class BatchNormalization`: Batch Normalization layer from (Ioffe et al.,\n2015).\n\n`class Conv1D`: 1D convolution layer (e.g. temporal convolution).\n\n`class Conv2D`: 2D convolution layer (e.g. spatial convolution over images).\n\n`class Conv2DTranspose`: Transposed 2D convolution layer (sometimes called 2D\nDeconvolution).\n\n`class Conv3D`: 3D convolution layer (e.g. spatial convolution over volumes).\n\n`class Conv3DTranspose`: Transposed 3D convolution layer (sometimes called 3D\nDeconvolution).\n\n`class Dense`: Densely-connected layer class.\n\n`class Dropout`: Applies Dropout to the input.\n\n`class Flatten`: Flattens an input tensor while preserving the batch axis\n(axis 0).\n\n`class InputSpec`: Specifies the rank, dtype and shape of every input to a\nlayer.\n\n`class Layer`: Base layer class.\n\n`class MaxPooling1D`: Max Pooling layer for 1D inputs.\n\n`class MaxPooling2D`: Max pooling layer for 2D inputs (e.g. images).\n\n`class MaxPooling3D`: Max pooling layer for 3D inputs (e.g. volumes).\n\n`class SeparableConv1D`: Depthwise separable 1D convolution.\n\n`class SeparableConv2D`: Depthwise separable 2D convolution.\n\n`average_pooling1d(...)`: Average Pooling layer for 1D inputs.\n\n`average_pooling2d(...)`: Average pooling layer for 2D inputs (e.g. images).\n\n`average_pooling3d(...)`: Average pooling layer for 3D inputs (e.g. volumes).\n\n`batch_normalization(...)`: Functional interface for the batch normalization\nlayer from_config(Ioffe et al., 2015).\n\n`conv1d(...)`: Functional interface for 1D convolution layer (e.g. temporal\nconvolution).\n\n`conv2d(...)`: Functional interface for the 2D convolution layer.\n\n`conv2d_transpose(...)`: Functional interface for transposed 2D convolution\nlayer.\n\n`conv3d(...)`: Functional interface for the 3D convolution layer.\n\n`conv3d_transpose(...)`: Functional interface for transposed 3D convolution\nlayer.\n\n`dense(...)`: Functional interface for the densely-connected layer.\n\n`dropout(...)`: Applies Dropout to the input.\n\n`flatten(...)`: Flattens an input tensor while preserving the batch axis (axis\n0).\n\n`max_pooling1d(...)`: Max Pooling layer for 1D inputs.\n\n`max_pooling2d(...)`: Max pooling layer for 2D inputs (e.g. images).\n\n`max_pooling3d(...)`: Max pooling layer for 3D inputs (e.g.\n\n`separable_conv1d(...)`: Functional interface for the depthwise separable 1D\nconvolution layer.\n\n`separable_conv2d(...)`: Functional interface for the depthwise separable 2D\nconvolution layer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.AveragePooling1D", "path": "compat/v1/layers/averagepooling1d", "type": "tf.compat", "text": "\nAverage Pooling layer for 1D inputs.\n\nInherits From: `AveragePooling1D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.AveragePooling2D", "path": "compat/v1/layers/averagepooling2d", "type": "tf.compat", "text": "\nAverage pooling layer for 2D inputs (e.g. images).\n\nInherits From: `AveragePooling2D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.AveragePooling3D", "path": "compat/v1/layers/averagepooling3d", "type": "tf.compat", "text": "\nAverage pooling layer for 3D inputs (e.g. volumes).\n\nInherits From: `AveragePooling3D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.average_pooling1d", "path": "compat/v1/layers/average_pooling1d", "type": "tf.compat", "text": "\nAverage Pooling layer for 1D inputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.average_pooling2d", "path": "compat/v1/layers/average_pooling2d", "type": "tf.compat", "text": "\nAverage pooling layer for 2D inputs (e.g. images).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.average_pooling3d", "path": "compat/v1/layers/average_pooling3d", "type": "tf.compat", "text": "\nAverage pooling layer for 3D inputs (e.g. volumes).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.BatchNormalization", "path": "compat/v1/layers/batchnormalization", "type": "tf.compat", "text": "\nBatch Normalization layer from (Ioffe et al., 2015).\n\nInherits From: `BatchNormalization`, `Layer`, `Layer`, `Module`\n\nKeras APIs handle BatchNormalization updates to the moving_mean and\nmoving_variance as part of their `fit()` and `evaluate()` loops. However, if a\ncustom training loop is used with an instance of `Model`, these updates need\nto be explicitly included. Here's a simple example of how it can be done:\n\nBatch Normalization - Accelerating Deep Network Training by Reducing Internal\nCovariate Shift: Ioffe et al., 2015 (pdf) Batch Renormalization - Towards\nReducing Minibatch Dependence in Batch-Normalized Models: Ioffe, 2017 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.batch_normalization", "path": "compat/v1/layers/batch_normalization", "type": "tf.compat", "text": "\nFunctional interface for the batch normalization layer from_config(Ioffe et\nal., 2015).\n\nBatch Normalization - Accelerating Deep Network Training by Reducing Internal\nCovariate Shift: Ioffe et al., 2015 (pdf) Batch Renormalization - Towards\nReducing Minibatch Dependence in Batch-Normalized Models: Ioffe, 2017 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Conv1D", "path": "compat/v1/layers/conv1d", "type": "tf.compat", "text": "\n1D convolution layer (e.g. temporal convolution).\n\nInherits From: `Conv1D`, `Layer`, `Layer`, `Module`\n\nThis layer creates a convolution kernel that is convolved (actually cross-\ncorrelated) with the layer input to produce a tensor of outputs. If `use_bias`\nis True (and a `bias_initializer` is provided), a bias vector is created and\nadded to the outputs. Finally, if `activation` is not `None`, it is applied to\nthe outputs as well.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Conv2D", "path": "compat/v1/layers/conv2d", "type": "tf.compat", "text": "\n2D convolution layer (e.g. spatial convolution over images).\n\nInherits From: `Conv2D`, `Layer`, `Layer`, `Module`\n\nThis layer creates a convolution kernel that is convolved (actually cross-\ncorrelated) with the layer input to produce a tensor of outputs. If `use_bias`\nis True (and a `bias_initializer` is provided), a bias vector is created and\nadded to the outputs. Finally, if `activation` is not `None`, it is applied to\nthe outputs as well.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Conv2DTranspose", "path": "compat/v1/layers/conv2dtranspose", "type": "tf.compat", "text": "\nTransposed 2D convolution layer (sometimes called 2D Deconvolution).\n\nInherits From: `Conv2DTranspose`, `Conv2D`, `Layer`, `Layer`, `Module`\n\nThe need for transposed convolutions generally arises from the desire to use a\ntransformation going in the opposite direction of a normal convolution, i.e.,\nfrom something that has the shape of the output of some convolution to\nsomething that has the shape of its input while maintaining a connectivity\npattern that is compatible with said convolution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.conv2d_transpose", "path": "compat/v1/layers/conv2d_transpose", "type": "tf.compat", "text": "\nFunctional interface for transposed 2D convolution layer.\n\nThe need for transposed convolutions generally arises from the desire to use a\ntransformation going in the opposite direction of a normal convolution, i.e.,\nfrom something that has the shape of the output of some convolution to\nsomething that has the shape of its input while maintaining a connectivity\npattern that is compatible with said convolution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Conv3D", "path": "compat/v1/layers/conv3d", "type": "tf.compat", "text": "\n3D convolution layer (e.g. spatial convolution over volumes).\n\nInherits From: `Conv3D`, `Layer`, `Layer`, `Module`\n\nThis layer creates a convolution kernel that is convolved (actually cross-\ncorrelated) with the layer input to produce a tensor of outputs. If `use_bias`\nis True (and a `bias_initializer` is provided), a bias vector is created and\nadded to the outputs. Finally, if `activation` is not `None`, it is applied to\nthe outputs as well.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Conv3DTranspose", "path": "compat/v1/layers/conv3dtranspose", "type": "tf.compat", "text": "\nTransposed 3D convolution layer (sometimes called 3D Deconvolution).\n\nInherits From: `Conv3DTranspose`, `Conv3D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.conv3d_transpose", "path": "compat/v1/layers/conv3d_transpose", "type": "tf.compat", "text": "\nFunctional interface for transposed 3D convolution layer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Dense", "path": "compat/v1/layers/dense", "type": "tf.compat", "text": "\nDensely-connected layer class.\n\nInherits From: `Dense`, `Layer`, `Layer`, `Module`\n\nThis layer implements the operation: `outputs = activation(inputs * kernel +\nbias)` Where `activation` is the activation function passed as the\n`activation` argument (if not `None`), `kernel` is a weights matrix created by\nthe layer, and `bias` is a bias vector created by the layer (only if\n`use_bias` is `True`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Dropout", "path": "compat/v1/layers/dropout", "type": "tf.compat", "text": "\nApplies Dropout to the input.\n\nInherits From: `Dropout`, `Layer`, `Layer`, `Module`\n\nDropout consists in randomly setting a fraction `rate` of input units to 0 at\neach update during training time, which helps prevent overfitting. The units\nthat are kept are scaled by `1 / (1 - rate)`, so that their sum is unchanged\nat training time and inference time.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.experimental", "path": "compat/v1/layers/experimental", "type": "tf.compat", "text": "\nPublic API for tf.layers.experimental namespace.\n\n`keras_style_scope(...)`: Use Keras-style variable management.\n\n`set_keras_style(...)`: Use Keras-style variable management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.experimental.keras_style_scope", "path": "compat/v1/layers/experimental/keras_style_scope", "type": "tf.compat", "text": "\nUse Keras-style variable management.\n\nAll tf.layers and tf RNN cells created in this scope use Keras-style variable\nmanagement. Creating such layers with a scope= argument is disallowed, and\nreuse=True is disallowed.\n\nThe purpose of this scope is to allow users of existing layers to slowly\ntransition to a Keras layers API without breaking existing functionality.\n\nOne example of this is when using TensorFlow's RNN classes with Keras Models\nor Networks. Because Keras models do not properly set variable scopes, users\nof RNNs may either accidentally share scopes between two different models, or\nget errors about variables that already exist.\n\nThe solution is to wrap the model construction and execution in a keras-style\nscope:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.experimental.set_keras_style", "path": "compat/v1/layers/experimental/set_keras_style", "type": "tf.compat", "text": "\nUse Keras-style variable management.\n\nAll tf.layers and tf RNN cells created after keras style ha been enabled use\nKeras-style variable management. Creating such layers with a scope= argument\nis disallowed, and reuse=True is disallowed.\n\nThe purpose of this function is to allow users of existing layers to slowly\ntransition to Keras layers API without breaking existing functionality.\n\nFor more details, see the documentation for `keras_style_scope`.\n\nNote, once keras style has been set, it is set globally for the entire program\nand cannot be unset.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Flatten", "path": "compat/v1/layers/flatten", "type": "tf.compat", "text": "\nFlattens an input tensor while preserving the batch axis (axis 0).\n\nInherits From: `Flatten`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.Layer", "path": "compat/v1/layers/layer", "type": "tf.compat", "text": "\nBase layer class.\n\nInherits From: `Layer`, `Module`\n\nIt is considered legacy, and we recommend the use of `tf.keras.layers.Layer`\ninstead.\n\nRead-only properties: name: The name of the layer (string). dtype: Default\ndtype of the layer's weights (default of `None` means use the type of the\nfirst input). trainable_variables: List of trainable variables.\nnon_trainable_variables: List of non-trainable variables. variables: List of\nall variables of this layer, trainable and non-trainable. updates: List of\nupdate ops of this layer. losses: List of losses added by this layer.\ntrainable_weights: List of variables to be included in backprop.\nnon_trainable_weights: List of variables that should not be included in\nbackprop. weights: The concatenation of the lists trainable_weights and\nnon_trainable_weights (in this order).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.MaxPooling1D", "path": "compat/v1/layers/maxpooling1d", "type": "tf.compat", "text": "\nMax Pooling layer for 1D inputs.\n\nInherits From: `MaxPool1D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.MaxPooling2D", "path": "compat/v1/layers/maxpooling2d", "type": "tf.compat", "text": "\nMax pooling layer for 2D inputs (e.g. images).\n\nInherits From: `MaxPool2D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.MaxPooling3D", "path": "compat/v1/layers/maxpooling3d", "type": "tf.compat", "text": "\nMax pooling layer for 3D inputs (e.g. volumes).\n\nInherits From: `MaxPool3D`, `Layer`, `Layer`, `Module`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.max_pooling1d", "path": "compat/v1/layers/max_pooling1d", "type": "tf.compat", "text": "\nMax Pooling layer for 1D inputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.max_pooling2d", "path": "compat/v1/layers/max_pooling2d", "type": "tf.compat", "text": "\nMax pooling layer for 2D inputs (e.g. images).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.max_pooling3d", "path": "compat/v1/layers/max_pooling3d", "type": "tf.compat", "text": "\nMax pooling layer for 3D inputs (e.g.\n\nvolumes).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.SeparableConv1D", "path": "compat/v1/layers/separableconv1d", "type": "tf.compat", "text": "\nDepthwise separable 1D convolution.\n\nInherits From: `SeparableConv1D`, `Layer`, `Layer`, `Module`\n\nThis layer performs a depthwise convolution that acts separately on channels,\nfollowed by a pointwise convolution that mixes channels. If `use_bias` is True\nand a bias initializer is provided, it adds a bias vector to the output. It\nthen optionally applies an activation function to produce the final output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.SeparableConv2D", "path": "compat/v1/layers/separableconv2d", "type": "tf.compat", "text": "\nDepthwise separable 2D convolution.\n\nInherits From: `SeparableConv2D`, `Layer`, `Layer`, `Module`\n\nThis layer performs a depthwise convolution that acts separately on channels,\nfollowed by a pointwise convolution that mixes channels. If `use_bias` is True\nand a bias initializer is provided, it adds a bias vector to the output. It\nthen optionally applies an activation function to produce the final output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.separable_conv1d", "path": "compat/v1/layers/separable_conv1d", "type": "tf.compat", "text": "\nFunctional interface for the depthwise separable 1D convolution layer.\n\nThis layer performs a depthwise convolution that acts separately on channels,\nfollowed by a pointwise convolution that mixes channels. If `use_bias` is True\nand a bias initializer is provided, it adds a bias vector to the output. It\nthen optionally applies an activation function to produce the final output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.layers.separable_conv2d", "path": "compat/v1/layers/separable_conv2d", "type": "tf.compat", "text": "\nFunctional interface for the depthwise separable 2D convolution layer.\n\nThis layer performs a depthwise convolution that acts separately on channels,\nfollowed by a pointwise convolution that mixes channels. If `use_bias` is True\nand a bias initializer is provided, it adds a bias vector to the output. It\nthen optionally applies an activation function to produce the final output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.linalg", "path": "compat/v1/linalg", "type": "tf.compat", "text": "\nOperations for linear algebra.\n\n`experimental` module: Public API for tf.linalg.experimental namespace.\n\n`class LinearOperator`: Base class defining a [batch of] linear operator[s].\n\n`class LinearOperatorAdjoint`: `LinearOperator` representing the adjoint of\nanother operator.\n\n`class LinearOperatorBlockDiag`: Combines one or more `LinearOperators` in to\na Block Diagonal matrix.\n\n`class LinearOperatorBlockLowerTriangular`: Combines `LinearOperators` into a\nblockwise lower-triangular matrix.\n\n`class LinearOperatorCirculant`: `LinearOperator` acting like a circulant\nmatrix.\n\n`class LinearOperatorCirculant2D`: `LinearOperator` acting like a block\ncirculant matrix.\n\n`class LinearOperatorCirculant3D`: `LinearOperator` acting like a nested block\ncirculant matrix.\n\n`class LinearOperatorComposition`: Composes one or more `LinearOperators`.\n\n`class LinearOperatorDiag`: `LinearOperator` acting like a [batch] square\ndiagonal matrix.\n\n`class LinearOperatorFullMatrix`: `LinearOperator` that wraps a [batch]\nmatrix.\n\n`class LinearOperatorHouseholder`: `LinearOperator` acting like a [batch] of\nHouseholder transformations.\n\n`class LinearOperatorIdentity`: `LinearOperator` acting like a [batch] square\nidentity matrix.\n\n`class LinearOperatorInversion`: `LinearOperator` representing the inverse of\nanother operator.\n\n`class LinearOperatorKronecker`: Kronecker product between two\n`LinearOperators`.\n\n`class LinearOperatorLowRankUpdate`: Perturb a `LinearOperator` with a rank\n`K` update.\n\n`class LinearOperatorLowerTriangular`: `LinearOperator` acting like a [batch]\nsquare lower triangular matrix.\n\n`class LinearOperatorPermutation`: `LinearOperator` acting like a [batch] of\npermutation matrices.\n\n`class LinearOperatorScaledIdentity`: `LinearOperator` acting like a scaled\n[batch] identity matrix `A = c I`.\n\n`class LinearOperatorToeplitz`: `LinearOperator` acting like a [batch] of\ntoeplitz matrices.\n\n`class LinearOperatorTridiag`: `LinearOperator` acting like a [batch] square\ntridiagonal matrix.\n\n`class LinearOperatorZeros`: `LinearOperator` acting like a [batch] zero\nmatrix.\n\n`adjoint(...)`: Transposes the last two dimensions of and conjugates tensor\n`matrix`.\n\n`band_part(...)`: Copy a tensor setting everything outside a central band in\neach innermost matrix to zero.\n\n`cholesky(...)`: Computes the Cholesky decomposition of one or more square\nmatrices.\n\n`cholesky_solve(...)`: Solves systems of linear eqns `A X = RHS`, given\nCholesky factorizations.\n\n`cross(...)`: Compute the pairwise cross product.\n\n`det(...)`: Computes the determinant of one or more square matrices.\n\n`diag(...)`: Returns a batched diagonal tensor with given batched diagonal\nvalues.\n\n`diag_part(...)`: Returns the batched diagonal part of a batched tensor.\n\n`eigh(...)`: Computes the eigen decomposition of a batch of self-adjoint\nmatrices.\n\n`eigvalsh(...)`: Computes the eigenvalues of one or more self-adjoint\nmatrices.\n\n`einsum(...)`: Tensor contraction over specified indices and outer product.\n\n`expm(...)`: Computes the matrix exponential of one or more square matrices.\n\n`eye(...)`: Construct an identity matrix, or a batch of matrices.\n\n`global_norm(...)`: Computes the global norm of multiple tensors.\n\n`inv(...)`: Computes the inverse of one or more square invertible matrices or\ntheir adjoints (conjugate transposes).\n\n`l2_normalize(...)`: Normalizes along dimension `axis` using an L2 norm.\n(deprecated arguments)\n\n`logdet(...)`: Computes log of the determinant of a hermitian positive\ndefinite matrix.\n\n`logm(...)`: Computes the matrix logarithm of one or more square matrices:\n\n`lstsq(...)`: Solves one or more linear least-squares problems.\n\n`lu(...)`: Computes the LU decomposition of one or more square matrices.\n\n`lu_matrix_inverse(...)`: Computes the inverse given the LU decomposition(s)\nof one or more matrices.\n\n`lu_reconstruct(...)`: The reconstruct one or more matrices from their LU\ndecomposition(s).\n\n`lu_solve(...)`: Solves systems of linear eqns `A X = RHS`, given LU\nfactorizations.\n\n`matmul(...)`: Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n\n`matrix_rank(...)`: Compute the matrix rank of one or more matrices.\n\n`matrix_transpose(...)`: Transposes last two dimensions of tensor `a`.\n\n`matvec(...)`: Multiplies matrix `a` by vector `b`, producing `a` * `b`.\n\n`norm(...)`: Computes the norm of vectors, matrices, and tensors. (deprecated\narguments)\n\n`normalize(...)`: Normalizes `tensor` along dimension `axis` using specified\nnorm.\n\n`pinv(...)`: Compute the Moore-Penrose pseudo-inverse of one or more matrices.\n\n`qr(...)`: Computes the QR decompositions of one or more matrices.\n\n`set_diag(...)`: Returns a batched matrix tensor with new batched diagonal\nvalues.\n\n`slogdet(...)`: Computes the sign and the log of the absolute value of the\ndeterminant of\n\n`solve(...)`: Solves systems of linear equations.\n\n`sqrtm(...)`: Computes the matrix square root of one or more square matrices:\n\n`svd(...)`: Computes the singular value decompositions of one or more\nmatrices.\n\n`tensor_diag(...)`: Returns a diagonal tensor with a given diagonal values.\n\n`tensor_diag_part(...)`: Returns the diagonal part of the tensor.\n\n`tensordot(...)`: Tensor contraction of a and b along specified axes and outer\nproduct.\n\n`trace(...)`: Compute the trace of a tensor `x`.\n\n`transpose(...)`: Transposes last two dimensions of tensor `a`.\n\n`triangular_solve(...)`: Solve systems of linear equations with upper or lower\ntriangular matrices.\n\n`tridiagonal_matmul(...)`: Multiplies tridiagonal matrix by matrix.\n\n`tridiagonal_solve(...)`: Solves tridiagonal systems of equations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.linalg.experimental", "path": "compat/v1/linalg/experimental", "type": "tf.compat", "text": "\nPublic API for tf.linalg.experimental namespace.\n\n`conjugate_gradient(...)`: Conjugate gradient solver.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.linalg.l2_normalize", "path": "compat/v1/linalg/l2_normalize", "type": "tf.compat", "text": "\nNormalizes along dimension `axis` using an L2 norm. (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.math.l2_normalize`, `tf.compat.v1.nn.l2_normalize`\n\nFor a 1-D tensor with `axis = 0`, computes\n\nFor `x` with more dimensions, independently normalizes each 1-D slice along\ndimension `axis`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite", "path": "compat/v1/lite", "type": "tf.compat", "text": "\nPublic API for tf.lite namespace.\n\n`constants` module: Public API for tf.lite.constants namespace.\n\n`experimental` module: Public API for tf.lite.experimental namespace.\n\n`class Interpreter`: Interpreter interface for TensorFlow Lite Models.\n\n`class OpHint`: A class that helps build tflite function invocations.\n\n`class OpsSet`: Enum class defining the sets of ops available to generate\nTFLite models.\n\n`class Optimize`: Enum defining the optimizations to apply when generating\ntflite graphs.\n\n`class RepresentativeDataset`: Representative dataset to evaluate\noptimizations.\n\n`class TFLiteConverter`: Convert a TensorFlow model into `output_format`.\n\n`class TargetSpec`: Specification of target device.\n\n`class TocoConverter`: Convert a TensorFlow model into `output_format` using\nTOCO.\n\n`toco_convert(...)`: Convert a model using TOCO. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.constants", "path": "compat/v1/lite/constants", "type": "tf.compat", "text": "\nPublic API for tf.lite.constants namespace.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental", "path": "compat/v1/lite/experimental", "type": "tf.compat", "text": "\nPublic API for tf.lite.experimental namespace.\n\n`nn` module: Public API for tf.lite.experimental.nn namespace.\n\n`convert_op_hints_to_stubs(...)`: Converts a graphdef with LiteOp hints into\nstub operations.\n\n`get_potentially_supported_ops(...)`: Returns operations potentially supported\nby TensorFlow Lite.\n\n`load_delegate(...)`: Returns loaded Delegate object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.convert_op_hints_to_stubs", "path": "compat/v1/lite/experimental/convert_op_hints_to_stubs", "type": "tf.compat", "text": "\nConverts a graphdef with LiteOp hints into stub operations.\n\nThis is used to prepare for toco conversion of complex intrinsic usages. Note:\nonly one of session or graph_def should be used, not both.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.get_potentially_supported_ops", "path": "compat/v1/lite/experimental/get_potentially_supported_ops", "type": "tf.compat", "text": "\nReturns operations potentially supported by TensorFlow Lite.\n\nThe potentially support list contains a list of ops that are partially or\nfully supported, which is derived by simply scanning op names to check whether\nthey can be handled without real conversion and specific parameters.\n\nGiven that some ops may be partially supported, the optimal way to determine\nif a model's operations are supported is by converting using the TensorFlow\nLite converter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.nn", "path": "compat/v1/lite/experimental/nn", "type": "tf.compat", "text": "\nPublic API for tf.lite.experimental.nn namespace.\n\n`class TFLiteLSTMCell`: Long short-term memory unit (LSTM) recurrent network\ncell.\n\n`class TfLiteRNNCell`: The most basic RNN cell.\n\n`dynamic_rnn(...)`: Creates a recurrent neural network specified by RNNCell\n`cell`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.nn.dynamic_rnn", "path": "compat/v1/lite/experimental/nn/dynamic_rnn", "type": "tf.compat", "text": "\nCreates a recurrent neural network specified by RNNCell `cell`.\n\nPerforms fully dynamic unrolling of `inputs`.\n\nIf time_major == False (default), this will be a `Tensor` shaped:\n`[batch_size, max_time, cell.output_size]`.\n\nIf time_major == True, this will be a `Tensor` shaped: `[max_time, batch_size,\ncell.output_size]`.\n\nNote, if `cell.output_size` is a (possibly nested) tuple of integers or\n`TensorShape` objects, then `outputs` will be a tuple having the same\nstructure as `cell.output_size`, containing Tensors having shapes\ncorresponding to the shape data in `cell.output_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.nn.TFLiteLSTMCell", "path": "compat/v1/lite/experimental/nn/tflitelstmcell", "type": "tf.compat", "text": "\nLong short-term memory unit (LSTM) recurrent network cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nThis is used only for TfLite, it provides hints and it also makes the\nvariables in the desired for the tflite ops (transposed and separated).\n\nThe default non-peephole implementation is based on:\n\nhttps://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf\n\nFelix Gers, Jurgen Schmidhuber, and Fred Cummins. \"Learning to forget:\nContinual prediction with LSTM.\" IET, 850-855, 1999.\n\nThe peephole implementation is based on:\n\nhttps://research.google.com/pubs/archive/43905.pdf\n\nHasim Sak, Andrew Senior, and Francoise Beaufays. \"Long short-term memory\nrecurrent neural network architectures for large scale acoustic modeling.\"\nINTERSPEECH, 2014.\n\nThe class uses optional peep-hole connections, optional cell clipping, and an\noptional projection layer.\n\nNote that this cell is not optimized for performance. Please use\n`tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n`tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\nbetter performance on CPU.\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.experimental.nn.TfLiteRNNCell", "path": "compat/v1/lite/experimental/nn/tfliternncell", "type": "tf.compat", "text": "\nThe most basic RNN cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nThis is used only for TfLite, it provides hints and it also makes the\nvariables in the desired for the tflite ops.\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.OpHint", "path": "compat/v1/lite/ophint", "type": "tf.compat", "text": "\nA class that helps build tflite function invocations.\n\nIt allows you to take a bunch of TensorFlow ops and annotate the construction\nsuch that toco knows how to convert it to tflite. This embeds a pseudo\nfunction in a TensorFlow graph. This allows embedding high-level API usage\ninformation in a lower level TensorFlow implementation so that an alternative\nimplementation can be substituted later.\n\nEssentially, any \"input\" into this pseudo op is fed into an identity, and\nattributes are added to that input before being used by the constituent ops\nthat make up the pseudo op. A similar process is done to any output that is to\nbe exported from the current op.\n\n`class OpHintArgumentTracker`\n\nView source\n\nAdd a wrapped input argument to the hint.\n\nView source\n\nAdd a sequence of inputs to the function invocation.\n\nView source\n\nAdd a wrapped output argument to the hint.\n\nView source\n\nAdd a sequence of outputs to the function invocation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.OpHint.OpHintArgumentTracker", "path": "compat/v1/lite/ophint/ophintargumenttracker", "type": "tf.compat", "text": "\nConceptually tracks indices of arguments of \"OpHint functions\".\n\nThe inputs and arguments of these functions both use an instance of the class\nso they can have independent numbering.\n\nView source\n\nReturn a wrapped tensor of an input tensor as an argument.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.TFLiteConverter", "path": "compat/v1/lite/tfliteconverter", "type": "tf.compat", "text": "\nConvert a TensorFlow model into `output_format`.\n\nThis is used to convert from a TensorFlow GraphDef, SavedModel or tf.keras\nmodel into either a TFLite FlatBuffer or graph visualization.\n\nView source\n\nConverts a TensorFlow GraphDef based on instance variables.\n\nView source\n\nCreates a TFLiteConverter class from a file containing a frozen GraphDef.\n\nView source\n\nCreates a TFLiteConverter class from a tf.keras model file.\n\nView source\n\nCreates a TFLiteConverter class from a SavedModel.\n\nView source\n\nCreates a TFLiteConverter class from a TensorFlow Session.\n\nView source\n\nReturns a list of the names of the input tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.TocoConverter", "path": "compat/v1/lite/tococonverter", "type": "tf.compat", "text": "\nConvert a TensorFlow model into `output_format` using TOCO.\n\nThis class has been deprecated. Please use `lite.TFLiteConverter` instead.\n\nView source\n\nCreates a TocoConverter class from a file containing a frozen graph.\n(deprecated)\n\nView source\n\nCreates a TocoConverter class from a tf.keras model file. (deprecated)\n\nView source\n\nCreates a TocoConverter class from a SavedModel. (deprecated)\n\nView source\n\nCreates a TocoConverter class from a TensorFlow Session. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lite.toco_convert", "path": "compat/v1/lite/toco_convert", "type": "tf.compat", "text": "\nConvert a model using TOCO. (deprecated)\n\nTypically this function is used to convert from TensorFlow GraphDef to TFLite.\nConversion can be customized by providing arguments that are forwarded to\n`build_toco_convert_protos` (see documentation for details). This function has\nbeen deprecated. Please use `lite.TFLiteConverter` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.LMDBReader", "path": "compat/v1/lmdbreader", "type": "tf.compat", "text": "\nA Reader that outputs the records from a LMDB file.\n\nInherits From: `ReaderBase`\n\nSee ReaderBase for supported methods.\n\nReaders are not compatible with eager execution. Instead, please use `tf.data`\nto get data into your model.\n\nView source\n\nReturns the number of records this reader has produced.\n\nThis is the same as the number of Read executions that have succeeded.\n\nView source\n\nReturns the number of work units this reader has finished processing.\n\nView source\n\nReturns the next record (key, value) pair produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g. when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile).\n\nView source\n\nReturns up to num_records (key, value) pairs produced by a reader.\n\nWill dequeue a work unit from queue if necessary (e.g., when the Reader needs\nto start reading from a new file since it has finished with the previous\nfile). It may return less than num_records even before the last batch.\n\nView source\n\nRestore a reader to its initial clean state.\n\nView source\n\nRestore a reader to a previously saved state.\n\nNot all Readers support being restored, so this can produce an Unimplemented\nerror.\n\nView source\n\nProduce a string tensor that encodes the state of a reader.\n\nNot all Readers support being serialized, so this can produce an Unimplemented\nerror.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.load_file_system_library", "path": "compat/v1/load_file_system_library", "type": "tf.compat", "text": "\nLoads a TensorFlow plugin, containing file system implementation. (deprecated)\n\nPass `library_filename` to a platform-specific mechanism for dynamically\nloading a library. The rules for determining the exact location of the library\nare platform-specific and are not documented here.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.local_variables", "path": "compat/v1/local_variables", "type": "tf.compat", "text": "\nReturns local variables.\n\nLocal variables - per process variables, usually not saved/restored to\ncheckpoint and used for temporary or intermediate values. For example, they\ncan be used as counters for metrics computation or number of epochs this\nmachine has read data. The `tf.contrib.framework.local_variable()` function\nautomatically adds the new variable to `GraphKeys.LOCAL_VARIABLES`. This\nconvenience function returns the contents of that collection.\n\nAn alternative to local variables are global variables. See\n`tf.compat.v1.global_variables`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.local_variables_initializer", "path": "compat/v1/local_variables_initializer", "type": "tf.compat", "text": "\nReturns an Op that initializes all local variables.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.initializers.local_variables`\n\nThis is just a shortcut for `variables_initializer(local_variables())`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging", "path": "compat/v1/logging", "type": "tf.compat", "text": "\nLogging and Summary Operations.\n\n`TaskLevelStatusMessage(...)`\n\n`debug(...)`\n\n`error(...)`\n\n`fatal(...)`\n\n`flush(...)`\n\n`get_verbosity(...)`: Return how much logging output will be produced.\n\n`info(...)`\n\n`log(...)`\n\n`log_every_n(...)`: Log 'msg % args' at level 'level' once per 'n' times.\n\n`log_first_n(...)`: Log 'msg % args' at level 'level' only first 'n' times.\n\n`log_if(...)`: Log 'msg % args' at level 'level' only if condition is\nfulfilled.\n\n`set_verbosity(...)`: Sets the threshold for what messages will be logged.\n\n`vlog(...)`\n\n`warn(...)`\n\n`warning(...)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.debug", "path": "compat/v1/logging/debug", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.error", "path": "compat/v1/logging/error", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.fatal", "path": "compat/v1/logging/fatal", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.flush", "path": "compat/v1/logging/flush", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.get_verbosity", "path": "compat/v1/logging/get_verbosity", "type": "tf.compat", "text": "\nReturn how much logging output will be produced.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.info", "path": "compat/v1/logging/info", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.log", "path": "compat/v1/logging/log", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.log_every_n", "path": "compat/v1/logging/log_every_n", "type": "tf.compat", "text": "\nLog 'msg % args' at level 'level' once per 'n' times.\n\nLogs the 1st call, (N+1)st call, (2N+1)st call, etc. Not threadsafe.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.log_first_n", "path": "compat/v1/logging/log_first_n", "type": "tf.compat", "text": "\nLog 'msg % args' at level 'level' only first 'n' times.\n\nNot threadsafe.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.log_if", "path": "compat/v1/logging/log_if", "type": "tf.compat", "text": "\nLog 'msg % args' at level 'level' only if condition is fulfilled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.set_verbosity", "path": "compat/v1/logging/set_verbosity", "type": "tf.compat", "text": "\nSets the threshold for what messages will be logged.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.TaskLevelStatusMessage", "path": "compat/v1/logging/tasklevelstatusmessage", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.vlog", "path": "compat/v1/logging/vlog", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.warn", "path": "compat/v1/logging/warn", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.logging.warning", "path": "compat/v1/logging/warning", "type": "tf.compat", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.LogMessage", "path": "compat/v1/logmessage", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lookup", "path": "compat/v1/lookup", "type": "tf.compat", "text": "\nPublic API for tf.lookup namespace.\n\n`experimental` module: Public API for tf.lookup.experimental namespace.\n\n`class KeyValueTensorInitializer`: Table initializers given `keys` and\n`values` tensors.\n\n`class StaticHashTable`: A generic hash table that is immutable once\ninitialized.\n\n`class StaticVocabularyTable`: String to Id table that assigns out-of-\nvocabulary keys to hash buckets.\n\n`class TextFileIndex`: The key and value content to get from each line.\n\n`class TextFileInitializer`: Table initializers from a text file.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lookup.experimental", "path": "compat/v1/lookup/experimental", "type": "tf.compat", "text": "\nPublic API for tf.lookup.experimental namespace.\n\n`class DatasetInitializer`: Creates a table initializer from a\n`tf.data.Dataset`.\n\n`class DenseHashTable`: A generic mutable hash table implementation using\ntensors as backing store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lookup.StaticHashTable", "path": "compat/v1/lookup/statichashtable", "type": "tf.compat", "text": "\nA generic hash table that is immutable once initialized.\n\nInherits From: `StaticHashTable`\n\nWhen running in graph mode, you must evaluate the tensor returned by\n`tf.tables_initializer()` before evaluating the tensor returned by this\nclass's `lookup()` method. Example usage in graph mode:\n\nIn eager mode, no special code is needed to initialize the table. Example\nusage in eager mode:\n\nView source\n\nReturns tensors of all keys and values in the table.\n\nView source\n\nLooks up `keys` in a table, outputs the corresponding values.\n\nThe `default_value` is used for keys not present in the table.\n\nView source\n\nCompute the number of elements in this table.\n\nView source\n\nLooks up `keys` in a table, outputs the corresponding values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.lookup.StaticVocabularyTable", "path": "compat/v1/lookup/staticvocabularytable", "type": "tf.compat", "text": "\nString to Id table that assigns out-of-vocabulary keys to hash buckets.\n\nInherits From: `StaticVocabularyTable`\n\nFor example, if an instance of `StaticVocabularyTable` is initialized with a\nstring-to-id initializer that maps:\n\nThe `Vocabulary` object will performs the following mapping:\n\nIf `initializer` is None, only out-of-vocabulary buckets are used.\n\nThe hash function used for generating out-of-vocabulary buckets ID is\nFingerprint64.\n\nView source\n\nLooks up `keys` in the table, outputs the corresponding values.\n\nIt assigns out-of-vocabulary keys to buckets based in their hashes.\n\nView source\n\nCompute the number of elements in this table.\n\nView source\n\nLooks up `keys` in a table, outputs the corresponding values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses", "path": "compat/v1/losses", "type": "tf.compat", "text": "\nLoss operations for use in neural networks.\n\n`class Reduction`: Types of loss reduction.\n\n`absolute_difference(...)`: Adds an Absolute Difference loss to the training\nprocedure.\n\n`add_loss(...)`: Adds a externally defined loss to the collection of losses.\n\n`compute_weighted_loss(...)`: Computes the weighted loss.\n\n`cosine_distance(...)`: Adds a cosine-distance loss to the training procedure.\n(deprecated arguments)\n\n`get_losses(...)`: Gets the list of losses from the loss_collection.\n\n`get_regularization_loss(...)`: Gets the total regularization loss.\n\n`get_regularization_losses(...)`: Gets the list of regularization losses.\n\n`get_total_loss(...)`: Returns a tensor whose value represents the total loss.\n\n`hinge_loss(...)`: Adds a hinge loss to the training procedure.\n\n`huber_loss(...)`: Adds a Huber Loss term to the training procedure.\n\n`log_loss(...)`: Adds a Log Loss term to the training procedure.\n\n`mean_pairwise_squared_error(...)`: Adds a pairwise-errors-squared loss to the\ntraining procedure.\n\n`mean_squared_error(...)`: Adds a Sum-of-Squares loss to the training\nprocedure.\n\n`sigmoid_cross_entropy(...)`: Creates a cross-entropy loss using\ntf.nn.sigmoid_cross_entropy_with_logits.\n\n`softmax_cross_entropy(...)`: Creates a cross-entropy loss using\ntf.nn.softmax_cross_entropy_with_logits_v2.\n\n`sparse_softmax_cross_entropy(...)`: Cross-entropy loss using\n`tf.nn.sparse_softmax_cross_entropy_with_logits`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.absolute_difference", "path": "compat/v1/losses/absolute_difference", "type": "tf.compat", "text": "\nAdds an Absolute Difference loss to the training procedure.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a `Tensor` of\nshape `[batch_size]`, then the total loss for each sample of the batch is\nrescaled by the corresponding element in the `weights` vector. If the shape of\n`weights` matches the shape of `predictions`, then the loss of each measurable\nelement of `predictions` is scaled by the corresponding value of `weights`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.add_loss", "path": "compat/v1/losses/add_loss", "type": "tf.compat", "text": "\nAdds a externally defined loss to the collection of losses.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.compute_weighted_loss", "path": "compat/v1/losses/compute_weighted_loss", "type": "tf.compat", "text": "\nComputes the weighted loss.\n\nWhen calculating the gradient of a weighted loss contributions from both\n`losses` and `weights` are considered. If your `weights` depend on some model\nparameters but you do not want this to affect the loss gradient, you need to\napply `tf.stop_gradient` to `weights` before passing them to\n`compute_weighted_loss`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.cosine_distance", "path": "compat/v1/losses/cosine_distance", "type": "tf.compat", "text": "\nAdds a cosine-distance loss to the training procedure. (deprecated arguments)\n\nNote that the function assumes that `predictions` and `labels` are already\nunit-normalized.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.get_losses", "path": "compat/v1/losses/get_losses", "type": "tf.compat", "text": "\nGets the list of losses from the loss_collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.get_regularization_loss", "path": "compat/v1/losses/get_regularization_loss", "type": "tf.compat", "text": "\nGets the total regularization loss.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.get_regularization_losses", "path": "compat/v1/losses/get_regularization_losses", "type": "tf.compat", "text": "\nGets the list of regularization losses.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.get_total_loss", "path": "compat/v1/losses/get_total_loss", "type": "tf.compat", "text": "\nReturns a tensor whose value represents the total loss.\n\nIn particular, this adds any losses you have added with `tf.add_loss()` to any\nregularization losses that have been added by regularization parameters on\nlayers constructors e.g. `tf.layers`. Be very sure to use this if you are\nconstructing a loss_op manually. Otherwise regularization arguments on\n`tf.layers` methods will not function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.hinge_loss", "path": "compat/v1/losses/hinge_loss", "type": "tf.compat", "text": "\nAdds a hinge loss to the training procedure.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.huber_loss", "path": "compat/v1/losses/huber_loss", "type": "tf.compat", "text": "\nAdds a Huber Loss term to the training procedure.\n\nFor each value x in `error=labels-predictions`, the following is calculated:\n\nwhere d is `delta`.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of size\n`[batch_size]`, then the total loss for each sample of the batch is rescaled\nby the corresponding element in the `weights` vector. If the shape of\n`weights` matches the shape of `predictions`, then the loss of each measurable\nelement of `predictions` is scaled by the corresponding value of `weights`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.log_loss", "path": "compat/v1/losses/log_loss", "type": "tf.compat", "text": "\nAdds a Log Loss term to the training procedure.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of size\n`[batch_size]`, then the total loss for each sample of the batch is rescaled\nby the corresponding element in the `weights` vector. If the shape of\n`weights` matches the shape of `predictions`, then the loss of each measurable\nelement of `predictions` is scaled by the corresponding value of `weights`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.mean_pairwise_squared_error", "path": "compat/v1/losses/mean_pairwise_squared_error", "type": "tf.compat", "text": "\nAdds a pairwise-errors-squared loss to the training procedure.\n\nUnlike `mean_squared_error`, which is a measure of the differences between\ncorresponding elements of `predictions` and `labels`,\n`mean_pairwise_squared_error` is a measure of the differences between pairs of\ncorresponding elements of `predictions` and `labels`.\n\nFor example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are\nthree pairs of differences are summed to compute the loss: loss = [ ((a-b) -\n(x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3\n\nNote that since the inputs are of shape `[batch_size, d0, ... dN]`, the\ncorresponding pairs are computed within each batch sample but not across\nsamples within a batch. For example, if `predictions` represents a batch of 16\ngrayscale images of dimension [batch_size, 100, 200], then the set of pairs is\ndrawn from each image, but not across images.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of size\n`[batch_size]`, then the total loss for each sample of the batch is rescaled\nby the corresponding element in the `weights` vector.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.mean_squared_error", "path": "compat/v1/losses/mean_squared_error", "type": "tf.compat", "text": "\nAdds a Sum-of-Squares loss to the training procedure.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of size\n`[batch_size]`, then the total loss for each sample of the batch is rescaled\nby the corresponding element in the `weights` vector. If the shape of\n`weights` matches the shape of `predictions`, then the loss of each measurable\nelement of `predictions` is scaled by the corresponding value of `weights`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.Reduction", "path": "compat/v1/losses/reduction", "type": "tf.compat", "text": "\nTypes of loss reduction.\n\nContains the following values:\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.sigmoid_cross_entropy", "path": "compat/v1/losses/sigmoid_cross_entropy", "type": "tf.compat", "text": "\nCreates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of\nshape `[batch_size]`, then the loss weights apply to each corresponding\nsample.\n\nIf `label_smoothing` is nonzero, smooth the labels towards 1/2:\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.softmax_cross_entropy", "path": "compat/v1/losses/softmax_cross_entropy", "type": "tf.compat", "text": "\nCreates a cross-entropy loss using tf.nn.softmax_cross_entropy_with_logits_v2.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of\nshape `[batch_size]`, then the loss weights apply to each corresponding\nsample.\n\nIf `label_smoothing` is nonzero, smooth the labels towards 1/num_classes:\nnew_onehot_labels = onehot_labels * (1 - label_smoothing)\n\nNote that `onehot_labels` and `logits` must have the same shape, e.g.\n`[batch_size, num_classes]`. The shape of `weights` must be broadcastable to\nloss, whose shape is decided by the shape of `logits`. In case the shape of\n`logits` is `[batch_size, num_classes]`, loss is a `Tensor` of shape\n`[batch_size]`.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.losses.sparse_softmax_cross_entropy", "path": "compat/v1/losses/sparse_softmax_cross_entropy", "type": "tf.compat", "text": "\nCross-entropy loss using `tf.nn.sparse_softmax_cross_entropy_with_logits`.\n\n`weights` acts as a coefficient for the loss. If a scalar is provided, then\nthe loss is simply scaled by the given value. If `weights` is a tensor of\nshape `[batch_size]`, then the loss weights apply to each corresponding\nsample.\n\nThe `loss_collection` argument is ignored when executing eagerly. Consider\nholding on to the return value or collecting losses via a `tf.keras.Model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.make_template", "path": "compat/v1/make_template", "type": "tf.compat", "text": "\nGiven an arbitrary function, wrap it so that it does variable sharing.\n\nThis wraps `func_` in a Template and partially evaluates it. Templates are\nfunctions that create variables the first time they are called and reuse them\nthereafter. In order for `func_` to be compatible with a `Template` it must\nhave the following properties:\n\nIn the following example, both `z` and `w` will be scaled by the same `y`. It\nis important to note that if we didn't assign `scalar_name` and used a\ndifferent name for z and w that a `ValueError` would be thrown because it\ncouldn't reuse the variable.\n\nAs a safe-guard, the returned function will raise a `ValueError` after the\nfirst call if trainable variables are created by calling `tf.Variable`.\n\nIf all of these are true, then 2 properties are enforced by the template:\n\nDepending on the value of `create_scope_now_`, the full variable scope may be\ncaptured either at the time of first call or at the time of construction. If\nthis option is set to True, then all Tensors created by repeated calls to the\ntemplate will have an extra trailing _N+1 to their name, as the first time the\nscope is entered in the Template constructor no Tensors are created.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.manip", "path": "compat/v1/manip", "type": "tf.compat", "text": "\nOperators for manipulating tensors.\n\n`batch_to_space_nd(...)`: BatchToSpace for N-D tensors of type T.\n\n`gather_nd(...)`: Gather slices from `params` into a Tensor with shape\nspecified by `indices`.\n\n`reshape(...)`: Reshapes a tensor.\n\n`reverse(...)`: Reverses specific dimensions of a tensor.\n\n`roll(...)`: Rolls the elements of a tensor along an axis.\n\n`scatter_nd(...)`: Scatter `updates` into a new tensor according to `indices`.\n\n`space_to_batch_nd(...)`: SpaceToBatch for N-D tensors of type T.\n\n`tile(...)`: Constructs a tensor by tiling a given tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.map_fn", "path": "compat/v1/map_fn", "type": "tf.compat", "text": "\nTransforms `elems` by applying `fn` to each element unstacked on axis 0.\n(deprecated arguments)\n\nSee also `tf.scan`.\n\n`map_fn` unstacks `elems` on axis 0 to obtain a sequence of elements; calls\n`fn` to transform each element; and then stacks the transformed values back\ntogether.\n\nIf `elems` is a single tensor and `fn`'s signature is `tf.Tensor->tf.Tensor`,\nthen `map_fn(fn, elems)` is equivalent to `tf.stack([fn(elem) for elem in\ntf.unstack(elems)])`. E.g.:\n\n`map_fn(fn, elems).shape = [elems.shape[0]] + fn(elems[0]).shape`.\n\n`map_fn` also supports functions with multi-arity inputs and outputs:\n\nIf `elems` is a tuple (or nested structure) of tensors, then those tensors\nmust all have the same outer-dimension size (`num_elems`); and `fn` is used to\ntransform each tuple (or structure) of corresponding slices from `elems`.\nE.g., if `elems` is a tuple `(t1, t2, t3)`, then `fn` is used to transform\neach tuple of slices `(t1[i], t2[i], t3[i])` (where `0 <= i < num_elems`).\n\nIf `fn` returns a tuple (or nested structure) of tensors, then the result is\nformed by stacking corresponding elements from those structures.\n\nIf `fn`'s input and output signatures are different, then the output signature\nmust be specified using `fn_output_signature`. (The input and output\nsignatures are differ if their structures, dtypes, or tensor types do not\nmatch). E.g.:\n\n`fn_output_signature` can be specified using any of the following:\n\n`map_fn` supports `tf.RaggedTensor` inputs and outputs. In particular:\n\nIf `elems` is a `RaggedTensor`, then `fn` will be called with each row of that\nragged tensor.\n\nIf the result of `map_fn` should be a `RaggedTensor`, then use a\n`tf.RaggedTensorSpec` to specify `fn_output_signature`.\n\nE.g.:\n\n`map_fn` supports `tf.sparse.SparseTensor` inputs and outputs. In particular:\n\nIf `elems` is a `SparseTensor`, then `fn` will be called with each row of that\nsparse tensor. In particular, the value passed to `fn` will be a\n`tf.sparse.SparseTensor` with one fewer dimension than `elems`.\n\nIf the result of `map_fn` should be a `SparseTensor`, then use a\n`tf.SparseTensorSpec` to specify `fn_output_signature`. The individual\n`SparseTensor`s returned by `fn` will be stacked into a single `SparseTensor`\nwith one more dimension.\n\nIf the function is expressible as TensorFlow ops, use:\n\nOtherwise, use:\n\n`map_fn` will apply the operations used by `fn` to each element of `elems`,\nresulting in `O(elems.shape[0])` total operations. This is somewhat mitigated\nby the fact that `map_fn` can process elements in parallel. However, a\ntransform expressed using `map_fn` is still typically less efficient than an\nequivalent transform expressed using vectorized operations.\n\n`map_fn` should typically only be used if one of the following is true:\n\nE.g., the example given above that maps `fn=lambda t: tf.range(t, t + 3)`\nacross `elems` could be rewritten more efficiently using vectorized ops:\n\nIn some cases, `tf.vectorized_map` can be used to automatically convert a\nfunction to a vectorized eqivalent.\n\nWhen executing eagerly, `map_fn` does not execute in parallel even if\n`parallel_iterations` is set to a value > 1\\. You can still get the\nperformance benefits of running a function in parallel by using the\n`tf.function` decorator:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.math", "path": "compat/v1/math", "type": "tf.compat", "text": "\nMath Operations.\n\nTensorFlow provides a variety of math functions including:\n\nSee: `tf.linalg` for matrix and tensor functions.\n\nTensorFlow provides several operations that you can use to perform common math\ncomputations on tensor segments. Here a segmentation is a partitioning of a\ntensor along the first dimension, i.e. it defines a mapping from the first\ndimension onto `segment_ids`. The `segment_ids` tensor should be the size of\nthe first dimension, `d0`, with consecutive IDs in the range `0` to `k`, where\n`k<d0`. In particular, a segmentation of a matrix tensor is a mapping of rows\nto segments.\n\nThe standard `segment_*` functions assert that the segment indices are sorted.\nIf you have unsorted indices use the equivalent `unsorted_segment_` function.\nThese functions take an additional argument `num_segments` so that the output\ntensor can be efficiently allocated.\n\n`special` module: Public API for tf.math.special namespace.\n\n`abs(...)`: Computes the absolute value of a tensor.\n\n`accumulate_n(...)`: Returns the element-wise sum of a list of tensors.\n\n`acos(...)`: Computes acos of x element-wise.\n\n`acosh(...)`: Computes inverse hyperbolic cosine of x element-wise.\n\n`add(...)`: Returns x + y element-wise.\n\n`add_n(...)`: Adds all input tensors element-wise.\n\n`angle(...)`: Returns the element-wise argument of a complex (or real) tensor.\n\n`argmax(...)`: Returns the index with the largest value across axes of a\ntensor. (deprecated arguments)\n\n`argmin(...)`: Returns the index with the smallest value across axes of a\ntensor. (deprecated arguments)\n\n`asin(...)`: Computes the trignometric inverse sine of x element-wise.\n\n`asinh(...)`: Computes inverse hyperbolic sine of x element-wise.\n\n`atan(...)`: Computes the trignometric inverse tangent of x element-wise.\n\n`atan2(...)`: Computes arctangent of `y/x` element-wise, respecting signs of\nthe arguments.\n\n`atanh(...)`: Computes inverse hyperbolic tangent of x element-wise.\n\n`bessel_i0(...)`: Computes the Bessel i0 function of `x` element-wise.\n\n`bessel_i0e(...)`: Computes the Bessel i0e function of `x` element-wise.\n\n`bessel_i1(...)`: Computes the Bessel i1 function of `x` element-wise.\n\n`bessel_i1e(...)`: Computes the Bessel i1e function of `x` element-wise.\n\n`betainc(...)`: Compute the regularized incomplete beta integral \\\\(I_x(a,\nb)\\\\).\n\n`bincount(...)`: Counts the number of occurrences of each value in an integer\narray.\n\n`ceil(...)`: Return the ceiling of the input, element-wise.\n\n`confusion_matrix(...)`: Computes the confusion matrix from predictions and\nlabels.\n\n`conj(...)`: Returns the complex conjugate of a complex number.\n\n`cos(...)`: Computes cos of x element-wise.\n\n`cosh(...)`: Computes hyperbolic cosine of x element-wise.\n\n`count_nonzero(...)`: Computes number of nonzero elements across dimensions of\na tensor. (deprecated arguments) (deprecated arguments)\n\n`cumprod(...)`: Compute the cumulative product of the tensor `x` along `axis`.\n\n`cumsum(...)`: Compute the cumulative sum of the tensor `x` along `axis`.\n\n`cumulative_logsumexp(...)`: Compute the cumulative log-sum-exp of the tensor\n`x` along `axis`.\n\n`digamma(...)`: Computes Psi, the derivative of Lgamma (the log of the\nabsolute value of\n\n`divide(...)`: Computes Python style division of `x` by `y`.\n\n`divide_no_nan(...)`: Computes a safe divide which returns 0 if the y is zero.\n\n`equal(...)`: Returns the truth value of (x == y) element-wise.\n\n`erf(...)`: Computes the Gauss error function of `x` element-wise.\n\n`erfc(...)`: Computes the complementary error function of `x` element-wise.\n\n`erfcinv(...)`: Computes the inverse of complementary error function.\n\n`erfinv(...)`: Compute inverse error function.\n\n`exp(...)`: Computes exponential of x element-wise. \\\\(y = e^x\\\\).\n\n`expm1(...)`: Computes `exp(x) - 1` element-wise.\n\n`floor(...)`: Returns element-wise largest integer not greater than x.\n\n`floordiv(...)`: Divides `x / y` elementwise, rounding toward the most\nnegative integer.\n\n`floormod(...)`: Returns element-wise remainder of division. When `x < 0` xor\n`y < 0` is\n\n`greater(...)`: Returns the truth value of (x > y) element-wise.\n\n`greater_equal(...)`: Returns the truth value of (x >= y) element-wise.\n\n`igamma(...)`: Compute the lower regularized incomplete Gamma function `P(a,\nx)`.\n\n`igammac(...)`: Compute the upper regularized incomplete Gamma function `Q(a,\nx)`.\n\n`imag(...)`: Returns the imaginary part of a complex (or real) tensor.\n\n`in_top_k(...)`: Says whether the targets are in the top `K` predictions.\n\n`invert_permutation(...)`: Computes the inverse permutation of a tensor.\n\n`is_finite(...)`: Returns which elements of x are finite.\n\n`is_inf(...)`: Returns which elements of x are Inf.\n\n`is_nan(...)`: Returns which elements of x are NaN.\n\n`is_non_decreasing(...)`: Returns `True` if `x` is non-decreasing.\n\n`is_strictly_increasing(...)`: Returns `True` if `x` is strictly increasing.\n\n`l2_normalize(...)`: Normalizes along dimension `axis` using an L2 norm.\n(deprecated arguments)\n\n`lbeta(...)`: Computes \\\\(ln(|Beta(x)|)\\\\), reducing along the last dimension.\n\n`less(...)`: Returns the truth value of (x < y) element-wise.\n\n`less_equal(...)`: Returns the truth value of (x <= y) element-wise.\n\n`lgamma(...)`: Computes the log of the absolute value of `Gamma(x)` element-\nwise.\n\n`log(...)`: Computes natural logarithm of x element-wise.\n\n`log1p(...)`: Computes natural logarithm of (1 + x) element-wise.\n\n`log_sigmoid(...)`: Computes log sigmoid of `x` element-wise.\n\n`log_softmax(...)`: Computes log softmax activations. (deprecated arguments)\n\n`logical_and(...)`: Logical AND function.\n\n`logical_not(...)`: Returns the truth value of `NOT x` element-wise.\n\n`logical_or(...)`: Returns the truth value of x OR y element-wise.\n\n`logical_xor(...)`: Logical XOR function.\n\n`maximum(...)`: Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n\n`minimum(...)`: Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n\n`mod(...)`: Returns element-wise remainder of division. When `x < 0` xor `y <\n0` is\n\n`multiply(...)`: Returns an element-wise x * y.\n\n`multiply_no_nan(...)`: Computes the product of x and y and returns 0 if the y\nis zero, even if x is NaN or infinite.\n\n`ndtri(...)`: Compute quantile of Standard Normal.\n\n`negative(...)`: Computes numerical negative value element-wise.\n\n`nextafter(...)`: Returns the next representable value of `x1` in the\ndirection of `x2`, element-wise.\n\n`not_equal(...)`: Returns the truth value of (x != y) element-wise.\n\n`polygamma(...)`: Compute the polygamma function \\\\(\\psi^{(n)}(x)\\\\).\n\n`polyval(...)`: Computes the elementwise value of a polynomial.\n\n`pow(...)`: Computes the power of one value to another.\n\n`real(...)`: Returns the real part of a complex (or real) tensor.\n\n`reciprocal(...)`: Computes the reciprocal of x element-wise.\n\n`reciprocal_no_nan(...)`: Performs a safe reciprocal operation, element wise.\n\n`reduce_all(...)`: Computes the \"logical and\" of elements across dimensions of\na tensor. (deprecated arguments)\n\n`reduce_any(...)`: Computes the \"logical or\" of elements across dimensions of\na tensor. (deprecated arguments)\n\n`reduce_euclidean_norm(...)`: Computes the Euclidean norm of elements across\ndimensions of a tensor.\n\n`reduce_logsumexp(...)`: Computes log(sum(exp(elements across dimensions of a\ntensor))). (deprecated arguments)\n\n`reduce_max(...)`: Computes the maximum of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_mean(...)`: Computes the mean of elements across dimensions of a\ntensor.\n\n`reduce_min(...)`: Computes the minimum of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_prod(...)`: Computes the product of elements across dimensions of a\ntensor. (deprecated arguments)\n\n`reduce_std(...)`: Computes the standard deviation of elements across\ndimensions of a tensor.\n\n`reduce_sum(...)`: Computes the sum of elements across dimensions of a tensor.\n(deprecated arguments)\n\n`reduce_variance(...)`: Computes the variance of elements across dimensions of\na tensor.\n\n`rint(...)`: Returns element-wise integer closest to x.\n\n`round(...)`: Rounds the values of a tensor to the nearest integer, element-\nwise.\n\n`rsqrt(...)`: Computes reciprocal of square root of x element-wise.\n\n`scalar_mul(...)`: Multiplies a scalar times a `Tensor` or `IndexedSlices`\nobject.\n\n`segment_max(...)`: Computes the maximum along segments of a tensor.\n\n`segment_mean(...)`: Computes the mean along segments of a tensor.\n\n`segment_min(...)`: Computes the minimum along segments of a tensor.\n\n`segment_prod(...)`: Computes the product along segments of a tensor.\n\n`segment_sum(...)`: Computes the sum along segments of a tensor.\n\n`sigmoid(...)`: Computes sigmoid of `x` element-wise.\n\n`sign(...)`: Returns an element-wise indication of the sign of a number.\n\n`sin(...)`: Computes sine of x element-wise.\n\n`sinh(...)`: Computes hyperbolic sine of x element-wise.\n\n`sobol_sample(...)`: Generates points from the Sobol sequence.\n\n`softmax(...)`: Computes softmax activations. (deprecated arguments)\n\n`softplus(...)`: Computes softplus: `log(exp(features) + 1)`.\n\n`softsign(...)`: Computes softsign: `features / (abs(features) + 1)`.\n\n`sqrt(...)`: Computes element-wise square root of the input tensor.\n\n`square(...)`: Computes square of x element-wise.\n\n`squared_difference(...)`: Returns conj(x - y)(x - y) element-wise.\n\n`subtract(...)`: Returns x - y element-wise.\n\n`tan(...)`: Computes tan of x element-wise.\n\n`tanh(...)`: Computes hyperbolic tangent of `x` element-wise.\n\n`top_k(...)`: Finds values and indices of the `k` largest entries for the last\ndimension.\n\n`truediv(...)`: Divides x / y elementwise (using Python 3 division operator\nsemantics).\n\n`unsorted_segment_max(...)`: Computes the maximum along segments of a tensor.\n\n`unsorted_segment_mean(...)`: Computes the mean along segments of a tensor.\n\n`unsorted_segment_min(...)`: Computes the minimum along segments of a tensor.\n\n`unsorted_segment_prod(...)`: Computes the product along segments of a tensor.\n\n`unsorted_segment_sqrt_n(...)`: Computes the sum along segments of a tensor\ndivided by the sqrt(N).\n\n`unsorted_segment_sum(...)`: Computes the sum along segments of a tensor.\n\n`xdivy(...)`: Returns 0 if x == 0, and x / y otherwise, elementwise.\n\n`xlog1py(...)`: Compute x * log1p(y).\n\n`xlogy(...)`: Returns 0 if x == 0, and x * log(y) otherwise, elementwise.\n\n`zero_fraction(...)`: Returns the fraction of zeros in `value`.\n\n`zeta(...)`: Compute the Hurwitz zeta function \\\\(\\zeta(x, q)\\\\).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.math.in_top_k", "path": "compat/v1/math/in_top_k", "type": "tf.compat", "text": "\nSays whether the targets are in the top `K` predictions.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.in_top_k`\n\nThis outputs a `batch_size` bool array, an entry `out[i]` is `true` if the\nprediction for the target class is finite (not inf, -inf, or nan) and among\nthe top `k` predictions among all predictions for example `i`. Note that the\nbehavior of `InTopK` differs from the `TopK` op in its handling of ties; if\nmultiple classes have the same prediction value and straddle the top-`k`\nboundary, all of those classes are considered to be in the top `k`.\n\nMore formally, let\n\n\\\\(predictions_i\\\\) be the predictions for all classes for example `i`,\n\\\\(targets_i\\\\) be the target class for example `i`, \\\\(out_i\\\\) be the output\nfor example `i`,\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.math.log_softmax", "path": "compat/v1/math/log_softmax", "type": "tf.compat", "text": "\nComputes log softmax activations. (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.log_softmax`\n\nFor each batch `i` and class `j` we have\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.math.softmax", "path": "compat/v1/math/softmax", "type": "tf.compat", "text": "\nComputes softmax activations. (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.softmax`\n\nThis function performs the equivalent of\n\nSee: https://en.wikipedia.org/wiki/Softmax_function\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.math.special", "path": "compat/v1/math/special", "type": "tf.compat", "text": "\nPublic API for tf.math.special namespace.\n\n`bessel_i0(...)`: Computes the Bessel i0 function of `x` element-wise.\n\n`bessel_i0e(...)`: Computes the Bessel i0e function of `x` element-wise.\n\n`bessel_i1(...)`: Computes the Bessel i1 function of `x` element-wise.\n\n`bessel_i1e(...)`: Computes the Bessel i1e function of `x` element-wise.\n\n`bessel_j0(...)`: Computes the Bessel j0 function of `x` element-wise.\n\n`bessel_j1(...)`: Computes the Bessel j1 function of `x` element-wise.\n\n`bessel_k0(...)`: Computes the Bessel k0 function of `x` element-wise.\n\n`bessel_k0e(...)`: Computes the Bessel k0e function of `x` element-wise.\n\n`bessel_k1(...)`: Computes the Bessel k1 function of `x` element-wise.\n\n`bessel_k1e(...)`: Computes the Bessel k1e function of `x` element-wise.\n\n`bessel_y0(...)`: Computes the Bessel y0 function of `x` element-wise.\n\n`bessel_y1(...)`: Computes the Bessel y1 function of `x` element-wise.\n\n`dawsn(...)`: Computes Dawson's integral of `x` element-wise.\n\n`expint(...)`: Computes the Exponential integral of `x` element-wise.\n\n`fresnel_cos(...)`: Computes Fresnel's cosine integral of `x` element-wise.\n\n`fresnel_sin(...)`: Computes Fresnel's sine integral of `x` element-wise.\n\n`spence(...)`: Computes Spence's integral of `x` element-wise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.MetaGraphDef", "path": "compat/v1/metagraphdef", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class CollectionDefEntry`\n\n`class MetaInfoDef`\n\n`class SignatureDefEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.MetaGraphDef.CollectionDefEntry", "path": "compat/v1/metagraphdef/collectiondefentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.MetaGraphDef.MetaInfoDef", "path": "compat/v1/metagraphdef/metainfodef", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class FunctionAliasesEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.MetaGraphDef.MetaInfoDef.FunctionAliasesEntry", "path": "compat/v1/metagraphdef/metainfodef/functionaliasesentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.MetaGraphDef.SignatureDefEntry", "path": "compat/v1/metagraphdef/signaturedefentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics", "path": "compat/v1/metrics", "type": "tf.compat", "text": "\nEvaluation-related metrics.\n\n`accuracy(...)`: Calculates how often `predictions` matches `labels`.\n\n`auc(...)`: Computes the approximate AUC via a Riemann sum. (deprecated)\n\n`average_precision_at_k(...)`: Computes average precision@k of predictions\nwith respect to sparse labels.\n\n`false_negatives(...)`: Computes the total number of false negatives.\n\n`false_negatives_at_thresholds(...)`: Computes false negatives at provided\nthreshold values.\n\n`false_positives(...)`: Sum the weights of false positives.\n\n`false_positives_at_thresholds(...)`: Computes false positives at provided\nthreshold values.\n\n`mean(...)`: Computes the (weighted) mean of the given values.\n\n`mean_absolute_error(...)`: Computes the mean absolute error between the\nlabels and predictions.\n\n`mean_cosine_distance(...)`: Computes the cosine distance between the labels\nand predictions.\n\n`mean_iou(...)`: Calculate per-step mean Intersection-Over-Union (mIOU).\n\n`mean_per_class_accuracy(...)`: Calculates the mean of the per-class\naccuracies.\n\n`mean_relative_error(...)`: Computes the mean relative error by normalizing\nwith the given values.\n\n`mean_squared_error(...)`: Computes the mean squared error between the labels\nand predictions.\n\n`mean_tensor(...)`: Computes the element-wise (weighted) mean of the given\ntensors.\n\n`percentage_below(...)`: Computes the percentage of values less than the given\nthreshold.\n\n`precision(...)`: Computes the precision of the predictions with respect to\nthe labels.\n\n`precision_at_k(...)`: Computes precision@k of the predictions with respect to\nsparse labels.\n\n`precision_at_thresholds(...)`: Computes precision values for different\n`thresholds` on `predictions`.\n\n`precision_at_top_k(...)`: Computes precision@k of the predictions with\nrespect to sparse labels.\n\n`recall(...)`: Computes the recall of the predictions with respect to the\nlabels.\n\n`recall_at_k(...)`: Computes recall@k of the predictions with respect to\nsparse labels.\n\n`recall_at_thresholds(...)`: Computes various recall values for different\n`thresholds` on `predictions`.\n\n`recall_at_top_k(...)`: Computes recall@k of top-k predictions with respect to\nsparse labels.\n\n`root_mean_squared_error(...)`: Computes the root mean squared error between\nthe labels and predictions.\n\n`sensitivity_at_specificity(...)`: Computes the specificity at a given\nsensitivity.\n\n`sparse_average_precision_at_k(...)`: Renamed to `average_precision_at_k`,\nplease use that method instead. (deprecated)\n\n`sparse_precision_at_k(...)`: Renamed to `precision_at_k`, please use that\nmethod instead. (deprecated)\n\n`specificity_at_sensitivity(...)`: Computes the specificity at a given\nsensitivity.\n\n`true_negatives(...)`: Sum the weights of true_negatives.\n\n`true_negatives_at_thresholds(...)`: Computes true negatives at provided\nthreshold values.\n\n`true_positives(...)`: Sum the weights of true_positives.\n\n`true_positives_at_thresholds(...)`: Computes true positives at provided\nthreshold values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.accuracy", "path": "compat/v1/metrics/accuracy", "type": "tf.compat", "text": "\nCalculates how often `predictions` matches `labels`.\n\nThe `accuracy` function creates two local variables, `total` and `count` that\nare used to compute the frequency with which `predictions` matches `labels`.\nThis frequency is ultimately returned as `accuracy`: an idempotent operation\nthat simply divides `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `accuracy`.\nInternally, an `is_correct` operation computes a `Tensor` with elements 1.0\nwhere the corresponding elements of `predictions` and `labels` match and 0.0\notherwise. Then `update_op` increments `total` with the reduced sum of the\nproduct of `weights` and `is_correct`, and it increments `count` with the\nreduced sum of `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.auc", "path": "compat/v1/metrics/auc", "type": "tf.compat", "text": "\nComputes the approximate AUC via a Riemann sum. (deprecated)\n\nThe `auc` function creates four local variables, `true_positives`,\n`true_negatives`, `false_positives` and `false_negatives` that are used to\ncompute the AUC. To discretize the AUC curve, a linearly spaced set of\nthresholds is used to compute pairs of recall and precision values. The area\nunder the ROC-curve is therefore computed using the height of the recall\nvalues by the false positive rate, while the area under the PR-curve is the\ncomputed using the height of the precision values by the recall.\n\nThis value is ultimately returned as `auc`, an idempotent operation that\ncomputes the area under a discretized curve of precision versus recall values\n(computed using the aforementioned variables). The `num_thresholds` variable\ncontrols the degree of discretization with larger numbers of thresholds more\nclosely approximating the true AUC. The quality of the approximation may vary\ndramatically depending on `num_thresholds`.\n\nFor best results, `predictions` should be distributed approximately uniformly\nin the range [0, 1] and not peaked around 0 or 1. The quality of the AUC\napproximation may be poor if this is not the case. Setting `summation_method`\nto 'minoring' or 'majoring' can help quantify the error in the approximation\nby providing lower or upper bound estimate of the AUC. The `thresholds`\nparameter can be used to manually specify thresholds which split the\npredictions more evenly.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `auc`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.average_precision_at_k", "path": "compat/v1/metrics/average_precision_at_k", "type": "tf.compat", "text": "\nComputes average precision@k of predictions with respect to sparse labels.\n\n`average_precision_at_k` creates two local variables,\n`average_precision_at_<k>/total` and `average_precision_at_<k>/max`, that are\nused to compute the frequency. This frequency is ultimately returned as\n`average_precision_at_<k>`: an idempotent operation that simply divides\n`average_precision_at_<k>/total` by `average_precision_at_<k>/max`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`precision_at_<k>`. Internally, a `top_k` operation computes a `Tensor`\nindicating the top `k` `predictions`. Set operations applied to `top_k` and\n`labels` calculate the true positives and false positives weighted by\n`weights`. Then `update_op` increments `true_positive_at_<k>` and\n`false_positive_at_<k>` using these values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.false_negatives", "path": "compat/v1/metrics/false_negatives", "type": "tf.compat", "text": "\nComputes the total number of false negatives.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.false_negatives_at_thresholds", "path": "compat/v1/metrics/false_negatives_at_thresholds", "type": "tf.compat", "text": "\nComputes false negatives at provided threshold values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.false_positives", "path": "compat/v1/metrics/false_positives", "type": "tf.compat", "text": "\nSum the weights of false positives.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.false_positives_at_thresholds", "path": "compat/v1/metrics/false_positives_at_thresholds", "type": "tf.compat", "text": "\nComputes false positives at provided threshold values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean", "path": "compat/v1/metrics/mean", "type": "tf.compat", "text": "\nComputes the (weighted) mean of the given values.\n\nThe `mean` function creates two local variables, `total` and `count` that are\nused to compute the average of `values`. This average is ultimately returned\nas `mean` which is an idempotent operation that simply divides `total` by\n`count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `mean`.\n`update_op` increments `total` with the reduced sum of the product of `values`\nand `weights`, and it increments `count` with the reduced sum of `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_absolute_error", "path": "compat/v1/metrics/mean_absolute_error", "type": "tf.compat", "text": "\nComputes the mean absolute error between the labels and predictions.\n\nThe `mean_absolute_error` function creates two local variables, `total` and\n`count` that are used to compute the mean absolute error. This average is\nweighted by `weights`, and it is ultimately returned as `mean_absolute_error`:\nan idempotent operation that simply divides `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`mean_absolute_error`. Internally, an `absolute_errors` operation computes the\nabsolute value of the differences between `predictions` and `labels`. Then\n`update_op` increments `total` with the reduced sum of the product of\n`weights` and `absolute_errors`, and it increments `count` with the reduced\nsum of `weights`\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_cosine_distance", "path": "compat/v1/metrics/mean_cosine_distance", "type": "tf.compat", "text": "\nComputes the cosine distance between the labels and predictions.\n\nThe `mean_cosine_distance` function creates two local variables, `total` and\n`count` that are used to compute the average cosine distance between\n`predictions` and `labels`. This average is weighted by `weights`, and it is\nultimately returned as `mean_distance`, which is an idempotent operation that\nsimply divides `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`mean_distance`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_iou", "path": "compat/v1/metrics/mean_iou", "type": "tf.compat", "text": "\nCalculate per-step mean Intersection-Over-Union (mIOU).\n\nMean Intersection-Over-Union is a common evaluation metric for semantic image\nsegmentation, which first computes the IOU for each semantic class and then\ncomputes the average over classes. IOU is defined as follows: IOU =\ntrue_positive / (true_positive + false_positive + false_negative). The\npredictions are accumulated in a confusion matrix, weighted by `weights`, and\nmIOU is then calculated from it.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `mean_iou`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_per_class_accuracy", "path": "compat/v1/metrics/mean_per_class_accuracy", "type": "tf.compat", "text": "\nCalculates the mean of the per-class accuracies.\n\nCalculates the accuracy for each class, then takes the mean of that.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates the accuracy of each class and returns\nthem.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_relative_error", "path": "compat/v1/metrics/mean_relative_error", "type": "tf.compat", "text": "\nComputes the mean relative error by normalizing with the given values.\n\nThe `mean_relative_error` function creates two local variables, `total` and\n`count` that are used to compute the mean relative absolute error. This\naverage is weighted by `weights`, and it is ultimately returned as\n`mean_relative_error`: an idempotent operation that simply divides `total` by\n`count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`mean_reative_error`. Internally, a `relative_errors` operation divides the\nabsolute value of the differences between `predictions` and `labels` by the\n`normalizer`. Then `update_op` increments `total` with the reduced sum of the\nproduct of `weights` and `relative_errors`, and it increments `count` with the\nreduced sum of `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_squared_error", "path": "compat/v1/metrics/mean_squared_error", "type": "tf.compat", "text": "\nComputes the mean squared error between the labels and predictions.\n\nThe `mean_squared_error` function creates two local variables, `total` and\n`count` that are used to compute the mean squared error. This average is\nweighted by `weights`, and it is ultimately returned as `mean_squared_error`:\nan idempotent operation that simply divides `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`mean_squared_error`. Internally, a `squared_error` operation computes the\nelement-wise square of the difference between `predictions` and `labels`. Then\n`update_op` increments `total` with the reduced sum of the product of\n`weights` and `squared_error`, and it increments `count` with the reduced sum\nof `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.mean_tensor", "path": "compat/v1/metrics/mean_tensor", "type": "tf.compat", "text": "\nComputes the element-wise (weighted) mean of the given tensors.\n\nIn contrast to the `mean` function which returns a scalar with the mean, this\nfunction returns an average tensor with the same shape as the input tensors.\n\nThe `mean_tensor` function creates two local variables, `total_tensor` and\n`count_tensor` that are used to compute the average of `values`. This average\nis ultimately returned as `mean` which is an idempotent operation that simply\ndivides `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `mean`.\n`update_op` increments `total` with the reduced sum of the product of `values`\nand `weights`, and it increments `count` with the reduced sum of `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.percentage_below", "path": "compat/v1/metrics/percentage_below", "type": "tf.compat", "text": "\nComputes the percentage of values less than the given threshold.\n\nThe `percentage_below` function creates two local variables, `total` and\n`count` that are used to compute the percentage of `values` that fall below\n`threshold`. This rate is weighted by `weights`, and it is ultimately returned\nas `percentage` which is an idempotent operation that simply divides `total`\nby `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`percentage`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.precision", "path": "compat/v1/metrics/precision", "type": "tf.compat", "text": "\nComputes the precision of the predictions with respect to the labels.\n\nThe `precision` function creates two local variables, `true_positives` and\n`false_positives`, that are used to compute the precision. This value is\nultimately returned as `precision`, an idempotent operation that simply\ndivides `true_positives` by the sum of `true_positives` and `false_positives`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`precision`. `update_op` weights each prediction by the corresponding value in\n`weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.precision_at_k", "path": "compat/v1/metrics/precision_at_k", "type": "tf.compat", "text": "\nComputes precision@k of the predictions with respect to sparse labels.\n\nIf `class_id` is specified, we calculate precision by considering only the\nentries in the batch for which `class_id` is in the top-k highest\n`predictions`, and computing the fraction of them for which `class_id` is\nindeed a correct label. If `class_id` is not specified, we'll calculate\nprecision as how often on average a class among the top-k classes with the\nhighest predicted values of a batch entry is correct and can be found in the\nlabel for that entry.\n\n`precision_at_k` creates two local variables, `true_positive_at_<k>` and\n`false_positive_at_<k>`, that are used to compute the precision@k frequency.\nThis frequency is ultimately returned as `precision_at_<k>`: an idempotent\noperation that simply divides `true_positive_at_<k>` by total\n(`true_positive_at_<k>` \\+ `false_positive_at_<k>`).\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`precision_at_<k>`. Internally, a `top_k` operation computes a `Tensor`\nindicating the top `k` `predictions`. Set operations applied to `top_k` and\n`labels` calculate the true positives and false positives weighted by\n`weights`. Then `update_op` increments `true_positive_at_<k>` and\n`false_positive_at_<k>` using these values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.precision_at_thresholds", "path": "compat/v1/metrics/precision_at_thresholds", "type": "tf.compat", "text": "\nComputes precision values for different `thresholds` on `predictions`.\n\nThe `precision_at_thresholds` function creates four local variables,\n`true_positives`, `true_negatives`, `false_positives` and `false_negatives`\nfor various values of thresholds. `precision[i]` is defined as the total\nweight of values in `predictions` above `thresholds[i]` whose corresponding\nentry in `labels` is `True`, divided by the total weight of values in\n`predictions` above `thresholds[i]` (`true_positives[i] / (true_positives[i] +\nfalse_positives[i])`).\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`precision`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.precision_at_top_k", "path": "compat/v1/metrics/precision_at_top_k", "type": "tf.compat", "text": "\nComputes precision@k of the predictions with respect to sparse labels.\n\nDiffers from `sparse_precision_at_k` in that predictions must be in the form\nof top `k` class indices, whereas `sparse_precision_at_k` expects logits.\nRefer to `sparse_precision_at_k` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.recall", "path": "compat/v1/metrics/recall", "type": "tf.compat", "text": "\nComputes the recall of the predictions with respect to the labels.\n\nThe `recall` function creates two local variables, `true_positives` and\n`false_negatives`, that are used to compute the recall. This value is\nultimately returned as `recall`, an idempotent operation that simply divides\n`true_positives` by the sum of `true_positives` and `false_negatives`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` that updates these variables and returns the `recall`. `update_op`\nweights each prediction by the corresponding value in `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.recall_at_k", "path": "compat/v1/metrics/recall_at_k", "type": "tf.compat", "text": "\nComputes recall@k of the predictions with respect to sparse labels.\n\nIf `class_id` is specified, we calculate recall by considering only the\nentries in the batch for which `class_id` is in the label, and computing the\nfraction of them for which `class_id` is in the top-k `predictions`. If\n`class_id` is not specified, we'll calculate recall as how often on average a\nclass among the labels of a batch entry is in the top-k `predictions`.\n\n`sparse_recall_at_k` creates two local variables, `true_positive_at_<k>` and\n`false_negative_at_<k>`, that are used to compute the recall_at_k frequency.\nThis frequency is ultimately returned as `recall_at_<k>`: an idempotent\noperation that simply divides `true_positive_at_<k>` by total\n(`true_positive_at_<k>` \\+ `false_negative_at_<k>`).\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`recall_at_<k>`. Internally, a `top_k` operation computes a `Tensor`\nindicating the top `k` `predictions`. Set operations applied to `top_k` and\n`labels` calculate the true positives and false negatives weighted by\n`weights`. Then `update_op` increments `true_positive_at_<k>` and\n`false_negative_at_<k>` using these values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.recall_at_thresholds", "path": "compat/v1/metrics/recall_at_thresholds", "type": "tf.compat", "text": "\nComputes various recall values for different `thresholds` on `predictions`.\n\nThe `recall_at_thresholds` function creates four local variables,\n`true_positives`, `true_negatives`, `false_positives` and `false_negatives`\nfor various values of thresholds. `recall[i]` is defined as the total weight\nof values in `predictions` above `thresholds[i]` whose corresponding entry in\n`labels` is `True`, divided by the total weight of `True` values in `labels`\n(`true_positives[i] / (true_positives[i] + false_negatives[i])`).\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the `recall`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.recall_at_top_k", "path": "compat/v1/metrics/recall_at_top_k", "type": "tf.compat", "text": "\nComputes recall@k of top-k predictions with respect to sparse labels.\n\nDiffers from `recall_at_k` in that predictions must be in the form of top `k`\nclass indices, whereas `recall_at_k` expects logits. Refer to `recall_at_k`\nfor more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.root_mean_squared_error", "path": "compat/v1/metrics/root_mean_squared_error", "type": "tf.compat", "text": "\nComputes the root mean squared error between the labels and predictions.\n\nThe `root_mean_squared_error` function creates two local variables, `total`\nand `count` that are used to compute the root mean squared error. This average\nis weighted by `weights`, and it is ultimately returned as\n`root_mean_squared_error`: an idempotent operation that takes the square root\nof the division of `total` by `count`.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`root_mean_squared_error`. Internally, a `squared_error` operation computes\nthe element-wise square of the difference between `predictions` and `labels`.\nThen `update_op` increments `total` with the reduced sum of the product of\n`weights` and `squared_error`, and it increments `count` with the reduced sum\nof `weights`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.sensitivity_at_specificity", "path": "compat/v1/metrics/sensitivity_at_specificity", "type": "tf.compat", "text": "\nComputes the specificity at a given sensitivity.\n\nThe `sensitivity_at_specificity` function creates four local variables,\n`true_positives`, `true_negatives`, `false_positives` and `false_negatives`\nthat are used to compute the sensitivity at the given specificity value. The\nthreshold for the given specificity value is computed and used to evaluate the\ncorresponding sensitivity.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`sensitivity`. `update_op` increments the `true_positives`, `true_negatives`,\n`false_positives` and `false_negatives` counts with the weight of each case\nfound in the `predictions` and `labels`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\nFor additional information about specificity and sensitivity, see the\nfollowing: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.sparse_average_precision_at_k", "path": "compat/v1/metrics/sparse_average_precision_at_k", "type": "tf.compat", "text": "\nRenamed to `average_precision_at_k`, please use that method instead.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.sparse_precision_at_k", "path": "compat/v1/metrics/sparse_precision_at_k", "type": "tf.compat", "text": "\nRenamed to `precision_at_k`, please use that method instead. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.specificity_at_sensitivity", "path": "compat/v1/metrics/specificity_at_sensitivity", "type": "tf.compat", "text": "\nComputes the specificity at a given sensitivity.\n\nThe `specificity_at_sensitivity` function creates four local variables,\n`true_positives`, `true_negatives`, `false_positives` and `false_negatives`\nthat are used to compute the specificity at the given sensitivity value. The\nthreshold for the given sensitivity value is computed and used to evaluate the\ncorresponding specificity.\n\nFor estimation of the metric over a stream of data, the function creates an\n`update_op` operation that updates these variables and returns the\n`specificity`. `update_op` increments the `true_positives`, `true_negatives`,\n`false_positives` and `false_negatives` counts with the weight of each case\nfound in the `predictions` and `labels`.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\nFor additional information about specificity and sensitivity, see the\nfollowing: https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.true_negatives", "path": "compat/v1/metrics/true_negatives", "type": "tf.compat", "text": "\nSum the weights of true_negatives.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.true_negatives_at_thresholds", "path": "compat/v1/metrics/true_negatives_at_thresholds", "type": "tf.compat", "text": "\nComputes true negatives at provided threshold values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.true_positives", "path": "compat/v1/metrics/true_positives", "type": "tf.compat", "text": "\nSum the weights of true_positives.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.metrics.true_positives_at_thresholds", "path": "compat/v1/metrics/true_positives_at_thresholds", "type": "tf.compat", "text": "\nComputes true positives at provided threshold values.\n\nIf `weights` is `None`, weights default to 1. Use weights of 0 to mask values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.min_max_variable_partitioner", "path": "compat/v1/min_max_variable_partitioner", "type": "tf.compat", "text": "\nPartitioner to allocate minimum size per slice.\n\nReturns a partitioner that partitions the variable of given shape and dtype\nsuch that each partition has a minimum of `min_slice_size` slice of the\nvariable. The maximum number of such partitions (upper bound) is given by\n`max_partitions`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mixed_precision", "path": "compat/v1/mixed_precision", "type": "tf.compat", "text": "\nPublic API for tf.mixed_precision namespace.\n\n`experimental` module: Public API for tf.mixed_precision.experimental\nnamespace.\n\n`class DynamicLossScale`: Loss scale that dynamically adjusts itself.\n\n`class FixedLossScale`: Loss scale with a fixed value.\n\n`class LossScale`: Base class for all TF1 loss scales.\n\n`class MixedPrecisionLossScaleOptimizer`: An optimizer that applies loss\nscaling.\n\n`disable_mixed_precision_graph_rewrite(...)`: Disables the mixed precision\ngraph rewrite.\n\n`enable_mixed_precision_graph_rewrite(...)`: Enable mixed precision via a\ngraph rewrite.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite", "path": "compat/v1/mixed_precision/disable_mixed_precision_graph_rewrite", "type": "tf.compat", "text": "\nDisables the mixed precision graph rewrite.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.train.experimental.disable_mixed_precision_graph_rewrite`\n\nAfter this is called, the mixed precision graph rewrite will no longer run for\nnew Sessions, and so float32 operations will no longer be converted to float16\nin such Sessions. However, any existing Sessions will continue to have the\ngraph rewrite enabled if they were created after\n`enable_mixed_precision_graph_rewrite` was called but before\n`disable_mixed_precision_graph_rewrite` was called.\n\nThis does not undo the effects of loss scaling. Any optimizers wrapped with a\nLossScaleOptimizer will continue to do loss scaling, although this loss\nscaling will no longer be useful if the optimizer is used in new Sessions, as\nthe graph rewrite no longer converts the graph to use float16.\n\nThis function is useful for unit testing. A unit tests can test using the\nmixed precision graph rewrite, then disable it so future unit tests continue\nusing float32. If this is done, unit tests should not share a single session,\nas `enable_mixed_precision_graph_rewrite` and\n`disable_mixed_precision_graph_rewrite` have no effect on existing sessions.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite", "path": "compat/v1/mixed_precision/enable_mixed_precision_graph_rewrite", "type": "tf.compat", "text": "\nEnable mixed precision via a graph rewrite.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.train.experimental.enable_mixed_precision_graph_rewrite`\n\nMixed precision is the use of both float32 and float16 data types when\ntraining a model to improve performance. This is achieved via a graph rewrite\noperation and a loss-scale optimizer.\n\nPerforming arithmetic operations in float16 takes advantage of specialized\nprocessing units, such as NVIDIA Tensor Cores, for much higher arithmetic\nthroughput. However, due to the smaller representable range, performing the\nentire training with float16 can result in gradient underflow, that is, small\ngradient values becoming zeroes. Instead, performing only select arithmetic\noperations in float16 results in higher throughput and decreased training time\nwhen using compatible hardware accelerators while also reducing memory usage,\ntypically without sacrificing model accuracy.\n\nCalling `enable_mixed_precision_graph_rewrite(opt)` enables the graph rewrite\noperation before computing gradients. The function additionally returns an\n`Optimizer` (`opt`) wrapped with a `LossScaleOptimizer`. This prevents\nunderflow in the float16 tensors during the backward pass. An optimizer of\ntype `tf.train.Optimizer` or `tf.keras.optimizers.Optimizer` must be passed to\nthis function, which will then be wrapped to use loss scaling.\n\nThe graph rewrite operation changes the `dtype` of certain operations in the\ngraph from float32 to float16. There are several categories of operations that\nare either included or excluded by this rewrite operation. The following\ncategories of Ops are defined inside corresponding functions under the class\n`AutoMixedPrecisionLists` in  auto_mixed_precision_lists.h:\n\nWhen this function is used, gradients should only be computed and applied with\nthe returned optimizer, either by calling `opt.minimize()` or\n`opt.compute_gradients()` followed by `opt.apply_gradients()`. Gradients\nshould not be computed with `tf.gradients` or `tf.GradientTape`. This is\nbecause the returned optimizer will apply loss scaling, and `tf.gradients` or\n`tf.GradientTape` will not. If you do directly use `tf.gradients` or\n`tf.GradientTape`, your model may not converge due to float16 underflow\nproblems.\n\nWhen eager execution is enabled, the mixed precision graph rewrite is only\nenabled within `tf.function`s, as outside `tf.function`s, there is no graph.\n\nFor NVIDIA GPUs with Tensor cores, as a general performance guide, dimensions\n(such as batch size, input size, output size, and channel counts) should be\npowers of two if under 256, or otherwise divisible by 8 if above 256. For more\ninformation, check out the NVIDIA Deep Learning Performance Guide.\n\nCurrently, mixed precision is only enabled on NVIDIA Tensor Core GPUs with\nCompute Capability 7.0 and above (Volta, Turing, or newer architectures). The\nparts of the graph on CPUs and TPUs are untouched by the graph rewrite.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mixed_precision.experimental", "path": "compat/v1/mixed_precision/experimental", "type": "tf.compat", "text": "\nPublic API for tf.mixed_precision.experimental namespace.\n\n`class DynamicLossScale`: Loss scale that dynamically adjusts itself.\n\n`class FixedLossScale`: Loss scale with a fixed value.\n\n`class LossScale`: Base class for all TF1 loss scales.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer", "path": "compat/v1/mixed_precision/mixedprecisionlossscaleoptimizer", "type": "tf.compat", "text": "\nAn optimizer that applies loss scaling.\n\nInherits From: `Optimizer`\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.train.experimental.MixedPrecisionLossScaleOptimizer`\n\nLoss scaling is a process that multiplies the loss by a multiplier called the\nloss scale, and divides each gradient by the same multiplier. The pseudocode\nfor this process is:\n\nMathematically, loss scaling has no effect, but can help avoid numerical\nunderflow in intermediate gradients when float16 tensors are used for mixed\nprecision training. By multiplying the loss, each intermediate gradient will\nhave the same multiplier applied.\n\nThe loss scale can either be a fixed constant, chosen by the user, or be\ndynamically determined. Dynamically determining the loss scale is convenient\nas a loss scale does not have to be explicitly chosen. However it reduces\nperformance.\n\nThis optimizer wraps another optimizer and applies loss scaling to it via a\n`LossScale`. Loss scaling is applied whenever gradients are computed, such as\nthrough `minimize()`.\n\nView source\n\nApply gradients to variables.\n\nThis is the second part of `minimize()`. It returns an `Operation` that\nconditionally applies gradients if all gradient values are finite. Otherwise\nno update is performed (nor is `global_step` incremented).\n\nView source\n\nCompute gradients of `loss` for the variables in `var_list`.\n\nThis adjusts the dynamic range of the gradient evaluation by scaling up the\n`loss` value. The gradient values are then scaled back down by the reciprocal\nof the loss scale. This is useful in reduced precision training where small\ngradient values would otherwise underflow the representable range.\n\nView source\n\nView source\n\nReturn a slot named `name` created for `var` by the Optimizer.\n\nSome `Optimizer` subclasses use additional variables. For example `Momentum`\nand `Adagrad` use variables to accumulate updates. This method gives access to\nthese `Variable` objects if for some reason you need them.\n\nUse `get_slot_names()` to get the list of slot names created by the\n`Optimizer`.\n\nView source\n\nReturn a list of the names of slots created by the `Optimizer`.\n\nSee `get_slot()`.\n\nView source\n\nAdd operations to minimize `loss` by updating `var_list`.\n\nThis method simply combines calls `compute_gradients()` and\n`apply_gradients()`. If you want to process the gradient before applying them\ncall `compute_gradients()` and `apply_gradients()` explicitly instead of using\nthis function.\n\nWhen eager execution is enabled, `loss` should be a Python function that takes\nno arguments and computes the value to be minimized. Minimization (and\ngradient computation) is done with respect to the elements of `var_list` if\nnot None, else with respect to any trainable variables created during the\nexecution of the `loss` function. `gate_gradients`, `aggregation_method`,\n`colocate_gradients_with_ops` and `grad_loss` are ignored when eager execution\nis enabled.\n\nView source\n\nReturns the variables of the Optimizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mlir", "path": "compat/v1/mlir", "type": "tf.compat", "text": "\nPublic API for tf.mlir namespace.\n\n`experimental` module: Public API for tf.mlir.experimental namespace.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.mlir.experimental", "path": "compat/v1/mlir/experimental", "type": "tf.compat", "text": "\nPublic API for tf.mlir.experimental namespace.\n\n`convert_function(...)`: Import a ConcreteFunction and convert it to a textual\nMLIR module.\n\n`convert_graph_def(...)`: Import a GraphDef and convert it to a textual MLIR\nmodule.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.model_variables", "path": "compat/v1/model_variables", "type": "tf.compat", "text": "\nReturns all variables in the MODEL_VARIABLES collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.moving_average_variables", "path": "compat/v1/moving_average_variables", "type": "tf.compat", "text": "\nReturns all variables that maintain their moving averages.\n\nIf an `ExponentialMovingAverage` object is created and the `apply()` method is\ncalled on a list of variables, these variables will be added to the\n`GraphKeys.MOVING_AVERAGE_VARIABLES` collection. This convenience function\nreturns the contents of that collection.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.multinomial", "path": "compat/v1/multinomial", "type": "tf.compat", "text": "\nDraws samples from a multinomial distribution. (deprecated)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.random.multinomial`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.NameAttrList", "path": "compat/v1/nameattrlist", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class AttrEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.NameAttrList.AttrEntry", "path": "compat/v1/nameattrlist/attrentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nest", "path": "compat/v1/nest", "type": "tf.compat", "text": "\nPublic API for tf.nest namespace.\n\n`assert_same_structure(...)`: Asserts that two structures are nested in the\nsame way.\n\n`flatten(...)`: Returns a flat list from a given nested structure.\n\n`is_nested(...)`: Returns true if its input is a collections.abc.Sequence\n(except strings).\n\n`map_structure(...)`: Applies `func` to each entry in `structure` and returns\na new structure.\n\n`pack_sequence_as(...)`: Returns a given flattened sequence packed into a\ngiven structure.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn", "path": "compat/v1/nn", "type": "tf.compat", "text": "\nWrappers for primitive Neural Net (NN) Operations.\n\n`rnn_cell` module: Module for constructing RNN Cells.\n\n`all_candidate_sampler(...)`: Generate the set of all classes.\n\n`atrous_conv2d(...)`: Atrous convolution (a.k.a. convolution with holes or\ndilated convolution).\n\n`atrous_conv2d_transpose(...)`: The transpose of `atrous_conv2d`.\n\n`avg_pool(...)`: Performs the average pooling on the input.\n\n`avg_pool1d(...)`: Performs the average pooling on the input.\n\n`avg_pool2d(...)`: Performs the average pooling on the input.\n\n`avg_pool3d(...)`: Performs the average pooling on the input.\n\n`avg_pool_v2(...)`: Performs the avg pooling on the input.\n\n`batch_norm_with_global_normalization(...)`: Batch normalization.\n\n`batch_normalization(...)`: Batch normalization.\n\n`bias_add(...)`: Adds `bias` to `value`.\n\n`bidirectional_dynamic_rnn(...)`: Creates a dynamic version of bidirectional\nrecurrent neural network. (deprecated)\n\n`collapse_repeated(...)`: Merge repeated labels into single labels.\n\n`compute_accidental_hits(...)`: Compute the position ids in\n`sampled_candidates` matching `true_classes`.\n\n`compute_average_loss(...)`: Scales per-example losses with sample_weights and\ncomputes their average.\n\n`conv1d(...)`: Computes a 1-D convolution of input with rank `>=3` and a `3-D`\nfilter. (deprecated argument values) (deprecated argument values)\n\n`conv1d_transpose(...)`: The transpose of `conv1d`.\n\n`conv2d(...)`: Computes a 2-D convolution given 4-D `input` and `filter`\ntensors.\n\n`conv2d_backprop_filter(...)`: Computes the gradients of convolution with\nrespect to the filter.\n\n`conv2d_backprop_input(...)`: Computes the gradients of convolution with\nrespect to the input.\n\n`conv2d_transpose(...)`: The transpose of `conv2d`.\n\n`conv3d(...)`: Computes a 3-D convolution given 5-D `input` and `filter`\ntensors.\n\n`conv3d_backprop_filter(...)`: Computes the gradients of 3-D convolution with\nrespect to the filter.\n\n`conv3d_backprop_filter_v2(...)`: Computes the gradients of 3-D convolution\nwith respect to the filter.\n\n`conv3d_transpose(...)`: The transpose of `conv3d`.\n\n`conv_transpose(...)`: The transpose of `convolution`.\n\n`convolution(...)`: Computes sums of N-D convolutions (actually cross-\ncorrelation).\n\n`crelu(...)`: Computes Concatenated ReLU.\n\n`ctc_beam_search_decoder(...)`: Performs beam search decoding on the logits\ngiven in input.\n\n`ctc_beam_search_decoder_v2(...)`: Performs beam search decoding on the logits\ngiven in input.\n\n`ctc_greedy_decoder(...)`: Performs greedy decoding on the logits given in\ninput (best path).\n\n`ctc_loss(...)`: Computes the CTC (Connectionist Temporal Classification)\nLoss.\n\n`ctc_loss_v2(...)`: Computes CTC (Connectionist Temporal Classification) loss.\n\n`ctc_unique_labels(...)`: Get unique labels and indices for batched labels for\n`tf.nn.ctc_loss`.\n\n`depth_to_space(...)`: DepthToSpace for tensors of type T.\n\n`depthwise_conv2d(...)`: Depthwise 2-D convolution.\n\n`depthwise_conv2d_backprop_filter(...)`: Computes the gradients of depthwise\nconvolution with respect to the filter.\n\n`depthwise_conv2d_backprop_input(...)`: Computes the gradients of depthwise\nconvolution with respect to the input.\n\n`depthwise_conv2d_native(...)`: Computes a 2-D depthwise convolution.\n\n`depthwise_conv2d_native_backprop_filter(...)`: Computes the gradients of\ndepthwise convolution with respect to the filter.\n\n`depthwise_conv2d_native_backprop_input(...)`: Computes the gradients of\ndepthwise convolution with respect to the input.\n\n`dilation2d(...)`: Computes the grayscale dilation of 4-D `input` and 3-D\n`filter` tensors.\n\n`dropout(...)`: Computes dropout. (deprecated arguments)\n\n`dynamic_rnn(...)`: Creates a recurrent neural network specified by RNNCell\n`cell`. (deprecated)\n\n`elu(...)`: Computes exponential linear: `exp(features) - 1` if < 0,\n`features` otherwise.\n\n`embedding_lookup(...)`: Looks up embeddings for the given `ids` from a list\nof tensors.\n\n`embedding_lookup_sparse(...)`: Looks up embeddings for the given ids and\nweights from a list of tensors.\n\n`erosion2d(...)`: Computes the grayscale erosion of 4-D `value` and 3-D\n`kernel` tensors.\n\n`fixed_unigram_candidate_sampler(...)`: Samples a set of classes using the\nprovided (fixed) base distribution.\n\n`fractional_avg_pool(...)`: Performs fractional average pooling on the input.\n(deprecated)\n\n`fractional_max_pool(...)`: Performs fractional max pooling on the input.\n(deprecated)\n\n`fused_batch_norm(...)`: Batch normalization.\n\n`in_top_k(...)`: Says whether the targets are in the top `K` predictions.\n\n`l2_loss(...)`: L2 Loss.\n\n`l2_normalize(...)`: Normalizes along dimension `axis` using an L2 norm.\n(deprecated arguments)\n\n`leaky_relu(...)`: Compute the Leaky ReLU activation function.\n\n`learned_unigram_candidate_sampler(...)`: Samples a set of classes from a\ndistribution learned during training.\n\n`local_response_normalization(...)`: Local Response Normalization.\n\n`log_poisson_loss(...)`: Computes log Poisson loss given `log_input`.\n\n`log_softmax(...)`: Computes log softmax activations. (deprecated arguments)\n\n`log_uniform_candidate_sampler(...)`: Samples a set of classes using a log-\nuniform (Zipfian) base distribution.\n\n`lrn(...)`: Local Response Normalization.\n\n`max_pool(...)`: Performs the max pooling on the input.\n\n`max_pool1d(...)`: Performs the max pooling on the input.\n\n`max_pool2d(...)`: Performs the max pooling on the input.\n\n`max_pool3d(...)`: Performs the max pooling on the input.\n\n`max_pool_v2(...)`: Performs the max pooling on the input.\n\n`max_pool_with_argmax(...)`: Performs max pooling on the input and outputs\nboth max values and indices.\n\n`moments(...)`: Calculate the mean and variance of `x`.\n\n`nce_loss(...)`: Computes and returns the noise-contrastive estimation\ntraining loss.\n\n`normalize_moments(...)`: Calculate the mean and variance of based on the\nsufficient statistics.\n\n`pool(...)`: Performs an N-D pooling operation.\n\n`quantized_avg_pool(...)`: Produces the average pool of the input tensor for\nquantized types.\n\n`quantized_conv2d(...)`: Computes a 2D convolution given quantized 4D input\nand filter tensors.\n\n`quantized_max_pool(...)`: Produces the max pool of the input tensor for\nquantized types.\n\n`quantized_relu_x(...)`: Computes Quantized Rectified Linear X:\n`min(max(features, 0), max_value)`\n\n`raw_rnn(...)`: Creates an `RNN` specified by RNNCell `cell` and loop function\n`loop_fn`.\n\n`relu(...)`: Computes rectified linear: `max(features, 0)`.\n\n`relu6(...)`: Computes Rectified Linear 6: `min(max(features, 0), 6)`.\n\n`relu_layer(...)`: Computes Relu(x * weight + biases).\n\n`safe_embedding_lookup_sparse(...)`: Lookup embedding results, accounting for\ninvalid IDs and empty features.\n\n`sampled_softmax_loss(...)`: Computes and returns the sampled softmax training\nloss.\n\n`scale_regularization_loss(...)`: Scales the sum of the given regularization\nlosses by number of replicas.\n\n`selu(...)`: Computes scaled exponential linear: `scale * alpha *\n(exp(features) - 1)`\n\n`separable_conv2d(...)`: 2-D convolution with separable filters.\n\n`sigmoid(...)`: Computes sigmoid of `x` element-wise.\n\n`sigmoid_cross_entropy_with_logits(...)`: Computes sigmoid cross entropy given\n`logits`.\n\n`silu(...)`: Computes the SiLU or Swish activation function: `x * sigmoid(x)`.\n\n`softmax(...)`: Computes softmax activations. (deprecated arguments)\n\n`softmax_cross_entropy_with_logits(...)`: Computes softmax cross entropy\nbetween `logits` and `labels`. (deprecated)\n\n`softmax_cross_entropy_with_logits_v2(...)`: Computes softmax cross entropy\nbetween `logits` and `labels`. (deprecated arguments)\n\n`softplus(...)`: Computes softplus: `log(exp(features) + 1)`.\n\n`softsign(...)`: Computes softsign: `features / (abs(features) + 1)`.\n\n`space_to_batch(...)`: SpaceToBatch for 4-D tensors of type T.\n\n`space_to_depth(...)`: SpaceToDepth for tensors of type T.\n\n`sparse_softmax_cross_entropy_with_logits(...)`: Computes sparse softmax cross\nentropy between `logits` and `labels`.\n\n`static_bidirectional_rnn(...)`: Creates a bidirectional recurrent neural\nnetwork. (deprecated)\n\n`static_rnn(...)`: Creates a recurrent neural network specified by RNNCell\n`cell`. (deprecated)\n\n`static_state_saving_rnn(...)`: RNN that accepts a state saver for time-\ntruncated RNN calculation. (deprecated)\n\n`sufficient_statistics(...)`: Calculate the sufficient statistics for the mean\nand variance of `x`.\n\n`swish(...)`: Computes the SiLU or Swish activation function: `x *\nsigmoid(x)`.\n\n`tanh(...)`: Computes hyperbolic tangent of `x` element-wise.\n\n`top_k(...)`: Finds values and indices of the `k` largest entries for the last\ndimension.\n\n`uniform_candidate_sampler(...)`: Samples a set of classes using a uniform\nbase distribution.\n\n`weighted_cross_entropy_with_logits(...)`: Computes a weighted cross entropy.\n(deprecated arguments)\n\n`weighted_moments(...)`: Returns the frequency-weighted mean and variance of\n`x`.\n\n`with_space_to_batch(...)`: Performs `op` on the space-to-batch representation\nof `input`.\n\n`xw_plus_b(...)`: Computes matmul(x, weights) + biases.\n\n`zero_fraction(...)`: Returns the fraction of zeros in `value`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.avg_pool", "path": "compat/v1/nn/avg_pool", "type": "tf.compat", "text": "\nPerforms the average pooling on the input.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.avg_pool2d`\n\nEach entry in `output` is the mean of the corresponding size `ksize` window in\n`value`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.batch_norm_with_global_normalization", "path": "compat/v1/nn/batch_norm_with_global_normalization", "type": "tf.compat", "text": "\nBatch normalization.\n\nThis op is deprecated. See `tf.nn.batch_normalization`.\n\nBatch Normalization - Accelerating Deep Network Training by Reducing Internal\nCovariate Shift: Ioffe et al., 2015 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.bidirectional_dynamic_rnn", "path": "compat/v1/nn/bidirectional_dynamic_rnn", "type": "tf.compat", "text": "\nCreates a dynamic version of bidirectional recurrent neural network.\n(deprecated)\n\nTakes input and builds independent forward and backward RNNs. The input_size\nof forward and backward cell must match. The initial state for both directions\nis zero by default (but can be set optionally) and no intermediate states are\never returned -- the network is fully unrolled for the given (passed in)\nlength(s) of the sequence(s) or completely unrolled if length(s) is not given.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv1d", "path": "compat/v1/nn/conv1d", "type": "tf.compat", "text": "\nComputes a 1-D convolution of input with rank `>=3` and a `3-D` filter.\n(deprecated argument values) (deprecated argument values)\n\nGiven an input tensor of shape `batch_shape + [in_width, in_channels]` if\n`data_format` is `\"NWC\"`, or `batch_shape + [in_channels, in_width]` if\n`data_format` is `\"NCW\"`, and a filter / kernel tensor of shape\n`[filter_width, in_channels, out_channels]`, this op reshapes the arguments to\npass them to `conv2d` to perform the equivalent convolution operation.\n\nInternally, this op reshapes the input tensors and invokes `tf.nn.conv2d`. For\nexample, if `data_format` does not start with \"NC\", a tensor of shape\n`batch_shape + [in_width, in_channels]` is reshaped to `batch_shape + [1,\nin_width, in_channels]`, and the filter is reshaped to `[1, filter_width,\nin_channels, out_channels]`. The result is then reshaped back to `batch_shape\n+ [out_width, out_channels]` (where out_width is a function of the stride and\npadding as in conv2d) and returned to the caller.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv2d", "path": "compat/v1/nn/conv2d", "type": "tf.compat", "text": "\nComputes a 2-D convolution given 4-D `input` and `filter` tensors.\n\nGiven an input tensor of shape `[batch, in_height, in_width, in_channels]` and\na filter / kernel tensor of shape `[filter_height, filter_width, in_channels,\nout_channels]`, this op performs the following:\n\nIn detail, with the default NHWC format,\n\nMust have `strides[0] = strides[3] = 1`. For the most common case of the same\nhorizontal and vertical strides, `strides = [1, stride, stride, 1]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv2d_backprop_filter", "path": "compat/v1/nn/conv2d_backprop_filter", "type": "tf.compat", "text": "\nComputes the gradients of convolution with respect to the filter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv2d_backprop_input", "path": "compat/v1/nn/conv2d_backprop_input", "type": "tf.compat", "text": "\nComputes the gradients of convolution with respect to the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv2d_transpose", "path": "compat/v1/nn/conv2d_transpose", "type": "tf.compat", "text": "\nThe transpose of `conv2d`.\n\nThis operation is sometimes called \"deconvolution\" after (Zeiler et al.,\n2010), but is really the transpose (gradient) of `conv2d` rather than an\nactual deconvolution.\n\nDeconvolutional Networks: Zeiler et al., 2010 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv3d", "path": "compat/v1/nn/conv3d", "type": "tf.compat", "text": "\nComputes a 3-D convolution given 5-D `input` and `filter` tensors.\n\nIn signal processing, cross-correlation is a measure of similarity of two\nwaveforms as a function of a time-lag applied to one of them. This is also\nknown as a sliding dot product or sliding inner-product.\n\nOur Conv3D implements a form of cross-correlation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv3d_backprop_filter", "path": "compat/v1/nn/conv3d_backprop_filter", "type": "tf.compat", "text": "\nComputes the gradients of 3-D convolution with respect to the filter.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.nn.conv3d_backprop_filter_v2`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.conv3d_transpose", "path": "compat/v1/nn/conv3d_transpose", "type": "tf.compat", "text": "\nThe transpose of `conv3d`.\n\nThis operation is sometimes called \"deconvolution\" after (Zeiler et al.,\n2010), but is really the transpose (gradient) of `conv3d` rather than an\nactual deconvolution.\n\nDeconvolutional Networks: Zeiler et al., 2010 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.convolution", "path": "compat/v1/nn/convolution", "type": "tf.compat", "text": "\nComputes sums of N-D convolutions (actually cross-correlation).\n\nThis also supports either output striding via the optional `strides` parameter\nor atrous convolution (also known as convolution with holes or dilated\nconvolution, based on the French word \"trous\" meaning holes in English) via\nthe optional `dilation_rate` parameter. Currently, however, output striding is\nnot supported for atrous convolutions.\n\nSpecifically, in the case that `data_format` does not start with \"NC\", given a\nrank (N+2) `input` Tensor of shape\n\n[num_batches, input_spatial_shape[0], ..., input_spatial_shape[N-1],\nnum_input_channels],\n\na rank (N+2) `filter` Tensor of shape\n\n[spatial_filter_shape[0], ..., spatial_filter_shape[N-1], num_input_channels,\nnum_output_channels],\n\nan optional `dilation_rate` tensor of shape N specifying the filter\nupsampling/input downsampling rate, and an optional list of N `strides`\n(defaulting [1]*N), this computes for each N-D spatial output position (x[0],\n..., x[N-1]):\n\nwhere b is the index into the batch, k is the output channel number, q is the\ninput channel number, and z is the N-D spatial offset within the filter. Here,\n`padded_input` is obtained by zero padding the input using an effective\nspatial filter shape of `(spatial_filter_shape-1) * dilation_rate + 1` and\noutput striding `strides` as described in the comment here.\n\nIn the case that `data_format` does start with `\"NC\"`, the `input` and output\n(but not the `filter`) are simply transposed as follows:\n\nconvolution(input, data_format, **kwargs) =\ntf.transpose(convolution(tf.transpose(input, [0] + range(2,N+2) + [1]),\n**kwargs), [0, N+1] + range(1, N+1))\n\nIt is required that 1 <= N <= 3.\n\n`[batch_size] + output_spatial_shape + [out_channels]`\n\nif data_format is None or does not start with \"NC\", or\n\n`[batch_size, out_channels] + output_spatial_shape`\n\nif data_format starts with \"NC\", where `output_spatial_shape` depends on the\nvalue of `padding`.\n\nIf padding == \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] /\nstrides[i])\n\nIf padding == \"VALID\": output_spatial_shape[i] = ceil((input_spatial_shape[i]\n- (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.crelu", "path": "compat/v1/nn/crelu", "type": "tf.compat", "text": "\nComputes Concatenated ReLU.\n\nConcatenates a ReLU which selects only the positive part of the activation\nwith a ReLU which selects only the negative part of the activation. Note that\nas a result this non-linearity doubles the depth of the activations. Source:\nUnderstanding and Improving Convolutional Neural Networks via Concatenated\nRectified Linear Units. W. Shang, et al.\n\nUnderstanding and Improving Convolutional Neural Networks via Concatenated\nRectified Linear Units: Shang et al., 2016 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.ctc_beam_search_decoder", "path": "compat/v1/nn/ctc_beam_search_decoder", "type": "tf.compat", "text": "\nPerforms beam search decoding on the logits given in input.\n\nIf `merge_repeated` is `True`, merge repeated classes in the output beams.\nThis means that if consecutive entries in a beam are the same, only the first\nof these is emitted. That is, when the sequence is `A B B * B * B` (where '*'\nis the blank label), the return value is:\n\n`decoded[j].indices`: Indices matrix `(total_decoded_outputs[j] x 2)` The rows\nstore: [batch, time].\n\n`decoded[j].values`: Values vector, size `(total_decoded_outputs[j])`. The\nvector stores the decoded classes for beam j.\n\n`decoded[j].dense_shape`: Shape vector, size `(2)`. The shape values are:\n`[batch_size, max_decoded_length[j]]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.ctc_loss", "path": "compat/v1/nn/ctc_loss", "type": "tf.compat", "text": "\nComputes the CTC (Connectionist Temporal Classification) Loss.\n\nThis op implements the CTC loss as presented in (Graves et al., 2006).\n\nThis class performs the softmax operation for you, so inputs should be e.g.\nlinear projections of outputs by an LSTM.\n\nThe `inputs` Tensor's innermost dimension size, `num_classes`, represents\n`num_labels + 1` classes, where num_labels is the number of true labels, and\nthe largest value `(num_classes - 1)` is reserved for the blank label.\n\nFor example, for a vocabulary containing 3 labels `[a, b, c]`, `num_classes =\n4` and the labels indexing is `{a: 0, b: 1, c: 2, blank: 3}`.\n\nRegarding the arguments `preprocess_collapse_repeated` and\n`ctc_merge_repeated`:\n\nIf `preprocess_collapse_repeated` is True, then a preprocessing step runs\nbefore loss calculation, wherein repeated labels passed to the loss are merged\ninto single labels. This is useful if the training labels come from, e.g.,\nforced alignments and therefore have unnecessary repetitions.\n\nIf `ctc_merge_repeated` is set False, then deep within the CTC calculation,\nrepeated non-blank labels will not be merged and are interpreted as individual\nlabels. This is a simplified (non-standard) version of CTC.\n\nHere is a table of the (roughly) expected first order behavior:\n\n`preprocess_collapse_repeated=False`, `ctc_merge_repeated=True`\n\nClassical CTC behavior: Outputs true repeated classes with blanks in between,\nand can also output repeated classes with no blanks in between that need to be\ncollapsed by the decoder.\n\n`preprocess_collapse_repeated=True`, `ctc_merge_repeated=False`\n\nNever learns to output repeated classes, as they are collapsed in the input\nlabels before training.\n\n`preprocess_collapse_repeated=False`, `ctc_merge_repeated=False`\n\nOutputs repeated classes with blanks in between, but generally does not\nrequire the decoder to collapse/merge repeated classes.\n\n`preprocess_collapse_repeated=True`, `ctc_merge_repeated=True`\n\nUntested. Very likely will not learn to output repeated classes.\n\nThe `ignore_longer_outputs_than_inputs` option allows to specify the behavior\nof the CTCLoss when dealing with sequences that have longer outputs than\ninputs. If true, the CTCLoss will simply return zero gradient for those items,\notherwise an InvalidArgument error is returned, stopping training.\n\nConnectionist Temporal Classification - Labeling Unsegmented Sequence Data\nwith Recurrent Neural Networks: Graves et al., 2006 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.ctc_loss_v2", "path": "compat/v1/nn/ctc_loss_v2", "type": "tf.compat", "text": "\nComputes CTC (Connectionist Temporal Classification) loss.\n\nThis op implements the CTC loss as presented in (Graves et al., 2006).\n\nConnectionist Temporal Classification - Labeling Unsegmented Sequence Data\nwith Recurrent Neural Networks: Graves et al., 2006 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.depthwise_conv2d", "path": "compat/v1/nn/depthwise_conv2d", "type": "tf.compat", "text": "\nDepthwise 2-D convolution.\n\nGiven a 4D input tensor ('NHWC' or 'NCHW' data formats) and a filter tensor of\nshape `[filter_height, filter_width, in_channels, channel_multiplier]`\ncontaining `in_channels` convolutional filters of depth 1, `depthwise_conv2d`\napplies a different filter to each input channel (expanding from 1 channel to\n`channel_multiplier` channels for each), then concatenates the results\ntogether. The output has `in_channels * channel_multiplier` channels.\n\nIn detail, with the default NHWC format,\n\nMust have `strides[0] = strides[3] = 1`. For the most common case of the same\nhorizontal and vertical strides, `strides = [1, stride, stride, 1]`. If any\nvalue in `rate` is greater than 1, we perform atrous depthwise convolution, in\nwhich case all values in the `strides` tensor must be equal to 1.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.depthwise_conv2d_native", "path": "compat/v1/nn/depthwise_conv2d_native", "type": "tf.compat", "text": "\nComputes a 2-D depthwise convolution.\n\nGiven an input tensor of shape `[batch, in_height, in_width, in_channels]` and\na filter / kernel tensor of shape `[filter_height, filter_width, in_channels,\nchannel_multiplier]`, containing `in_channels` convolutional filters of depth\n1, `depthwise_conv2d` applies a different filter to each input channel\n(expanding from 1 channel to `channel_multiplier` channels for each), then\nconcatenates the results together. Thus, the output has `in_channels *\nchannel_multiplier` channels.\n\nMust have `strides[0] = strides[3] = 1`. For the most common case of the same\nhorizontal and vertices strides, `strides = [1, stride, stride, 1]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.dilation2d", "path": "compat/v1/nn/dilation2d", "type": "tf.compat", "text": "\nComputes the grayscale dilation of 4-D `input` and 3-D `filter` tensors.\n\nThe `input` tensor has shape `[batch, in_height, in_width, depth]` and the\n`filter` tensor has shape `[filter_height, filter_width, depth]`, i.e., each\ninput channel is processed independently of the others with its own\nstructuring function. The `output` tensor has shape `[batch, out_height,\nout_width, depth]`. The spatial dimensions of the output tensor depend on the\n`padding` algorithm. We currently only support the default \"NHWC\"\n`data_format`.\n\nIn detail, the grayscale morphological 2-D dilation is the max-sum correlation\n(for consistency with `conv2d`, we use unmirrored filters):\n\nMax-pooling is a special case when the filter has size equal to the pooling\nkernel size and contains all zeros.\n\nNote on duality: The dilation of `input` by the `filter` is equal to the\nnegation of the erosion of `-input` by the reflected `filter`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.dropout", "path": "compat/v1/nn/dropout", "type": "tf.compat", "text": "\nComputes dropout. (deprecated arguments)\n\nFor each element of `x`, with probability `rate`, outputs `0`, and otherwise\nscales up the input by `1 / (1-rate)`. The scaling is such that the expected\nsum is unchanged.\n\nBy default, each element is kept or dropped independently. If `noise_shape` is\nspecified, it must be broadcastable to the shape of `x`, and only dimensions\nwith `noise_shape[i] == shape(x)[i]` will make independent decisions. For\nexample, if `shape(x) = [k, l, m, n]` and `noise_shape = [k, 1, 1, n]`, each\nbatch and channel component will be kept independently and each row and column\nwill be kept or not kept together.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.dynamic_rnn", "path": "compat/v1/nn/dynamic_rnn", "type": "tf.compat", "text": "\nCreates a recurrent neural network specified by RNNCell `cell`. (deprecated)\n\nPerforms fully dynamic unrolling of `inputs`.\n\nIf time_major == False (default), this will be a `Tensor` shaped:\n`[batch_size, max_time, cell.output_size]`.\n\nIf time_major == True, this will be a `Tensor` shaped: `[max_time, batch_size,\ncell.output_size]`.\n\nNote, if `cell.output_size` is a (possibly nested) tuple of integers or\n`TensorShape` objects, then `outputs` will be a tuple having the same\nstructure as `cell.output_size`, containing Tensors having shapes\ncorresponding to the shape data in `cell.output_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.embedding_lookup", "path": "compat/v1/nn/embedding_lookup", "type": "tf.compat", "text": "\nLooks up embeddings for the given `ids` from a list of tensors.\n\nThis function is used to perform parallel lookups on the list of tensors in\n`params`. It is a generalization of `tf.gather`, where `params` is interpreted\nas a partitioning of a large embedding tensor. `params` may be a\n`PartitionedVariable` as returned by using `tf.compat.v1.get_variable()` with\na partitioner.\n\nIf `len(params) > 1`, each element `id` of `ids` is partitioned between the\nelements of `params` according to the `partition_strategy`. In all strategies,\nif the id space does not evenly divide the number of partitions, each of the\nfirst `(max_id + 1) % len(params)` partitions will be assigned one more id.\n\nIf `partition_strategy` is `\"mod\"`, we assign each id to partition `p = id %\nlen(params)`. For instance, 13 ids are split across 5 partitions as: `[[0, 5,\n10], [1, 6, 11], [2, 7, 12], [3, 8], [4, 9]]`\n\nIf `partition_strategy` is `\"div\"`, we assign ids to partitions in a\ncontiguous manner. In this case, 13 ids are split across 5 partitions as:\n`[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10], [11, 12]]`\n\nIf the input ids are ragged tensors, partition variables are not supported and\nthe partition strategy and the max_norm are ignored. The results of the lookup\nare concatenated into a dense tensor. The returned tensor has shape\n`shape(ids) + shape(params)[1:]`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.embedding_lookup_sparse", "path": "compat/v1/nn/embedding_lookup_sparse", "type": "tf.compat", "text": "\nLooks up embeddings for the given ids and weights from a list of tensors.\n\nThis op assumes that there is at least one id for each row in the dense tensor\nrepresented by sp_ids (i.e. there are no rows with empty features), and that\nall the indices of sp_ids are in canonical row-major order.\n\n`sp_ids` and `sp_weights` (if not None) are `SparseTensor`s with rank of 2.\nEmbeddings are always aggregated along the last dimension.\n\nIt also assumes that all id values lie in the range [0, p0), where p0 is the\nsum of the size of params along dimension 0.\n\nIn other words, if\n\n`shape(combined params) = [p0, p1, ..., pm]`\n\nand\n\n`shape(sp_ids) = shape(sp_weights) = [d0, d1]`\n\nthen\n\n`shape(output) = [d0, p1, ..., pm]`.\n\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\nwith `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.erosion2d", "path": "compat/v1/nn/erosion2d", "type": "tf.compat", "text": "\nComputes the grayscale erosion of 4-D `value` and 3-D `kernel` tensors.\n\nThe `value` tensor has shape `[batch, in_height, in_width, depth]` and the\n`kernel` tensor has shape `[kernel_height, kernel_width, depth]`, i.e., each\ninput channel is processed independently of the others with its own\nstructuring function. The `output` tensor has shape `[batch, out_height,\nout_width, depth]`. The spatial dimensions of the output tensor depend on the\n`padding` algorithm. We currently only support the default \"NHWC\"\n`data_format`.\n\nIn detail, the grayscale morphological 2-D erosion is given by:\n\nDuality: The erosion of `value` by the `kernel` is equal to the negation of\nthe dilation of `-value` by the reflected `kernel`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.fractional_avg_pool", "path": "compat/v1/nn/fractional_avg_pool", "type": "tf.compat", "text": "\nPerforms fractional average pooling on the input. (deprecated)\n\nThis is a deprecated version of `fractional_avg_pool`.\n\nFractional average pooling is similar to Fractional max pooling in the pooling\nregion generation step. The only difference is that after pooling regions are\ngenerated, a mean operation is performed instead of a max operation in each\npooling region.\n\nA tuple of `Tensor` objects (`output`, `row_pooling_sequence`,\n`col_pooling_sequence`). output: Output `Tensor` after fractional avg pooling.\nHas the same type as `value`. row_pooling_sequence: A `Tensor` of type\n`int64`. col_pooling_sequence: A `Tensor` of type `int64`.\n\nFractional Max-Pooling: Graham, 2015 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.fractional_max_pool", "path": "compat/v1/nn/fractional_max_pool", "type": "tf.compat", "text": "\nPerforms fractional max pooling on the input. (deprecated)\n\nThis is a deprecated version of `fractional_max_pool`.\n\nFractional max pooling is slightly different than regular max pooling. In\nregular max pooling, you downsize an input set by taking the maximum value of\nsmaller N x N subsections of the set (often 2x2), and try to reduce the set by\na factor of N, where N is an integer. Fractional max pooling, as you might\nexpect from the word \"fractional\", means that the overall reduction ratio N\ndoes not have to be an integer.\n\nThe sizes of the pooling regions are generated randomly but are fairly\nuniform. For example, let's look at the height dimension, and the constraints\non the list of rows that will be pool boundaries.\n\nFirst we define the following:\n\nThen, row_pooling_sequence should satisfy:\n\nA tuple of `Tensor` objects (`output`, `row_pooling_sequence`,\n`col_pooling_sequence`). output: Output `Tensor` after fractional max pooling.\nHas the same type as `value`. row_pooling_sequence: A `Tensor` of type\n`int64`. col_pooling_sequence: A `Tensor` of type `int64`.\n\nFractional Max-Pooling: Graham, 2015 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.fused_batch_norm", "path": "compat/v1/nn/fused_batch_norm", "type": "tf.compat", "text": "\nBatch normalization.\n\nSee Source: Batch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift; S. Ioffe, C. Szegedy.\n\nBatch Normalization - Accelerating Deep Network Training by Reducing Internal\nCovariate Shift: Ioffe et al., 2015 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.max_pool", "path": "compat/v1/nn/max_pool", "type": "tf.compat", "text": "\nPerforms the max pooling on the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.max_pool_with_argmax", "path": "compat/v1/nn/max_pool_with_argmax", "type": "tf.compat", "text": "\nPerforms max pooling on the input and outputs both max values and indices.\n\nThe indices in `argmax` are flattened, so that a maximum value at position\n`[b, y, x, c]` becomes flattened index: `(y * width + x) * channels + c` if\n`include_batch_in_index` is False; `((b * height + y) * width + x) * channels\n+ c` if `include_batch_in_index` is True.\n\nThe indices returned are always in `[0, height) x [0, width)` before\nflattening, even if padding is involved and the mathematically correct answer\nis outside (either negative or too large). This is a bug, but fixing it is\ndifficult to do in a safe backwards compatible way, especially due to\nflattening.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.moments", "path": "compat/v1/nn/moments", "type": "tf.compat", "text": "\nCalculate the mean and variance of `x`.\n\nThe mean and variance are calculated by aggregating the contents of `x` across\n`axes`. If `x` is 1-D and `axes = [0]` this is just the mean and variance of a\nvector.\n\nWhen using these moments for batch normalization (see\n`tf.nn.batch_normalization`):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.nce_loss", "path": "compat/v1/nn/nce_loss", "type": "tf.compat", "text": "\nComputes and returns the noise-contrastive estimation training loss.\n\nA common use case is to use this method for training, and calculate the full\nsigmoid loss for evaluation or inference. In this case, you must set\n`partition_strategy=\"div\"` for the two losses to be consistent, as in the\nfollowing example:\n\nNoise-contrastive estimation - A new estimation principle for unnormalized\nstatistical models: Gutmann et al., 2010 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.pool", "path": "compat/v1/nn/pool", "type": "tf.compat", "text": "\nPerforms an N-D pooling operation.\n\nIn the case that `data_format` does not start with \"NC\", computes for 0 <= b <\nbatch_size, 0 <= x[i] < output_spatial_shape[i], 0 <= c < num_channels:\n\nwhere the reduction function REDUCE depends on the value of `pooling_type`,\nand pad_before is defined based on the value of `padding` as described in the\n\"returns\" section of `tf.nn.convolution` for details. The reduction never\nincludes out-of-bounds positions.\n\nIn the case that `data_format` starts with `\"NC\"`, the `input` and output are\nsimply transposed as follows:\n\nif data_format is None or does not start with \"NC\", or\n\n[batch_size, num_channels] + output_spatial_shape\n\nif data_format starts with \"NC\", where `output_spatial_shape` depends on the\nvalue of padding:\n\nIf padding = \"SAME\": output_spatial_shape[i] = ceil(input_spatial_shape[i] /\nstrides[i])\n\nIf padding = \"VALID\": output_spatial_shape[i] = ceil((input_spatial_shape[i] -\n(window_shape[i] - 1) * dilation_rate[i]) / strides[i]).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.quantized_avg_pool", "path": "compat/v1/nn/quantized_avg_pool", "type": "tf.compat", "text": "\nProduces the average pool of the input tensor for quantized types.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.quantized_conv2d", "path": "compat/v1/nn/quantized_conv2d", "type": "tf.compat", "text": "\nComputes a 2D convolution given quantized 4D input and filter tensors.\n\nThe inputs are quantized tensors where the lowest value represents the real\nnumber of the associated minimum, and the highest represents the maximum. This\nmeans that you can only interpret the quantized output in the same way, by\ntaking the returned minimum and maximum values into account.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.quantized_max_pool", "path": "compat/v1/nn/quantized_max_pool", "type": "tf.compat", "text": "\nProduces the max pool of the input tensor for quantized types.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.quantized_relu_x", "path": "compat/v1/nn/quantized_relu_x", "type": "tf.compat", "text": "\nComputes Quantized Rectified Linear X: `min(max(features, 0), max_value)`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.raw_rnn", "path": "compat/v1/nn/raw_rnn", "type": "tf.compat", "text": "\nCreates an `RNN` specified by RNNCell `cell` and loop function `loop_fn`.\n\nThis function is a more primitive version of `dynamic_rnn` that provides more\ndirect access to the inputs each iteration. It also provides more control over\nwhen to start and finish reading the sequence, and what to emit for the\noutput.\n\nFor example, it can be used to implement the dynamic decoder of a seq2seq\nmodel.\n\nInstead of working with `Tensor` objects, most operations work with\n`TensorArray` objects directly.\n\nThe operation of `raw_rnn`, in pseudo-code, is basically the following:\n\nwith the additional properties that output and state may be (possibly nested)\ntuples, as determined by `cell.output_size` and `cell.state_size`, and as a\nresult the final `state` and `emit_ta` may themselves be tuples.\n\nA simple implementation of `dynamic_rnn` via `raw_rnn` looks like this:\n\n`emit_ta`: The RNN output `TensorArray`. If `loop_fn` returns a (possibly\nnested) set of Tensors for `emit_output` during initialization, (inputs `time\n= 0`, `cell_output = None`, and `loop_state = None`), then `emit_ta` will have\nthe same structure, dtypes, and shapes as `emit_output` instead. If `loop_fn`\nreturns `emit_output = None` during this call, the structure of\n`cell.output_size` is used: If `cell.output_size` is a (possibly nested) tuple\nof integers or `TensorShape` objects, then `emit_ta` will be a tuple having\nthe same structure as `cell.output_size`, containing TensorArrays whose\nelements' shapes correspond to the shape data in `cell.output_size`.\n\n`final_state`: The final cell state. If `cell.state_size` is an int, this will\nbe shaped `[batch_size, cell.state_size]`. If it is a `TensorShape`, this will\nbe shaped `[batch_size] + cell.state_size`. If it is a (possibly nested) tuple\nof ints or `TensorShape`, this will be a tuple having the corresponding\nshapes.\n\n`final_loop_state`: The final loop state as returned by `loop_fn`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.relu_layer", "path": "compat/v1/nn/relu_layer", "type": "tf.compat", "text": "\nComputes Relu(x * weight + biases).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell", "path": "compat/v1/nn/rnn_cell", "type": "tf.compat", "text": "\nModule for constructing RNN Cells.\n\n`class BasicLSTMCell`: DEPRECATED: Please use\n`tf.compat.v1.nn.rnn_cell.LSTMCell` instead.\n\n`class BasicRNNCell`: The most basic RNN cell.\n\n`class DeviceWrapper`: Operator that ensures an RNNCell runs on a particular\ndevice.\n\n`class DropoutWrapper`: Operator adding dropout to inputs and outputs of the\ngiven cell.\n\n`class GRUCell`: Gated Recurrent Unit cell.\n\n`class LSTMCell`: Long short-term memory unit (LSTM) recurrent network cell.\n\n`class LSTMStateTuple`: Tuple used by LSTM Cells for `state_size`,\n`zero_state`, and output state.\n\n`class MultiRNNCell`: RNN cell composed sequentially of multiple simple cells.\n\n`class RNNCell`: Abstract object representing an RNN cell.\n\n`class ResidualWrapper`: RNNCell wrapper that ensures cell inputs are added to\nthe outputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.BasicLSTMCell", "path": "compat/v1/nn/rnn_cell/basiclstmcell", "type": "tf.compat", "text": "\nDEPRECATED: Please use `tf.compat.v1.nn.rnn_cell.LSTMCell` instead.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nBasic LSTM recurrent network cell.\n\nThe implementation is based on\n\nWe add forget_bias (default: 1) to the biases of the forget gate in order to\nreduce the scale of forgetting in the beginning of the training.\n\nIt does not allow cell clipping, a projection layer, and does not use peep-\nhole connections: it is the basic baseline.\n\nFor advanced models, please use the full `tf.compat.v1.nn.rnn_cell.LSTMCell`\nthat follows.\n\nNote that this cell is not optimized for performance. Please use\n`tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n`tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\nbetter performance on CPU.\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.BasicRNNCell", "path": "compat/v1/nn/rnn_cell/basicrnncell", "type": "tf.compat", "text": "\nThe most basic RNN cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nNote that this cell is not optimized for performance. Please use\n`tf.contrib.cudnn_rnn.CudnnRNNTanh` for better performance on GPU.\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.DeviceWrapper", "path": "compat/v1/nn/rnn_cell/devicewrapper", "type": "tf.compat", "text": "\nOperator that ensures an RNNCell runs on a particular device.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.DropoutWrapper", "path": "compat/v1/nn/rnn_cell/dropoutwrapper", "type": "tf.compat", "text": "\nOperator adding dropout to inputs and outputs of the given cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.GRUCell", "path": "compat/v1/nn/rnn_cell/grucell", "type": "tf.compat", "text": "\nGated Recurrent Unit cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nNote that this cell is not optimized for performance. Please use\n`tf.contrib.cudnn_rnn.CudnnGRU` for better performance on GPU, or\n`tf.contrib.rnn.GRUBlockCellV2` for better performance on CPU.\n\nReferences: Learning Phrase Representations using RNN Encoder Decoder for\nStatistical Machine Translation: Cho et al., 2014 (pdf)\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.LSTMCell", "path": "compat/v1/nn/rnn_cell/lstmcell", "type": "tf.compat", "text": "\nLong short-term memory unit (LSTM) recurrent network cell.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nThe default non-peephole implementation is based on (Gers et al., 1999). The\npeephole implementation is based on (Sak et al., 2014).\n\nThe class uses optional peep-hole connections, optional cell clipping, and an\noptional projection layer.\n\nNote that this cell is not optimized for performance. Please use\n`tf.contrib.cudnn_rnn.CudnnLSTM` for better performance on GPU, or\n`tf.contrib.rnn.LSTMBlockCell` and `tf.contrib.rnn.LSTMBlockFusedCell` for\nbetter performance on CPU. References: Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic modeling: Sak et al., 2014\n(pdf) Learning to forget: Gers et al., 1999 (pdf) Long Short-Term Memory:\nHochreiter et al., 1997 (pdf)\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.LSTMStateTuple", "path": "compat/v1/nn/rnn_cell/lstmstatetuple", "type": "tf.compat", "text": "\nTuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n\nStores two elements: `(c, h)`, in that order. Where `c` is the hidden state\nand `h` is the output.\n\nOnly used when `state_is_tuple=True`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.MultiRNNCell", "path": "compat/v1/nn/rnn_cell/multirnncell", "type": "tf.compat", "text": "\nRNN cell composed sequentially of multiple simple cells.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.ResidualWrapper", "path": "compat/v1/nn/rnn_cell/residualwrapper", "type": "tf.compat", "text": "\nRNNCell wrapper that ensures cell inputs are added to the outputs.\n\nInherits From: `RNNCell`, `Layer`, `Layer`, `Module`\n\nView source\n\nView source\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.rnn_cell.RNNCell", "path": "compat/v1/nn/rnn_cell/rnncell", "type": "tf.compat", "text": "\nAbstract object representing an RNN cell.\n\nInherits From: `Layer`, `Layer`, `Module`\n\nEvery `RNNCell` must have the properties below and implement `call` with the\nsignature `(output, next_state) = call(input, state)`. The optional third\ninput argument, `scope`, is allowed for backwards compatibility purposes; but\nshould be left off for new subclasses.\n\nThis definition of cell differs from the definition used in the literature. In\nthe literature, 'cell' refers to an object with a single scalar output. This\ndefinition refers to a horizontal array of such units.\n\nAn RNN cell, in the most abstract setting, is anything that has a state and\nperforms some operation that takes a matrix of inputs. This operation results\nin an output matrix with `self.output_size` columns. If `self.state_size` is\nan integer, this operation also results in a new state matrix with\n`self.state_size` columns. If `self.state_size` is a (possibly nested tuple\nof) TensorShape object(s), then it should return a matching structure of\nTensors having shape `[batch_size].concatenate(s)` for each `s` in\n`self.batch_size`.\n\nIt can be represented by an Integer, a TensorShape or a tuple of Integers or\nTensorShapes.\n\nView source\n\nView source\n\nReturn zero-filled state tensor(s).\n\nIf `state_size` is a nested list or tuple, then the return value is a nested\nlist or tuple (of the same structure) of `2-D` tensors with the shapes\n`[batch_size, s]` for each s in `state_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.safe_embedding_lookup_sparse", "path": "compat/v1/nn/safe_embedding_lookup_sparse", "type": "tf.compat", "text": "\nLookup embedding results, accounting for invalid IDs and empty features.\n\nThe partitioned embedding in `embedding_weights` must all be the same shape\nexcept for the first dimension. The first dimension is allowed to vary as the\nvocabulary size is not necessarily a multiple of `P`. `embedding_weights` may\nbe a `PartitionedVariable` as returned by using `tf.compat.v1.get_variable()`\nwith a partitioner.\n\nInvalid IDs (< 0) are pruned from input IDs and weights, as well as any IDs\nwith non-positive weight. For an entry with no features, the embedding vector\nfor `default_id` is returned, or the 0-vector if `default_id` is not supplied.\n\nThe ids and weights may be multi-dimensional. Embeddings are always aggregated\nalong the last dimension.\n\nIn other words, if\n\n`shape(combined embedding_weights) = [p0, p1, ..., pm]`\n\nand\n\n`shape(sparse_ids) = shape(sparse_weights) = [d0, d1, ..., dn]`\n\nthen\n\n`shape(output) = [d0, d1, ... dn-1, p1, ..., pm]`.\n\nFor instance, if params is a 10x20 matrix, and sp_ids / sp_weights are\n\n`default_id` is 0.\n\nwith `combiner`=\"mean\", then the output will be a 3x20 matrix where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.sampled_softmax_loss", "path": "compat/v1/nn/sampled_softmax_loss", "type": "tf.compat", "text": "\nComputes and returns the sampled softmax training loss.\n\nThis is a faster way to train a softmax classifier over a huge number of\nclasses.\n\nThis operation is for training only. It is generally an underestimate of the\nfull softmax loss.\n\nA common use case is to use this method for training, and calculate the full\nsoftmax loss for evaluation or inference. In this case, you must set\n`partition_strategy=\"div\"` for the two losses to be consistent, as in the\nfollowing example:\n\nSee our Candidate Sampling Algorithms Reference (pdf). Also see Section 3 of\n(Jean et al., 2014) for the math.\n\nOn Using Very Large Target Vocabulary for Neural Machine Translation: Jean et\nal., 2014 (pdf)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.separable_conv2d", "path": "compat/v1/nn/separable_conv2d", "type": "tf.compat", "text": "\n2-D convolution with separable filters.\n\nPerforms a depthwise convolution that acts separately on channels followed by\na pointwise convolution that mixes channels. Note that this is separability\nbetween dimensions `[1, 2]` and `3`, not spatial separability between\ndimensions `1` and `2`.\n\nIn detail, with the default NHWC format,\n\n`strides` controls the strides for the depthwise convolution only, since the\npointwise convolution has implicit strides of `[1, 1, 1, 1]`. Must have\n`strides[0] = strides[3] = 1`. For the most common case of the same horizontal\nand vertical strides, `strides = [1, stride, stride, 1]`. If any value in\n`rate` is greater than 1, we perform atrous depthwise convolution, in which\ncase all values in the `strides` tensor must be equal to 1.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.sigmoid_cross_entropy_with_logits", "path": "compat/v1/nn/sigmoid_cross_entropy_with_logits", "type": "tf.compat", "text": "\nComputes sigmoid cross entropy given `logits`.\n\nMeasures the probability error in discrete classification tasks in which each\nclass is independent and not mutually exclusive. For instance, one could\nperform multilabel classification where a picture can contain both an elephant\nand a dog at the same time.\n\nFor brevity, let `x = logits`, `z = labels`. The logistic loss is\n\nFor x < 0, to avoid overflow in exp(-x), we reformulate the above\n\nHence, to ensure stability and avoid overflow, the implementation uses this\nequivalent formulation\n\n`logits` and `labels` must have the same type and shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.softmax_cross_entropy_with_logits", "path": "compat/v1/nn/softmax_cross_entropy_with_logits", "type": "tf.compat", "text": "\nComputes softmax cross entropy between `logits` and `labels`. (deprecated)\n\nFuture major versions of TensorFlow will allow gradients to flow into the\nlabels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nMeasures the probability error in discrete classification tasks in which the\nclasses are mutually exclusive (each entry is in exactly one class). For\nexample, each CIFAR-10 image is labeled with one and only one label: an image\ncan be a dog or a truck, but not both.\n\nIf using exclusive `labels` (wherein one and only one class is true at a\ntime), see `sparse_softmax_cross_entropy_with_logits`.\n\nA common use case is to have logits and labels of shape `[batch_size,\nnum_classes]`, but higher dimensions are supported, with the `dim` argument\nspecifying the class dimension.\n\nBackpropagation will happen only into `logits`. To calculate a cross entropy\nloss that allows backpropagation into both `logits` and `labels`, see\n`tf.nn.softmax_cross_entropy_with_logits_v2`.\n\nNote that to avoid confusion, it is required to pass only named arguments to\nthis function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2", "path": "compat/v1/nn/softmax_cross_entropy_with_logits_v2", "type": "tf.compat", "text": "\nComputes softmax cross entropy between `logits` and `labels`. (deprecated\narguments)\n\nMeasures the probability error in discrete classification tasks in which the\nclasses are mutually exclusive (each entry is in exactly one class). For\nexample, each CIFAR-10 image is labeled with one and only one label: an image\ncan be a dog or a truck, but not both.\n\nIf using exclusive `labels` (wherein one and only one class is true at a\ntime), see `sparse_softmax_cross_entropy_with_logits`.\n\nA common use case is to have logits and labels of shape `[batch_size,\nnum_classes]`, but higher dimensions are supported, with the `axis` argument\nspecifying the class dimension.\n\n`logits` and `labels` must have the same dtype (either `float16`, `float32`,\nor `float64`).\n\nBackpropagation will happen into both `logits` and `labels`. To disallow\nbackpropagation into `labels`, pass label tensors through `tf.stop_gradient`\nbefore feeding it to this function.\n\nNote that to avoid confusion, it is required to pass only named arguments to\nthis function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits", "path": "compat/v1/nn/sparse_softmax_cross_entropy_with_logits", "type": "tf.compat", "text": "\nComputes sparse softmax cross entropy between `logits` and `labels`.\n\nMeasures the probability error in discrete classification tasks in which the\nclasses are mutually exclusive (each entry is in exactly one class). For\nexample, each CIFAR-10 image is labeled with one and only one label: an image\ncan be a dog or a truck, but not both.\n\nA common use case is to have logits of shape `[batch_size, num_classes]` and\nhave labels of shape `[batch_size]`, but higher dimensions are supported, in\nwhich case the `dim`-th dimension is assumed to be of size `num_classes`.\n`logits` must have the dtype of `float16`, `float32`, or `float64`, and\n`labels` must have the dtype of `int32` or `int64`.\n\nNote that to avoid confusion, it is required to pass only named arguments to\nthis function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.static_bidirectional_rnn", "path": "compat/v1/nn/static_bidirectional_rnn", "type": "tf.compat", "text": "\nCreates a bidirectional recurrent neural network. (deprecated)\n\nSimilar to the unidirectional case above (rnn) but takes input and builds\nindependent forward and backward RNNs with the final forward and backward\noutputs depth-concatenated, such that the output will have the format\n[time][batch][cell_fw.output_size + cell_bw.output_size]. The input_size of\nforward and backward cell must match. The initial state for both directions is\nzero by default (but can be set optionally) and no intermediate states are\never returned -- the network is fully unrolled for the given (passed in)\nlength(s) of the sequence(s) or completely unrolled if length(s) is not given.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.static_rnn", "path": "compat/v1/nn/static_rnn", "type": "tf.compat", "text": "\nCreates a recurrent neural network specified by RNNCell `cell`. (deprecated)\n\nThe simplest form of RNN network generated is:\n\nHowever, a few other options are available:\n\nAn initial state can be provided. If the sequence_length vector is provided,\ndynamic calculation is performed. This method of calculation does not compute\nthe RNN steps past the maximum sequence length of the minibatch (thus saving\ncomputational time), and properly propagates the state at an example's\nsequence length to the final state output.\n\nThe dynamic calculation performed is, at time `t` for batch row `b`,\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.static_state_saving_rnn", "path": "compat/v1/nn/static_state_saving_rnn", "type": "tf.compat", "text": "\nRNN that accepts a state saver for time-truncated RNN calculation.\n(deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.sufficient_statistics", "path": "compat/v1/nn/sufficient_statistics", "type": "tf.compat", "text": "\nCalculate the sufficient statistics for the mean and variance of `x`.\n\nThese sufficient statistics are computed using the one pass algorithm on an\ninput that's optionally shifted. See:\nhttps://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.weighted_cross_entropy_with_logits", "path": "compat/v1/nn/weighted_cross_entropy_with_logits", "type": "tf.compat", "text": "\nComputes a weighted cross entropy. (deprecated arguments)\n\nThis is like `sigmoid_cross_entropy_with_logits()` except that `pos_weight`,\nallows one to trade off recall and precision by up- or down-weighting the cost\nof a positive error relative to a negative error.\n\nThe usual cross-entropy cost is defined as:\n\nA value `pos_weight > 1` decreases the false negative count, hence increasing\nthe recall. Conversely setting `pos_weight < 1` decreases the false positive\ncount and increases the precision. This can be seen from the fact that\n`pos_weight` is introduced as a multiplicative coefficient for the positive\nlabels term in the loss expression:\n\nFor brevity, let `x = logits`, `z = labels`, `q = pos_weight`. The loss is:\n\nSetting `l = (1 + (q - 1) * z)`, to ensure stability and avoid overflow, the\nimplementation uses\n\n`logits` and `labels` must have the same type and shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.weighted_moments", "path": "compat/v1/nn/weighted_moments", "type": "tf.compat", "text": "\nReturns the frequency-weighted mean and variance of `x`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.nn.xw_plus_b", "path": "compat/v1/nn/xw_plus_b", "type": "tf.compat", "text": "\nComputes matmul(x, weights) + biases.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.NodeDef", "path": "compat/v1/nodedef", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class AttrEntry`\n\n`class ExperimentalDebugInfo`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.NodeDef.AttrEntry", "path": "compat/v1/nodedef/attrentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.NodeDef.ExperimentalDebugInfo", "path": "compat/v1/nodedef/experimentaldebuginfo", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.norm", "path": "compat/v1/norm", "type": "tf.compat", "text": "\nComputes the norm of vectors, matrices, and tensors. (deprecated arguments)\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.linalg.norm`\n\nThis function can compute several different vector norms (the 1-norm, the\nEuclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0) and\nmatrix norms (Frobenius, 1-norm, 2-norm and inf-norm).\n\nMostly equivalent to numpy.linalg.norm. Not supported: ord <= 0, 2-norm for\nmatrices, nuclear norm. Other differences: a) If axis is `None`, treats the\nflattened `tensor` as a vector regardless of rank. b) Explicitly supports\n'euclidean' norm as the default, including for higher order tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.no_regularizer", "path": "compat/v1/no_regularizer", "type": "tf.compat", "text": "\nUse this function to prevent regularization of variables.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ones_like", "path": "compat/v1/ones_like", "type": "tf.compat", "text": "\nCreates a tensor with all elements set to 1.\n\nSee also `tf.ones`.\n\nGiven a single tensor (`tensor`), this operation returns a tensor of the same\ntype and shape as `tensor` with all elements set to 1. Optionally, you can\nspecify a new type (`dtype`) for the returned tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.OptimizerOptions", "path": "compat/v1/optimizeroptions", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.op_scope", "path": "compat/v1/op_scope", "type": "tf.compat", "text": "\nDEPRECATED. Same as name_scope above, just different argument order.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.pad", "path": "compat/v1/pad", "type": "tf.compat", "text": "\nPads a tensor.\n\nThis operation pads a `tensor` according to the `paddings` you specify.\n`paddings` is an integer tensor with shape `[n, 2]`, where n is the rank of\n`tensor`. For each dimension D of `input`, `paddings[D, 0]` indicates how many\nvalues to add before the contents of `tensor` in that dimension, and\n`paddings[D, 1]` indicates how many values to add after the contents of\n`tensor` in that dimension. If `mode` is \"REFLECT\" then both `paddings[D, 0]`\nand `paddings[D, 1]` must be no greater than `tensor.dim_size(D) - 1`. If\n`mode` is \"SYMMETRIC\" then both `paddings[D, 0]` and `paddings[D, 1]` must be\nno greater than `tensor.dim_size(D)`.\n\nThe padded size of each dimension D of the output is:\n\n`paddings[D, 0] + tensor.dim_size(D) + paddings[D, 1]`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.parse_example", "path": "compat/v1/parse_example", "type": "tf.compat", "text": "\nParses `Example` protos into a `dict` of tensors.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.io.parse_example`\n\nParses a number of serialized `Example` protos given in `serialized`. We refer\nto `serialized` as a batch with `batch_size` many entries of individual\n`Example` protos.\n\n`example_names` may contain descriptive names for the corresponding serialized\nprotos. These may be useful for debugging purposes, but they have no effect on\nthe output. If not `None`, `example_names` must be the same length as\n`serialized`.\n\nThis op parses serialized examples into a dictionary mapping keys to `Tensor`\n`SparseTensor`, and `RaggedTensor` objects. `features` is a dict from keys to\n`VarLenFeature`, `SparseFeature`, `RaggedFeature`, and `FixedLenFeature`\nobjects. Each `VarLenFeature` and `SparseFeature` is mapped to a\n`SparseTensor`; each `FixedLenFeature` is mapped to a `Tensor`; and each\n`RaggedFeature` is mapped to a `RaggedTensor`.\n\nEach `VarLenFeature` maps to a `SparseTensor` of the specified type\nrepresenting a ragged matrix. Its indices are `[batch, index]` where `batch`\nidentifies the example in `serialized`, and `index` is the value's index in\nthe list of values associated with that feature and example.\n\nEach `SparseFeature` maps to a `SparseTensor` of the specified type\nrepresenting a Tensor of `dense_shape` `[batch_size] + SparseFeature.size`.\nIts `values` come from the feature in the examples with key `value_key`. A\n`values[i]` comes from a position `k` in the feature of an example at batch\nentry `batch`. This positional information is recorded in `indices[i]` as\n`[batch, index_0, index_1, ...]` where `index_j` is the `k-th` value of the\nfeature in the example at with key `SparseFeature.index_key[j]`. In other\nwords, we split the indices (except the first index indicating the batch\nentry) of a `SparseTensor` by dimension into different features of the\n`Example`. Due to its complexity a `VarLenFeature` should be preferred over a\n`SparseFeature` whenever possible.\n\nEach `FixedLenFeature` `df` maps to a `Tensor` of the specified type (or\n`tf.float32` if not specified) and shape `(serialized.size(),) + df.shape`.\n\n`FixedLenFeature` entries with a `default_value` are optional. With no default\nvalue, we will fail if that `Feature` is missing from any example in\n`serialized`.\n\nEach `FixedLenSequenceFeature` `df` maps to a `Tensor` of the specified type\n(or `tf.float32` if not specified) and shape `(serialized.size(), None) +\ndf.shape`. All examples in `serialized` will be padded with `default_value`\nalong the second dimension.\n\nEach `RaggedFeature` maps to a `RaggedTensor` of the specified type. It is\nformed by stacking the `RaggedTensor` for each example, where the\n`RaggedTensor` for each individual example is constructed using the tensors\nspecified by `RaggedTensor.values_key` and `RaggedTensor.partition`. See the\n`tf.io.RaggedFeature` documentation for details and examples.\n\nFor example, if one expects a `tf.float32` `VarLenFeature` `ft` and three\nserialized `Example`s are provided:\n\nthen the output will look like:\n\nIf instead a `FixedLenSequenceFeature` with `default_value = -1.0` and\n`shape=[]` is used then the output will look like:\n\nGiven two `Example` input protos in `serialized`:\n\nAnd arguments\n\nThen the output is a dictionary:\n\nFor dense results in two serialized `Example`s:\n\nAnd the expected output is:\n\nAn alternative to `VarLenFeature` to obtain a `SparseTensor` is\n`SparseFeature`. For example, given two `Example` input protos in\n`serialized`:\n\nAnd arguments\n\nThen the output is a dictionary:\n\nSee the `tf.io.RaggedFeature` documentation for examples showing how\n`RaggedFeature` can be used to obtain `RaggedTensor`s.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.parse_single_example", "path": "compat/v1/parse_single_example", "type": "tf.compat", "text": "\nParses a single `Example` proto.\n\nCompat aliases for migration\n\nSee Migration guide for more details.\n\n`tf.compat.v1.io.parse_single_example`\n\nSimilar to `parse_example`, except:\n\nFor dense tensors, the returned `Tensor` is identical to the output of\n`parse_example`, except there is no batch dimension, the output shape is the\nsame as the shape given in `dense_shape`.\n\nFor `SparseTensor`s, the first (batch) column of the indices matrix is removed\n(the indices matrix is a column vector), the values vector is unchanged, and\nthe first (`batch_size`) entry of the shape vector is removed (it is now a\nsingle element vector).\n\nOne might see performance advantages by batching `Example` protos with\n`parse_example` instead of using this function directly.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.placeholder", "path": "compat/v1/placeholder", "type": "tf.compat", "text": "\nInserts a placeholder for a tensor that will be always fed.\n\nPlaceholders are not compatible with eager execution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.placeholder_with_default", "path": "compat/v1/placeholder_with_default", "type": "tf.compat", "text": "\nA placeholder op that passes through `input` when its output is not fed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.Print", "path": "compat/v1/print", "type": "tf.compat", "text": "\nPrints a list of tensors. (deprecated)\n\nThis is an identity op (behaves like `tf.identity`) with the side effect of\nprinting `data` when evaluating.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler", "path": "compat/v1/profiler", "type": "tf.compat", "text": "\nPublic API for tf.profiler namespace.\n\n`class AdviceProto`: A ProtocolMessage\n\n`class GraphNodeProto`: A ProtocolMessage\n\n`class MultiGraphNodeProto`: A ProtocolMessage\n\n`class OpLogProto`: A ProtocolMessage\n\n`class ProfileOptionBuilder`: Option Builder for Profiling API.\n\n`class Profiler`: TensorFlow multi-step profiler.\n\n`advise(...)`: Auto profile and advise.\n\n`profile(...)`: Profile model.\n\n`write_op_log(...)`: Log provided 'op_log', and add additional model\ninformation below.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.AdviceProto", "path": "compat/v1/profiler/adviceproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class Checker`\n\n`class CheckersEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.AdviceProto.Checker", "path": "compat/v1/profiler/adviceproto/checker", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.AdviceProto.CheckersEntry", "path": "compat/v1/profiler/adviceproto/checkersentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.advise", "path": "compat/v1/profiler/advise", "type": "tf.compat", "text": "\nAuto profile and advise.\n\nBuilds profiles and automatically check anomalies of various aspects. For more\ndetails:\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.GraphNodeProto", "path": "compat/v1/profiler/graphnodeproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class InputShapesEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.GraphNodeProto.InputShapesEntry", "path": "compat/v1/profiler/graphnodeproto/inputshapesentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.MultiGraphNodeProto", "path": "compat/v1/profiler/multigraphnodeproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.OpLogProto", "path": "compat/v1/profiler/oplogproto", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n`class IdToStringEntry`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.OpLogProto.IdToStringEntry", "path": "compat/v1/profiler/oplogproto/idtostringentry", "type": "tf.compat", "text": "\nA ProtocolMessage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.profile", "path": "compat/v1/profiler/profile", "type": "tf.compat", "text": "\nProfile model.\n\nTutorials and examples can be found in:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/profiler/g3doc/python_api.md\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.ProfileOptionBuilder", "path": "compat/v1/profiler/profileoptionbuilder", "type": "tf.compat", "text": "\nOption Builder for Profiling API.\n\nFor tutorial on the options, see\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md\n\nView source\n\nWhether only account the statistics of displayed profiler nodes.\n\nView source\n\nBuild a profiling option.\n\nView source\n\nOptions used to profile float operations.\n\nPlease see\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md\non the caveats of calculating float operations.\n\nView source\n\nOrder the displayed profiler nodes based on a attribute.\n\nSupported attribute includes micros, bytes, occurrence, params, etc.\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md\n\nView source\n\nSelect the attributes to display.\n\nSee\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/options.md\nfor supported attributes.\n\nView source\n\nShow operation time and memory consumptions.\n\nView source\n\nOptions used to profile trainable variable parameters.\n\nNormally used together with 'scope' view.\n\nView source\n\nSelectively counting statistics based on node types.\n\nHere, 'types' means the profiler nodes' properties. Profiler by default\nconsider device name (e.g. /job:xx/.../device:GPU:0) and operation type (e.g.\nMatMul) as profiler nodes' properties. User can also associate customized\n'types' to profiler nodes through OpLogProto proto.\n\nFor example, user can select profiler nodes placed on gpu:0 with:\n`account_type_regexes=['.*gpu:0.*']`\n\nIf none of a node's properties match the specified regexes, the node is not\ndisplayed nor accounted.\n\nView source\n\nDo not generate side-effect outputs.\n\nView source\n\nPrint the result to a file.\n\nView source\n\nSet the maximum depth of display.\n\nThe depth depends on profiling view. For 'scope' view, it's the depth of name\nscope hierarchy (tree), for 'op' view, it's the number of operation types\n(list), etc.\n\nView source\n\nOnly show profiler nodes consuming no less than 'min_micros'.\n\nView source\n\nOnly show profiler nodes consuming no less than 'min_float_ops'.\n\nPlease see\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/g3doc/profile_model_architecture.md\non the caveats of calculating float operations.\n\nView source\n\nOnly show profiler nodes consuming no less than 'min_bytes'.\n\nView source\n\nOnly show profiler nodes including no less than 'min_occurrence' graph nodes.\n\nA \"node\" means a profiler output node, which can be a python line (code view),\nan operation type (op view), or a graph node (graph/scope view). A python line\nincludes all graph nodes created by that line, while an operation type\nincludes all graph nodes of that type.\n\nView source\n\nOnly show profiler nodes holding no less than 'min_params' parameters.\n\n'Parameters' normally refers the weights of in TensorFlow variables. It\nreflects the 'capacity' of models.\n\nView source\n\nRegular expressions used to select profiler nodes to display.\n\nAfter 'with_accounted_types' is evaluated, 'with_node_names' are evaluated as\nfollows:\n\nFor a profile data structure, profiler first finds the profiler nodes matching\n'start_name_regexes', and starts displaying profiler nodes from there. Then,\nif a node matches 'show_name_regexes' and doesn't match 'hide_name_regexes',\nit's displayed. If a node matches 'trim_name_regexes', profiler stops further\nsearching that branch.\n\nView source\n\nGenerate a pprof profile gzip file.\n\npprof -png --nodecount=100 --sample_index=1\n\nView source\n\nPrint the result to stdout.\n\nView source\n\nWhich profile step to use for profiling.\n\nThe 'step' here refers to the step defined by `Profiler.add_step()` API.\n\nView source\n\nGenerate a timeline json file.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.Profiler", "path": "compat/v1/profiler/profiler", "type": "tf.compat", "text": "\nTensorFlow multi-step profiler.\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tensorflow/core/profiler/README.md\n\nView source\n\nAdd statistics of a step.\n\nView source\n\nAutomatically detect problems and generate reports.\n\nView source\n\nProfile the statistics of graph nodes, organized by dataflow graph.\n\nView source\n\nProfile the statistics of graph nodes, organized by name scope.\n\nView source\n\nProfile the statistics of the Operation types (e.g. MatMul, Conv2D).\n\nView source\n\nProfile the statistics of the Python codes.\n\nBy default, it shows the call stack from root. To avoid redundant output, you\nmay use options to filter as below options['show_name_regexes'] =\n['.my_code.py.']\n\nView source\n\nSerialize the ProfileProto to a binary string.\n\nUsers can write it to file for offline analysis by tfprof commandline or\ngraphical interface.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.profiler.write_op_log", "path": "compat/v1/profiler/write_op_log", "type": "tf.compat", "text": "\nLog provided 'op_log', and add additional model information below.\n\nThe API also assigns ops in tf.compat.v1.trainable_variables() an op type\ncalled '_trainable_variables'. The API also logs 'flops' statistics for ops\nwith op.RegisterStatistics() defined. flops calculation depends on Tensor\nshapes defined in 'graph', which might not be complete. 'run_meta', if\nprovided, completes the shape information with best effort.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.python_io", "path": "compat/v1/python_io", "type": "tf.compat", "text": "\nPython functions for directly manipulating TFRecord-formatted files.\n\n`class TFRecordCompressionType`: The type of compression for the record.\n\n`class TFRecordOptions`: Options used for manipulating TFRecord files.\n\n`class TFRecordWriter`: A class to write records to a TFRecords file.\n\n`tf_record_iterator(...)`: An iterator that read the records from a TFRecords\nfile. (deprecated)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.py_func", "path": "compat/v1/py_func", "type": "tf.compat", "text": "\nWraps a python function and uses it as a TensorFlow op.\n\nGiven a python function `func`, which takes numpy arrays as its arguments and\nreturns numpy arrays as its outputs, wrap this function as an operation in a\nTensorFlow graph. The following snippet constructs a simple TensorFlow graph\nthat invokes the `np.sinh()` NumPy function as a operation in the graph:\n\nThe body of the function (i.e. `func`) will not be serialized in a `GraphDef`.\nTherefore, you should not use this function if you need to serialize your\nmodel and restore it in a different environment.\n\nThe operation must run in the same address space as the Python program that\ncalls `tf.compat.v1.py_func()`. If you are using distributed TensorFlow, you\nmust run a `tf.distribute.Server` in the same process as the program that\ncalls `tf.compat.v1.py_func()` and you must pin the created operation to a\ndevice in that server (e.g. using `with tf.device():`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.quantization", "path": "compat/v1/quantization", "type": "tf.compat", "text": "\nPublic API for tf.quantization namespace.\n\n`dequantize(...)`: Dequantize the 'input' tensor into a float or bfloat16\nTensor.\n\n`fake_quant_with_min_max_args(...)`: Fake-quantize the 'inputs' tensor, type\nfloat to 'outputs' tensor of same type.\n\n`fake_quant_with_min_max_args_gradient(...)`: Compute gradients for a\nFakeQuantWithMinMaxArgs operation.\n\n`fake_quant_with_min_max_vars(...)`: Fake-quantize the 'inputs' tensor of type\nfloat via global float scalars\n\n`fake_quant_with_min_max_vars_gradient(...)`: Compute gradients for a\nFakeQuantWithMinMaxVars operation.\n\n`fake_quant_with_min_max_vars_per_channel(...)`: Fake-quantize the 'inputs'\ntensor of type float via per-channel floats\n\n`fake_quant_with_min_max_vars_per_channel_gradient(...)`: Compute gradients\nfor a FakeQuantWithMinMaxVarsPerChannel operation.\n\n`quantize(...)`: Quantize the 'input' tensor of type float to 'output' tensor\nof type 'T'.\n\n`quantize_and_dequantize(...)`: Quantizes then dequantizes a tensor.\n(deprecated)\n\n`quantize_and_dequantize_v2(...)`: Quantizes then dequantizes a tensor.\n\n`quantized_concat(...)`: Concatenates quantized tensors along one dimension.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.quantize_v2", "path": "compat/v1/quantize_v2", "type": "tf.compat", "text": "\nPlease use `tf.quantization.quantize` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.queue", "path": "compat/v1/queue", "type": "tf.compat", "text": "\nPublic API for tf.queue namespace.\n\n`class FIFOQueue`: A queue implementation that dequeues elements in first-in\nfirst-out order.\n\n`class PaddingFIFOQueue`: A FIFOQueue that supports batching variable-sized\ntensors by padding.\n\n`class PriorityQueue`: A queue implementation that dequeues elements in\nprioritized order.\n\n`class QueueBase`: Base class for queue implementations.\n\n`class RandomShuffleQueue`: A queue implementation that dequeues elements in a\nrandom order.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "tf.compat.v1.ragged", "path": "compat/v1/ragged", "type": "tf.compat", "text": "\nRagged Tensors.\n\nThis package defines ops for manipulating ragged tensors (`tf.RaggedTensor`),\nwhich are tensors with non-uniform shapes. In particular, each `RaggedTensor`\nhas one or more ragged dimensions, which are dimensions whose slices may have\ndifferent lengths. For example, the inner (column) dimension of `rt=[[3, 1, 4,\n1], [], [5, 9, 2], [6], []]` is ragged, since the column slices (`rt[0, :]`,\n..., `rt[4, :]`) have different lengths. For a more detailed description of\nragged tensors, see the `tf.RaggedTensor` class documentation and the Ragged\nTensor Guide.\n\nArguments that accept `RaggedTensor`s are marked in bold.\n\n`class RaggedTensorValue`: Represents the value of a `RaggedTensor`.\n\n`boolean_mask(...)`: Applies a boolean mask to `data` without flattening the\nmask dimensions.\n\n`constant(...)`: Constructs a constant RaggedTensor from a nested Python list.\n\n`constant_value(...)`: Constructs a RaggedTensorValue from a nested Python\nlist.\n\n`cross(...)`: Generates feature cross from a list of tensors.\n\n`cross_hashed(...)`: Generates hashed feature cross from a list of tensors.\n\n`map_flat_values(...)`: Applies `op` to the values of one or more\nRaggedTensors.\n\n`placeholder(...)`: Creates a placeholder for a `tf.RaggedTensor` that will\nalways be fed.\n\n`range(...)`: Returns a `RaggedTensor` containing the specified sequences of\nnumbers.\n\n`row_splits_to_segment_ids(...)`: Generates the segmentation corresponding to\na RaggedTensor `row_splits`.\n\n`segm