[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": "\nClears the cuFFT plan cache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": "\nA `int` that controls cache capacity of cuFFT plan.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch", "path": "torch", "type": "torch", "text": "\nThe torch package contains data structures for multi-dimensional tensors and\ndefines mathematical operations over these tensors. Additionally, it provides\nmany utilities for efficient serializing of Tensors and arbitrary types, and\nother useful utilities.\n\nIt has a CUDA counterpart, that enables you to run your tensor computations on\nan NVIDIA GPU with compute capability >= 3.0\n\nReturns True if `obj` is a PyTorch tensor.\n\nReturns True if `obj` is a PyTorch storage object.\n\nReturns True if the data type of `input` is a complex data type i.e., one of\n`torch.complex64`, and `torch.complex128`.\n\nReturns True if the data type of `input` is a floating point data type i.e.,\none of `torch.float64`, `torch.float32`, `torch.float16`, and\n`torch.bfloat16`.\n\nReturns True if the `input` is a single element tensor which is not equal to\nzero after type conversions.\n\nSets the default floating point dtype to `d`.\n\nGet the current default floating point `torch.dtype`.\n\nSets the default `torch.Tensor` type to floating point tensor type `t`.\n\nReturns the total number of elements in the `input` tensor.\n\nSet options for printing.\n\nDisables denormal floating numbers on CPU.\n\nNote\n\nRandom sampling creation ops are listed under Random sampling and include:\n`torch.rand()` `torch.rand_like()` `torch.randn()` `torch.randn_like()`\n`torch.randint()` `torch.randint_like()` `torch.randperm()` You may also use\n`torch.empty()` with the In-place random sampling methods to create\n`torch.Tensor` s with values sampled from a broader range of distributions.\n\nConstructs a tensor with `data`.\n\nConstructs a sparse tensor in COO(rdinate) format with specified values at the\ngiven `indices`.\n\nConvert the data into a `torch.Tensor`.\n\nCreate a view of an existing `torch.Tensor` `input` with specified `size`,\n`stride` and `storage_offset`.\n\nCreates a `Tensor` from a `numpy.ndarray`.\n\nReturns a tensor filled with the scalar value `0`, with the shape defined by\nthe variable argument `size`.\n\nReturns a tensor filled with the scalar value `0`, with the same size as\n`input`.\n\nReturns a tensor filled with the scalar value `1`, with the shape defined by\nthe variable argument `size`.\n\nReturns a tensor filled with the scalar value `1`, with the same size as\n`input`.\n\nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rceil with values from the interval `[start,\nend)` taken with common difference `step` beginning from `start`.\n\nReturns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rfloor + 1 with values from `start` to `end`\nwith step `step`.\n\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from `start` to `end`, inclusive.\n\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale\nwith base `base`.\n\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\nReturns a tensor filled with uninitialized data.\n\nReturns an uninitialized tensor with the same size as `input`.\n\nReturns a tensor filled with uninitialized data.\n\nCreates a tensor of size `size` filled with `fill_value`.\n\nReturns a tensor with the same size as `input` filled with `fill_value`.\n\nConverts a float tensor to a quantized tensor with given scale and zero point.\n\nConverts a float tensor to a per-channel quantized tensor with given scales\nand zero points.\n\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\nConstructs a complex tensor with its real part equal to `real` and its\nimaginary part equal to `imag`.\n\nConstructs a complex tensor whose elements are Cartesian coordinates\ncorresponding to the polar coordinates with absolute value `abs` and angle\n`angle`.\n\nComputes the Heaviside step function for each element in `input`.\n\nConcatenates the given sequence of `seq` tensors in the given dimension.\n\nSplits a tensor into a specific number of chunks.\n\nCreates a new tensor by horizontally stacking the tensors in `tensors`.\n\nStack tensors in sequence depthwise (along third axis).\n\nGathers values along an axis specified by `dim`.\n\nStack tensors in sequence horizontally (column wise).\n\nReturns a new tensor which indexes the `input` tensor along dimension `dim`\nusing the entries in `index` which is a `LongTensor`.\n\nReturns a new 1-D tensor which indexes the `input` tensor according to the\nboolean mask `mask` which is a `BoolTensor`.\n\nMoves the dimension(s) of `input` at the position(s) in `source` to the\nposition(s) in `destination`.\n\nAlias for `torch.movedim()`.\n\nReturns a new tensor that is a narrowed version of `input` tensor.\n\nReturns a tensor with the same data and number of elements as `input`, but\nwith the specified shape.\n\nAlias of `torch.vstack()`.\n\nOut-of-place version of `torch.Tensor.scatter_()`\n\nOut-of-place version of `torch.Tensor.scatter_add_()`\n\nSplits the tensor into chunks.\n\nReturns a tensor with all the dimensions of `input` of size `1` removed.\n\nConcatenates a sequence of tensors along a new dimension.\n\nAlias for `torch.transpose()`.\n\nAlias for `torch.transpose()`.\n\nExpects `input` to be <= 2-D tensor and transposes dimensions 0 and 1.\n\nReturns a new tensor with the elements of `input` at the given indices.\n\nSplits a tensor into multiple sub-tensors, all of which are views of `input`,\nalong dimension `dim` according to the indices or number of sections specified\nby `indices_or_sections`.\n\nConstructs a tensor by repeating the elements of `input`.\n\nReturns a tensor that is a transposed version of `input`.\n\nRemoves a tensor dimension.\n\nReturns a new tensor with a dimension of size one inserted at the specified\nposition.\n\nStack tensors in sequence vertically (row wise).\n\nReturn a tensor of elements selected from either `x` or `y`, depending on\n`condition`.\n\nCreates and returns a generator object that manages the state of the algorithm\nwhich produces pseudo random numbers.\n\nSets the seed for generating random numbers to a non-deterministic random\nnumber.\n\nSets the seed for generating random numbers.\n\nReturns the initial seed for generating random numbers as a Python `long`.\n\nReturns the random number generator state as a `torch.ByteTensor`.\n\nSets the random number generator state.\n\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\nReturns a tensor where each row contains `num_samples` indices sampled from\nthe multinomial probability distribution located in the corresponding row of\ntensor `input`.\n\nReturns a tensor of random numbers drawn from separate normal distributions\nwhose mean and standard deviation are given.\n\nReturns a tensor of the same size as `input` with each element sampled from a\nPoisson distribution with rate parameter given by the corresponding element in\n`input` i.e.,\n\nReturns a tensor filled with random numbers from a uniform distribution on the\ninterval [0,1)[0, 1)\n\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a uniform distribution on the interval [0,1)[0, 1) .\n\nReturns a tensor filled with random integers generated uniformly between `low`\n(inclusive) and `high` (exclusive).\n\nReturns a tensor with the same shape as Tensor `input` filled with random\nintegers generated uniformly between `low` (inclusive) and `high` (exclusive).\n\nReturns a tensor filled with random numbers from a normal distribution with\nmean `0` and variance `1` (also called the standard normal distribution).\n\nReturns a tensor with the same size as `input` that is filled with random\nnumbers from a normal distribution with mean 0 and variance 1.\n\nReturns a random permutation of integers from `0` to `n - 1`.\n\nThere are a few more in-place random sampling functions defined on Tensors as\nwell. Click through to refer to their documentation:\n\n`quasirandom.SobolEngine`\n\nThe `torch.quasirandom.SobolEngine` is an engine for generating (scrambled)\nSobol sequences.\n\nSaves an object to a disk file.\n\nLoads an object saved with `torch.save()` from a file.\n\nReturns the number of threads used for parallelizing CPU operations\n\nSets the number of threads used for intraop parallelism on CPU.\n\nReturns the number of threads used for inter-op parallelism on CPU (e.g.\n\nSets the number of threads used for interop parallelism (e.g.\n\nThe context managers `torch.no_grad()`, `torch.enable_grad()`, and\n`torch.set_grad_enabled()` are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more\ndetails on their usage. These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the `threading` module, etc.\n\nExamples:\n\nContext-manager that disabled gradient calculation.\n\nContext-manager that enables gradient calculation.\n\nContext-manager that sets gradient calculation to on or off.\n\nComputes the absolute value of each element in `input`.\n\nAlias for `torch.abs()`\n\nComputes the inverse cosine of each element in `input`.\n\nAlias for `torch.acos()`.\n\nReturns a new tensor with the inverse hyperbolic cosine of the elements of\n`input`.\n\nAlias for `torch.acosh()`.\n\nAdds the scalar `other` to each element of the input `input` and returns a new\nresulting tensor.\n\nPerforms the element-wise division of `tensor1` by `tensor2`, multiply the\nresult by the scalar `value` and add it to `input`.\n\nPerforms the element-wise multiplication of `tensor1` by `tensor2`, multiply\nthe result by the scalar `value` and add it to `input`.\n\nComputes the element-wise angle (in radians) of the given `input` tensor.\n\nReturns a new tensor with the arcsine of the elements of `input`.\n\nAlias for `torch.asin()`.\n\nReturns a new tensor with the inverse hyperbolic sine of the elements of\n`input`.\n\nAlias for `torch.asinh()`.\n\nReturns a new tensor with the arctangent of the elements of `input`.\n\nAlias for `torch.atan()`.\n\nReturns a new tensor with the inverse hyperbolic tangent of the elements of\n`input`.\n\nAlias for `torch.atanh()`.\n\nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}\nwith consideration of the quadrant.\n\nComputes the bitwise NOT of the given input tensor.\n\nComputes the bitwise AND of `input` and `other`.\n\nComputes the bitwise OR of `input` and `other`.\n\nComputes the bitwise XOR of `input` and `other`.\n\nReturns a new tensor with the ceil of the elements of `input`, the smallest\ninteger greater than or equal to each element.\n\nClamp all elements in `input` into the range `[` `min`, `max` `]`.\n\nAlias for `torch.clamp()`.\n\nComputes the element-wise conjugate of the given `input` tensor.\n\nCreate a new floating-point tensor with the magnitude of `input` and the sign\nof `other`, elementwise.\n\nReturns a new tensor with the cosine of the elements of `input`.\n\nReturns a new tensor with the hyperbolic cosine of the elements of `input`.\n\nReturns a new tensor with each of the elements of `input` converted from\nangles in degrees to radians.\n\nDivides each element of the input `input` by the corresponding element of\n`other`.\n\nAlias for `torch.div()`.\n\nComputes the logarithmic derivative of the gamma function on `input`.\n\nComputes the error function of each element.\n\nComputes the complementary error function of each element of `input`.\n\nComputes the inverse error function of each element of `input`.\n\nReturns a new tensor with the exponential of the elements of the input tensor\n`input`.\n\nComputes the base two exponential function of `input`.\n\nReturns a new tensor with the exponential of the elements minus 1 of `input`.\n\nReturns a new tensor with the data in `input` fake quantized per channel using\n`scale`, `zero_point`, `quant_min` and `quant_max`, across the channel\nspecified by `axis`.\n\nReturns a new tensor with the data in `input` fake quantized using `scale`,\n`zero_point`, `quant_min` and `quant_max`.\n\nAlias for `torch.trunc()`\n\nRaises `input` to the power of `exponent`, elementwise, in double precision.\n\nReturns a new tensor with the floor of the elements of `input`, the largest\ninteger less than or equal to each element.\n\nComputes the element-wise remainder of division.\n\nComputes the fractional portion of each element in `input`.\n\nReturns a new tensor containing imaginary values of the `self` tensor.\n\nMultiplies `input` by 2**:attr:`other`.\n\nDoes a linear interpolation of two tensors `start` (given by `input`) and\n`end` based on a scalar or tensor `weight` and returns the resulting `out`\ntensor.\n\nComputes the logarithm of the gamma function on `input`.\n\nReturns a new tensor with the natural logarithm of the elements of `input`.\n\nReturns a new tensor with the logarithm to the base 10 of the elements of\n`input`.\n\nReturns a new tensor with the natural logarithm of (1 + `input`).\n\nReturns a new tensor with the logarithm to the base 2 of the elements of\n`input`.\n\nLogarithm of the sum of exponentiations of the inputs.\n\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\nComputes the element-wise logical AND of the given input tensors.\n\nComputes the element-wise logical NOT of the given input tensor.\n\nComputes the element-wise logical OR of the given input tensors.\n\nComputes the element-wise logical XOR of the given input tensors.\n\nReturns a new tensor with the logit of the elements of `input`.\n\nGiven the legs of a right triangle, return its hypotenuse.\n\nComputes the zeroth order modified Bessel function of the first kind for each\nelement of `input`.\n\nComputes the regularized lower incomplete gamma function:\n\nComputes the regularized upper incomplete gamma function:\n\nMultiplies each element of the input `input` with the scalar `other` and\nreturns a new resulting tensor.\n\nAlias for `torch.mul()`.\n\nComputes the multivariate log-gamma function) with dimension pp element-wise,\ngiven by\n\nReplaces `NaN`, positive infinity, and negative infinity values in `input`\nwith the values specified by `nan`, `posinf`, and `neginf`, respectively.\n\nReturns a new tensor with the negative of the elements of `input`.\n\nAlias for `torch.neg()`\n\nReturn the next floating-point value after `input` towards `other`,\nelementwise.\n\nComputes the nthn^{th} derivative of the digamma function on `input`.\n\nTakes the power of each element in `input` with `exponent` and returns a\ntensor with the result.\n\nReturns a new tensor with each of the elements of `input` converted from\nangles in radians to degrees.\n\nReturns a new tensor containing real values of the `self` tensor.\n\nReturns a new tensor with the reciprocal of the elements of `input`\n\nComputes the element-wise remainder of division.\n\nReturns a new tensor with each of the elements of `input` rounded to the\nclosest integer.\n\nReturns a new tensor with the reciprocal of the square-root of each of the\nelements of `input`.\n\nReturns a new tensor with the sigmoid of the elements of `input`.\n\nReturns a new tensor with the signs of the elements of `input`.\n\nFor complex tensors, this function returns a new tensor whose elemants have\nthe same angle as that of the elements of `input` and absolute value 1.\n\nTests if each element of `input` has its sign bit set (is less than zero) or\nnot.\n\nReturns a new tensor with the sine of the elements of `input`.\n\nComputes the normalized sinc of `input.`\n\nReturns a new tensor with the hyperbolic sine of the elements of `input`.\n\nReturns a new tensor with the square-root of the elements of `input`.\n\nReturns a new tensor with the square of the elements of `input`.\n\nSubtracts `other`, scaled by `alpha`, from `input`.\n\nAlias for `torch.sub()`.\n\nReturns a new tensor with the tangent of the elements of `input`.\n\nReturns a new tensor with the hyperbolic tangent of the elements of `input`.\n\nAlias for `torch.div()` with `rounding_mode=None`.\n\nReturns a new tensor with the truncated integer values of the elements of\n`input`.\n\nComputes `input * log(other)` with the following cases.\n\nReturns the indices of the maximum value of all elements in the `input`\ntensor.\n\nReturns the indices of the minimum value(s) of the flattened tensor or along a\ndimension\n\nReturns the maximum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nReturns the minimum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nTests if all elements in `input` evaluate to `True`.\n\nthe input tensor.\n\nReturns the maximum value of all elements in the `input` tensor.\n\nReturns the minimum value of all elements in the `input` tensor.\n\nReturns the p-norm of (`input` \\- `other`)\n\nReturns the log of summed exponentials of each row of the `input` tensor in\nthe given dimension `dim`.\n\nReturns the mean value of all elements in the `input` tensor.\n\nReturns the median of the values in `input`.\n\nReturns the median of the values in `input`, ignoring `NaN` values.\n\nReturns a namedtuple `(values, indices)` where `values` is the mode value of\neach row of the `input` tensor in the given dimension `dim`, i.e.\n\nReturns the matrix norm or vector norm of a given tensor.\n\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\nReturns the product of all elements in the `input` tensor.\n\nReturns the q-th quantiles of all elements in the `input` tensor, doing a\nlinear interpolation when the q-th quantile lies between two data points.\n\nThis is a variant of `torch.quantile()` that \u201cignores\u201d `NaN` values, computing\nthe quantiles `q` as if `NaN` values in `input` did not exist.\n\nReturns the standard-deviation of all elements in the `input` tensor.\n\nReturns the standard-deviation and mean of all elements in the `input` tensor.\n\nReturns the sum of all elements in the `input` tensor.\n\nReturns the unique elements of the input tensor.\n\nEliminates all but the first element from every consecutive group of\nequivalent elements.\n\nReturns the variance of all elements in the `input` tensor.\n\nReturns the variance and mean of all elements in the `input` tensor.\n\nCounts the number of non-zero values in the tensor `input` along the given\n`dim`.\n\nThis function checks if all `input` and `other` satisfy the condition:\n\nReturns the indices that sort a tensor along a given dimension in ascending\norder by value.\n\nComputes element-wise equality\n\n`True` if two tensors have the same size and elements, `False` otherwise.\n\nComputes input\u2265other\\text{input} \\geq \\text{other} element-wise.\n\nAlias for `torch.ge()`.\n\nComputes input>other\\text{input} > \\text{other} element-wise.\n\nAlias for `torch.gt()`.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is \u201cclose\u201d to the corresponding element of `other`.\n\nReturns a new tensor with boolean elements representing if each element is\n`finite` or not.\n\nTests if each element of `input` is infinite (positive or negative infinity)\nor not.\n\nTests if each element of `input` is positive infinity or not.\n\nTests if each element of `input` is negative infinity or not.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is NaN or not.\n\nReturns a new tensor with boolean elements representing if each element of\n`input` is real-valued or not.\n\nReturns a namedtuple `(values, indices)` where `values` is the `k` th smallest\nelement of each row of the `input` tensor in the given dimension `dim`.\n\nComputes input\u2264other\\text{input} \\leq \\text{other} element-wise.\n\nAlias for `torch.le()`.\n\nComputes input<other\\text{input} < \\text{other} element-wise.\n\nAlias for `torch.lt()`.\n\nComputes the element-wise maximum of `input` and `other`.\n\nComputes the element-wise minimum of `input` and `other`.\n\nComputes the element-wise maximum of `input` and `other`.\n\nComputes the element-wise minimum of `input` and `other`.\n\nComputes input\u2260other\\text{input} \\neq \\text{other} element-wise.\n\nAlias for `torch.ne()`.\n\nSorts the elements of the `input` tensor along a given dimension in ascending\norder by value.\n\nReturns the `k` largest elements of the given `input` tensor along a given\ndimension.\n\nSorts the elements of the `input` tensor along its first dimension in\nascending order by value.\n\nShort-time Fourier transform (STFT).\n\nInverse short time Fourier Transform.\n\nBartlett window function.\n\nBlackman window function.\n\nHamming window function.\n\nHann window function.\n\nComputes the Kaiser window with window length `window_length` and shape\nparameter `beta`.\n\nReturns a 1-dimensional view of each input tensor with zero dimensions.\n\nReturns a 2-dimensional view of each input tensor with zero dimensions.\n\nReturns a 3-dimensional view of each input tensor with zero dimensions.\n\nCount the frequency of each value in an array of non-negative ints.\n\nCreate a block diagonal matrix from provided tensors.\n\nBroadcasts the given tensors according to Broadcasting semantics.\n\nBroadcasts `input` to the shape `shape`.\n\nSimilar to `broadcast_tensors()` but for shapes.\n\nReturns the indices of the buckets to which each value in the `input` belongs,\nwhere the boundaries of the buckets are set by `boundaries`.\n\nDo cartesian product of the given sequence of tensors.\n\nComputes batched the p-norm distance between each pair of the two collections\nof row vectors.\n\nReturns a copy of `input`.\n\nCompute combinations of length rr of the given tensor.\n\nReturns the cross product of vectors in dimension `dim` of `input` and\n`other`.\n\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nmaximum of elements of `input` in the dimension `dim`.\n\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nminimum of elements of `input` in the dimension `dim`.\n\nReturns the cumulative product of elements of `input` in the dimension `dim`.\n\nReturns the cumulative sum of elements of `input` in the dimension `dim`.\n\nCreates a tensor whose diagonals of certain 2D planes (specified by `dim1` and\n`dim2`) are filled by `input`.\n\nReturns a partial view of `input` with the its diagonal elements with respect\nto `dim1` and `dim2` appended as a dimension at the end of the shape.\n\nComputes the n-th forward difference along the given dimension.\n\nSums the product of the elements of the input `operands` along dimensions\nspecified using a notation based on the Einstein summation convention.\n\nFlattens `input` by reshaping it into a one-dimensional tensor.\n\nReverse the order of a n-D tensor along given axis in dims.\n\nFlip tensor in the left/right direction, returning a new tensor.\n\nFlip tensor in the up/down direction, returning a new tensor.\n\nComputes the Kronecker product, denoted by \u2297\\otimes , of `input` and `other`.\n\nRotate a n-D tensor by 90 degrees in the plane specified by dims axis.\n\nComputes the element-wise greatest common divisor (GCD) of `input` and\n`other`.\n\nComputes the histogram of a tensor.\n\nTake NN tensors, each of which can be either scalar or 1-dimensional vector,\nand create NN N-dimensional grids, where the ii th grid is defined by\nexpanding the ii th input over dimensions defined by other inputs.\n\nComputes the element-wise least common multiple (LCM) of `input` and `other`.\n\nReturns the logarithm of the cumulative summation of the exponentiation of\nelements of `input` in the dimension `dim`.\n\nReturn a contiguous flattened tensor.\n\nReturns a tensor where each sub-tensor of `input` along dimension `dim` is\nnormalized such that the `p`-norm of the sub-tensor is lower than the value\n`maxnorm`\n\nRepeat elements of a tensor.\n\nRoll the tensor along the given dimension(s).\n\nFind the indices from the innermost dimension of `sorted_sequence` such that,\nif the corresponding values in `values` were inserted before the indices, the\norder of the corresponding innermost dimension within `sorted_sequence` would\nbe preserved.\n\nReturns a contraction of a and b over multiple dimensions.\n\nReturns the sum of the elements of the diagonal of the input 2-D matrix.\n\nReturns the lower triangular part of the matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nReturns the indices of the lower triangular part of a `row`-by- `col` matrix\nin a 2-by-N Tensor, where the first row contains row coordinates of all\nindices and the second row contains column coordinates.\n\nReturns the upper triangular part of a matrix (2-D tensor) or batch of\nmatrices `input`, the other elements of the result tensor `out` are set to 0.\n\nReturns the indices of the upper triangular part of a `row` by `col` matrix in\na 2-by-N Tensor, where the first row contains row coordinates of all indices\nand the second row contains column coordinates.\n\nGenerates a Vandermonde matrix.\n\nReturns a view of `input` as a real tensor.\n\nReturns a view of `input` as a complex tensor.\n\nPerforms a batch matrix-matrix product of matrices stored in `batch1` and\n`batch2`, with a reduced add step (all matrix multiplications get accumulated\nalong the first dimension).\n\nPerforms a matrix multiplication of the matrices `mat1` and `mat2`.\n\nPerforms a matrix-vector product of the matrix `mat` and the vector `vec`.\n\nPerforms the outer-product of vectors `vec1` and `vec2` and adds it to the\nmatrix `input`.\n\nPerforms a batch matrix-matrix product of matrices in `batch1` and `batch2`.\n\nPerforms a batch matrix-matrix product of matrices stored in `input` and\n`mat2`.\n\nReturns the matrix product of the NN 2-D tensors.\n\nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA\nor for batches of symmetric positive-definite matrices.\n\nComputes the inverse of a symmetric positive-definite matrix AA using its\nCholesky factor uu : returns matrix `inv`.\n\nSolves a linear system of equations with a positive semidefinite matrix to be\ninverted given its Cholesky factor matrix uu .\n\nComputes the dot product of two 1D tensors.\n\nComputes the eigenvalues and eigenvectors of a real square matrix.\n\nThis is a low-level function for calling LAPACK directly.\n\nAlias of `torch.outer()`.\n\nComputes the dot product for 1D tensors.\n\nTakes the inverse of the square matrix `input`.\n\nCalculates determinant of a square matrix or batches of square matrices.\n\nCalculates log determinant of a square matrix or batches of square matrices.\n\nCalculates the sign and log absolute value of the determinant(s) of a square\nmatrix or batches of square matrices.\n\nComputes the solution to the least squares and least norm problems for a full\nrank matrix AA of size (m\u00d7n)(m \\times n) and a matrix BB of size (m\u00d7k)(m\n\\times k) .\n\nComputes the LU factorization of a matrix or batches of matrices `A`.\n\nReturns the LU solve of the linear system Ax=bAx = b using the partially\npivoted LU factorization of A from `torch.lu()`.\n\nUnpacks the data and pivots from a LU factorization of a tensor.\n\nMatrix product of two tensors.\n\nReturns the matrix raised to the power `n` for square matrices.\n\nReturns the numerical rank of a 2-D tensor.\n\nReturns the matrix exponential.\n\nPerforms a matrix multiplication of the matrices `input` and `mat2`.\n\nPerforms a matrix-vector product of the matrix `input` and the vector `vec`.\n\nComputes the orthogonal matrix `Q` of a QR factorization, from the `(input,\ninput2)` tuple returned by `torch.geqrf()`.\n\nMultiplies `mat` (given by `input3`) by the orthogonal `Q` matrix of the QR\nfactorization formed by `torch.geqrf()` that is represented by `(a, tau)`\n(given by (`input`, `input2`)).\n\nOuter product of `input` and `vec2`.\n\nCalculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a\n2D tensor.\n\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nThis function returns the solution to the system of linear equations\nrepresented by AX=BAX = B and the LU factorization of A, in order as a\nnamedtuple `solution, LU`.\n\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\n\nReturn the singular value decomposition `(U, S, V)` of a matrix, batches of\nmatrices, or a sparse matrix AA such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T\n.\n\nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix,\nbatches of such matrices, or sparse matrix.\n\nThis function returns eigenvalues and eigenvectors of a real symmetric matrix\n`input` or a batch of real symmetric matrices, represented by a namedtuple\n(eigenvalues, eigenvectors).\n\nFind the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized eigenvalue problem\nusing matrix-free LOBPCG methods.\n\nEstimate \u222bydx\\int y\\,dx along `dim`, using the trapezoid rule.\n\nSolves a system of equations with a triangular coefficient matrix AA and\nmultiple right-hand sides bb .\n\nComputes the dot product of two 1D tensors.\n\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\nReturns the `torch.dtype` that would result from performing an arithmetic\noperation on the provided input tensors.\n\nDetermines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.\n\nReturns the `torch.dtype` with the smallest size and scalar kind that is not\nsmaller nor of lower kind than either `type1` or `type2`.\n\nSets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.\n\nReturns True if the global deterministic flag is turned on.\n\nA wrapper around Python\u2019s assert which is symbolically traceable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": "\nComputes the absolute value of each element in `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": "\nAlias for `torch.abs()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": "\nComputes the inverse cosine of each element in `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic cosine of the elements of\n`input`.\n\nNote\n\nThe domain of the inverse hyperbolic cosine is `[1, inf)` and values outside\nthis range will be mapped to `NaN`, except for `+ INF` for which the output is\nmapped to `+ INF`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": "\nAdds the scalar `other` to each element of the input `input` and returns a new\nresulting tensor.\n\nIf `input` is of type FloatTensor or DoubleTensor, `other` must be a real\nnumber, otherwise it should be an integer.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nEach element of the tensor `other` is multiplied by the scalar `alpha` and\nadded to each element of the tensor `input`. The resulting tensor is returned.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nIf `other` is of type FloatTensor or DoubleTensor, `alpha` must be a real\nnumber, otherwise it should be an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `batch1` and\n`batch2`, with a reduced add step (all matrix multiplications get accumulated\nalong the first dimension). `input` is added to the final result.\n\n`batch1` and `batch2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `batch1` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `batch2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, `input` must be broadcastable with a (n\u00d7p)(n \\times\np) tensor and `out` will be a (n\u00d7p)(n \\times p) tensor.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": "\nPerforms the element-wise division of `tensor1` by `tensor2`, multiply the\nresult by the scalar `value` and add it to `input`.\n\nWarning\n\nInteger division with addcdiv is no longer supported, and in a future release\naddcdiv will perform a true division of tensor1 and tensor2. The historic\naddcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 /\ntensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 /\ntensor2) for float inputs. The future addcdiv behavior is just the latter\nimplementation: (input + value * tensor1 / tensor2), for all dtypes.\n\nThe shapes of `input`, `tensor1`, and `tensor2` must be broadcastable.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, `value` must be a real\nnumber, otherwise an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": "\nPerforms the element-wise multiplication of `tensor1` by `tensor2`, multiply\nthe result by the scalar `value` and add it to `input`.\n\nThe shapes of `tensor`, `tensor1`, and `tensor2` must be broadcastable.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, `value` must be a real\nnumber, otherwise an integer.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `mat1` and `mat2`. The matrix\n`input` is added to the final result.\n\nIf `mat1` is a (n\u00d7m)(n \\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p) tensor,\nthen `input` must be broadcastable with a (n\u00d7p)(n \\times p) tensor and `out`\nwill be a (n\u00d7p)(n \\times p) tensor.\n\n`alpha` and `beta` are scaling factors on matrix-vector product between `mat1`\nand `mat2` and the added matrix `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `mat` and the vector `vec`. The\nvector `input` is added to the final result.\n\nIf `mat` is a (n\u00d7m)(n \\times m) tensor, `vec` is a 1-D tensor of size `m`,\nthen `input` must be broadcastable with a 1-D tensor of size `n` and `out`\nwill be 1-D tensor of size `n`.\n\n`alpha` and `beta` are scaling factors on matrix-vector product between `mat`\nand `vec` and the added tensor `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": "\nPerforms the outer-product of vectors `vec1` and `vec2` and adds it to the\nmatrix `input`.\n\nOptional values `beta` and `alpha` are scaling factors on the outer product\nbetween `vec1` and `vec2` and the added matrix `input` respectively.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nIf `vec1` is a vector of size `n` and `vec2` is a vector of size `m`, then\n`input` must be broadcastable with a matrix of size (n\u00d7m)(n \\times m) and\n`out` will be a matrix of size (n\u00d7m)(n \\times m) .\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": "\nTests if all elements in `input` evaluate to `True`.\n\nNote\n\nThis function matches the behaviour of NumPy in returning output of dtype\n`bool` for all supported dtypes except `uint8`. For `uint8` the dtype of\noutput is `uint8` itself.\n\nExample:\n\nFor each row of `input` in the given dimension `dim`, returns `True` if all\nelements in the row evaluate to `True` and `False` otherwise.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1\nfewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": "\nThis function checks if all `input` and `other` satisfy the condition:\n\nelementwise, for all elements of `input` and `other`. The behaviour of this\nfunction is analogous to numpy.allclose\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": "\nReturns the maximum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nNote\n\nIf `keepdim is ``True``, the output tensors are of the same size as `input`\nexcept in the dimension(s) `dim` where they are of size 1. Otherwise, `dim`s\nare squeezed (see :func:`torch.squeeze`), resulting in the output tensors\nhaving fewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": "\nReturns the minimum value of each slice of the `input` tensor in the given\ndimension(s) `dim`.\n\nNote\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension(s) `dim` where they are of size 1. Otherwise, `dim`s\nare squeezed (see :func:`torch.squeeze`), resulting in the output tensors\nhaving fewer dimensions than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": "\nComputes the element-wise angle (in radians) of the given `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nNote\n\nStarting in PyTorch 1.8, angle returns pi for negative real numbers, zero for\nnon-negative real numbers, and propagates NaNs. Previously the function would\nreturn zero for all real numbers and not propagate floating-point NaNs.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": "\ninput (Tensor) \u2013 the input tensor.\n\nTests if any element in `input` evaluates to `True`.\n\nNote\n\nThis function matches the behaviour of NumPy in returning output of dtype\n`bool` for all supported dtypes except `uint8`. For `uint8` the dtype of\noutput is `uint8` itself.\n\nExample:\n\nFor each row of `input` in the given dimension `dim`, returns `True` if any\nelement in the row evaluate to `True` and `False` otherwise.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1\nfewer dimension than `input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": "\nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} -\n\\text{start}}{\\text{step}} \\right\\rceil with values from the interval `[start,\nend)` taken with common difference `step` beginning from `start`.\n\nNote that non-integer `step` is subject to floating point rounding errors when\ncomparing against `end`; to avoid inconsistency, we advise adding a small\nepsilon to `end` in such cases.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": "\nAlias for `torch.acos()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": "\nAlias for `torch.acosh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": "\nAlias for `torch.asin()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": "\nAlias for `torch.asinh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": "\nAlias for `torch.atan()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": "\nAlias for `torch.atanh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": "\nReturns True if the global deterministic flag is turned on. Refer to\n`torch.use_deterministic_algorithms()` documentation for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": "\nReturns the indices of the maximum value of all elements in the `input`\ntensor.\n\nThis is the second value returned by `torch.max()`. See its documentation for\nthe exact semantics of this method.\n\nNote\n\nIf there are multiple minimal values then the indices of the first minimal\nvalue are returned.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns the indices of the maximum values of a tensor across a dimension.\n\nThis is the second value returned by `torch.max()`. See its documentation for\nthe exact semantics of this method.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": "\nReturns the indices of the minimum value(s) of the flattened tensor or along a\ndimension\n\nThis is the second value returned by `torch.min()`. See its documentation for\nthe exact semantics of this method.\n\nNote\n\nIf there are multiple minimal values then the indices of the first minimal\nvalue are returned.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": "\nReturns the indices that sort a tensor along a given dimension in ascending\norder by value.\n\nThis is the second value returned by `torch.sort()`. See its documentation for\nthe exact semantics of this method.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": "\nReturns a new tensor with the arcsine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic sine of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": "\nCreate a view of an existing `torch.Tensor` `input` with specified `size`,\n`stride` and `storage_offset`.\n\nWarning\n\nMore than one element of a created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nMany PyTorch functions, which return a view of a tensor, are internally\nimplemented with this function. Those functions, like `torch.Tensor.expand()`,\nare easier to read and are therefore more advisable to use.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": "\nConvert the data into a `torch.Tensor`. If the data is already a `Tensor` with\nthe same `dtype` and `device`, no copy will be performed, otherwise a new\n`Tensor` will be returned with computational graph retained if data `Tensor`\nhas `requires_grad=True`. Similarly, if the data is an `ndarray` of the\ncorresponding `dtype` and the `device` is the cpu, no copy will be performed.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": "\nReturns a new tensor with the arctangent of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": "\nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}\nwith consideration of the quadrant. Returns a new tensor with the signed\nangles in radians between vector (otheri,inputi)(\\text{other}_{i},\n\\text{input}_{i}) and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} ,\nthe second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the\nfirst parameter, is the y-coordinate.)\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": "\nReturns a new tensor with the inverse hyperbolic tangent of the elements of\n`input`.\n\nNote\n\nThe domain of the inverse hyperbolic tangent is `(-1, 1)` and values outside\nthis range will be mapped to `NaN`, except for the values `1` and `-1` for\nwhich the output is mapped to `+/-INF` respectively.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": "\nReturns a 1-dimensional view of each input tensor with zero dimensions. Input\ntensors with one or more dimensions are returned as-is.\n\ninput (Tensor or list of Tensors) \u2013\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": "\nReturns a 2-dimensional view of each input tensor with zero dimensions. Input\ntensors with two or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": "\nReturns a 3-dimensional view of each input tensor with zero dimensions. Input\ntensors with three or more dimensions are returned as-is. :param input: :type\ninput: Tensor or list of Tensors\n\noutput (Tensor or tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": "\n`torch.autograd` provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare `Tensor` s for which\ngradients should be computed with the `requires_grad=True` keyword. As of now,\nwe only support autograd for floating point `Tensor` types ( half, float,\ndouble and bfloat16) and complex `Tensor` types (cfloat, cdouble).\n\nComputes the sum of gradients of given tensors w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If any of `tensors` are non-\nscalar (i.e. their data has more than one element) and require gradient, then\nthe Jacobian-vector product would be computed, in this case the function\nadditionally requires specifying `grad_tensors`. It should be a sequence of\nmatching length, that contains the \u201cvector\u201d in the Jacobian-vector product,\nusually the gradient of the differentiated function w.r.t. corresponding\ntensors (`None` is an acceptable value for all tensors that don\u2019t need\ngradient tensors).\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nUsing this method with `create_graph=True` will create a reference cycle\nbetween the parameter and its gradient which can cause a memory leak. We\nrecommend using `autograd.grad` when creating the graph to avoid this. If you\nhave to use this function, make sure to reset the `.grad` fields of your\nparameters to `None` after use to break the cycle and avoid the leak.\n\nNote\n\nIf you run any forward ops, create `grad_tensors`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n\n`grad_outputs` should be a sequence of length matching `output` containing the\n\u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t.\neach of the outputs. If an output doesn\u2019t require_grad, then the gradient can\nbe `None`).\n\nIf `only_inputs` is `True`, the function will only return a list of gradients\nw.r.t the specified inputs. If it\u2019s `False`, then gradient w.r.t. all\nremaining leaves will still be computed, and will be accumulated into their\n`.grad` attribute.\n\nNote\n\nIf you run any forward ops, create `grad_outputs`, and/or call `grad` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nWarning\n\nThis API is in beta. Even though the function signatures are very unlikely to\nchange, major improvements to performances are planned before we consider this\nstable.\n\nThis section contains the higher level API for the autograd that builds on the\nbasic API above and allows you to compute jacobians, hessians, etc.\n\nThis API works with user-provided functions that take only Tensors as input\nand return only Tensors. If your function takes other arguments that are not\nTensors or Tensors that don\u2019t have requires_grad set, you can use a lambda to\ncapture them. For example, for a function `f` that takes three inputs, a\nTensor for which we want the jacobian, another tensor that should be\nconsidered constant and a boolean flag as `f(input, constant, flag=flag)` you\ncan use it as `functional.jacobian(lambda x: f(x, constant, flag=flag),\ninput)`.\n\nFunction that computes the Jacobian of a given function.\n\nif there is a single input and output, this will be a single Tensor containing\nthe Jacobian for the linearized inputs and output. If one of the two is a\ntuple, then the Jacobian will be a tuple of Tensors. If both of them are\ntuples, then the Jacobian will be a tuple of tuple of Tensors where\n`Jacobian[i][j]` will contain the Jacobian of the `i`th output and `j`th input\nand will have as size the concatenation of the sizes of the corresponding\noutput and the corresponding input and will have same dtype and device as the\ncorresponding input.\n\nJacobian (Tensor or nested tuple of Tensors)\n\nFunction that computes the Hessian of a given scalar function.\n\nif there is a single input, this will be a single Tensor containing the\nHessian for the input. If it is a tuple, then the Hessian will be a tuple of\ntuples where `Hessian[i][j]` will contain the Hessian of the `i`th input and\n`j`th input with size the sum of the size of the `i`th input plus the size of\nthe `j`th input. `Hessian[i][j]` will have the same dtype and device as the\ncorresponding `i`th input.\n\nHessian (Tensor or a tuple of tuple of Tensors)\n\nFunction that computes the dot product between a vector `v` and the Jacobian\nof the given function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvjp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nFunction that computes the dot product between the Jacobian of the given\nfunction at the point given by the inputs and a vector `v`.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\njvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the output.\n\noutput (tuple)\n\nNote\n\nThe jvp is currently computed by using the backward of the backward (sometimes\ncalled the double backwards trick) as we don\u2019t have support for forward mode\nAD in PyTorch at the moment.\n\nFunction that computes the dot product between a vector `v` and the Hessian of\na given scalar function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvhp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nFunction that computes the dot product between the Hessian of a given scalar\nfunction and a vector `v` at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nhvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nNote\n\nThis function is significantly slower than `vhp` due to backward mode AD\nconstraints. If your functions is twice continuously differentiable, then hvp\n= vhp.t(). So if you know that your function satisfies this condition, you\nshould use vhp instead that is much faster with the current implementation.\n\nContext-manager that disabled gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call `Tensor.backward()`. It will reduce memory consumption for\ncomputations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have `requires_grad=False`,\neven when the inputs have `requires_grad=True`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\nContext-manager that sets gradient calculation to on or off.\n\n`set_grad_enabled` will enable or disable grads based on its argument `mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nmode (bool) \u2013 Flag whether to enable grad (`True`), or disable (`False`). This\ncan be used to conditionally enable gradients.\n\nExample:\n\nWhen a non-sparse `param` receives a non-sparse gradient during\n`torch.autograd.backward()` or `torch.Tensor.backward()` `param.grad` is\naccumulated as follows.\n\nIf `param.grad` is initially `None`:\n\nIf `param` already has a non-sparse `.grad` attribute:\n\nThe default behavior (letting `.grad`s be `None` before the first\n`backward()`, such that their layout is created according to 1 or 2, and\nretained over time according to 3 or 4) is recommended for best performance.\nCalls to `model.zero_grad()` or `optimizer.zero_grad()` will not affect\n`.grad` layouts.\n\nIn fact, resetting all `.grad`s to `None` before each accumulation phase,\ne.g.:\n\nsuch that they\u2019re recreated according to 1 or 2 every time, is a valid\nalternative to `model.zero_grad()` or `optimizer.zero_grad()` that may improve\nperformance for some networks.\n\nIf you need manual control over `.grad`\u2019s strides, assign `param.grad =` a\nzeroed tensor with desired strides before the first `backward()`, and never\nreset it to `None`. 3 guarantees your layout is preserved as long as\n`create_graph=False`. 4 indicates your layout is likely preserved even if\n`create_graph=True`.\n\nSupporting in-place operations in autograd is a hard matter, and we discourage\ntheir use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes\nit very efficient and there are very few occasions when in-place operations\nactually lower memory usage by any significant amount. Unless you\u2019re operating\nunder heavy memory pressure, you might never need to use them.\n\nAll `Tensor` s keep track of in-place operations applied to them, and if the\nimplementation detects that a tensor was saved for backward in one of the\nfunctions, but it was modified in-place afterwards, an error will be raised\nonce backward pass is started. This ensures that if you\u2019re using in-place\nfunctions and not seeing any errors, you can be sure that the computed\ngradients are correct.\n\nWarning\n\nThe Variable API has been deprecated: Variables are no longer necessary to use\nautograd with tensors. Autograd automatically supports Tensors with\n`requires_grad` set to `True`. Below please find a quick guide on what has\nchanged:\n\nIn addition, one can now create tensors with `requires_grad=True` using\nfactory methods such as `torch.randn()`, `torch.zeros()`, `torch.ones()`, and\nothers like the following:\n\n`autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)`\n\nThis attribute is `None` by default and becomes a Tensor the first time a call\nto `backward()` computes gradients for `self`. The attribute will then contain\nthe gradients computed and future calls to `backward()` will accumulate (add)\ngradients into it.\n\nIs `True` if gradients need to be computed for this Tensor, `False` otherwise.\n\nNote\n\nThe fact that gradients need to be computed for a Tensor do not mean that the\n`grad` attribute will be populated, see `is_leaf` for more details.\n\nAll Tensors that have `requires_grad` which is `False` will be leaf Tensors by\nconvention.\n\nFor Tensors that have `requires_grad` which is `True`, they will be leaf\nTensors if they were created by the user. This means that they are not the\nresult of an operation and so `grad_fn` is None.\n\nOnly leaf Tensors will have their `grad` populated during a call to\n`backward()`. To get `grad` populated for non-leaf Tensors, you can use\n`retain_grad()`.\n\nExample:\n\nComputes the gradient of current tensor w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If the tensor is non-scalar\n(i.e. its data has more than one element) and requires gradient, the function\nadditionally requires specifying `gradient`. It should be a tensor of matching\ntype and location, that contains the gradient of the differentiated function\nw.r.t. `self`.\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nIf you run any forward ops, create `gradient`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\nReturns a new Tensor, detached from the current graph.\n\nThe result will never require gradient.\n\nNote\n\nReturned Tensor shares the same storage with the original one. In-place\nmodifications on either of them will be seen, and may trigger errors in\ncorrectness checks. IMPORTANT NOTE: Previously, in-place size / stride /\nstorage changes (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to\nthe returned tensor also update the original tensor. Now, these in-place\nchanges will not update the original tensor anymore, and will instead trigger\nan error. For sparse tensors: In-place indices / values changes (such as\n`zero_` / `copy_` / `add_`) to the returned tensor will not update the\noriginal tensor anymore, and will instead trigger an error.\n\nDetaches the Tensor from the graph that created it, making it a leaf. Views\ncannot be detached in-place.\n\nRegisters a backward hook.\n\nThe hook will be called every time a gradient with respect to the Tensor is\ncomputed. The hook should have the following signature:\n\nThe hook should not modify its argument, but it can optionally return a new\ngradient which will be used in place of `grad`.\n\nThis function returns a handle with a method `handle.remove()` that removes\nthe hook from the module.\n\nExample:\n\nEnables .grad attribute for non-leaf Tensors.\n\nRecords operation history and defines formulas for differentiating ops.\n\nSee the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-\ntorch-autograd\n\nEvery operation performed on `Tensor` s creates a new function object, that\nperforms the computation, and records that it happened. The history is\nretained in the form of a DAG of functions, with edges denoting data\ndependencies (`input <- output`). Then, when backward is called, the graph is\nprocessed in the topological ordering, by calling `backward()` methods of each\n`Function` object, and passing returned gradients on to next `Function` s.\n\nNormally, the only way users interact with functions is by creating subclasses\nand defining new operations. This is a recommended way of extending\ntorch.autograd.\n\nExamples:\n\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\nWhen creating a new `Function`, the following methods are available to `ctx`.\n\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\nCheck gradients computed via small finite differences against analytical\ngradients w.r.t. tensors in `inputs` that are of floating point or complex\ntype and with `requires_grad=True`.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nFor complex functions, no notion of Jacobian exists. Gradcheck verifies if the\nnumerical and analytical values of Wirtinger and Conjugate Wirtinger\nderivative are consistent. The gradient computation is done under the\nassumption that the overall function has a real valued output. For functions\nwith complex output, gradcheck compares the numerical and analytical gradients\nfor two values of `grad_output`: 1 and 1j. For more details, check out\nAutograd for Complex Numbers.\n\nNote\n\nThe default values are designed for `input` of double precision. This check\nwill likely fail if `input` is of less precision, e.g., `FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` has overlapping memory, i.e., different\nindices pointing to the same memory address (e.g., from `torch.expand()`),\nthis check will likely fail because the numerical gradients computed by point\nperturbation at such indices will change values at all other indices that\nshare the same memory address.\n\nTrue if all differences satisfy allclose condition\n\nCheck gradients of gradients computed via small finite differences against\nanalytical gradients w.r.t. tensors in `inputs` and `grad_outputs` that are of\nfloating point or complex type and with `requires_grad=True`.\n\nThis function checks that backpropagating through the gradients computed to\nthe given `grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nNote\n\nThe default values are designed for `input` and `grad_outputs` of double\nprecision. This check will likely fail if they are of less precision, e.g.,\n`FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` and `grad_outputs` has overlapping memory,\ni.e., different indices pointing to the same memory address (e.g., from\n`torch.expand()`), this check will likely fail because the numerical gradients\ncomputed by point perturbation at such indices will change values at all other\nindices that share the same memory address.\n\nTrue if all differences satisfy allclose condition\n\nAutograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using `profile`. and nvprof based\n(registers both CPU and GPU activity) using `emit_nvtx`.\n\nContext manager that manages autograd profiler state and holds a summary of\nresults. Under the hood it just records events of functions being executed in\nC++ and exposes those events to Python. You can wrap any code into it and it\nwill only report runtime of PyTorch functions. Note: profiler is thread local\nand is automatically propagated into the async tasks\n\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\nAverages all events.\n\nA FunctionEventAvg object.\n\nContext manager that makes every autograd operation emit an NVTX range.\n\nIt is useful when running the program under nvprof:\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them. Then,\neither NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\n`torch.autograd.profiler.load_nvprof()` can load the results for inspection\ne.g. in Python REPL.\n\nForward-backward correlation\n\nWhen viewing a profile created using `emit_nvtx` in the Nvidia Visual\nProfiler, correlating each backward-pass op with the corresponding forward-\npass op can be difficult. To ease this task, `emit_nvtx` appends sequence\nnumber information to the ranges it generates.\n\nDuring the forward pass, each function range is decorated with `seq=<N>`.\n`seq` is a running counter, incremented each time a new backward Function\nobject is created and stashed for backward. Thus, the `seq=<N>` annotation\nassociated with each forward function range tells you that if a backward\nFunction object is created by this forward function, the backward object will\nreceive sequence number N. During the backward pass, the top-level range\nwrapping each C++ backward Function\u2019s `apply()` call is decorated with\n`stashed seq=<M>`. `M` is the sequence number that the backward object was\ncreated with. By comparing `stashed seq` numbers in backward with `seq`\nnumbers in forward, you can track down which forward op created each backward\nFunction.\n\nAny functions executed during the backward pass are also decorated with\n`seq=<N>`. During default backward (with `create_graph=False`) this\ninformation is irrelevant, and in fact, `N` may simply be 0 for all such\nfunctions. Only the top-level ranges associated with backward Function\nobjects\u2019 `apply()` methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\n\nDouble-backward\n\nIf, on the other hand, a backward pass with `create_graph=True` is underway\n(in other words, if you are setting up for a double-backward), each function\u2019s\nexecution during backward is given a nonzero, useful `seq=<N>`. Those\nfunctions may themselves create Function objects to be executed later during\ndouble-backward, just as the original functions in the forward pass did. The\nrelationship between backward and double-backward is conceptually the same as\nthe relationship between forward and backward: The functions still emit\ncurrent-sequence-number-tagged ranges, the Function objects they create still\nstash those sequence numbers, and during the eventual double-backward, the\nFunction objects\u2019 `apply()` ranges are still tagged with `stashed seq`\nnumbers, which can be compared to `seq` numbers from the backward pass.\n\nOpens an nvprof trace file and parses autograd annotations.\n\npath (str) \u2013 path to nvprof trace\n\nContext-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\nWarning\n\nThis mode should be enabled only for debugging as the different tests will\nslow down your program execution.\n\nContext-manager that sets the anomaly detection for the autograd engine on or\noff.\n\n`set_detect_anomaly` will enable or disable the autograd anomaly detection\nbased on its argument `mode`. It can be used as a context-manager or as a\nfunction.\n\nSee `detect_anomaly` above for details of the anomaly detection behaviour.\n\nmode (bool) \u2013 Flag whether to enable anomaly detection (`True`), or disable\n(`False`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": "\nComputes the sum of gradients of given tensors w.r.t. graph leaves.\n\nThe graph is differentiated using the chain rule. If any of `tensors` are non-\nscalar (i.e. their data has more than one element) and require gradient, then\nthe Jacobian-vector product would be computed, in this case the function\nadditionally requires specifying `grad_tensors`. It should be a sequence of\nmatching length, that contains the \u201cvector\u201d in the Jacobian-vector product,\nusually the gradient of the differentiated function w.r.t. corresponding\ntensors (`None` is an acceptable value for all tensors that don\u2019t need\ngradient tensors).\n\nThis function accumulates gradients in the leaves - you might need to zero\n`.grad` attributes or set them to `None` before calling it. See Default\ngradient layouts for details on the memory layout of accumulated gradients.\n\nNote\n\nUsing this method with `create_graph=True` will create a reference cycle\nbetween the parameter and its gradient which can cause a memory leak. We\nrecommend using `autograd.grad` when creating the graph to avoid this. If you\nhave to use this function, make sure to reset the `.grad` fields of your\nparameters to `None` after use to break the cycle and avoid the leak.\n\nNote\n\nIf you run any forward ops, create `grad_tensors`, and/or call `backward` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that enable anomaly detection for the autograd engine.\n\nThis does two things:\n\nWarning\n\nThis mode should be enabled only for debugging as the different tests will\nslow down your program execution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": "\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": "\nRecords operation history and defines formulas for differentiating ops.\n\nSee the Note on extending the autograd engine for more details on how to use\nthis class: https://pytorch.org/docs/stable/notes/extending.html#extending-\ntorch-autograd\n\nEvery operation performed on `Tensor` s creates a new function object, that\nperforms the computation, and records that it happened. The history is\nretained in the form of a DAG of functions, with edges denoting data\ndependencies (`input <- output`). Then, when backward is called, the graph is\nprocessed in the topological ordering, by calling `backward()` methods of each\n`Function` object, and passing returned gradients on to next `Function` s.\n\nNormally, the only way users interact with functions is by creating subclasses\nand defining new operations. This is a recommended way of extending\ntorch.autograd.\n\nExamples:\n\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": "\nDefines a formula for differentiating the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context `ctx` as the first argument, followed by as many\noutputs did `forward()` return, and it should return as many tensors, as there\nwere inputs to `forward()`. Each argument is the gradient w.r.t the given\noutput, and each returned value should be the gradient w.r.t. the\ncorresponding input.\n\nThe context can be used to retrieve tensors saved during the forward pass. It\nalso has an attribute `ctx.needs_input_grad` as a tuple of booleans\nrepresenting whether each input needs gradient. E.g., `backward()` will have\n`ctx.needs_input_grad[0] = True` if the first input to `forward()` needs\ngradient computated w.r.t. the output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": "\nPerforms the operation.\n\nThis function is to be overridden by all subclasses.\n\nIt must accept a context ctx as the first argument, followed by any number of\narguments (tensors or other types).\n\nThe context can be used to store tensors that can be then retrieved during the\nbackward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": "\nMarks given tensors as modified in an in-place operation.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be inputs.\n\nEvery tensor that\u2019s been modified in-place in a call to `forward()` should be\ngiven to this function, to ensure correctness of our checks. It doesn\u2019t matter\nwhether the function is called before or after modification.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": "\nMarks outputs as non-differentiable.\n\nThis should be called at most once, only from inside the `forward()` method,\nand all arguments should be outputs.\n\nThis will mark outputs as not requiring gradients, increasing the efficiency\nof backward computation. You still need to accept a gradient for each output\nin `backward()`, but it\u2019s always going to be a zero tensor with the same shape\nas the shape of a corresponding output.\n\nThis is used e.g. for indices returned from a max `Function`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": "\nSaves given tensors for a future call to `backward()`.\n\nThis should be called at most once, and only from inside the `forward()`\nmethod.\n\nLater, saved tensors can be accessed through the `saved_tensors` attribute.\nBefore returning them to the user, a check is made to ensure they weren\u2019t used\nin any in-place operation that modified their content.\n\nArguments can also be `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": "\nSets whether to materialize output grad tensors. Default is true.\n\nThis should be called only from inside the `forward()` method\n\nIf true, undefined output grad tensors will be expanded to tensors full of\nzeros prior to calling the `backward()` method.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": "\nFunction that computes the Hessian of a given scalar function.\n\nif there is a single input, this will be a single Tensor containing the\nHessian for the input. If it is a tuple, then the Hessian will be a tuple of\ntuples where `Hessian[i][j]` will contain the Hessian of the `i`th input and\n`j`th input with size the sum of the size of the `i`th input plus the size of\nthe `j`th input. `Hessian[i][j]` will have the same dtype and device as the\ncorresponding `i`th input.\n\nHessian (Tensor or a tuple of tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Hessian of a given scalar\nfunction and a vector `v` at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nhvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\nNote\n\nThis function is significantly slower than `vhp` due to backward mode AD\nconstraints. If your functions is twice continuously differentiable, then hvp\n= vhp.t(). So if you know that your function satisfies this condition, you\nshould use vhp instead that is much faster with the current implementation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": "\nFunction that computes the Jacobian of a given function.\n\nif there is a single input and output, this will be a single Tensor containing\nthe Jacobian for the linearized inputs and output. If one of the two is a\ntuple, then the Jacobian will be a tuple of Tensors. If both of them are\ntuples, then the Jacobian will be a tuple of tuple of Tensors where\n`Jacobian[i][j]` will contain the Jacobian of the `i`th output and `j`th input\nand will have as size the concatenation of the sizes of the corresponding\noutput and the corresponding input and will have same dtype and device as the\ncorresponding input.\n\nJacobian (Tensor or nested tuple of Tensors)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between the Jacobian of the given\nfunction at the point given by the inputs and a vector `v`.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\njvp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the output.\n\noutput (tuple)\n\nNote\n\nThe jvp is currently computed by using the backward of the backward (sometimes\ncalled the double backwards trick) as we don\u2019t have support for forward mode\nAD in PyTorch at the moment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Hessian of\na given scalar function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvhp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": "\nFunction that computes the dot product between a vector `v` and the Jacobian\nof the given function at the point given by the inputs.\n\nfunc_output (tuple of Tensors or Tensor): output of `func(inputs)`\n\nvjp (tuple of Tensors or Tensor): result of the dot product with the same\nshape as the inputs.\n\noutput (tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": "\nComputes and returns the sum of gradients of outputs w.r.t. the inputs.\n\n`grad_outputs` should be a sequence of length matching `output` containing the\n\u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t.\neach of the outputs. If an output doesn\u2019t require_grad, then the gradient can\nbe `None`).\n\nIf `only_inputs` is `True`, the function will only return a list of gradients\nw.r.t the specified inputs. If it\u2019s `False`, then gradient w.r.t. all\nremaining leaves will still be computed, and will be accumulated into their\n`.grad` attribute.\n\nNote\n\nIf you run any forward ops, create `grad_outputs`, and/or call `grad` in a\nuser-specified CUDA stream context, see Stream semantics of backward passes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": "\nCheck gradients computed via small finite differences against analytical\ngradients w.r.t. tensors in `inputs` that are of floating point or complex\ntype and with `requires_grad=True`.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nFor complex functions, no notion of Jacobian exists. Gradcheck verifies if the\nnumerical and analytical values of Wirtinger and Conjugate Wirtinger\nderivative are consistent. The gradient computation is done under the\nassumption that the overall function has a real valued output. For functions\nwith complex output, gradcheck compares the numerical and analytical gradients\nfor two values of `grad_output`: 1 and 1j. For more details, check out\nAutograd for Complex Numbers.\n\nNote\n\nThe default values are designed for `input` of double precision. This check\nwill likely fail if `input` is of less precision, e.g., `FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` has overlapping memory, i.e., different\nindices pointing to the same memory address (e.g., from `torch.expand()`),\nthis check will likely fail because the numerical gradients computed by point\nperturbation at such indices will change values at all other indices that\nshare the same memory address.\n\nTrue if all differences satisfy allclose condition\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": "\nCheck gradients of gradients computed via small finite differences against\nanalytical gradients w.r.t. tensors in `inputs` and `grad_outputs` that are of\nfloating point or complex type and with `requires_grad=True`.\n\nThis function checks that backpropagating through the gradients computed to\nthe given `grad_outputs` are correct.\n\nThe check between numerical and analytical gradients uses `allclose()`.\n\nNote\n\nThe default values are designed for `input` and `grad_outputs` of double\nprecision. This check will likely fail if they are of less precision, e.g.,\n`FloatTensor`.\n\nWarning\n\nIf any checked tensor in `input` and `grad_outputs` has overlapping memory,\ni.e., different indices pointing to the same memory address (e.g., from\n`torch.expand()`), this check will likely fail because the numerical gradients\ncomputed by point perturbation at such indices will change values at all other\nindices that share the same memory address.\n\nTrue if all differences satisfy allclose condition\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": "\nContext-manager that disabled gradient calculation.\n\nDisabling gradient calculation is useful for inference, when you are sure that\nyou will not call `Tensor.backward()`. It will reduce memory consumption for\ncomputations that would otherwise have `requires_grad=True`.\n\nIn this mode, the result of every computation will have `requires_grad=False`,\neven when the inputs have `requires_grad=True`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": "\nContext manager that makes every autograd operation emit an NVTX range.\n\nIt is useful when running the program under nvprof:\n\nUnfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them. Then,\neither NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\n`torch.autograd.profiler.load_nvprof()` can load the results for inspection\ne.g. in Python REPL.\n\nForward-backward correlation\n\nWhen viewing a profile created using `emit_nvtx` in the Nvidia Visual\nProfiler, correlating each backward-pass op with the corresponding forward-\npass op can be difficult. To ease this task, `emit_nvtx` appends sequence\nnumber information to the ranges it generates.\n\nDuring the forward pass, each function range is decorated with `seq=<N>`.\n`seq` is a running counter, incremented each time a new backward Function\nobject is created and stashed for backward. Thus, the `seq=<N>` annotation\nassociated with each forward function range tells you that if a backward\nFunction object is created by this forward function, the backward object will\nreceive sequence number N. During the backward pass, the top-level range\nwrapping each C++ backward Function\u2019s `apply()` call is decorated with\n`stashed seq=<M>`. `M` is the sequence number that the backward object was\ncreated with. By comparing `stashed seq` numbers in backward with `seq`\nnumbers in forward, you can track down which forward op created each backward\nFunction.\n\nAny functions executed during the backward pass are also decorated with\n`seq=<N>`. During default backward (with `create_graph=False`) this\ninformation is irrelevant, and in fact, `N` may simply be 0 for all such\nfunctions. Only the top-level ranges associated with backward Function\nobjects\u2019 `apply()` methods are useful, as a way to correlate these Function\nobjects with the earlier forward pass.\n\nDouble-backward\n\nIf, on the other hand, a backward pass with `create_graph=True` is underway\n(in other words, if you are setting up for a double-backward), each function\u2019s\nexecution during backward is given a nonzero, useful `seq=<N>`. Those\nfunctions may themselves create Function objects to be executed later during\ndouble-backward, just as the original functions in the forward pass did. The\nrelationship between backward and double-backward is conceptually the same as\nthe relationship between forward and backward: The functions still emit\ncurrent-sequence-number-tagged ranges, the Function objects they create still\nstash those sequence numbers, and during the eventual double-backward, the\nFunction objects\u2019 `apply()` ranges are still tagged with `stashed seq`\nnumbers, which can be compared to `seq` numbers from the backward pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": "\nOpens an nvprof trace file and parses autograd annotations.\n\npath (str) \u2013 path to nvprof trace\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": "\nContext manager that manages autograd profiler state and holds a summary of\nresults. Under the hood it just records events of functions being executed in\nC++ and exposes those events to Python. You can wrap any code into it and it\nwill only report runtime of PyTorch functions. Note: profiler is thread local\nand is automatically propagated into the async tasks\n\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\nAverages all events.\n\nA FunctionEventAvg object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": "\nExports an EventList as a Chrome tracing tools file.\n\nThe checkpoint can be later loaded and inspected under `chrome://tracing` URL.\n\npath (str) \u2013 Path where the trace will be written.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": "\nAverages all function events over their keys.\n\nAn EventList containing FunctionEventAvg objects.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": "\nReturns total time spent on CPU obtained as a sum of all self times across all\nthe events.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": "\nPrints an EventList as a nicely formatted table.\n\nA string containing the table.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": "\nAverages all events.\n\nA FunctionEventAvg object.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": "\nContext-manager that sets the anomaly detection for the autograd engine on or\noff.\n\n`set_detect_anomaly` will enable or disable the autograd anomaly detection\nbased on its argument `mode`. It can be used as a context-manager or as a\nfunction.\n\nSee `detect_anomaly` above for details of the anomaly detection behaviour.\n\nmode (bool) \u2013 Flag whether to enable anomaly detection (`True`), or disable\n(`False`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": "\nContext-manager that sets gradient calculation to on or off.\n\n`set_grad_enabled` will enable or disable grads based on its argument `mode`.\nIt can be used as a context-manager or as a function.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nmode (bool) \u2013 Flag whether to enable grad (`True`), or disable (`False`). This\ncan be used to conditionally enable gradients.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": "\n`torch.backends` controls the behavior of various backends that PyTorch\nsupports.\n\nThese backends include:\n\nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t\nnecessarily mean CUDA is available; just that if this PyTorch binary were run\na machine with working CUDA drivers and devices, we would be able to use it.\n\nA `bool` that controls whether TensorFloat-32 tensor cores may be used in\nmatrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on\nAmpere devices.\n\n`cufft_plan_cache` caches the cuFFT plans\n\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\nA `int` that controls cache capacity of cuFFT plan.\n\nClears the cuFFT plan cache.\n\nReturns the version of cuDNN\n\nReturns a bool indicating if CUDNN is currently available.\n\nA `bool` that controls whether cuDNN is enabled.\n\nA `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere\ndevices.\n\nA `bool` that, if True, causes cuDNN to only use deterministic convolution\nalgorithms. See also `torch.are_deterministic_algorithms_enabled()` and\n`torch.use_deterministic_algorithms()`.\n\nA `bool` that, if True, causes cuDNN to benchmark multiple convolution\nalgorithms and select the fastest.\n\nReturns whether PyTorch is built with MKL support.\n\nReturns whether PyTorch is built with MKL-DNN support.\n\nReturns whether PyTorch is built with OpenMP support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": "\n`cufft_plan_cache` caches the cuFFT plans\n\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\nA `int` that controls cache capacity of cuFFT plan.\n\nClears the cuFFT plan cache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t\nnecessarily mean CUDA is available; just that if this PyTorch binary were run\na machine with working CUDA drivers and devices, we would be able to use it.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls whether TensorFloat-32 tensor cores may be used in\nmatrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on\nAmpere devices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": "\nA readonly `int` that shows the number of plans currently in the cuFFT plan\ncache.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": "\nA `bool` that controls where TensorFloat-32 tensor cores may be used in cuDNN\nconvolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere\ndevices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to benchmark multiple convolution\nalgorithms and select the fastest.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": "\nA `bool` that, if True, causes cuDNN to only use deterministic convolution\nalgorithms. See also `torch.are_deterministic_algorithms_enabled()` and\n`torch.use_deterministic_algorithms()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": "\nA `bool` that controls whether cuDNN is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": "\nReturns a bool indicating if CUDNN is currently available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": "\nReturns the version of cuDNN\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with MKL-DNN support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": "\nReturns whether PyTorch is built with OpenMP support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices in `batch1` and `batch2`.\n`input` is added to the final result.\n\n`batch1` and `batch2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `batch1` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `batch2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, then `input` must be broadcastable with a (b\u00d7n\u00d7p)(b\n\\times n \\times p) tensor and `out` will be a (b\u00d7n\u00d7p)(b \\times n \\times p)\ntensor. Both `alpha` and `beta` mean the same as the scaling factors used in\n`torch.addbmm()`.\n\nIf `beta` is 0, then `input` will be ignored, and `nan` and `inf` in it will\nnot be propagated.\n\nFor inputs of type `FloatTensor` or `DoubleTensor`, arguments `beta` and\n`alpha` must be real numbers, otherwise they should be integers.\n\nThis operator supports TensorFloat32.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": "\nBartlett window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.bartlett_window(L, periodic=True)` equal to `torch.bartlett_window(L +\n1, periodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": "\nDraws binary random numbers (0 or 1) from a Bernoulli distribution.\n\nThe `input` tensor should be a tensor containing probabilities to be used for\ndrawing the binary random number. Hence, all values in `input` have to be in\nthe range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1 .\n\nThe ith\\text{i}^{th} element of the output tensor will draw a value 11\naccording to the ith\\text{i}^{th} probability value given in `input`.\n\nThe returned `out` tensor only has values 0 or 1 and is of the same shape as\n`input`.\n\n`out` can have integral `dtype`, but `input` must have floating point `dtype`.\n\ninput (Tensor) \u2013 the input tensor of probability values for the Bernoulli\ndistribution\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": "\nCount the frequency of each value in an array of non-negative ints.\n\nThe number of bins (size 1) is one larger than the largest value in `input`\nunless `input` is empty, in which case the result is a tensor of size 0. If\n`minlength` is specified, the number of bins is at least `minlength` and if\n`input` is empty, then the result is tensor of size `minlength` filled with\nzeros. If `n` is the value at position `i`, `out[n] += weights[i]` if\n`weights` is specified else `out[n] += 1`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\na tensor of shape `Size([max(input) + 1])` if `input` is non-empty, else\n`Size(0)`\n\noutput (Tensor)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": "\nComputes the bitwise AND of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical AND.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": "\nComputes the bitwise NOT of the given input tensor. The input tensor must be\nof integral or Boolean types. For bool tensors, it computes the logical NOT.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": "\nComputes the bitwise OR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical OR.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": "\nComputes the bitwise XOR of `input` and `other`. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": "\nBlackman window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.blackman_window(L, periodic=True)` equal to `torch.blackman_window(L +\n1, periodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": "\nCreate a block diagonal matrix from provided tensors.\n\n*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.\norder such that their upper left and lower right corners are diagonally\nadjacent. All other elements are set to 0.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": "\nPerforms a batch matrix-matrix product of matrices stored in `input` and\n`mat2`.\n\n`input` and `mat2` must be 3-D tensors each containing the same number of\nmatrices.\n\nIf `input` is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, `mat2` is a (b\u00d7m\u00d7p)(b\n\\times m \\times p) tensor, `out` will be a (b\u00d7n\u00d7p)(b \\times n \\times p)\ntensor.\n\nThis operator supports TensorFloat32.\n\nNote\n\nThis function does not broadcast. For broadcasting matrix products, see\n`torch.matmul()`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": "\nSimilar to `broadcast_tensors()` but for shapes.\n\nThis is equivalent to `torch.broadcast_tensors(*map(torch.empty,\nshapes))[0].shape` but avoids the need create to intermediate tensors. This is\nuseful for broadcasting tensors of common batch shape but different rightmost\nshape, e.g. to broadcast mean vectors with covariance matrices.\n\nExample:\n\n*shapes (torch.Size) \u2013 Shapes of tensors.\nA shape compatible with all input shapes.\n\nshape (torch.Size)\n\nRuntimeError \u2013 If shapes are incompatible.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": "\nBroadcasts the given tensors according to Broadcasting semantics.\n\n*tensors \u2013 any number of tensors of the same type\nWarning\n\nMore than one element of a broadcasted tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": "\nBroadcasts `input` to the shape `shape`. Equivalent to calling\n`input.expand(shape)`. See `expand()` for details.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": "\nReturns the indices of the buckets to which each value in the `input` belongs,\nwhere the boundaries of the buckets are set by `boundaries`. Return a new\ntensor with the same size as `input`. If `right` is False (default), then the\nleft boundary is closed. More formally, the returned index satisfies the\nfollowing rules:\n\n`right`\n\nreturned index satisfies\n\nFalse\n\n`boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]`\n\nTrue\n\n`boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": "\nDetermines if a type conversion is allowed under PyTorch casting rules\ndescribed in the type promotion documentation.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": "\nDo cartesian product of the given sequence of tensors. The behavior is similar\nto python\u2019s `itertools.product`.\n\n*tensors \u2013 any number of 1 dimensional tensors.\ndo `itertools.product` on these lists, and finally convert the resulting list\ninto tensor.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": "\nConcatenates the given sequence of `seq` tensors in the given dimension. All\ntensors must either have the same shape (except in the concatenating\ndimension) or be empty.\n\n`torch.cat()` can be seen as an inverse operation for `torch.split()` and\n`torch.chunk()`.\n\n`torch.cat()` can be best understood via examples.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": "\nComputes batched the p-norm distance between each pair of the two collections\nof row vectors.\n\nIf x1 has shape B\u00d7P\u00d7MB \\times P \\times M and x2 has shape B\u00d7R\u00d7MB \\times R\n\\times M then the output will have shape B\u00d7P\u00d7RB \\times P \\times R .\n\nThis function is equivalent to\n`scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0,\n\\infty) . When p=0p = 0 it is equivalent to\n`scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M`. When p=\u221ep = \\infty , the\nclosest scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y:\nnp.abs(x - y).max())`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": "\nReturns a new tensor with the ceil of the elements of `input`, the smallest\ninteger greater than or equal to each element.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": "\nReturns the matrix product of the NN 2-D tensors. This product is efficiently\ncomputed using the matrix chain order algorithm which selects the order in\nwhich incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note\nthat since this is a function to compute the product, NN needs to be greater\nthan or equal to 2; if equal to 2 then a trivial matrix-matrix product is\nreturned. If NN is 1, then this is a no-op - the original matrix is returned\nas is.\n\nmatrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is\nto be determined.\n\nif the ithi^{th} tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1} , then\nthe product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1} .\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": "\nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA\nor for batches of symmetric positive-definite matrices.\n\nIf `upper` is `True`, the returned matrix `U` is upper-triangular, and the\ndecomposition has the form:\n\nIf `upper` is `False`, the returned matrix `L` is lower-triangular, and the\ndecomposition has the form:\n\nIf `upper` is `True`, and AA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular\nCholesky factors of each of the individual matrices. Similarly, when `upper`\nis `False`, the returned tensor will be composed of lower-triangular Cholesky\nfactors of each of the individual matrices.\n\nNote\n\n`torch.linalg.cholesky()` should be used over `torch.cholesky` when possible.\nNote however that `torch.linalg.cholesky()` does not yet support the `upper`\nparameter and instead always returns the lower triangular matrix.\n\nout (Tensor, optional) \u2013 the output matrix\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": "\nComputes the inverse of a symmetric positive-definite matrix AA using its\nCholesky factor uu : returns matrix `inv`. The inverse is computed using\nLAPACK routines `dpotri` and `spotri` (and the corresponding MAGMA routines).\n\nIf `upper` is `False`, uu is lower triangular such that the returned tensor is\n\nIf `upper` is `True` or not provided, uu is upper triangular such that the\nreturned tensor is\n\nout (Tensor, optional) \u2013 the output tensor for `inv`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": "\nSolves a linear system of equations with a positive semidefinite matrix to be\ninverted given its Cholesky factor matrix uu .\n\nIf `upper` is `False`, uu is and lower triangular and `c` is returned such\nthat:\n\nIf `upper` is `True` or not provided, uu is upper triangular and `c` is\nreturned such that:\n\n`torch.cholesky_solve(b, u)` can take in 2D inputs `b, u` or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns batched\noutputs `c`\n\nSupports real-valued and complex-valued inputs. For the complex-valued inputs\nthe transpose operator above is the conjugate transpose.\n\nout (Tensor, optional) \u2013 the output tensor for `c`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": "\nSplits a tensor into a specific number of chunks. Each chunk is a view of the\ninput tensor.\n\nLast chunk will be smaller if the tensor size along the given dimension `dim`\nis not divisible by `chunks`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": "\nClamp all elements in `input` into the range `[` `min`, `max` `]`. Let\nmin_value and max_value be `min` and `max`, respectively, this returns:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nClamps all elements in `input` to be larger or equal `min`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nClamps all elements in `input` to be smaller or equal `max`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": "\nAlias for `torch.clamp()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": "\nReturns a copy of `input`.\n\nNote\n\nThis function is differentiable, so gradients will flow back from the result\nof this operation to `input`. To create a tensor without an autograd\nrelationship to `input` see `detach()`.\n\ninput (Tensor) \u2013 the input tensor.\n\nmemory_format (`torch.memory_format`, optional) \u2013 the desired memory format of\nreturned tensor. Default: `torch.preserve_format`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": "\nCreates a new tensor by horizontally stacking the tensors in `tensors`.\n\nEquivalent to `torch.hstack(tensors)`, except each zero or one dimensional\ntensor `t` in `tensors` is first reshaped into a `(t.numel(), 1)` column\nbefore being stacked horizontally.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": "\nCompute combinations of length rr of the given tensor. The behavior is similar\nto python\u2019s `itertools.combinations` when `with_replacement` is set to\n`False`, and `itertools.combinations_with_replacement` when `with_replacement`\nis set to `True`.\n\nA tensor equivalent to converting all the input tensors into lists, do\n`itertools.combinations` or `itertools.combinations_with_replacement` on these\nlists, and finally convert the resulting list into tensor.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": "\nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": "\nConstructs a complex tensor with its real part equal to `real` and its\nimaginary part equal to `imag`.\n\nout (Tensor) \u2013 If the inputs are `torch.float32`, must be `torch.complex64`.\nIf the inputs are `torch.float64`, must be `torch.complex128`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": "\nComputes the element-wise conjugate of the given `input` tensor. If\n:attr`input` has a non-complex dtype, this function just returns `input`.\n\nWarning\n\nIn the future, `torch.conj()` may return a non-writeable view for an `input`\nof non-complex dtype. It\u2019s recommended that programs not modify the tensor\nreturned by `torch.conj()` when `input` is of non-complex dtype to be\ncompatible with this change.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": "\nCreate a new floating-point tensor with the magnitude of `input` and the sign\nof `other`, elementwise.\n\nSupports broadcasting to a common shape, and integer and float inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": "\nReturns a new tensor with the cosine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": "\nReturns a new tensor with the hyperbolic cosine of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\nNote\n\nWhen `input` is on the CPU, the implementation of torch.cosh may use the Sleef\nlibrary, which rounds very large results to infinity or negative infinity. See\nhere for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": "\nCounts the number of non-zero values in the tensor `input` along the given\n`dim`. If no dim is specified then all non-zeros in the tensor are counted.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": "\nReturns the cross product of vectors in dimension `dim` of `input` and\n`other`.\n\n`input` and `other` must have the same size, and the size of their `dim`\ndimension should be 3.\n\nIf `dim` is not given, it defaults to the first dimension found with the size\n3. Note that this might be unexpected.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": "\nThis package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation.\n\nIt is lazily initialized, so you can always import it, and use\n`is_available()` to determine if your system supports CUDA.\n\nCUDA semantics has more details about working with CUDA.\n\nChecks if peer access between two devices is possible.\n\nReturns cublasHandle_t pointer to current cuBLAS handle\n\nReturns the index of a currently selected device.\n\nReturns the currently selected `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the\ncurrently selected `Stream` for the current device, given by\n`current_device()`, if `device` is `None` (default).\n\nReturns the default `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the default\n`Stream` for the current device, given by `current_device()`, if `device` is\n`None` (default).\n\nContext-manager that changes the selected device.\n\ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this\nargument is a negative integer or `None`.\n\nReturns the number of GPUs available.\n\nContext-manager that changes the current device to that of given object.\n\nYou can use both tensors and storages as arguments. If a given object is not\nallocated on a GPU, this is a no-op.\n\nobj (Tensor or Storage) \u2013 object allocated on the selected device.\n\nReturns list CUDA architectures this library was compiled for.\n\nGets the cuda capability of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the device\ncapability. This function is a no-op if this argument is a negative integer.\nIt uses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nthe major and minor cuda capability of the device\n\ntuple(int, int)\n\nGets the name of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the name.\nThis function is a no-op if this argument is a negative integer. It uses the\ncurrent device, given by `current_device()`, if `device` is `None` (default).\n\nthe name of the device\n\nstr\n\nGets the properties of a device.\n\ndevice (torch.device or int or str) \u2013 device for which to return the\nproperties of the device.\n\nthe properties of the device\n\n_CudaDeviceProperties\n\nReturns NVCC gencode flags this library were compiled with.\n\nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you\nare interacting with PyTorch via its C API, as Python bindings for CUDA\nfunctionality will not be available until this initialization takes place.\nOrdinary users should not need this, as all of PyTorch\u2019s CUDA methods\nautomatically initialize CUDA state on-demand.\n\nDoes nothing if the CUDA state is already initialized.\n\nForce collects GPU memory after it has been released by CUDA IPC.\n\nNote\n\nChecks if any sent CUDA tensors could be cleaned from the memory. Force closes\nshared memory file used for reference counting if there is no active counters.\nUseful when the producer process stopped actively sending tensors and want to\nrelease unused memory.\n\nReturns a bool indicating if CUDA is currently available.\n\nReturns whether PyTorch\u2019s CUDA state has been initialized.\n\nSets the current device.\n\nUsage of this function is discouraged in favor of `device`. In most cases it\u2019s\nbetter to use `CUDA_VISIBLE_DEVICES` environmental variable.\n\ndevice (torch.device or int) \u2013 selected device. This function is a no-op if\nthis argument is negative.\n\nContext-manager that selects a given stream.\n\nAll CUDA kernels queued within its context will be enqueued on a selected\nstream.\n\nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s `None`.\n\nNote\n\nStreams are per-device. If the selected stream is not on the current device,\nthis function will also change the current device to match the stream.\n\nWaits for all kernels in all streams on a CUDA device to complete.\n\ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It\nuses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nReturns the random number generator state of the specified GPU as a\nByteTensor.\n\ndevice (torch.device or int, optional) \u2013 The device to return the RNG state\nof. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).\n\nWarning\n\nThis function eagerly initializes CUDA.\n\nReturns a list of ByteTensor representing the random number states of all\ndevices.\n\nSets the random number generator state of the specified GPU.\n\nSets the random number generator state of all devices.\n\nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device\n\nSets the seed for generating random numbers for the current GPU. It\u2019s safe to\ncall this function if CUDA is not available; in that case, it is silently\nignored.\n\nseed (int) \u2013 The desired seed.\n\nWarning\n\nIf you are working with a multi-GPU model, this function is insufficient to\nget determinism. To seed all GPUs, use `manual_seed_all()`.\n\nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call\nthis function if CUDA is not available; in that case, it is silently ignored.\n\nseed (int) \u2013 The desired seed.\n\nSets the seed for generating random numbers to a random number for the current\nGPU. It\u2019s safe to call this function if CUDA is not available; in that case,\nit is silently ignored.\n\nWarning\n\nIf you are working with a multi-GPU model, this function will only initialize\nthe seed on one GPU. To initialize all GPUs, use `seed_all()`.\n\nSets the seed for generating random numbers to a random number on all GPUs.\nIt\u2019s safe to call this function if CUDA is not available; in that case, it is\nsilently ignored.\n\nReturns the current random seed of the current GPU.\n\nWarning\n\nThis function eagerly initializes CUDA.\n\nBroadcasts a tensor to specified GPU devices.\n\nNote\n\nExactly one of `devices` and `out` must be specified.\n\na tuple containing copies of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a copy of `tensor`.\n\nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first\ncoalesced into a buffer to reduce the number of synchronizations.\n\nA tuple containing copies of `tensor`, placed on `devices`.\n\nSums tensors from multiple GPUs.\n\nAll inputs should have matching shapes, dtype, and layout. The output tensor\nwill be of the same shape, dtype, and layout.\n\nA tensor containing an elementwise sum of all inputs, placed on the\n`destination` device.\n\nScatters tensor across multiple GPUs.\n\nNote\n\nExactly one of `devices` and `out` must be specified. When `out` is specified,\n`chunk_sizes` must not be specified and will be inferred from sizes of `out`.\n\na tuple containing chunks of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a chunk of `tensor`.\n\nGathers tensors from multiple GPU devices.\n\nNote\n\n`destination` must not be specified when `out` is specified.\n\na tensor located on `destination` device, that is a result of concatenating\n`tensors` along `dim`.\n\nthe `out` tensor, now containing results of concatenating `tensors` along\n`dim`.\n\nWrapper around a CUDA stream.\n\nA CUDA stream is a linear sequence of execution that belongs to a specific\ndevice, independent from other streams. See CUDA semantics for details.\n\nNote\n\nAlthough CUDA versions >= 11 support more than two levels of priorities, in\nPyTorch, we only support two levels of priorities.\n\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\nWrapper around a CUDA event.\n\nCUDA events are synchronization markers that can be used to monitor the\ndevice\u2019s progress, to accurately measure timing, and to synchronize CUDA\nstreams.\n\nThe underlying CUDA events are lazily initialized when the event is first\nrecorded or exported to another process. After creation, only streams on the\nsame device may record the event. However, streams on any device can wait on\nthe event.\n\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\nReconstruct an event from an IPC handle on the given device.\n\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\nReleases all unoccupied cached memory currently held by the caching allocator\nso that those can be used in other GPU application and visible in `nvidia-\nsmi`.\n\nNote\n\n`empty_cache()` doesn\u2019t increase the amount of GPU memory available for\nPyTorch. However, it may help reduce fragmentation of GPU memory in certain\ncases. See Memory management for more details about GPU memory management.\n\nReturns a human-readable printout of the running processes and their GPU\nmemory use for a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for\nthe current device, given by `current_device()`, if `device` is `None`\n(default).\n\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\nThe return value of this function is a dictionary of statistics, each of which\nis a non-negative integer.\n\nCore statistics:\n\nFor these core statistics, values are broken down as follows.\n\nPool type:\n\nMetric type:\n\nIn addition to the core statistics, we also provide some simple event\ncounters:\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns a human-readable printout of the current memory allocator statistics\nfor a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\nInterpreting the output of this function requires familiarity with the memory\nallocator internals.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the current GPU memory occupied by tensors in bytes for a given\ndevice.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nThis is likely less than the amount shown in `nvidia-smi` since some unused\nmemory can be held by the caching allocator and some context needs to be\ncreated on GPU. See Memory management for more details about GPU memory\nmanagement.\n\nReturns the maximum GPU memory occupied by tensors in bytes for a given\ndevice.\n\nBy default, this returns the peak allocated memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\nallocated memory usage of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nResets the starting point in tracking maximum GPU memory occupied by tensors\nfor a given device.\n\nSee `max_memory_allocated()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the current GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nReturns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\nBy default, this returns the peak cached memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\ncached memory amount of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nSet memory fraction for a process. The fraction is used to limit an caching\nallocator to allocated memory on a CUDA device. The allowed value equals the\ntotal visible memory multiplied fraction. If trying to allocate more than the\nallowed value in a process, will raise an out of memory error in allocator.\n\nNote\n\nIn general, the total available free memory is less than the total capacity.\n\nDeprecated; see `memory_reserved()`.\n\nDeprecated; see `max_memory_reserved()`.\n\nResets the starting point in tracking maximum GPU memory managed by the\ncaching allocator for a given device.\n\nSee `max_memory_cached()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\nDescribe an instantaneous event that occurred at some point.\n\nmsg (string) \u2013 ASCII message to associate with the event.\n\nPushes a range onto a stack of nested range span. Returns zero-based depth of\nthe range that is started.\n\nmsg (string) \u2013 ASCII message to associate with range\n\nPops a range off of a stack of nested range spans. Returns the zero-based\ndepth of the range that is ended.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": "\n`torch.cuda.amp` provides convenience methods for mixed precision, where some\noperations use the `torch.float32` (`float`) datatype and other operations use\n`torch.float16` (`half`). Some ops, like linear layers and convolutions, are\nmuch faster in `float16`. Other ops, like reductions, often require the\ndynamic range of `float32`. Mixed precision tries to match each op to its\nappropriate datatype.\n\nOrdinarily, \u201cautomatic mixed precision training\u201d uses\n`torch.cuda.amp.autocast` and `torch.cuda.amp.GradScaler` together, as shown\nin the Automatic Mixed Precision examples and Automatic Mixed Precision\nrecipe. However, `autocast` and `GradScaler` are modular, and may be used\nseparately if desired.\n\nAutocast Op Reference\n\nOp-Specific Behavior\n\nInstances of `autocast` serve as context managers or decorators that allow\nregions of your script to run in mixed precision.\n\nIn these regions, CUDA ops run in an op-specific dtype chosen by autocast to\nimprove performance while maintaining accuracy. See the Autocast Op Reference\nfor details.\n\nWhen entering an autocast-enabled region, Tensors may be any type. You should\nnot call `.half()` on your model(s) or inputs when using autocasting.\n\n`autocast` should wrap only the forward pass(es) of your network, including\nthe loss computation(s). Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward\nops.\n\nExample:\n\nSee the Automatic Mixed Precision examples for usage (along with gradient\nscaling) in more complex scenarios (e.g., gradient penalty, multiple\nmodels/losses, custom autograd functions).\n\n`autocast` can also be used as a decorator, e.g., on the `forward` method of\nyour model:\n\nFloating-point Tensors produced in an autocast-enabled region may be\n`float16`. After returning to an autocast-disabled region, using them with\nfloating-point Tensors of different dtypes may cause type mismatch errors. If\nso, cast the Tensor(s) produced in the autocast region back to `float32` (or\nother dtype if desired). If a Tensor from the autocast region is already\n`float32`, the cast is a no-op, and incurs no additional overhead. Example:\n\nType mismatch errors in an autocast-enabled region are a bug; if this is what\nyou observe, please file an issue.\n\n`autocast(enabled=False)` subregions can be nested in autocast-enabled\nregions. Locally disabling autocast can be useful, for example, if you want to\nforce a subregion to run in a particular `dtype`. Disabling autocast gives you\nexplicit control over the execution type. In the subregion, inputs from the\nsurrounding region should be cast to `dtype` before use:\n\nThe autocast state is thread-local. If you want it enabled in a new thread,\nthe context manager or decorator must be invoked in that thread. This affects\n`torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel` when\nused with more than one GPU per process (see Working with Multiple GPUs).\n\nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled\nin the region.\n\nHelper decorator for `forward` methods of custom autograd functions\n(subclasses of `torch.autograd.Function`). See the example page for more\ndetail.\n\ncast_inputs (`torch.dtype` or None, optional, default=None) \u2013 If not `None`,\nwhen `forward` runs in an autocast-enabled region, casts incoming floating-\npoint CUDA Tensors to the target dtype (non-floating-point Tensors are not\naffected), then executes `forward` with autocast disabled. If `None`,\n`forward`\u2019s internal ops execute with the current autocast state.\n\nNote\n\nIf the decorated `forward` is called outside an autocast-enabled region,\n`custom_fwd` is a no-op and `cast_inputs` has no effect.\n\nHelper decorator for backward methods of custom autograd functions (subclasses\nof `torch.autograd.Function`). Ensures that `backward` executes with the same\nautocast state as `forward`. See the example page for more detail.\n\nIf the forward pass for a particular op has `float16` inputs, the backward\npass for that op will produce `float16` gradients. Gradient values with small\nmagnitudes may not be representable in `float16`. These values will flush to\nzero (\u201cunderflow\u201d), so the update for the corresponding parameters will be\nlost.\n\nTo prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by\na scale factor and invokes a backward pass on the scaled loss(es). Gradients\nflowing backward through the network are then scaled by the same factor. In\nother words, gradient values have a larger magnitude, so they don\u2019t flush to\nzero.\n\nEach parameter\u2019s gradient (`.grad` attribute) should be unscaled before the\noptimizer updates the parameters, so the scale factor does not interfere with\nthe learning rate.\n\nReturns a Python float containing the scale backoff factor.\n\nReturns a Python float containing the scale growth factor.\n\nReturns a Python int containing the growth interval.\n\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\nReturns a bool indicating whether this instance is enabled.\n\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\nOnly CUDA ops are eligible for autocasting.\n\nOps that run in `float64` or non-floating-point dtypes are not eligible, and\nwill run in these types whether or not autocast is enabled.\n\nOnly out-of-place ops and Tensor methods are eligible. In-place variants and\ncalls that explicitly supply an `out=...` Tensor are allowed in autocast-\nenabled regions, but won\u2019t go through autocasting. For example, in an\nautocast-enabled region `a.addmm(b, c)` can autocast, but `a.addmm_(b, c)` and\n`a.addmm(b, c, out=d)` cannot. For best performance and stability, prefer out-\nof-place ops in autocast-enabled regions.\n\nOps called with an explicit `dtype=...` argument are not eligible, and will\nproduce output that respects the `dtype` argument.\n\nThe following lists describe the behavior of eligible ops in autocast-enabled\nregions. These ops always go through autocasting whether they are invoked as\npart of a `torch.nn.Module`, as a function, or as a `torch.Tensor` method. If\nfunctions are exposed in multiple namespaces, they go through autocasting\nregardless of the namespace.\n\nOps not listed below do not go through autocasting. They run in the type\ndefined by their inputs. However, autocasting may still change the type in\nwhich unlisted ops run if they\u2019re downstream from autocasted ops.\n\nIf an op is unlisted, we assume it\u2019s numerically stable in `float16`. If you\nbelieve an unlisted op is numerically unstable in `float16`, please file an\nissue.\n\n`__matmul__`, `addbmm`, `addmm`, `addmv`, `addr`, `baddbmm`, `bmm`,\n`chain_matmul`, `conv1d`, `conv2d`, `conv3d`, `conv_transpose1d`,\n`conv_transpose2d`, `conv_transpose3d`, `GRUCell`, `linear`, `LSTMCell`,\n`matmul`, `mm`, `mv`, `prelu`, `RNNCell`\n\n`__pow__`, `__rdiv__`, `__rpow__`, `__rtruediv__`, `acos`, `asin`,\n`binary_cross_entropy_with_logits`, `cosh`, `cosine_embedding_loss`, `cdist`,\n`cosine_similarity`, `cross_entropy`, `cumprod`, `cumsum`, `dist`, `erfinv`,\n`exp`, `expm1`, `gelu`, `group_norm`, `hinge_embedding_loss`, `kl_div`,\n`l1_loss`, `layer_norm`, `log`, `log_softmax`, `log10`, `log1p`, `log2`,\n`margin_ranking_loss`, `mse_loss`, `multilabel_margin_loss`,\n`multi_margin_loss`, `nll_loss`, `norm`, `normalize`, `pdist`,\n`poisson_nll_loss`, `pow`, `prod`, `reciprocal`, `rsqrt`, `sinh`,\n`smooth_l1_loss`, `soft_margin_loss`, `softmax`, `softmin`, `softplus`, `sum`,\n`renorm`, `tan`, `triplet_margin_loss`\n\nThese ops don\u2019t require a particular dtype for stability, but take multiple\ninputs and require that the inputs\u2019 dtypes match. If all of the inputs are\n`float16`, the op runs in `float16`. If any of the inputs is `float32`,\nautocast casts all inputs to `float32` and runs the op in `float32`.\n\n`addcdiv`, `addcmul`, `atan2`, `bilinear`, `cat`, `cross`, `dot`, `equal`,\n`index_put`, `stack`, `tensordot`\n\nSome ops not listed here (e.g., binary ops like `add`) natively promote inputs\nwithout autocasting\u2019s intervention. If inputs are a mixture of `float16` and\n`float32`, these ops run in `float32` and produce `float32` output, regardless\nof whether autocast is enabled.\n\nThe backward passes of `torch.nn.functional.binary_cross_entropy()` (and\n`torch.nn.BCELoss`, which wraps it) can produce gradients that aren\u2019t\nrepresentable in `float16`. In autocast-enabled regions, the forward input may\nbe `float16`, which means the backward gradient must be representable in\n`float16` (autocasting `float16` forward inputs to `float32` doesn\u2019t help,\nbecause that cast must be reversed in backward). Therefore,\n`binary_cross_entropy` and `BCELoss` raise an error in autocast-enabled\nregions.\n\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using\n`torch.nn.functional.binary_cross_entropy_with_logits()` or\n`torch.nn.BCEWithLogitsLoss`. `binary_cross_entropy_with_logits` and\n`BCEWithLogits` are safe to autocast.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": "\nInstances of `autocast` serve as context managers or decorators that allow\nregions of your script to run in mixed precision.\n\nIn these regions, CUDA ops run in an op-specific dtype chosen by autocast to\nimprove performance while maintaining accuracy. See the Autocast Op Reference\nfor details.\n\nWhen entering an autocast-enabled region, Tensors may be any type. You should\nnot call `.half()` on your model(s) or inputs when using autocasting.\n\n`autocast` should wrap only the forward pass(es) of your network, including\nthe loss computation(s). Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward\nops.\n\nExample:\n\nSee the Automatic Mixed Precision examples for usage (along with gradient\nscaling) in more complex scenarios (e.g., gradient penalty, multiple\nmodels/losses, custom autograd functions).\n\n`autocast` can also be used as a decorator, e.g., on the `forward` method of\nyour model:\n\nFloating-point Tensors produced in an autocast-enabled region may be\n`float16`. After returning to an autocast-disabled region, using them with\nfloating-point Tensors of different dtypes may cause type mismatch errors. If\nso, cast the Tensor(s) produced in the autocast region back to `float32` (or\nother dtype if desired). If a Tensor from the autocast region is already\n`float32`, the cast is a no-op, and incurs no additional overhead. Example:\n\nType mismatch errors in an autocast-enabled region are a bug; if this is what\nyou observe, please file an issue.\n\n`autocast(enabled=False)` subregions can be nested in autocast-enabled\nregions. Locally disabling autocast can be useful, for example, if you want to\nforce a subregion to run in a particular `dtype`. Disabling autocast gives you\nexplicit control over the execution type. In the subregion, inputs from the\nsurrounding region should be cast to `dtype` before use:\n\nThe autocast state is thread-local. If you want it enabled in a new thread,\nthe context manager or decorator must be invoked in that thread. This affects\n`torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel` when\nused with more than one GPU per process (see Working with Multiple GPUs).\n\nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled\nin the region.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for backward methods of custom autograd functions (subclasses\nof `torch.autograd.Function`). Ensures that `backward` executes with the same\nautocast state as `forward`. See the example page for more detail.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": "\nHelper decorator for `forward` methods of custom autograd functions\n(subclasses of `torch.autograd.Function`). See the example page for more\ndetail.\n\ncast_inputs (`torch.dtype` or None, optional, default=None) \u2013 If not `None`,\nwhen `forward` runs in an autocast-enabled region, casts incoming floating-\npoint CUDA Tensors to the target dtype (non-floating-point Tensors are not\naffected), then executes `forward` with autocast disabled. If `None`,\n`forward`\u2019s internal ops execute with the current autocast state.\n\nNote\n\nIf the decorated `forward` is called outside an autocast-enabled region,\n`custom_fwd` is a no-op and `cast_inputs` has no effect.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\nReturns a Python float containing the scale growth factor.\n\nReturns a Python int containing the growth interval.\n\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\nReturns a bool indicating whether this instance is enabled.\n\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale backoff factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the scale growth factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": "\nReturns a Python int containing the growth interval.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": "\nReturns a Python float containing the current scale, or 1.0 if scaling is\ndisabled.\n\nWarning\n\n`get_scale()` incurs a CPU-GPU sync.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": "\nReturns a bool indicating whether this instance is enabled.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": "\nLoads the scaler state. If this instance is disabled, `load_state_dict()` is a\nno-op.\n\nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to\n`state_dict()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": "\nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor.\n\nReturns scaled outputs. If this instance of `GradScaler` is not enabled,\noutputs are returned unmodified.\n\noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale backoff factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": "\nnew_scale (float) \u2013 Value to use as the new scale growth factor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": "\nnew_interval (int) \u2013 Value to use as the new growth interval.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": "\nReturns the state of the scaler as a `dict`. It contains five entries:\n\nIf this instance is not enabled, returns an empty dict.\n\nNote\n\nIf you wish to checkpoint the scaler\u2019s state after a particular iteration,\n`state_dict()` should be called after `update()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": "\n`step()` carries out the following two operations:\n\n`*args` and `**kwargs` are forwarded to `optimizer.step()`.\n\nReturns the return value of `optimizer.step(*args, **kwargs)`.\n\nWarning\n\nClosure use is not currently supported.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": "\nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor.\n\n`unscale_()` is optional, serving cases where you need to modify or inspect\ngradients between the backward pass(es) and `step()`. If `unscale_()` is not\ncalled explicitly, gradients will be unscaled automatically during `step()`.\n\nSimple example, using `unscale_()` to enable clipping of unscaled gradients:\n\noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be\nunscaled.\n\nNote\n\n`unscale_()` does not incur a CPU-GPU sync.\n\nWarning\n\n`unscale_()` should only be called once per optimizer per `step()` call, and\nonly after all gradients for that optimizer\u2019s assigned parameters have been\naccumulated. Calling `unscale_()` twice for a given optimizer between each\n`step()` triggers a RuntimeError.\n\nWarning\n\n`unscale_()` may unscale sparse gradients out of place, replacing the `.grad`\nattribute.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": "\nUpdates the scale factor.\n\nIf any optimizer steps were skipped the scale is multiplied by\n`backoff_factor` to reduce it. If `growth_interval` unskipped iterations\noccurred consecutively, the scale is multiplied by `growth_factor` to increase\nit.\n\nPassing `new_scale` sets the scale directly.\n\nnew_scale (float or `torch.cuda.FloatTensor`, optional, default=None) \u2013 New\nscale factor.\n\nWarning\n\n`update()` should only be called at the end of the iteration, after\n`scaler.step(optimizer)` has been invoked for all optimizers used this\niteration.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": "\nChecks if peer access between two devices is possible.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": "\nBroadcasts a tensor to specified GPU devices.\n\nNote\n\nExactly one of `devices` and `out` must be specified.\n\na tuple containing copies of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a copy of `tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": "\nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first\ncoalesced into a buffer to reduce the number of synchronizations.\n\nA tuple containing copies of `tensor`, placed on `devices`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": "\nGathers tensors from multiple GPU devices.\n\nNote\n\n`destination` must not be specified when `out` is specified.\n\na tensor located on `destination` device, that is a result of concatenating\n`tensors` along `dim`.\n\nthe `out` tensor, now containing results of concatenating `tensors` along\n`dim`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": "\nSums tensors from multiple GPUs.\n\nAll inputs should have matching shapes, dtype, and layout. The output tensor\nwill be of the same shape, dtype, and layout.\n\nA tensor containing an elementwise sum of all inputs, placed on the\n`destination` device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": "\nScatters tensor across multiple GPUs.\n\nNote\n\nExactly one of `devices` and `out` must be specified. When `out` is specified,\n`chunk_sizes` must not be specified and will be inferred from sizes of `out`.\n\na tuple containing chunks of `tensor`, placed on `devices`.\n\na tuple containing `out` tensors, each containing a chunk of `tensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": "\nReturns cublasHandle_t pointer to current cuBLAS handle\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": "\nReturns the index of a currently selected device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": "\nReturns the currently selected `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the\ncurrently selected `Stream` for the current device, given by\n`current_device()`, if `device` is `None` (default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": "\nReturns the default `Stream` for a given device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns the default\n`Stream` for the current device, given by `current_device()`, if `device` is\n`None` (default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": "\nContext-manager that changes the selected device.\n\ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this\nargument is a negative integer or `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": "\nReturns the number of GPUs available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": "\nContext-manager that changes the current device to that of given object.\n\nYou can use both tensors and storages as arguments. If a given object is not\nallocated on a GPU, this is a no-op.\n\nobj (Tensor or Storage) \u2013 object allocated on the selected device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": "\nReleases all unoccupied cached memory currently held by the caching allocator\nso that those can be used in other GPU application and visible in `nvidia-\nsmi`.\n\nNote\n\n`empty_cache()` doesn\u2019t increase the amount of GPU memory available for\nPyTorch. However, it may help reduce fragmentation of GPU memory in certain\ncases. See Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": "\nWrapper around a CUDA event.\n\nCUDA events are synchronization markers that can be used to monitor the\ndevice\u2019s progress, to accurately measure timing, and to synchronize CUDA\nstreams.\n\nThe underlying CUDA events are lazily initialized when the event is first\nrecorded or exported to another process. After creation, only streams on the\nsame device may record the event. However, streams on any device can wait on\nthe event.\n\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\nReconstruct an event from an IPC handle on the given device.\n\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": "\nReturns the time elapsed in milliseconds after the event was recorded and\nbefore the end_event was recorded.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": "\nReconstruct an event from an IPC handle on the given device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": "\nReturns an IPC handle of this event. If not recorded yet, the event will use\nthe current device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": "\nChecks if all work currently captured by event has completed.\n\nA boolean indicating if all work currently captured by event has completed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": "\nRecords the event in a given stream.\n\nUses `torch.cuda.current_stream()` if no stream is specified. The stream\u2019s\ndevice must match the event\u2019s device.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": "\nWaits for the event to complete.\n\nWaits until the completion of all work currently captured in this event. This\nprevents the CPU thread from proceeding until the event completes.\n\nNote\n\nThis is a wrapper around `cudaEventSynchronize()`: see CUDA Event\ndocumentation for more info.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": "\nMakes all future work submitted to the given stream wait for this event.\n\nUse `torch.cuda.current_stream()` if no stream is specified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": "\nReturns list CUDA architectures this library was compiled for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": "\nGets the cuda capability of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the device\ncapability. This function is a no-op if this argument is a negative integer.\nIt uses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nthe major and minor cuda capability of the device\n\ntuple(int, int)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": "\nGets the name of a device.\n\ndevice (torch.device or int, optional) \u2013 device for which to return the name.\nThis function is a no-op if this argument is a negative integer. It uses the\ncurrent device, given by `current_device()`, if `device` is `None` (default).\n\nthe name of the device\n\nstr\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": "\nGets the properties of a device.\n\ndevice (torch.device or int or str) \u2013 device for which to return the\nproperties of the device.\n\nthe properties of the device\n\n_CudaDeviceProperties\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": "\nReturns NVCC gencode flags this library were compiled with.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": "\nReturns the random number generator state of the specified GPU as a\nByteTensor.\n\ndevice (torch.device or int, optional) \u2013 The device to return the RNG state\nof. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).\n\nWarning\n\nThis function eagerly initializes CUDA.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": "\nReturns a list of ByteTensor representing the random number states of all\ndevices.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": "\nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you\nare interacting with PyTorch via its C API, as Python bindings for CUDA\nfunctionality will not be available until this initialization takes place.\nOrdinary users should not need this, as all of PyTorch\u2019s CUDA methods\nautomatically initialize CUDA state on-demand.\n\nDoes nothing if the CUDA state is already initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": "\nReturns the current random seed of the current GPU.\n\nWarning\n\nThis function eagerly initializes CUDA.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": "\nForce collects GPU memory after it has been released by CUDA IPC.\n\nNote\n\nChecks if any sent CUDA tensors could be cleaned from the memory. Force closes\nshared memory file used for reference counting if there is no active counters.\nUseful when the producer process stopped actively sending tensors and want to\nrelease unused memory.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": "\nReturns a bool indicating if CUDA is currently available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": "\nReturns whether PyTorch\u2019s CUDA state has been initialized.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the running processes and their GPU\nmemory use for a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for\nthe current device, given by `current_device()`, if `device` is `None`\n(default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers for the current GPU. It\u2019s safe to\ncall this function if CUDA is not available; in that case, it is silently\nignored.\n\nseed (int) \u2013 The desired seed.\n\nWarning\n\nIf you are working with a multi-GPU model, this function is insufficient to\nget determinism. To seed all GPUs, use `manual_seed_all()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call\nthis function if CUDA is not available; in that case, it is silently ignored.\n\nseed (int) \u2013 The desired seed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory occupied by tensors in bytes for a given\ndevice.\n\nBy default, this returns the peak allocated memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\nallocated memory usage of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `max_memory_reserved()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": "\nReturns the maximum GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\nBy default, this returns the peak cached memory since the beginning of this\nprogram. `reset_peak_stats()` can be used to reset the starting point in\ntracking this metric. For example, these two functions can measure the peak\ncached memory amount of each iteration in a training loop.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": "\nReturns the current GPU memory occupied by tensors in bytes for a given\ndevice.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nThis is likely less than the amount shown in `nvidia-smi` since some unused\nmemory can be held by the caching allocator and some context needs to be\ncreated on GPU. See Memory management for more details about GPU memory\nmanagement.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": "\nDeprecated; see `memory_reserved()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": "\nReturns the current GPU memory managed by the caching allocator in bytes for a\ngiven device.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": "\nReturns a snapshot of the CUDA memory allocator state across all devices.\n\nInterpreting the output of this function requires familiarity with the memory\nallocator internals.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": "\nReturns a dictionary of CUDA memory allocator statistics for a given device.\n\nThe return value of this function is a dictionary of statistics, each of which\nis a non-negative integer.\n\nCore statistics:\n\nFor these core statistics, values are broken down as follows.\n\nPool type:\n\nMetric type:\n\nIn addition to the core statistics, we also provide some simple event\ncounters:\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": "\nReturns a human-readable printout of the current memory allocator statistics\nfor a given device.\n\nThis can be useful to display periodically during training, or when handling\nout-of-memory exceptions.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": "\nDescribe an instantaneous event that occurred at some point.\n\nmsg (string) \u2013 ASCII message to associate with the event.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": "\nPops a range off of a stack of nested range spans. Returns the zero-based\ndepth of the range that is ended.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": "\nPushes a range onto a stack of nested range span. Returns zero-based depth of\nthe range that is started.\n\nmsg (string) \u2013 ASCII message to associate with range\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory occupied by tensors\nfor a given device.\n\nSee `max_memory_allocated()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": "\nResets the starting point in tracking maximum GPU memory managed by the\ncaching allocator for a given device.\n\nSee `max_memory_cached()` for details.\n\ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic\nfor the current device, given by `current_device()`, if `device` is `None`\n(default).\n\nWarning\n\nThis function now calls `reset_peak_memory_stats()`, which resets /all/ peak\nmemory stats.\n\nNote\n\nSee Memory management for more details about GPU memory management.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number for the current\nGPU. It\u2019s safe to call this function if CUDA is not available; in that case,\nit is silently ignored.\n\nWarning\n\nIf you are working with a multi-GPU model, this function will only initialize\nthe seed on one GPU. To initialize all GPUs, use `seed_all()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": "\nSets the seed for generating random numbers to a random number on all GPUs.\nIt\u2019s safe to call this function if CUDA is not available; in that case, it is\nsilently ignored.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": "\nSets the current device.\n\nUsage of this function is discouraged in favor of `device`. In most cases it\u2019s\nbetter to use `CUDA_VISIBLE_DEVICES` environmental variable.\n\ndevice (torch.device or int) \u2013 selected device. This function is a no-op if\nthis argument is negative.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": "\nSet memory fraction for a process. The fraction is used to limit an caching\nallocator to allocated memory on a CUDA device. The allowed value equals the\ntotal visible memory multiplied fraction. If trying to allocate more than the\nallowed value in a process, will raise an out of memory error in allocator.\n\nNote\n\nIn general, the total available free memory is less than the total capacity.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": "\nSets the random number generator state of the specified GPU.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": "\nSets the random number generator state of all devices.\n\nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": "\nWrapper around a CUDA stream.\n\nA CUDA stream is a linear sequence of execution that belongs to a specific\ndevice, independent from other streams. See CUDA semantics for details.\n\nNote\n\nAlthough CUDA versions >= 11 support more than two levels of priorities, in\nPyTorch, we only support two levels of priorities.\n\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": "\nContext-manager that selects a given stream.\n\nAll CUDA kernels queued within its context will be enqueued on a selected\nstream.\n\nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s `None`.\n\nNote\n\nStreams are per-device. If the selected stream is not on the current device,\nthis function will also change the current device to match the stream.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": "\nChecks if all the work submitted has been completed.\n\nA boolean indicating if all kernels in this stream are completed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": "\nRecords an event.\n\nevent (Event, optional) \u2013 event to record. If not given, a new one will be\nallocated.\n\nRecorded event.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": "\nWait for all the kernels in this stream to complete.\n\nNote\n\nThis is a wrapper around `cudaStreamSynchronize()`: see CUDA Stream\ndocumentation for more info.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": "\nMakes all future work submitted to the stream wait for an event.\n\nevent (Event) \u2013 an event to wait for.\n\nNote\n\nThis is a wrapper around `cudaStreamWaitEvent()`: see CUDA Stream\ndocumentation for more info.\n\nThis function returns without waiting for `event`: only future operations are\naffected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": "\nSynchronizes with another stream.\n\nAll future work submitted to this stream will wait until all kernels submitted\nto a given stream at the time of call complete.\n\nstream (Stream) \u2013 a stream to synchronize.\n\nNote\n\nThis function returns without waiting for currently enqueued kernels in\n`stream`: only future operations are affected.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": "\nWaits for all kernels in all streams on a CUDA device to complete.\n\ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It\nuses the current device, given by `current_device()`, if `device` is `None`\n(default).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nmaximum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the cumulative\nminimum of elements of `input` in the dimension `dim`. And `indices` is the\nindex location of each maximum value found in the dimension `dim`.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": "\nReturns the cumulative product of elements of `input` in the dimension `dim`.\n\nFor example, if `input` is a vector of size N, the result will also be a\nvector of size N, with elements.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": "\nReturns the cumulative sum of elements of `input` in the dimension `dim`.\n\nFor example, if `input` is a vector of size N, the result will also be a\nvector of size N, with elements.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": "\nReturns a new tensor with each of the elements of `input` converted from\nangles in degrees to radians.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": "\nReturns an fp32 Tensor by dequantizing a quantized Tensor\n\ntensor (Tensor) \u2013 A quantized Tensor\n\nGiven a list of quantized Tensors, dequantize them and return a list of fp32\nTensors\n\ntensors (sequence of Tensors) \u2013 A list of quantized Tensors\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": "\nCalculates determinant of a square matrix or batches of square matrices.\n\nNote\n\n`torch.det()` is deprecated. Please use `torch.linalg.det()` instead.\n\nNote\n\nBackward through detdet internally uses SVD results when `input` is not\ninvertible. In this case, double backward through detdet will be unstable when\n`input` doesn\u2019t have distinct singular values. See torch.svd~torch.svd for\ndetails.\n\ninput (Tensor) \u2013 the input tensor of size `(*, n, n)` where `*` is zero or\nmore batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": "\nThe argument `diagonal` controls which diagonal to consider:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nSee also\n\n`torch.diagonal()` always returns the diagonal of its input.\n\n`torch.diagflat()` always constructs a tensor with diagonal elements specified\nby the input.\n\nExamples:\n\nGet the square matrix where the input vector is the diagonal:\n\nGet the k-th diagonal of a given matrix:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": "\nThe argument `offset` controls which diagonal to consider:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": "\nReturns a partial view of `input` with the its diagonal elements with respect\nto `dim1` and `dim2` appended as a dimension at the end of the shape.\n\nThe argument `offset` controls which diagonal to consider:\n\nApplying `torch.diag_embed()` to the output of this function with the same\narguments yields a diagonal matrix with the diagonal entries of the input.\nHowever, `torch.diag_embed()` has different default dimensions, so those need\nto be explicitly specified.\n\nNote\n\nTo take a batch diagonal, pass in dim1=-2, dim2=-1.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": "\nCreates a tensor whose diagonals of certain 2D planes (specified by `dim1` and\n`dim2`) are filled by `input`. To facilitate creating batched diagonal\nmatrices, the 2D planes formed by the last two dimensions of the returned\ntensor are chosen by default.\n\nThe argument `offset` controls which diagonal to consider:\n\nThe size of the new matrix will be calculated to make the specified diagonal\nof the size of the last input dimension. Note that for `offset` other than 00\n, the order of `dim1` and `dim2` matters. Exchanging them is equivalent to\nchanging the sign of `offset`.\n\nApplying `torch.diagonal()` to the output of this function with the same\narguments yields a matrix identical to input. However, `torch.diagonal()` has\ndifferent default dimensions, so those need to be explicitly specified.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": "\nComputes the n-th forward difference along the given dimension.\n\nThe first-order differences are given by `out[i] = input[i + 1] - input[i]`.\nHigher-order differences are calculated by using `torch.diff()` recursively.\n\nNote\n\nOnly `n = 1` is currently supported\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": "\nComputes the logarithmic derivative of the gamma function on `input`.\n\ninput (Tensor) \u2013 the tensor to compute the digamma function on\n\nout (Tensor, optional) \u2013 the output tensor.\n\nNote\n\nThis function is similar to SciPy\u2019s `scipy.special.digamma`.\n\nNote\n\nFrom PyTorch 1.8 onwards, the digamma function returns `-Inf` for `0`.\nPreviously it returned `NaN` for `0`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": "\nReturns the p-norm of (`input` \\- `other`)\n\nThe shapes of `input` and `other` must be broadcastable.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": "\nNote\n\nPlease refer to PyTorch Distributed Overview for a brief introduction to all\nfeatures related to distributed training.\n\n`torch.distributed` supports three built-in backends, each with different\ncapabilities. The table below shows which functions are available for use with\nCPU / CUDA tensors. MPI supports CUDA only if the implementation used to build\nPyTorch supports it.\n\nBackend\n\n`gloo`\n\n`mpi`\n\n`nccl`\n\nDevice\n\nCPU\n\nGPU\n\nCPU\n\nGPU\n\nCPU\n\nGPU\n\nsend\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nrecv\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nbroadcast\n\n\u2713\n\n\u2713\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nall_reduce\n\n\u2713\n\n\u2713\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nreduce\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nall_gather\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\ngather\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nscatter\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nreduce_scatter\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2718\n\n\u2713\n\nall_to_all\n\n\u2718\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2718\n\nbarrier\n\n\u2713\n\n\u2718\n\n\u2713\n\n?\n\n\u2718\n\n\u2713\n\nPyTorch distributed package supports Linux (stable), MacOS (stable), and\nWindows (prototype). By default for Linux, the Gloo and NCCL backends are\nbuilt and included in PyTorch distributed (NCCL only when building with CUDA).\nMPI is an optional backend that can only be included if you build PyTorch from\nsource. (e.g.building PyTorch on a host that has MPI installed.)\n\nNote\n\nAs of PyTorch v1.8, Windows supports all collective communications backend but\nNCCL, If the `init_method` argument of `init_process_group()` points to a file\nit must adhere to the following schema:\n\nSame as on Linux platform, you can enable TcpStore by setting environment\nvariables, MASTER_ADDR and MASTER_PORT.\n\nIn the past, we were often asked: \u201cwhich backend should I use?\u201d.\n\nRule of thumb\n\nGPU hosts with InfiniBand interconnect\n\nGPU hosts with Ethernet interconnect\n\nCPU hosts with InfiniBand interconnect\n\nCPU hosts with Ethernet interconnect\n\nBy default, both the NCCL and Gloo backends will try to find the right network\ninterface to use. If the automatically detected interface is not correct, you\ncan override it using the following environment variables (applicable to the\nrespective backend):\n\nIf you\u2019re using the Gloo backend, you can specify multiple interfaces by\nseparating them by a comma, like this: `export\nGLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3`. The backend will dispatch operations\nin a round-robin fashion across these interfaces. It is imperative that all\nprocesses specify the same number of interfaces in this variable.\n\nNCCL has also provided a number of environment variables for fine-tuning\npurposes.\n\nCommonly used ones include the following for debugging purposes:\n\nFor the full list of NCCL environment variables, please refer to NVIDIA NCCL\u2019s\nofficial documentation\n\nThe `torch.distributed` package provides PyTorch support and communication\nprimitives for multiprocess parallelism across several computation nodes\nrunning on one or more machines. The class\n`torch.nn.parallel.DistributedDataParallel()` builds on this functionality to\nprovide synchronous distributed training as a wrapper around any PyTorch\nmodel. This differs from the kinds of parallelism provided by Multiprocessing\npackage - torch.multiprocessing and `torch.nn.DataParallel()` in that it\nsupports multiple network-connected machines and in that the user must\nexplicitly launch a separate copy of the main training script for each\nprocess.\n\nIn the single-machine synchronous case, `torch.distributed` or the\n`torch.nn.parallel.DistributedDataParallel()` wrapper may still have\nadvantages over other approaches to data-parallelism, including\n`torch.nn.DataParallel()`:\n\nThe package needs to be initialized using the\n`torch.distributed.init_process_group()` function before calling any other\nmethods. This blocks until all processes have joined.\n\nReturns `True` if the distributed package is available. Otherwise,\n`torch.distributed` does not expose any other APIs. Currently,\n`torch.distributed` is available on Linux, MacOS and Windows. Set\n`USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently,\nthe default value is `USE_DISTRIBUTED=1` for Linux and Windows,\n`USE_DISTRIBUTED=0` for MacOS.\n\nInitializes the default distributed process group, and this will also\ninitialize the distributed package.\n\nIf neither is specified, `init_method` is assumed to be \u201cenv://\u201d.\n\nTo enable `backend == Backend.MPI`, PyTorch needs to be built from source on a\nsystem that supports MPI.\n\nAn enum-like class of available backends: GLOO, NCCL, MPI, and other\nregistered backends.\n\nThe values of this class are lowercase strings, e.g., `\"gloo\"`. They can be\naccessed as attributes, e.g., `Backend.NCCL`.\n\nThis class can be directly called to parse the string, e.g.,\n`Backend(backend_str)` will check if `backend_str` is valid, and return the\nparsed lowercase string if so. It also accepts uppercase strings, e.g.,\n`Backend(\"GLOO\")` returns `\"gloo\"`.\n\nNote\n\nThe entry `Backend.UNDEFINED` is present but only used as initial value of\nsome fields. Users should neither use it directly nor assume its existence.\n\nReturns the backend of the given process group.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is\nthe general main process group. If another specific group is specified, the\ncalling process must be part of `group`.\n\nThe backend of the given process group as a lower case string.\n\nReturns the rank of current process group\n\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 to\n`world_size`.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe rank of the process group -1, if not part of the group\n\nReturns the number of processes in the current process group\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe world size of the process group -1, if not part of the group\n\nChecking if the default process group has been initialized\n\nChecks if the MPI backend is available.\n\nChecks if the NCCL backend is available.\n\nCurrently three initialization methods are supported:\n\nThere are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired `world_size`. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified\nranks.\n\nNote that multicast address is not supported anymore in the latest distributed\npackage. `group_name` is deprecated as well.\n\nAnother initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired `world_size`. The\nURL should start with `file://` and contain a path to a non-existent file (in\nan existing directory) on a shared file system. File-system initialization\nwill automatically create that file if it doesn\u2019t exist, but will not delete\nthe file. Therefore, it is your responsibility to make sure that the file is\ncleaned up before the next `init_process_group()` call on the same file\npath/name.\n\nNote that automatic rank assignment is not supported anymore in the latest\ndistributed package and `group_name` is deprecated as well.\n\nWarning\n\nThis method assumes that the file system supports locking using `fcntl` \\-\nmost local systems and NFS support it.\n\nWarning\n\nThis method will always create the file and try its best to clean up and\nremove the file at the end of the program. In other words, each initialization\nwith the file init method will need a brand new empty file in order for the\ninitialization to succeed. If the same file used by the previous\ninitialization (which happens not to get cleaned up) is used again, this is\nunexpected behavior and can often cause deadlocks and failures. Therefore,\neven though this method will try its best to clean up the file, if the auto-\ndelete happens to be unsuccessful, it is your responsibility to ensure that\nthe file is removed at the end of the training to prevent the same file to be\nreused again during the next time. This is especially important if you plan to\ncall `init_process_group()` multiple times on the same file name. In other\nwords, if the file is not removed/cleaned up and you call\n`init_process_group()` again on that file, failures are expected. The rule of\nthumb here is that, make sure that the file is non-existent or empty every\ntime `init_process_group()` is called.\n\nThis method will read the configuration from environment variables, allowing\none to fully customize how the information is obtained. The variables to be\nset are:\n\nThe machine with rank 0 will be used to set up all connections.\n\nThis is the default method, meaning that `init_method` does not have to be\nspecified (or can be `env://`).\n\nThe distributed package comes with a distributed key-value store, which can be\nused to share information between processes in the group as well as to\ninitialize the distributed pacakge in `torch.distributed.init_process_group()`\n(by explicitly creating the store as an alternative to specifying\n`init_method`.) There are 3 choices for Key-Value Stores: `TCPStore`,\n`FileStore`, and `HashStore`.\n\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (`TCPStore`, `FileStore`, and `HashStore`).\n\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as `set()` to insert a key-value pair, `get()` to\nretrieve a key-value pair, etc.\n\nA thread-safe store implementation based on an underlying hashmap. This store\ncan be used within the same process (for example, by other threads), but\ncannot be used across processes.\n\nA store implementation that uses a file to store the underlying key-value\npairs.\n\nA wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and\n`HashStore`) that adds a prefix to each key inserted to the store.\n\nInserts the key-value pair into the store based on the supplied `key` and\n`value`. If `key` already exists in the store, it will overwrite the old value\nwith the new supplied `value`.\n\nRetrieves the value associated with the given `key` in the store. If `key` is\nnot present in the store, the function will wait for `timeout`, which is\ndefined when initializing the store, before throwing an exception.\n\nkey (str) \u2013 The function will return the value associated with this key.\n\nValue associated with `key` if `key` is in the store.\n\nThe first call to add for a given `key` creates a counter associated with\n`key` in the store, initialized to `amount`. Subsequent calls to add with the\nsame `key` increment the counter by the specified `amount`. Calling `add()`\nwith a key that has already been set in the store by `set()` will result in an\nexception.\n\nOverloaded function.\n\nWaits for each key in `keys` to be added to the store. If not all keys are set\nbefore the `timeout` (set during store initialization), then `wait` will throw\nan exception.\n\nkeys (list) \u2013 List of keys on which to wait until they are set in the store.\n\nWaits for each key in `keys` to be added to the store, and throws an exception\nif the keys have not been set by the supplied `timeout`.\n\nReturns the number of keys set in the store. Note that this number will\ntypically be one greater than the number of keys added by `set()` and `add()`\nsince one key is used to coordinate all the workers using the store.\n\nWarning\n\nWhen used with the `TCPStore`, `num_keys` returns the number of keys written\nto the underlying file. If the store is destructed and another store is\ncreated with the same file, the original keys will be retained.\n\nThe number of keys present in the store.\n\nDeletes the key-value pair associated with `key` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\nWarning\n\nThe `delete_key` API is only supported by the `TCPStore` and `HashStore`.\nUsing this API with the `FileStore` will result in an exception.\n\nkey (str) \u2013 The key to be deleted from the store\n\n`True` if `key` was deleted, otherwise `False`.\n\nSets the store\u2019s default timeout. This timeout is used during initialization\nand in `wait()` and `get()`.\n\ntimeout (timedelta) \u2013 timeout to be set in the store.\n\nBy default collectives operate on the default group (also called the world)\nand require all processes to enter the distributed function call. However,\nsome workloads can benefit from more fine-grained communication. This is where\ndistributed groups come into play. `new_group()` function can be used to\ncreate new groups, with arbitrary subsets of all processes. It returns an\nopaque group handle that can be given as a `group` argument to all collectives\n(collectives are distributed functions to exchange information in certain\nwell-known programming patterns).\n\nCreates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even if\nthey are not going to be members of the group. Additionally, groups should be\ncreated in the same order in all processes.\n\nWarning\n\nUsing multiple process groups with the `NCCL` backend concurrently is not safe\nand the user should perform explicit synchronization in their application to\nensure only one process group is used at a time. This means collectives from\none process group should have completed execution on the device (not just\nenqueued since CUDA execution is async) before collectives from another\nprocess group are enqueued. See Using multiple NCCL communicators concurrently\nfor more details.\n\nA handle of distributed group that can be given to collective calls.\n\nSends a tensor synchronously.\n\nReceives a tensor synchronously.\n\nSender rank -1, if not part of the group\n\n`isend()` and `irecv()` return distributed request objects when used. In\ngeneral, the type of this object is unspecified as they should never be\ncreated manually, but they are guaranteed to support two methods:\n\nSends a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\nReceives a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\nEvery collective operation function supports the following two kinds of\noperations, depending on the setting of the `async_op` flag passed into the\ncollective:\n\nSynchronous operation \\- the default mode, when `async_op` is set to `False`.\nWhen the function returns, it is guaranteed that the collective operation is\nperformed. In the case of CUDA operations, it is not guaranteed that the CUDA\noperation is completed, since CUDA operations are asynchronous. For CPU\ncollectives, any further function calls utilizing the output of the collective\ncall will behave as expected. For CUDA collectives, function calls utilizing\nthe output on the same CUDA stream will behave as expected. Users must take\ncare of synchronization under the scenario of running under different streams.\nFor details on CUDA semantics such as stream synchronization, see CUDA\nSemantics. See the below script to see examples of differences in these\nsemantics for CPU and CUDA operations.\n\nAsynchronous operation \\- when `async_op` is set to True. The collective\noperation function returns a distributed request object. In general, you don\u2019t\nneed to create it manually and it is guaranteed to support two methods:\n\nExample\n\nThe following code can serve as a reference regarding semantics for CUDA\noperations when using distributed collectives. It shows the explicit need to\nsynchronize when using collective outputs on different CUDA streams:\n\nBroadcasts the tensor to the whole group.\n\n`tensor` must have the same number of elements in all processes participating\nin the collective.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nBroadcasts picklable objects in `object_list` to the whole group. Similar to\n`broadcast()`, but Python objects can be passed in. Note that all objects in\n`object_list` must be picklable in order to be broadcasted.\n\n`None`. If rank is part of the group, `object_list` will contain the\nbroadcasted objects from `src` rank.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`broadcast_object_list()` uses `pickle` module implicitly, which is known to\nbe insecure. It is possible to construct malicious pickle data which will\nexecute arbitrary code during unpickling. Only call this function with data\nyou trust.\n\nReduces the tensor data across all machines in such a way that all get the\nfinal result.\n\nAfter the call `tensor` is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data across all machines.\n\nOnly the process with rank `dst` is going to receive the final result.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers tensors from the whole group in a list.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers picklable objects from the whole group into a list. Similar to\n`all_gather()`, but Python objects can be passed in. Note that the object must\nbe picklable in order to be gathered.\n\nNone. If the calling rank is part of this group, the output of the collective\nwill be populated into the input `object_list`. If the calling rank is not\npart of the group, the passed in `object_list` will be unmodified.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nWarning\n\n`all_gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nGathers a list of tensors in a single process.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nGathers picklable objects from the whole group in a single process. Similar to\n`gather()`, but Python objects can be passed in. Note that the object must be\npicklable in order to be gathered.\n\nNone. On the `dst` rank, `object_gather_list` will contain the output of the\ncollective.\n\nNote\n\nNote that this API differs slightly from the gather collective since it does\nnot provide an async_op handle and thus will be a blocking call.\n\nNote\n\nNote that this API is not supported when using the NCCL backend.\n\nWarning\n\n`gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nScatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the\n`tensor` argument.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nScatters picklable objects in `scatter_object_input_list` to the whole group.\nSimilar to `scatter()`, but Python objects can be passed in. On each rank, the\nscattered object will be stored as the first element of\n`scatter_object_output_list`. Note that all objects in\n`scatter_object_input_list` must be picklable in order to be scattered.\n\n`None`. If rank is part of the group, `scatter_object_output_list` will have\nits first element set to the scattered object for this rank.\n\nNote\n\nNote that this API differs slightly from the scatter collective since it does\nnot provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`scatter_object_list()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\nReduces, then scatters a list of tensors to all processes in a group.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nEach process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nWarning\n\n`all_to_all` is experimental and subject to change.\n\nSynchronizes all processes.\n\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nAn enum-like class for available reduction operations: `SUM`, `PRODUCT`,\n`MIN`, `MAX`, `BAND`, `BOR`, and `BXOR`.\n\nNote that `BAND`, `BOR`, and `BXOR` reductions are not available when using\nthe `NCCL` backend.\n\nAdditionally, `MAX`, `MIN` and `PRODUCT` are not supported for complex\ntensors.\n\nThe values of this class can be accessed as attributes, e.g., `ReduceOp.SUM`.\nThey are used in specifying strategies for reduction collectives, e.g.,\n`reduce()`, `all_reduce_multigpu()`, etc.\n\nMembers:\n\nSUM\n\nPRODUCT\n\nMIN\n\nMAX\n\nBAND\n\nBOR\n\nBXOR\n\nDeprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`,\nand `MAX`.\n\n`ReduceOp` is recommended to use instead.\n\nIf you want to use collective communication functions supporting autograd you\ncan find an implementation of those in the `torch.distributed.nn.*` module.\n\nFunctions here are synchronous and will be inserted in the autograd graph, so\nyou need to ensure that all the processes that participated in the collective\noperation will do the backward pass for the backward communication to\neffectively happen and don\u2019t cause a deadlock.\n\nPlease notice that currently the only backend where all the functions are\nguaranteed to work is `gloo`. .. autofunction:: torch.distributed.nn.broadcast\n.. autofunction:: torch.distributed.nn.gather .. autofunction::\ntorch.distributed.nn.scatter .. autofunction:: torch.distributed.nn.reduce ..\nautofunction:: torch.distributed.nn.all_gather .. autofunction::\ntorch.distributed.nn.all_to_all .. autofunction::\ntorch.distributed.nn.all_reduce\n\nIf you have more than one GPU on each node, when using the NCCL and Gloo\nbackend, `broadcast_multigpu()` `all_reduce_multigpu()` `reduce_multigpu()`\n`all_gather_multigpu()` and `reduce_scatter_multigpu()` support distributed\ncollective operations among multiple GPUs within each node. These functions\ncan potentially improve the overall distributed training performance and be\neasily used by passing a list of tensors. Each Tensor in the passed tensor\nlist needs to be on a separate GPU device of the host where the function is\ncalled. Note that the length of the tensor list needs to be identical among\nall the distributed processes. Also note that currently the multi-GPU\ncollective functions are only supported by the NCCL backend.\n\nFor example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:\n\nCode running on Node 0\n\nCode running on Node 1\n\nAfter the call, all 16 tensors on the two nodes will have the all-reduced\nvalue of 16\n\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\n`tensor` must have the same number of elements in all the GPUs from all\nprocesses participating in the collective. each tensor in the list must be on\na different GPU\n\nOnly nccl and gloo backend are currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data across all machines in such a way that all get the\nfinal result. This function reduces a number of tensors on every node, while\neach tensor resides on different GPUs. Therefore, the input tensor in the\ntensor list needs to be GPU tensors. Also, each tensor in the tensor list\nneeds to reside on a different GPU.\n\nAfter the call, all `tensor` in `tensor_list` is going to be bitwise identical\nin all processes.\n\nComplex tensors are supported.\n\nOnly nccl and gloo backend is currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduces the tensor data on multiple GPUs across all machines. Each tensor in\n`tensor_list` should reside on a separate GPU\n\nOnly the GPU of `tensor_list[dst_tensor]` on the process with rank `dst` is\ngoing to receive the final result.\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nAsync work handle, if async_op is set to True. None, otherwise\n\nGathers tensors from the whole group in a list. Each tensor in `tensor_list`\nshould reside on a separate GPU\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nComplex tensors are supported.\n\noutput_tensor_lists (List[List[Tensor]]) \u2013\n\nOutput lists. It should contain correctly-sized tensors on each GPU to be used\nfor output of the collective, e.g. `output_tensor_lists[i]` contains the\nall_gather result that resides on the GPU of `input_tensor_list[i]`.\n\nNote that each element of `output_tensor_lists` has the size of `world_size *\nlen(input_tensor_list)`, since the function all gathers the result from every\nsingle GPU in the group. To interpret each element of\n`output_tensor_lists[i]`, note that `input_tensor_list[j]` of rank k will be\nappear in `output_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(output_tensor_lists)`, and the size of each element in\n`output_tensor_lists` (each element is a list, therefore\n`len(output_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\nReduce and scatter a list of tensors to the whole group. Only nccl backend is\ncurrently supported.\n\nEach tensor in `output_tensor_list` should reside on a separate GPU, as should\neach list of tensors in `input_tensor_lists`.\n\noutput_tensor_list (List[Tensor]) \u2013\n\nOutput tensors (on different GPUs) to receive the result of the operation.\n\nNote that `len(output_tensor_list)` needs to be the same for all the\ndistributed processes calling this function.\n\ninput_tensor_lists (List[List[Tensor]]) \u2013\n\nInput lists. It should contain correctly-sized tensors on each GPU to be used\nfor input of the collective, e.g. `input_tensor_lists[i]` contains the\nreduce_scatter input that resides on the GPU of `output_tensor_list[i]`.\n\nNote that each element of `input_tensor_lists` has the size of `world_size *\nlen(output_tensor_list)`, since the function scatters the result from every\nsingle GPU in the group. To interpret each element of `input_tensor_lists[i]`,\nnote that `output_tensor_list[j]` of rank k receives the reduce-scattered\nresult from `input_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(input_tensor_lists)`, and the size of each element in\n`input_tensor_lists` (each element is a list, therefore\n`len(input_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nBesides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party\nbackends through a run-time register mechanism. For references on how to\ndevelop a third-party backend through C++ Extension, please refer to Tutorials\n- Custom C++ and CUDA Extensions and\n`test/cpp_extensions/cpp_c10d_extension.cpp`. The capability of third-party\nbackends are decided by their own implementations.\n\nThe new backend derives from `c10d.ProcessGroup` and registers the backend\nname and the instantiating interface through\n`torch.distributed.Backend.register_backend()` when imported.\n\nWhen manually importing this backend and invoking\n`torch.distributed.init_process_group()` with the corresponding backend name,\nthe `torch.distributed` package runs on the new backend.\n\nWarning\n\nThe support of third-party backend is experimental and subject to change.\n\nThe `torch.distributed` package also provides a launch utility in\n`torch.distributed.launch`. This helper utility can be used to launch multiple\nprocesses per node for distributed training.\n\n`torch.distributed.launch` is a module that spawns up multiple distributed\ntraining processes on each of the training nodes.\n\nThe utility can be used for single-node distributed training, in which one or\nmore processes per node will be spawned. The utility can be used for either\nCPU training or GPU training. If the utility is used for GPU training, each\ndistributed process will be operating on a single GPU. This can achieve well-\nimproved single-node training performance. It can also be used in multi-node\ndistributed training, by spawning up multiple processes on each node for well-\nimproved multi-node distributed training performance as well. This will\nespecially be benefitial for systems with multiple Infiniband interfaces that\nhave direct-GPU support, since all of them can be utilized for aggregated\ncommunication bandwidth.\n\nIn both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(`--nproc_per_node`). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (`nproc_per_node`), and\neach process will be operating on a single GPU from GPU 0 to GPU\n(nproc_per_node - 1).\n\nHow to use this module:\n\nNode 1: (IP: 192.168.1.1, and has a free port: 1234)\n\nNode 2:\n\nImportant Notices:\n\n1\\. This utility and multi-process distributed (single-node or multi-node) GPU\ntraining currently only achieves the best performance using the NCCL\ndistributed backend. Thus NCCL backend is the recommended backend to use for\nGPU training.\n\n2\\. In your training program, you must parse the command-line argument:\n`--local_rank=LOCAL_PROCESS_RANK`, which will be provided by this module. If\nyour training program uses GPUs, you should ensure that your code only runs on\nthe GPU device of LOCAL_PROCESS_RANK. This can be done by:\n\nParsing the local_rank argument\n\nSet your device to local rank using either\n\nor\n\n3\\. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses `env://`, which is the only supported `init_method` by\nthis module.\n\n4\\. In your training program, you can either use regular distributed functions\nor use `torch.nn.parallel.DistributedDataParallel()` module. If your training\nprogram uses GPUs for training and you would like to use\n`torch.nn.parallel.DistributedDataParallel()` module, here is how to configure\nit.\n\nPlease ensure that `device_ids` argument is set to be the only GPU device id\nthat your code will be operating on. This is generally the local rank of the\nprocess. In other words, the `device_ids` needs to be `[args.local_rank]`, and\n`output_device` needs to be `args.local_rank` in order to use this utility\n\n5\\. Another way to pass `local_rank` to the subprocesses via environment\nvariable `LOCAL_RANK`. This behavior is enabled when you launch the script\nwith `--use_env=True`. You must adjust the subprocess example above to replace\n`args.local_rank` with `os.environ['LOCAL_RANK']`; the launcher will not pass\n`--local_rank` when you specify this flag.\n\nWarning\n\n`local_rank` is NOT globally unique: it is only unique per process on a\nmachine. Thus, don\u2019t use it to decide if you should, e.g., write to a\nnetworked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for\nan example of how things can go wrong if you don\u2019t do this correctly.\n\nThe Multiprocessing package - torch.multiprocessing package also provides a\n`spawn` function in `torch.multiprocessing.spawn()`. This helper function can\nbe used to spawn multiple processes. It works by passing in the function that\nyou want to run and spawns N processes to run it. This can be used for\nmultiprocess distributed training as well.\n\nFor references on how to use it, please refer to PyTorch example - ImageNet\nimplementation\n\nNote that this function requires Python 3.4 or higher.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook just calls `allreduce` using `GradBucket` tensors.\nOnce gradient tensors are aggregated across all workers, its `then` callback\ntakes the mean and returns the result. If user registers this hook, DDP\nresults is expected to be same as the case where no hook was registered.\nHence, this won\u2019t change behavior of DDP and user can use this as a reference\nor modify this hook to log useful information or any other purposes while\nunaffecting DDP behavior.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simple gradient compression approach\nthat converts `GradBucket` tensors whose type is assumed to be `torch.float32`\nto half-precision floating point format (`torch.float16`). It allreduces those\n`float16` gradient tensors. Once compressed gradient tensors are allreduced,\nits then callback called `decompress` converts the aggregated result back to\n`float32` and takes the mean.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements a simplified PowerSGD gradient\ncompression algorithm described in the paper. This variant does not compress\nthe gradients layer by layer, but instead compresses the flattened input\ntensor that batches all the gradients. Therefore, it is faster than\n`powerSGD_hook()`, but usually results in a much lower accuracy, unless\n`matrix_approximation_rank` is 1.\n\nWarning\n\nIncreasing `matrix_approximation_rank` here may not necessarily increase the\naccuracy, because batching per-parameter tensors without column/row alignment\ncan destroy low-rank structure. Therefore, the user should always consider\n`powerSGD_hook()` first, and only consider this variant when a satisfactory\naccuracy can be achieved when `matrix_approximation_rank` is 1.\n\nOnce gradient tensors are aggregated across all workers, this hook applies\ncompression as follows:\n\nNote that this communication hook enforces vanilla allreduce for the first\n`state.start_powerSGD_iter` iterations. This not only gives the user more\ncontrol over the tradeoff between speedup and accuracy, but also helps\nabstract away some complexity of the internal optimization of DDP for future\ncommunication hook developers.\n\nFuture handler of the communication, which updates the gradients in place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": "\nStores both the algorithm\u2019s hyperparameters and the internal state for all the\ngradients during the training. Particularly, `matrix_approximation_rank` and\n`start_powerSGD_iter` are the main hyperparameters that should be tuned by the\nuser. For performance, we suggest to keep binary hyperparameters\n`use_error_feedback` and `warm_start` on.\n\n`matrix_approximation_rank` controls the size of compressed low-rank tensors,\nwhich determines the compression rate. The lower the rank, the stronger the\ncompression.\n\n1.1. If `matrix_approximation_rank` is too low, the full model quality will\nneed more training steps to reach or will never reach and yield loss in\naccuracy.\n\n1.2. The increase of `matrix_approximation_rank` can substantially increase\nthe computation costs of the compression, and the accuracy may not be futher\nimproved beyond a certain `matrix_approximation_rank` threshold.\n\nTo tune `matrix_approximation_rank`, we suggest to start from 1 and increase\nby factors of 2 (like an expoential grid search, 1, 2, 4, \u2026), until a\nsatisfactory accuracy is reached. Typically only a small value 1-4 is used.\nFor some NLP tasks (as shown in Appendix D of the original paper), this value\nhas been increased to 32.\n\nTo tune `start_powerSGD_iter`, we suggest to start with 10% of total training\nsteps, and increase it until a satisfactory accuracy is reached.\n\nWarning\n\nIf error feedback or warm-up is enabled, the minimum value of\n`start_powerSGD_iter` allowed in DDP is 2. This is because there is another\ninternal optimization that rebuilds buckets at iteration 1 in DDP, and this\ncan conflict with any tensor memorized before the rebuild process.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": "\nThis DDP communication hook implements PowerSGD gradient compression algorithm\ndescribed in the paper. Once gradient tensors are aggregated across all\nworkers, this hook applies compression as follows:\n\nHandles rank-1 tensors by allreducing them without compression:\n\n2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all\nthe rank-1 tensors as a batch, without compression;\n\n2.2. Copies the individual rank-1 tensors from the contiguous memory back to\nthe input tensor.\n\nHandles high-rank tensors by PowerSGD compression:\n\n3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for\ndecomposing M, such that M = PQ^T, where Q is initialized from a standard\nnormal distribution and orthogonalized;\n\n3.2. Computes each P in Ps, which is equal to MQ;\n\n3.3. Allreduces Ps as a batch;\n\n3.4. Orthogonalizes each P in Ps;\n\n3.5. Computes each Q in Qs, which is approximately equal to M^TP;\n\n3.6. Allreduces Qs as a batch;\n\n3.7. Computes each M among all the high-rank tensors, which is approximately\nequal to PQ^T.\n\nNote that this communication hook enforces vanilla allreduce for the first\n`state.start_powerSGD_iter` iterations. This not only gives the user more\ncontrol over the tradeoff between speedup and accuracy, but also helps\nabstract away some complexity of the internal optimization of DDP for future\ncommunication hook developers.\n\nFuture handler of the communication, which updates the gradients in place.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": "\nGathers tensors from the whole group in a list. Each tensor in `tensor_list`\nshould reside on a separate GPU\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nComplex tensors are supported.\n\noutput_tensor_lists (List[List[Tensor]]) \u2013\n\nOutput lists. It should contain correctly-sized tensors on each GPU to be used\nfor output of the collective, e.g. `output_tensor_lists[i]` contains the\nall_gather result that resides on the GPU of `input_tensor_list[i]`.\n\nNote that each element of `output_tensor_lists` has the size of `world_size *\nlen(input_tensor_list)`, since the function all gathers the result from every\nsingle GPU in the group. To interpret each element of\n`output_tensor_lists[i]`, note that `input_tensor_list[j]` of rank k will be\nappear in `output_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(output_tensor_lists)`, and the size of each element in\n`output_tensor_lists` (each element is a list, therefore\n`len(output_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group into a list. Similar to\n`all_gather()`, but Python objects can be passed in. Note that the object must\nbe picklable in order to be gathered.\n\nNone. If the calling rank is part of this group, the output of the collective\nwill be populated into the input `object_list`. If the calling rank is not\npart of the group, the passed in `object_list` will be unmodified.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nWarning\n\n`all_gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result.\n\nAfter the call `tensor` is going to be bitwise identical in all processes.\n\nComplex tensors are supported.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines in such a way that all get the\nfinal result. This function reduces a number of tensors on every node, while\neach tensor resides on different GPUs. Therefore, the input tensor in the\ntensor list needs to be GPU tensors. Also, each tensor in the tensor list\nneeds to reside on a different GPU.\n\nAfter the call, all `tensor` in `tensor_list` is going to be bitwise identical\nin all processes.\n\nComplex tensors are supported.\n\nOnly nccl and gloo backend is currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": "\nEach process scatters list of input tensors to all processes in a group and\nreturn gathered list of tensors in output list.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\nWarning\n\n`all_to_all` is experimental and subject to change.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": "\nKicks off the distributed backward pass using the provided roots. This\ncurrently implements the FAST mode algorithm which assumes all RPC messages\nsent in the same distributed autograd context across workers would be part of\nthe autograd graph during the backward pass.\n\nWe use the provided roots to discover the autograd graph and compute\nappropriate dependencies. This method blocks until the entire autograd\ncomputation is done.\n\nWe accumulate the gradients in the appropriate\n`torch.distributed.autograd.context` on each of the nodes. The autograd\ncontext to be used is looked up given the `context_id` that is passed in when\n`torch.distributed.autograd.backward()` is called. If there is no valid\nautograd context corresponding to the given ID, we throw an error. You can\nretrieve the accumulated gradients using the `get_gradients()` API.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": "\nContext object to wrap forward and backward passes when using distributed\nautograd. The `context_id` generated in the `with` statement is required to\nuniquely identify a distributed backward pass on all workers. Each worker\nstores metadata associated with this `context_id`, which is required to\ncorrectly execute a distributed autograd pass.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": "\nRetrieves a map from Tensor to the appropriate gradient for that Tensor\naccumulated in the provided context corresponding to the given `context_id` as\npart of the distributed autograd backward pass.\n\ncontext_id (int) \u2013 The autograd context id for which we should retrieve the\ngradients.\n\nA map where the key is the Tensor and the value is the associated gradient for\nthat Tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": "\nAn enum-like class of available backends: GLOO, NCCL, MPI, and other\nregistered backends.\n\nThe values of this class are lowercase strings, e.g., `\"gloo\"`. They can be\naccessed as attributes, e.g., `Backend.NCCL`.\n\nThis class can be directly called to parse the string, e.g.,\n`Backend(backend_str)` will check if `backend_str` is valid, and return the\nparsed lowercase string if so. It also accepts uppercase strings, e.g.,\n`Backend(\"GLOO\")` returns `\"gloo\"`.\n\nNote\n\nThe entry `Backend.UNDEFINED` is present but only used as initial value of\nsome fields. Users should neither use it directly nor assume its existence.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": "\nSynchronizes all processes.\n\nThis collective blocks processes until the whole group enters this function,\nif async_op is False, or if async work handle is called on wait().\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group.\n\n`tensor` must have the same number of elements in all processes participating\nin the collective.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": "\nBroadcasts the tensor to the whole group with multiple GPU tensors per node.\n\n`tensor` must have the same number of elements in all the GPUs from all\nprocesses participating in the collective. each tensor in the list must be on\na different GPU\n\nOnly nccl and gloo backend are currently supported tensors should only be GPU\ntensors\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": "\nBroadcasts picklable objects in `object_list` to the whole group. Similar to\n`broadcast()`, but Python objects can be passed in. Note that all objects in\n`object_list` must be picklable in order to be broadcasted.\n\n`None`. If rank is part of the group, `object_list` will contain the\nbroadcasted objects from `src` rank.\n\nNote\n\nFor NCCL-based processed groups, internal tensor representations of objects\nmust be moved to the GPU device before communication takes place. In this\ncase, the device used is given by `torch.cuda.current_device()` and it is the\nuser\u2019s responsiblity to ensure that this is set so that each rank has an\nindividual GPU, via `torch.cuda.set_device()`.\n\nNote\n\nNote that this API differs slightly from the `all_gather()` collective since\nit does not provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`broadcast_object_list()` uses `pickle` module implicitly, which is known to\nbe insecure. It is possible to construct malicious pickle data which will\nexecute arbitrary code during unpickling. Only call this function with data\nyou trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": "\nA store implementation that uses a file to store the underlying key-value\npairs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": "\nGathers a list of tensors in a single process.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": "\nGathers picklable objects from the whole group in a single process. Similar to\n`gather()`, but Python objects can be passed in. Note that the object must be\npicklable in order to be gathered.\n\nNone. On the `dst` rank, `object_gather_list` will contain the output of the\ncollective.\n\nNote\n\nNote that this API differs slightly from the gather collective since it does\nnot provide an async_op handle and thus will be a blocking call.\n\nNote\n\nNote that this API is not supported when using the NCCL backend.\n\nWarning\n\n`gather_object()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": "\nReturns the backend of the given process group.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is\nthe general main process group. If another specific group is specified, the\ncalling process must be part of `group`.\n\nThe backend of the given process group as a lower case string.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": "\nReturns the rank of current process group\n\nRank is a unique identifier assigned to each process within a distributed\nprocess group. They are always consecutive integers ranging from 0 to\n`world_size`.\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe rank of the process group -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": "\nReturns the number of processes in the current process group\n\ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the\ndefault process group will be used.\n\nThe world size of the process group -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": "\nA thread-safe store implementation based on an underlying hashmap. This store\ncan be used within the same process (for example, by other threads), but\ncannot be used across processes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": "\nInitializes the default distributed process group, and this will also\ninitialize the distributed package.\n\nIf neither is specified, `init_method` is assumed to be \u201cenv://\u201d.\n\nTo enable `backend == Backend.MPI`, PyTorch needs to be built from source on a\nsystem that supports MPI.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": "\nReceives a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": "\nSends a tensor asynchronously.\n\nA distributed request object. None, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": "\nReturns `True` if the distributed package is available. Otherwise,\n`torch.distributed` does not expose any other APIs. Currently,\n`torch.distributed` is available on Linux, MacOS and Windows. Set\n`USE_DISTRIBUTED=1` to enable it when building PyTorch from source. Currently,\nthe default value is `USE_DISTRIBUTED=1` for Linux and Windows,\n`USE_DISTRIBUTED=0` for MacOS.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": "\nChecking if the default process group has been initialized\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": "\nChecks if the MPI backend is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": "\nChecks if the NCCL backend is available.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": "\nCreates a new distributed group.\n\nThis function requires that all processes in the main group (i.e. all\nprocesses that are part of the distributed job) enter this function, even if\nthey are not going to be members of the group. Additionally, groups should be\ncreated in the same order in all processes.\n\nWarning\n\nUsing multiple process groups with the `NCCL` backend concurrently is not safe\nand the user should perform explicit synchronization in their application to\nensure only one process group is used at a time. This means collectives from\none process group should have completed execution on the device (not just\nenqueued since CUDA execution is async) before collectives from another\nprocess group are enqueued. See Using multiple NCCL communicators concurrently\nfor more details.\n\nA handle of distributed group that can be given to collective calls.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": "\nDistributedOptimizer takes remote references to parameters scattered across\nworkers and applies the given optimizer locally for each parameter.\n\nThis class uses `get_gradients()` in order to retrieve the gradients for\nspecific parameters.\n\nConcurrent calls to `step()`, either from the same or different clients, will\nbe serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one\nset of gradients at a time. However, there is no guarantee that the full\nforward-backward-optimizer sequence will execute for one client at a time.\nThis means that the gradients being applied may not correspond to the latest\nforward pass executed on a given worker. Also, there is no guaranteed ordering\nacross workers.\n\n`DistributedOptimizer` creates the local optimizer with TorchScript enabled by\ndefault, so that optimizer updates are not blocked by the Python Global\nInterpreter Lock (GIL) during multithreaded training (e.g. Distributed Model\nParallel). This feature is currently in beta stage, enabled for optimizers\nincluding `Adagrad`, `Adam`, `SGD`, `RMSprop`, `AdamW` and `Adadelta`. We are\nincreasing the coverage to all optimizers in future releases.\n\nPerforms a single optimization step.\n\nThis will call `torch.optim.Optimizer.step()` on each worker containing\nparameters to be optimized, and will block until all workers return. The\nprovided `context_id` will be used to retrieve the corresponding `context`\nthat contains the gradients that should be applied to the parameters.\n\ncontext_id \u2013 the autograd context id for which we should run the optimizer\nstep.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": "\nPerforms a single optimization step.\n\nThis will call `torch.optim.Optimizer.step()` on each worker containing\nparameters to be optimized, and will block until all workers return. The\nprovided `context_id` will be used to retrieve the corresponding `context`\nthat contains the gradients that should be applied to the parameters.\n\ncontext_id \u2013 the autograd context id for which we should run the optimizer\nstep.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": "\nWraps an arbitrary `nn.Sequential` module to train on using synchronous\npipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on\na single GPU, pipeline parallelism is a useful technique to employ for\ntraining.\n\nThe implementation is based on the torchgpipe paper.\n\nPipe combines pipeline parallelism with checkpointing to reduce peak memory\nrequired to train while minimizing device under-utilization.\n\nYou should place all the modules on the appropriate devices and wrap them into\nan `nn.Sequential` module defining the desired order of execution.\n\nPipeline of two FC layers across GPUs 0 and 1.\n\nNote\n\nYou can wrap a `Pipe` model with `torch.nn.parallel.DistributedDataParallel`\nonly when the checkpoint parameter of `Pipe` is `'never'`.\n\nNote\n\n`Pipe` only supports intra-node pipelining currently, but will be expanded to\nsupport inter-node pipelining in the future. The forward function returns an\n`RRef` to allow for inter-node pipelining in the future, where the output\nmight be on a remote host. For intra-node pipelinining you can use\n`local_value()` to retrieve the output locally.\n\nWarning\n\n`Pipe` is experimental and subject to change.\n\nProcesses a single input mini-batch through the pipe and returns an `RRef`\npointing to the output. `Pipe` is a fairly transparent module wrapper. It\ndoesn\u2019t modify the input and output signature of the underlying module. But\nthere\u2019s type restriction. Input and output have to be a `Tensor` or a sequence\nof tensors. This restriction is applied at partition boundaries too.\n\nThe input tensor is split into multiple micro-batches based on the `chunks`\nparameter used to initialize `Pipe`. The batch size is assumed to be the first\ndimension of the tensor and if the batch size is less than `chunks`, the\nnumber of micro-batches is equal to the batch size.\n\ninput (torch.Tensor or sequence of `Tensor`) \u2013 input mini-batch\n\n`RRef` to the output of the mini-batch\n\nTypeError \u2013 input is not a tensor or sequence of tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": "\nProcesses a single input mini-batch through the pipe and returns an `RRef`\npointing to the output. `Pipe` is a fairly transparent module wrapper. It\ndoesn\u2019t modify the input and output signature of the underlying module. But\nthere\u2019s type restriction. Input and output have to be a `Tensor` or a sequence\nof tensors. This restriction is applied at partition boundaries too.\n\nThe input tensor is split into multiple micro-batches based on the `chunks`\nparameter used to initialize `Pipe`. The batch size is assumed to be the first\ndimension of the tensor and if the batch size is less than `chunks`, the\nnumber of micro-batches is equal to the batch size.\n\ninput (torch.Tensor or sequence of `Tensor`) \u2013 input mini-batch\n\n`RRef` to the output of the mini-batch\n\nTypeError \u2013 input is not a tensor or sequence of tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": "\nThe command to pop a skip tensor.\n\nname (str) \u2013 name of skip tensor\n\nthe skip tensor previously stashed by another layer under the same name\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": "\nThe decorator to define a `nn.Module` with skip connections. Decorated modules\nare called \u201cskippable\u201d. This functionality works perfectly fine even when the\nmodule is not wrapped by `Pipe`.\n\nEach skip tensor is managed by its name. Before manipulating skip tensors, a\nskippable module must statically declare the names for skip tensors by `stash`\nand/or `pop` parameters. Skip tensors with pre-declared name can be stashed by\n`yield stash(name, tensor)` or popped by `tensor = yield pop(name)`.\n\nHere is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed\nand popped at the first and last layer, respectively:\n\nOne skippable module can stash or pop multiple skip tensors:\n\nEvery skip tensor must be associated with exactly one pair of `stash` and\n`pop`. `Pipe` checks this restriction automatically when wrapping a module.\nYou can also check the restriction by `verify_skippables()` without `Pipe`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": "\nThe command to stash a skip tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": "\nVerifies if the underlying skippable modules satisfy integrity.\n\nEvery skip tensor must have only one pair of `stash` and `pop`. If there are\none or more unmatched pairs, it will raise `TypeError` with the detailed\nmessages.\n\nHere are a few failure cases. `verify_skippables()` will report failure for\nthese cases:\n\nTo use the same name for multiple skip tensors, they must be isolated by\ndifferent namespaces. See `isolate()`.\n\nTypeError \u2013 one or more pairs of `stash` and `pop` are not matched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": "\nA wrapper around any of the 3 key-value stores (`TCPStore`, `FileStore`, and\n`HashStore`) that adds a prefix to each key inserted to the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": "\nReceives a tensor synchronously.\n\nSender rank -1, if not part of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": "\nReduces the tensor data across all machines.\n\nOnly the process with rank `dst` is going to receive the final result.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": "\nAn enum-like class for available reduction operations: `SUM`, `PRODUCT`,\n`MIN`, `MAX`, `BAND`, `BOR`, and `BXOR`.\n\nNote that `BAND`, `BOR`, and `BXOR` reductions are not available when using\nthe `NCCL` backend.\n\nAdditionally, `MAX`, `MIN` and `PRODUCT` are not supported for complex\ntensors.\n\nThe values of this class can be accessed as attributes, e.g., `ReduceOp.SUM`.\nThey are used in specifying strategies for reduction collectives, e.g.,\n`reduce()`, `all_reduce_multigpu()`, etc.\n\nMembers:\n\nSUM\n\nPRODUCT\n\nMIN\n\nMAX\n\nBAND\n\nBOR\n\nBXOR\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": "\nReduces the tensor data on multiple GPUs across all machines. Each tensor in\n`tensor_list` should reside on a separate GPU\n\nOnly the GPU of `tensor_list[dst_tensor]` on the process with rank `dst` is\ngoing to receive the final result.\n\nOnly nccl backend is currently supported tensors should only be GPU tensors\n\nAsync work handle, if async_op is set to True. None, otherwise\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": "\nDeprecated enum-like class for reduction operations: `SUM`, `PRODUCT`, `MIN`,\nand `MAX`.\n\n`ReduceOp` is recommended to use instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": "\nReduces, then scatters a list of tensors to all processes in a group.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": "\nReduce and scatter a list of tensors to the whole group. Only nccl backend is\ncurrently supported.\n\nEach tensor in `output_tensor_list` should reside on a separate GPU, as should\neach list of tensors in `input_tensor_lists`.\n\noutput_tensor_list (List[Tensor]) \u2013\n\nOutput tensors (on different GPUs) to receive the result of the operation.\n\nNote that `len(output_tensor_list)` needs to be the same for all the\ndistributed processes calling this function.\n\ninput_tensor_lists (List[List[Tensor]]) \u2013\n\nInput lists. It should contain correctly-sized tensors on each GPU to be used\nfor input of the collective, e.g. `input_tensor_lists[i]` contains the\nreduce_scatter input that resides on the GPU of `output_tensor_list[i]`.\n\nNote that each element of `input_tensor_lists` has the size of `world_size *\nlen(output_tensor_list)`, since the function scatters the result from every\nsingle GPU in the group. To interpret each element of `input_tensor_lists[i]`,\nnote that `output_tensor_list[j]` of rank k receives the reduce-scattered\nresult from `input_tensor_lists[i][k * world_size + j]`\n\nAlso note that `len(input_tensor_lists)`, and the size of each element in\n`input_tensor_lists` (each element is a list, therefore\n`len(input_tensor_lists[i])`) need to be the same for all the distributed\nprocesses calling this function.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": "\nAn enum class of available backends.\n\nPyTorch ships with two builtin backends: `BackendType.TENSORPIPE` and\n`BackendType.PROCESS_GROUP`. Additional ones can be registered using the\n`register_backend()` function.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": "\nA decorator for a function indicating that the return value of the function is\nguaranteed to be a `Future` object and this function can run asynchronously on\nthe RPC callee. More specifically, the callee extracts the `Future` returned\nby the wrapped function and installs subsequent processing steps as a callback\nto that `Future`. The installed callback will read the value from the `Future`\nwhen completed and send the value back as the RPC response. That also means\nthe returned `Future` only exists on the callee side and is never sent through\nRPC. This decorator is useful when the wrapped function\u2019s (`fn`) execution\nneeds to pause and resume due to, e.g., containing `rpc_async()` or waiting\nfor other signals.\n\nNote\n\nTo enable asynchronous execution, applications must pass the function object\nreturned by this decorator to RPC APIs. If RPC detected attributes installed\nby this decorator, it knows that this function returns a `Future` object and\nwill handle that accordingly. However, this does not mean this decorator has\nto be outmost one when defining a function. For example, when combined with\n`@staticmethod` or `@classmethod`, `@rpc.functions.async_execution` needs to\nbe the inner decorator to allow the target function be recognized as a static\nor class function. This target function can still execute asynchronously\nbecause, when accessed, the static or class method preserves attributes\ninstalled by `@rpc.functions.async_execution`.\n\nThe returned `Future` object can come from `rpc_async()`, `then()`, or\n`Future` constructor. The example below shows directly using the `Future`\nreturned by `then()`.\n\nWhen combined with TorchScript decorators, this decorator must be the outmost\none.\n\nWhen combined with static or class method, this decorator must be the inner\none.\n\nThis decorator also works with RRef helpers, i.e., .\n`torch.distributed.rpc.RRef.rpc_sync()`,\n`torch.distributed.rpc.RRef.rpc_async()`, and\n`torch.distributed.rpc.RRef.remote()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": "\nGet `WorkerInfo` of a given worker name. Use this `WorkerInfo` to avoid\npassing an expensive string on every invocation.\n\nworker_name (str) \u2013 the string name of a worker. If `None`, return the the id\nof the current worker. (default `None`)\n\n`WorkerInfo` instance for the given `worker_name` or `WorkerInfo` of the\ncurrent worker if `worker_name` is `None`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": "\nInitializes RPC primitives such as the local RPC agent and distributed\nautograd, which immediately makes the current process ready to send and\nreceive RPCs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options class for `ProcessGroupAgent`, which is derived from\n`RpcBackendOptions`.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nThe number of threads in the thread-pool used by ProcessGroupAgent.\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by ProcessGroupAgent.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": "\nMake a remote call to run `func` on worker `to` and return an `RRef` to the\nresult value immediately. Worker `to` will be the owner of the returned\n`RRef`, and the worker calling `remote` is a user. The owner manages the\nglobal reference count of its `RRef`, and the owner `RRef` is only destructed\nwhen globally there are no living references to it.\n\nA user `RRef` instance to the result value. Use the blocking API\n`torch.distributed.rpc.RRef.to_here()` to retrieve the result value locally.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nWarning\n\nThe `remote` API does not copy storages of argument tensors until sending them\nover the wire, which could be done by a different thread depending on the RPC\nbackend type. The caller should make sure that the contents of those tensors\nstay intact until the returned RRef is confirmed by the owner, which can be\nchecked using the `torch.distributed.rpc.RRef.confirmed_by_owner()` API.\n\nWarning\n\nErrors such as timeouts for the `remote` API are handled on a best-effort\nbasis. This means that when remote calls initiated by `remote` fail, such as\nwith a timeout error, we take a best-effort approach to error handling. This\nmeans that errors are handled and set on the resulting RRef on an asynchronous\nbasis. If the RRef has not been used by the application before this handling\n(such as `to_here` or fork call), then future uses of the `RRef` will\nappropriately raise errors. However, it is possible that the user application\nwill use the `RRef` before the errors are handled. In this case, errors may\nnot be raised as they have not yet been handled.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nAn abstract structure encapsulating the options passed into the RPC backend.\nAn instance of this class can be passed in to `init_rpc()` in order to\ninitialize RPC with specific configurations, such as the RPC timeout and\n`init_method` to be used.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": "\nMake a non-blocking RPC call to run function `func` on worker `to`. RPC\nmessages are sent and received in parallel to execution of Python code. This\nmethod is thread-safe. This method will immediately return a `Future` that can\nbe awaited on.\n\nReturns a `Future` object that can be waited on. When completed, the return\nvalue of `func` on `args` and `kwargs` can be retrieved from the `Future`\nobject.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nWarning\n\nThe `rpc_async` API does not copy storages of argument tensors until sending\nthem over the wire, which could be done by a different thread depending on the\nRPC backend type. The caller should make sure that the contents of those\ntensors stay intact until the returned `Future` completes.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": "\nMake a blocking RPC call to run function `func` on worker `to`. RPC messages\nare sent and received in parallel to execution of Python code. This method is\nthread-safe.\n\nReturns the result of running `func` with `args` and `kwargs`.\n\nWarning\n\nUsing GPU tensors as arguments or return values of `func` is not supported\nsince we don\u2019t support sending GPU tensors over the wire. You need to\nexplicitly copy GPU tensors to CPU before using them as arguments or return\nvalues of `func`.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\nBelow is an example of running a TorchScript function using RPC.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\nReturns whether this `RRef` has been confirmed by the owner. `OwnerRRef`\nalways returns true, while `UserRRef` only returns true when the owner knowns\nabout this `UserRRef`.\n\nReturns whether or not the current node is the owner of this `RRef`.\n\nIf the current node is the owner, returns a reference to the local value.\nOtherwise, throws an exception.\n\nReturns worker information of the node that owns this `RRef`.\n\nReturns worker name of the node that owns this `RRef`.\n\nCreate a helper proxy to easily launch a `remote` using the owner of the RRef\nas the destination to run functions on the object referenced by this RRef.\nMore specifically, `rref.remote().func_name(*args, **kwargs)` is the same as\nthe following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.remote()`. If the creation of\nthis `RRef` is not successfully completed within the timeout, then the next\ntime there is an attempt to use the RRef (such as `to_here`), a timeout will\nbe raised. If not provided, the default RPC timeout will be used. Please see\n`rpc.remote()` for specific timeout semantics for `RRef`.\n\nCreate a helper proxy to easily launch an `rpc_async` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_async()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\nCreate a helper proxy to easily launch an `rpc_sync` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_sync()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\nBlocking call that copies the value of the RRef from the owner to the local\nnode and returns it. If the current node is the owner, returns a reference to\nthe local value.\n\ntimeout (float, optional) \u2013 Timeout for `to_here`. If the call does not\ncomplete within this timeframe, an exception indicating so will be raised. If\nthis argument is not provided, the default RPC timeout (60s) will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": "\nRuns the backward pass using the RRef as the root of the backward pass. If\n`dist_autograd_ctx_id` is provided, we perform a distributed backward pass\nusing the provided ctx_id starting from the owner of the RRef. In this case,\n`get_gradients()` should be used to retrieve the gradients. If\n`dist_autograd_ctx_id` is `None`, it is assumed that this is a local autograd\ngraph and we only perform a local backward pass. In the local case, the node\ncalling this API has to be the owner of the RRef. The value of the RRef is\nexpected to be a scalar Tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether this `RRef` has been confirmed by the owner. `OwnerRRef`\nalways returns true, while `UserRRef` only returns true when the owner knowns\nabout this `UserRRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": "\nReturns whether or not the current node is the owner of this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": "\nIf the current node is the owner, returns a reference to the local value.\nOtherwise, throws an exception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": "\nReturns worker information of the node that owns this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": "\nReturns worker name of the node that owns this `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch a `remote` using the owner of the RRef\nas the destination to run functions on the object referenced by this RRef.\nMore specifically, `rref.remote().func_name(*args, **kwargs)` is the same as\nthe following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.remote()`. If the creation of\nthis `RRef` is not successfully completed within the timeout, then the next\ntime there is an attempt to use the RRef (such as `to_here`), a timeout will\nbe raised. If not provided, the default RPC timeout will be used. Please see\n`rpc.remote()` for specific timeout semantics for `RRef`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_async` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_async().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_async()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": "\nCreate a helper proxy to easily launch an `rpc_sync` using the owner of the\nRRef as the destination to run functions on the object referenced by this\nRRef. More specifically, `rref.rpc_sync().func_name(*args, **kwargs)` is the\nsame as the following:\n\ntimeout (float, optional) \u2013 Timeout for `rref.rpc_sync()`. If the call does\nnot complete within this timeframe, an exception indicating so will be raised.\nIf this argument is not provided, the default RPC timeout will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": "\nBlocking call that copies the value of the RRef from the owner to the local\nnode and returns it. If the current node is the owner, returns a reference to\nthe local value.\n\ntimeout (float, optional) \u2013 Timeout for `to_here`. If the call does not\ncomplete within this timeframe, an exception indicating so will be raised. If\nthis argument is not provided, the default RPC timeout (60s) will be used.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": "\nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This\nstops the local agent from accepting outstanding requests, and shuts down the\nRPC framework by terminating all RPC threads. If `graceful=True`, this will\nblock until all local and remote RPC processes reach this method and wait for\nall outstanding work to complete. Otherwise, if `graceful=False`, this is a\nlocal shutdown, and it does not wait for other RPC processes to reach this\nmethod.\n\nWarning\n\nFor `Future` objects returned by `rpc_async()`, `future.wait()` should not be\ncalled after `shutdown()`.\n\ngraceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will\n1) wait until there is no pending system messages for `UserRRefs` and delete\nthem; 2) block until all local and remote RPC processes have reached this\nmethod and wait for all outstanding work to complete.\n\nMake sure that `MASTER_ADDR` and `MASTER_PORT` are set properly on both\nworkers. Refer to `init_process_group()` API for more details. For example,\n\nThen run the following code in two different processes:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": "\nThe backend options for `TensorPipeAgent`, derived from `RpcBackendOptions`.\n\nThe device map locations.\n\nURL specifying how to initialize the process group. Default is `env://`\n\nThe number of threads in the thread-pool used by `TensorPipeAgent` to execute\nrequests.\n\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\nSet device mapping between each RPC caller and callee pair. This function can\nbe called multiple times to incrementally add device placement configurations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": "\nThe device map locations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": "\nURL specifying how to initialize the process group. Default is `env://`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": "\nThe number of threads in the thread-pool used by `TensorPipeAgent` to execute\nrequests.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": "\nA float indicating the timeout to use for all RPCs. If an RPC does not\ncomplete in this timeframe, it will complete with an exception indicating that\nit has timed out.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": "\nSet device mapping between each RPC caller and callee pair. This function can\nbe called multiple times to incrementally add device placement configurations.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": "\nA structure that encapsulates information of a worker in the system. Contains\nthe name and ID of the worker. This class is not meant to be constructed\ndirectly, rather, an instance can be retrieved through `get_worker_info()` and\nthe result can be passed in to functions such as `rpc_sync()`, `rpc_async()`,\n`remote()` to avoid copying a string on every invocation.\n\nGlobally unique id to identify the worker.\n\nThe name of the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": "\nGlobally unique id to identify the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": "\nThe name of the worker.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": "\nScatters a list of tensors to all processes in a group.\n\nEach process will receive exactly one tensor and store its data in the\n`tensor` argument.\n\nAsync work handle, if async_op is set to True. None, if not async_op or if not\npart of the group\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": "\nScatters picklable objects in `scatter_object_input_list` to the whole group.\nSimilar to `scatter()`, but Python objects can be passed in. On each rank, the\nscattered object will be stored as the first element of\n`scatter_object_output_list`. Note that all objects in\n`scatter_object_input_list` must be picklable in order to be scattered.\n\n`None`. If rank is part of the group, `scatter_object_output_list` will have\nits first element set to the scattered object for this rank.\n\nNote\n\nNote that this API differs slightly from the scatter collective since it does\nnot provide an `async_op` handle and thus will be a blocking call.\n\nWarning\n\n`scatter_object_list()` uses `pickle` module implicitly, which is known to be\ninsecure. It is possible to construct malicious pickle data which will execute\narbitrary code during unpickling. Only call this function with data you trust.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": "\nSends a tensor synchronously.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": "\nBase class for all store implementations, such as the 3 provided by PyTorch\ndistributed: (`TCPStore`, `FileStore`, and `HashStore`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": "\nThe first call to add for a given `key` creates a counter associated with\n`key` in the store, initialized to `amount`. Subsequent calls to add with the\nsame `key` increment the counter by the specified `amount`. Calling `add()`\nwith a key that has already been set in the store by `set()` will result in an\nexception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": "\nDeletes the key-value pair associated with `key` from the store. Returns\n`true` if the key was successfully deleted, and `false` if it was not.\n\nWarning\n\nThe `delete_key` API is only supported by the `TCPStore` and `HashStore`.\nUsing this API with the `FileStore` will result in an exception.\n\nkey (str) \u2013 The key to be deleted from the store\n\n`True` if `key` was deleted, otherwise `False`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": "\nRetrieves the value associated with the given `key` in the store. If `key` is\nnot present in the store, the function will wait for `timeout`, which is\ndefined when initializing the store, before throwing an exception.\n\nkey (str) \u2013 The function will return the value associated with this key.\n\nValue associated with `key` if `key` is in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": "\nReturns the number of keys set in the store. Note that this number will\ntypically be one greater than the number of keys added by `set()` and `add()`\nsince one key is used to coordinate all the workers using the store.\n\nWarning\n\nWhen used with the `TCPStore`, `num_keys` returns the number of keys written\nto the underlying file. If the store is destructed and another store is\ncreated with the same file, the original keys will be retained.\n\nThe number of keys present in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": "\nInserts the key-value pair into the store based on the supplied `key` and\n`value`. If `key` already exists in the store, it will overwrite the old value\nwith the new supplied `value`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": "\nSets the store\u2019s default timeout. This timeout is used during initialization\nand in `wait()` and `get()`.\n\ntimeout (timedelta) \u2013 timeout to be set in the store.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": "\nOverloaded function.\n\nWaits for each key in `keys` to be added to the store. If not all keys are set\nbefore the `timeout` (set during store initialization), then `wait` will throw\nan exception.\n\nkeys (list) \u2013 List of keys on which to wait until they are set in the store.\n\nWaits for each key in `keys` to be added to the store, and throws an exception\nif the keys have not been set by the supplied `timeout`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": "\nA TCP-based distributed key-value store implementation. The server store holds\nthe data, while the client stores can connect to the server store over TCP and\nperform actions such as `set()` to insert a key-value pair, `get()` to\nretrieve a key-value pair, etc.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": "\nThe `distributions` package contains parameterizable probability distributions\nand sampling functions. This allows the construction of stochastic computation\ngraphs and stochastic gradient estimators for optimization. This package\ngenerally follows the design of the TensorFlow Distributions package.\n\nIt is not possible to directly backpropagate through random samples. However,\nthere are two main methods for creating surrogate functions that can be\nbackpropagated through. These are the score function estimator/likelihood\nratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is\ncommonly seen as the basis for policy gradient methods in reinforcement\nlearning, and the pathwise derivative estimator is commonly seen in the\nreparameterization trick in variational autoencoders. Whilst the score\nfunction only requires the value of samples f(x)f(x) , the pathwise derivative\nrequires the derivative f\u2032(x)f'(x) . The next sections discuss these two in a\nreinforcement learning example. For more details see Gradient Estimation Using\nStochastic Computation Graphs .\n\nWhen the probability density function is differentiable with respect to its\nparameters, we only need `sample()` and `log_prob()` to implement REINFORCE:\n\nwhere \u03b8\\theta are the parameters, \u03b1\\alpha is the learning rate, rr is the\nreward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s)) is the probability of taking action aa\nin state ss given policy \u03c0\u03b8\\pi^\\theta .\n\nIn practice we would sample an action from the output of a network, apply this\naction in an environment, and then use `log_prob` to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:\n\nThe other way to implement these stochastic/policy gradients would be to use\nthe reparameterization trick from the `rsample()` method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The\nreparameterized sample therefore becomes differentiable. The code for\nimplementing the pathwise derivative would be as follows:\n\nBases: `object`\n\nDistribution is the abstract base class for probability distributions.\n\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\nReturns the shape over which parameters are batched.\n\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\nReturns the shape of a single sample (without batching).\n\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the mean of the distribution.\n\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\nReturns the standard deviation of the distribution.\n\nReturns a `Constraint` object representing this distribution\u2019s support.\n\nReturns the variance of the distribution.\n\nBases: `torch.distributions.distribution.Distribution`\n\nExponentialFamily is the abstract base class for probability distributions\nbelonging to an exponential family, whose probability mass/density function\nhas the form is defined below\n\nwhere \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient\nstatistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and\nk(x)k(x) is the carrier measure.\n\nNote\n\nThis class is an intermediary between the `Distribution` class and\ndistributions which belong to an exponential family mainly to check the\ncorrectness of the `.entropy()` and analytic KL divergence methods. We use\nthis class to compute the entropy and KL divergence using the AD framework and\nBregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies\nand Cross-entropies of Exponential Families).\n\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Bernoulli distribution parameterized by `probs` or `logits` (but not\nboth).\n\nSamples are binary (0 or 1). They take the value `1` with probability `p` and\n`0` with probability `1 - p`.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nBeta distribution parameterized by `concentration1` and `concentration0`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Binomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). `total_count` must be broadcastable with\n`probs`/`logits`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a categorical distribution parameterized by either `probs` or `logits`\n(but not both).\n\nNote\n\nIt is equivalent to the distribution that `torch.multinomial()` samples from.\n\nSamples are integers from {0,\u2026,K\u22121}\\\\{0, \\ldots, K-1\\\\} where `K` is\n`probs.size(-1)`.\n\nIf `probs` is 1-dimensional with length-`K`, each element is the relative\nprobability of sampling the class at that index.\n\nIf `probs` is N-dimensional, the first N-1 dimensions are treated as a batch\nof relative probability vectors.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.multinomial()`\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means `0` follows a\nCauchy distribution.\n\nExample:\n\nBases: `torch.distributions.gamma.Gamma`\n\nCreates a Chi2 distribution parameterized by shape parameter `df`. This is\nexactly equivalent to `Gamma(alpha=0.5*df, beta=0.5)`\n\nExample:\n\ndf (float or Tensor) \u2013 shape parameter of the distribution\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a continuous Bernoulli distribution parameterized by `probs` or\n`logits` (but not both).\n\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to log-\nodds, but the same names are used due to the similarity with the Bernoulli.\nSee [1] for more details.\n\nExample:\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\nhttps://arxiv.org/abs/1907.06845\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Dirichlet distribution parameterized by concentration\n`concentration`.\n\nExample:\n\nconcentration (Tensor) \u2013 concentration parameter of the distribution (often\nreferred to as alpha)\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Exponential distribution parameterized by `rate`.\n\nExample:\n\nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Fisher-Snedecor distribution parameterized by `df1` and `df2`.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Gamma distribution parameterized by shape `concentration` and\n`rate`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Geometric distribution parameterized by `probs`, where `probs` is\nthe probability of success of Bernoulli trials. It represents the probability\nthat in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a\nsuccess.\n\nSamples are non-negative integers [0, inf\u2061\\inf ).\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Gumbel Distribution.\n\nExamples:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-Cauchy distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Cauchy distribution\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-normal distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Normal distribution\n\nBases: `torch.distributions.distribution.Distribution`\n\nReinterprets some of the batch dims of a distribution as event dims.\n\nThis is mainly useful for changing the shape of the result of `log_prob()`.\nFor example to create a diagonal Normal distribution with the same shape as a\nMultivariate Normal distribution (so they are interchangeable), you can:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Kumaraswamy distribution.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nLKJ distribution for lower Cholesky factor of correlation matrices. The\ndistribution is controlled by `concentration` parameter \u03b7\\eta to make the\nprobability of the correlation matrix MM generated from a Cholesky factor\npropotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when\n`concentration == 1`, we have a uniform distribution over Cholesky factors of\ncorrelation matrices. Note that this distribution samples the Cholesky factor\nof correlation matrices and not the correlation matrices themselves and\nthereby differs slightly from the derivations in [1] for the `LKJCorr`\ndistribution. For sampling, this uses the Onion method from [1] Section 3.\n\nL ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)\n\nExample:\n\nReferences\n\n[1] `Generating random correlation matrices based on vines and extended onion\nmethod`, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Laplace distribution parameterized by `loc` and `scale`.\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a log-normal distribution parameterized by `loc` and `scale` where:\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal distribution with covariance matrix having a\nlow-rank form parameterized by `cov_factor` and `cov_diag`:\n\nNote\n\nThe computation for determinant and inverse of covariance matrix is avoided\nwhen `cov_factor.shape[1] << cov_factor.shape[0]` thanks to Woodbury matrix\nidentity and matrix determinant lemma. Thanks to these formulas, we just need\nto compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:\n\nBases: `torch.distributions.distribution.Distribution`\n\nThe `MixtureSameFamily` distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of the\nsame distribution type. It is parameterized by a `Categorical` \u201cselecting\ndistribution\u201d (over `k` component) and a component distribution, i.e., a\n`Distribution` with a rightmost batch shape (equal to `[k]`) which indexes\neach (batch of) component.\n\nExamples:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Multinomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). The innermost dimension of `probs` indexes\nover categories. All other dimensions index over batches.\n\nNote that `total_count` need not be specified if only `log_prob()` is called\n(see example below)\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\n\nThe multivariate normal distribution can be parameterized either in terms of a\npositive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite\nprecision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix\nL\\mathbf{L} with positive-valued diagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can\nbe obtained via e.g. Cholesky decomposition of the covariance.\n\nNote\n\nOnly one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be\nspecified.\n\nUsing `scale_tril` will be more efficient: all computations internally are\nbased on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed\ninstead, it is only used to compute the corresponding lower triangular\nmatrices using a Cholesky decomposition.\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Negative Binomial distribution, i.e. distribution of the number of\nsuccessful independent and identical Bernoulli trials before `total_count`\nfailures are achieved. The probability of failure of each Bernoulli trial is\n`probs`.\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a normal (also called Gaussian) distribution parameterized by `loc`\nand `scale`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a one-hot categorical distribution parameterized by `probs` or\n`logits`.\n\nSamples are one-hot coded vectors of size `probs.size(-1)`.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.distributions.Categorical()` for specifications of `probs`\nand `logits`.\n\nExample:\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Pareto Type 1 distribution.\n\nExample:\n\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Poisson distribution parameterized by `rate`, the rate parameter.\n\nSamples are nonnegative integers, with a pmf given by\n\nExample:\n\nrate (Number, Tensor) \u2013 the rate parameter\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedBernoulli distribution, parametrized by `temperature`, and\neither `probs` or `logits` (but not both). This is a relaxed version of the\n`Bernoulli` distribution, so the values are in (0, 1), and has\nreparametrizable samples.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a LogitRelaxedBernoulli distribution parameterized by `probs` or\n`logits` (but not both), which is the logit of a RelaxedBernoulli\ndistribution.\n\nSamples are logits of values in (0, 1). See [1] for more details.\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al, 2017)\n\n[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedOneHotCategorical distribution parametrized by `temperature`,\nand either `probs` or `logits`. This is a relaxed version of the\n`OneHotCategorical` distribution, so its samples are on simplex, and are\nreparametrizable.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Student\u2019s t-distribution parameterized by degree of freedom `df`,\nmean `loc` and scale `scale`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nExtension of the Distribution class, which applies a sequence of Transforms to\na base distribution. Let f be the composition of transforms applied:\n\nNote that the `.event_shape` of a `TransformedDistribution` is the maximum\nshape of its base distribution and its transforms, since transforms can\nintroduce correlations among events.\n\nAn example for the usage of `TransformedDistribution` would be:\n\nFor more examples, please look at the implementations of `Gumbel`,\n`HalfCauchy`, `HalfNormal`, `LogNormal`, `Pareto`, `Weibull`,\n`RelaxedBernoulli` and `RelaxedOneHotCategorical`\n\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\nBases: `torch.distributions.distribution.Distribution`\n\nGenerates uniformly distributed random samples from the half-open interval\n`[low, high)`.\n\nExample:\n\nBases: `torch.distributions.distribution.Distribution`\n\nA circular von Mises distribution.\n\nThis implementation uses polar coordinates. The `loc` and `value` args can be\nany real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\n\nThe provided mean is the circular one.\n\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\nThe provided variance is the circular one.\n\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a two-parameter Weibull distribution.\n\nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two\ndistributions.\n\nA batch of KL divergences of shape `batch_shape`.\n\nTensor\n\nNotImplementedError \u2013 If the distribution types have not been registered via\n`register_kl()`.\n\nDecorator to register a pairwise function with `kl_divergence()`. Usage:\n\nLookup returns the most specific (type,type) match ordered by subclass. If the\nmatch is ambiguous, a `RuntimeWarning` is raised. For example to resolve the\nambiguous situation:\n\nyou should register a third most-specific implementation, e.g.:\n\nAbstract class for invertable transformations with computable log det\njacobians. They are primarily used in\n`torch.distributions.TransformedDistribution`.\n\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values since\nthe autograd graph may be reversed. For example while the following works with\nor without caching:\n\nHowever the following will error when caching due to dependency reversal:\n\nDerived classes should implement one or both of `_call()` or `_inverse()`.\nDerived classes that set `bijective=True` should also implement\n`log_abs_det_jacobian()`.\n\ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the\nlatest single value is cached. Only 0 and 1 are supported.\n\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\nComposes multiple transforms in a chain. The transforms being composed are\nresponsible for caching.\n\nWrapper around another transform to treat `reinterpreted_batch_ndims`-many\nextra of the right most dimensions as dependent. This has no effect on the\nforward or backward transforms, but does sum out\n`reinterpreted_batch_ndims`-many of the rightmost dimensions in\n`log_abs_det_jacobian()`.\n\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\nNote that `in_shape` and `out_shape` must have the same number of elements,\njust as for `torch.Tensor.reshape()`.\n\nTransform via the mapping y=exp\u2061(x)y = \\exp(x) .\n\nTransform via the mapping y=xexponenty = x^{\\text{exponent}} .\n\nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and\nx=logit(y)x = \\text{logit}(y) .\n\nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) .\n\nIt is equivalent to `` ComposeTransform([AffineTransform(0., 2.),\nSigmoidTransform(), AffineTransform(-1., 2.)]) `` However this might not be\nnumerically stable, thus it is recommended to use `TanhTransform` instead.\n\nNote that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n\nTransform via the mapping y=\u2223x\u2223y = |x| .\n\nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} +\n\\text{scale} \\times x .\n\nTransforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into\nthe Cholesky factor of a D-dimension correlation matrix. This Cholesky factor\nis a lower triangular matrix with positive diagonals and unit Euclidean norm\nfor each row. The transform is processed as follows:\n\nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)\nthen normalizing.\n\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is appropriate\nfor coordinate-wise optimization algorithms.\n\nTransform from unconstrained space to the simplex of one additional dimension\nvia a stick-breaking process.\n\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of the `Dirichlet` distribution: the first logit is transformed\nvia sigmoid to the first probability and the probability of everything else,\nand then the process recurses.\n\nThis is bijective and appropriate for use in HMC; however it mixes coordinates\ntogether and is less appropriate for optimization.\n\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\n\nThis is useful for parameterizing positive definite matrices in terms of their\nCholesky factorization.\n\nTransform functor that applies a sequence of transforms `tseq` component-wise\nto each submatrix at `dim` in a way compatible with `torch.stack()`.\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t =\nStackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)\n\nThe following constraints are implemented:\n\nAbstract base class for constraints.\n\nA constraint object represents a region over which a variable is valid, e.g.\nwithin which a variable can be optimized.\n\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\nalias of `torch.distributions.constraints._DependentProperty`\n\nalias of `torch.distributions.constraints._IndependentConstraint`\n\nalias of `torch.distributions.constraints._IntegerInterval`\n\nalias of `torch.distributions.constraints._GreaterThan`\n\nalias of `torch.distributions.constraints._GreaterThanEq`\n\nalias of `torch.distributions.constraints._LessThan`\n\nalias of `torch.distributions.constraints._Multinomial`\n\nalias of `torch.distributions.constraints._Interval`\n\nalias of `torch.distributions.constraints._HalfOpenInterval`\n\nalias of `torch.distributions.constraints._Cat`\n\nalias of `torch.distributions.constraints._Stack`\n\nPyTorch provides two global `ConstraintRegistry` objects that link\n`Constraint` objects to `Transform` objects. These objects both input\nconstraints and return transforms, but they have different guarantees on\nbijectivity.\n\nThe `transform_to()` registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s `.arg_constraints` dict. These transforms\noften overparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:\n\nThe `biject_to()` registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained `.support` are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:\n\nNote\n\nAn example where `transform_to` and `biject_to` differ is\n`constraints.simplex`: `transform_to(constraints.simplex)` returns a\n`SoftmaxTransform` that simply exponentiates and normalizes its inputs; this\nis a cheap and mostly coordinate-wise operation appropriate for algorithms\nlike SVI. In contrast, `biject_to(constraints.simplex)` returns a\n`StickBreakingTransform` that bijects its input down to a one-fewer-\ndimensional space; this a more expensive less numerically stable transform but\nis needed for algorithms like HMC.\n\nThe `biject_to` and `transform_to` objects can be extended by user-defined\nconstraints and transforms using their `.register()` method either as a\nfunction on singleton constraints:\n\nor as a decorator on parameterized constraints:\n\nYou can create your own registry by creating a new `ConstraintRegistry`\nobject.\n\nRegistry to link constraints to transforms.\n\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Bernoulli distribution parameterized by `probs` or `logits` (but not\nboth).\n\nSamples are binary (0 or 1). They take the value `1` with probability `p` and\n`0` with probability `1 - p`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nBeta distribution parameterized by `concentration1` and `concentration0`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Binomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). `total_count` must be broadcastable with\n`probs`/`logits`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a categorical distribution parameterized by either `probs` or `logits`\n(but not both).\n\nNote\n\nIt is equivalent to the distribution that `torch.multinomial()` samples from.\n\nSamples are integers from {0,\u2026,K\u22121}\\\\{0, \\ldots, K-1\\\\} where `K` is\n`probs.size(-1)`.\n\nIf `probs` is 1-dimensional with length-`K`, each element is the relative\nprobability of sampling the class at that index.\n\nIf `probs` is N-dimensional, the first N-1 dimensions are treated as a batch\nof relative probability vectors.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.multinomial()`\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means `0` follows a\nCauchy distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": "\nBases: `torch.distributions.gamma.Gamma`\n\nCreates a Chi2 distribution parameterized by shape parameter `df`. This is\nexactly equivalent to `Gamma(alpha=0.5*df, beta=0.5)`\n\nExample:\n\ndf (float or Tensor) \u2013 shape parameter of the distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Cat`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": "\nAbstract base class for constraints.\n\nA constraint object represents a region over which a variable is valid, e.g.\nwithin which a variable can be optimized.\n\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": "\nReturns a byte tensor of `sample_shape + batch_shape` indicating whether each\nevent in value satisfies this constraint.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._DependentProperty`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThan`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._GreaterThanEq`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._HalfOpenInterval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IndependentConstraint`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._IntegerInterval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Interval`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._LessThan`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Multinomial`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": "\nalias of `torch.distributions.constraints._Stack`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": "\nRegistry to link constraints to transforms.\n\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": "\nRegisters a `Constraint` subclass in this registry. Usage:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a continuous Bernoulli distribution parameterized by `probs` or\n`logits` (but not both).\n\nThe distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to log-\nodds, but the same names are used due to the similarity with the Bernoulli.\nSee [1] for more details.\n\nExample:\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational\nautoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.\nhttps://arxiv.org/abs/1907.06845\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Dirichlet distribution parameterized by concentration\n`concentration`.\n\nExample:\n\nconcentration (Tensor) \u2013 concentration parameter of the distribution (often\nreferred to as alpha)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": "\nBases: `object`\n\nDistribution is the abstract base class for probability distributions.\n\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\nReturns the shape over which parameters are batched.\n\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\nReturns the shape of a single sample (without batching).\n\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\nReturns the mean of the distribution.\n\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\nReturns the standard deviation of the distribution.\n\nReturns a `Constraint` object representing this distribution\u2019s support.\n\nReturns the variance of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": "\nReturns a dictionary from argument names to `Constraint` objects that should\nbe satisfied by each argument of this distribution. Args that are not tensors\nneed not appear in this dict.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": "\nReturns the shape over which parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": "\nReturns the cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": "\nReturns entropy of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": "\nReturns tensor containing all values supported by a discrete distribution. The\nresult will enumerate over dimension 0, so the shape of the result will be\n`(cardinality,) + batch_shape + event_shape` (where `event_shape = ()` for\nunivariate distributions).\n\nNote that this enumerates over all batched tensors in lock-step `[[0, 0], [1,\n1], \u2026]`. With `expand=False`, enumeration happens along dim 0, but with the\nremaining batch dimensions being singleton dimensions, `[[0], [1], ..`.\n\nTo iterate over the full Cartesian product use\n`itertools.product(m.enumerate_support())`.\n\nexpand (bool) \u2013 whether to expand the support over the batch dims to match the\ndistribution\u2019s `batch_shape`.\n\nTensor iterating over dimension 0.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": "\nReturns the shape of a single sample (without batching).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": "\nReturns a new distribution instance (or populates an existing instance\nprovided by a derived class) with batch dimensions expanded to `batch_shape`.\nThis method calls `expand` on the distribution\u2019s parameters. As such, this\ndoes not allocate new memory for the expanded distribution instance.\nAdditionally, this does not repeat any args checking or parameter broadcasting\nin `__init__.py`, when an instance is first created.\n\nNew distribution instance with batch dimensions expanded to `batch_size`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": "\nReturns the inverse cumulative density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": "\nReturns the log of the probability density/mass function evaluated at `value`.\n\nvalue (Tensor) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": "\nReturns the mean of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": "\nReturns perplexity of distribution, batched over batch_shape.\n\nTensor of shape batch_shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": "\nGenerates n samples or n batches of samples if the distribution parameters are\nbatched.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": "\nSets whether validation is enabled or disabled.\n\nThe default behavior mimics Python\u2019s `assert` statement: validation is on by\ndefault, but is disabled if Python is run in optimized mode (via `python -O`).\nValidation may be expensive, so you may want to disable it once a model is\nworking.\n\nvalue (bool) \u2013 Whether to enable validation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": "\nReturns the standard deviation of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": "\nReturns a `Constraint` object representing this distribution\u2019s support.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": "\nReturns the variance of the distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Exponential distribution parameterized by `rate`.\n\nExample:\n\nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nExponentialFamily is the abstract base class for probability distributions\nbelonging to an exponential family, whose probability mass/density function\nhas the form is defined below\n\nwhere \u03b8\\theta denotes the natural parameters, t(x)t(x) denotes the sufficient\nstatistic, F(\u03b8)F(\\theta) is the log normalizer function for a given family and\nk(x)k(x) is the carrier measure.\n\nNote\n\nThis class is an intermediary between the `Distribution` class and\ndistributions which belong to an exponential family mainly to check the\ncorrectness of the `.entropy()` and analytic KL divergence methods. We use\nthis class to compute the entropy and KL divergence using the AD framework and\nBregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies\nand Cross-entropies of Exponential Families).\n\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": "\nMethod to compute the entropy using Bregman divergence of the log normalizer.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Fisher-Snedecor distribution parameterized by `df1` and `df2`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Gamma distribution parameterized by shape `concentration` and\n`rate`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Geometric distribution parameterized by `probs`, where `probs` is\nthe probability of success of Bernoulli trials. It represents the probability\nthat in k+1k + 1 Bernoulli trials, the first kk trials failed, before seeing a\nsuccess.\n\nSamples are non-negative integers [0, inf\u2061\\inf ).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Gumbel Distribution.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-Cauchy distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Cauchy distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a half-normal distribution parameterized by `scale` where:\n\nExample:\n\nscale (float or Tensor) \u2013 scale of the full Normal distribution\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nReinterprets some of the batch dims of a distribution as event dims.\n\nThis is mainly useful for changing the shape of the result of `log_prob()`.\nFor example to create a diagonal Normal distribution with the same shape as a\nMultivariate Normal distribution (so they are interchangeable), you can:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": "\nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q) between two\ndistributions.\n\nA batch of KL divergences of shape `batch_shape`.\n\nTensor\n\nNotImplementedError \u2013 If the distribution types have not been registered via\n`register_kl()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": "\nDecorator to register a pairwise function with `kl_divergence()`. Usage:\n\nLookup returns the most specific (type,type) match ordered by subclass. If the\nmatch is ambiguous, a `RuntimeWarning` is raised. For example to resolve the\nambiguous situation:\n\nyou should register a third most-specific implementation, e.g.:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Kumaraswamy distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Laplace distribution parameterized by `loc` and `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nLKJ distribution for lower Cholesky factor of correlation matrices. The\ndistribution is controlled by `concentration` parameter \u03b7\\eta to make the\nprobability of the correlation matrix MM generated from a Cholesky factor\npropotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when\n`concentration == 1`, we have a uniform distribution over Cholesky factors of\ncorrelation matrices. Note that this distribution samples the Cholesky factor\nof correlation matrices and not the correlation matrices themselves and\nthereby differs slightly from the derivations in [1] for the `LKJCorr`\ndistribution. For sampling, this uses the Onion method from [1] Section 3.\n\nL ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration)\n\nExample:\n\nReferences\n\n[1] `Generating random correlation matrices based on vines and extended onion\nmethod`, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a log-normal distribution parameterized by `loc` and `scale` where:\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal distribution with covariance matrix having a\nlow-rank form parameterized by `cov_factor` and `cov_diag`:\n\nNote\n\nThe computation for determinant and inverse of covariance matrix is avoided\nwhen `cov_factor.shape[1] << cov_factor.shape[0]` thanks to Woodbury matrix\nidentity and matrix determinant lemma. Thanks to these formulas, we just need\nto compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nThe `MixtureSameFamily` distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of the\nsame distribution type. It is parameterized by a `Categorical` \u201cselecting\ndistribution\u201d (over `k` component) and a component distribution, i.e., a\n`Distribution` with a rightmost batch shape (equal to `[k]`) which indexes\neach (batch of) component.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Multinomial distribution parameterized by `total_count` and either\n`probs` or `logits` (but not both). The innermost dimension of `probs` indexes\nover categories. All other dimensions index over batches.\n\nNote that `total_count` need not be specified if only `log_prob()` is called\n(see example below)\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a multivariate normal (also called Gaussian) distribution\nparameterized by a mean vector and a covariance matrix.\n\nThe multivariate normal distribution can be parameterized either in terms of a\npositive definite covariance matrix \u03a3\\mathbf{\\Sigma} or a positive definite\nprecision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1} or a lower-triangular matrix\nL\\mathbf{L} with positive-valued diagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can\nbe obtained via e.g. Cholesky decomposition of the covariance.\n\nNote\n\nOnly one of `covariance_matrix` or `precision_matrix` or `scale_tril` can be\nspecified.\n\nUsing `scale_tril` will be more efficient: all computations internally are\nbased on `scale_tril`. If `covariance_matrix` or `precision_matrix` is passed\ninstead, it is only used to compute the corresponding lower triangular\nmatrices using a Cholesky decomposition.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Negative Binomial distribution, i.e. distribution of the number of\nsuccessful independent and identical Bernoulli trials before `total_count`\nfailures are achieved. The probability of failure of each Bernoulli trial is\n`probs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a normal (also called Gaussian) distribution parameterized by `loc`\nand `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a one-hot categorical distribution parameterized by `probs` or\n`logits`.\n\nSamples are one-hot coded vectors of size `probs.size(-1)`.\n\nNote\n\nThe `probs` argument must be non-negative, finite and have a non-zero sum, and\nit will be normalized to sum to 1 along the last dimension. attr:`probs` will\nreturn this normalized value. The `logits` argument will be interpreted as\nunnormalized log probabilities and can therefore be any real number. It will\nlikewise be normalized so that the resulting probabilities sum to 1 along the\nlast dimension. attr:`logits` will return this normalized value.\n\nSee also: `torch.distributions.Categorical()` for specifications of `probs`\nand `logits`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a Pareto Type 1 distribution.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": "\nBases: `torch.distributions.exp_family.ExponentialFamily`\n\nCreates a Poisson distribution parameterized by `rate`, the rate parameter.\n\nSamples are nonnegative integers, with a pmf given by\n\nExample:\n\nrate (Number, Tensor) \u2013 the rate parameter\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a LogitRelaxedBernoulli distribution parameterized by `probs` or\n`logits` (but not both), which is the logit of a RelaxedBernoulli\ndistribution.\n\nSamples are logits of values in (0, 1). See [1] for more details.\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random\nVariables (Maddison et al, 2017)\n\n[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedBernoulli distribution, parametrized by `temperature`, and\neither `probs` or `logits` (but not both). This is a relaxed version of the\n`Bernoulli` distribution, so the values are in (0, 1), and has\nreparametrizable samples.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nCreates a RelaxedOneHotCategorical distribution parametrized by `temperature`,\nand either `probs` or `logits`. This is a relaxed version of the\n`OneHotCategorical` distribution, so its samples are on simplex, and are\nreparametrizable.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nCreates a Student\u2019s t-distribution parameterized by degree of freedom `df`,\nmean `loc` and scale `scale`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nExtension of the Distribution class, which applies a sequence of Transforms to\na base distribution. Let f be the composition of transforms applied:\n\nNote that the `.event_shape` of a `TransformedDistribution` is the maximum\nshape of its base distribution and its transforms, since transforms can\nintroduce correlations among events.\n\nAn example for the usage of `TransformedDistribution` would be:\n\nFor more examples, please look at the implementations of `Gumbel`,\n`HalfCauchy`, `HalfNormal`, `LogNormal`, `Pareto`, `Weibull`,\n`RelaxedBernoulli` and `RelaxedOneHotCategorical`\n\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": "\nComputes the cumulative distribution function by inverting the transform(s)\nand computing the score of the base distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": "\nComputes the inverse cumulative distribution function using transform(s) and\ncomputing the score of the base distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": "\nScores the sample by inverting the transform(s) and computing the score using\nthe score of the base distribution and the log abs det jacobian.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped\nbatch of reparameterized samples if the distribution parameters are batched.\nSamples first from base distribution and applies `transform()` for every\ntransform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": "\nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples\nif the distribution parameters are batched. Samples first from base\ndistribution and applies `transform()` for every transform in the list.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=\u2223x\u2223y = |x| .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": "\nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} +\n\\text{scale} \\times x .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": "\nComposes multiple transforms in a chain. The transforms being composed are\nresponsible for caching.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": "\nTransforms an uncontrained real vector xx with length D\u2217(D\u22121)/2D*(D-1)/2 into\nthe Cholesky factor of a D-dimension correlation matrix. This Cholesky factor\nis a lower triangular matrix with positive diagonals and unit Euclidean norm\nfor each row. The transform is processed as follows:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=exp\u2061(x)y = \\exp(x) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": "\nWrapper around another transform to treat `reinterpreted_batch_ndims`-many\nextra of the right most dimensions as dependent. This has no effect on the\nforward or backward transforms, but does sum out\n`reinterpreted_batch_ndims`-many of the rightmost dimensions in\n`log_abs_det_jacobian()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained matrices to lower-triangular matrices with\nnonnegative diagonal entries.\n\nThis is useful for parameterizing positive definite matrices in terms of their\nCholesky factorization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=xexponenty = x^{\\text{exponent}} .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": "\nUnit Jacobian transform to reshape the rightmost part of a tensor.\n\nNote that `in_shape` and `out_shape` must have the same number of elements,\njust as for `torch.Tensor.reshape()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)} and\nx=logit(y)x = \\text{logit}(y) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)\nthen normalizing.\n\nThis is not bijective and cannot be used for HMC. However this acts mostly\ncoordinate-wise (except for the final normalization), and thus is appropriate\nfor coordinate-wise optimization algorithms.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": "\nTransform functor that applies a sequence of transforms `tseq` component-wise\nto each submatrix at `dim` in a way compatible with `torch.stack()`.\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t =\nStackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": "\nTransform from unconstrained space to the simplex of one additional dimension\nvia a stick-breaking process.\n\nThis transform arises as an iterated sigmoid transform in a stick-breaking\nconstruction of the `Dirichlet` distribution: the first logit is transformed\nvia sigmoid to the first probability and the probability of everything else,\nand then the process recurses.\n\nThis is bijective and appropriate for use in HMC; however it mixes coordinates\ntogether and is less appropriate for optimization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": "\nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) .\n\nIt is equivalent to `` ComposeTransform([AffineTransform(0., 2.),\nSigmoidTransform(), AffineTransform(-1., 2.)]) `` However this might not be\nnumerically stable, thus it is recommended to use `TanhTransform` instead.\n\nNote that one should use `cache_size=1` when it comes to `NaN/Inf` values.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": "\nAbstract class for invertable transformations with computable log det\njacobians. They are primarily used in\n`torch.distributions.TransformedDistribution`.\n\nCaching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values since\nthe autograd graph may be reversed. For example while the following works with\nor without caching:\n\nHowever the following will error when caching due to dependency reversal:\n\nDerived classes should implement one or both of `_call()` or `_inverse()`.\nDerived classes that set `bijective=True` should also implement\n`log_abs_det_jacobian()`.\n\ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the\nlatest single value is cached. Only 0 and 1 are supported.\n\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": "\nInfers the shape of the forward computation, given the input shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": "\nReturns the inverse `Transform` of this transform. This should satisfy\n`t.inv.inv is t`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": "\nInfers the shapes of the inverse computation, given the output shape. Defaults\nto preserving shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": "\nComputes the log det jacobian `log |dy/dx|` given input and output.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": "\nReturns the sign of the determinant of the Jacobian, if applicable. In general\nthis only makes sense for bijective transforms.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nGenerates uniformly distributed random samples from the half-open interval\n`[low, high)`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": "\nBases: `torch.distributions.distribution.Distribution`\n\nA circular von Mises distribution.\n\nThis implementation uses polar coordinates. The `loc` and `value` args can be\nany real number (to facilitate unconstrained optimization), but are\ninterpreted as angles modulo 2 pi.\n\nThe provided mean is the circular one.\n\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\nThe provided variance is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": "\nThe provided mean is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": "\nThe sampling algorithm for the von Mises distribution is based on the\nfollowing paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of\nthe von Mises distribution.\u201d Applied Statistics (1979): 152-157.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": "\nThe provided variance is the circular one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": "\nBases: `torch.distributions.transformed_distribution.TransformedDistribution`\n\nSamples from a two-parameter Weibull distribution.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": "\nDivides each element of the input `input` by the corresponding element of\n`other`.\n\nNote\n\nBy default, this performs a \u201ctrue\u201d division like Python 3. See the\n`rounding_mode` argument for floor division.\n\nSupports broadcasting to a common shape, type promotion, and integer, float,\nand complex inputs. Always promotes integer types to the default scalar type.\n\nrounding_mode (str, optional) \u2013\n\nType of rounding applied to the result:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": "\nAlias for `torch.div()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": "\nComputes the dot product of two 1D tensors.\n\nNote\n\nUnlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot\nproduct of two 1D tensors with the same number of elements.\n\n{out} \u2013\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": "\nStack tensors in sequence depthwise (along third axis).\n\nThis is equivalent to concatenation along the third axis after 1-D and 2-D\ntensors have been reshaped by `torch.atleast_3d()`.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": "\nComputes the eigenvalues and eigenvectors of a real square matrix.\n\nNote\n\nSince eigenvalues and eigenvectors might be complex, backward pass is\nsupported only if eigenvalues and eigenvectors are all real valued.\n\nWhen `input` is on CUDA, `torch.eig()` causes host-device synchronization.\n\nout (tuple, optional) \u2013 the output tensors\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\n(Tensor, Tensor)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": "\nSums the product of the elements of the input `operands` along dimensions\nspecified using a notation based on the Einstein summation convention.\n\nEinsum allows computing many common multi-dimensional linear algebraic array\noperations by representing them in a short-hand format based on the Einstein\nsummation convention, given by `equation`. The details of this format are\ndescribed below, but the general idea is to label every dimension of the input\n`operands` with some subscript and define which subscripts are part of the\noutput. The output is then computed by summing the product of the elements of\nthe `operands` along the dimensions whose subscripts are not part of the\noutput. For example, matrix multiplication can be computed using einsum as\n`torch.einsum(\u201cij,jk->ik\u201d, A, B)`. Here, j is the summation subscript and i\nand k the output subscripts (see section below for more details on why).\n\nEquation:\n\nThe `equation` string specifies the subscripts (lower case letters `[\u2018a\u2019,\n\u2018z\u2019]`) for each dimension of the input `operands` in the same order as the\ndimensions, separating subcripts for each operand by a comma (\u2018,\u2019), e.g.\n`\u2018ij,jk\u2019` specify subscripts for two 2D operands. The dimensions labeled with\nthe same subscript must be broadcastable, that is, their size must either\nmatch or be `1`. The exception is if a subscript is repeated for the same\ninput operand, in which case the dimensions labeled with this subscript for\nthis operand must match in size and the operand will be replaced by its\ndiagonal along these dimensions. The subscripts that appear exactly once in\nthe `equation` will be part of the output, sorted in increasing alphabetical\norder. The output is computed by multiplying the input `operands` element-\nwise, with their dimensions aligned based on the subscripts, and then summing\nout the dimensions whose subscripts are not part of the output.\n\nOptionally, the output subscripts can be explicitly defined by adding an arrow\n(\u2018->\u2019) at the end of the equation followed by the subscripts for the output.\nFor instance, the following equation computes the transpose of a matrix\nmultiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once\nfor some input operand and at most once for the output.\n\nEllipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions\ncovered by the ellipsis. Each input operand may contain at most one ellipsis\nwhich will cover the dimensions not covered by subscripts, e.g. for an input\noperand with 5 dimensions, the ellipsis in the equation `\u2018ab\u2026c\u2019` cover the\nthird and fourth dimensions. The ellipsis does not need to cover the same\nnumber of dimensions across the `operands` but the \u2018shape\u2019 of the ellipsis\n(the size of the dimensions covered by them) must broadcast together. If the\noutput is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis\nwill come first in the output (left-most dimensions), before the subscript\nlabels that appear exactly once for the input operands. e.g. the following\nequation implements batch matrix multiplication `\u2018\u2026ij,\u2026jk\u2019`.\n\nA few final notes: the equation may contain whitespaces between the different\nelements (subscripts, ellipsis, arrow and comma) but something like `\u2018\u2026\u2019` is\nnot valid. An empty string `\u2018\u2019` is valid for scalar operands.\n\nNote\n\n`torch.einsum` handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows\ndimensions covered by the ellipsis to be summed over, that is, ellipsis are\nnot required to be part of the output.\n\nNote\n\nThis function does not optimize the given expression, so a different formula\nfor the same computation may run faster or consume less memory. Projects like\nopt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) can optimize\nthe formula for you.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape of the tensor is\ndefined by the variable argument `size`.\n\nsize (int...) \u2013 a sequence of integers defining the shape of the output\ntensor. Can be a variable number of arguments or a collection like a list or\ntuple.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": "\nReturns an uninitialized tensor with the same size as `input`.\n`torch.empty_like(input)` is equivalent to `torch.empty(input.size(),\ndtype=input.dtype, layout=input.layout, device=input.device)`.\n\ninput (Tensor) \u2013 the size of `input` will determine size of the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": "\nReturns a tensor filled with uninitialized data. The shape and strides of the\ntensor is defined by the variable argument `size` and `stride` respectively.\n`torch.empty_strided(size, stride)` is equivalent to\n`torch.empty(size).as_strided(size, stride)`.\n\nWarning\n\nMore than one element of the created tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensors, please clone them first.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": "\nContext-manager that enables gradient calculation.\n\nEnables gradient calculation, if it has been disabled via `no_grad` or\n`set_grad_enabled`.\n\nThis context manager is thread local; it will not affect computation in other\nthreads.\n\nAlso functions as a decorator. (Make sure to instantiate with parenthesis.)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": "\nComputes element-wise equality\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is equal to `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": "\n`True` if two tensors have the same size and elements, `False` otherwise.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": "\nComputes the error function of each element. The error function is defined as\nfollows:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": "\nComputes the complementary error function of each element of `input`. The\ncomplementary error function is defined as follows:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": "\nComputes the inverse error function of each element of `input`. The inverse\nerror function is defined in the range (\u22121,1)(-1, 1) as:\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements of the input tensor\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": "\nComputes the base two exponential function of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": "\nReturns a new tensor with the exponential of the elements minus 1 of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": "\nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\nA 2-D tensor with ones on the diagonal and zeros elsewhere\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized per channel using\n`scale`, `zero_point`, `quant_min` and `quant_max`, across the channel\nspecified by `axis`.\n\nA newly fake_quantized per channel tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": "\nReturns a new tensor with the data in `input` fake quantized using `scale`,\n`zero_point`, `quant_min` and `quant_max`.\n\nA newly fake_quantized tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": "\nDiscrete Fourier transforms and related functions.\n\nComputes the one dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i] = conj(X[-i])`. This function always returns both the positive\nand negative frequency terms even though, for real inputs, the negative\nfrequencies are redundant. `rfft()` returns the more compact one-sided\nrepresentation where only the positive frequencies are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft()`), these correspond to:\n\nCalling the backward transform (`ifft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nComputes the one dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft()`), these correspond\nto:\n\nCalling the forward transform (`fft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nComputes the 2 dimensional discrete Fourier transform of `input`. Equivalent\nto `fftn()` but FFTs only the last two dimensions by default.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i, j] = conj(X[-i, -j])`. This function always returns all\npositive and negative frequency terms even though, for real inputs, half of\nthese values are redundant. `rfft2()` returns the more compact one-sided\nrepresentation where only the positive frequencies of the last dimension are\nreturned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft2()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fft2()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\nComputes the 2 dimensional inverse discrete Fourier transform of `input`.\nEquivalent to `ifftn()` but IFFTs only the last two dimensions by default.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifft2()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\nComputes the N dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])`. This function always\nreturns all positive and negative frequency terms even though, for real\ninputs, half of these values are redundant. `rfftn()` returns the more compact\none-sided representation where only the positive frequencies of the last\ndimension are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fftn()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fftn()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\nComputes the N dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifftn()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\nComputes the one dimensional Fourier transform of real-valued `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])` so the\noutput contains only the positive frequencies below the Nyquist frequency. To\ncompute the full output, use `fft()`\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft()`), these correspond to:\n\nCalling the backward transform (`irfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompare against the full output from `fft()`:\n\nNotice that the symmetric element `T[-1] == T[1].conj()` is omitted. At the\nNyquist frequency `T[-2] == T[2]` is it\u2019s own symmetric pair, and therefore\nmust always be real-valued.\n\nComputes the inverse of `rfft()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft()`), these correspond\nto:\n\nCalling the forward transform (`rfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length:\n\nSo, it is recommended to always pass the signal length `n`:\n\nComputes the 2-dimensional discrete Fourier transform of real `input`.\nEquivalent to `rfftn()` but FFTs only the last two dimensions by default.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i, j] = conj(X[-i, -j])`,\nso the full `fft2()` output contains redundant information. `rfft2()` instead\nomits the negative frequencies in the last dimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fft2()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfft2()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\nComputes the inverse of `rfft2()`. Equivalent to `irfftn()` but IFFTs only the\nlast two dimensions by default.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft2()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft2()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\nComputes the N-dimensional discrete Fourier transform of real `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i_1, ..., i_n] =\nconj(X[-i_1, ..., -i_n])` so the full `fftn()` output contains redundant\ninformation. `rfftn()` instead omits the negative frequencies in the last\ndimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fftn()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfftn()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\nComputes the inverse of `rfftn()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfftn()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric `input` signal.\n\nNote\n\n`hfft()`/`ihfft()` are analogous to `rfft()`/`irfft()`. The real FFT expects a\nreal signal in the time-domain and gives a Hermitian symmetry in the\nfrequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\nthe time-domain and real-valued in the frequency-domain. For this reason,\nspecial care needs to be taken with the length argument `n`, in the same way\nas with `irfft()`.\n\nNote\n\nBecause the signal is Hermitian in the time-domain, the result will be real in\nthe frequency domain. Note that some input frequencies must be real-valued to\nsatisfy the Hermitian property. In these cases the imaginary component will be\nignored. For example, any imaginary component in `input[0]` would result in\none or more complex frequency terms which cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`hfft()`), these correspond to:\n\nCalling the backward transform (`ihfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nTaking a real-valued frequency signal and bringing it into the time domain\ngives Hermitian symmetric output:\n\nNote that `T[1] == T[-1].conj()` and `T[2] == T[-2].conj()` is redundant. We\ncan thus compute the forward transform without considering negative\nfrequencies:\n\nLike with `irfft()`, the output length must be given in order to recover an\neven length output:\n\nComputes the inverse of `hfft()`.\n\n`input` must be a real-valued signal, interpreted in the Fourier domain. The\nIFFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])`. `ihfft()`\nrepresents this in the one-sided form where only the positive frequencies\nbelow the Nyquist frequency are included. To compute the full output, use\n`ifft()`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ihfft()`), these correspond\nto:\n\nCalling the forward transform (`hfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nCompare against the full output from `ifft()`:\n\nComputes the discrete Fourier Transform sample frequencies for a signal of\nsize `n`.\n\nNote\n\nBy convention, `fft()` returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. For an FFT of length\n`n` and with inputs spaced in length unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftfreq()` follows NumPy\u2019s convention of taking\nit to be negative.\n\nFor even input, we can see the Nyquist frequency at `f[2]` is given as\nnegative:\n\nComputes the sample frequencies for `rfft()` with a signal of size `n`.\n\nNote\n\n`rfft()` returns Hermitian one-sided output, so only the positive frequency\nterms are returned. For a real FFT of length `n` and with inputs spaced in\nlength unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. Unlike `fftfreq()`, `rfftfreq()` always returns\nit as positive.\n\nCompared to the output from `fftfreq()`, we see that the Nyquist frequency at\n`f[2]` has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500,\n-0.5000, -0.2500])\n\nReorders n-dimensional FFT data, as provided by `fftn()`, to have negative\nfrequency terms first.\n\nThis performs a periodic shift of n-dimensional data such that the origin `(0,\n..., 0)` is moved to the center of the tensor. Specifically, to\n`input.shape[dim] // 2` in each selected dimension.\n\nNote\n\nBy convention, the FFT returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. `fftshift()` rearranges\nall frequencies into ascending order from negative to positive with the zero-\nfrequency term in the center.\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftshift()` always puts the Nyquist term at the\n0-index. This is the same convention used by `fftfreq()`.\n\nAlso notice that the Nyquist frequency term at `f[2]` was moved to the\nbeginning of the tensor.\n\nThis also works for multi-dimensional transforms:\n\n`fftshift()` can also be useful for spatial data. If our data is defined on a\ncentered grid (`[-(N//2), (N-1)//2]`) then we can use the standard FFT defined\non an uncentered grid (`[0, N)`) by first applying an `ifftshift()`.\n\nSimilarly, we can convert the frequency domain components to centered\nconvention by applying `fftshift()`.\n\nThe inverse transform, from centered Fourier space back to centered spatial\ndata, can be performed by applying the inverse shifts in reverse order:\n\nInverse of `fftshift()`.\n\nA round-trip through `fftshift()` and `ifftshift()` gives the same result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i] = conj(X[-i])`. This function always returns both the positive\nand negative frequency terms even though, for real inputs, the negative\nfrequencies are redundant. `rfft()` returns the more compact one-sided\nrepresentation where only the positive frequencies are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft()`), these correspond to:\n\nCalling the backward transform (`ifft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional discrete Fourier transform of `input`. Equivalent\nto `fftn()` but FFTs only the last two dimensions by default.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i, j] = conj(X[-i, -j])`. This function always returns all\npositive and negative frequency terms even though, for real inputs, half of\nthese values are redundant. `rfft2()` returns the more compact one-sided\nrepresentation where only the positive frequencies of the last dimension are\nreturned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fft2()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fft2()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": "\nComputes the discrete Fourier Transform sample frequencies for a signal of\nsize `n`.\n\nNote\n\nBy convention, `fft()` returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. For an FFT of length\n`n` and with inputs spaced in length unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftfreq()` follows NumPy\u2019s convention of taking\nit to be negative.\n\nFor even input, we can see the Nyquist frequency at `f[2]` is given as\nnegative:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": "\nComputes the N dimensional discrete Fourier transform of `input`.\n\nNote\n\nThe Fourier domain representation of any real signal satisfies the Hermitian\nproperty: `X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n])`. This function always\nreturns all positive and negative frequency terms even though, for real\ninputs, half of these values are redundant. `rfftn()` returns the more compact\none-sided representation where only the positive frequencies of the last\ndimension are returned.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`fftn()`), these correspond to:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`ifftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nThe discrete Fourier transform is separable, so `fftn()` here is equivalent to\ntwo one-dimensional `fft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": "\nReorders n-dimensional FFT data, as provided by `fftn()`, to have negative\nfrequency terms first.\n\nThis performs a periodic shift of n-dimensional data such that the origin `(0,\n..., 0)` is moved to the center of the tensor. Specifically, to\n`input.shape[dim] // 2` in each selected dimension.\n\nNote\n\nBy convention, the FFT returns positive frequency terms first, followed by the\nnegative frequencies in reverse order, so that `f[-i]` for all 0<i\u2264n/20 < i\n\\leq n/2 in Python gives the negative frequency terms. `fftshift()` rearranges\nall frequencies into ascending order from negative to positive with the zero-\nfrequency term in the center.\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. `fftshift()` always puts the Nyquist term at the\n0-index. This is the same convention used by `fftfreq()`.\n\nAlso notice that the Nyquist frequency term at `f[2]` was moved to the\nbeginning of the tensor.\n\nThis also works for multi-dimensional transforms:\n\n`fftshift()` can also be useful for spatial data. If our data is defined on a\ncentered grid (`[-(N//2), (N-1)//2]`) then we can use the standard FFT defined\non an uncentered grid (`[0, N)`) by first applying an `ifftshift()`.\n\nSimilarly, we can convert the frequency domain components to centered\nconvention by applying `fftshift()`.\n\nThe inverse transform, from centered Fourier space back to centered spatial\ndata, can be performed by applying the inverse shifts in reverse order:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": "\nComputes the one dimensional discrete Fourier transform of a Hermitian\nsymmetric `input` signal.\n\nNote\n\n`hfft()`/`ihfft()` are analogous to `rfft()`/`irfft()`. The real FFT expects a\nreal signal in the time-domain and gives a Hermitian symmetry in the\nfrequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in\nthe time-domain and real-valued in the frequency-domain. For this reason,\nspecial care needs to be taken with the length argument `n`, in the same way\nas with `irfft()`.\n\nNote\n\nBecause the signal is Hermitian in the time-domain, the result will be real in\nthe frequency domain. Note that some input frequencies must be real-valued to\nsatisfy the Hermitian property. In these cases the imaginary component will be\nignored. For example, any imaginary component in `input[0]` would result in\none or more complex frequency terms which cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`hfft()`), these correspond to:\n\nCalling the backward transform (`ihfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nTaking a real-valued frequency signal and bringing it into the time domain\ngives Hermitian symmetric output:\n\nNote that `T[1] == T[-1].conj()` and `T[2] == T[-2].conj()` is redundant. We\ncan thus compute the forward transform without considering negative\nfrequencies:\n\nLike with `irfft()`, the output length must be given in order to recover an\neven length output:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": "\nComputes the one dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft()`), these correspond\nto:\n\nCalling the forward transform (`fft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ifft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": "\nComputes the 2 dimensional inverse discrete Fourier transform of `input`.\nEquivalent to `ifftn()` but IFFTs only the last two dimensions by default.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifft2()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": "\nComputes the N dimensional inverse discrete Fourier transform of `input`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ifftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`fftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`ifftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nThe discrete Fourier transform is separable, so `ifftn()` here is equivalent\nto two one-dimensional `ifft()` calls:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": "\nInverse of `fftshift()`.\n\nA round-trip through `fftshift()` and `ifftshift()` gives the same result:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": "\nComputes the inverse of `hfft()`.\n\n`input` must be a real-valued signal, interpreted in the Fourier domain. The\nIFFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])`. `ihfft()`\nrepresents this in the one-sided form where only the positive frequencies\nbelow the Nyquist frequency are included. To compute the full output, use\n`ifft()`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`ihfft()`), these correspond\nto:\n\nCalling the forward transform (`hfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `ihfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nCompare against the full output from `ifft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": "\nComputes the inverse of `rfft()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `n`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal length `n`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft()`), these correspond\nto:\n\nCalling the forward transform (`rfft()`) with the same normalization mode will\napply an overall normalization of `1/n` between the two transforms. This is\nrequired to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length:\n\nSo, it is recommended to always pass the signal length `n`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": "\nComputes the inverse of `rfft2()`. Equivalent to `irfftn()` but IFFTs only the\nlast two dimensions by default.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfft2()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft2()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": "\nComputes the inverse of `rfftn()`.\n\n`input` is interpreted as a one-sided Hermitian signal in the Fourier domain,\nas produced by `rfftn()`. By the Hermitian property, the output will be real-\nvalued.\n\nNote\n\nSome input frequencies must be real-valued to satisfy the Hermitian property.\nIn these cases the imaginary component will be ignored. For example, any\nimaginary component in the zero-frequency term cannot be represented in a real\noutput and so will always be ignored.\n\nNote\n\nThe correct interpretation of the Hermitian input depends on the length of the\noriginal data, as given by `s`. This is because each input shape could\ncorrespond to either an odd or even length signal. By default, the signal is\nassumed to be even length and odd signals will not round-trip properly. So, it\nis recommended to always pass the signal shape `s`.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the backward transform (`irfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical IFFT size. Calling the forward transform\n(`rfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (normalize by `1/n`).\n\nWithout specifying the output length to `irfft()`, the output will not round-\ntrip properly because the input is odd-length in the last dimension:\n\nSo, it is recommended to always pass the signal shape `s`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": "\nComputes the one dimensional Fourier transform of real-valued `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i] = conj(X[-i])` so the\noutput contains only the positive frequencies below the Nyquist frequency. To\ncompute the full output, use `fft()`\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft()`), these correspond to:\n\nCalling the backward transform (`irfft()`) with the same normalization mode\nwill apply an overall normalization of `1/n` between the two transforms. This\nis required to make `irfft()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompare against the full output from `fft()`:\n\nNotice that the symmetric element `T[-1] == T[1].conj()` is omitted. At the\nNyquist frequency `T[-2] == T[2]` is it\u2019s own symmetric pair, and therefore\nmust always be real-valued.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": "\nComputes the 2-dimensional discrete Fourier transform of real `input`.\nEquivalent to `rfftn()` but FFTs only the last two dimensions by default.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i, j] = conj(X[-i, -j])`,\nso the full `fft2()` output contains redundant information. `rfft2()` instead\nomits the negative frequencies in the last dimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfft2()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfft2()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfft2()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fft2()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfft2()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": "\nComputes the sample frequencies for `rfft()` with a signal of size `n`.\n\nNote\n\n`rfft()` returns Hermitian one-sided output, so only the positive frequency\nterms are returned. For a real FFT of length `n` and with inputs spaced in\nlength unit `d`, the frequencies are:\n\nNote\n\nFor even lengths, the Nyquist frequency at `f[n/2]` can be thought of as\neither negative or positive. Unlike `fftfreq()`, `rfftfreq()` always returns\nit as positive.\n\nCompared to the output from `fftfreq()`, we see that the Nyquist frequency at\n`f[2]` has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500,\n-0.5000, -0.2500])\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": "\nComputes the N-dimensional discrete Fourier transform of real `input`.\n\nThe FFT of a real signal is Hermitian-symmetric, `X[i_1, ..., i_n] =\nconj(X[-i_1, ..., -i_n])` so the full `fftn()` output contains redundant\ninformation. `rfftn()` instead omits the negative frequencies in the last\ndimension.\n\nnorm (str, optional) \u2013\n\nNormalization mode. For the forward transform (`rfftn()`), these correspond\nto:\n\nWhere `n = prod(s)` is the logical FFT size. Calling the backward transform\n(`irfftn()`) with the same normalization mode will apply an overall\nnormalization of `1/n` between the two transforms. This is required to make\n`irfftn()` the exact inverse.\n\nDefault is `\"backward\"` (no normalization).\n\nCompared against the full output from `fftn()`, we have all elements up to the\nNyquist frequency.\n\nThe discrete Fourier transform is separable, so `rfftn()` here is equivalent\nto a combination of `fft()` and `rfft()`:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": "\nAlias for `torch.trunc()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": "\nFlattens `input` by reshaping it into a one-dimensional tensor. If `start_dim`\nor `end_dim` are passed, only dimensions starting with `start_dim` and ending\nwith `end_dim` are flattened. The order of elements in `input` is unchanged.\n\nUnlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may\nreturn the original object, a view, or copy. If no dimensions are flattened,\nthen the original object `input` is returned. Otherwise, if input can be\nviewed as the flattened shape, then that view is returned. Finally, only if\nthe input cannot be viewed as the flattened shape is input\u2019s data copied. See\n`torch.Tensor.view()` for details on when a view will be returned.\n\nNote\n\nFlattening a zero-dimensional tensor will return a one-dimensional view.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": "\nReverse the order of a n-D tensor along given axis in dims.\n\nNote\n\n`torch.flip` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.flip`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.flip` is expected to be\nslower than `np.flip`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": "\nFlip tensor in the left/right direction, returning a new tensor.\n\nFlip the entries in each row in the left/right direction. Columns are\npreserved, but appear in a different order than before.\n\nNote\n\nRequires the tensor to be at least 2-D.\n\nNote\n\n`torch.fliplr` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.fliplr`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.fliplr` is expected to be\nslower than `np.fliplr`.\n\ninput (Tensor) \u2013 Must be at least 2-dimensional.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": "\nFlip tensor in the up/down direction, returning a new tensor.\n\nFlip the entries in each column in the up/down direction. Rows are preserved,\nbut appear in a different order than before.\n\nNote\n\nRequires the tensor to be at least 1-D.\n\nNote\n\n`torch.flipud` makes a copy of `input`\u2019s data. This is different from NumPy\u2019s\n`np.flipud`, which returns a view in constant time. Since copying a tensor\u2019s\ndata is more work than viewing that data, `torch.flipud` is expected to be\nslower than `np.flipud`.\n\ninput (Tensor) \u2013 Must be at least 1-dimensional.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\nCasts this storage to bool type\n\nCasts this storage to byte type\n\nCasts this storage to char type\n\nReturns a copy of this storage\n\nCasts this storage to complex double type\n\nCasts this storage to complex float type\n\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\nCasts this storage to double type\n\nCasts this storage to float type\n\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n`size` is the number of elements in the storage. If `shared` is `False`, then\nthe file must contain at least `size * sizeof(Type)` bytes (`Type` is the type\nof storage). If `shared` is `True` the file will be created if needed.\n\nCasts this storage to half type\n\nCasts this storage to int type\n\nCasts this storage to long type\n\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\nMoves the storage to shared memory.\n\nThis is a no-op for storages already in shared memory and for CUDA storages,\nwhich do not need to be moved for sharing across processes. Storages in shared\nmemory cannot be resized.\n\nReturns: self\n\nCasts this storage to short type\n\nReturns a list containing the elements of this storage\n\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": "\nCasts this storage to bfloat16 type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": "\nCasts this storage to bool type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": "\nCasts this storage to byte type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": "\nCasts this storage to char type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": "\nReturns a copy of this storage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": "\nCasts this storage to complex double type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": "\nCasts this storage to complex float type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": "\nReturns a CPU copy of this storage if it\u2019s not already on the CPU\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": "\nReturns a copy of this object in CUDA memory.\n\nIf this object is already in CUDA memory and on the correct device, then no\ncopy is performed and the original object is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": "\nCasts this storage to double type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": "\nCasts this storage to float type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": "\nIf `shared` is `True`, then memory is shared between all processes. All\nchanges are written to the file. If `shared` is `False`, then the changes on\nthe storage do not affect the file.\n\n`size` is the number of elements in the storage. If `shared` is `False`, then\nthe file must contain at least `size * sizeof(Type)` bytes (`Type` is the type\nof storage). If `shared` is `True` the file will be created if needed.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": "\nCasts this storage to half type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": "\nCasts this storage to int type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": "\nCasts this storage to long type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": "\nCopies the storage to pinned memory, if it\u2019s not already pinned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": "\nMoves the storage to shared memory.\n\nThis is a no-op for storages already in shared memory and for CUDA storages,\nwhich do not need to be moved for sharing across processes. Storages in shared\nmemory cannot be resized.\n\nReturns: self\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": "\nCasts this storage to short type\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": "\nReturns a list containing the elements of this storage\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": "\nReturns the type if `dtype` is not provided, else casts this object to the\nspecified type.\n\nIf this is already of the correct type, no copy is performed and the original\nobject is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": "\nRaises `input` to the power of `exponent`, elementwise, in double precision.\nIf neither input is complex returns a `torch.float64` tensor, and if one or\nmore inputs is complex returns a `torch.complex128` tensor.\n\nNote\n\nThis function always computes in double precision, unlike `torch.pow()`, which\nimplements more typical type promotion. This is useful when the computation\nneeds to be performed in a wider or more precise dtype, or the results of the\ncomputation may contain fractional values not representable in the input\ndtypes, like when an integer base is raised to a negative integer exponent.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": "\nReturns a new tensor with the floor of the elements of `input`, the largest\ninteger less than or equal to each element.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": "\nWarning\n\nThis function\u2019s name is a misnomer. It actually rounds the quotient towards\nzero instead of taking its floor. This behavior will be deprecated in a future\nPyTorch release.\n\nComputes `input` divided by `other`, elementwise, and rounds each quotient\ntowards zero. Equivalently, it truncates the quotient(s):\n\nSupports broadcasting to a common shape, type promotion, and integer and float\ninputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\nThis is like `torch.maximum()` except it handles NaNs differently: if exactly\none of the two elements being compared is a NaN then the non-NaN element is\ntaken as the maximum. Only if both elements are NaN is NaN propagated.\n\nThis function is a wrapper around C++\u2019s `std::fmax` and is similar to NumPy\u2019s\n`fmax` function.\n\nSupports broadcasting to a common shape, type promotion, and integer and\nfloating-point inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\nThis is like `torch.minimum()` except it handles NaNs differently: if exactly\none of the two elements being compared is a NaN then the non-NaN element is\ntaken as the minimum. Only if both elements are NaN is NaN propagated.\n\nThis function is a wrapper around C++\u2019s `std::fmin` and is similar to NumPy\u2019s\n`fmin` function.\n\nSupports broadcasting to a common shape, type promotion, and integer and\nfloating-point inputs.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": "\nComputes the element-wise remainder of division.\n\nThe dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend `input`.\n\nSupports broadcasting to a common shape, type promotion, and integer and float\ninputs.\n\nNote\n\nWhen the divisor is zero, returns `NaN` for floating point dtypes on both CPU\nand GPU; raises `RuntimeError` for integer division by zero on CPU; Integer\ndivision by zero on GPU may return any value.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": "\nComputes the fractional portion of each element in `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": "\nCreates a `Tensor` from a `numpy.ndarray`.\n\nThe returned tensor and `ndarray` share the same memory. Modifications to the\ntensor will be reflected in the `ndarray` and vice versa. The returned tensor\nis not resizable.\n\nIt currently accepts `ndarray` with dtypes of `numpy.float64`,\n`numpy.float32`, `numpy.float16`, `numpy.complex64`, `numpy.complex128`,\n`numpy.int64`, `numpy.int32`, `numpy.int16`, `numpy.int8`, `numpy.uint8`, and\n`numpy.bool`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": "\nCreates a tensor of size `size` filled with `fill_value`. The tensor\u2019s dtype\nis inferred from `fill_value`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": "\nReturns a tensor with the same size as `input` filled with `fill_value`.\n`torch.full_like(input, fill_value)` is equivalent to\n`torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout,\ndevice=input.device)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": "\nWarning\n\nThe `torch.futures` package is experimental and subject to change.\n\nThis package provides a `Future` type that encapsulates an asynchronous\nexecution and a set of utility functions to simplify operations on `Future`\nobjects. Currently, the `Future` type is primarily used by the Distributed RPC\nFramework.\n\nWrapper around a `torch._C.Future` which encapsulates an asynchronous\nexecution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to\nadd callback functions and set results.\n\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\nCollects the provided `Future` objects into a single combined `Future` that is\ncompleted when all of the sub-futures are completed.\n\nfutures (list) \u2013 a list of `Future` objects.\n\nReturns a `Future` object to a list of the passed in Futures.\n\nWaits for all provided futures to be complete, and returns the list of\ncompleted values.\n\nfutures (list) \u2013 a list of `Future` object.\n\nA list of the completed `Future` results. This method will throw an error if\n`wait` on any `Future` throws.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": "\nCollects the provided `Future` objects into a single combined `Future` that is\ncompleted when all of the sub-futures are completed.\n\nfutures (list) \u2013 a list of `Future` objects.\n\nReturns a `Future` object to a list of the passed in Futures.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": "\nWrapper around a `torch._C.Future` which encapsulates an asynchronous\nexecution of a callable, e.g. `rpc_async()`. It also exposes a set of APIs to\nadd callback functions and set results.\n\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": "\nReturn `True` if this `Future` is done. A `Future` is done if it has a result\nor an exception.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": "\nSet an exception for this `Future`, which will mark this `Future` as completed\nwith an error and trigger all attached callbacks. Note that when calling\nwait()/value() on this `Future`, the exception set here will be raised inline.\n\nresult (BaseException) \u2013 the exception for this `Future`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": "\nSet the result for this `Future`, which will mark this `Future` as completed\nand trigger all attached callbacks. Note that a `Future` cannot be marked\ncompleted twice.\n\nresult (object) \u2013 the result object of this `Future`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": "\nAppend the given callback function to this `Future`, which will be run when\nthe `Future` is completed. Multiple callbacks can be added to the same\n`Future`, and will be invoked in the same order as they were added. The\ncallback must take one argument, which is the reference to this `Future`. The\ncallback function can use the `Future.wait()` API to get the value. Note that\nif this `Future` is already completed, the given callback will be run\nimmediately inline.\n\ncallback (`Callable`) \u2013 a `Callable` that takes this `Future` as the only\nargument.\n\nA new `Future` object that holds the return value of the `callback` and will\nbe marked as completed when the given `callback` finishes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": "\nBlock until the value of this `Future` is ready.\n\nThe value held by this `Future`. If the function (callback or RPC) creating\nthe value has thrown an error, this `wait` method will also throw an error.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": "\nWaits for all provided futures to be complete, and returns the list of\ncompleted values.\n\nfutures (list) \u2013 a list of `Future` object.\n\nA list of the completed `Future` results. This method will throw an error if\n`wait` on any `Future` throws.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": "\nThis feature is under a Beta release and its API may change.\n\nFX is a toolkit for developers to use to transform `nn.Module` instances. FX\nconsists of three main components: a symbolic tracer, an intermediate\nrepresentation, and Python code generation. A demonstration of these\ncomponents in action:\n\nThe symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds\nfake values, called Proxies, through the code. Operations on theses Proxies\nare recorded. More information about symbolic tracing can be found in the\n`symbolic_trace()` and `Tracer` documentation.\n\nThe intermediate representation is the container for the operations that were\nrecorded during symbolic tracing. It consists of a list of Nodes that\nrepresent function inputs, callsites (to functions, methods, or\n`torch.nn.Module` instances), and return values. More information about the IR\ncan be found in the documentation for `Graph`. The IR is the format on which\ntransformations are applied.\n\nPython code generation is what makes FX a Python-to-Python (or Module-to-\nModule) transformation toolkit. For each Graph IR, we can create valid Python\ncode matching the Graph\u2019s semantics. This functionality is wrapped up in\n`GraphModule`, which is a `torch.nn.Module` instance that holds a `Graph` as\nwell as a `forward` method generated from the Graph.\n\nTaken together, this pipeline of components (symbolic tracing \u2192 intermediate\nrepresentation \u2192 transforms \u2192 Python code generation) constitutes the Python-\nto-Python transformation pipeline of FX. In addition, these components can be\nused separately. For example, symbolic tracing can be used in isolation to\ncapture a form of the code for analysis (and not transformation) purposes.\nCode generation can be used for programmatically generating models, for\nexample from a config file. There are many uses for FX!\n\nSeveral example transformations can be found at the examples repository.\n\nWhat is an FX transform? Essentially, it\u2019s a function that looks like this.\n\nYour transform will take in an `torch.nn.Module`, acquire a `Graph` from it,\ndo some modifications, and return a new `torch.nn.Module`. You should think of\nthe `torch.nn.Module` that your FX transform returns as identical to a regular\n`torch.nn.Module` \u2013 you can pass it to another FX transform, you can pass it\nto TorchScript, or you can run it. Ensuring that the inputs and outputs of\nyour FX transform are a `torch.nn.Module` will allow for composability.\n\nNote\n\nIt is also possible to modify an existing `GraphModule` instead of creating a\nnew one, like so:\n\nNote that you MUST call `GraphModule.recompile()` to bring the generated\n`forward()` method on the `GraphModule` in sync with the modified `Graph`.\n\nGiven that you\u2019ve passed in a `torch.nn.Module` that has been traced into a\n`Graph`, there are now two primary approaches you can take to building a new\n`Graph`.\n\nFull treatment of the semantics of graphs can be found in the `Graph`\ndocumentation, but we are going to cover the basics here. A `Graph` is a data\nstructure that represents a method on a `GraphModule`. The information that\nthis requires is:\n\nAll three of these concepts are represented with `Node` instances. Let\u2019s see\nwhat we mean by that with a short example:\n\nHere we define a module `MyModule` for demonstration purposes, instantiate it,\nsymbolically trace it, then call the `Graph.print_tabular()` method to print\nout a table showing the nodes of this `Graph`:\n\nopcode\n\nname\n\ntarget\n\nargs\n\nkwargs\n\nplaceholder\n\nx\n\nx\n\n()\n\n{}\n\nget_attr\n\nlinear_weight\n\nlinear.weight\n\n()\n\n{}\n\ncall_function\n\nadd_1\n\n<built-in function add>\n\n(x, linear_weight)\n\n{}\n\ncall_module\n\nlinear_1\n\nlinear\n\n(add_1,)\n\n{}\n\ncall_method\n\nrelu_1\n\nrelu\n\n(linear_1,)\n\n{}\n\ncall_function\n\nsum_1\n\n<built-in method sum \u2026>\n\n(relu_1,)\n\n{\u2018dim\u2019: -1}\n\ncall_function\n\ntopk_1\n\n<built-in method topk \u2026>\n\n(sum_1, 3)\n\n{}\n\noutput\n\noutput\n\noutput\n\n(topk_1,)\n\n{}\n\nWe can use this information to answer the questions we posed above.\n\nGiven that we now know the basics of how code is represented in FX, we can now\nexplore how we would edit a `Graph`.\n\nOne approach to building this new `Graph` is to directly manipulate your old\none. To aid in this, we can simply take the `Graph` we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\n`torch.add()` calls with `torch.mul()` calls.\n\nWe can also do more involved `Graph` rewrites, such as deleting or appending\nnodes. To aid in these transformations, FX has utility functions for\ntransforming the graph that can be found in the `Graph` documentation. An\nexample of using these APIs to append a `torch.relu()` call can be found\nbelow.\n\nFor simple transformations that only consist of substitutions, you can also\nmake use of the subgraph rewriter.\n\nFX also provides another level of automation on top of direct graph\nmanipulation. The `replace_pattern()` API is essentially a \u201cfind/replace\u201d tool\nfor editing `Graph`s. It allows you to specify a `pattern` and `replacement`\nfunction and it will trace through those functions, find instances of the\ngroup of operations in the `pattern` graph, and replace those instances with\ncopies of the `replacement` graph. This can help to greatly automate tedious\ngraph manipulation code, which can get unwieldy as the transformations get\nmore complex.\n\nAnother way of manipulating `Graph`s is by reusing the `Proxy` machinery used\nin symbolic tracing. For example, let\u2019s imagine that we wanted to write a\ntransformation that decomposed PyTorch functions into smaller operations. It\nwould transform every `F.relu(x)` call into `(x > 0) * x`. One possibility\nwould be to perform the requisite graph rewriting to insert the comparison and\nmultiplication after the `F.relu`, and then clean up the original `F.relu`.\nHowever, we can automate this process by using `Proxy` objects to\nautomatically record operations into the `Graph`.\n\nTo use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with `Proxy` objects as arugments. These\n`Proxy` objects will capture the operations that are performed on them and\nappend them to the `Graph`.\n\nIn addition to avoiding explicit graph manipulation, using `Proxy`s also\nallows you to specify your rewrite rules as native Python code. For\ntransformations that require a large amount of rewrite rules (such as vmap or\ngrad), this can often improve readability and maintainability of the rules.\n\nA worked example of using `Proxy`s for `Graph` manipulation can be found here.\n\nA useful code organizational pattern in FX is to loop over all the `Node`s in\na `Graph` and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the\ncode via retracing with `Proxy`s. For example, suppose we want to run a\n`GraphModule` and record the `torch.Tensor` shape and dtype properties on the\nnodes as we see them at runtime. That might look like:\n\nAs you can see, a full interpreter for FX is not that complicated but it can\nbe very useful. To ease using this pattern, we provide the `Interpreter`\nclass, which encompasses the above logic in a way that certain aspects of the\ninterpreter\u2019s execution can be overridden via method overrides.\n\nIn addition to executing operations, we can also generate a new `Graph` by\nfeeding `Proxy` values through an interpreter. Similarly, we provide the\n`Transformer` class to encompass this pattern. `Transformer` behaves similarly\nto `Interpreter`, but instead of calling the `run` method to get a concrete\noutput value from the Module, you would call the `Transformer.transform()`\nmethod to return a new `GraphModule` which was subject to any transformation\nrules you installed as overridden methods.\n\nOften in the course of authoring transformations, our code will not be quite\nright. In this case, we may need to do some debugging. The key is to work\nbackwards: first, check the results of invoking the generated module to prove\nor disprove correctness. Then, inspect and debug the generated code. Then,\ndebug the process of transformations that led to the generated code.\n\nIf you\u2019re not familiar with debuggers, please see the auxiliary section\nAvailable Debuggers.\n\nBecause the output of most deep learning modules consists of floating point\n`torch.Tensor` instances, checking for equivalence between the results of two\n`torch.nn.Module` is not as straightforward as doing a simple equality check.\nTo motivate this, let\u2019s use an example:\n\nHere, we\u2019ve tried to check equality of the values of two deep learning models\nwith the `==` equality operator. However, this is not well- defined both due\nto the issue of that operator returning a tensor and not a bool, but also\nbecause comparison of floating point values should use a margin of error (or\nepsilon) to account for the non-commutativity of floating point operations\n(see here for more details). We can use `torch.allclose()` instead, which will\ngive us an approximate comparison taking into account a relative and absolute\ntolerance threshold:\n\nThis is the first tool in our toolbox to check if transformed modules are\nbehaving as we expect compared to a reference implementation.\n\nBecause FX generates the `forward()` function on `GraphModule`s, using\ntraditional debugging techniques like `print` statements or `pdb` is not as\nstraightfoward. Luckily, we have several techniques we can use for debugging\nthe generated code.\n\nInvoke `pdb` to step into the running program. Although the code that\nrepresents the `Graph` is not in any source file, we can still step into it\nmanually using `pdb` when the forward pass is invoked.\n\nIf you\u2019d like to run the same code multiple times, then it can be a bit\ntedious to step to the right code with `pdb`. In that case, one approach is to\nsimply copy-paste the generated `forward` pass into your code and examine it\nfrom there.\n\n`GraphModule.to_folder()` is a method in `GraphModule` that allows you to dump\nout the generated FX code to a folder. Although copying the forward pass into\nthe code often suffices as in Print the Generated Code, it may be easier to\nexamine modules and parameters using `to_folder`.\n\nAfter running the above example, we can then look at the code within\n`foo/module.py` and modify it as desired (e.g. adding `print` statements or\nusing `pdb`) to debug the generated code.\n\nNow that we\u2019ve identified that a transformation is creating incorrect code,\nit\u2019s time to debug the transformation itself. First, we\u2019ll check the\nLimitations of Symbolic Tracing section in the documentation. Once we verify\nthat tracing is working as expected, the goal becomes figuring out what went\nwrong during our `GraphModule` transformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to examine our\ntraced module:\n\nUsing the utility functions above, we can compare our traced Module before and\nafter we\u2019ve applied our transformations. Sometimes, a simple visual comparison\nis enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a\ndebugger like `pdb` can be a good next step.\n\nGoing off of the example above, consider the following code:\n\nUsing the above example, let\u2019s say that the call to `print(traced)` showed us\nthat there was an error in our transforms. We want to find what goes wrong\nusing a debugger. We start a `pdb` session. We can see what\u2019s happening during\nthe transform by breaking on `transform_graph(traced)`, then pressing `s` to\n\u201cstep into\u201d the call to `transform_graph(traced)`.\n\nWe may also have good luck by editing the `print_tabular` method to print\ndifferent attributes of the Nodes in the Graph. (For example, we might want to\nsee the Node\u2019s `input_nodes` and `users`.)\n\nThe most common Python debugger is pdb. You can start your program in \u201cdebug\nmode\u201d with `pdb` by typing `python -m pdb FILENAME.py` into the command line,\nwhere `FILENAME` is the name of the file you want to debug. After that, you\ncan use the `pdb` debugger commands to move through your running program\nstepwise. It\u2019s common to set a breakpoint (`b LINE-NUMBER`) when you start\n`pdb`, then call `c` to run the program until that point. This prevents you\nfrom having to step through each line of execution (using `s` or `n`) to get\nto the part of the code you want to examine. Alternatively, you can write\n`import pdb; pdb.set_trace()` before the line you want to break at. If you add\n`pdb.set_trace()`, your program will automatically start in debug mode when\nyou run it. (In other words, you can just type `python FILENAME.py` into the\ncommand line instead of `python -m pdb FILENAME.py`.) Once you\u2019re running your\nfile in debug mode, you can step through the code and examine your program\u2019s\ninternal state using certain commands. There are many excellent tutorials on\n`pdb` online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d.\n\nIDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you\ncan choose to either a) use `pdb` by pulling up a terminal window in your IDE\n(e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a\ngraphical wrapper around `pdb`).\n\nFX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the\nsemantics of programs in a transformable/analyzable form. The system is\ntracing in that it executes the program (really a `torch.nn.Module` or\nfunction) to record operations. It is symbolic in that the data flowing\nthrough the program during this execution is not real data, but rather symbols\n(`Proxy` in FX parlance).\n\nAlthough symbolic tracing works for most neural net code, it has some\nlimitations.\n\nThe main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or `if` statements where the condition\nmay depend on the input values of the program.\n\nFor example, let\u2019s examine the following program:\n\nThe condition to the `if` statement relies on the value of `dim0`, which\neventually relies on the value of `x`, a function input. Since `x` can change\n(i.e. if you pass a new input tensor to the traced function), this is dynamic\ncontrol flow. The traceback walks back up through your code to show you where\nthis situation happens.\n\nOn the other hand, so-called static control flow is supported. Static control\nflow is loops or `if` statements whose value cannot change across invocations.\nTypically, in PyTorch programs, this control flow arises for code making\ndecisions about a model\u2019s architecture based on hyper-parameters. As a\nconcrete example:\n\nThe if-statement `if self.do_activation` does not depend on any function\ninputs, thus it is static. `do_activation` can be considered to be a hyper-\nparameter, and the traces of different instances of `MyModule` with different\nvalues for that parameter have different code. This is a valid pattern that is\nsupported by symbolic tracing.\n\nMany instances of dynamic control flow are semantically static control flow.\nThese instances can be made to support symbolic tracing by removing the data\ndependencies on input values, for example by moving values to `Module`\nattributes or by passing constant values during symbolic tracing:\n\nIn the case of truly dynamic control flow, the sections of the program that\ncontain this code can be traced as calls to the Method (see Customizing\nTracing with the Tracer class) or function (see `wrap()`) rather than tracing\nthrough them.\n\nFX uses `__torch_function__` as the mechanism by which it intercepts calls\n(see the technical overview for more information about this). Some functions,\nsuch as builtin Python functions or those in the `math` module, are things\nthat are not covered by `__torch_function__`, but we would still like to\ncapture them in symbolic tracing. For example:\n\nThe error tells us that the built-in function `len` is not supported. We can\nmake it so that functions like this are recorded in the trace as direct calls\nusing the `wrap()` API:\n\nThe `Tracer` class is the class that underlies the implementation of\n`symbolic_trace`. The behavior of tracing can be customized by subclassing\nTracer, like so:\n\nLeaf Modules are the modules that appear as calls in the symbolic trace rather\nthan being traced through. The default set of leaf modules is the set of\nstandard `torch.nn` module instances. For example:\n\nThe set of leaf modules can be customized by overriding\n`Tracer.is_leaf_module()`.\n\nTensor constructors (e.g. `torch.zeros`, `torch.ones`, `torch.rand`,\n`torch.randn`, `torch.sparse_coo_tensor`) are currently not traceable.\n\nType annotations\n\nSymbolic tracing API\n\nGiven an `nn.Module` or function instance `root`, this function will return a\n`GraphModule` constructed by recording operations seen while tracing through\n`root`.\n\na Module created from the recorded operations from `root`.\n\nGraphModule\n\nThis function can be called at module-level scope to register fn_or_name as a\n\u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in\nthe FX trace instead of being traced through:\n\nThis function can also equivalently be used as a decorator:\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the\nconcept of \u201cleaf modules\u201d, that is, they are functions that are left as calls\nin the FX trace rather than traced through.\n\nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global\nfunction to insert into the graph when it\u2019s called\n\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n`graph` attribute, as well as `code` and `forward` attributes generated from\nthat `graph`.\n\nWarning\n\nWhen `graph` is reassigned, `code` and `forward` will be automatically\nregenerated. However, if you edit the contents of the `graph` without\nreassigning the `graph` attribute itself, you must call `recompile()` to\nupdate the generated code.\n\nConstruct a GraphModule.\n\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\nReturn the `Graph` underlying this `GraphModule`\n\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n`Graph` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of `Node` s, each representing callsites (or other\nsyntactic constructs). The list of `Node` s, taken together, constitute a\nvalid Python function.\n\nFor example, the following code\n\nWill produce the following Graph:\n\nFor the semantics of operations represented in the `Graph`, please see `Node`.\n\nConstruct an empty Graph.\n\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nPrints the intermediate representation of the graph in tabular format.\n\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n`Node` is the data structure that represents individual operations within a\n`Graph`. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each `Node` has a function specified by\nits `op` property. The `Node` semantics for each value of `op` are as follows:\n\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n`Tracer` is the class that implements the symbolic tracing functionality of\n`torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to\n`Tracer().trace(m)`.\n\nTracer can be subclassed to override various behaviors of the tracing process.\nThe different behaviors that can be overridden are described in the docstrings\nof the methods on this class.\n\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n`Proxy` objects are `Node` wrappers that flow through the program during\nsymbolic tracing and record all the operations (`torch` function calls, method\ncalls, operators) that they touch into the growing FX Graph.\n\nIf you\u2019re doing graph transforms, you can wrap your own `Proxy` method around\na raw `Node` so that you can use the overloaded operators to add additional\nthings to a `Graph`.\n\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful\nfor many things, including writing code transformations as well as analysis\npasses.\n\nMethods in the Interpreter class can be overridden to customize the behavior\nof execution. The map of overrideable methods in terms of call hierarchy:\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\nInterpreter like so:\n\nmodule (GraphModule) \u2013 The module to be executed\n\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n`Transformer` is a special type of interpreter that produces a new `Module`.\nIt exposes a `transform()` method that returns the transformed `Module`.\n`Transformer` does not require arguments to run, as `Interpreter` does.\n`Transformer` works entirely symbolically.\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\n`Transformer` like so:\n\nmodule (GraphModule) \u2013 The `Module` to be transformed.\n\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\nTransform `self.module` and return the transformed `GraphModule`.\n\nMatches all possible non-overlapping sets of operators and their data\ndependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces\neach of these matched subgraphs with another subgraph (`replacement`).\n\nA list of `Match` objects representing the places in the original graph that\n`pattern` was matched to. The list is empty if there are no matches. `Match`\nis defined as:\n\nList[Match]\n\nExamples:\n\nThe above code will first match `pattern` in the `forward` method of\n`traced_module`. Pattern-matching is done based on use-def relationships, not\nnode names. For example, if you had `p = torch.cat([a, b])` in `pattern`, you\ncould match `m = torch.cat([a, b])` in the original `forward` function,\ndespite the variable names being different (`p` vs `m`).\n\nThe `return` statement in `pattern` is matched based on its value only; it may\nor may not match to the `return` statement in the larger graph. In other\nwords, the pattern doesn\u2019t have to extend to the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger function and\nreplaced by `replacement`. If there are multiple matches for `pattern` in the\nlarger function, each non-overlapping match will be replaced. In the case of a\nmatch overlap, the first found match in the set of overlapping matches will be\nreplaced. (\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node is the\nparameter that appears directly after `self`, while the last Node is whatever\nthe function returns.)\n\nOne important thing to note is that the parameters of the `pattern` Callable\nmust be used in the Callable itself, and the parameters of the `replacement`\nCallable must match the pattern. The first rule is why, in the above code\nblock, the `forward` function has parameters `x, w1, w2`, but the `pattern`\nfunction only has parameters `w1, w2`. `pattern` doesn\u2019t use `x`, so it\nshouldn\u2019t specify `x` as a parameter. As an example of the second rule,\nconsider replacing\n\nwith\n\nIn this case, `replacement` needs the same number of parameters as `pattern`\n(both `x` and `y`), even though the parameter `y` isn\u2019t used in `replacement`.\n\nAfter calling `subgraph_rewriter.replace_pattern`, the generated Python code\nlooks like this:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": "\n`Graph` is the main data structure used in the FX Intermediate Representation.\nIt consists of a series of `Node` s, each representing callsites (or other\nsyntactic constructs). The list of `Node` s, taken together, constitute a\nvalid Python function.\n\nFor example, the following code\n\nWill produce the following Graph:\n\nFor the semantics of operations represented in the `Graph`, please see `Node`.\n\nConstruct an empty Graph.\n\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\nPrints the intermediate representation of the graph in tabular format.\n\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": "\nInsert a `call_function` `Node` into the `Graph`. A `call_function` node\nrepresents a call to a Python callable, specified by `the_function`.\n`the_function` can be\n\nReturns\n\nThe newly created and inserted `call_function` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": "\nInsert a `call_method` `Node` into the `Graph`. A `call_method` node\nrepresents a call to a given method on the 0th element of `args`.\n\nThe newly created and inserted `call_method` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": "\nInsert a `call_module` `Node` into the `Graph`. A `call_module` node\nrepresents a call to the forward() function of a `Module` in the `Module`\nhierarchy.\n\nThe newly-created and inserted `call_module` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": "\nCreate a `Node` and add it to the `Graph` at the current insert-point. Note\nthat the current insert-point can be set via `Graph.inserting_before()` and\n`Graph.inserting_after()`.\n\nThe newly-created and inserted node.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": "\nErases a `Node` from the `Graph`. Throws an exception if there are still users\nof that node in the `Graph`.\n\nto_erase (Node) \u2013 The `Node` to erase from the `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": "\nInsert a `get_attr` node into the Graph. A `get_attr` `Node` represents the\nfetch of an attribute from the `Module` hierarchy.\n\nThe newly-created and inserted `get_attr` node.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": "\nCopy all nodes from a given graph into `self`.\n\nThe value in `self` that is now equivalent to the output value in `g`, if `g`\nhad an `output` node. `None` otherwise.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nafter the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": "\nSet the point at which create_node and companion methods will insert into the\ngraph. When used within a \u2018with\u2019 statement, this will temporary set the insert\npoint and then restore it when the with statement exits:\n\nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert\nbefore the beginning of the entire graph.\n\nA resource manager that will restore the insert point on `__exit__`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": "\nRuns various checks on this Graph to make sure it is well-formed. In\nparticular: - Checks Nodes have correct ownership (owned by this graph) -\nChecks Nodes appear in topological order - If `root` is provided, checks that\ntargets exist in `root`\n\nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for\ntargets. This is equivalent to the `root` argument that is passed when\nconstructing a `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": "\nGet the list of Nodes that constitute this Graph.\n\nNote that this `Node` list representation is a doubly-linked list. Mutations\nduring iteration (e.g. delete a Node, add a Node) are safe.\n\nA doubly-linked list of Nodes. Note that `reversed` can be called on this list\nto switch iteration order.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": "\nCopy a node from one graph into another. `arg_transform` needs to transform\narguments from the graph of node to the graph of self. Example:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": "\nInsert an `output` `Node` into the `Graph`. An `output` node represents a\n`return` statement in Python code. `result` is the value that should be\nreturned.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": "\nInsert a `placeholder` node into the Graph. A `placeholder` represents a\nfunction input.\n\nNote\n\nThe same insertion point and type expression rules apply for this method as\n`Graph.create_node`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": "\nPrints the intermediate representation of the graph in tabular format.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": "\nTurn this `Graph` into valid Python code.\n\nroot_module (str) \u2013 The name of the root module on which to look-up qualified\nname targets. This is usually \u2018self\u2019.\n\nThe string source code generated from this `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": "\nConstruct an empty Graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": "\nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a\n`graph` attribute, as well as `code` and `forward` attributes generated from\nthat `graph`.\n\nWarning\n\nWhen `graph` is reassigned, `code` and `forward` will be automatically\nregenerated. However, if you edit the contents of the `graph` without\nreassigning the `graph` attribute itself, you must call `recompile()` to\nupdate the generated code.\n\nConstruct a GraphModule.\n\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\nReturn the `Graph` underlying this `GraphModule`\n\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": "\nReturn the Python code generated from the `Graph` underlying this\n`GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": "\nReturn the `Graph` underlying this `GraphModule`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": "\nRecompile this GraphModule from its `graph` attribute. This should be called\nafter editing the contained `graph`, otherwise the generated code of this\n`GraphModule` will be out of date.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": "\nDumps out module to `folder` with `module_name` so that it can be imported\nwith `from <folder> import <module_name>`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": "\nConstruct a GraphModule.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": "\nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful\nfor many things, including writing code transformations as well as analysis\npasses.\n\nMethods in the Interpreter class can be overridden to customize the behavior\nof execution. The map of overrideable methods in terms of call hierarchy:\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\nInterpreter like so:\n\nmodule (GraphModule) \u2013 The module to be executed\n\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": "\nExecute a `call_function` node and return the result.\n\nAny: The value returned by the function invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": "\nExecute a `call_method` node and return the result.\n\nAny: The value returned by the method invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": "\nExecute a `call_module` node and return the result.\n\nAny: The value returned by the module invocation\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": "\nFetch the concrete values of `args` and `kwargs` of node `n` from the current\nexecution environment.\n\nn (Node) \u2013 The node for which `args` and `kwargs` should be fetched.\n\n`args` and `kwargs` with concrete values for `n`.\n\nTuple[Tuple, Dict]\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": "\nFetch an attribute from the `Module` hierarchy of `self.module`.\n\ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch\n\nThe value of the attribute.\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. Will retrieve an attribute value from the `Module`\nhierarchy of `self.module`.\n\nThe value of the attribute that was retrieved\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": "\nRecursively descend through `args` and look up the concrete value for each\n`Node` in the current execution environment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": "\nExecute an `output` node. This really just retrieves the value referenced by\nthe `output` node and returns it.\n\nThe return value referenced by the output node\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. Note that this is stateful: `Interpreter`\nmaintains an internal iterator over arguments passed to `run` and this method\nreturns next() on that iterator.\n\nThe argument value that was retrieved.\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": "\nRun `module` via interpretation and return the result.\n\nThe value returned from executing the Module\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": "\nRun a specific node `n` and return the result. Calls into placeholder,\nget_attr, call_function, call_method, call_module, or output depending on\n`node.op`\n\nn (Node) \u2013 The Node to execute\n\nThe result of executing `n`\n\nAny\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": "\n`Node` is the data structure that represents individual operations within a\n`Graph`. For the most part, Nodes represent callsites to various entities,\nsuch as operators, methods, and Modules (some exceptions include nodes that\nspecify function inputs and outputs). Each `Node` has a function specified by\nits `op` property. The `Node` semantics for each value of `op` are as follows:\n\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": "\nReturn all Nodes that are inputs to this Node. This is equivalent to iterating\nover `args` and `kwargs` and only collecting the values that are Nodes.\n\nList of `Nodes` that appear in the `args` and `kwargs` of this `Node`, in that\norder.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": "\nInsert x after this node in the list of nodes in the graph. Equvalent to\n`self.next.prepend(x)`\n\nx (Node) \u2013 The node to put after this node. Must be a member of the same\ngraph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": "\nThe tuple of arguments to this `Node`. The interpretation of arguments depends\non the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": "\nThe dict of keyword arguments to this `Node`. The interpretation of arguments\ndepends on the node\u2019s opcode. See the `Node` docstring for more information.\n\nAssignment to this property is allowed. All accounting of uses and users is\nupdated automatically on assignment.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": "\nReturns the next `Node` in the linked list of Nodes.\n\nThe next `Node` in the linked list of Nodes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": "\nInsert x before this node in the list of nodes in the graph. Example:\n\nx (Node) \u2013 The node to put before this node. Must be a member of the same\ngraph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": "\nReturns the previous `Node` in the linked list of Nodes.\n\nThe previous `Node` in the linked list of Nodes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": "\nReplace all uses of `self` in the Graph with the Node `replace_with`.\n\nreplace_with (Node) \u2013 The node to replace all uses of `self` with.\n\nThe list of Nodes on which this change was made.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": "\n`Proxy` objects are `Node` wrappers that flow through the program during\nsymbolic tracing and record all the operations (`torch` function calls, method\ncalls, operators) that they touch into the growing FX Graph.\n\nIf you\u2019re doing graph transforms, you can wrap your own `Proxy` method around\na raw `Node` so that you can use the overloaded operators to add additional\nthings to a `Graph`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": "\nMatches all possible non-overlapping sets of operators and their data\ndependencies (`pattern`) in the Graph of a GraphModule (`gm`), then replaces\neach of these matched subgraphs with another subgraph (`replacement`).\n\nA list of `Match` objects representing the places in the original graph that\n`pattern` was matched to. The list is empty if there are no matches. `Match`\nis defined as:\n\nList[Match]\n\nExamples:\n\nThe above code will first match `pattern` in the `forward` method of\n`traced_module`. Pattern-matching is done based on use-def relationships, not\nnode names. For example, if you had `p = torch.cat([a, b])` in `pattern`, you\ncould match `m = torch.cat([a, b])` in the original `forward` function,\ndespite the variable names being different (`p` vs `m`).\n\nThe `return` statement in `pattern` is matched based on its value only; it may\nor may not match to the `return` statement in the larger graph. In other\nwords, the pattern doesn\u2019t have to extend to the end of the larger graph.\n\nWhen the pattern is matched, it will be removed from the larger function and\nreplaced by `replacement`. If there are multiple matches for `pattern` in the\nlarger function, each non-overlapping match will be replaced. In the case of a\nmatch overlap, the first found match in the set of overlapping matches will be\nreplaced. (\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node is the\nparameter that appears directly after `self`, while the last Node is whatever\nthe function returns.)\n\nOne important thing to note is that the parameters of the `pattern` Callable\nmust be used in the Callable itself, and the parameters of the `replacement`\nCallable must match the pattern. The first rule is why, in the above code\nblock, the `forward` function has parameters `x, w1, w2`, but the `pattern`\nfunction only has parameters `w1, w2`. `pattern` doesn\u2019t use `x`, so it\nshouldn\u2019t specify `x` as a parameter. As an example of the second rule,\nconsider replacing\n\nwith\n\nIn this case, `replacement` needs the same number of parameters as `pattern`\n(both `x` and `y`), even though the parameter `y` isn\u2019t used in `replacement`.\n\nAfter calling `subgraph_rewriter.replace_pattern`, the generated Python code\nlooks like this:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": "\nSymbolic tracing API\n\nGiven an `nn.Module` or function instance `root`, this function will return a\n`GraphModule` constructed by recording operations seen while tracing through\n`root`.\n\na Module created from the recorded operations from `root`.\n\nGraphModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": "\n`Tracer` is the class that implements the symbolic tracing functionality of\n`torch.fx.symbolic_trace`. A call to `symbolic_trace(m)` is equivalent to\n`Tracer().trace(m)`.\n\nTracer can be subclassed to override various behaviors of the tracing process.\nThe different behaviors that can be overridden are described in the docstrings\nof the methods on this class.\n\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": "\nMethod that specifies the behavior of this `Tracer` when it encounters a call\nto an `nn.Module` instance.\n\nBy default, the behavior is to check if the called module is a leaf module via\n`is_leaf_module`. If it is, emit a `call_module` node referring to `m` in the\n`Graph`. Otherwise, call the `Module` normally, tracing through the operations\nin its `forward` function.\n\nThis method can be overridden to\u2013for example\u2013create nested traced\nGraphModules, or any other behavior you would want while tracing across\n`Module` boundaries. `Module` boundaries.\n\nThe return value from the Module call. In the case that a `call_module` node\nwas emitted, this is a `Proxy` value. Otherwise, it is whatever value was\nreturned from the `Module` invocation.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": "\nA method to specify the behavior of tracing when preparing values to be used\nas arguments to nodes in the `Graph`.\n\nBy default, the behavior includes:\n\nGiven a non-Proxy Tensor object, emit IR for various cases:\n\nThis method can be overridden to support more types.\n\na (Any) \u2013 The value to be emitted as an `Argument` in the `Graph`.\n\nThe value `a` converted into the appropriate `Argument`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": "\nCreate `placeholder` nodes corresponding to the signature of the `root`\nModule. This method introspects root\u2019s signature and emits those nodes\naccordingly, also supporting `*args` and `**kwargs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": "\nA method to specify whether a given `nn.Module` is a \u201cleaf\u201d module.\n\nLeaf modules are the atomic units that appear in the IR, referenced by\n`call_module` calls. By default, Modules in the PyTorch standard library\nnamespace (torch.nn) are leaf modules. All other modules are traced through\nand their constituent ops are recorded, unless specified otherwise via this\nparameter.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": "\nHelper method to find the qualified name of `mod` in the Module hierarchy of\n`root`. For example, if `root` has a submodule named `foo`, which has a\nsubmodule named `bar`, passing `bar` into this function will return the string\n\u201cfoo.bar\u201d.\n\nmod (str) \u2013 The `Module` to retrieve the qualified name for.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": "\nTrace `root` and return the corresponding FX `Graph` representation. `root`\ncan either be an `nn.Module` instance or a Python callable.\n\nNote that after this call, `self.root` may be different from the `root` passed\nin here. For example, when a free function is passed to `trace()`, we will\ncreate an `nn.Module` instance to use as the root and add embedded constants\nto.\n\nroot (Union[Module, Callable]) \u2013 Either a `Module` or a function to be traced\nthrough.\n\nA `Graph` representing the semantics of the passed-in `root`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": "\n`Transformer` is a special type of interpreter that produces a new `Module`.\nIt exposes a `transform()` method that returns the transformed `Module`.\n`Transformer` does not require arguments to run, as `Interpreter` does.\n`Transformer` works entirely symbolically.\n\nSuppose we want to swap all instances of `torch.neg` with `torch.sigmoid` and\nvice versa (including their `Tensor` method equivalents). We could subclass\n`Transformer` like so:\n\nmodule (GraphModule) \u2013 The `Module` to be transformed.\n\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\nTransform `self.module` and return the transformed `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": "\nExecute a `get_attr` node. In `Transformer`, this is overridden to insert a\nnew `get_attr` node into the output graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": "\nExecute a `placeholder` node. In `Transformer`, this is overridden to insert a\nnew `placeholder` into the output graph.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": "\nTransform `self.module` and return the transformed `GraphModule`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": "\nThis function can be called at module-level scope to register fn_or_name as a\n\u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in\nthe FX trace instead of being traced through:\n\nThis function can also equivalently be used as a decorator:\n\nA wrapped function can be thought of a \u201cleaf function\u201d, analogous to the\nconcept of \u201cleaf modules\u201d, that is, they are functions that are left as calls\nin the FX trace rather than traced through.\n\nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global\nfunction to insert into the graph when it\u2019s called\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": "\nGathers values along an axis specified by `dim`.\n\nFor a 3-D tensor the output is specified by:\n\n`input` and `index` must have the same number of dimensions. It is also\nrequired that `index.size(d) <= input.size(d)` for all dimensions `d != dim`.\n`out` will have the same shape as `index`. Note that `input` and `index` do\nnot broadcast against each other.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gcd()", "path": "generated/torch.gcd#torch.gcd", "type": "torch", "text": "\nComputes the element-wise greatest common divisor (GCD) of `input` and\n`other`.\n\nBoth `input` and `other` must have integer types.\n\nNote\n\nThis defines gcd(0,0)=0gcd(0, 0) = 0 .\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ge()", "path": "generated/torch.ge#torch.ge", "type": "torch", "text": "\nComputes input\u2265other\\text{input} \\geq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is greater than or equal to\n`other` and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator", "path": "generated/torch.generator#torch.Generator", "type": "torch", "text": "\nCreates and returns a generator object that manages the state of the algorithm\nwhich produces pseudo random numbers. Used as a keyword argument in many In-\nplace random sampling functions.\n\ndevice (`torch.device`, optional) \u2013 the desired device for the generator.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\nGenerator.device -> device\n\nGets the current device of the generator.\n\nExample:\n\nReturns the Generator state as a `torch.ByteTensor`.\n\nA `torch.ByteTensor` which contains all the necessary bits to restore a\nGenerator to a specific point in time.\n\nTensor\n\nExample:\n\nReturns the initial seed for generating random numbers.\n\nExample:\n\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject. It is recommended to set a large seed, i.e. a number that has a good\nbalance of 0 and 1 bits. Avoid having many 0 bits in the seed.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\nGets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.\n\nExample:\n\nSets the Generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.device", "path": "generated/torch.generator#torch.Generator.device", "type": "torch", "text": "\nGenerator.device -> device\n\nGets the current device of the generator.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.get_state()", "path": "generated/torch.generator#torch.Generator.get_state", "type": "torch", "text": "\nReturns the Generator state as a `torch.ByteTensor`.\n\nA `torch.ByteTensor` which contains all the necessary bits to restore a\nGenerator to a specific point in time.\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.initial_seed()", "path": "generated/torch.generator#torch.Generator.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.manual_seed()", "path": "generated/torch.generator#torch.Generator.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject. It is recommended to set a large seed, i.e. a number that has a good\nbalance of 0 and 1 bits. Avoid having many 0 bits in the seed.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\nAn torch.Generator object.\n\nGenerator\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.seed()", "path": "generated/torch.generator#torch.Generator.seed", "type": "torch", "text": "\nGets a non-deterministic random number from std::random_device or the current\ntime and uses it to seed a Generator.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.Generator.set_state()", "path": "generated/torch.generator#torch.Generator.set_state", "type": "torch", "text": "\nSets the Generator state.\n\nnew_state (torch.ByteTensor) \u2013 The desired state.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.geqrf()", "path": "generated/torch.geqrf#torch.geqrf", "type": "torch", "text": "\nThis is a low-level function for calling LAPACK directly. This function\nreturns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf .\n\nYou\u2019ll generally want to use `torch.qr()` instead.\n\nComputes a QR decomposition of `input`, but without constructing QQ and RR as\nexplicit separate matrices.\n\nRather, this directly calls the underlying LAPACK function `?geqrf` which\nproduces a sequence of \u2018elementary reflectors\u2019.\n\nSee LAPACK documentation for geqrf for further details.\n\ninput (Tensor) \u2013 the input matrix\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ger()", "path": "generated/torch.ger#torch.ger", "type": "torch", "text": "\nAlias of `torch.outer()`.\n\nWarning\n\nThis function is deprecated and will be removed in a future PyTorch release.\nUse `torch.outer()` instead.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_default_dtype()", "path": "generated/torch.get_default_dtype#torch.get_default_dtype", "type": "torch", "text": "\nGet the current default floating point `torch.dtype`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_num_interop_threads()", "path": "generated/torch.get_num_interop_threads#torch.get_num_interop_threads", "type": "torch", "text": "\nReturns the number of threads used for inter-op parallelism on CPU (e.g. in\nJIT interpreter)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_num_threads()", "path": "generated/torch.get_num_threads#torch.get_num_threads", "type": "torch", "text": "\nReturns the number of threads used for parallelizing CPU operations\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.get_rng_state()", "path": "generated/torch.get_rng_state#torch.get_rng_state", "type": "torch", "text": "\nReturns the random number generator state as a `torch.ByteTensor`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.greater()", "path": "generated/torch.greater#torch.greater", "type": "torch", "text": "\nAlias for `torch.gt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.greater_equal()", "path": "generated/torch.greater_equal#torch.greater_equal", "type": "torch", "text": "\nAlias for `torch.ge()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.gt()", "path": "generated/torch.gt#torch.gt", "type": "torch", "text": "\nComputes input>other\\text{input} > \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is greater than `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hamming_window()", "path": "generated/torch.hamming_window#torch.hamming_window", "type": "torch", "text": "\nHamming window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.hamming_window(L, periodic=True)` equal to `torch.hamming_window(L + 1,\nperiodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nNote\n\nThis is a generalized version of `torch.hann_window()`.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hann_window()", "path": "generated/torch.hann_window#torch.hann_window", "type": "torch", "text": "\nHann window function.\n\nwhere NN is the full window size.\n\nThe input `window_length` is a positive integer controlling the returned\nwindow size. `periodic` flag determines whether the returned window trims off\nthe last duplicate value from the symmetric window and is ready to be used as\na periodic window with functions like `torch.stft()`. Therefore, if `periodic`\nis true, the NN in above formula is in fact\nwindow_length+1\\text{window\\\\_length} + 1 . Also, we always have\n`torch.hann_window(L, periodic=True)` equal to `torch.hann_window(L + 1,\nperiodic=False)[:-1])`.\n\nNote\n\nIf `window_length` =1=1 , the returned window contains a single value 1.\n\nA 1-D tensor of size (window_length,)(\\text{window\\\\_length},) containing the\nwindow\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.heaviside()", "path": "generated/torch.heaviside#torch.heaviside", "type": "torch", "text": "\nComputes the Heaviside step function for each element in `input`. The\nHeaviside step function is defined as:\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.histc()", "path": "generated/torch.histc#torch.histc", "type": "torch", "text": "\nComputes the histogram of a tensor.\n\nThe elements are sorted into equal width bins between `min` and `max`. If\n`min` and `max` are both zero, the minimum and maximum values of the data are\nused.\n\nElements lower than min and higher than max are ignored.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nHistogram represented as a tensor\n\nTensor\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hspmm()", "path": "sparse#torch.hspmm", "type": "torch.sparse", "text": "\nPerforms a matrix multiplication of a sparse COO matrix `mat1` and a strided\nmatrix `mat2`. The result is a (1 + 1)-dimensional hybrid COO matrix.\n\n{out} \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hstack()", "path": "generated/torch.hstack#torch.hstack", "type": "torch", "text": "\nStack tensors in sequence horizontally (column wise).\n\nThis is equivalent to concatenation along the first axis for 1-D tensors, and\nalong the second axis for all other tensors.\n\ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub", "path": "hub", "type": "torch.hub", "text": "\nPytorch Hub is a pre-trained model repository designed to facilitate research\nreproducibility.\n\nPytorch Hub supports publishing pre-trained models(model definitions and pre-\ntrained weights) to a github repository by adding a simple `hubconf.py` file;\n\n`hubconf.py` can have multiple entrypoints. Each entrypoint is defined as a\npython function (example: a pre-trained model you want to publish).\n\nHere is a code snippet specifies an entrypoint for `resnet18` model if we\nexpand the implementation in `pytorch/vision/hubconf.py`. In most case\nimporting the right function in `hubconf.py` is sufficient. Here we just want\nto use the expanded version as an example to show how it works. You can see\nthe full script in pytorch/vision repo\n\nPytorch Hub provides convenient APIs to explore all available models in hub\nthrough `torch.hub.list()`, show docstring and examples through\n`torch.hub.help()` and load the pre-trained models using `torch.hub.load()`.\n\nList all entrypoints available in `github` hubconf.\n\na list of available entrypoint names\n\nentrypoints\n\nShow the docstring of entrypoint `model`.\n\nLoad a model from a github repo or a local directory.\n\nNote: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.\n\nIf `source` is `'github'`, `repo_or_dir` is expected to be of the form\n`repo_owner/repo_name[:tag_name]` with an optional tag/branch.\n\nIf `source` is `'local'`, `repo_or_dir` is expected to be a path to a local\ndirectory.\n\nThe output of the `model` callable when called with the given `*args` and\n`**kwargs`.\n\nDownload object at the given URL to a local path.\n\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\nNote that `*args` and `**kwargs` in `torch.hub.load()` are used to instantiate\na model. After you have loaded a model, how can you find out what you can do\nwith the model? A suggested workflow is\n\nTo help users explore without referring to documentation back and forth, we\nstrongly recommend repo owners make function help messages clear and succinct.\nIt\u2019s also helpful to include a minimal working example.\n\nThe locations are used in the order of\n\nGet the Torch Hub cache directory used for storing downloaded models &\nweights.\n\nIf `set_dir()` is not called, default path is `$TORCH_HOME/hub` where\nenvironment variable `$TORCH_HOME` defaults to `$XDG_CACHE_HOME/torch`.\n`$XDG_CACHE_HOME` follows the X Design Group specification of the Linux\nfilesystem layout, with a default value `~/.cache` if the environment variable\nis not set.\n\nOptionally set the Torch Hub directory used to save downloaded models &\nweights.\n\nd (string) \u2013 path to a local folder to save downloaded models & weights.\n\nBy default, we don\u2019t clean up files after loading it. Hub uses the cache by\ndefault if it already exists in the directory returned by `get_dir()`.\n\nUsers can force a reload by calling `hub.load(..., force_reload=True)`. This\nwill delete the existing github folder and downloaded weights, reinitialize a\nfresh download. This is useful when updates are published to the same branch,\nusers can keep up with the latest release.\n\nTorch hub works by importing the package as if it was installed. There\u2019re some\nside effects introduced by importing in Python. For example, you can see new\nitems in Python caches `sys.modules` and `sys.path_importer_cache` which is\nnormal Python behavior.\n\nA known limitation that worth mentioning here is user CANNOT load two\ndifferent branches of the same repo in the same python process. It\u2019s just like\ninstalling two packages with the same name in Python, which is not good. Cache\nmight join the party and give you surprises if you actually try that. Of\ncourse it\u2019s totally fine to load them in separate processes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.download_url_to_file()", "path": "hub#torch.hub.download_url_to_file", "type": "torch.hub", "text": "\nDownload object at the given URL to a local path.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.get_dir()", "path": "hub#torch.hub.get_dir", "type": "torch.hub", "text": "\nGet the Torch Hub cache directory used for storing downloaded models &\nweights.\n\nIf `set_dir()` is not called, default path is `$TORCH_HOME/hub` where\nenvironment variable `$TORCH_HOME` defaults to `$XDG_CACHE_HOME/torch`.\n`$XDG_CACHE_HOME` follows the X Design Group specification of the Linux\nfilesystem layout, with a default value `~/.cache` if the environment variable\nis not set.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.help()", "path": "hub#torch.hub.help", "type": "torch.hub", "text": "\nShow the docstring of entrypoint `model`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.list()", "path": "hub#torch.hub.list", "type": "torch.hub", "text": "\nList all entrypoints available in `github` hubconf.\n\na list of available entrypoint names\n\nentrypoints\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.load()", "path": "hub#torch.hub.load", "type": "torch.hub", "text": "\nLoad a model from a github repo or a local directory.\n\nNote: Loading a model is the typical use case, but this can also be used to\nfor loading other objects such as tokenizers, loss functions, etc.\n\nIf `source` is `'github'`, `repo_or_dir` is expected to be of the form\n`repo_owner/repo_name[:tag_name]` with an optional tag/branch.\n\nIf `source` is `'local'`, `repo_or_dir` is expected to be a path to a local\ndirectory.\n\nThe output of the `model` callable when called with the given `*args` and\n`**kwargs`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.load_state_dict_from_url()", "path": "hub#torch.hub.load_state_dict_from_url", "type": "torch.hub", "text": "\nLoads the Torch serialized object at the given URL.\n\nIf downloaded file is a zip file, it will be automatically decompressed.\n\nIf the object is already present in `model_dir`, it\u2019s deserialized and\nreturned. The default value of `model_dir` is `<hub_dir>/checkpoints` where\n`hub_dir` is the directory returned by `get_dir()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hub.set_dir()", "path": "hub#torch.hub.set_dir", "type": "torch.hub", "text": "\nOptionally set the Torch Hub directory used to save downloaded models &\nweights.\n\nd (string) \u2013 path to a local folder to save downloaded models & weights.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.hypot()", "path": "generated/torch.hypot#torch.hypot", "type": "torch", "text": "\nGiven the legs of a right triangle, return its hypotenuse.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.i0()", "path": "generated/torch.i0#torch.i0", "type": "torch", "text": "\nComputes the zeroth order modified Bessel function of the first kind for each\nelement of `input`.\n\ninput (Tensor) \u2013 the input tensor\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.igamma()", "path": "generated/torch.igamma#torch.igamma", "type": "torch", "text": "\nComputes the regularized lower incomplete gamma function:\n\nwhere both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive\nand at least one is strictly positive. If both are zero or either is negative\nthen outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot) in the equation above\nis the gamma function,\n\nSee `torch.igammac()` and `torch.lgamma()` for related functions.\n\nSupports broadcasting to a common shape and float inputs.\n\nNote\n\nThe backward pass with respect to `input` is not yet supported. Please open an\nissue on PyTorch\u2019s Github to request it.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.igammac()", "path": "generated/torch.igammac#torch.igammac", "type": "torch", "text": "\nComputes the regularized upper incomplete gamma function:\n\nwhere both inputi\\text{input}_i and otheri\\text{other}_i are weakly positive\nand at least one is strictly positive. If both are zero or either is negative\nthen outi=nan\\text{out}_i=\\text{nan} . \u0393(\u22c5)\\Gamma(\\cdot) in the equation above\nis the gamma function,\n\nSee `torch.igamma()` and `torch.lgamma()` for related functions.\n\nSupports broadcasting to a common shape and float inputs.\n\nNote\n\nThe backward pass with respect to `input` is not yet supported. Please open an\nissue on PyTorch\u2019s Github to request it.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.imag()", "path": "generated/torch.imag#torch.imag", "type": "torch", "text": "\nReturns a new tensor containing imaginary values of the `self` tensor. The\nreturned tensor and `self` share the same underlying storage.\n\nWarning\n\n`imag()` is only supported for tensors with complex dtypes.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.index_select()", "path": "generated/torch.index_select#torch.index_select", "type": "torch", "text": "\nReturns a new tensor which indexes the `input` tensor along dimension `dim`\nusing the entries in `index` which is a `LongTensor`.\n\nThe returned tensor has the same number of dimensions as the original tensor\n(`input`). The `dim`th dimension has the same size as the length of `index`;\nother dimensions have the same size as in the original tensor.\n\nNote\n\nThe returned tensor does not use the same storage as the original tensor. If\n`out` has a different shape than expected, we silently change it to the\ncorrect shape, reallocating the underlying storage if necessary.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.initial_seed()", "path": "generated/torch.initial_seed#torch.initial_seed", "type": "torch", "text": "\nReturns the initial seed for generating random numbers as a Python `long`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.inner()", "path": "generated/torch.inner#torch.inner", "type": "torch", "text": "\nComputes the dot product for 1D tensors. For higher dimensions, sums the\nproduct of elements from `input` and `other` along their last dimension.\n\nNote\n\nIf either `input` or `other` is a scalar, the result is equivalent to\n`torch.mul(input, other)`.\n\nIf both `input` and `other` are non-scalars, the size of their last dimension\nmust match and the result is equivalent to `torch.tensordot(input, other,\ndims=([-1], [-1]))`\n\nout (Tensor, optional) \u2013 Optional output tensor to write result into. The\noutput shape is `input.shape[:-1] + other.shape[:-1]`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.inverse()", "path": "generated/torch.inverse#torch.inverse", "type": "torch", "text": "\nTakes the inverse of the square matrix `input`. `input` can be batches of 2D\nsquare tensors, in which case this function would return a tensor composed of\nindividual inverses.\n\nSupports real and complex input.\n\nNote\n\n`torch.inverse()` is deprecated. Please use `torch.linalg.inv()` instead.\n\nNote\n\nIrrespective of the original strides, the returned tensors will be transposed,\ni.e. with strides like `input.contiguous().transpose(-2, -1).stride()`\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) where `*` is zero\nor more batch dimensions\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isclose()", "path": "generated/torch.isclose#torch.isclose", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is \u201cclose\u201d to the corresponding element of `other`. Closeness is\ndefined as:\n\nwhere `input` and `other` are finite. Where `input` and/or `other` are\nnonfinite they are close if and only if they are equal, with NaNs being\nconsidered equal to each other when `equal_nan` is True.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isfinite()", "path": "generated/torch.isfinite#torch.isfinite", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element is\n`finite` or not.\n\nReal values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite.\n\ninput (Tensor): the input tensor.\n\nA boolean tensor that is True where `input` is finite and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isinf()", "path": "generated/torch.isinf#torch.isinf", "type": "torch", "text": "\nTests if each element of `input` is infinite (positive or negative infinity)\nor not.\n\nNote\n\nComplex values are infinite when their real or imaginary part is infinite.\n\n{input}\n\nA boolean tensor that is True where `input` is infinite and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isnan()", "path": "generated/torch.isnan#torch.isnan", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is NaN or not. Complex values are considered NaN when either their\nreal and/or imaginary part is NaN.\n\ninput (Tensor) \u2013 the input tensor.\n\nA boolean tensor that is True where `input` is NaN and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isneginf()", "path": "generated/torch.isneginf#torch.isneginf", "type": "torch", "text": "\nTests if each element of `input` is negative infinity or not.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isposinf()", "path": "generated/torch.isposinf#torch.isposinf", "type": "torch", "text": "\nTests if each element of `input` is positive infinity or not.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.isreal()", "path": "generated/torch.isreal#torch.isreal", "type": "torch", "text": "\nReturns a new tensor with boolean elements representing if each element of\n`input` is real-valued or not. All real-valued types are considered real.\nComplex values are considered real when their imaginary part is 0.\n\ninput (Tensor) \u2013 the input tensor.\n\nA boolean tensor that is True where `input` is real and False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.istft()", "path": "generated/torch.istft#torch.istft", "type": "torch", "text": "\nInverse short time Fourier Transform. This is expected to be the inverse of\n`stft()`. It has the same parameters (+ additional optional parameter of\n`length`) and it should return the least squares estimation of the original\nsignal. The algorithm will check using the NOLA condition ( nonzero overlap).\n\nImportant consideration in the parameters `window` and `center` so that the\nenvelop created by the summation of all the windows is never zero at certain\npoint in time. Specifically,\n\u2211t=\u2212\u221e\u221e\u2223w\u22232[n\u2212t\u00d7hop_length]=0\\sum_{t=-\\infty}^{\\infty} |w|^2[n-t\\times\nhop\\\\_length] \\cancel{=} 0 .\n\nSince `stft()` discards elements at the end of the signal if they do not fit\nin a frame, `istft` may return a shorter signal than the original signal (can\noccur if `center` is False since the signal isn\u2019t padded).\n\nIf `center` is `True`, then there will be padding e.g. `'constant'`,\n`'reflect'`, etc. Left padding can be trimmed off exactly because they can be\ncalculated but right padding cannot be calculated without additional\ninformation.\n\nExample: Suppose the last window is: `[17, 18, 0, 0, 0]` vs `[18, 0, 0, 0, 0]`\n\nThe `n_fft`, `hop_length`, `win_length` are all the same which prevents the\ncalculation of right padding. These additional values could be zeros or a\nreflection of the signal so providing `length` could be useful. If `length` is\n`None` then padding will be aggressively removed (some loss of signal).\n\n[1] D. W. Griffin and J. S. Lim, \u201cSignal estimation from modified short-time\nFourier transform,\u201d IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.\n\ninput (Tensor) \u2013\n\nThe input tensor. Expected to be output of `stft()`, can either be complex\n(`channel`, `fft_size`, `n_frame`), or real (`channel`, `fft_size`, `n_frame`,\n2) where the `channel` dimension is optional.\n\nDeprecated since version 1.8.0: Real input is deprecated, use complex inputs\nas returned by `stft(..., return_complex=True)` instead.\n\nLeast squares estimation of the original signal of size (\u2026, signal_length)\n\nTensor\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_complex()", "path": "generated/torch.is_complex#torch.is_complex", "type": "torch", "text": "\nReturns True if the data type of `input` is a complex data type i.e., one of\n`torch.complex64`, and `torch.complex128`.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_floating_point()", "path": "generated/torch.is_floating_point#torch.is_floating_point", "type": "torch", "text": "\nReturns True if the data type of `input` is a floating point data type i.e.,\none of `torch.float64`, `torch.float32`, `torch.float16`, and\n`torch.bfloat16`.\n\ninput (Tensor) \u2013 the input tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_nonzero()", "path": "generated/torch.is_nonzero#torch.is_nonzero", "type": "torch", "text": "\nReturns True if the `input` is a single element tensor which is not equal to\nzero after type conversions. i.e. not equal to `torch.tensor([0.])` or\n`torch.tensor([0])` or `torch.tensor([False])`. Throws a `RuntimeError` if\n`torch.numel() != 1` (even in case of sparse tensors).\n\ninput (Tensor) \u2013 the input tensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_storage()", "path": "generated/torch.is_storage#torch.is_storage", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch storage object.\n\nobj (Object) \u2013 Object to test\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.is_tensor()", "path": "generated/torch.is_tensor#torch.is_tensor", "type": "torch", "text": "\nReturns True if `obj` is a PyTorch tensor.\n\nNote that this function is simply doing `isinstance(obj, Tensor)`. Using that\n`isinstance` check is better for typechecking with mypy, and more explicit -\nso it\u2019s recommended to use that instead of `is_tensor`.\n\nobj (Object) \u2013 Object to test\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.export()", "path": "jit#torch.jit.export", "type": "TorchScript", "text": "\nThis decorator indicates that a method on an `nn.Module` is used as an entry\npoint into a `ScriptModule` and should be compiled.\n\n`forward` implicitly is assumed to be an entry point, so it does not need this\ndecorator. Functions and methods called from `forward` are compiled as they\nare seen by the compiler, so they do not need this decorator either.\n\nExample (using `@torch.jit.export` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.fork()", "path": "generated/torch.jit.fork#torch.jit.fork", "type": "TorchScript", "text": "\nCreates an asynchronous task executing `func` and a reference to the value of\nthe result of this execution. `fork` will return immediately, so the return\nvalue of `func` may not have been computed yet. To force completion of the\ntask and access the return value invoke `torch.jit.wait` on the Future. `fork`\ninvoked with a `func` which returns `T` is typed as `torch.jit.Future[T]`.\n`fork` calls can be arbitrarily nested, and may be invoked with positional and\nkeyword arguments. Asynchronous execution will only occur when run in\nTorchScript. If run in pure python, `fork` will not execute in parallel.\n`fork` will also not execute in parallel when invoked while tracing, however\nthe `fork` and `wait` calls will be captured in the exported IR Graph. ..\nwarning:\n\na reference to the execution of `func`. The value `T` can only be accessed by\nforcing completion of `func` through `torch.jit.wait`.\n\n`torch.jit.Future[T]`\n\nExample (fork a free function):\n\nExample (fork a module method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.freeze()", "path": "generated/torch.jit.freeze#torch.jit.freeze", "type": "TorchScript", "text": "\nFreezing a `ScriptModule` will clone it and attempt to inline the cloned\nmodule\u2019s submodules, parameters, and attributes as constants in the\nTorchScript IR Graph. By default, `forward` will be preserved, as well as\nattributes & methods specified in `preserved_attrs`. Additionally, any\nattribute that is modified within a preserved method will be preserved.\n\nFreezing currently only accepts ScriptModules that are in eval mode.\n\nFrozen `ScriptModule`.\n\nExample (Freezing a simple module with a Parameter):\n\nExample (Freezing a module with preserved attributes)\n\nNote\n\nIf you\u2019re not sure why an attribute is not being inlined as a constant, you\ncan run `dump_alias_db` on frozen_module.forward.graph to see if freezing has\ndetected the attribute is being modified.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ignore()", "path": "generated/torch.jit.ignore#torch.jit.ignore", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and left as a Python function. This allows you to leave code in your\nmodel that is not yet TorchScript compatible. If called from TorchScript,\nignored functions will dispatch the call to the Python interpreter. Models\nwith ignored functions cannot be exported; use `@torch.jit.unused` instead.\n\nExample (using `@torch.jit.ignore` on a method):\n\nExample (using `@torch.jit.ignore(drop=True)` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.isinstance()", "path": "generated/torch.jit.isinstance#torch.jit.isinstance", "type": "TorchScript", "text": "\nThis function provides for conatiner type refinement in TorchScript. It can\nrefine parameterized containers of the List, Dict, Tuple, and Optional types.\nE.g. `List[str]`, `Dict[str, List[torch.Tensor]]`,\n`Optional[Tuple[int,str,int]]`. It can also refine basic types such as bools\nand ints that are available in TorchScript.\n\nFalse otherwise with no new type refinement\n\n`bool`\n\nExample (using `torch.jit.isinstance` for type refinement): .. testcode:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.is_scripting()", "path": "jit_language_reference#torch.jit.is_scripting", "type": "TorchScript", "text": "\nFunction that returns True when in compilation and False otherwise. This is\nuseful especially with the @unused decorator to leave code in your model that\nis not yet TorchScript compatible. .. testcode:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.load()", "path": "generated/torch.jit.load#torch.jit.load", "type": "TorchScript", "text": "\nLoad a `ScriptModule` or `ScriptFunction` previously saved with\n`torch.jit.save`\n\nAll previously saved modules, no matter their device, are first loaded onto\nCPU, and then are moved to the devices they were saved from. If this fails\n(e.g. because the run time system doesn\u2019t have certain devices), an exception\nis raised.\n\nA `ScriptModule` object.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.save()", "path": "generated/torch.jit.save#torch.jit.save", "type": "TorchScript", "text": "\nSave an offline version of this module for use in a separate process. The\nsaved module serializes all of the methods, submodules, parameters, and\nattributes of this module. It can be loaded into the C++ API using\n`torch::jit::load(filename)` or into the Python API with `torch.jit.load`.\n\nTo be able to save a module, it must not make any calls to native Python\nfunctions. This means that all submodules must be subclasses of `ScriptModule`\nas well.\n\nDanger\n\nAll modules, no matter their device, are always loaded onto the CPU during\nloading. This is different from `torch.load()`\u2019s semantics and may change in\nthe future.\n\nNote\n\ntorch.jit.save attempts to preserve the behavior of some operators across\nversions. For example, dividing two integer tensors in PyTorch 1.5 performed\nfloor division, and if the module containing that code is saved in PyTorch 1.5\nand loaded in PyTorch 1.6 its division behavior will be preserved. The same\nmodule saved in PyTorch 1.6 will fail to load in PyTorch 1.5, however, since\nthe behavior of division changed in 1.6, and 1.5 does not know how to\nreplicate the 1.6 behavior.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.script()", "path": "generated/torch.jit.script#torch.jit.script", "type": "TorchScript", "text": "\nScripting a function or `nn.Module` will inspect the source code, compile it\nas TorchScript code using the TorchScript compiler, and return a\n`ScriptModule` or `ScriptFunction`. TorchScript itself is a subset of the\nPython language, so not all features in Python work, but we provide enough\nfunctionality to compute on tensors and do control-dependent operations. For a\ncomplete guide, see the TorchScript Language Reference.\n\n`torch.jit.script` can be used as a function for modules and functions, and as\na decorator `@torch.jit.script` for TorchScript Classes and functions.\n\nobj (callable, class, or `nn.Module`) \u2013 The `nn.Module`, function, or class\ntype to compile.\n\nIf `obj` is `nn.Module`, `script` returns a `ScriptModule` object. The\nreturned `ScriptModule` will have the same set of sub-modules and parameters\nas the original `nn.Module`. If `obj` is a standalone function, a\n`ScriptFunction` will be returned.\n\nThe `@torch.jit.script` decorator will construct a `ScriptFunction` by\ncompiling the body of the function.\n\nExample (scripting a function):\n\nScripting an `nn.Module` by default will compile the `forward` method and\nrecursively compile any methods, submodules, and functions called by\n`forward`. If a `nn.Module` only uses features supported in TorchScript, no\nchanges to the original module code should be necessary. `script` will\nconstruct `ScriptModule` that has copies of the attributes, parameters, and\nmethods of the original module.\n\nExample (scripting a simple module with a Parameter):\n\nExample (scripting a module with traced submodules):\n\nTo compile a method other than `forward` (and recursively compile anything it\ncalls), add the `@torch.jit.export` decorator to the method. To opt out of\ncompilation use `@torch.jit.ignore` or `@torch.jit.unused`.\n\nExample (an exported and ignored method in a module):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction", "type": "TorchScript", "text": "\nFunctionally equivalent to a `ScriptModule`, but represents a single function\nand does not have any attributes or Parameters.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.get_debug_state()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.get_debug_state", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptFunction.save_to_buffer()", "path": "generated/torch.jit.scriptfunction#torch.jit.ScriptFunction.save_to_buffer", "type": "TorchScript", "text": "\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule", "type": "TorchScript", "text": "\nA wrapper around C++ `torch::jit::Module`. `ScriptModule`s contain methods,\nattributes, parameters, and constants. These can be accessed the same as on a\nnormal `nn.Module`.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nReturns a pretty-printed representation (as valid Python syntax) of the\ninternal graph for the `forward` method. See Inspecting Code for details.\n\nReturns a tuple of:\n\n[0] a pretty-printed representation (as valid Python syntax) of the internal\ngraph for the `forward` method. See `code`. [1] a ConstMap following the\nCONSTANT.cN format of the output in [0]. The indices in the [0] output are\nkeys to the underlying constant\u2019s values.\n\nSee Inspecting Code for details.\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nReturns a string representation of the internal graph for the `forward`\nmethod. See Interpreting Graphs for details.\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nReturns a string representation of the internal graph for the `forward`\nmethod. This graph will be preprocessed to inline all function and method\ncalls. See Interpreting Graphs for details.\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nSee `torch.jit.save` for details.\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.add_module()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.add_module", "type": "TorchScript", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.apply()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.apply", "type": "TorchScript", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.bfloat16()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.bfloat16", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code", "type": "TorchScript", "text": "\nReturns a pretty-printed representation (as valid Python syntax) of the\ninternal graph for the `forward` method. See Inspecting Code for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.code_with_constants()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.code_with_constants", "type": "TorchScript", "text": "\nReturns a tuple of:\n\n[0] a pretty-printed representation (as valid Python syntax) of the internal\ngraph for the `forward` method. See `code`. [1] a ConstMap following the\nCONSTANT.cN format of the output in [0]. The indices in the [0] output are\nkeys to the underlying constant\u2019s values.\n\nSee Inspecting Code for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.cuda()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.cuda", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.double()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.double", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.eval()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.eval", "type": "TorchScript", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.extra_repr()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.extra_repr", "type": "TorchScript", "text": "\nSet the extra representation of the module\n\nTo print customized extra information, you should re-implement this method in\nyour own modules. Both single-line and multi-line strings are acceptable.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.float()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.float", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. See Interpreting Graphs for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.half()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.half", "type": "TorchScript", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.inlined_graph()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.inlined_graph", "type": "TorchScript", "text": "\nReturns a string representation of the internal graph for the `forward`\nmethod. This graph will be preprocessed to inline all function and method\ncalls. See Interpreting Graphs for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.load_state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.load_state_dict", "type": "TorchScript", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_buffers()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_buffers", "type": "TorchScript", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_children()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_children", "type": "TorchScript", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_modules()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_modules", "type": "TorchScript", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.named_parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.named_parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.parameters()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.parameters", "type": "TorchScript", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_buffer()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_buffer", "type": "TorchScript", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_hook", "type": "TorchScript", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_forward_pre_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_forward_pre_hook", "type": "TorchScript", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_full_backward_hook()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_full_backward_hook", "type": "TorchScript", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.register_parameter()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.register_parameter", "type": "TorchScript", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.requires_grad_()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.requires_grad_", "type": "TorchScript", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.save()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.save", "type": "TorchScript", "text": "\nSee `torch.jit.save` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.state_dict()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.state_dict", "type": "TorchScript", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.to()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.to", "type": "TorchScript", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.train()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.train", "type": "TorchScript", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.type()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.type", "type": "TorchScript", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.xpu()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.xpu", "type": "TorchScript", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.ScriptModule.zero_grad()", "path": "generated/torch.jit.scriptmodule#torch.jit.ScriptModule.zero_grad", "type": "TorchScript", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.script_if_tracing()", "path": "generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing", "type": "TorchScript", "text": "\nCompiles `fn` when it is first called during tracing. `torch.jit.script` has a\nnon-negligible start up time when it is first called due to lazy-\ninitializations of many compiler builtins. Therefore you should not use it in\nlibrary code. However, you may want to have parts of your library work in\ntracing even if they use control flow. In these cases, you should use\n`@torch.jit.script_if_tracing` to substitute for `torch.jit.script`.\n\nfn \u2013 A function to compile.\n\nIf called during tracing, a `ScriptFunction` created by `torch.jit.script` is\nreturned. Otherwise, the original function `fn` is returned.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.trace()", "path": "generated/torch.jit.trace#torch.jit.trace", "type": "TorchScript", "text": "\nTrace a function and return an executable or `ScriptFunction` that will be\noptimized using just-in-time compilation. Tracing is ideal for code that\noperates only on `Tensor`s and lists, dictionaries, and tuples of `Tensor`s.\n\nUsing `torch.jit.trace` and `torch.jit.trace_module`, you can turn an existing\nmodule or Python function into a TorchScript `ScriptFunction` or\n`ScriptModule`. You must provide example inputs, and we run the function,\nrecording the operations performed on all the tensors.\n\nThis module also contains any parameters that the original module had as well.\n\nWarning\n\nTracing only correctly records functions and modules which are not data\ndependent (e.g., do not have conditionals on data in tensors) and do not have\nany untracked external dependencies (e.g., perform input/output or access\nglobal variables). Tracing only records operations done when the given\nfunction is run on the given tensors. Therefore, the returned `ScriptModule`\nwill always run the same traced graph on any input. This has some important\nimplications when your module is expected to run different sets of operations,\ndepending on the input and/or the module state. For example,\n\nIn cases like these, tracing would not be appropriate and `scripting` is a\nbetter choice. If you trace such models, you may silently get incorrect\nresults on subsequent invocations of the model. The tracer will try to emit\nwarnings when doing something that may cause an incorrect trace to be\nproduced.\n\nIf `func` is `nn.Module` or `forward` of `nn.Module`, `trace` returns a\n`ScriptModule` object with a single `forward` method containing the traced\ncode. The returned `ScriptModule` will have the same set of sub-modules and\nparameters as the original `nn.Module`. If `func` is a standalone function,\n`trace` returns `ScriptFunction`.\n\nExample (tracing a function):\n\nExample (tracing an existing module):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.trace_module()", "path": "generated/torch.jit.trace_module#torch.jit.trace_module", "type": "TorchScript", "text": "\nTrace a module and return an executable `ScriptModule` that will be optimized\nusing just-in-time compilation. When a module is passed to `torch.jit.trace`,\nonly the `forward` method is run and traced. With `trace_module`, you can\nspecify a dictionary of method names to example inputs to trace (see the\n`inputs`) argument below.\n\nSee `torch.jit.trace` for more information on tracing.\n\nA `ScriptModule` object with a single `forward` method containing the traced\ncode. When `func` is a `torch.nn.Module`, the returned `ScriptModule` will\nhave the same set of sub-modules and parameters as `func`.\n\nExample (tracing a module with multiple methods):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.unused()", "path": "generated/torch.jit.unused#torch.jit.unused", "type": "TorchScript", "text": "\nThis decorator indicates to the compiler that a function or method should be\nignored and replaced with the raising of an exception. This allows you to\nleave code in your model that is not yet TorchScript compatible and still\nexport your model.\n\nExample (using `@torch.jit.unused` on a method):\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.jit.wait()", "path": "generated/torch.jit.wait#torch.jit.wait", "type": "TorchScript", "text": "\nForces completion of a `torch.jit.Future[T]` asynchronous task, returning the\nresult of the task. See `fork()` for docs and examples. :param func: an\nasynchronous task reference, created through `torch.jit.fork` :type func:\ntorch.jit.Future[T]\n\nthe return value of the the completed task\n\n`T`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kaiser_window()", "path": "generated/torch.kaiser_window#torch.kaiser_window", "type": "torch", "text": "\nComputes the Kaiser window with window length `window_length` and shape\nparameter `beta`.\n\nLet I_0 be the zeroth order modified Bessel function of the first kind (see\n`torch.i0()`) and `N = L - 1` if `periodic` is False and `L` if `periodic` is\nTrue, where `L` is the `window_length`. This function computes:\n\nCalling `torch.kaiser_window(L, B, periodic=True)` is equivalent to calling\n`torch.kaiser_window(L + 1, B, periodic=False)[:-1])`. The `periodic` argument\nis intended as a helpful shorthand to produce a periodic window as input to\nfunctions like `torch.stft()`.\n\nNote\n\nIf `window_length` is one, then the returned window is a single element tensor\ncontaining a one.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kron()", "path": "generated/torch.kron#torch.kron", "type": "torch", "text": "\nComputes the Kronecker product, denoted by \u2297\\otimes , of `input` and `other`.\n\nIf `input` is a (a0\u00d7a1\u00d7\u22ef\u00d7an)(a_0 \\times a_1 \\times \\dots \\times a_n) tensor\nand `other` is a (b0\u00d7b1\u00d7\u22ef\u00d7bn)(b_0 \\times b_1 \\times \\dots \\times b_n) tensor,\nthe result will be a (a0\u2217b0\u00d7a1\u2217b1\u00d7\u22ef\u00d7an\u2217bn)(a_0*b_0 \\times a_1*b_1 \\times \\dots\n\\times a_n*b_n) tensor with the following entries:\n\nwhere kt=it\u2217bt+jtk_t = i_t * b_t + j_t for 0\u2264t\u2264n0 \\leq t \\leq n . If one\ntensor has fewer dimensions than the other it is unsqueezed until it has the\nsame number of dimensions.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nThis function generalizes the typical definition of the Kronecker product for\ntwo matrices to two tensors, as described above. When `input` is a (m\u00d7n)(m\n\\times n) matrix and `other` is a (p\u00d7q)(p \\times q) matrix, the result will be\na (p\u2217m\u00d7q\u2217n)(p*m \\times q*n) block matrix:\n\nwhere `input` is A\\mathbf{A} and `other` is B\\mathbf{B} .\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.kthvalue()", "path": "generated/torch.kthvalue#torch.kthvalue", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the `k` th smallest\nelement of each row of the `input` tensor in the given dimension `dim`. And\n`indices` is the index location of each element found.\n\nIf `dim` is not given, the last dimension of the `input` is chosen.\n\nIf `keepdim` is `True`, both the `values` and `indices` tensors are the same\nsize as `input`, except in the dimension `dim` where they are of size 1.\nOtherwise, `dim` is squeezed (see `torch.squeeze()`), resulting in both the\n`values` and `indices` tensors having 1 fewer dimension than the `input`\ntensor.\n\nNote\n\nWhen `input` is a CUDA tensor and there are multiple valid `k` th values, this\nfunction may nondeterministically return `indices` for any of them.\n\nout (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be\noptionally given to be used as output buffers\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lcm()", "path": "generated/torch.lcm#torch.lcm", "type": "torch", "text": "\nComputes the element-wise least common multiple (LCM) of `input` and `other`.\n\nBoth `input` and `other` must have integer types.\n\nNote\n\nThis defines lcm(0,0)=0lcm(0, 0) = 0 and lcm(0,a)=0lcm(0, a) = 0 .\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ldexp()", "path": "generated/torch.ldexp#torch.ldexp", "type": "torch", "text": "\nMultiplies `input` by 2**:attr:`other`.\n\nTypically this function is used to construct floating point numbers by\nmultiplying mantissas in `input` with integral powers of two created from the\nexponents in :attr:\u2019other\u2019.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.le()", "path": "generated/torch.le#torch.le", "type": "torch", "text": "\nComputes input\u2264other\\text{input} \\leq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is less than or equal to `other`\nand False elsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lerp()", "path": "generated/torch.lerp#torch.lerp", "type": "torch", "text": "\nDoes a linear interpolation of two tensors `start` (given by `input`) and\n`end` based on a scalar or tensor `weight` and returns the resulting `out`\ntensor.\n\nThe shapes of `start` and `end` must be broadcastable. If `weight` is a\ntensor, then the shapes of `weight`, `start`, and `end` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.less()", "path": "generated/torch.less#torch.less", "type": "torch", "text": "\nAlias for `torch.lt()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.less_equal()", "path": "generated/torch.less_equal#torch.less_equal", "type": "torch", "text": "\nAlias for `torch.le()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lgamma()", "path": "generated/torch.lgamma#torch.lgamma", "type": "torch", "text": "\nComputes the logarithm of the gamma function on `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg", "path": "linalg", "type": "torch.linalg", "text": "\nCommon linear algebra operations.\n\nThis module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details.\n\nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-\nvalued matrices) positive-definite matrix or the Cholesky decompositions for a\nbatch of such matrices. Each decomposition has the form:\n\nwhere LL is a lower-triangular matrix and LHL^H is the conjugate transpose of\nLL , which is just a transpose for the case of real-valued input matrices. In\ncode it translates to `input = L @ L.t()` if `input` is real-valued and `input\n= L @ L.conj().t()` if `input` is complex-valued. The batch of LL matrices is\nreturned.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nLAPACK\u2019s `potrf` is used for CPU inputs, and MAGMA\u2019s `potrf` is used for CUDA\ninputs.\n\nNote\n\nIf `input` is not a Hermitian positive-definite matrix, or if it\u2019s a batch of\nmatrices and one or more of them is not a Hermitian positive-definite matrix,\nthen a RuntimeError will be thrown. If `input` is a batch of matrices, then\nthe error message will include the batch index of the first matrix that is not\nHermitian positive-definite.\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) consisting of\nHermitian positive-definite n\u00d7nn \\times n matrices, where \u2217* is zero or more\nbatch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes the condition number of a matrix `input`, or of each matrix in a\nbatched `input`, using the matrix norm defined by `p`.\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` this is defined as the matrix\nnorm of `input` times the matrix norm of the inverse of `input` computed using\n`torch.linalg.norm()`. While for norms `{None, 2, -2}` this is defined as the\nratio between the largest and smallest singular values computed using\n`torch.linalg.svd()`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function may synchronize that device\nwith the CPU depending on which norm `p` is used.\n\nNote\n\nFor norms `{None, 2, -2}`, `input` may be a non-square matrix or batch of non-\nsquare matrices. For other norms, however, `input` must be a square matrix or\na batch of square matrices, and if this requirement is not satisfied a\nRuntimeError will be thrown.\n\nNote\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` if `input` is a non-invertible\nmatrix then a tensor containing infinity will be returned. If `input` is a\nbatch of matrices and one or more of them is not invertible then a\nRuntimeError will be thrown.\n\np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nthe type of the matrix norm to use in the computations. inf refers to\n`float('inf')`, numpy\u2019s `inf` object, or any equivalent object. The following\nnorms can be used:\n\np\n\nnorm for matrices\n\nNone\n\nratio of the largest singular value to the smallest singular value\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2019nuc\u2019\n\nnuclear norm\n\ninf\n\nmax(sum(abs(x), dim=1))\n\n-inf\nmin(sum(abs(x), dim=1))\n\n1\n\nmax(sum(abs(x), dim=0))\n\n-1\nmin(sum(abs(x), dim=0))\n\n2\n\nratio of the largest singular value to the smallest singular value\n\n-2\nratio of the smallest singular value to the largest singular value\n\nDefault: `None`\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nThe condition number of `input`. The output dtype is always real valued even\nfor complex inputs (e.g. float if `input` is cfloat).\n\nExamples:\n\nComputes the determinant of a square matrix `input`, or of each square matrix\nin a batched `input`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nBackward through `det` internally uses `torch.linalg.svd()` when `input` is\nnot invertible. In this case, double backward through `det` will be unstable\nwhen `input` doesn\u2019t have distinct singular values. See `torch.linalg.svd()`\nfor more details.\n\ninput (Tensor) \u2013 the input matrix of size `(n, n)` or the batch of matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nExample:\n\nCalculates the sign and natural logarithm of the absolute value of a square\nmatrix\u2019s determinant, or of the absolute values of the determinants of a batch\nof square matrices `input`. The determinant can be computed with `sign *\nexp(logabsdet)`.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nFor matrices that have zero determinant, this returns `(0, -inf)`. If `input`\nis batched then the entries in the result tensors corresponding to matrices\nwith the zero determinant have sign 0 and the natural logarithm of the\nabsolute value of the determinant -inf.\n\ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n) or the batch of matrices\nof size (\u2217,n,n)(*, n, n) where \u2217* is one or more batch dimensions.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to.\n\nA namedtuple (sign, logabsdet) containing the sign of the determinant and the\nnatural logarithm of the absolute value of determinant, respectively.\n\nExample:\n\nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real\nsymmetric) matrix `input`, or of each such matrix in a batched `input`.\n\nFor a single matrix `input`, the tensor of eigenvalues `w` and the tensor of\neigenvectors `V` decompose the `input` such that `input = V diag(w) V\u1d34`, where\n`V\u1d34` is the transpose of `V` for real-valued `input`, or the conjugate\ntranspose of `V` for complex-valued `input`.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues/eigenvectors are computed using LAPACK\u2019s `syevd` and `heevd`\nroutines for CPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA\ninputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThe eigenvectors of matrices are not unique, so any eigenvector multiplied by\na constant remains a valid eigenvector. This function may compute different\neigenvector representations on different device types. Usually the difference\nis only in the sign of the eigenvector.\n\nNote\n\nSee `torch.linalg.eigvalsh()` for a related function that computes only\neigenvalues. However, that function is not differentiable.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default\nis `None`.\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\nThe eigenvalues in ascending order.\n\nThe orthonormal eigenvectors of the `input`.\n\n(Tensor, Tensor)\n\nExamples:\n\nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix\n`input`, or of each such matrix in a batched `input`. The eigenvalues are\nreturned in ascending order.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues are computed using LAPACK\u2019s `syevd` and `heevd` routines for\nCPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA inputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThis function doesn\u2019t support backpropagation, please use\n`torch.linalg.eigh()` instead, which also computes the eigenvectors.\n\nNote\n\nSee `torch.linalg.eigh()` for a related function that computes both\neigenvalues and eigenvectors.\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\nComputes the numerical rank of a matrix `input`, or of each matrix in a\nbatched `input`.\n\nThe matrix rank is computed as the number of singular values (or absolute\neigenvalues when `hermitian` is `True`) that are greater than the specified\n`tol` threshold.\n\nIf `tol` is not specified, `tol` is set to\n`S.max(dim=-1)*max(input.shape[-2:])*eps`, where `S` is the singular values\n(or absolute eigenvalues when `hermitian` is `True`), and `eps` is the epsilon\nvalue for the datatype of `input`. The epsilon value can be obtained using the\n`eps` attribute of `torch.finfo`.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe matrix rank is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation is\ndone by obtaining the eigenvalues (see `torch.linalg.eigvalsh()`).\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\nReturns the matrix norm or vector norm of a given tensor.\n\nThis function can calculate one of eight different types of matrix norms, or\none of an infinite number of vector norms, depending on both the number of\nreduction dimensions and the value of the `ord` parameter.\n\nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nThe order of norm. inf refers to `float('inf')`, numpy\u2019s `inf` object, or any\nequivalent object. The following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2013 not supported \u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013 not supported \u2013\n\ninf\n\nmax(sum(abs(x), dim=1))\n\nmax(abs(x))\n\n-inf\nmin(sum(abs(x), dim=1))\n\nmin(abs(x))\n\n0\n\n\u2013 not supported \u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), dim=0))\n\nas below\n\n-1\nmin(sum(abs(x), dim=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\nsmallest singular value\n\nas below\n\nother\n\n\u2013 not supported \u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nDefault: `None`\n\nExamples:\n\nUsing the `dim` argument to compute vector norms:\n\nUsing the `dim` argument to compute matrix norms:\n\nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a\nmatrix `input`, or of each matrix in a batched `input`.\n\nThe singular values (or the absolute values of the eigenvalues when\n`hermitian` is `True`) that are below the specified `rcond` threshold are\ntreated as zero and discarded in the computation.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe pseudo-inverse is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation of the\npseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see\n`torch.linalg.eigh()`).\n\nNote\n\nIf singular value decomposition or eigenvalue decomposition algorithms do not\nconverge then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\u201d The singular value decomposition is represented as a\nnamedtuple `(U, S, Vh)`, such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@}\ndiag(S) \\times Vh . If `input` is a batch of tensors, then `U`, `S`, and `Vh`\nare also batched with the same batch dimensions as `input`.\n\nIf `full_matrices` is `False` (default), the method returns the reduced\nsingular value decomposition i.e., if the last two dimensions of `input` are\n`m` and `n`, then the returned `U` and `V` matrices will contain only\nmin(n,m)min(n, m) orthonormal columns.\n\nIf `compute_uv` is `False`, the returned `U` and `Vh` will be empy tensors\nwith no elements and the same device as `input`. The `full_matrices` argument\nhas no effect when `compute_uv` is False.\n\nThe dtypes of `U` and `V` are the same as `input`\u2019s. `S` will always be real-\nvalued, even if `input` is complex.\n\nNote\n\nUnlike NumPy\u2019s `linalg.svd`, this always returns a namedtuple of three\ntensors, even when `compute_uv=False`. This behavior may change in a future\nPyTorch release.\n\nNote\n\nThe singular values are returned in descending order. If `input` is a batch of\nmatrices, then the singular values of each matrix in the batch is returned in\ndescending order.\n\nNote\n\nThe implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-\nand-conquer algorithm) instead of `?gesvd` for speed. Analogously, the SVD on\nGPU uses the cuSOLVER routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243\nand later, and uses the MAGMA routine `gesdd` on earlier versions of CUDA.\n\nNote\n\nThe returned matrix `U` will be transposed, i.e. with strides\n`U.contiguous().transpose(-2, -1).stride()`.\n\nNote\n\nGradients computed using `U` and `Vh` may be unstable if `input` is not full\nrank or has non-unique singular values.\n\nNote\n\nWhen `full_matrices` = `True`, the gradients on `U[..., :, min(m, n):]` and\n`V[..., :, min(m, n):]` will be ignored in backward as those vectors can be\narbitrary bases of the subspaces.\n\nNote\n\nThe `S` tensor can only be used to compute gradients if `compute_uv` is True.\n\nNote\n\nSince `U` and `V` of an SVD is not unique, each vector can be multiplied by an\narbitrary phase factor ei\u03d5e^{i \\phi} while the SVD result is still correct.\nDifferent platforms, like Numpy, or inputs on different device types, may\nproduce different `U` and `V` tensors.\n\nExample:\n\nComputes the solution `x` to the matrix equation `matmul(input, x) = other`\nwith a square matrix, or batches of such matrices, `input` and one or more\nright-hand side vectors `other`. If `input` is batched and `other` is not,\nthen `other` is broadcast to have the same batch dimensions as `input`. The\nresulting tensor has the same shape as the (possibly broadcast) `other`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` dtypes.\n\nNote\n\nIf `input` is a non-square or non-invertible matrix, or a batch containing\nnon-square matrices or one or more non-invertible matrices, then a\nRuntimeError will be thrown.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nBatched input:\n\nComputes a tensor `input_inv` such that `tensordot(input_inv, input, ind) ==\nI_n` (inverse tensor equation), where `I_n` is the n-dimensional identity\ntensor and `n` is equal to `input.ndim`. The resulting tensor `input_inv` has\nshape equal to `input.shape[ind:] + input.shape[:ind]`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` data types.\n\nNote\n\nIf `input` is not invertible or does not satisfy the requirement\n`prod(input.shape[ind:]) == prod(input.shape[:ind])`, then a RuntimeError will\nbe thrown.\n\nNote\n\nWhen `input` is a 2-dimensional tensor and `ind=1`, this function computes the\n(multiplicative) inverse of `input`, equivalent to calling `torch.inverse()`.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes a tensor `x` such that `tensordot(input, x, dims=x.ndim) = other`.\nThe resulting tensor `x` has the same shape as `input[other.ndim:]`.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nIf `input` does not satisfy the requirement `prod(input.shape[other.ndim:]) ==\nprod(input.shape[:other.ndim])` after (optionally) moving the dimensions using\n`dims`, then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nComputes the multiplicative inverse matrix of a square matrix `input`, or of\neach square matrix in a batched `input`. The result satisfies the relation:\n\n`matmul(inv(input),input)` = `matmul(input,inv(input))` =\n`eye(input.shape[0]).expand_as(input)`.\n\nSupports input of float, double, cfloat and cdouble data types.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe inverse matrix is computed using LAPACK\u2019s `getrf` and `getri` routines for\nCPU inputs. For CUDA inputs, cuSOLVER\u2019s `getrf` and `getrs` routines as well\nas cuBLAS\u2019 `getrf` and `getri` routines are used if CUDA version >= 10.1.243,\notherwise MAGMA\u2019s `getrf` and `getri` routines are used instead.\n\nNote\n\nIf `input` is a non-invertible matrix or non-square matrix, or batch with at\nleast one such matrix, then a RuntimeError will be thrown.\n\ninput (Tensor) \u2013 the square `(n, n)` matrix or the batch of such matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nDepending on the value of `mode` this function returns the reduced or complete\nQR factorization. See below for a list of valid modes.\n\nNote\n\nDifferences with `numpy.linalg.qr`:\n\nNote\n\nBackpropagation is not supported for `mode='r'`. Use `mode='reduced'` instead.\n\nBackpropagation is also not supported if the first\nmin\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))\ncolumns of any matrix in `input` are not linearly independent. While no error\nwill be thrown when this occurs the values of the \u201cgradient\u201d produced may be\nanything. This behavior may change in the future.\n\nNote\n\nThis function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may\nproduce different (valid) decompositions on different device types or\ndifferent platforms.\n\nmode (str, optional) \u2013\n\nif `k = min(m, n)` then:\n\nout (tuple, optional) \u2013 tuple of `Q` and `R` tensors. The dimensions of `Q`\nand `R` are detailed in the description of `mode` above.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.cholesky()", "path": "linalg#torch.linalg.cholesky", "type": "torch.linalg", "text": "\nComputes the Cholesky decomposition of a Hermitian (or symmetric for real-\nvalued matrices) positive-definite matrix or the Cholesky decompositions for a\nbatch of such matrices. Each decomposition has the form:\n\nwhere LL is a lower-triangular matrix and LHL^H is the conjugate transpose of\nLL , which is just a transpose for the case of real-valued input matrices. In\ncode it translates to `input = L @ L.t()` if `input` is real-valued and `input\n= L @ L.conj().t()` if `input` is complex-valued. The batch of LL matrices is\nreturned.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nLAPACK\u2019s `potrf` is used for CPU inputs, and MAGMA\u2019s `potrf` is used for CUDA\ninputs.\n\nNote\n\nIf `input` is not a Hermitian positive-definite matrix, or if it\u2019s a batch of\nmatrices and one or more of them is not a Hermitian positive-definite matrix,\nthen a RuntimeError will be thrown. If `input` is a batch of matrices, then\nthe error message will include the batch index of the first matrix that is not\nHermitian positive-definite.\n\ninput (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n) consisting of\nHermitian positive-definite n\u00d7nn \\times n matrices, where \u2217* is zero or more\nbatch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.cond()", "path": "linalg#torch.linalg.cond", "type": "torch.linalg", "text": "\nComputes the condition number of a matrix `input`, or of each matrix in a\nbatched `input`, using the matrix norm defined by `p`.\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` this is defined as the matrix\nnorm of `input` times the matrix norm of the inverse of `input` computed using\n`torch.linalg.norm()`. While for norms `{None, 2, -2}` this is defined as the\nratio between the largest and smallest singular values computed using\n`torch.linalg.svd()`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function may synchronize that device\nwith the CPU depending on which norm `p` is used.\n\nNote\n\nFor norms `{None, 2, -2}`, `input` may be a non-square matrix or batch of non-\nsquare matrices. For other norms, however, `input` must be a square matrix or\na batch of square matrices, and if this requirement is not satisfied a\nRuntimeError will be thrown.\n\nNote\n\nFor norms `{\u2018fro\u2019, \u2018nuc\u2019, inf, -inf, 1, -1}` if `input` is a non-invertible\nmatrix then a tensor containing infinity will be returned. If `input` is a\nbatch of matrices and one or more of them is not invertible then a\nRuntimeError will be thrown.\n\np (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nthe type of the matrix norm to use in the computations. inf refers to\n`float('inf')`, numpy\u2019s `inf` object, or any equivalent object. The following\nnorms can be used:\n\np\n\nnorm for matrices\n\nNone\n\nratio of the largest singular value to the smallest singular value\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2019nuc\u2019\n\nnuclear norm\n\ninf\n\nmax(sum(abs(x), dim=1))\n\n-inf\nmin(sum(abs(x), dim=1))\n\n1\n\nmax(sum(abs(x), dim=0))\n\n-1\nmin(sum(abs(x), dim=0))\n\n2\n\nratio of the largest singular value to the smallest singular value\n\n-2\nratio of the smallest singular value to the largest singular value\n\nDefault: `None`\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nThe condition number of `input`. The output dtype is always real valued even\nfor complex inputs (e.g. float if `input` is cfloat).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.det()", "path": "linalg#torch.linalg.det", "type": "torch.linalg", "text": "\nComputes the determinant of a square matrix `input`, or of each square matrix\nin a batched `input`.\n\nThis function supports float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nBackward through `det` internally uses `torch.linalg.svd()` when `input` is\nnot invertible. In this case, double backward through `det` will be unstable\nwhen `input` doesn\u2019t have distinct singular values. See `torch.linalg.svd()`\nfor more details.\n\ninput (Tensor) \u2013 the input matrix of size `(n, n)` or the batch of matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.eigh()", "path": "linalg#torch.linalg.eigh", "type": "torch.linalg", "text": "\nComputes the eigenvalues and eigenvectors of a complex Hermitian (or real\nsymmetric) matrix `input`, or of each such matrix in a batched `input`.\n\nFor a single matrix `input`, the tensor of eigenvalues `w` and the tensor of\neigenvectors `V` decompose the `input` such that `input = V diag(w) V\u1d34`, where\n`V\u1d34` is the transpose of `V` for real-valued `input`, or the conjugate\ntranspose of `V` for complex-valued `input`.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues/eigenvectors are computed using LAPACK\u2019s `syevd` and `heevd`\nroutines for CPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA\ninputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThe eigenvectors of matrices are not unique, so any eigenvector multiplied by\na constant remains a valid eigenvector. This function may compute different\neigenvector representations on different device types. Usually the difference\nis only in the sign of the eigenvector.\n\nNote\n\nSee `torch.linalg.eigvalsh()` for a related function that computes only\neigenvalues. However, that function is not differentiable.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to. Default\nis `None`.\n\nA namedtuple (eigenvalues, eigenvectors) containing\n\nThe eigenvalues in ascending order.\n\nThe orthonormal eigenvectors of the `input`.\n\n(Tensor, Tensor)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.eigvalsh()", "path": "linalg#torch.linalg.eigvalsh", "type": "torch.linalg", "text": "\nComputes the eigenvalues of a complex Hermitian (or real symmetric) matrix\n`input`, or of each such matrix in a batched `input`. The eigenvalues are\nreturned in ascending order.\n\nSince the matrix or matrices in `input` are assumed to be Hermitian, the\nimaginary part of their diagonals is always treated as zero. When `UPLO` is\n\u201cL\u201d, its default value, only the lower triangular part of each matrix is used\nin the computation. When `UPLO` is \u201cU\u201d only the upper triangular part of each\nmatrix is used.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe eigenvalues are computed using LAPACK\u2019s `syevd` and `heevd` routines for\nCPU inputs, and MAGMA\u2019s `syevd` and `heevd` routines for CUDA inputs.\n\nNote\n\nThe eigenvalues of real symmetric or complex Hermitian matrices are always\nreal.\n\nNote\n\nThis function doesn\u2019t support backpropagation, please use\n`torch.linalg.eigh()` instead, which also computes the eigenvectors.\n\nNote\n\nSee `torch.linalg.eigh()` for a related function that computes both\neigenvalues and eigenvectors.\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.inv()", "path": "linalg#torch.linalg.inv", "type": "torch.linalg", "text": "\nComputes the multiplicative inverse matrix of a square matrix `input`, or of\neach square matrix in a batched `input`. The result satisfies the relation:\n\n`matmul(inv(input),input)` = `matmul(input,inv(input))` =\n`eye(input.shape[0]).expand_as(input)`.\n\nSupports input of float, double, cfloat and cdouble data types.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe inverse matrix is computed using LAPACK\u2019s `getrf` and `getri` routines for\nCPU inputs. For CUDA inputs, cuSOLVER\u2019s `getrf` and `getrs` routines as well\nas cuBLAS\u2019 `getrf` and `getri` routines are used if CUDA version >= 10.1.243,\notherwise MAGMA\u2019s `getrf` and `getri` routines are used instead.\n\nNote\n\nIf `input` is a non-invertible matrix or non-square matrix, or batch with at\nleast one such matrix, then a RuntimeError will be thrown.\n\ninput (Tensor) \u2013 the square `(n, n)` matrix or the batch of such matrices of\nsize `(*, n, n)` where `*` is one or more batch dimensions.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.matrix_rank()", "path": "linalg#torch.linalg.matrix_rank", "type": "torch.linalg", "text": "\nComputes the numerical rank of a matrix `input`, or of each matrix in a\nbatched `input`.\n\nThe matrix rank is computed as the number of singular values (or absolute\neigenvalues when `hermitian` is `True`) that are greater than the specified\n`tol` threshold.\n\nIf `tol` is not specified, `tol` is set to\n`S.max(dim=-1)*max(input.shape[-2:])*eps`, where `S` is the singular values\n(or absolute eigenvalues when `hermitian` is `True`), and `eps` is the epsilon\nvalue for the datatype of `input`. The epsilon value can be obtained using the\n`eps` attribute of `torch.finfo`.\n\nSupports input of float, double, cfloat and cdouble dtypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe matrix rank is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation is\ndone by obtaining the eigenvalues (see `torch.linalg.eigvalsh()`).\n\nout (Tensor, optional) \u2013 tensor to write the output to. Default is `None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.norm()", "path": "linalg#torch.linalg.norm", "type": "torch.linalg", "text": "\nReturns the matrix norm or vector norm of a given tensor.\n\nThis function can calculate one of eight different types of matrix norms, or\none of an infinite number of vector norms, depending on both the number of\nreduction dimensions and the value of the `ord` parameter.\n\nord (int, float, inf, -inf, 'fro', 'nuc', optional) \u2013\n\nThe order of norm. inf refers to `float('inf')`, numpy\u2019s `inf` object, or any\nequivalent object. The following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2019fro\u2019\n\nFrobenius norm\n\n\u2013 not supported \u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013 not supported \u2013\n\ninf\n\nmax(sum(abs(x), dim=1))\n\nmax(abs(x))\n\n-inf\nmin(sum(abs(x), dim=1))\n\nmin(abs(x))\n\n0\n\n\u2013 not supported \u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), dim=0))\n\nas below\n\n-1\nmin(sum(abs(x), dim=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\nsmallest singular value\n\nas below\n\nother\n\n\u2013 not supported \u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nDefault: `None`\n\nExamples:\n\nUsing the `dim` argument to compute vector norms:\n\nUsing the `dim` argument to compute matrix norms:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.pinv()", "path": "linalg#torch.linalg.pinv", "type": "torch.linalg", "text": "\nComputes the pseudo-inverse (also known as the Moore-Penrose inverse) of a\nmatrix `input`, or of each matrix in a batched `input`.\n\nThe singular values (or the absolute values of the eigenvalues when\n`hermitian` is `True`) that are below the specified `rcond` threshold are\ntreated as zero and discarded in the computation.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe pseudo-inverse is computed using singular value decomposition (see\n`torch.linalg.svd()`) by default. If `hermitian` is `True`, then `input` is\nassumed to be Hermitian (symmetric if real-valued), and the computation of the\npseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see\n`torch.linalg.eigh()`).\n\nNote\n\nIf singular value decomposition or eigenvalue decomposition algorithms do not\nconverge then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default is\n`None`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.qr()", "path": "linalg#torch.linalg.qr", "type": "torch.linalg", "text": "\nComputes the QR decomposition of a matrix or a batch of matrices `input`, and\nreturns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R\nwith QQ being an orthogonal matrix or batch of orthogonal matrices and RR\nbeing an upper triangular matrix or batch of upper triangular matrices.\n\nDepending on the value of `mode` this function returns the reduced or complete\nQR factorization. See below for a list of valid modes.\n\nNote\n\nDifferences with `numpy.linalg.qr`:\n\nNote\n\nBackpropagation is not supported for `mode='r'`. Use `mode='reduced'` instead.\n\nBackpropagation is also not supported if the first\nmin\u2061(input.size(\u22121),input.size(\u22122))\\min(input.size(-1), input.size(-2))\ncolumns of any matrix in `input` are not linearly independent. While no error\nwill be thrown when this occurs the values of the \u201cgradient\u201d produced may be\nanything. This behavior may change in the future.\n\nNote\n\nThis function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may\nproduce different (valid) decompositions on different device types or\ndifferent platforms.\n\nmode (str, optional) \u2013\n\nif `k = min(m, n)` then:\n\nout (tuple, optional) \u2013 tuple of `Q` and `R` tensors. The dimensions of `Q`\nand `R` are detailed in the description of `mode` above.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.slogdet()", "path": "linalg#torch.linalg.slogdet", "type": "torch.linalg", "text": "\nCalculates the sign and natural logarithm of the absolute value of a square\nmatrix\u2019s determinant, or of the absolute values of the determinants of a batch\nof square matrices `input`. The determinant can be computed with `sign *\nexp(logabsdet)`.\n\nSupports input of float, double, cfloat and cdouble datatypes.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nNote\n\nThe determinant is computed using LU factorization. LAPACK\u2019s `getrf` is used\nfor CPU inputs, and MAGMA\u2019s `getrf` is used for CUDA inputs.\n\nNote\n\nFor matrices that have zero determinant, this returns `(0, -inf)`. If `input`\nis batched then the entries in the result tensors corresponding to matrices\nwith the zero determinant have sign 0 and the natural logarithm of the\nabsolute value of the determinant -inf.\n\ninput (Tensor) \u2013 the input matrix of size (n,n)(n, n) or the batch of matrices\nof size (\u2217,n,n)(*, n, n) where \u2217* is one or more batch dimensions.\n\nout (tuple, optional) \u2013 tuple of two tensors to write the output to.\n\nA namedtuple (sign, logabsdet) containing the sign of the determinant and the\nnatural logarithm of the absolute value of determinant, respectively.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.solve()", "path": "linalg#torch.linalg.solve", "type": "torch.linalg", "text": "\nComputes the solution `x` to the matrix equation `matmul(input, x) = other`\nwith a square matrix, or batches of such matrices, `input` and one or more\nright-hand side vectors `other`. If `input` is batched and `other` is not,\nthen `other` is broadcast to have the same batch dimensions as `input`. The\nresulting tensor has the same shape as the (possibly broadcast) `other`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` dtypes.\n\nNote\n\nIf `input` is a non-square or non-invertible matrix, or a batch containing\nnon-square matrices or one or more non-invertible matrices, then a\nRuntimeError will be thrown.\n\nNote\n\nWhen given inputs on a CUDA device, this function synchronizes that device\nwith the CPU.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\nBatched input:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.svd()", "path": "linalg#torch.linalg.svd", "type": "torch.linalg", "text": "\nComputes the singular value decomposition of either a matrix or batch of\nmatrices `input`.\u201d The singular value decomposition is represented as a\nnamedtuple `(U, S, Vh)`, such that input=U@diag(S)\u00d7Vhinput = U \\mathbin{@}\ndiag(S) \\times Vh . If `input` is a batch of tensors, then `U`, `S`, and `Vh`\nare also batched with the same batch dimensions as `input`.\n\nIf `full_matrices` is `False` (default), the method returns the reduced\nsingular value decomposition i.e., if the last two dimensions of `input` are\n`m` and `n`, then the returned `U` and `V` matrices will contain only\nmin(n,m)min(n, m) orthonormal columns.\n\nIf `compute_uv` is `False`, the returned `U` and `Vh` will be empy tensors\nwith no elements and the same device as `input`. The `full_matrices` argument\nhas no effect when `compute_uv` is False.\n\nThe dtypes of `U` and `V` are the same as `input`\u2019s. `S` will always be real-\nvalued, even if `input` is complex.\n\nNote\n\nUnlike NumPy\u2019s `linalg.svd`, this always returns a namedtuple of three\ntensors, even when `compute_uv=False`. This behavior may change in a future\nPyTorch release.\n\nNote\n\nThe singular values are returned in descending order. If `input` is a batch of\nmatrices, then the singular values of each matrix in the batch is returned in\ndescending order.\n\nNote\n\nThe implementation of SVD on CPU uses the LAPACK routine `?gesdd` (a divide-\nand-conquer algorithm) instead of `?gesvd` for speed. Analogously, the SVD on\nGPU uses the cuSOLVER routines `gesvdj` and `gesvdjBatched` on CUDA 10.1.243\nand later, and uses the MAGMA routine `gesdd` on earlier versions of CUDA.\n\nNote\n\nThe returned matrix `U` will be transposed, i.e. with strides\n`U.contiguous().transpose(-2, -1).stride()`.\n\nNote\n\nGradients computed using `U` and `Vh` may be unstable if `input` is not full\nrank or has non-unique singular values.\n\nNote\n\nWhen `full_matrices` = `True`, the gradients on `U[..., :, min(m, n):]` and\n`V[..., :, min(m, n):]` will be ignored in backward as those vectors can be\narbitrary bases of the subspaces.\n\nNote\n\nThe `S` tensor can only be used to compute gradients if `compute_uv` is True.\n\nNote\n\nSince `U` and `V` of an SVD is not unique, each vector can be multiplied by an\narbitrary phase factor ei\u03d5e^{i \\phi} while the SVD result is still correct.\nDifferent platforms, like Numpy, or inputs on different device types, may\nproduce different `U` and `V` tensors.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.tensorinv()", "path": "linalg#torch.linalg.tensorinv", "type": "torch.linalg", "text": "\nComputes a tensor `input_inv` such that `tensordot(input_inv, input, ind) ==\nI_n` (inverse tensor equation), where `I_n` is the n-dimensional identity\ntensor and `n` is equal to `input.ndim`. The resulting tensor `input_inv` has\nshape equal to `input.shape[ind:] + input.shape[:ind]`.\n\nSupports input of `float`, `double`, `cfloat` and `cdouble` data types.\n\nNote\n\nIf `input` is not invertible or does not satisfy the requirement\n`prod(input.shape[ind:]) == prod(input.shape[:ind])`, then a RuntimeError will\nbe thrown.\n\nNote\n\nWhen `input` is a 2-dimensional tensor and `ind=1`, this function computes the\n(multiplicative) inverse of `input`, equivalent to calling `torch.inverse()`.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linalg.tensorsolve()", "path": "linalg#torch.linalg.tensorsolve", "type": "torch.linalg", "text": "\nComputes a tensor `x` such that `tensordot(input, x, dims=x.ndim) = other`.\nThe resulting tensor `x` has the same shape as `input[other.ndim:]`.\n\nSupports real-valued and complex-valued inputs.\n\nNote\n\nIf `input` does not satisfy the requirement `prod(input.shape[other.ndim:]) ==\nprod(input.shape[:other.ndim])` after (optionally) moving the dimensions using\n`dims`, then a RuntimeError will be thrown.\n\nout (Tensor, optional) \u2013 The output tensor. Ignored if `None`. Default: `None`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.linspace()", "path": "generated/torch.linspace#torch.linspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from `start` to `end`, inclusive. That is, the value are:\n\nWarning\n\nNot providing a value for `steps` is deprecated. For backwards compatibility,\nnot providing a value for `steps` will create a tensor with 100 elements. Note\nthat this behavior is not reflected in the documented function signature and\nshould not be relied on. In a future PyTorch release, failing to provide a\nvalue for `steps` will throw a runtime error.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.load()", "path": "generated/torch.load#torch.load", "type": "torch", "text": "\nLoads an object saved with `torch.save()` from a file.\n\n`torch.load()` uses Python\u2019s unpickling facilities but treats storages, which\nunderlie tensors, specially. They are first deserialized on the CPU and are\nthen moved to the device they were saved from. If this fails (e.g. because the\nrun time system doesn\u2019t have certain devices), an exception is raised.\nHowever, storages can be dynamically remapped to an alternative set of devices\nusing the `map_location` argument.\n\nIf `map_location` is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument will be\nthe initial deserialization of the storage, residing on the CPU. Each\nserialized storage has a location tag associated with it which identifies the\ndevice it was saved from, and this tag is the second argument passed to\n`map_location`. The builtin location tags are `'cpu'` for CPU tensors and\n`'cuda:device_id'` (e.g. `'cuda:2'`) for CUDA tensors. `map_location` should\nreturn either `None` or a storage. If `map_location` returns a storage, it\nwill be used as the final deserialized object, already moved to the right\ndevice. Otherwise, `torch.load()` will fall back to the default behavior, as\nif `map_location` wasn\u2019t specified.\n\nIf `map_location` is a `torch.device` object or a string containing a device\ntag, it indicates the location where all tensors should be loaded.\n\nOtherwise, if `map_location` is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the storages\n(values).\n\nUser extensions can register their own location tags and tagging and\ndeserialization methods using `torch.serialization.register_package()`.\n\nWarning\n\n`torch.load()` uses `pickle` module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary\ncode during unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust.\n\nNote\n\nWhen you call `torch.load()` on a file which contains GPU tensors, those\ntensors will be loaded to GPU by default. You can call `torch.load(..,\nmap_location='cpu')` and then `load_state_dict()` to avoid GPU RAM surge when\nloading a model checkpoint.\n\nNote\n\nBy default, we decode byte strings as `utf-8`. This is to avoid a common error\ncase `UnicodeDecodeError: 'ascii' codec can't decode byte 0x...` when loading\nfiles saved by Python 2 in Python 3. If this default is incorrect, you may use\nan extra `encoding` keyword argument to specify how these objects should be\nloaded, e.g., `encoding='latin1'` decodes them to strings using `latin1`\nencoding, and `encoding='bytes'` keeps them as byte arrays which can be\ndecoded later with `byte_array.decode(...)`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lobpcg()", "path": "generated/torch.lobpcg#torch.lobpcg", "type": "torch", "text": "\nFind the k largest (or smallest) eigenvalues and the corresponding\neigenvectors of a symmetric positive defined generalized eigenvalue problem\nusing matrix-free LOBPCG methods.\n\nThis function is a front-end to the following LOBPCG algorithms selectable via\n`method` argument:\n\n`method=\u201dbasic\u201d` \\- the LOBPCG method introduced by Andrew Knyazev, see\n[Knyazev2001]. A less robust method, may fail when Cholesky is applied to\nsingular input.\n\n`method=\u201dortho\u201d` \\- the LOBPCG method with orthogonal basis selection\n[StathopoulosEtal2002]. A robust method.\n\nSupported inputs are dense, sparse, and batches of dense matrices.\n\nNote\n\nIn general, the basic method spends least time per iteration. However, the\nrobust methods converge much faster and are more stable. So, the usage of the\nbasic method is generally not recommended but there exist cases where the\nusage of the basic method may be preferred.\n\nWarning\n\nThe backward method does not support sparse and complex inputs. It works only\nwhen `B` is not provided (i.e. `B == None`). We are actively working on\nextensions, and the details of the algorithms are going to be published\npromptly.\n\nWarning\n\nWhile it is assumed that `A` is symmetric, `A.grad` is not. To make sure that\n`A.grad` is symmetric, so that `A - t * A.grad` is symmetric in first-order\noptimization routines, prior to running `lobpcg` we do the following\nsymmetrization map: `A -> (A + A.t()) / 2`. The map is performed only when the\n`A` requires gradients.\n\ntracker (callable, optional) \u2013\n\na function for tracing the iteration process. When specified, it is called at\neach iteration step with LOBPCG instance as an argument. The LOBPCG instance\nholds the full state of the iteration process in the following attributes:\n\n`iparams`, `fparams`, `bparams` \\- dictionaries of integer, float, and boolean\nvalued input parameters, respectively\n\n`ivars`, `fvars`, `bvars`, `tvars` \\- dictionaries of integer, float, boolean,\nand Tensor valued iteration variables, respectively.\n\n`A`, `B`, `iK` \\- input Tensor arguments.\n\n`E`, `X`, `S`, `R` \\- iteration Tensor variables.\n\nFor instance:\n\n`ivars[\u201cistep\u201d]` \\- the current iteration step `X` \\- the current\napproximation of eigenvectors `E` \\- the current approximation of eigenvalues\n`R` \\- the current residual `ivars[\u201cconverged_count\u201d]` \\- the current number\nof converged eigenpairs `tvars[\u201crerr\u201d]` \\- the current state of convergence\ncriteria\n\nNote that when `tracker` stores Tensor objects from the LOBPCG instance, it\nmust make copies of these.\n\nIf `tracker` sets `bvars[\u201cforce_stop\u201d] = True`, the iteration process will be\nhard-stopped.\n\ntensor of eigenvalues of size (\u2217,k)(*, k)\n\nX (Tensor): tensor of eigenvectors of size (\u2217,m,k)(*, m, k)\n\nE (Tensor)\n\n[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned\nEigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method.\nSIAM J. Sci. Comput., 23(2), 517-541. (25 pages)\nhttps://epubs.siam.org/doi/abs/10.1137/S1064827500366124\n\n[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block\nOrthogonalization Procedure with Constant Synchronization Requirements. SIAM\nJ. Sci. Comput., 23(6), 2165-2182. (18 pages)\nhttps://epubs.siam.org/doi/10.1137/S1064827500370883\n\n[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A\nRobust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5),\nC655-C676. (22 pages) https://epubs.siam.org/doi/abs/10.1137/17M1129830\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log()", "path": "generated/torch.log#torch.log", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log10()", "path": "generated/torch.log10#torch.log10", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 10 of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log1p()", "path": "generated/torch.log1p#torch.log1p", "type": "torch", "text": "\nReturns a new tensor with the natural logarithm of (1 + `input`).\n\nNote\n\nThis function is more accurate than `torch.log()` for small values of `input`\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.log2()", "path": "generated/torch.log2#torch.log2", "type": "torch", "text": "\nReturns a new tensor with the logarithm to the base 2 of the elements of\n`input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logaddexp()", "path": "generated/torch.logaddexp#torch.logaddexp", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs.\n\nCalculates pointwise log\u2061(ex+ey)\\log\\left(e^x + e^y\\right) . This function is\nuseful in statistics where the calculated probabilities of events may be so\nsmall as to exceed the range of normal floating point numbers. In such cases\nthe logarithm of the calculated probability is stored. This function allows\nadding probabilities stored in such a fashion.\n\nThis op should be disambiguated with `torch.logsumexp()` which performs a\nreduction on a single tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logaddexp2()", "path": "generated/torch.logaddexp2#torch.logaddexp2", "type": "torch", "text": "\nLogarithm of the sum of exponentiations of the inputs in base-2.\n\nCalculates pointwise log\u20612(2x+2y)\\log_2\\left(2^x + 2^y\\right) . See\n`torch.logaddexp()` for more details.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logcumsumexp()", "path": "generated/torch.logcumsumexp#torch.logcumsumexp", "type": "torch", "text": "\nReturns the logarithm of the cumulative summation of the exponentiation of\nelements of `input` in the dimension `dim`.\n\nFor summation index jj given by `dim` and other indices ii , the result is\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logdet()", "path": "generated/torch.logdet#torch.logdet", "type": "torch", "text": "\nCalculates log determinant of a square matrix or batches of square matrices.\n\nNote\n\nResult is `-inf` if `input` has zero log determinant, and is `nan` if `input`\nhas negative determinant.\n\nNote\n\nBackward through `logdet()` internally uses SVD results when `input` is not\ninvertible. In this case, double backward through `logdet()` will be unstable\nin when `input` doesn\u2019t have distinct singular values. See `svd()` for\ndetails.\n\ninput (Tensor) \u2013 the input tensor of size `(*, n, n)` where `*` is zero or\nmore batch dimensions.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_and()", "path": "generated/torch.logical_and#torch.logical_and", "type": "torch", "text": "\nComputes the element-wise logical AND of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_not()", "path": "generated/torch.logical_not#torch.logical_not", "type": "torch", "text": "\nComputes the element-wise logical NOT of the given input tensor. If not\nspecified, the output tensor will have the bool dtype. If the input tensor is\nnot a bool tensor, zeros are treated as `False` and non-zeros are treated as\n`True`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_or()", "path": "generated/torch.logical_or#torch.logical_or", "type": "torch", "text": "\nComputes the element-wise logical OR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logical_xor()", "path": "generated/torch.logical_xor#torch.logical_xor", "type": "torch", "text": "\nComputes the element-wise logical XOR of the given input tensors. Zeros are\ntreated as `False` and nonzeros are treated as `True`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logit()", "path": "generated/torch.logit#torch.logit", "type": "torch", "text": "\nReturns a new tensor with the logit of the elements of `input`. `input` is\nclamped to [eps, 1 - eps] when eps is not None. When eps is None and `input` <\n0 or `input` > 1, the function will yields NaN.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logspace()", "path": "generated/torch.logspace#torch.logspace", "type": "torch", "text": "\nCreates a one-dimensional tensor of size `steps` whose values are evenly\nspaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}} to\nbaseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale\nwith base `base`. That is, the values are:\n\nWarning\n\nNot providing a value for `steps` is deprecated. For backwards compatibility,\nnot providing a value for `steps` will create a tensor with 100 elements. Note\nthat this behavior is not reflected in the documented function signature and\nshould not be relied on. In a future PyTorch release, failing to provide a\nvalue for `steps` will throw a runtime error.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.logsumexp()", "path": "generated/torch.logsumexp#torch.logsumexp", "type": "torch", "text": "\nReturns the log of summed exponentials of each row of the `input` tensor in\nthe given dimension `dim`. The computation is numerically stabilized.\n\nFor summation index jj given by `dim` and other indices ii , the result is\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lstsq()", "path": "generated/torch.lstsq#torch.lstsq", "type": "torch", "text": "\nComputes the solution to the least squares and least norm problems for a full\nrank matrix AA of size (m\u00d7n)(m \\times n) and a matrix BB of size (m\u00d7k)(m\n\\times k) .\n\nIf m\u2265nm \\geq n , `lstsq()` solves the least-squares problem:\n\nIf m<nm < n , `lstsq()` solves the least-norm problem:\n\nReturned tensor XX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k) . The first nn\nrows of XX contains the solution. If m\u2265nm \\geq n , the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in\nthe remaining m\u2212nm - n rows of that column.\n\nNote\n\nThe case when m<nm < n is not supported on the GPU.\n\nout (tuple, optional) \u2013 the optional destination tensor\n\nA namedtuple (solution, QR) containing:\n\n(Tensor, Tensor)\n\nNote\n\nThe returned matrices will always be transposed, irrespective of the strides\nof the input matrices. That is, they will have stride `(1, m)` instead of `(m,\n1)`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lt()", "path": "generated/torch.lt#torch.lt", "type": "torch", "text": "\nComputes input<other\\text{input} < \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is less than `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu()", "path": "generated/torch.lu#torch.lu", "type": "torch", "text": "\nComputes the LU factorization of a matrix or batches of matrices `A`. Returns\na tuple containing the LU factorization and pivots of `A`. Pivoting is done if\n`pivot` is set to `True`.\n\nNote\n\nThe pivots returned by the function are 1-indexed. If `pivot` is `False`, then\nthe returned pivots is a tensor filled with zeros of the appropriate size.\n\nNote\n\nLU factorization with `pivot` = `False` is not available for CPU, and\nattempting to do so will throw an error. However, LU factorization with\n`pivot` = `False` is available for CUDA.\n\nNote\n\nThis function does not check if the factorization was successful or not if\n`get_infos` is `True` since the status of the factorization is present in the\nthird element of the return tuple.\n\nNote\n\nIn the case of batches of square matrices with size less or equal to 32 on a\nCUDA device, the LU factorization is repeated for singular matrices due to the\nbug in the MAGMA library (see magma issue 13).\n\nNote\n\n`L`, `U`, and `P` can be derived using `torch.lu_unpack()`.\n\nWarning\n\nThe LU factorization does have backward support, but only for square inputs of\nfull rank.\n\nA tuple of tensors containing\n\n(Tensor, IntTensor, IntTensor (optional))\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu_solve()", "path": "generated/torch.lu_solve#torch.lu_solve", "type": "torch", "text": "\nReturns the LU solve of the linear system Ax=bAx = b using the partially\npivoted LU factorization of A from `torch.lu()`.\n\nThis function supports `float`, `double`, `cfloat` and `cdouble` dtypes for\n`input`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.lu_unpack()", "path": "generated/torch.lu_unpack#torch.lu_unpack", "type": "torch", "text": "\nUnpacks the data and pivots from a LU factorization of a tensor.\n\nReturns a tuple of tensors as `(the pivots, the L tensor, the U tensor)`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.manual_seed()", "path": "generated/torch.manual_seed#torch.manual_seed", "type": "torch", "text": "\nSets the seed for generating random numbers. Returns a `torch.Generator`\nobject.\n\nseed (int) \u2013 The desired seed. Value must be within the inclusive range\n`[-0x8000_0000_0000_0000, 0xffff_ffff_ffff_ffff]`. Otherwise, a RuntimeError\nis raised. Negative inputs are remapped to positive values with the formula\n`0xffff_ffff_ffff_ffff + seed`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.masked_select()", "path": "generated/torch.masked_select#torch.masked_select", "type": "torch", "text": "\nReturns a new 1-D tensor which indexes the `input` tensor according to the\nboolean mask `mask` which is a `BoolTensor`.\n\nThe shapes of the `mask` tensor and the `input` tensor don\u2019t need to match,\nbut they must be broadcastable.\n\nNote\n\nThe returned tensor does not use the same storage as the original tensor\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matmul()", "path": "generated/torch.matmul#torch.matmul", "type": "torch", "text": "\nMatrix product of two tensors.\n\nThe behavior depends on the dimensionality of the tensors as follows:\n\nIf both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned. If\nthe first argument is 1-dimensional, a 1 is prepended to its dimension for the\npurpose of the batched matrix multiply and removed after. If the second\nargument is 1-dimensional, a 1 is appended to its dimension for the purpose of\nthe batched matrix multiple and removed after. The non-matrix (i.e. batch)\ndimensions are broadcasted (and thus must be broadcastable). For example, if\n`input` is a (j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n) tensor and `other` is a\n(k\u00d7n\u00d7n)(k \\times n \\times n) tensor, `out` will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k\n\\times n \\times n) tensor.\n\nNote that the broadcasting logic only looks at the batch dimensions when\ndetermining if the inputs are broadcastable, and not the matrix dimensions.\nFor example, if `input` is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m) tensor\nand `other` is a (k\u00d7m\u00d7p)(k \\times m \\times p) tensor, these inputs are valid\nfor broadcasting even though the final two dimensions (i.e. the matrix\ndimensions) are different. `out` will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n\n\\times p) tensor.\n\nThis operator supports TensorFloat32.\n\nNote\n\nThe 1-dimensional dot product version of this function does not support an\n`out` parameter.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_exp()", "path": "generated/torch.matrix_exp#torch.matrix_exp", "type": "torch", "text": "\nReturns the matrix exponential. Supports batched input. For a matrix `A`, the\nmatrix exponential is defined as\n\nThe implementation is based on:\n\nBader, P.; Blanes, S.; Casas, F. Computing the Matrix Exponential with an\nOptimized Taylor Polynomial Approximation. Mathematics 2019, 7, 1174.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_power()", "path": "generated/torch.matrix_power#torch.matrix_power", "type": "torch", "text": "\nReturns the matrix raised to the power `n` for square matrices. For batch of\nmatrices, each individual matrix is raised to the power `n`.\n\nIf `n` is negative, then the inverse of the matrix (if invertible) is raised\nto the power `n`. For a batch of matrices, the batched inverse (if invertible)\nis raised to the power `n`. If `n` is 0, then an identity matrix is returned.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.matrix_rank()", "path": "generated/torch.matrix_rank#torch.matrix_rank", "type": "torch", "text": "\nReturns the numerical rank of a 2-D tensor. The method to compute the matrix\nrank is done using SVD by default. If `symmetric` is `True`, then `input` is\nassumed to be symmetric, and the computation of the rank is done by obtaining\nthe eigenvalues.\n\n`tol` is the threshold below which the singular values (or the eigenvalues\nwhen `symmetric` is `True`) are considered to be 0. If `tol` is not specified,\n`tol` is set to `S.max() * max(S.size()) * eps` where `S` is the singular\nvalues (or the eigenvalues when `symmetric` is `True`), and `eps` is the\nepsilon value for the datatype of `input`.\n\nNote\n\n`torch.matrix_rank()` is deprecated. Please use `torch.linalg.matrix_rank()`\ninstead. The parameter `symmetric` was renamed in `torch.linalg.matrix_rank()`\nto `hermitian`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.max()", "path": "generated/torch.max#torch.max", "type": "torch", "text": "\nReturns the maximum value of all elements in the `input` tensor.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `max(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` is the maximum value\nof each row of the `input` tensor in the given dimension `dim`. And `indices`\nis the index location of each maximum value found (argmax).\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nIf there are multiple maximal values in a reduced row then the indices of the\nfirst maximal value are returned.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (max,\nmax_indices)\n\nExample:\n\nSee `torch.maximum()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.maximum()", "path": "generated/torch.maximum#torch.maximum", "type": "torch", "text": "\nComputes the element-wise maximum of `input` and `other`.\n\nNote\n\nIf one of the elements being compared is a NaN, then that element is returned.\n`maximum()` is not supported for tensors with complex dtypes.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mean()", "path": "generated/torch.mean#torch.mean", "type": "torch", "text": "\nReturns the mean value of all elements in the `input` tensor.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns the mean value of each row of the `input` tensor in the given\ndimension `dim`. If `dim` is a list of dimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.median()", "path": "generated/torch.median#torch.median", "type": "torch", "text": "\nReturns the median of the values in `input`.\n\nNote\n\nThe median is not unique for `input` tensors with an even number of elements.\nIn this case the lower of the two medians is returned. To compute the mean of\nboth medians, use `torch.quantile()` with `q=0.5` instead.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `median(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` contains the median of\neach row of `input` in the dimension `dim`, and `indices` contains the index\nof the median values found in the dimension `dim`.\n\nBy default, `dim` is the last dimension of the `input` tensor.\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the outputs tensor having 1\nfewer dimension than `input`.\n\nNote\n\nThe median is not unique for `input` tensors with an even number of elements\nin the dimension `dim`. In this case the lower of the two medians is returned.\nTo compute the mean of both medians in `input`, use `torch.quantile()` with\n`q=0.5` instead.\n\nWarning\n\n`indices` does not necessarily contain the first occurrence of each median\nvalue found, unless it is unique. The exact implementation details are device-\nspecific. Do not expect the same result when run on CPU and GPU in general.\nFor the same reason do not expect the gradients to be deterministic.\n\nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the\nmedian values and the second tensor, which must have dtype long, with their\nindices in the dimension `dim` of `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.meshgrid()", "path": "generated/torch.meshgrid#torch.meshgrid", "type": "torch", "text": "\nTake NN tensors, each of which can be either scalar or 1-dimensional vector,\nand create NN N-dimensional grids, where the ii th grid is defined by\nexpanding the ii th input over dimensions defined by other inputs.\n\ntensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars\nwill be treated as tensors of size (1,)(1,) automatically\n\nIf the input has kk tensors of size (N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots\n, (N_k,) , then the output would also have kk tensors, where all tensors are\nof size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k) .\n\nseq (sequence of Tensors)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.min()", "path": "generated/torch.min#torch.min", "type": "torch", "text": "\nReturns the minimum value of all elements in the `input` tensor.\n\nWarning\n\nThis function produces deterministic (sub)gradients unlike `min(dim=0)`\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` is the minimum value\nof each row of the `input` tensor in the given dimension `dim`. And `indices`\nis the index location of each minimum value found (argmin).\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nIf there are multiple minimal values in a reduced row then the indices of the\nfirst minimal value are returned.\n\nout (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)\n\nExample:\n\nSee `torch.minimum()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.minimum()", "path": "generated/torch.minimum#torch.minimum", "type": "torch", "text": "\nComputes the element-wise minimum of `input` and `other`.\n\nNote\n\nIf one of the elements being compared is a NaN, then that element is returned.\n`minimum()` is not supported for tensors with complex dtypes.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mm()", "path": "generated/torch.mm#torch.mm", "type": "torch", "text": "\nPerforms a matrix multiplication of the matrices `input` and `mat2`.\n\nIf `input` is a (n\u00d7m)(n \\times m) tensor, `mat2` is a (m\u00d7p)(m \\times p)\ntensor, `out` will be a (n\u00d7p)(n \\times p) tensor.\n\nNote\n\nThis function does not broadcast. For broadcasting matrix products, see\n`torch.matmul()`.\n\nSupports strided and sparse 2-D tensors as inputs, autograd with respect to\nstrided inputs.\n\nThis operator supports TensorFloat32.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mode()", "path": "generated/torch.mode#torch.mode", "type": "torch", "text": "\nReturns a namedtuple `(values, indices)` where `values` is the mode value of\neach row of the `input` tensor in the given dimension `dim`, i.e. a value\nwhich appears most often in that row, and `indices` is the index location of\neach mode value found.\n\nBy default, `dim` is the last dimension of the `input` tensor.\n\nIf `keepdim` is `True`, the output tensors are of the same size as `input`\nexcept in the dimension `dim` where they are of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensors having 1\nfewer dimension than `input`.\n\nNote\n\nThis function is not defined for `torch.cuda.Tensor` yet.\n\nout (tuple, optional) \u2013 the result tuple of two output tensors (values,\nindices)\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.moveaxis()", "path": "generated/torch.moveaxis#torch.moveaxis", "type": "torch", "text": "\nAlias for `torch.movedim()`.\n\nThis function is equivalent to NumPy\u2019s moveaxis function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.movedim()", "path": "generated/torch.movedim#torch.movedim", "type": "torch", "text": "\nMoves the dimension(s) of `input` at the position(s) in `source` to the\nposition(s) in `destination`.\n\nOther dimensions of `input` that are not explicitly moved remain in their\noriginal order and appear at the positions not specified in `destination`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.msort()", "path": "generated/torch.msort#torch.msort", "type": "torch", "text": "\nSorts the elements of the `input` tensor along its first dimension in\nascending order by value.\n\nNote\n\n`torch.msort(t)` is equivalent to `torch.sort(t, dim=0)[0]`. See also\n`torch.sort()`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mul()", "path": "generated/torch.mul#torch.mul", "type": "torch", "text": "\nMultiplies each element of the input `input` with the scalar `other` and\nreturns a new resulting tensor.\n\nIf `input` is of type `FloatTensor` or `DoubleTensor`, `other` should be a\nreal number, otherwise it should be an integer\n\n{out} \u2013\n\nExample:\n\nEach element of the tensor `input` is multiplied by the corresponding element\nof the Tensor `other`. The resulting tensor is returned.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multinomial()", "path": "generated/torch.multinomial#torch.multinomial", "type": "torch", "text": "\nReturns a tensor where each row contains `num_samples` indices sampled from\nthe multinomial probability distribution located in the corresponding row of\ntensor `input`.\n\nNote\n\nThe rows of `input` do not need to sum to one (in which case we use the values\nas weights), but must be non-negative, finite and have a non-zero sum.\n\nIndices are ordered from left to right according to when each was sampled\n(first samples are placed in first column).\n\nIf `input` is a vector, `out` is a vector of size `num_samples`.\n\nIf `input` is a matrix with `m` rows, `out` is an matrix of shape\n(m\u00d7num_samples)(m \\times \\text{num\\\\_samples}) .\n\nIf replacement is `True`, samples are drawn with replacement.\n\nIf not, they are drawn without replacement, which means that when a sample\nindex is drawn for a row, it cannot be drawn again for that row.\n\nNote\n\nWhen drawn without replacement, `num_samples` must be lower than number of\nnon-zero elements in `input` (or the min number of non-zero elements in each\nrow of `input` if it is a matrix).\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiply()", "path": "generated/torch.multiply#torch.multiply", "type": "torch", "text": "\nAlias for `torch.mul()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing", "path": "multiprocessing", "type": "torch.multiprocessing", "text": "\ntorch.multiprocessing is a wrapper around the native `multiprocessing` module.\nIt registers custom reducers, that use shared memory to provide shared views\non the same data in different processes. Once the tensor/storage is moved to\nshared_memory (see `share_memory_()`), it will be possible to send it to other\nprocesses without making any copies.\n\nThe API is 100% compatible with the original module - it\u2019s enough to change\n`import multiprocessing` to `import torch.multiprocessing` to have all the\ntensors sent through the queues or shared via other mechanisms, moved to\nshared memory.\n\nBecause of the similarity of APIs we do not document most of this package\ncontents, and we recommend referring to very good docs of the original module.\n\nWarning\n\nIf the main process exits abruptly (e.g. because of an incoming signal),\nPython\u2019s `multiprocessing` sometimes fails to clean up its children. It\u2019s a\nknown caveat, so if you\u2019re seeing any resource leaks after interrupting the\ninterpreter, it probably means that this has just happened to you.\n\nReturns a set of sharing strategies supported on a current system.\n\nReturns the current strategy for sharing CPU tensors.\n\nSets the strategy for sharing CPU tensors.\n\nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the\nvalues returned by `get_all_sharing_strategies()`.\n\nSharing CUDA tensors between processes is supported only in Python 3, using a\n`spawn` or `forkserver` start methods.\n\nUnlike CPU tensors, the sending process is required to keep the original\ntensor as long as the receiving process retains a copy of the tensor. The\nrefcounting is implemented under the hood but requires users to follow the\nnext best practices.\n\nWarning\n\nIf the consumer process dies abnormally to a fatal signal, the shared tensor\ncould be forever kept in memory as long as the sending process is running.\n\n2\\. Keep producer process running until all consumers exits. This will prevent\nthe situation when the producer process releasing memory which is still in use\nby the consumer.\n\nThis section provides a brief overview into how different sharing strategies\nwork. Note that it applies only to CPU tensor - CUDA tensors will always use\nthe CUDA API, as that\u2019s the only way they can be shared.\n\nNote\n\nThis is the default strategy (except for macOS and OS X where it\u2019s not\nsupported).\n\nThis strategy will use file descriptors as shared memory handles. Whenever a\nstorage is moved to shared memory, a file descriptor obtained from `shm_open`\nis cached with the object, and when it\u2019s going to be sent to other processes,\nthe file descriptor will be transferred (e.g. via UNIX sockets) to it. The\nreceiver will also cache the file descriptor and `mmap` it, to obtain a shared\nview onto the storage data.\n\nNote that if there will be a lot of tensors shared, this strategy will keep a\nlarge number of file descriptors open most of the time. If your system has low\nlimits for the number of open file descriptors, and you can\u2019t raise them, you\nshould use the `file_system` strategy.\n\nThis strategy will use file names given to `shm_open` to identify the shared\nmemory regions. This has a benefit of not requiring the implementation to\ncache the file descriptors obtained from it, but at the same time is prone to\nshared memory leaks. The file can\u2019t be deleted right after its creation,\nbecause other processes need to access it to open their views. If the\nprocesses fatally crash, or are killed, and don\u2019t call the storage\ndestructors, the files will remain in the system. This is very serious,\nbecause they keep using up the memory until the system is restarted, or\nthey\u2019re freed manually.\n\nTo counter the problem of shared memory file leaks, `torch.multiprocessing`\nwill spawn a daemon named `torch_shm_manager` that will isolate itself from\nthe current process group, and will keep track of all shared memory\nallocations. Once all processes connected to it exit, it will wait a moment to\nensure there will be no new connections, and will iterate over all shared\nmemory files allocated by the group. If it finds that any of them still exist,\nthey will be deallocated. We\u2019ve tested this method and it proved to be robust\nto various failures. Still, if your system has high enough limits, and\n`file_descriptor` is a supported strategy, we do not recommend switching to\nthis one.\n\nNote\n\nAvailable for Python >= 3.4.\n\nThis depends on the `spawn` start method in Python\u2019s `multiprocessing`\npackage.\n\nSpawning a number of subprocesses to perform some function can be done by\ncreating `Process` instances and calling `join` to wait for their completion.\nThis approach works fine when dealing with a single subprocess but presents\npotential issues when dealing with multiple processes.\n\nNamely, joining processes sequentially implies they will terminate\nsequentially. If they don\u2019t, and the first process does not terminate, the\nprocess termination will go unnoticed. Also, there are no native facilities\nfor error propagation.\n\nThe `spawn` function below addresses these concerns and takes care of error\npropagation, out of order termination, and will actively terminate processes\nupon detecting an error in one of them.\n\nSpawns `nprocs` processes that run `fn` with `args`.\n\nIf one of the processes exits with a non-zero exit status, the remaining\nprocesses are killed and an exception is raised with the cause of termination.\nIn the case an exception was caught in the child process, it is forwarded and\nits traceback is included in the exception raised in the parent process.\n\nfn (function) \u2013\n\nFunction is called as the entrypoint of the spawned process. This function\nmust be defined at the top level of a module so it can be pickled and spawned.\nThis is a requirement imposed by multiprocessing.\n\nThe function is called as `fn(i, *args)`, where `i` is the process index and\n`args` is the passed through tuple of arguments.\n\nNone if `join` is `True`, `ProcessContext` if `join` is `False`\n\nReturned by `spawn()` when called with `join=False`.\n\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_all_sharing_strategies()", "path": "multiprocessing#torch.multiprocessing.get_all_sharing_strategies", "type": "torch.multiprocessing", "text": "\nReturns a set of sharing strategies supported on a current system.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.get_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.get_sharing_strategy", "type": "torch.multiprocessing", "text": "\nReturns the current strategy for sharing CPU tensors.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.set_sharing_strategy()", "path": "multiprocessing#torch.multiprocessing.set_sharing_strategy", "type": "torch.multiprocessing", "text": "\nSets the strategy for sharing CPU tensors.\n\nnew_strategy (str) \u2013 Name of the selected strategy. Should be one of the\nvalues returned by `get_all_sharing_strategies()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.spawn()", "path": "multiprocessing#torch.multiprocessing.spawn", "type": "torch.multiprocessing", "text": "\nSpawns `nprocs` processes that run `fn` with `args`.\n\nIf one of the processes exits with a non-zero exit status, the remaining\nprocesses are killed and an exception is raised with the cause of termination.\nIn the case an exception was caught in the child process, it is forwarded and\nits traceback is included in the exception raised in the parent process.\n\nfn (function) \u2013\n\nFunction is called as the entrypoint of the spawned process. This function\nmust be defined at the top level of a module so it can be pickled and spawned.\nThis is a requirement imposed by multiprocessing.\n\nThe function is called as `fn(i, *args)`, where `i` is the process index and\n`args` is the passed through tuple of arguments.\n\nNone if `join` is `True`, `ProcessContext` if `join` is `False`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext", "path": "multiprocessing#torch.multiprocessing.SpawnContext", "type": "torch.multiprocessing", "text": "\nReturned by `spawn()` when called with `join=False`.\n\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.multiprocessing.SpawnContext.join()", "path": "multiprocessing#torch.multiprocessing.SpawnContext.join", "type": "torch.multiprocessing", "text": "\nTries to join one or more processes in this spawn context. If one of them\nexited with a non-zero exit status, this function kills the remaining\nprocesses and raises an exception with the cause of the first process exiting.\n\nReturns `True` if all processes have been joined successfully, `False` if\nthere are more processes that need to be joined.\n\ntimeout (float) \u2013 Wait this long before giving up on waiting.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mv()", "path": "generated/torch.mv#torch.mv", "type": "torch", "text": "\nPerforms a matrix-vector product of the matrix `input` and the vector `vec`.\n\nIf `input` is a (n\u00d7m)(n \\times m) tensor, `vec` is a 1-D tensor of size mm ,\n`out` will be 1-D of size nn .\n\nNote\n\nThis function does not broadcast.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.mvlgamma()", "path": "generated/torch.mvlgamma#torch.mvlgamma", "type": "torch", "text": "\nComputes the multivariate log-gamma function) with dimension pp element-wise,\ngiven by\n\nwhere C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4} and\n\u0393(\u22c5)\\Gamma(\\cdot) is the Gamma function.\n\nAll elements must be greater than p\u221212\\frac{p - 1}{2} , otherwise an error\nwould be thrown.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nanmedian()", "path": "generated/torch.nanmedian#torch.nanmedian", "type": "torch", "text": "\nReturns the median of the values in `input`, ignoring `NaN` values.\n\nThis function is identical to `torch.median()` when there are no `NaN` values\nin `input`. When `input` has one or more `NaN` values, `torch.median()` will\nalways return `NaN`, while this function will return the median of the\nnon-`NaN` elements in `input`. If all the elements in `input` are `NaN` it\nwill also return `NaN`.\n\ninput (Tensor) \u2013 the input tensor.\n\nExample:\n\nReturns a namedtuple `(values, indices)` where `values` contains the median of\neach row of `input` in the dimension `dim`, ignoring `NaN` values, and\n`indices` contains the index of the median values found in the dimension\n`dim`.\n\nThis function is identical to `torch.median()` when there are no `NaN` values\nin a reduced row. When a reduced row has one or more `NaN` values,\n`torch.median()` will always reduce it to `NaN`, while this function will\nreduce it to the median of the non-`NaN` elements. If all the elements in a\nreduced row are `NaN` then it will be reduced to `NaN`, too.\n\nout ((Tensor, Tensor), optional) \u2013 The first tensor will be populated with the\nmedian values and the second tensor, which must have dtype long, with their\nindices in the dimension `dim` of `input`.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nanquantile()", "path": "generated/torch.nanquantile#torch.nanquantile", "type": "torch", "text": "\nThis is a variant of `torch.quantile()` that \u201cignores\u201d `NaN` values, computing\nthe quantiles `q` as if `NaN` values in `input` did not exist. If all values\nin a reduced row are `NaN` then the quantiles for that reduction will be\n`NaN`. See the documentation for `torch.quantile()`.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nansum()", "path": "generated/torch.nansum#torch.nansum", "type": "torch", "text": "\nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.\n\ninput (Tensor) \u2013 the input tensor.\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\nReturns the sum of each row of the `input` tensor in the given dimension\n`dim`, treating Not a Numbers (NaNs) as zero. If `dim` is a list of\ndimensions, reduce over all of them.\n\nIf `keepdim` is `True`, the output tensor is of the same size as `input`\nexcept in the dimension(s) `dim` where it is of size 1. Otherwise, `dim` is\nsqueezed (see `torch.squeeze()`), resulting in the output tensor having 1 (or\n`len(dim)`) fewer dimension(s).\n\ndtype (`torch.dtype`, optional) \u2013 the desired data type of returned tensor. If\nspecified, the input tensor is casted to `dtype` before the operation is\nperformed. This is useful for preventing data type overflows. Default: None.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nan_to_num()", "path": "generated/torch.nan_to_num#torch.nan_to_num", "type": "torch", "text": "\nReplaces `NaN`, positive infinity, and negative infinity values in `input`\nwith the values specified by `nan`, `posinf`, and `neginf`, respectively. By\ndefault, `NaN`s are replaced with zero, positive infinity is replaced with the\ngreatest finite value representable by :attr:`input`\u2019s dtype, and negative\ninfinity is replaced with the least finite value representable by `input`\u2019s\ndtype.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.narrow()", "path": "generated/torch.narrow#torch.narrow", "type": "torch", "text": "\nReturns a new tensor that is a narrowed version of `input` tensor. The\ndimension `dim` is input from `start` to `start + length`. The returned tensor\nand `input` tensor share the same underlying storage.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.ne()", "path": "generated/torch.ne#torch.ne", "type": "torch", "text": "\nComputes input\u2260other\\text{input} \\neq \\text{other} element-wise.\n\nThe second argument can be a number or a tensor whose shape is broadcastable\nwith the first argument.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nA boolean tensor that is True where `input` is not equal to `other` and False\nelsewhere\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.neg()", "path": "generated/torch.neg#torch.neg", "type": "torch", "text": "\nReturns a new tensor with the negative of the elements of `input`.\n\ninput (Tensor) \u2013 the input tensor.\n\nout (Tensor, optional) \u2013 the output tensor.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.negative()", "path": "generated/torch.negative#torch.negative", "type": "torch", "text": "\nAlias for `torch.neg()`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nextafter()", "path": "generated/torch.nextafter#torch.nextafter", "type": "torch", "text": "\nReturn the next floating-point value after `input` towards `other`,\nelementwise.\n\nThe shapes of `input` and `other` must be broadcastable.\n\nout (Tensor, optional) \u2013 the output tensor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn", "path": "nn", "type": "torch.nn", "text": "\nThese are the basic building block for graphs\n\ntorch.nn\n\nA kind of Tensor that is to be considered a module parameter.\n\nA parameter that is not initialized.\n\nBase class for all neural network modules.\n\nA sequential container.\n\nHolds submodules in a list.\n\nHolds submodules in a dictionary.\n\nHolds parameters in a list.\n\nHolds parameters in a dictionary.\n\nGlobal Hooks For Module\n\nRegisters a forward pre-hook common to all modules.\n\nRegisters a global forward hook for all the modules\n\nRegisters a backward hook common to all the modules.\n\n`nn.Conv1d`\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\n`nn.Conv2d`\n\nApplies a 2D convolution over an input signal composed of several input\nplanes.\n\n`nn.Conv3d`\n\nApplies a 3D convolution over an input signal composed of several input\nplanes.\n\n`nn.ConvTranspose1d`\n\nApplies a 1D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.ConvTranspose2d`\n\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.ConvTranspose3d`\n\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes.\n\n`nn.LazyConv1d`\n\nA `torch.nn.Conv1d` module with lazy initialization of the `in_channels`\nargument of the `Conv1d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConv2d`\n\nA `torch.nn.Conv2d` module with lazy initialization of the `in_channels`\nargument of the `Conv2d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConv3d`\n\nA `torch.nn.Conv3d` module with lazy initialization of the `in_channels`\nargument of the `Conv3d` that is inferred from the `input.size(1)`.\n\n`nn.LazyConvTranspose1d`\n\nA `torch.nn.ConvTranspose1d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose1d` that is inferred from the\n`input.size(1)`.\n\n`nn.LazyConvTranspose2d`\n\nA `torch.nn.ConvTranspose2d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose2d` that is inferred from the\n`input.size(1)`.\n\n`nn.LazyConvTranspose3d`\n\nA `torch.nn.ConvTranspose3d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose3d` that is inferred from the\n`input.size(1)`.\n\n`nn.Unfold`\n\nExtracts sliding local blocks from a batched input tensor.\n\n`nn.Fold`\n\nCombines an array of sliding local blocks into a large containing tensor.\n\n`nn.MaxPool1d`\n\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxPool2d`\n\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxPool3d`\n\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\n`nn.MaxUnpool1d`\n\nComputes a partial inverse of `MaxPool1d`.\n\n`nn.MaxUnpool2d`\n\nComputes a partial inverse of `MaxPool2d`.\n\n`nn.MaxUnpool3d`\n\nComputes a partial inverse of `MaxPool3d`.\n\n`nn.AvgPool1d`\n\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\n`nn.AvgPool2d`\n\nApplies a 2D average pooling over an input signal composed of several input\nplanes.\n\n`nn.AvgPool3d`\n\nApplies a 3D average pooling over an input signal composed of several input\nplanes.\n\n`nn.FractionalMaxPool2d`\n\nApplies a 2D fractional max pooling over an input signal composed of several\ninput planes.\n\n`nn.LPPool1d`\n\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes.\n\n`nn.LPPool2d`\n\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool1d`\n\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool2d`\n\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveMaxPool3d`\n\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool1d`\n\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool2d`\n\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.AdaptiveAvgPool3d`\n\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\n`nn.ReflectionPad1d`\n\nPads the input tensor using the reflection of the input boundary.\n\n`nn.ReflectionPad2d`\n\nPads the input tensor using the reflection of the input boundary.\n\n`nn.ReplicationPad1d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ReplicationPad2d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ReplicationPad3d`\n\nPads the input tensor using replication of the input boundary.\n\n`nn.ZeroPad2d`\n\nPads the input tensor boundaries with zero.\n\n`nn.ConstantPad1d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ConstantPad2d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ConstantPad3d`\n\nPads the input tensor boundaries with a constant value.\n\n`nn.ELU`\n\nApplies the element-wise function:\n\n`nn.Hardshrink`\n\nApplies the hard shrinkage function element-wise:\n\n`nn.Hardsigmoid`\n\nApplies the element-wise function:\n\n`nn.Hardtanh`\n\nApplies the HardTanh function element-wise\n\n`nn.Hardswish`\n\nApplies the hardswish function, element-wise, as described in the paper:\n\n`nn.LeakyReLU`\n\nApplies the element-wise function:\n\n`nn.LogSigmoid`\n\nApplies the element-wise function:\n\n`nn.MultiheadAttention`\n\nAllows the model to jointly attend to information from different\nrepresentation subspaces.\n\n`nn.PReLU`\n\nApplies the element-wise function:\n\n`nn.ReLU`\n\nApplies the rectified linear unit function element-wise:\n\n`nn.ReLU6`\n\nApplies the element-wise function:\n\n`nn.RReLU`\n\nApplies the randomized leaky rectified liner unit function, element-wise, as\ndescribed in the paper:\n\n`nn.SELU`\n\nApplied element-wise, as:\n\n`nn.CELU`\n\nApplies the element-wise function:\n\n`nn.GELU`\n\nApplies the Gaussian Error Linear Units function:\n\n`nn.Sigmoid`\n\nApplies the element-wise function:\n\n`nn.SiLU`\n\nApplies the silu function, element-wise.\n\n`nn.Softplus`\n\nApplies the element-wise function:\n\n`nn.Softshrink`\n\nApplies the soft shrinkage function elementwise:\n\n`nn.Softsign`\n\nApplies the element-wise function:\n\n`nn.Tanh`\n\nApplies the element-wise function:\n\n`nn.Tanhshrink`\n\nApplies the element-wise function:\n\n`nn.Threshold`\n\nThresholds each element of the input Tensor.\n\n`nn.Softmin`\n\nApplies the Softmin function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range `[0,\n1]` and sum to 1.\n\n`nn.Softmax`\n\nApplies the Softmax function to an n-dimensional input Tensor rescaling them\nso that the elements of the n-dimensional output Tensor lie in the range [0,1]\nand sum to 1.\n\n`nn.Softmax2d`\n\nApplies SoftMax over features to each spatial location.\n\n`nn.LogSoftmax`\n\nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an\nn-dimensional input Tensor.\n\n`nn.AdaptiveLogSoftmaxWithLoss`\n\nEfficient softmax approximation as described in Efficient softmax\napproximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David\nGrangier, and Herv\u00e9 J\u00e9gou.\n\n`nn.BatchNorm1d`\n\nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs\nwith optional additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n`nn.BatchNorm2d`\n\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n`nn.BatchNorm3d`\n\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\n`nn.GroupNorm`\n\nApplies Group Normalization over a mini-batch of inputs as described in the\npaper Group Normalization\n\n`nn.SyncBatchNorm`\n\nApplies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D\ninputs with additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\n`nn.InstanceNorm1d`\n\nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with\noptional additional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.InstanceNorm2d`\n\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.InstanceNorm3d`\n\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\n`nn.LayerNorm`\n\nApplies Layer Normalization over a mini-batch of inputs as described in the\npaper Layer Normalization\n\n`nn.LocalResponseNorm`\n\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension.\n\n`nn.RNNBase`\n\n`nn.RNN`\n\nApplies a multi-layer Elman RNN with tanh\u2061\\tanh or ReLU\\text{ReLU} non-\nlinearity to an input sequence.\n\n`nn.LSTM`\n\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\n`nn.GRU`\n\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\n`nn.RNNCell`\n\nAn Elman RNN cell with tanh or ReLU non-linearity.\n\n`nn.LSTMCell`\n\nA long short-term memory (LSTM) cell.\n\n`nn.GRUCell`\n\nA gated recurrent unit (GRU) cell\n\n`nn.Transformer`\n\nA transformer model.\n\n`nn.TransformerEncoder`\n\nTransformerEncoder is a stack of N encoder layers\n\n`nn.TransformerDecoder`\n\nTransformerDecoder is a stack of N decoder layers\n\n`nn.TransformerEncoderLayer`\n\nTransformerEncoderLayer is made up of self-attn and feedforward network.\n\n`nn.TransformerDecoderLayer`\n\nTransformerDecoderLayer is made up of self-attn, multi-head-attn and\nfeedforward network.\n\n`nn.Identity`\n\nA placeholder identity operator that is argument-insensitive.\n\n`nn.Linear`\n\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b\n\n`nn.Bilinear`\n\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\n`nn.LazyLinear`\n\nA `torch.nn.Linear` module with lazy initialization.\n\n`nn.Dropout`\n\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\n`nn.Dropout2d`\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ).\n\n`nn.Dropout3d`\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ).\n\n`nn.AlphaDropout`\n\nApplies Alpha Dropout over the input.\n\n`nn.Embedding`\n\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\n`nn.EmbeddingBag`\n\nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the\nintermediate embeddings.\n\n`nn.CosineSimilarity`\n\nReturns cosine similarity between x1x_1 and x2x_2 , computed along dim.\n\n`nn.PairwiseDistance`\n\nComputes the batchwise pairwise distance between vectors v1v_1 , v2v_2 using\nthe p-norm:\n\n`nn.L1Loss`\n\nCreates a criterion that measures the mean absolute error (MAE) between each\nelement in the input xx and target yy .\n\n`nn.MSELoss`\n\nCreates a criterion that measures the mean squared error (squared L2 norm)\nbetween each element in the input xx and target yy .\n\n`nn.CrossEntropyLoss`\n\nThis criterion combines `LogSoftmax` and `NLLLoss` in one single class.\n\n`nn.CTCLoss`\n\nThe Connectionist Temporal Classification loss.\n\n`nn.NLLLoss`\n\nThe negative log likelihood loss.\n\n`nn.PoissonNLLLoss`\n\nNegative log likelihood loss with Poisson distribution of target.\n\n`nn.GaussianNLLLoss`\n\nGaussian negative log likelihood loss.\n\n`nn.KLDivLoss`\n\nThe Kullback-Leibler divergence loss measure\n\n`nn.BCELoss`\n\nCreates a criterion that measures the Binary Cross Entropy between the target\nand the output:\n\n`nn.BCEWithLogitsLoss`\n\nThis loss combines a `Sigmoid` layer and the `BCELoss` in one single class.\n\n`nn.MarginRankingLoss`\n\nCreates a criterion that measures the loss given inputs x1x1 , x2x2 , two 1D\nmini-batch `Tensors`, and a label 1D mini-batch tensor yy (containing 1 or\n-1).\n\n`nn.HingeEmbeddingLoss`\n\nMeasures the loss given an input tensor xx and a labels tensor yy (containing\n1 or -1).\n\n`nn.MultiLabelMarginLoss`\n\nCreates a criterion that optimizes a multi-class multi-classification hinge\nloss (margin-based loss) between input xx (a 2D mini-batch `Tensor`) and\noutput yy (which is a 2D `Tensor` of target class indices).\n\n`nn.SmoothL1Loss`\n\nCreates a criterion that uses a squared term if the absolute element-wise\nerror falls below beta and an L1 term otherwise.\n\n`nn.SoftMarginLoss`\n\nCreates a criterion that optimizes a two-class classification logistic loss\nbetween input tensor xx and target tensor yy (containing 1 or -1).\n\n`nn.MultiLabelSoftMarginLoss`\n\nCreates a criterion that optimizes a multi-label one-versus-all loss based on\nmax-entropy, between input xx and target yy of size (N,C)(N, C) .\n\n`nn.CosineEmbeddingLoss`\n\nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2\nand a `Tensor` label yy with values 1 or -1.\n\n`nn.MultiMarginLoss`\n\nCreates a criterion that optimizes a multi-class classification hinge loss\n(margin-based loss) between input xx (a 2D mini-batch `Tensor`) and output yy\n(which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq\n\\text{x.size}(1)-1 ):\n\n`nn.TripletMarginLoss`\n\nCreates a criterion that measures the triplet loss given an input tensors x1x1\n, x2x2 , x3x3 and a margin with a value greater than 00 .\n\n`nn.TripletMarginWithDistanceLoss`\n\nCreates a criterion that measures the triplet loss given input tensors aa , pp\n, and nn (representing anchor, positive, and negative examples, respectively),\nand a nonnegative, real-valued function (\u201cdistance function\u201d) used to compute\nthe relationship between the anchor and positive example (\u201cpositive distance\u201d)\nand the anchor and negative example (\u201cnegative distance\u201d).\n\n`nn.PixelShuffle`\n\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nan upscale factor.\n\n`nn.PixelUnshuffle`\n\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is a downscale factor.\n\n`nn.Upsample`\n\nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric)\ndata.\n\n`nn.UpsamplingNearest2d`\n\nApplies a 2D nearest neighbor upsampling to an input signal composed of\nseveral input channels.\n\n`nn.UpsamplingBilinear2d`\n\nApplies a 2D bilinear upsampling to an input signal composed of several input\nchannels.\n\n`nn.ChannelShuffle`\n\nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups\nand rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the\noriginal tensor shape.\n\n`nn.DataParallel`\n\nImplements data parallelism at the module level.\n\n`nn.parallel.DistributedDataParallel`\n\nImplements distributed data parallelism that is based on `torch.distributed`\npackage at the module level.\n\nFrom the `torch.nn.utils` module\n\nClips gradient norm of an iterable of parameters.\n\nClips gradient of an iterable of parameters at specified value.\n\nConvert parameters to one vector\n\nConvert one vector to the parameters\n\n`prune.BasePruningMethod`\n\nAbstract base class for creation of new pruning techniques.\n\n`prune.PruningContainer`\n\nContainer holding a sequence of pruning methods for iterative pruning.\n\n`prune.Identity`\n\nUtility pruning method that does not prune any units but generates the pruning\nparametrization with a mask of ones.\n\n`prune.RandomUnstructured`\n\nPrune (currently unpruned) units in a tensor at random.\n\n`prune.L1Unstructured`\n\nPrune (currently unpruned) units in a tensor by zeroing out the ones with the\nlowest L1-norm.\n\n`prune.RandomStructured`\n\nPrune entire (currently unpruned) channels in a tensor at random.\n\n`prune.LnStructured`\n\nPrune entire (currently unpruned) channels in a tensor based on their Ln-norm.\n\n`prune.CustomFromMask`\n\n`prune.identity`\n\nApplies pruning reparametrization to the tensor corresponding to the parameter\ncalled `name` in `module` without actually pruning any units.\n\n`prune.random_unstructured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units selected at random.\n\n`prune.l1_unstructured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) units with the lowest L1-norm.\n\n`prune.random_structured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` selected at random.\n\n`prune.ln_structured`\n\nPrunes tensor corresponding to parameter called `name` in `module` by removing\nthe specified `amount` of (currently unpruned) channels along the specified\n`dim` with the lowest L``n``-norm.\n\n`prune.global_unstructured`\n\nGlobally prunes tensors corresponding to all parameters in `parameters` by\napplying the specified `pruning_method`.\n\n`prune.custom_from_mask`\n\nPrunes tensor corresponding to parameter called `name` in `module` by applying\nthe pre-computed mask in `mask`.\n\n`prune.remove`\n\nRemoves the pruning reparameterization from a module and the pruning method\nfrom the forward hook.\n\n`prune.is_pruned`\n\nCheck whether `module` is pruned by looking for `forward_pre_hooks` in its\nmodules that inherit from the `BasePruningMethod`.\n\nApplies weight normalization to a parameter in the given module.\n\nRemoves the weight normalization reparameterization from a module.\n\nApplies spectral normalization to a parameter in the given module.\n\nRemoves the spectral normalization reparameterization from a module.\n\nUtility functions in other modules\n\n`nn.utils.rnn.PackedSequence`\n\nHolds the data and list of `batch_sizes` of a packed sequence.\n\n`nn.utils.rnn.pack_padded_sequence`\n\nPacks a Tensor containing padded sequences of variable length.\n\n`nn.utils.rnn.pad_packed_sequence`\n\nPads a packed batch of variable length sequences.\n\n`nn.utils.rnn.pad_sequence`\n\nPad a list of variable length Tensors with `padding_value`\n\n`nn.utils.rnn.pack_sequence`\n\nPacks a list of variable length Tensors\n\n`nn.Flatten`\n\nFlattens a contiguous range of dims into a tensor.\n\n`nn.Unflatten`\n\nUnflattens a tensor dim expanding it to a desired shape.\n\nQuantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. PyTorch supports\nboth per tensor and per channel asymmetric linear quantization. To learn more\nhow to use quantized functions in PyTorch, please refer to the Quantization\ndocumentation.\n\n`nn.modules.lazy.LazyModuleMixin`\n\nA mixin for modules that lazily initialize parameters, also known as \u201clazy\nmodules.\u201d\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool1d", "path": "generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output size is H, for any input size. The number of output features is\nequal to the number of input planes.\n\noutput_size \u2013 the target output size H\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool2d", "path": "generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output is of size H x W, for any input size. The number of output features\nis equal to the number of input planes.\n\noutput_size \u2013 the target output size of the image of the form H x W. Can be a\ntuple (H, W) or a single H for a square image H x H. H and W can be either a\n`int`, or `None` which means the size will be the same as that of the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveAvgPool3d", "path": "generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nThe output is of size D x H x W, for any input size. The number of output\nfeatures is equal to the number of input planes.\n\noutput_size \u2013 the target output size of the form D x H x W. Can be a tuple (D,\nH, W) or a single number D for a cube D x D x D. D, H and W can be either a\n`int`, or `None` which means the size will be the same as that of the input.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "torch.nn", "text": "\nEfficient softmax approximation as described in Efficient softmax\napproximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David\nGrangier, and Herv\u00e9 J\u00e9gou.\n\nAdaptive softmax is an approximate strategy for training models with large\noutput spaces. It is most effective when the label distribution is highly\nimbalanced, for example in natural language modelling, where the word\nfrequency distribution approximately follows the Zipf\u2019s law.\n\nAdaptive softmax partitions the labels into several clusters, according to\ntheir frequency. These clusters may contain different number of targets each.\nAdditionally, clusters containing less frequent labels assign lower\ndimensional embeddings to those labels, which speeds up the computation. For\neach minibatch, only clusters for which at least one target is present are\nevaluated.\n\nThe idea is that the clusters which are accessed frequently (like the first\none, containing most frequent labels), should also be cheap to compute \u2013 that\nis, contain a small number of assigned labels.\n\nWe highly recommend taking a look at the original paper for more details.\n\nWarning\n\nLabels passed as inputs to this module should be sorted according to their\nfrequency. This means that the most frequent label should be represented by\nthe index `0`, and the least frequent label should be represented by the index\n`n_classes - 1`.\n\nNote\n\nThis module returns a `NamedTuple` with `output` and `loss` fields. See\nfurther documentation for details.\n\nNote\n\nTo compute log-probabilities for all classes, the `log_prob` method can be\nused.\n\n`NamedTuple` with `output` and `loss` fields\n\nComputes log probabilities for all n_classes\\texttt{n\\\\_classes}\n\ninput (Tensor) \u2013 a minibatch of examples\n\nlog-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <=\n\\texttt{n\\\\_classes} , where n_classes\\texttt{n\\\\_classes} is a parameter\npassed to `AdaptiveLogSoftmaxWithLoss` constructor.\n\nThis is equivalent to `self.log_pob(input).argmax(dim=1)`, but is more\nefficient in some cases.\n\ninput (Tensor) \u2013 a minibatch of examples\n\na class with the highest probability for each example\n\noutput (Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "torch.nn", "text": "\nComputes log probabilities for all n_classes\\texttt{n\\\\_classes}\n\ninput (Tensor) \u2013 a minibatch of examples\n\nlog-probabilities of for each class cc in range 0<=c<=n_classes0 <= c <=\n\\texttt{n\\\\_classes} , where n_classes\\texttt{n\\\\_classes} is a parameter\npassed to `AdaptiveLogSoftmaxWithLoss` constructor.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict()", "path": "generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "torch.nn", "text": "\nThis is equivalent to `self.log_pob(input).argmax(dim=1)`, but is more\nefficient in some cases.\n\ninput (Tensor) \u2013 a minibatch of examples\n\na class with the highest probability for each example\n\noutput (Tensor)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool1d", "path": "generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d", "type": "torch.nn", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output size is H, for any input size. The number of output features is\nequal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool2d", "path": "generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output is of size H x W, for any input size. The number of output features\nis equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AdaptiveMaxPool3d", "path": "generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d", "type": "torch.nn", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nThe output is of size D x H x W, for any input size. The number of output\nfeatures is equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AlphaDropout", "path": "generated/torch.nn.alphadropout#torch.nn.AlphaDropout", "type": "torch.nn", "text": "\nApplies Alpha Dropout over the input.\n\nAlpha Dropout is a type of Dropout that maintains the self-normalizing\nproperty. For an input with zero mean and unit standard deviation, the output\nof Alpha Dropout maintains the original mean and standard deviation of the\ninput. Alpha Dropout goes hand-in-hand with SELU activation function, which\nensures that the outputs have zero mean and unit standard deviation.\n\nDuring training, it randomly masks some of the elements of the input tensor\nwith probability p using samples from a bernoulli distribution. The elements\nto masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit standard deviation.\n\nDuring evaluation the module simply computes an identity function.\n\nMore details can be found in the paper Self-Normalizing Neural Networks .\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool1d", "path": "generated/torch.nn.avgpool1d#torch.nn.AvgPool1d", "type": "torch.nn", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size (N,C,L)(N,\nC, L) , output (N,C,Lout)(N, C, L_{out}) and `kernel_size` kk can be precisely\ndescribed as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding` can each be an `int` or a\none-element tuple.\n\nOutput: (N,C,Lout)(N, C, L_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool2d", "path": "generated/torch.nn.avgpool2d#torch.nn.AvgPool2d", "type": "torch.nn", "text": "\nApplies a 2D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,H,W)(N, C, H, W) , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) and\n`kernel_size` (kH,kW)(kH, kW) can be precisely described as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on both\nsides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride`, `padding` can either be:\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.AvgPool3d", "path": "generated/torch.nn.avgpool3d#torch.nn.AvgPool3d", "type": "torch.nn", "text": "\nApplies a 3D average pooling over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,C,D,H,W)(N, C, D, H, W) , output (N,C,Dout,Hout,Wout)(N, C, D_{out},\nH_{out}, W_{out}) and `kernel_size` (kD,kH,kW)(kD, kH, kW) can be precisely\ndescribed as:\n\nIf `padding` is non-zero, then the input is implicitly zero-padded on all\nthree sides for `padding` number of points.\n\nNote\n\nWhen ceil_mode=True, sliding windows are allowed to go off-bounds if they\nstart within the left padding or the input. Sliding windows that would start\nin the right padded region are ignored.\n\nThe parameters `kernel_size`, `stride` can either be:\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm1d", "path": "generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs\nwith optional additional channel dimension) as described in the paper Batch\nNormalization: Accelerating Deep Network Training by Reducing Internal\nCovariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, L)` slices, it\u2019s common terminology to call this Temporal\nBatch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm2d", "path": "generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, H, W)` slices, it\u2019s common terminology to call this Spatial\nBatch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BatchNorm3d", "path": "generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d", "type": "torch.nn", "text": "\nApplies Batch Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Batch Normalization:\nAccelerating Deep Network Training by Reducing Internal Covariate Shift .\n\nThe mean and standard-deviation are calculated per-dimension over the mini-\nbatches and \u03b3\\gamma and \u03b2\\beta are learnable parameter vectors of size `C`\n(where `C` is the input size). By default, the elements of \u03b3\\gamma are set to\n1 and the elements of \u03b2\\beta are set to 0. The standard-deviation is\ncalculated via the biased estimator, equivalent to `torch.var(input,\nunbiased=False)`.\n\nAlso by default, during training this layer keeps running estimates of its\ncomputed mean and variance, which are then used for normalization during\nevaluation. The running estimates are kept with a default `momentum` of 0.1.\n\nIf `track_running_stats` is set to `False`, this layer then does not keep\nrunning estimates, and batch statistics are instead used during evaluation\ntime as well.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nBecause the Batch Normalization is done over the `C` dimension, computing\nstatistics on `(N, D, H, W)` slices, it\u2019s common terminology to call this\nVolumetric Batch Normalization or Spatio-temporal Batch Normalization.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BCELoss", "path": "generated/torch.nn.bceloss#torch.nn.BCELoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the Binary Cross Entropy between the target\nand the output:\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen\n\nThis is used for measuring the error of a reconstruction in for example an\nauto-encoder. Note that the targets yy should be numbers between 0 and 1.\n\nNotice that if xnx_n is either 0 or 1, one of the log terms would be\nmathematically undefined in the above loss equation. PyTorch chooses to set\nlog\u2061(0)=\u2212\u221e\\log (0) = -\\infty , since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) =\n-\\infty . However, an infinite term in the loss equation is not desirable for\nseveral reasons.\n\nFor one, if either yn=0y_n = 0 or (1\u2212yn)=0(1 - y_n) = 0 , then we would be\nmultiplying 0 with infinity. Secondly, if we have an infinite loss value, then\nwe would also have an infinite term in our gradient, since\nlim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\infty . This would\nmake BCELoss\u2019s backward method nonlinear with respect to xnx_n , and using it\nfor things like linear regression would not be straight-forward.\n\nOur solution is that BCELoss clamps its log function outputs to be greater\nthan or equal to -100. This way, we can always have a finite loss value and a\nlinear backward method.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.BCEWithLogitsLoss", "path": "generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss", "type": "torch.nn", "text": "\nThis loss combines a `Sigmoid` layer and the `BCELoss` in one single class.\nThis version is more numerically stable than using a plain `Sigmoid` followed\nby a `BCELoss` as, by combining the operations into one layer, we take\nadvantage of the log-sum-exp trick for numerical stability.\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen\n\nThis is used for measuring the error of a reconstruction in for example an\nauto-encoder. Note that the targets `t[i]` should be numbers between 0 and 1.\n\nIt\u2019s possible to trade off recall and precision by adding weights to positive\nexamples. In the case of multi-label classification the loss can be described\nas:\n\nwhere cc is the class number (c>1c > 1 for multi-label binary classification,\nc=1c = 1 for single-label binary classification), nn is the number of the\nsample in the batch and pcp_c is the weight of the positive answer for the\nclass cc .\n\npc>1p_c > 1 increases the recall, pc<1p_c < 1 increases the precision.\n\nFor example, if a dataset contains 100 positive and 300 negative examples of a\nsingle class, then `pos_weight` for the class should be equal to\n300100=3\\frac{300}{100}=3 . The loss would act as if the dataset contains\n3\u00d7100=3003\\times 100=300 positive examples.\n\nExamples:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Bilinear", "path": "generated/torch.nn.bilinear#torch.nn.Bilinear", "type": "torch.nn", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CELU", "path": "generated/torch.nn.celu#torch.nn.CELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nMore details can be found in the paper Continuously Differentiable Exponential\nLinear Units .\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ChannelShuffle", "path": "generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle", "type": "torch.nn", "text": "\nDivide the channels in a tensor of shape (\u2217,C,H,W)(*, C , H, W) into g groups\nand rearrange them as (\u2217,Cg,g,H,W)(*, C \\frac g, g, H, W) , while keeping the\noriginal tensor shape.\n\ngroups (int) \u2013 number of groups to divide channels in.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad1d", "path": "generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in both boundaries. If a 2-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} )\n\nOutput: (N,C,Wout)(N, C, W_{out}) where\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad2d", "path": "generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 4-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} )\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) where\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConstantPad3d", "path": "generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d", "type": "torch.nn", "text": "\nPads the input tensor boundaries with a constant value.\n\nFor `N`-dimensional padding, use `torch.nn.functional.pad()`.\n\npadding (int, tuple) \u2013 the size of the padding. If is `int`, uses the same\npadding in all boundaries. If a 6-`tuple`, uses\n(padding_left\\text{padding\\\\_left} , padding_right\\text{padding\\\\_right} ,\npadding_top\\text{padding\\\\_top} , padding_bottom\\text{padding\\\\_bottom} ,\npadding_front\\text{padding\\\\_front} , padding_back\\text{padding\\\\_back} )\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out}) where\n\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\\\_front} +\n\\text{padding\\\\_back}\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\\\_top} +\n\\text{padding\\\\_bottom}\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\\\_left} +\n\\text{padding\\\\_right}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv1d", "path": "generated/torch.nn.conv1d#torch.nn.Conv1d", "type": "torch.nn", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,L)(N, C_{\\text{in}}, L) and output (N,Cout,Lout)(N, C_{\\text{out}},\nL_{\\text{out}}) can be precisely described as:\n\nwhere \u22c6\\star is the valid cross-correlation operator, NN is a batch size, CC\ndenotes a number of channels, LL is a length of signal sequence.\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out}) where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv2d", "path": "generated/torch.nn.conv2d#torch.nn.Conv2d", "type": "torch.nn", "text": "\nApplies a 2D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,H,W)(N, C_{\\text{in}}, H, W) and output (N,Cout,Hout,Wout)(N,\nC_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}}) can be precisely described as:\n\nwhere \u22c6\\star is the valid 2D cross-correlation operator, NN is a batch size,\nCC denotes a number of channels, HH is a height of input planes in pixels, and\nWW is width in pixels.\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out}) where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Conv3d", "path": "generated/torch.nn.conv3d#torch.nn.Conv3d", "type": "torch.nn", "text": "\nApplies a 3D convolution over an input signal composed of several input\nplanes.\n\nIn the simplest case, the output value of the layer with input size\n(N,Cin,D,H,W)(N, C_{in}, D, H, W) and output (N,Cout,Dout,Hout,Wout)(N,\nC_{out}, D_{out}, H_{out}, W_{out}) can be precisely described as:\n\nwhere \u22c6\\star is the valid 3D cross-correlation operator\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `dilation` can either be:\n\nNote\n\nWhen `groups == in_channels` and `out_channels == K * in_channels`, where `K`\nis a positive integer, this operation is also known as a \u201cdepthwise\nconvolution\u201d.\n\nIn other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in}) , a\ndepthwise convolution with a depthwise multiplier `K` can be performed with\nthe arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_\\text{in},\nC_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in}) .\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out}) where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose1d", "path": "generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d", "type": "torch.nn", "text": "\nApplies a 1D transposed convolution operator over an input image composed of\nseveral input planes.\n\nThis module can be seen as the gradient of Conv1d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv1d` and a `ConvTranspose1d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv1d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when using the CUDA backend with CuDNN, this operator\nmay select a nondeterministic algorithm to increase performance. If this is\nundesirable, you can try to make the operation deterministic (potentially at a\nperformance cost) by setting `torch.backends.cudnn.deterministic = True`.\nPlease see the notes on Reproducibility for background.\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out}) where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose2d", "path": "generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d", "type": "torch.nn", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes.\n\nThis module can be seen as the gradient of Conv2d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `output_padding` can either\nbe:\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv2d` and a `ConvTranspose2d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv2d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ConvTranspose3d", "path": "generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d", "type": "torch.nn", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes. The transposed convolution operator multiplies each\ninput value element-wise by a learnable kernel, and sums over the outputs from\nall input feature planes.\n\nThis module can be seen as the gradient of Conv3d with respect to its input.\nIt is also known as a fractionally-strided convolution or a deconvolution\n(although it is not an actual deconvolution operation).\n\nThis module supports TensorFloat32.\n\n`groups` controls the connections between inputs and outputs. `in_channels`\nand `out_channels` must both be divisible by `groups`. For example,\n\nThe parameters `kernel_size`, `stride`, `padding`, `output_padding` can either\nbe:\n\nNote\n\nThe `padding` argument effectively adds `dilation * (kernel_size - 1) -\npadding` amount of zero padding to both sizes of the input. This is set so\nthat when a `Conv3d` and a `ConvTranspose3d` are initialized with same\nparameters, they are inverses of each other in regard to the input and output\nshapes. However, when `stride > 1`, `Conv3d` maps multiple input shapes to the\nsame output shape. `output_padding` is provided to resolve this ambiguity by\neffectively increasing the calculated output shape on one side. Note that\n`output_padding` is only used to find output shape, but does not actually add\nzero-padding to output.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CosineEmbeddingLoss", "path": "generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss", "type": "torch.nn", "text": "\nCreates a criterion that measures the loss given input tensors x1x_1 , x2x_2\nand a `Tensor` label yy with values 1 or -1. This is used for measuring\nwhether two inputs are similar or dissimilar, using the cosine distance, and\nis typically used for learning nonlinear embeddings or semi-supervised\nlearning.\n\nThe loss function for each sample is:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CosineSimilarity", "path": "generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity", "type": "torch.nn", "text": "\nReturns cosine similarity between x1x_1 and x2x_2 , computed along dim.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CrossEntropyLoss", "path": "generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss", "type": "torch.nn", "text": "\nThis criterion combines `LogSoftmax` and `NLLLoss` in one single class.\n\nIt is useful when training a classification problem with `C` classes. If\nprovided, the optional argument `weight` should be a 1D `Tensor` assigning\nweight to each of the classes. This is particularly useful when you have an\nunbalanced training set.\n\nThe `input` is expected to contain raw, unnormalized scores for each class.\n\n`input` has to be a Tensor of size either (minibatch,C)(minibatch, C) or\n(minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K) with K\u22651K \\geq 1\nfor the `K`-dimensional case (described later).\n\nThis criterion expects a class index in the range [0,C\u22121][0, C-1] as the\n`target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`\nis specified, this criterion also accepts this class index (this index may not\nnecessarily be in the class range).\n\nThe loss can be described as:\n\nor in the case of the `weight` argument being specified:\n\nThe losses are averaged across observations for each minibatch. If the\n`weight` argument is specified then this is a weighted average:\n\nCan also be used for higher dimension inputs, such as 2D images, by providing\nan input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)\nwith K\u22651K \\geq 1 , where KK is the number of dimensions, and a target of\nappropriate shape (see below).\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.CTCLoss", "path": "generated/torch.nn.ctcloss#torch.nn.CTCLoss", "type": "torch.nn", "text": "\nThe Connectionist Temporal Classification loss.\n\nCalculates loss between a continuous (unsegmented) time series and a target\nsequence. CTCLoss sums over the probability of possible alignments of input to\ntarget, producing a loss value which is differentiable with respect to each\ninput node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d,\nwhich limits the length of the target sequence such that it must be \u2264\\leq the\ninput length.\n\nExamples:\n\nA. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented\nSequence Data with Recurrent Neural Networks:\nhttps://www.cs.toronto.edu/~graves/icml_2006.pdf\n\nNote\n\nIn order to use CuDNN, the following must be satisfied: `targets` must be in\nconcatenated format, all `input_lengths` must be `T`. blank=0blank=0 ,\n`target_lengths` \u2264256\\leq 256 , the integer arguments must be of dtype\n`torch.int32`.\n\nThe regular implementation uses the (more common in PyTorch) `torch.long`\ndtype.\n\nNote\n\nIn some circumstances when using the CUDA backend with CuDNN, this operator\nmay select a nondeterministic algorithm to increase performance. If this is\nundesirable, you can try to make the operation deterministic (potentially at a\nperformance cost) by setting `torch.backends.cudnn.deterministic = True`.\nPlease see the notes on Reproducibility for background.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.DataParallel", "path": "generated/torch.nn.dataparallel#torch.nn.DataParallel", "type": "torch.nn", "text": "\nImplements data parallelism at the module level.\n\nThis container parallelizes the application of the given `module` by splitting\nthe input across the specified devices by chunking in the batch dimension\n(other objects will be copied once per device). In the forward pass, the\nmodule is replicated on each device, and each replica handles a portion of the\ninput. During the backwards pass, gradients from each replica are summed into\nthe original module.\n\nThe batch size should be larger than the number of GPUs used.\n\nWarning\n\nIt is recommended to use `DistributedDataParallel`, instead of this class, to\ndo multi-GPU training, even if there is only a single node. See: Use\nnn.parallel.DistributedDataParallel instead of multiprocessing or\nnn.DataParallel and Distributed Data Parallel.\n\nArbitrary positional and keyword inputs are allowed to be passed into\nDataParallel but some types are specially handled. tensors will be scattered\non dim specified (default 0). tuple, list and dict types will be shallow\ncopied. The other types will be shared among different threads and can be\ncorrupted if written to in the model\u2019s forward pass.\n\nThe parallelized `module` must have its parameters and buffers on\n`device_ids[0]` before running this `DataParallel` module.\n\nWarning\n\nIn each forward, `module` is replicated on each device, so any updates to the\nrunning module in `forward` will be lost. For example, if `module` has a\ncounter attribute that is incremented in each `forward`, it will always stay\nat the initial value because the update is done on the replicas which are\ndestroyed after `forward`. However, `DataParallel` guarantees that the replica\non `device[0]` will have its parameters and buffers sharing storage with the\nbase parallelized `module`. So in-place updates to the parameters or buffers\non `device[0]` will be recorded. E.g., `BatchNorm2d` and `spectral_norm()`\nrely on this behavior to update the buffers.\n\nWarning\n\nForward and backward hooks defined on `module` and its submodules will be\ninvoked `len(device_ids)` times, each with inputs located on a particular\ndevice. Particularly, the hooks are only guaranteed to be executed in correct\norder with respect to operations on corresponding devices. For example, it is\nnot guaranteed that hooks set via `register_forward_pre_hook()` be executed\nbefore `all` `len(device_ids)` `forward()` calls, but that each such hook be\nexecuted before the corresponding `forward()` call of that device.\n\nWarning\n\nWhen `module` returns a scalar (i.e., 0-dimensional tensor) in `forward()`,\nthis wrapper will return a vector of length equal to number of devices used in\ndata parallelism, containing the result from each device.\n\nNote\n\nThere is a subtlety in using the `pack sequence -> recurrent network -> unpack\nsequence` pattern in a `Module` wrapped in `DataParallel`. See My recurrent\nnetwork doesn\u2019t work with data parallelism section in FAQ for details.\n\n~DataParallel.module (Module) \u2013 the module to be parallelized\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout", "path": "generated/torch.nn.dropout#torch.nn.Dropout", "type": "torch.nn", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution. Each channel will\nbe zeroed out independently on every forward call.\n\nThis has proven to be an effective technique for regularization and preventing\nthe co-adaptation of neurons as described in the paper Improving neural\nnetworks by preventing co-adaptation of feature detectors .\n\nFurthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p} during\ntraining. This means that during evaluation the module simply computes an\nidentity function.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout2d", "path": "generated/torch.nn.dropout2d#torch.nn.Dropout2d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\nUsually the input comes from `nn.Conv2d` modules.\n\nAs described in the paper Efficient Object Localization Using Convolutional\nNetworks , if adjacent pixels within feature maps are strongly correlated (as\nis normally the case in early convolution layers) then i.i.d. dropout will not\nregularize the activations and will otherwise just result in an effective\nlearning rate decrease.\n\nIn this case, `nn.Dropout2d()` will help promote independence between feature\nmaps and should be used instead.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Dropout3d", "path": "generated/torch.nn.dropout3d#torch.nn.Dropout3d", "type": "torch.nn", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ). Each channel will be zeroed out independently\non every forward call with probability `p` using samples from a Bernoulli\ndistribution.\n\nUsually the input comes from `nn.Conv3d` modules.\n\nAs described in the paper Efficient Object Localization Using Convolutional\nNetworks , if adjacent pixels within feature maps are strongly correlated (as\nis normally the case in early convolution layers) then i.i.d. dropout will not\nregularize the activations and will otherwise just result in an effective\nlearning rate decrease.\n\nIn this case, `nn.Dropout3d()` will help promote independence between feature\nmaps and should be used instead.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.ELU", "path": "generated/torch.nn.elu#torch.nn.ELU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Embedding", "path": "generated/torch.nn.embedding#torch.nn.Embedding", "type": "torch.nn", "text": "\nA simple lookup table that stores embeddings of a fixed dictionary and size.\n\nThis module is often used to store word embeddings and retrieve them using\nindices. The input to the module is a list of indices, and the output is the\ncorresponding word embeddings.\n\n~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape\n(num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)\n\nNote\n\nKeep in mind that only a limited number of optimizers support sparse\ngradients: currently it\u2019s `optim.SGD` (`CUDA` and `CPU`), `optim.SparseAdam`\n(`CUDA` and `CPU`) and `optim.Adagrad` (`CPU`)\n\nNote\n\nWith `padding_idx` set, the embedding vector at `padding_idx` is initialized\nto all zeros. However, note that this vector can be modified afterwards, e.g.,\nusing a customized initialization method, and thus changing the vector used to\npad the output. The gradient for this vector from `Embedding` is always zero.\n\nNote\n\nWhen `max_norm` is not `None`, `Embedding`\u2019s forward method will modify the\n`weight` tensor in-place. Since tensors needed for gradient computations\ncannot be modified in-place, performing a differentiable operation on\n`Embedding.weight` before calling `Embedding`\u2019s forward method requires\ncloning `Embedding.weight` when `max_norm` is not `None`. For example:\n\nExamples:\n\nCreates Embedding instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Embedding.from_pretrained()", "path": "generated/torch.nn.embedding#torch.nn.Embedding.from_pretrained", "type": "torch.nn", "text": "\nCreates Embedding instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag", "type": "torch.nn", "text": "\nComputes sums or means of \u2018bags\u2019 of embeddings, without instantiating the\nintermediate embeddings.\n\nFor bags of constant length and no `per_sample_weights` and 2D inputs, this\nclass\n\nHowever, `EmbeddingBag` is much more time and memory efficient than using a\nchain of these operations.\n\nEmbeddingBag also supports per-sample weights as an argument to the forward\npass. This scales the output of the Embedding before performing a weighted\nreduction as specified by `mode`. If `per_sample_weights`` is passed, the only\nsupported `mode` is `\"sum\"`, which computes a weighted sum according to\n`per_sample_weights`.\n\n~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape\n`(num_embeddings, embedding_dim)` initialized from N(0,1)\\mathcal{N}(0, 1) .\n\n`per_index_weights` (Tensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nto indicate all weights should be taken to be `1`. If specified,\n`per_sample_weights` must have exactly the same shape as input and is treated\nas having the same `offsets`, if those are not `None`. Only supported for\n`mode='sum'`.\n\nOutput shape: `(B, embedding_dim)`\n\nExamples:\n\nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.EmbeddingBag.from_pretrained()", "path": "generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag.from_pretrained", "type": "torch.nn", "text": "\nCreates EmbeddingBag instance from given 2-dimensional FloatTensor.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten", "path": "generated/torch.nn.flatten#torch.nn.Flatten", "type": "torch.nn", "text": "\nFlattens a contiguous range of dims into a tensor. For use with `Sequential`.\n\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.add_module()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.add_module", "type": "torch.nn", "text": "\nAdds a child module to the current module.\n\nThe module can be accessed as an attribute using the given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.apply()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.apply", "type": "torch.nn", "text": "\nApplies `fn` recursively to every submodule (as returned by `.children()`) as\nwell as self. Typical use includes initializing the parameters of a model (see\nalso torch.nn.init).\n\nfn (`Module` -> None) \u2013 function to be applied to each submodule\n\nself\n\nModule\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.bfloat16()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.bfloat16", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `bfloat16` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers.\n\nrecurse (bool) \u2013 if True, then yields buffers of this module and all\nsubmodules. Otherwise, yields only buffers that are direct members of this\nmodule.\n\ntorch.Tensor \u2013 module buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules.\n\nModule \u2013 a child module\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the CPU.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.cuda()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.cuda", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the GPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on GPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.double()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.double", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `double` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.eval()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.eval", "type": "torch.nn", "text": "\nSets the module in evaluation mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nThis is equivalent with `self.train(False)`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.float()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.float", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to float datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.half()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.half", "type": "torch.nn", "text": "\nCasts all floating point parameters and buffers to `half` datatype.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.load_state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.load_state_dict", "type": "torch.nn", "text": "\nCopies parameters and buffers from `state_dict` into this module and its\ndescendants. If `strict` is `True`, then the keys of `state_dict` must exactly\nmatch the keys returned by this module\u2019s `state_dict()` function.\n\n`NamedTuple` with `missing_keys` and `unexpected_keys` fields\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network.\n\nModule \u2013 a module in the network\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_buffers()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_buffers", "type": "torch.nn", "text": "\nReturns an iterator over module buffers, yielding both the name of the buffer\nas well as the buffer itself.\n\n(string, torch.Tensor) \u2013 Tuple containing the name and buffer\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_children()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_children", "type": "torch.nn", "text": "\nReturns an iterator over immediate children modules, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple containing a name and child module\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_modules()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_modules", "type": "torch.nn", "text": "\nReturns an iterator over all modules in the network, yielding both the name of\nthe module as well as the module itself.\n\n(string, Module) \u2013 Tuple of name and module\n\nNote\n\nDuplicate modules are returned only once. In the following example, `l` will\nbe returned only once.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.named_parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.named_parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters, yielding both the name of the\nparameter as well as the parameter itself.\n\n(string, Parameter) \u2013 Tuple containing the name and parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.parameters()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.parameters", "type": "torch.nn", "text": "\nReturns an iterator over module parameters.\n\nThis is typically passed to an optimizer.\n\nrecurse (bool) \u2013 if True, then yields parameters of this module and all\nsubmodules. Otherwise, yields only parameters that are direct members of this\nmodule.\n\nParameter \u2013 module parameter\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThis function is deprecated in favor of\n`nn.Module.register_full_backward_hook()` and the behavior of this function\nwill change in future versions.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_buffer()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_buffer", "type": "torch.nn", "text": "\nAdds a buffer to the module.\n\nThis is typically used to register a buffer that should not to be considered a\nmodel parameter. For example, BatchNorm\u2019s `running_mean` is not a parameter,\nbut is part of the module\u2019s state. Buffers, by default, are persistent and\nwill be saved alongside parameters. This behavior can be changed by setting\n`persistent` to `False`. The only difference between a persistent buffer and a\nnon-persistent buffer is that the latter will not be a part of this module\u2019s\n`state_dict`.\n\nBuffers can be accessed as attributes using given names.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_hook", "type": "torch.nn", "text": "\nRegisters a forward hook on the module.\n\nThe hook will be called every time after `forward()` has computed an output.\nIt should have the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the output. It can modify the input inplace but it will not have effect\non forward since this is called after `forward()` is called.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_forward_pre_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_forward_pre_hook", "type": "torch.nn", "text": "\nRegisters a forward pre-hook on the module.\n\nThe hook will be called every time before `forward()` is invoked. It should\nhave the following signature:\n\nThe input contains only the positional arguments given to the module. Keyword\narguments won\u2019t be passed to the hooks and only to the `forward`. The hook can\nmodify the input. User can either return a tuple or a single modified value in\nthe hook. We will wrap the value into a tuple if a single value is\nreturned(unless that value is already a tuple).\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_full_backward_hook()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_full_backward_hook", "type": "torch.nn", "text": "\nRegisters a backward hook on the module.\n\nThe hook will be called every time the gradients with respect to module inputs\nare computed. The hook should have the following signature:\n\nThe `grad_input` and `grad_output` are tuples that contain the gradients with\nrespect to the inputs and outputs respectively. The hook should not modify its\narguments, but it can optionally return a new gradient with respect to the\ninput that will be used in place of `grad_input` in subsequent computations.\n`grad_input` will only correspond to the inputs given as positional arguments\nand all kwarg arguments are ignored. Entries in `grad_input` and `grad_output`\nwill be `None` for all non-Tensor arguments.\n\nWarning\n\nModifying inputs or outputs inplace is not allowed when using backward hooks\nand will raise an error.\n\na handle that can be used to remove the added hook by calling\n`handle.remove()`\n\n`torch.utils.hooks.RemovableHandle`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.register_parameter()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.register_parameter", "type": "torch.nn", "text": "\nAdds a parameter to the module.\n\nThe parameter can be accessed as an attribute using given name.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.requires_grad_()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.requires_grad_", "type": "torch.nn", "text": "\nChange if autograd should record operations on parameters in this module.\n\nThis method sets the parameters\u2019 `requires_grad` attributes in-place.\n\nThis method is helpful for freezing part of the module for finetuning or\ntraining parts of a model individually (e.g., GAN training).\n\nrequires_grad (bool) \u2013 whether autograd should record operations on parameters\nin this module. Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.state_dict()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.state_dict", "type": "torch.nn", "text": "\nReturns a dictionary containing a whole state of the module.\n\nBoth parameters and persistent buffers (e.g. running averages) are included.\nKeys are corresponding parameter and buffer names.\n\na dictionary containing a whole state of the module\n\ndict\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.to()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.to", "type": "torch.nn", "text": "\nMoves and/or casts the parameters and buffers.\n\nThis can be called as\n\nIts signature is similar to `torch.Tensor.to()`, but only accepts floating\npoint or complex `dtype`s. In addition, this method will only cast the\nfloating point or complex parameters and buffers to :attr:`dtype` (if given).\nThe integral parameters and buffers will be moved `device`, if that is given,\nbut with dtypes unchanged. When `non_blocking` is set, it tries to\nconvert/move asynchronously with respect to the host if possible, e.g., moving\nCPU Tensors with pinned memory to CUDA devices.\n\nSee below for examples.\n\nNote\n\nThis method modifies the module in-place.\n\nself\n\nModule\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.train()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.train", "type": "torch.nn", "text": "\nSets the module in training mode.\n\nThis has any effect only on certain modules. See documentations of particular\nmodules for details of their behaviors in training/evaluation mode, if they\nare affected, e.g. `Dropout`, `BatchNorm`, etc.\n\nmode (bool) \u2013 whether to set training mode (`True`) or evaluation mode\n(`False`). Default: `True`.\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.type()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.type", "type": "torch.nn", "text": "\nCasts all parameters and buffers to `dst_type`.\n\ndst_type (type or string) \u2013 the desired type\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.xpu()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.xpu", "type": "torch.nn", "text": "\nMoves all model parameters and buffers to the XPU.\n\nThis also makes associated parameters and buffers different objects. So it\nshould be called before constructing optimizer if the module will live on XPU\nwhile being optimized.\n\ndevice (int, optional) \u2013 if specified, all parameters will be copied to that\ndevice\n\nself\n\nModule\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Flatten.zero_grad()", "path": "generated/torch.nn.flatten#torch.nn.Flatten.zero_grad", "type": "torch.nn", "text": "\nSets gradients of all model parameters to zero. See similar function under\n`torch.optim.Optimizer` for more context.\n\nset_to_none (bool) \u2013 instead of setting to zero, set the grads to None. See\n`torch.optim.Optimizer.zero_grad()` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Fold", "path": "generated/torch.nn.fold#torch.nn.Fold", "type": "torch.nn", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\nConsider a batched `input` tensor containing sliding local blocks, e.g.,\npatches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times\n\\prod(\\text{kernel\\\\_size}), L) , where NN is batch dimension,\nC\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\\\_size}) is the number of values\nwithin a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\\\_size}) spatial\nlocations each containing a CC -channeled vector), and LL is the total number\nof blocks. (This is exactly the same specification as the output shape of\n`Unfold`.) This operation combines these local blocks into the large `output`\ntensor of shape (N,C,output_size[0],output_size[1],\u2026)(N, C,\n\\text{output\\\\_size}[0], \\text{output\\\\_size}[1], \\dots) by summing the\noverlapping values. Similar to `Unfold`, the arguments must satisfy\n\nwhere dd is over all spatial dimensions.\n\nThe `padding`, `stride` and `dilation` arguments specify how the sliding\nblocks are retrieved.\n\nNote\n\n`Fold` calculates each combined value in the resulting large tensor by summing\nall values from all containing blocks. `Unfold` extracts the values in the\nlocal blocks by copying from the large tensor. So, if the blocks overlap, they\nare not inverses of each other.\n\nIn general, folding and unfolding operations are related as follows. Consider\n`Fold` and `Unfold` instances created with the same parameters:\n\nThen for any (supported) `input` tensor the following equality holds:\n\nwhere `divisor` is a tensor that depends only on the shape and dtype of the\n`input`:\n\nWhen the `divisor` tensor contains no zero elements, then `fold` and `unfold`\noperations are inverses of each other (up to constant divisor).\n\nWarning\n\nCurrently, only 4-D output tensors (batched image-like tensors) are supported.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.FractionalMaxPool2d", "path": "generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d", "type": "torch.nn", "text": "\nApplies a 2D fractional max pooling over an input signal composed of several\ninput planes.\n\nFractional MaxPooling is described in detail in the paper Fractional\nMaxPooling by Ben Graham\n\nThe max-pooling operation is applied in kH\u00d7kWkH \\times kW regions by a\nstochastic step size determined by the target output size. The number of\noutput features is equal to the number of input planes.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional", "path": "nn.functional", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 2D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 3D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 1D transposed convolution operator over an input signal composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\nExtracts sliding local blocks from a batched input tensor.\n\nWarning\n\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\n\nWarning\n\nMore than one element of the unfolded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensor, please clone it first.\n\nSee `torch.nn.Unfold` for details\n\nCombines an array of sliding local blocks into a large containing tensor.\n\nWarning\n\nCurrently, only 3-D output tensors (unfolded batched image-like tensors) are\nsupported.\n\nSee `torch.nn.Fold` for details\n\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nSee `AvgPool1d` for details and output shape.\n\nExamples:\n\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\nSee `AvgPool2d` for details and output shape.\n\nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions\nby step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output\nfeatures is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input\nplanes}}{sT}\\rfloor .\n\nSee `AvgPool3d` for details and output shape.\n\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool1d` for details.\n\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool2d` for details.\n\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool3d` for details.\n\nComputes a partial inverse of `MaxPool1d`.\n\nSee `MaxUnpool1d` for details.\n\nComputes a partial inverse of `MaxPool2d`.\n\nSee `MaxUnpool2d` for details.\n\nComputes a partial inverse of `MaxPool3d`.\n\nSee `MaxUnpool3d` for details.\n\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool1d` for details.\n\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool2d` for details.\n\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool1d` for details and output shape.\n\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool2d` for details and output shape.\n\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool3d` for details and output shape.\n\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool1d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer)\n\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool2d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or double-integer tuple)\n\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool3d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or triple-integer tuple)\n\nThresholds each element of the input Tensor.\n\nSee `Threshold` for more details.\n\nIn-place version of `threshold()`.\n\nApplies the rectified linear unit function element-wise. See `ReLU` for more\ndetails.\n\nIn-place version of `relu()`.\n\nApplies the HardTanh function element-wise. See `Hardtanh` for more details.\n\nIn-place version of `hardtanh()`.\n\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\nSee `Hardswish` for more details.\n\nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) =\n\\min(\\max(0,x), 6) .\n\nSee `ReLU6` for more details.\n\nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) =\n\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .\n\nSee `ELU` for more details.\n\nIn-place version of `elu()`.\n\nApplies element-wise,\nSELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale *\n(\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with\n\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\nand\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946\n.\n\nSee `SELU` for more details.\n\nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x)\n= \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .\n\nSee `CELU` for more details.\n\nApplies element-wise,\nLeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0,\nx) + \\text{negative\\\\_slope} * \\min(0, x)\n\nSee `LeakyReLU` for more details.\n\nIn-place version of `leaky_relu()`.\n\nApplies element-wise the function\nPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight}\n* \\min(0,x) where weight is a learnable parameter.\n\nSee `PReLU` for more details.\n\nRandomized leaky ReLU.\n\nSee `RReLU` for more details.\n\nIn-place version of `rrelu()`.\n\nThe gated linear unit. Computes:\n\nwhere `input` is split in half along `dim` to form `a` and `b`, \u03c3\\sigma is the\nsigmoid function and \u2297\\otimes is the element-wise product between matrices.\n\nSee Language Modeling with Gated Convolutional Networks.\n\nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nSee Gaussian Error Linear Units (GELUs).\n\nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) =\n\\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)\n\nSee `LogSigmoid` for more details.\n\nApplies the hard shrinkage function element-wise\n\nSee `Hardshrink` for more details.\n\nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x -\n\\text{Tanh}(x)\n\nSee `Tanhshrink` for more details.\n\nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) =\n\\frac{x}{1 + |x|}\n\nSee `Softsign` for more details.\n\nApplies element-wise, the function\nSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1\n+ \\exp(\\beta * x)) .\n\nFor numerical stability the implementation reverts to the linear function when\ninput\u00d7\u03b2>thresholdinput \\times \\beta > threshold .\n\nSee `Softplus` for more details.\n\nApplies a softmin function.\n\nNote that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See\nsoftmax definition for mathematical formula.\n\nSee `Softmin` for more details.\n\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j\n\\exp(x_j)}\n\nIt is applied to all slices along dim, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\nSee `Softmax` for more details.\n\nNote\n\nThis function doesn\u2019t work directly with NLLLoss, which expects the Log to be\ncomputed between the Softmax and itself. Use log_softmax instead (it\u2019s faster\nand has better numerical properties).\n\nApplies the soft shrinkage function elementwise\n\nSee `Softshrink` for more details.\n\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally\ndiscretizes.\n\nSampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\nIf `hard=True`, the returned samples will be one-hot, otherwise they will be\nprobability distributions that sum to 1 across `dim`.\n\nNote\n\nThis function is here for legacy reasons, may be removed from nn.Functional in\nthe future.\n\nNote\n\nThe main trick for `hard` is to do `y_hard - y_soft.detach() + y_soft`\n\nIt achieves two things: - makes the output value exactly one-hot (since we add\nthen subtract y_soft value) - makes the gradient equal to y_soft gradient\n(since we strip all other gradients)\n\nApplies a softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two operations\nseparately is slower, and numerically unstable. This function uses an\nalternative formulation to compute the output and gradient correctly.\n\nSee `LogSoftmax` for more details.\n\nApplies element-wise,\nTanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) =\n\\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n\nSee `Tanh` for more details.\n\nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) =\n\\frac{1}{1 + \\exp(-x)}\n\nSee `Sigmoid` for more details.\n\nApplies the element-wise function\n\ninplace \u2013 If set to `True`, will do this operation in-place. Default: `False`\n\nSee `Hardsigmoid` for more details.\n\nApplies the silu function, element-wise.\n\nNote\n\nSee Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)\nwas originally coined, and see Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning and Swish: a Self-\nGated Activation Function where the SiLU was experimented with later.\n\nSee `SiLU` for more details.\n\nApplies Batch Normalization for each channel across a batch of data.\n\nSee `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d` for details.\n\nApplies Instance Normalization for each channel in each data sample in a\nbatch.\n\nSee `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d` for details.\n\nApplies Layer Normalization for last certain number of dimensions.\n\nSee `LayerNorm` for details.\n\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nSee `LocalResponseNorm` for details.\n\nPerforms LpL_p normalization of inputs over specified dimension.\n\nFor a tensor `input` of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ...,\nn_k) , each ndimn_{dim} -element vector vv along dimension `dim` is\ntransformed as\n\nWith the default arguments it uses the Euclidean norm over vectors along\ndimension 11 for normalization.\n\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b .\n\nThis operator supports TensorFloat32.\n\nShape:\n\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nShape:\n\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\nSee `Dropout` for details.\n\nApplies alpha dropout to the input.\n\nSee `AlphaDropout` for details.\n\nRandomly masks out entire channels (a channel is a feature map, e.g. the jj\n-th channel of the ii -th sample in the batch input is a tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting\nactivations to zero, as in regular Dropout, the activations are set to the\nnegative saturation value of the SELU activation function.\n\nEach element will be masked independently on every forward call with\nprobability `p` using samples from a Bernoulli distribution. The elements to\nbe masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit variance.\n\nSee `FeatureAlphaDropout` for details.\n\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout2d` for details.\n\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout3d` for details.\n\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\nThis module is often used to retrieve word embeddings using indices. The input\nto the module is a list of indices, and the embedding matrix, and the output\nis the corresponding word embeddings.\n\nSee `torch.nn.Embedding` for more details.\n\nwhere V = maximum index + 1 and embedding_dim = the embedding size\n\nExamples:\n\nComputes sums, means or maxes of `bags` of embeddings, without instantiating\nthe intermediate embeddings.\n\nSee `torch.nn.EmbeddingBag` for more details.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nShape:\n\n`input` (LongTensor) and `offsets` (LongTensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nExamples:\n\nTakes LongTensor with index values of shape `(*)` and returns a tensor of\nshape `(*, num_classes)` that have zeros everywhere except where the index of\nlast dimension matches the corresponding value of the input tensor, in which\ncase it will be 1.\n\nSee also One-hot on Wikipedia .\n\nLongTensor that has one more dimension with 1 values at the index of last\ndimension indicated by the input, and 0 everywhere else.\n\nSee `torch.nn.PairwiseDistance` for details\n\nReturns cosine similarity between x1 and x2, computed along dim.\n\nExample:\n\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape N\u00d7MN \\times M then the output will have shape\n12N(N\u22121)\\frac{1}{2} N (N - 1) .\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n\u2018minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0 it is\nequivalent to `scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M`. When p=\u221ep\n= \\infty , the closest scipy function is `scipy.spatial.distance.pdist(xn,\nlambda x, y: np.abs(x - y).max())`.\n\nFunction that measures the Binary Cross Entropy between the target and the\noutput.\n\nSee `BCELoss` for details.\n\nExamples:\n\nFunction that measures Binary Cross Entropy between target and output logits.\n\nSee `BCEWithLogitsLoss` for details.\n\nExamples:\n\nPoisson negative log likelihood loss.\n\nSee `PoissonNLLLoss` for details.\n\nSee `CosineEmbeddingLoss` for details.\n\nThis criterion combines `log_softmax` and `nll_loss` in a single function.\n\nSee `CrossEntropyLoss` for details.\n\nExamples:\n\nThe Connectionist Temporal Classification loss.\n\nSee `CTCLoss` for details.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nExample:\n\nSee `HingeEmbeddingLoss` for details.\n\nThe Kullback-Leibler divergence Loss\n\nSee `KLDivLoss` for details.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n:attr:`reduction` = `'mean'` doesn\u2019t return the true kl divergence value,\nplease use :attr:`reduction` = `'batchmean'` which aligns with KL math\ndefinition. In the next major release, `'mean'` will be changed to be the same\nas \u2018batchmean\u2019.\n\nFunction that takes the mean element-wise absolute value difference.\n\nSee `L1Loss` for details.\n\nMeasures the element-wise mean squared error.\n\nSee `MSELoss` for details.\n\nSee `MarginRankingLoss` for details.\n\nSee `MultiLabelMarginLoss` for details.\n\nSee `MultiLabelSoftMarginLoss` for details.\n\nreduce=None, reduction=\u2019mean\u2019) -> Tensor\n\nSee `MultiMarginLoss` for details.\n\nThe negative log likelihood loss.\n\nSee `NLLLoss` for details.\n\nExample:\n\nFunction that uses a squared term if the absolute element-wise error falls\nbelow beta and an L1 term otherwise.\n\nSee `SmoothL1Loss` for details.\n\nSee `SoftMarginLoss` for details.\n\nSee `TripletMarginLoss` for details\n\nSee `TripletMarginWithDistanceLoss` for details.\n\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nthe `upscale_factor`.\n\nSee `PixelShuffle` for details.\n\nExamples:\n\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the `downscale_factor`.\n\nSee `PixelUnshuffle` for details.\n\nExamples:\n\nPads tensor.\n\nThe padding size by which to pad some dimensions of `input` are described\nstarting from the last dimension and moving forward.\n\u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of\n`input` will be padded. For example, to pad only the last dimension of the\ninput tensor, then `pad` has the form\n(padding_left,padding_right)(\\text{padding\\\\_left}, \\text{padding\\\\_right}) ;\nto pad the last 2 dimensions of the input tensor, then use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom)\\text{padding\\\\_top}, \\text{padding\\\\_bottom}) ; to\npad the last 3 dimensions, use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom\\text{padding\\\\_top}, \\text{padding\\\\_bottom}\npadding_front,padding_back)\\text{padding\\\\_front}, \\text{padding\\\\_back}) .\n\nSee `torch.nn.ConstantPad2d`, `torch.nn.ReflectionPad2d`, and\n`torch.nn.ReplicationPad2d` for concrete examples on how each of the padding\nmodes works. Constant padding is implemented for arbitrary dimensions.\nReplicate padding is implemented for padding the last 3 dimensions of 5D input\ntensor, or the last 2 dimensions of 4D input tensor, or the last dimension of\n3D input tensor. Reflect padding is only implemented for padding the last 2\ndimensions of 4D input tensor, or the last dimension of 3D input tensor.\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nExamples:\n\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\nThe algorithm used for interpolation is determined by `mode`.\n\nCurrently temporal, spatial and volumetric sampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for resizing are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nWarning\n\nWhen scale_factor is specified, if recompute_scale_factor=True, scale_factor\nis used to compute the output_size which will then be used to infer new scales\nfor the interpolation. The default behavior for recompute_scale_factor changed\nto False in 1.6.0, and scale_factor is used in the interpolation calculation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nUpsamples the input to either the given `size` or the given `scale_factor`\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(...)`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nThe algorithm used for upsampling is determined by `mode`.\n\nCurrently temporal, spatial and volumetric upsampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for upsampling are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='nearest')`.\n\nCurrently spatial and volumetric upsampling are supported (i.e. expected\ninputs are 4 or 5 dimensional).\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nUpsamples the input, using bilinear upsampling.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='bilinear',\nalign_corners=True)`.\n\nExpected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\nvolumetric (5 dimensional) inputs.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nGiven an `input` and a flow-field `grid`, computes the `output` using `input`\nvalues and pixel locations from `grid`.\n\nCurrently, only spatial (4-D) and volumetric (5-D) `input` are supported.\n\nIn the spatial (4-D) case, for `input` with shape (N,C,Hin,Win)(N, C,\nH_\\text{in}, W_\\text{in}) and `grid` with shape (N,Hout,Wout,2)(N,\nH_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N,\nC, H_\\text{out}, W_\\text{out}) .\n\nFor each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h,\nw]` specifies `input` pixel locations `x` and `y`, which are used to\ninterpolate the output value `output[n, :, h, w]`. In the case of 5D inputs,\n`grid[n, d, h, w]` specifies the `x`, `y`, `z` pixel locations for\ninterpolating `output[n, :, d, h, w]`. `mode` argument specifies `nearest` or\n`bilinear` interpolation method to sample the input pixels.\n\n`grid` specifies the sampling pixel locations normalized by the `input`\nspatial dimensions. Therefore, it should have most values in the range of\n`[-1, 1]`. For example, values `x = -1, y = -1` is the left-top pixel of\n`input`, and values `x = 1, y = 1` is the right-bottom pixel of `input`.\n\nIf `grid` has values outside the range of `[-1, 1]`, the corresponding outputs\nare handled as defined by `padding_mode`. Options are\n\nNote\n\nThis function is often used in conjunction with `affine_grid()` to build\nSpatial Transformer Networks .\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nNote\n\nNaN values in `grid` would be interpreted as `-1`.\n\noutput Tensor\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nNote\n\n`mode='bicubic'` is implemented using the cubic convolution algorithm with\n\u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha might be different from packages to\npackages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This\nalgorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example,\nit may produce negative values or values greater than 255 when interpolating\ninput in [0, 255]. Clamp the results with :func: `torch.clamp` to ensure they\nare within the valid range.\n\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine\nmatrices `theta`.\n\nNote\n\nThis function is often used in conjunction with `grid_sample()` to build\nSpatial Transformer Networks .\n\noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nWarning\n\nWhen `align_corners = True`, 2D affine transforms on 1D data and 3D affine\ntransforms on 2D data (that is, when one of the spatial dimensions has unit\nsize) are ill-defined, and not an intended use case. This is not a problem\nwhen `align_corners = False`. Up to version 1.2.0, all grid points along a\nunit dimension were considered arbitrarily to be at `-1`. From version 1.3.0,\nunder `align_corners = True` all grid points along a unit dimension are\nconsidered to be at ``0` (the center of the input image).\n\nEvaluates module(input) in parallel across the GPUs given in device_ids.\n\nThis is the functional version of the DataParallel module.\n\na Tensor containing the result of module(input) located on output_device\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool1d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool2d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or double-integer tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_avg_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive average pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveAvgPool3d` for details and output shape.\n\noutput_size \u2013 the target output size (single integer or triple-integer tuple)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool1d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool1d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool2d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool2d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.adaptive_max_pool3d()", "path": "nn.functional#torch.nn.functional.adaptive_max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D adaptive max pooling over an input signal composed of several\ninput planes.\n\nSee `AdaptiveMaxPool3d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.affine_grid()", "path": "nn.functional#torch.nn.functional.affine_grid", "type": "torch.nn.functional", "text": "\nGenerates a 2D or 3D flow field (sampling grid), given a batch of affine\nmatrices `theta`.\n\nNote\n\nThis function is often used in conjunction with `grid_sample()` to build\nSpatial Transformer Networks .\n\noutput Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2 )\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nWarning\n\nWhen `align_corners = True`, 2D affine transforms on 1D data and 3D affine\ntransforms on 2D data (that is, when one of the spatial dimensions has unit\nsize) are ill-defined, and not an intended use case. This is not a problem\nwhen `align_corners = False`. Up to version 1.2.0, all grid points along a\nunit dimension were considered arbitrarily to be at `-1`. From version 1.3.0,\nunder `align_corners = True` all grid points along a unit dimension are\nconsidered to be at ``0` (the center of the input image).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.alpha_dropout()", "path": "nn.functional#torch.nn.functional.alpha_dropout", "type": "torch.nn.functional", "text": "\nApplies alpha dropout to the input.\n\nSee `AlphaDropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool1d()", "path": "nn.functional#torch.nn.functional.avg_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D average pooling over an input signal composed of several input\nplanes.\n\nSee `AvgPool1d` for details and output shape.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool2d()", "path": "nn.functional#torch.nn.functional.avg_pool2d", "type": "torch.nn.functional", "text": "\nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size\nsH\u00d7sWsH \\times sW steps. The number of output features is equal to the number\nof input planes.\n\nSee `AvgPool2d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.avg_pool3d()", "path": "nn.functional#torch.nn.functional.avg_pool3d", "type": "torch.nn.functional", "text": "\nApplies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kW regions\nby step size sT\u00d7sH\u00d7sWsT \\times sH \\times sW steps. The number of output\nfeatures is equal to \u230ainput planessT\u230b\\lfloor\\frac{\\text{input\nplanes}}{sT}\\rfloor .\n\nSee `AvgPool3d` for details and output shape.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.batch_norm()", "path": "nn.functional#torch.nn.functional.batch_norm", "type": "torch.nn.functional", "text": "\nApplies Batch Normalization for each channel across a batch of data.\n\nSee `BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.bilinear()", "path": "nn.functional#torch.nn.functional.bilinear", "type": "torch.nn.functional", "text": "\nApplies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A\nx_2 + b\n\nShape:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy", "type": "torch.nn.functional", "text": "\nFunction that measures the Binary Cross Entropy between the target and the\noutput.\n\nSee `BCELoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.binary_cross_entropy_with_logits()", "path": "nn.functional#torch.nn.functional.binary_cross_entropy_with_logits", "type": "torch.nn.functional", "text": "\nFunction that measures Binary Cross Entropy between target and output logits.\n\nSee `BCEWithLogitsLoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.celu()", "path": "nn.functional#torch.nn.functional.celu", "type": "torch.nn.functional", "text": "\nApplies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x)\n= \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)) .\n\nSee `CELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv1d()", "path": "nn.functional#torch.nn.functional.conv1d", "type": "torch.nn.functional", "text": "\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv2d()", "path": "nn.functional#torch.nn.functional.conv2d", "type": "torch.nn.functional", "text": "\nApplies a 2D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv3d()", "path": "nn.functional#torch.nn.functional.conv3d", "type": "torch.nn.functional", "text": "\nApplies a 3D convolution over an input image composed of several input planes.\n\nThis operator supports TensorFloat32.\n\nSee `Conv3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose1d()", "path": "nn.functional#torch.nn.functional.conv_transpose1d", "type": "torch.nn.functional", "text": "\nApplies a 1D transposed convolution operator over an input signal composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose1d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose2d()", "path": "nn.functional#torch.nn.functional.conv_transpose2d", "type": "torch.nn.functional", "text": "\nApplies a 2D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d.\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose2d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.conv_transpose3d()", "path": "nn.functional#torch.nn.functional.conv_transpose3d", "type": "torch.nn.functional", "text": "\nApplies a 3D transposed convolution operator over an input image composed of\nseveral input planes, sometimes also called \u201cdeconvolution\u201d\n\nThis operator supports TensorFloat32.\n\nSee `ConvTranspose3d` for details and output shape.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_embedding_loss()", "path": "nn.functional#torch.nn.functional.cosine_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `CosineEmbeddingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cosine_similarity()", "path": "nn.functional#torch.nn.functional.cosine_similarity", "type": "torch.nn.functional", "text": "\nReturns cosine similarity between x1 and x2, computed along dim.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.cross_entropy()", "path": "nn.functional#torch.nn.functional.cross_entropy", "type": "torch.nn.functional", "text": "\nThis criterion combines `log_softmax` and `nll_loss` in a single function.\n\nSee `CrossEntropyLoss` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.ctc_loss()", "path": "nn.functional#torch.nn.functional.ctc_loss", "type": "torch.nn.functional", "text": "\nThe Connectionist Temporal Classification loss.\n\nSee `CTCLoss` for details.\n\nNote\n\nIn some circumstances when given tensors on a CUDA device and using CuDNN,\nthis operator may select a nondeterministic algorithm to increase performance.\nIf this is undesirable, you can try to make the operation deterministic\n(potentially at a performance cost) by setting\n`torch.backends.cudnn.deterministic = True`. See Reproducibility for more\ninformation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout()", "path": "nn.functional#torch.nn.functional.dropout", "type": "torch.nn.functional", "text": "\nDuring training, randomly zeroes some of the elements of the input tensor with\nprobability `p` using samples from a Bernoulli distribution.\n\nSee `Dropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout2d()", "path": "nn.functional#torch.nn.functional.dropout2d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 2D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 2D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.dropout3d()", "path": "nn.functional#torch.nn.functional.dropout3d", "type": "torch.nn.functional", "text": "\nRandomly zero out entire channels (a channel is a 3D feature map, e.g., the jj\n-th channel of the ii -th sample in the batched input is a 3D tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Each channel will be\nzeroed out independently on every forward call with probability `p` using\nsamples from a Bernoulli distribution.\n\nSee `Dropout3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.elu()", "path": "nn.functional#torch.nn.functional.elu", "type": "torch.nn.functional", "text": "\nApplies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) =\n\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)) .\n\nSee `ELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.elu_()", "path": "nn.functional#torch.nn.functional.elu_", "type": "torch.nn.functional", "text": "\nIn-place version of `elu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding()", "path": "nn.functional#torch.nn.functional.embedding", "type": "torch.nn.functional", "text": "\nA simple lookup table that looks up embeddings in a fixed dictionary and size.\n\nThis module is often used to retrieve word embeddings using indices. The input\nto the module is a list of indices, and the embedding matrix, and the output\nis the corresponding word embeddings.\n\nSee `torch.nn.Embedding` for more details.\n\nwhere V = maximum index + 1 and embedding_dim = the embedding size\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.embedding_bag()", "path": "nn.functional#torch.nn.functional.embedding_bag", "type": "torch.nn.functional", "text": "\nComputes sums, means or maxes of `bags` of embeddings, without instantiating\nthe intermediate embeddings.\n\nSee `torch.nn.EmbeddingBag` for more details.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nShape:\n\n`input` (LongTensor) and `offsets` (LongTensor, optional)\n\nIf `input` is 2D of shape `(B, N)`,\n\nit will be treated as `B` bags (sequences) each of fixed length `N`, and this\nwill return `B` values aggregated in a way depending on the `mode`. `offsets`\nis ignored and required to be `None` in this case.\n\nIf `input` is 1D of shape `(N)`,\n\nit will be treated as a concatenation of multiple bags (sequences). `offsets`\nis required to be a 1D tensor containing the starting index positions of each\nbag in `input`. Therefore, for `offsets` of shape `(B)`, `input` will be\nviewed as having `B` bags. Empty bags (i.e., having 0-length) will have\nreturned vectors filled by zeros.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.feature_alpha_dropout()", "path": "nn.functional#torch.nn.functional.feature_alpha_dropout", "type": "torch.nn.functional", "text": "\nRandomly masks out entire channels (a channel is a feature map, e.g. the jj\n-th channel of the ii -th sample in the batch input is a tensor\ninput[i,j]\\text{input}[i, j] ) of the input tensor). Instead of setting\nactivations to zero, as in regular Dropout, the activations are set to the\nnegative saturation value of the SELU activation function.\n\nEach element will be masked independently on every forward call with\nprobability `p` using samples from a Bernoulli distribution. The elements to\nbe masked are randomized on every forward call, and scaled and shifted to\nmaintain zero mean and unit variance.\n\nSee `FeatureAlphaDropout` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.fold()", "path": "nn.functional#torch.nn.functional.fold", "type": "torch.nn.functional", "text": "\nCombines an array of sliding local blocks into a large containing tensor.\n\nWarning\n\nCurrently, only 3-D output tensors (unfolded batched image-like tensors) are\nsupported.\n\nSee `torch.nn.Fold` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.gelu()", "path": "nn.functional#torch.nn.functional.gelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nSee Gaussian Error Linear Units (GELUs).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.glu()", "path": "nn.functional#torch.nn.functional.glu", "type": "torch.nn.functional", "text": "\nThe gated linear unit. Computes:\n\nwhere `input` is split in half along `dim` to form `a` and `b`, \u03c3\\sigma is the\nsigmoid function and \u2297\\otimes is the element-wise product between matrices.\n\nSee Language Modeling with Gated Convolutional Networks.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.grid_sample()", "path": "nn.functional#torch.nn.functional.grid_sample", "type": "torch.nn.functional", "text": "\nGiven an `input` and a flow-field `grid`, computes the `output` using `input`\nvalues and pixel locations from `grid`.\n\nCurrently, only spatial (4-D) and volumetric (5-D) `input` are supported.\n\nIn the spatial (4-D) case, for `input` with shape (N,C,Hin,Win)(N, C,\nH_\\text{in}, W_\\text{in}) and `grid` with shape (N,Hout,Wout,2)(N,\nH_\\text{out}, W_\\text{out}, 2) , the output will have shape (N,C,Hout,Wout)(N,\nC, H_\\text{out}, W_\\text{out}) .\n\nFor each output location `output[n, :, h, w]`, the size-2 vector `grid[n, h,\nw]` specifies `input` pixel locations `x` and `y`, which are used to\ninterpolate the output value `output[n, :, h, w]`. In the case of 5D inputs,\n`grid[n, d, h, w]` specifies the `x`, `y`, `z` pixel locations for\ninterpolating `output[n, :, d, h, w]`. `mode` argument specifies `nearest` or\n`bilinear` interpolation method to sample the input pixels.\n\n`grid` specifies the sampling pixel locations normalized by the `input`\nspatial dimensions. Therefore, it should have most values in the range of\n`[-1, 1]`. For example, values `x = -1, y = -1` is the left-top pixel of\n`input`, and values `x = 1, y = 1` is the right-bottom pixel of `input`.\n\nIf `grid` has values outside the range of `[-1, 1]`, the corresponding outputs\nare handled as defined by `padding_mode`. Options are\n\nNote\n\nThis function is often used in conjunction with `affine_grid()` to build\nSpatial Transformer Networks .\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nNote\n\nNaN values in `grid` would be interpreted as `-1`.\n\noutput Tensor\n\noutput (Tensor)\n\nWarning\n\nWhen `align_corners = True`, the grid positions depend on the pixel size\nrelative to the input image size, and so the locations sampled by\n`grid_sample()` will differ for the same input given at different resolutions\n(that is, after being upsampled or downsampled). The default behavior up to\nversion 1.2.0 was `align_corners = True`. Since then, the default behavior has\nbeen changed to `align_corners = False`, in order to bring it in line with the\ndefault for `interpolate()`.\n\nNote\n\n`mode='bicubic'` is implemented using the cubic convolution algorithm with\n\u03b1=\u22120.75\\alpha=-0.75 . The constant \u03b1\\alpha might be different from packages to\npackages. For example, PIL and OpenCV use -0.5 and -0.75 respectively. This\nalgorithm may \u201covershoot\u201d the range of values it\u2019s interpolating. For example,\nit may produce negative values or values greater than 255 when interpolating\ninput in [0, 255]. Clamp the results with :func: `torch.clamp` to ensure they\nare within the valid range.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.gumbel_softmax()", "path": "nn.functional#torch.nn.functional.gumbel_softmax", "type": "torch.nn.functional", "text": "\nSamples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally\ndiscretizes.\n\nSampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\nIf `hard=True`, the returned samples will be one-hot, otherwise they will be\nprobability distributions that sum to 1 across `dim`.\n\nNote\n\nThis function is here for legacy reasons, may be removed from nn.Functional in\nthe future.\n\nNote\n\nThe main trick for `hard` is to do `y_hard - y_soft.detach() + y_soft`\n\nIt achieves two things: - makes the output value exactly one-hot (since we add\nthen subtract y_soft value) - makes the gradient equal to y_soft gradient\n(since we strip all other gradients)\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardshrink()", "path": "nn.functional#torch.nn.functional.hardshrink", "type": "torch.nn.functional", "text": "\nApplies the hard shrinkage function element-wise\n\nSee `Hardshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardsigmoid()", "path": "nn.functional#torch.nn.functional.hardsigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function\n\ninplace \u2013 If set to `True`, will do this operation in-place. Default: `False`\n\nSee `Hardsigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardswish()", "path": "nn.functional#torch.nn.functional.hardswish", "type": "torch.nn.functional", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\nSee `Hardswish` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh()", "path": "nn.functional#torch.nn.functional.hardtanh", "type": "torch.nn.functional", "text": "\nApplies the HardTanh function element-wise. See `Hardtanh` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hardtanh_()", "path": "nn.functional#torch.nn.functional.hardtanh_", "type": "torch.nn.functional", "text": "\nIn-place version of `hardtanh()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.hinge_embedding_loss()", "path": "nn.functional#torch.nn.functional.hinge_embedding_loss", "type": "torch.nn.functional", "text": "\nSee `HingeEmbeddingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.instance_norm()", "path": "nn.functional#torch.nn.functional.instance_norm", "type": "torch.nn.functional", "text": "\nApplies Instance Normalization for each channel in each data sample in a\nbatch.\n\nSee `InstanceNorm1d`, `InstanceNorm2d`, `InstanceNorm3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.interpolate()", "path": "nn.functional#torch.nn.functional.interpolate", "type": "torch.nn.functional", "text": "\nDown/up samples the input to either the given `size` or the given\n`scale_factor`\n\nThe algorithm used for interpolation is determined by `mode`.\n\nCurrently temporal, spatial and volumetric sampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for resizing are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\nWarning\n\nWhen scale_factor is specified, if recompute_scale_factor=True, scale_factor\nis used to compute the output_size which will then be used to infer new scales\nfor the interpolation. The default behavior for recompute_scale_factor changed\nto False in 1.6.0, and scale_factor is used in the interpolation calculation.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.kl_div()", "path": "nn.functional#torch.nn.functional.kl_div", "type": "torch.nn.functional", "text": "\nThe Kullback-Leibler divergence Loss\n\nSee `KLDivLoss` for details.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n:attr:`reduction` = `'mean'` doesn\u2019t return the true kl divergence value,\nplease use :attr:`reduction` = `'batchmean'` which aligns with KL math\ndefinition. In the next major release, `'mean'` will be changed to be the same\nas \u2018batchmean\u2019.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.l1_loss()", "path": "nn.functional#torch.nn.functional.l1_loss", "type": "torch.nn.functional", "text": "\nFunction that takes the mean element-wise absolute value difference.\n\nSee `L1Loss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.layer_norm()", "path": "nn.functional#torch.nn.functional.layer_norm", "type": "torch.nn.functional", "text": "\nApplies Layer Normalization for last certain number of dimensions.\n\nSee `LayerNorm` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu()", "path": "nn.functional#torch.nn.functional.leaky_relu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nLeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0,\nx) + \\text{negative\\\\_slope} * \\min(0, x)\n\nSee `LeakyReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.leaky_relu_()", "path": "nn.functional#torch.nn.functional.leaky_relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `leaky_relu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.linear()", "path": "nn.functional#torch.nn.functional.linear", "type": "torch.nn.functional", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b .\n\nThis operator supports TensorFloat32.\n\nShape:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.local_response_norm()", "path": "nn.functional#torch.nn.functional.local_response_norm", "type": "torch.nn.functional", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nSee `LocalResponseNorm` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.logsigmoid()", "path": "nn.functional#torch.nn.functional.logsigmoid", "type": "torch.nn.functional", "text": "\nApplies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) =\n\\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)\n\nSee `LogSigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.log_softmax()", "path": "nn.functional#torch.nn.functional.log_softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax followed by a logarithm.\n\nWhile mathematically equivalent to log(softmax(x)), doing these two operations\nseparately is slower, and numerically unstable. This function uses an\nalternative formulation to compute the output and gradient correctly.\n\nSee `LogSoftmax` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool1d()", "path": "nn.functional#torch.nn.functional.lp_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.lp_pool2d()", "path": "nn.functional#torch.nn.functional.lp_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes. If the sum of all inputs to the power of `p` is zero, the\ngradient is set to zero as well.\n\nSee `LPPool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.margin_ranking_loss()", "path": "nn.functional#torch.nn.functional.margin_ranking_loss", "type": "torch.nn.functional", "text": "\nSee `MarginRankingLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool1d()", "path": "nn.functional#torch.nn.functional.max_pool1d", "type": "torch.nn.functional", "text": "\nApplies a 1D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool2d()", "path": "nn.functional#torch.nn.functional.max_pool2d", "type": "torch.nn.functional", "text": "\nApplies a 2D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_pool3d()", "path": "nn.functional#torch.nn.functional.max_pool3d", "type": "torch.nn.functional", "text": "\nApplies a 3D max pooling over an input signal composed of several input\nplanes.\n\nSee `MaxPool3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool1d()", "path": "nn.functional#torch.nn.functional.max_unpool1d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool1d`.\n\nSee `MaxUnpool1d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool2d()", "path": "nn.functional#torch.nn.functional.max_unpool2d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool2d`.\n\nSee `MaxUnpool2d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.max_unpool3d()", "path": "nn.functional#torch.nn.functional.max_unpool3d", "type": "torch.nn.functional", "text": "\nComputes a partial inverse of `MaxPool3d`.\n\nSee `MaxUnpool3d` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.mse_loss()", "path": "nn.functional#torch.nn.functional.mse_loss", "type": "torch.nn.functional", "text": "\nMeasures the element-wise mean squared error.\n\nSee `MSELoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multilabel_soft_margin_loss()", "path": "nn.functional#torch.nn.functional.multilabel_soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `MultiLabelSoftMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.multi_margin_loss()", "path": "nn.functional#torch.nn.functional.multi_margin_loss", "type": "torch.nn.functional", "text": "\nreduce=None, reduction=\u2019mean\u2019) -> Tensor\n\nSee `MultiMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.nll_loss()", "path": "nn.functional#torch.nn.functional.nll_loss", "type": "torch.nn.functional", "text": "\nThe negative log likelihood loss.\n\nSee `NLLLoss` for details.\n\nExample:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.normalize()", "path": "nn.functional#torch.nn.functional.normalize", "type": "torch.nn.functional", "text": "\nPerforms LpL_p normalization of inputs over specified dimension.\n\nFor a tensor `input` of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ...,\nn_k) , each ndimn_{dim} -element vector vv along dimension `dim` is\ntransformed as\n\nWith the default arguments it uses the Euclidean norm over vectors along\ndimension 11 for normalization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.one_hot()", "path": "nn.functional#torch.nn.functional.one_hot", "type": "torch.nn.functional", "text": "\nTakes LongTensor with index values of shape `(*)` and returns a tensor of\nshape `(*, num_classes)` that have zeros everywhere except where the index of\nlast dimension matches the corresponding value of the input tensor, in which\ncase it will be 1.\n\nSee also One-hot on Wikipedia .\n\nLongTensor that has one more dimension with 1 values at the index of last\ndimension indicated by the input, and 0 everywhere else.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pad()", "path": "nn.functional#torch.nn.functional.pad", "type": "torch.nn.functional", "text": "\nPads tensor.\n\nThe padding size by which to pad some dimensions of `input` are described\nstarting from the last dimension and moving forward.\n\u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor dimensions of\n`input` will be padded. For example, to pad only the last dimension of the\ninput tensor, then `pad` has the form\n(padding_left,padding_right)(\\text{padding\\\\_left}, \\text{padding\\\\_right}) ;\nto pad the last 2 dimensions of the input tensor, then use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom)\\text{padding\\\\_top}, \\text{padding\\\\_bottom}) ; to\npad the last 3 dimensions, use\n(padding_left,padding_right,(\\text{padding\\\\_left}, \\text{padding\\\\_right},\npadding_top,padding_bottom\\text{padding\\\\_top}, \\text{padding\\\\_bottom}\npadding_front,padding_back)\\text{padding\\\\_front}, \\text{padding\\\\_back}) .\n\nSee `torch.nn.ConstantPad2d`, `torch.nn.ReflectionPad2d`, and\n`torch.nn.ReplicationPad2d` for concrete examples on how each of the padding\nmodes works. Constant padding is implemented for arbitrary dimensions.\nReplicate padding is implemented for padding the last 3 dimensions of 5D input\ntensor, or the last 2 dimensions of 4D input tensor, or the last dimension of\n3D input tensor. Reflect padding is only implemented for padding the last 2\ndimensions of 4D input tensor, or the last dimension of 3D input tensor.\n\nNote\n\nWhen using the CUDA backend, this operation may induce nondeterministic\nbehaviour in its backward pass that is not easily switched off. Please see the\nnotes on Reproducibility for background.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pairwise_distance()", "path": "nn.functional#torch.nn.functional.pairwise_distance", "type": "torch.nn.functional", "text": "\nSee `torch.nn.PairwiseDistance` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pdist()", "path": "nn.functional#torch.nn.functional.pdist", "type": "torch.nn.functional", "text": "\nComputes the p-norm distance between every pair of row vectors in the input.\nThis is identical to the upper triangular portion, excluding the diagonal, of\n`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster\nif the rows are contiguous.\n\nIf input has shape N\u00d7MN \\times M then the output will have shape\n12N(N\u22121)\\frac{1}{2} N (N - 1) .\n\nThis function is equivalent to `scipy.spatial.distance.pdist(input,\n\u2018minkowski\u2019, p=p)` if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0 it is\nequivalent to `scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M`. When p=\u221ep\n= \\infty , the closest scipy function is `scipy.spatial.distance.pdist(xn,\nlambda x, y: np.abs(x - y).max())`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_shuffle()", "path": "nn.functional#torch.nn.functional.pixel_shuffle", "type": "torch.nn.functional", "text": "\nRearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)\nto a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) , where r is\nthe `upscale_factor`.\n\nSee `PixelShuffle` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.pixel_unshuffle()", "path": "nn.functional#torch.nn.functional.pixel_unshuffle", "type": "torch.nn.functional", "text": "\nReverses the `PixelShuffle` operation by rearranging elements in a tensor of\nshape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r) to a tensor of shape\n(\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W) , where r is the `downscale_factor`.\n\nSee `PixelUnshuffle` for details.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.poisson_nll_loss()", "path": "nn.functional#torch.nn.functional.poisson_nll_loss", "type": "torch.nn.functional", "text": "\nPoisson negative log likelihood loss.\n\nSee `PoissonNLLLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.prelu()", "path": "nn.functional#torch.nn.functional.prelu", "type": "torch.nn.functional", "text": "\nApplies element-wise the function\nPReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight}\n* \\min(0,x) where weight is a learnable parameter.\n\nSee `PReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu()", "path": "nn.functional#torch.nn.functional.relu", "type": "torch.nn.functional", "text": "\nApplies the rectified linear unit function element-wise. See `ReLU` for more\ndetails.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu6()", "path": "nn.functional#torch.nn.functional.relu6", "type": "torch.nn.functional", "text": "\nApplies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) =\n\\min(\\max(0,x), 6) .\n\nSee `ReLU6` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.relu_()", "path": "nn.functional#torch.nn.functional.relu_", "type": "torch.nn.functional", "text": "\nIn-place version of `relu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu()", "path": "nn.functional#torch.nn.functional.rrelu", "type": "torch.nn.functional", "text": "\nRandomized leaky ReLU.\n\nSee `RReLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.rrelu_()", "path": "nn.functional#torch.nn.functional.rrelu_", "type": "torch.nn.functional", "text": "\nIn-place version of `rrelu()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.selu()", "path": "nn.functional#torch.nn.functional.selu", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nSELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale *\n(\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))) , with\n\u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\nand\nscale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946\n.\n\nSee `SELU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.sigmoid()", "path": "nn.functional#torch.nn.functional.sigmoid", "type": "torch.nn.functional", "text": "\nApplies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) =\n\\frac{1}{1 + \\exp(-x)}\n\nSee `Sigmoid` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.silu()", "path": "nn.functional#torch.nn.functional.silu", "type": "torch.nn.functional", "text": "\nApplies the silu function, element-wise.\n\nNote\n\nSee Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit)\nwas originally coined, and see Sigmoid-Weighted Linear Units for Neural\nNetwork Function Approximation in Reinforcement Learning and Swish: a Self-\nGated Activation Function where the SiLU was experimented with later.\n\nSee `SiLU` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.smooth_l1_loss()", "path": "nn.functional#torch.nn.functional.smooth_l1_loss", "type": "torch.nn.functional", "text": "\nFunction that uses a squared term if the absolute element-wise error falls\nbelow beta and an L1 term otherwise.\n\nSee `SmoothL1Loss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softmax()", "path": "nn.functional#torch.nn.functional.softmax", "type": "torch.nn.functional", "text": "\nApplies a softmax function.\n\nSoftmax is defined as:\n\nSoftmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j\n\\exp(x_j)}\n\nIt is applied to all slices along dim, and will re-scale them so that the\nelements lie in the range `[0, 1]` and sum to 1.\n\nSee `Softmax` for more details.\n\nNote\n\nThis function doesn\u2019t work directly with NLLLoss, which expects the Log to be\ncomputed between the Softmax and itself. Use log_softmax instead (it\u2019s faster\nand has better numerical properties).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softmin()", "path": "nn.functional#torch.nn.functional.softmin", "type": "torch.nn.functional", "text": "\nApplies a softmin function.\n\nNote that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x) . See\nsoftmax definition for mathematical formula.\n\nSee `Softmin` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softplus()", "path": "nn.functional#torch.nn.functional.softplus", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function\nSoftplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1\n+ \\exp(\\beta * x)) .\n\nFor numerical stability the implementation reverts to the linear function when\ninput\u00d7\u03b2>thresholdinput \\times \\beta > threshold .\n\nSee `Softplus` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softshrink()", "path": "nn.functional#torch.nn.functional.softshrink", "type": "torch.nn.functional", "text": "\nApplies the soft shrinkage function elementwise\n\nSee `Softshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.softsign()", "path": "nn.functional#torch.nn.functional.softsign", "type": "torch.nn.functional", "text": "\nApplies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) =\n\\frac{x}{1 + |x|}\n\nSee `Softsign` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.soft_margin_loss()", "path": "nn.functional#torch.nn.functional.soft_margin_loss", "type": "torch.nn.functional", "text": "\nSee `SoftMarginLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.tanh()", "path": "nn.functional#torch.nn.functional.tanh", "type": "torch.nn.functional", "text": "\nApplies element-wise,\nTanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) =\n\\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n\nSee `Tanh` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.tanhshrink()", "path": "nn.functional#torch.nn.functional.tanhshrink", "type": "torch.nn.functional", "text": "\nApplies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x -\n\\text{Tanh}(x)\n\nSee `Tanhshrink` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold()", "path": "nn.functional#torch.nn.functional.threshold", "type": "torch.nn.functional", "text": "\nThresholds each element of the input Tensor.\n\nSee `Threshold` for more details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.threshold_()", "path": "nn.functional#torch.nn.functional.threshold_", "type": "torch.nn.functional", "text": "\nIn-place version of `threshold()`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginLoss` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.triplet_margin_with_distance_loss()", "path": "nn.functional#torch.nn.functional.triplet_margin_with_distance_loss", "type": "torch.nn.functional", "text": "\nSee `TripletMarginWithDistanceLoss` for details.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.unfold()", "path": "nn.functional#torch.nn.functional.unfold", "type": "torch.nn.functional", "text": "\nExtracts sliding local blocks from a batched input tensor.\n\nWarning\n\nCurrently, only 4-D input tensors (batched image-like tensors) are supported.\n\nWarning\n\nMore than one element of the unfolded tensor may refer to a single memory\nlocation. As a result, in-place operations (especially ones that are\nvectorized) may result in incorrect behavior. If you need to write to the\ntensor, please clone it first.\n\nSee `torch.nn.Unfold` for details\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample()", "path": "nn.functional#torch.nn.functional.upsample", "type": "torch.nn.functional", "text": "\nUpsamples the input to either the given `size` or the given `scale_factor`\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(...)`.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\nThe algorithm used for upsampling is determined by `mode`.\n\nCurrently temporal, spatial and volumetric upsampling are supported, i.e.\nexpected inputs are 3-D, 4-D or 5-D in shape.\n\nThe input dimensions are interpreted in the form: `mini-batch x channels x\n[optional depth] x [optional height] x width`.\n\nThe modes available for upsampling are: `nearest`, `linear` (3D-only),\n`bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)\n\nNote\n\nWith `mode='bicubic'`, it\u2019s possible to cause overshoot, in other words it can\nproduce negative values or values greater than 255 for images. Explicitly call\n`result.clamp(min=0, max=255)` if you want to reduce the overshoot when\ndisplaying the image.\n\nWarning\n\nWith `align_corners = True`, the linearly interpolating modes (`linear`,\n`bilinear`, and `trilinear`) don\u2019t proportionally align the output and input\npixels, and thus the output values can depend on the input size. This was the\ndefault behavior for these modes up to version 0.3.1. Since then, the default\nbehavior is `align_corners = False`. See `Upsample` for concrete examples on\nhow this affects the outputs.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_bilinear()", "path": "nn.functional#torch.nn.functional.upsample_bilinear", "type": "torch.nn.functional", "text": "\nUpsamples the input, using bilinear upsampling.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='bilinear',\nalign_corners=True)`.\n\nExpected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo\nvolumetric (5 dimensional) inputs.\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.functional.upsample_nearest()", "path": "nn.functional#torch.nn.functional.upsample_nearest", "type": "torch.nn.functional", "text": "\nUpsamples the input, using nearest neighbours\u2019 pixel values.\n\nWarning\n\nThis function is deprecated in favor of `torch.nn.functional.interpolate()`.\nThis is equivalent with `nn.functional.interpolate(..., mode='nearest')`.\n\nCurrently spatial and volumetric upsampling are supported (i.e. expected\ninputs are 4 or 5 dimensional).\n\nNote\n\nThis operation may produce nondeterministic gradients when given tensors on a\nCUDA device. See Reproducibility for more information.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GaussianNLLLoss", "path": "generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss", "type": "torch.nn", "text": "\nGaussian negative log likelihood loss.\n\nThe targets are treated as samples from Gaussian distributions with\nexpectations and variances predicted by the neural network. For a\nD-dimensional `target` tensor modelled as having heteroscedastic Gaussian\ndistributions with a D-dimensional tensor of expectations `input` and a\nD-dimensional tensor of positive variances `var` the loss is:\n\nwhere `eps` is used for stability. By default, the constant term of the loss\nfunction is omitted unless `full` is `True`. If `var` is a scalar (implying\n`target` tensor has homoscedastic Gaussian distributions) it is broadcasted to\nbe the same size as the input.\n\nExamples:\n\nNote\n\nThe clamping of `var` is ignored with respect to autograd, and so the\ngradients are unaffected by it.\n\nNix, D. A. and Weigend, A. S., \u201cEstimating the mean and variance of the target\nprobability distribution\u201d, Proceedings of 1994 IEEE International Conference\non Neural Networks (ICNN\u201994), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi:\n10.1109/ICNN.1994.374138.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GELU", "path": "generated/torch.nn.gelu#torch.nn.GELU", "type": "torch.nn", "text": "\nApplies the Gaussian Error Linear Units function:\n\nwhere \u03a6(x)\\Phi(x) is the Cumulative Distribution Function for Gaussian\nDistribution.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GroupNorm", "path": "generated/torch.nn.groupnorm#torch.nn.GroupNorm", "type": "torch.nn", "text": "\nApplies Group Normalization over a mini-batch of inputs as described in the\npaper Group Normalization\n\nThe input channels are separated into `num_groups` groups, each containing\n`num_channels / num_groups` channels. The mean and standard-deviation are\ncalculated separately over the each group. \u03b3\\gamma and \u03b2\\beta are learnable\nper-channel affine transform parameter vectors of size `num_channels` if\n`affine` is `True`. The standard-deviation is calculated via the biased\nestimator, equivalent to `torch.var(input, unbiased=False)`.\n\nThis layer uses statistics computed from input data in both training and\nevaluation modes.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GRU", "path": "generated/torch.nn.gru#torch.nn.GRU", "type": "torch.nn", "text": "\nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.\n\nFor each element in the input sequence, each layer computes the following\nfunction:\n\nwhere hth_t is the hidden state at time `t`, xtx_t is the input at time `t`,\nh(t\u22121)h_{(t-1)} is the hidden state of the layer at time `t-1` or the initial\nhidden state at time `0`, and rtr_t , ztz_t , ntn_t are the reset, update, and\nnew gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the\nHadamard product.\n\nIn a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2\n) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by\ndropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a\nBernoulli random variable which is 00 with probability `dropout`.\n\noutput of shape `(seq_len, batch, num_directions * hidden_size)`: tensor\ncontaining the output features h_t from the last layer of the GRU, for each\n`t`. If a `torch.nn.utils.rnn.PackedSequence` has been given as the input, the\noutput will also be a packed sequence. For the unpacked case, the directions\ncan be separated using `output.view(seq_len, batch, num_directions,\nhidden_size)`, with forward and backward being direction `0` and `1`\nrespectively.\n\nSimilarly, the directions can be separated in the packed case.\n\nh_n of shape `(num_layers * num_directions, batch, hidden_size)`: tensor\ncontaining the hidden state for `t = seq_len`\n\nLike output, the layers can be separated using `h_n.view(num_layers,\nnum_directions, batch, hidden_size)`.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nNote\n\nIf the following conditions are satisfied: 1) cudnn is enabled, 2) input data\nis on the GPU 3) input data has dtype `torch.float16` 4) V100 GPU is used, 5)\ninput data is not in `PackedSequence` format persistent algorithm can be\nselected to improve performance.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.GRUCell", "path": "generated/torch.nn.grucell#torch.nn.GRUCell", "type": "torch.nn", "text": "\nA gated recurrent unit (GRU) cell\n\nwhere \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product.\n\nNote\n\nAll the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k},\n\\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\\\_size}}\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardshrink", "path": "generated/torch.nn.hardshrink#torch.nn.Hardshrink", "type": "torch.nn", "text": "\nApplies the hard shrinkage function element-wise:\n\nlambd \u2013 the \u03bb\\lambda value for the Hardshrink formulation. Default: 0.5\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardsigmoid", "path": "generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardswish", "path": "generated/torch.nn.hardswish#torch.nn.Hardswish", "type": "torch.nn", "text": "\nApplies the hardswish function, element-wise, as described in the paper:\n\nSearching for MobileNetV3.\n\ninplace \u2013 can optionally do the operation in-place. Default: `False`\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Hardtanh", "path": "generated/torch.nn.hardtanh#torch.nn.Hardtanh", "type": "torch.nn", "text": "\nApplies the HardTanh function element-wise\n\nHardTanh is defined as:\n\nThe range of the linear region [\u22121,1][-1, 1] can be adjusted using `min_val`\nand `max_val`.\n\nKeyword arguments `min_value` and `max_value` have been deprecated in favor of\n`min_val` and `max_val`.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.HingeEmbeddingLoss", "path": "generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss", "type": "torch.nn", "text": "\nMeasures the loss given an input tensor xx and a labels tensor yy (containing\n1 or -1). This is usually used for measuring whether two inputs are similar or\ndissimilar, e.g. using the L1 pairwise distance as xx , and is typically used\nfor learning nonlinear embeddings or semi-supervised learning.\n\nThe loss function for nn -th sample in the mini-batch is\n\nand the total loss functions is\n\nwhere L={l1,\u2026,lN}\u22a4L = \\\\{l_1,\\dots,l_N\\\\}^\\top .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Identity", "path": "generated/torch.nn.identity#torch.nn.Identity", "type": "torch.nn", "text": "\nA placeholder identity operator that is argument-insensitive.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init", "path": "nn.init", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\nnonlinearity\n\ngain\n\nLinear / Identity\n\n11\n\nConv{1,2,3}D\n\n11\n\nSigmoid\n\n11\n\nTanh\n\n53\\frac{5}{3}\n\nReLU\n\n2\\sqrt{2}\n\nLeaky Relu\n\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\\\_slope}^2}}\n\nSELU\n\n34\\frac{3}{4}\n\nFills the input Tensor with values drawn from the uniform distribution\nU(a,b)\\mathcal{U}(a, b) .\n\nFills the input Tensor with values drawn from the normal distribution\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .\n\nFills the input Tensor with the value val\\text{val} .\n\nFills the input Tensor with the scalar value `1`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\nFills the input Tensor with the scalar value `0`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\nFills the 2-dimensional input `Tensor` with the identity matrix. Preserves the\nidentity of the inputs in `Linear` layers, where as many inputs are preserved\nas possible.\n\ntensor \u2013 a 2-dimensional `torch.Tensor`\n\nFills the {3, 4, 5}-dimensional input `Tensor` with the Dirac delta function.\nPreserves the identity of the inputs in `Convolutional` layers, where as many\ninput channels are preserved as possible. In case of groups>1, each group of\nchannels preserves identity\n\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting\ntensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where\n\nAlso known as Glorot initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting\ntensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)\nwhere\n\nAlso known as Glorot initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a uniform distribution. The\nresulting tensor will have values sampled from\nU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where\n\nAlso known as He initialization.\n\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a normal distribution. The\nresulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0,\n\\text{std}^2) where\n\nAlso known as He initialization.\n\nFills the input `Tensor` with a (semi) orthogonal matrix, as described in\n`Exact solutions to the nonlinear dynamics of learning in deep linear neural\nnetworks` \\- Saxe, A. et al. (2013). The input tensor must have at least 2\ndimensions, and for tensors with more than 2 dimensions the trailing\ndimensions are flattened.\n\nFills the 2D input `Tensor` as a sparse matrix, where the non-zero elements\nwill be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as\ndescribed in `Deep learning via Hessian-free optimization` \\- Martens, J.\n(2010).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.calculate_gain()", "path": "nn.init#torch.nn.init.calculate_gain", "type": "torch.nn.init", "text": "\nReturn the recommended gain value for the given nonlinearity function. The\nvalues are as follows:\n\nnonlinearity\n\ngain\n\nLinear / Identity\n\n11\n\nConv{1,2,3}D\n\n11\n\nSigmoid\n\n11\n\nTanh\n\n53\\frac{5}{3}\n\nReLU\n\n2\\sqrt{2}\n\nLeaky Relu\n\n21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\\\_slope}^2}}\n\nSELU\n\n34\\frac{3}{4}\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.constant_()", "path": "nn.init#torch.nn.init.constant_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the value val\\text{val} .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.dirac_()", "path": "nn.init#torch.nn.init.dirac_", "type": "torch.nn.init", "text": "\nFills the {3, 4, 5}-dimensional input `Tensor` with the Dirac delta function.\nPreserves the identity of the inputs in `Convolutional` layers, where as many\ninput channels are preserved as possible. In case of groups>1, each group of\nchannels preserves identity\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.eye_()", "path": "nn.init#torch.nn.init.eye_", "type": "torch.nn.init", "text": "\nFills the 2-dimensional input `Tensor` with the identity matrix. Preserves the\nidentity of the inputs in `Linear` layers, where as many inputs are preserved\nas possible.\n\ntensor \u2013 a 2-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_normal_()", "path": "nn.init#torch.nn.init.kaiming_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a normal distribution. The\nresulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0,\n\\text{std}^2) where\n\nAlso known as He initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.kaiming_uniform_()", "path": "nn.init#torch.nn.init.kaiming_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Delving deep into rectifiers: Surpassing human-level performance on ImageNet\nclassification` \\- He, K. et al. (2015), using a uniform distribution. The\nresulting tensor will have values sampled from\nU(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound}) where\n\nAlso known as He initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.normal_()", "path": "nn.init#torch.nn.init.normal_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the normal distribution\nN(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.ones_()", "path": "nn.init#torch.nn.init.ones_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `1`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.orthogonal_()", "path": "nn.init#torch.nn.init.orthogonal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with a (semi) orthogonal matrix, as described in\n`Exact solutions to the nonlinear dynamics of learning in deep linear neural\nnetworks` \\- Saxe, A. et al. (2013). The input tensor must have at least 2\ndimensions, and for tensors with more than 2 dimensions the trailing\ndimensions are flattened.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.sparse_()", "path": "nn.init#torch.nn.init.sparse_", "type": "torch.nn.init", "text": "\nFills the 2D input `Tensor` as a sparse matrix, where the non-zero elements\nwill be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01) , as\ndescribed in `Deep learning via Hessian-free optimization` \\- Martens, J.\n(2010).\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.uniform_()", "path": "nn.init#torch.nn.init.uniform_", "type": "torch.nn.init", "text": "\nFills the input Tensor with values drawn from the uniform distribution\nU(a,b)\\mathcal{U}(a, b) .\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_normal_()", "path": "nn.init#torch.nn.init.xavier_normal_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a normal distribution. The resulting\ntensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)\nwhere\n\nAlso known as Glorot initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.xavier_uniform_()", "path": "nn.init#torch.nn.init.xavier_uniform_", "type": "torch.nn.init", "text": "\nFills the input `Tensor` with values according to the method described in\n`Understanding the difficulty of training deep feedforward neural networks` \\-\nGlorot, X. & Bengio, Y. (2010), using a uniform distribution. The resulting\ntensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a) where\n\nAlso known as Glorot initialization.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.init.zeros_()", "path": "nn.init#torch.nn.init.zeros_", "type": "torch.nn.init", "text": "\nFills the input Tensor with the scalar value `0`.\n\ntensor \u2013 an n-dimensional `torch.Tensor`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm1d", "path": "generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 3D input (a mini-batch of 1D inputs with\noptional additional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size `C` (where `C` is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm1d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm1d` is applied on each channel of channeled data\nlike multidimensional time series, but `LayerNorm` is usually applied on\nentire sample and often in NLP tasks. Additionally, `LayerNorm` applies\nelementwise affine transform, while `InstanceNorm1d` usually don\u2019t apply\naffine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm2d", "path": "generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 4D input (a mini-batch of 2D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size `C` (where `C` is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm2d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm2d` is applied on each channel of channeled data\nlike RGB images, but `LayerNorm` is usually applied on entire sample and often\nin NLP tasks. Additionally, `LayerNorm` applies elementwise affine transform,\nwhile `InstanceNorm2d` usually don\u2019t apply affine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.InstanceNorm3d", "path": "generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d", "type": "torch.nn", "text": "\nApplies Instance Normalization over a 5D input (a mini-batch of 3D inputs with\nadditional channel dimension) as described in the paper Instance\nNormalization: The Missing Ingredient for Fast Stylization.\n\nThe mean and standard-deviation are calculated per-dimension separately for\neach object in a mini-batch. \u03b3\\gamma and \u03b2\\beta are learnable parameter\nvectors of size C (where C is the input size) if `affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nBy default, this layer uses instance statistics computed from input data in\nboth training and evaluation modes.\n\nIf `track_running_stats` is set to `True`, during training this layer keeps\nrunning estimates of its computed mean and variance, which are then used for\nnormalization during evaluation. The running estimates are kept with a default\n`momentum` of 0.1.\n\nNote\n\nThis `momentum` argument is different from one used in optimizer classes and\nthe conventional notion of momentum. Mathematically, the update rule for\nrunning statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new}\n= (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t , where\nx^\\hat{x} is the estimated statistic and xtx_t is the new observed value.\n\nNote\n\n`InstanceNorm3d` and `LayerNorm` are very similar, but have some subtle\ndifferences. `InstanceNorm3d` is applied on each channel of channeled data\nlike 3D models with RGB color, but `LayerNorm` is usually applied on entire\nsample and often in NLP tasks. Additionally, `LayerNorm` applies elementwise\naffine transform, while `InstanceNorm3d` usually don\u2019t apply affine transform.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d and Batch Norm 1d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBn2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d and Batch Norm 2d\nmodules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvBnReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and\nReLU modules. During quantization this will be replaced with the corresponding\nfused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU1d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv1d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.ConvReLU2d", "path": "torch.nn.intrinsic#torch.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": "\nThis is a sequential container which calls the Conv2d and ReLU modules. During\nquantization this will be replaced with the corresponding fused module.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBn2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": "\nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\nWe combined the interface of `torch.nn.Conv2d` and `torch.nn.BatchNorm2d`.\n\nSimilar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to\ndefault.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvBnReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": "\nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU,\nattached with FakeQuantize modules for weight, used in quantization aware\ntraining.\n\nWe combined the interface of `torch.nn.Conv2d` and `torch.nn.BatchNorm2d` and\n`torch.nn.ReLU`.\n\nSimilar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to\ndefault.\n\n~ConvBnReLU2d.weight_fake_quant \u2013 fake quant module for weight\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.ConvReLU2d", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with\nFakeQuantize modules for weight for quantization aware training.\n\nWe combined the interface of `Conv2d` and `BatchNorm2d`.\n\n~ConvReLU2d.weight_fake_quant \u2013 fake quant module for weight\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.qat.LinearReLU", "path": "torch.nn.intrinsic.qat#torch.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules, attached with\nFakeQuantize modules for weight, used in quantization aware training.\n\nWe adopt the same interface as `torch.nn.Linear`.\n\nSimilar to `torch.nn.intrinsic.LinearReLU`, with FakeQuantize modules\ninitialized to default.\n\n~LinearReLU.weight \u2013 fake quant module for weight\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU2d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": "\nA ConvReLU2d module is a fused module of Conv2d and ReLU\n\nWe adopt the same interface as `torch.nn.quantized.Conv2d`.\n\nas torch.nn.quantized.Conv2d (Same) \u2013\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.ConvReLU3d", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": "\nA ConvReLU3d module is a fused module of Conv3d and ReLU\n\nWe adopt the same interface as `torch.nn.quantized.Conv3d`.\n\nAttributes: Same as torch.nn.quantized.Conv3d\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.intrinsic.quantized.LinearReLU", "path": "torch.nn.intrinsic.quantized#torch.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": "\nA LinearReLU module fused from Linear and ReLU modules\n\nWe adopt the same interface as `torch.nn.quantized.Linear`.\n\nas torch.nn.quantized.Linear (Same) \u2013\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.KLDivLoss", "path": "generated/torch.nn.kldivloss#torch.nn.KLDivLoss", "type": "torch.nn", "text": "\nThe Kullback-Leibler divergence loss measure\n\nKullback-Leibler divergence is a useful distance measure for continuous\ndistributions and is often useful when performing direct regression over the\nspace of (discretely sampled) continuous output distributions.\n\nAs with `NLLLoss`, the `input` given is expected to contain log-probabilities\nand is not restricted to a 2D Tensor. The targets are interpreted as\nprobabilities by default, but could be considered as log-probabilities with\n`log_target` set to `True`.\n\nThis criterion expects a `target` `Tensor` of the same size as the `input`\n`Tensor`.\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere the index NN spans all dimensions of `input` and LL has the same shape\nas `input`. If `reduction` is not `'none'` (default `'mean'`), then:\n\nIn default `reduction` mode `'mean'`, the losses are averaged for each\nminibatch over observations as well as over dimensions. `'batchmean'` mode\ngives the correct KL divergence where losses are averaged over batch dimension\nonly. `'mean'` mode\u2019s behavior will be changed to the same as `'batchmean'` in\nthe next major release.\n\nNote\n\n`size_average` and `reduce` are in the process of being deprecated, and in the\nmeantime, specifying either of those two args will override `reduction`.\n\nNote\n\n`reduction` = `'mean'` doesn\u2019t return the true kl divergence value, please use\n`reduction` = `'batchmean'` which aligns with KL math definition. In the next\nmajor release, `'mean'` will be changed to be the same as `'batchmean'`.\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.L1Loss", "path": "generated/torch.nn.l1loss#torch.nn.L1Loss", "type": "torch.nn", "text": "\nCreates a criterion that measures the mean absolute error (MAE) between each\nelement in the input xx and target yy .\n\nThe unreduced (i.e. with `reduction` set to `'none'`) loss can be described\nas:\n\nwhere NN is the batch size. If `reduction` is not `'none'` (default `'mean'`),\nthen:\n\nxx and yy are tensors of arbitrary shapes with a total of nn elements each.\n\nThe sum operation still operates over all the elements, and divides by nn .\n\nThe division by nn can be avoided if one sets `reduction = 'sum'`.\n\nSupports real-valued and complex-valued inputs.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LayerNorm", "path": "generated/torch.nn.layernorm#torch.nn.LayerNorm", "type": "torch.nn", "text": "\nApplies Layer Normalization over a mini-batch of inputs as described in the\npaper Layer Normalization\n\nThe mean and standard-deviation are calculated separately over the last\ncertain number dimensions which have to be of the shape specified by\n`normalized_shape`. \u03b3\\gamma and \u03b2\\beta are learnable affine transform\nparameters of `normalized_shape` if `elementwise_affine` is `True`. The\nstandard-deviation is calculated via the biased estimator, equivalent to\n`torch.var(input, unbiased=False)`.\n\nNote\n\nUnlike Batch Normalization and Instance Normalization, which applies scalar\nscale and bias for each entire channel/plane with the `affine` option, Layer\nNormalization applies per-element scale and bias with `elementwise_affine`.\n\nThis layer uses statistics computed from input data in both training and\nevaluation modes.\n\nnormalized_shape (int or list or torch.Size) \u2013\n\ninput shape from an expected input of size\n\nIf a single integer is used, it is treated as a singleton list, and this\nmodule will normalize over the last dimension which is expected to be of that\nspecific size.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d", "type": "torch.nn", "text": "\nA `torch.nn.Conv1d` module with lazy initialization of the `in_channels`\nargument of the `Conv1d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv1d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv1d.cls_to_become", "path": "generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d", "type": "torch.nn", "text": "\nA `torch.nn.Conv2d` module with lazy initialization of the `in_channels`\nargument of the `Conv2d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv2d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv2d.cls_to_become", "path": "generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d", "type": "torch.nn", "text": "\nA `torch.nn.Conv3d` module with lazy initialization of the `in_channels`\nargument of the `Conv3d` that is inferred from the `input.size(1)`.\n\nSee also\n\n`torch.nn.Conv3d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `Conv3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConv3d.cls_to_become", "path": "generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `Conv3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose1d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose1d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose1d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose1d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose1d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose2d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose2d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose2d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose2d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose2d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d", "type": "torch.nn", "text": "\nA `torch.nn.ConvTranspose3d` module with lazy initialization of the\n`in_channels` argument of the `ConvTranspose3d` that is inferred from the\n`input.size(1)`.\n\nSee also\n\n`torch.nn.ConvTranspose3d` and `torch.nn.modules.lazy.LazyModuleMixin`\n\nalias of `ConvTranspose3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyConvTranspose3d.cls_to_become", "path": "generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d.cls_to_become", "type": "torch.nn", "text": "\nalias of `ConvTranspose3d`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear", "type": "torch.nn", "text": "\nA `torch.nn.Linear` module with lazy initialization.\n\nIn this module, the `weight` and `bias` are of\n`torch.nn.UninitializedParameter` class. They will be initialized after the\nfirst call to `forward` is done and the module will become a regular\n`torch.nn.Linear` module.\n\nCheck the `torch.nn.modules.lazy.LazyModuleMixin` for further documentation on\nlazy modules and their limitations.\n\nalias of `Linear`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LazyLinear.cls_to_become", "path": "generated/torch.nn.lazylinear#torch.nn.LazyLinear.cls_to_become", "type": "torch.nn", "text": "\nalias of `Linear`\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LeakyReLU", "path": "generated/torch.nn.leakyrelu#torch.nn.LeakyReLU", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nor\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.Linear", "path": "generated/torch.nn.linear#torch.nn.Linear", "type": "torch.nn", "text": "\nApplies a linear transformation to the incoming data: y=xAT+by = xA^T + b\n\nThis module supports TensorFloat32.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LocalResponseNorm", "path": "generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm", "type": "torch.nn", "text": "\nApplies local response normalization over an input signal composed of several\ninput planes, where channels occupy the second dimension. Applies\nnormalization across channels.\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LogSigmoid", "path": "generated/torch.nn.logsigmoid#torch.nn.LogSigmoid", "type": "torch.nn", "text": "\nApplies the element-wise function:\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LogSoftmax", "path": "generated/torch.nn.logsoftmax#torch.nn.LogSoftmax", "type": "torch.nn", "text": "\nApplies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x)) function to an\nn-dimensional input Tensor. The LogSoftmax formulation can be simplified as:\n\ndim (int) \u2013 A dimension along which LogSoftmax will be computed.\n\na Tensor of the same dimension and shape as the input with values in the range\n[-inf, 0)\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LPPool1d", "path": "generated/torch.nn.lppool1d#torch.nn.LPPool1d", "type": "torch.nn", "text": "\nApplies a 1D power-average pooling over an input signal composed of several\ninput planes.\n\nOn each window, the function computed is:\n\nNote\n\nIf the sum to the power of `p` is zero, the gradient of this function is not\ndefined. This implementation will set the gradient to zero in this case.\n\nOutput: (N,C,Lout)(N, C, L_{out}) , where\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LPPool2d", "path": "generated/torch.nn.lppool2d#torch.nn.LPPool2d", "type": "torch.nn", "text": "\nApplies a 2D power-average pooling over an input signal composed of several\ninput planes.\n\nOn each window, the function computed is:\n\nThe parameters `kernel_size`, `stride` can either be:\n\nNote\n\nIf the sum to the power of `p` is zero, the gradient of this function is not\ndefined. This implementation will set the gradient to zero in this case.\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out}) , where\n\nExamples:\n\n  *[FIFO]: first-in, first-out\n  *[LIFO]: last-in, first-out\n\n"}, {"name": "torch.nn.LSTM", "path": "generated/torch.nn.lstm#torch.nn.LSTM", "type": "torch.nn", "text": "\nApplies a multi-layer long short-term memory (LSTM) RNN to an input sequence.\n\nFor each element in the input sequence, each layer computes the following\nfunction:\n\nwhere hth_t is the hidden state at time `t`, ctc_t is the cell state at time\n`t`, xtx_t is the input at time `t`, ht\u22121h_{t-1} is the hidden state of the\nlayer at time `t-1` or the initial hidden state at time `0`, and iti_t , ftf_t\n, gtg_t , oto_t are the input, forget, cell, and output gates, respectively.\n\u03c3\\sigma is the sigmoid function, and \u2299\\odot is the Hadamard product.\n\nIn a multilayer LSTM, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2\n) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by\ndropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a\nBernoulli random variable which is 00 with probability `dropout`.\n\nIf `proj_size > 0` is specified, LSTM with projections will be used. This\nchanges the LSTM cell in the following way. First, the dimension of hth_t will\nbe changed from `hidden_size` to `proj_size` (dimensions of WhiW_{hi} will be\nchanged accordingly). Second, the output hidden state of each layer will be\nmultiplied by a learnable projection matrix: ht=Whrhth_t = W_{hr}h_t . Note\nthat as a consequence of this, the output of LSTM network will be of different\nshape as well. See Inputs/Outputs sections below for exact dimensions of all\nvariables. You can find more deta