[{"name": "torch.__config__", "path": "config_mod", "type": "Miscellaneous", "text": "torch.__config__  \ntorch.__config__.show() [source]\n \nReturn a human-readable string with descriptions of the configuration of PyTorch. \n  \ntorch.__config__.parallel_info() [source]\n \nReturns detailed string with parallelization settings \n\n"}, {"name": "torch.__config__.parallel_info()", "path": "config_mod#torch.__config__.parallel_info", "type": "Miscellaneous", "text": " \ntorch.__config__.parallel_info() [source]\n \nReturns detailed string with parallelization settings \n"}, {"name": "torch.__config__.show()", "path": "config_mod#torch.__config__.show", "type": "Miscellaneous", "text": " \ntorch.__config__.show() [source]\n \nReturn a human-readable string with descriptions of the configuration of PyTorch. \n"}, {"name": "torch._assert", "path": "generated/torch._assert", "type": "Torch", "text": "torch._assert  \ntorch._assert(condition, message) [source]\n \nA wrapper around Python\u2019s assert which is symbolically traceable. \n\n"}, {"name": "torch._foreach_abs", "path": "generated/torch._foreach_abs", "type": "Torch", "text": "torch._foreach_abs  \ntorch._foreach_abs(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.abs() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_abs_", "path": "generated/torch._foreach_abs_", "type": "Torch", "text": "torch._foreach_abs_  \ntorch._foreach_abs_(self: List[Tensor]) \u2192 None  \nApply torch.abs() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_acos", "path": "generated/torch._foreach_acos", "type": "Torch", "text": "torch._foreach_acos  \ntorch._foreach_acos(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.acos() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_acos_", "path": "generated/torch._foreach_acos_", "type": "Torch", "text": "torch._foreach_acos_  \ntorch._foreach_acos_(self: List[Tensor]) \u2192 None  \nApply torch.acos() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_asin", "path": "generated/torch._foreach_asin", "type": "Torch", "text": "torch._foreach_asin  \ntorch._foreach_asin(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.asin() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_asin_", "path": "generated/torch._foreach_asin_", "type": "Torch", "text": "torch._foreach_asin_  \ntorch._foreach_asin_(self: List[Tensor]) \u2192 None  \nApply torch.asin() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_atan", "path": "generated/torch._foreach_atan", "type": "Torch", "text": "torch._foreach_atan  \ntorch._foreach_atan(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.atan() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_atan_", "path": "generated/torch._foreach_atan_", "type": "Torch", "text": "torch._foreach_atan_  \ntorch._foreach_atan_(self: List[Tensor]) \u2192 None  \nApply torch.atan() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_ceil", "path": "generated/torch._foreach_ceil", "type": "Torch", "text": "torch._foreach_ceil  \ntorch._foreach_ceil(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.ceil() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_ceil_", "path": "generated/torch._foreach_ceil_", "type": "Torch", "text": "torch._foreach_ceil_  \ntorch._foreach_ceil_(self: List[Tensor]) \u2192 None  \nApply torch.ceil() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_cos", "path": "generated/torch._foreach_cos", "type": "Torch", "text": "torch._foreach_cos  \ntorch._foreach_cos(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.cos() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_cos_", "path": "generated/torch._foreach_cos_", "type": "Torch", "text": "torch._foreach_cos_  \ntorch._foreach_cos_(self: List[Tensor]) \u2192 None  \nApply torch.cos() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_cosh", "path": "generated/torch._foreach_cosh", "type": "Torch", "text": "torch._foreach_cosh  \ntorch._foreach_cosh(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.cosh() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_cosh_", "path": "generated/torch._foreach_cosh_", "type": "Torch", "text": "torch._foreach_cosh_  \ntorch._foreach_cosh_(self: List[Tensor]) \u2192 None  \nApply torch.cosh() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_erf", "path": "generated/torch._foreach_erf", "type": "Torch", "text": "torch._foreach_erf  \ntorch._foreach_erf(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.erf() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_erf_", "path": "generated/torch._foreach_erf_", "type": "Torch", "text": "torch._foreach_erf_  \ntorch._foreach_erf_(self: List[Tensor]) \u2192 None  \nApply torch.erf() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_erfc", "path": "generated/torch._foreach_erfc", "type": "Torch", "text": "torch._foreach_erfc  \ntorch._foreach_erfc(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.erfc() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_erfc_", "path": "generated/torch._foreach_erfc_", "type": "Torch", "text": "torch._foreach_erfc_  \ntorch._foreach_erfc_(self: List[Tensor]) \u2192 None  \nApply torch.erfc() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_exp", "path": "generated/torch._foreach_exp", "type": "Torch", "text": "torch._foreach_exp  \ntorch._foreach_exp(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.exp() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_exp_", "path": "generated/torch._foreach_exp_", "type": "Torch", "text": "torch._foreach_exp_  \ntorch._foreach_exp_(self: List[Tensor]) \u2192 None  \nApply torch.exp() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_expm1", "path": "generated/torch._foreach_expm1", "type": "Torch", "text": "torch._foreach_expm1  \ntorch._foreach_expm1(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.expm1() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_expm1_", "path": "generated/torch._foreach_expm1_", "type": "Torch", "text": "torch._foreach_expm1_  \ntorch._foreach_expm1_(self: List[Tensor]) \u2192 None  \nApply torch.expm1() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_floor", "path": "generated/torch._foreach_floor", "type": "Torch", "text": "torch._foreach_floor  \ntorch._foreach_floor(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.floor() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_floor_", "path": "generated/torch._foreach_floor_", "type": "Torch", "text": "torch._foreach_floor_  \ntorch._foreach_floor_(self: List[Tensor]) \u2192 None  \nApply torch.floor() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_frac", "path": "generated/torch._foreach_frac", "type": "Torch", "text": "torch._foreach_frac  \ntorch._foreach_frac(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.frac() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_frac_", "path": "generated/torch._foreach_frac_", "type": "Torch", "text": "torch._foreach_frac_  \ntorch._foreach_frac_(self: List[Tensor]) \u2192 None  \nApply torch.frac() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_lgamma", "path": "generated/torch._foreach_lgamma", "type": "Torch", "text": "torch._foreach_lgamma  \ntorch._foreach_lgamma(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.lgamma() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_lgamma_", "path": "generated/torch._foreach_lgamma_", "type": "Torch", "text": "torch._foreach_lgamma_  \ntorch._foreach_lgamma_(self: List[Tensor]) \u2192 None  \nApply torch.lgamma() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log", "path": "generated/torch._foreach_log", "type": "Torch", "text": "torch._foreach_log  \ntorch._foreach_log(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.log() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log10", "path": "generated/torch._foreach_log10", "type": "Torch", "text": "torch._foreach_log10  \ntorch._foreach_log10(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.log10() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log10_", "path": "generated/torch._foreach_log10_", "type": "Torch", "text": "torch._foreach_log10_  \ntorch._foreach_log10_(self: List[Tensor]) \u2192 None  \nApply torch.log10() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log1p", "path": "generated/torch._foreach_log1p", "type": "Torch", "text": "torch._foreach_log1p  \ntorch._foreach_log1p(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.log1p() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log1p_", "path": "generated/torch._foreach_log1p_", "type": "Torch", "text": "torch._foreach_log1p_  \ntorch._foreach_log1p_(self: List[Tensor]) \u2192 None  \nApply torch.log1p() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log2", "path": "generated/torch._foreach_log2", "type": "Torch", "text": "torch._foreach_log2  \ntorch._foreach_log2(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.log2() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log2_", "path": "generated/torch._foreach_log2_", "type": "Torch", "text": "torch._foreach_log2_  \ntorch._foreach_log2_(self: List[Tensor]) \u2192 None  \nApply torch.log2() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_log_", "path": "generated/torch._foreach_log_", "type": "Torch", "text": "torch._foreach_log_  \ntorch._foreach_log_(self: List[Tensor]) \u2192 None  \nApply torch.log() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_neg", "path": "generated/torch._foreach_neg", "type": "Torch", "text": "torch._foreach_neg  \ntorch._foreach_neg(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.neg() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_neg_", "path": "generated/torch._foreach_neg_", "type": "Torch", "text": "torch._foreach_neg_  \ntorch._foreach_neg_(self: List[Tensor]) \u2192 None  \nApply torch.neg() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_reciprocal", "path": "generated/torch._foreach_reciprocal", "type": "Torch", "text": "torch._foreach_reciprocal  \ntorch._foreach_reciprocal(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.reciprocal() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_reciprocal_", "path": "generated/torch._foreach_reciprocal_", "type": "Torch", "text": "torch._foreach_reciprocal_  \ntorch._foreach_reciprocal_(self: List[Tensor]) \u2192 None  \nApply torch.reciprocal() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_round", "path": "generated/torch._foreach_round", "type": "Torch", "text": "torch._foreach_round  \ntorch._foreach_round(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.round() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_round_", "path": "generated/torch._foreach_round_", "type": "Torch", "text": "torch._foreach_round_  \ntorch._foreach_round_(self: List[Tensor]) \u2192 None  \nApply torch.round() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sigmoid", "path": "generated/torch._foreach_sigmoid", "type": "Torch", "text": "torch._foreach_sigmoid  \ntorch._foreach_sigmoid(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.sigmoid() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sigmoid_", "path": "generated/torch._foreach_sigmoid_", "type": "Torch", "text": "torch._foreach_sigmoid_  \ntorch._foreach_sigmoid_(self: List[Tensor]) \u2192 None  \nApply torch.sigmoid() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sin", "path": "generated/torch._foreach_sin", "type": "Torch", "text": "torch._foreach_sin  \ntorch._foreach_sin(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.sin() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sin_", "path": "generated/torch._foreach_sin_", "type": "Torch", "text": "torch._foreach_sin_  \ntorch._foreach_sin_(self: List[Tensor]) \u2192 None  \nApply torch.sin() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sinh", "path": "generated/torch._foreach_sinh", "type": "Torch", "text": "torch._foreach_sinh  \ntorch._foreach_sinh(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.sinh() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sinh_", "path": "generated/torch._foreach_sinh_", "type": "Torch", "text": "torch._foreach_sinh_  \ntorch._foreach_sinh_(self: List[Tensor]) \u2192 None  \nApply torch.sinh() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sqrt", "path": "generated/torch._foreach_sqrt", "type": "Torch", "text": "torch._foreach_sqrt  \ntorch._foreach_sqrt(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.sqrt() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_sqrt_", "path": "generated/torch._foreach_sqrt_", "type": "Torch", "text": "torch._foreach_sqrt_  \ntorch._foreach_sqrt_(self: List[Tensor]) \u2192 None  \nApply torch.sqrt() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_tan", "path": "generated/torch._foreach_tan", "type": "Torch", "text": "torch._foreach_tan  \ntorch._foreach_tan(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.tan() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_tan_", "path": "generated/torch._foreach_tan_", "type": "Torch", "text": "torch._foreach_tan_  \ntorch._foreach_tan_(self: List[Tensor]) \u2192 None  \nApply torch.tan() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_trunc", "path": "generated/torch._foreach_trunc", "type": "Torch", "text": "torch._foreach_trunc  \ntorch._foreach_trunc(self: List[Tensor]) \u2192 List[Tensor]  \nApply torch.trunc() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_trunc_", "path": "generated/torch._foreach_trunc_", "type": "Torch", "text": "torch._foreach_trunc_  \ntorch._foreach_trunc_(self: List[Tensor]) \u2192 None  \nApply torch.trunc() to each Tensor of the input list. \n\n"}, {"name": "torch._foreach_zero_", "path": "generated/torch._foreach_zero_", "type": "Torch", "text": "torch._foreach_zero_  \ntorch._foreach_zero_(self: List[Tensor]) \u2192 None  \nApply torch.zero() to each Tensor of the input list. \n\n"}, {"name": "torch._logging", "path": "logging", "type": "Miscellaneous", "text": "torch._logging PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component\u2019s log messages can be completely disabled, while another component\u2019s log messages can be set to maximum verbosity.  Warning This feature is a prototype and may have compatibility breaking changes in the future.   Warning This feature has not been expanded to control the log messages of all components in PyTorch yet.  There are two ways to configure the logging system: through the environment variable TORCH_LOGS or the python API torch._logging.set_logs.  \n\nset_logs\n Sets the log level for individual components and toggles individual log artifact types.   The environment variable TORCH_LOGS is a comma-separated list of [+-]<component> pairs, where <component> is a component specified below. The + prefix will decrease the log level of the component, displaying more log messages while the - prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in TORCH_LOGS. In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with + or - will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be off_by_default. This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the TORCH_LOGS environment variable (see torch._logging.set_logs for the python API):  Components:\n\n \nall \n\nSpecial component which configures the default log level of all components. Default: logging.WARN  \ndynamo \n\nThe log level for the TorchDynamo component. Default: logging.WARN  \naot \n\nThe log level for the AOTAutograd component. Default: logging.WARN  \ninductor \n\nThe log level for the TorchInductor component. Default: logging.WARN  \nyour.custom.module \n\nThe log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default: logging.WARN    Artifacts:\n\n \nbytecode \n\nWhether to emit the original and generated bytecode from TorchDynamo. Default: False  \naot_graphs \n\nWhether to emit the graphs generated by AOTAutograd. Default: False  \naot_joint_graph \n\nWhether to emit the joint forward-backward graph generated by AOTAutograd. Default: False  \nddp_graphs \n\nWhether to emit graphs generated by DDPOptimizer. Default: False  \ngraph \n\nWhether to emit the graph captured by TorchDynamo in tabular format. Default: False  \ngraph_code \n\nWhether to emit the python source of the graph captured by TorchDynamo. Default: False  \ngraph_breaks \n\nWhether to emit a message when a unique graph break is encountered during TorchDynamo tracing. Default: False  \nguards \n\nWhether to emit the guards generated by TorchDynamo for each compiled function. Default: False  \nrecompiles \n\nWhether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: False  \noutput_code \n\nWhether to emit the TorchInductor output code. Default: False  \nschedule \n\nWhether to emit the TorchInductor schedule. Default: False    Examples:\n\nTORCH_LOGS=\"+dynamo,aot\" will set the log level of TorchDynamo to logging.DEBUG and AOT to logging.INFO TORCH_LOGS=\"-dynamo,+inductor\" will set the log level of TorchDynamo to logging.ERROR and TorchInductor to logging.DEBUG TORCH_LOGS=\"aot_graphs\" will enable the aot_graphs artifact TORCH_LOGS=\"+dynamo,schedule\" will enable set the log level of TorchDynamo to logging.DEBUG and enable the schedule artifact TORCH_LOGS=\"+some.random.module,schedule\" will set the log level of some.random.module to logging.DEBUG and enable the schedule artifact  \n"}, {"name": "torch._logging.set_logs()", "path": "generated/torch._logging.set_logs#torch._logging.set_logs", "type": "Miscellaneous", "text": " \ntorch._logging.set_logs(*, all=None, dynamo=None, aot=None, dynamic=None, inductor=None, distributed=None, onnx=None, bytecode=False, aot_graphs=False, aot_joint_graph=False, ddp_graphs=False, graph=False, graph_code=False, graph_breaks=False, graph_sizes=False, guards=False, recompiles=False, trace_source=False, trace_call=False, output_code=False, schedule=False, perf_hints=False, onnx_diagnostics=False, modules=None) [source]\n \nSets the log level for individual components and toggles individual log artifact types.  Warning This feature is a prototype and may have compatibility breaking changes in the future.   Note The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.  A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:  logging.CRITICAL logging.ERROR logging.WARNING logging.INFO logging.DEBUG logging.NOTSET  See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.  Keyword Arguments \n \nall (Optional[int]) \u2013 The default log level for all components. Default: logging.WARN\n \ndynamo (Optional[int]) \u2013 The log level for the TorchDynamo component. Default: logging.WARN\n \naot (Optional[int]) \u2013 The log level for the AOTAutograd component. Default: logging.WARN\n \ninductor (Optional[int]) \u2013 The log level for the TorchInductor component. Default: logging.WARN\n \ndynamic (Optional[int]) \u2013 The log level for dynamic shapes. Default: logging.WARN\n \ndistributed (Optional[int]) \u2013 Whether to log communication operations and other debug info from pytorch distributed components. Default: logging.WARN\n \nonnx (Optional[int]) \u2013 The log level for the ONNX exporter component. Default: logging.WARN\n \nbytecode (bool) \u2013 Whether to emit the original and generated bytecode from TorchDynamo. Default: False\n \naot_graphs (bool) \u2013 Whether to emit the graphs generated by AOTAutograd. Default: False\n \naot_joint_graph (bool) \u2013 Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: False\n \nddp_graphs (bool) \u2013 Whether to emit graphs generated by DDPOptimizer. Default: False\n \ngraph (bool) \u2013 Whether to emit the graph captured by TorchDynamo in tabular format. Default: False\n \ngraph_code (bool) \u2013 Whether to emit the python source of the graph captured by TorchDynamo. Default: False\n \ngraph_breaks (bool) \u2013 Whether to emit the graph breaks encountered by TorchDynamo. Default: False\n \ngraph_sizes (bool) \u2013 Whether to emit tensor sizes of the graph captured by TorchDynamo. Default: False\n \nguards (bool) \u2013 Whether to emit the guards generated by TorchDynamo for each compiled function. Default: False\n \nrecompiles (bool) \u2013 Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: False\n \ntrace_source (bool) \u2013 Whether to emit when TorchDynamo begins tracing a new line. Default: False\n \ntrace_call (bool) \u2013 Whether to emit detailed line location when TorchDynamo creates an FX node corresponding to function call. Python 3.11+ only. Default: False\n \noutput_code (bool) \u2013 Whether to emit the TorchInductor output code. Default: False\n \nschedule (bool) \u2013 Whether to emit the TorchInductor schedule. Default: False\n \nperf_hints (bool) \u2013 Whether to emit the TorchInductor perf hints. Default: False\n \nonnx_diagnostics (bool) \u2013 Whether to emit the ONNX exporter diagnostics in logging. Default: False\n \nmodules (dict) \u2013 This argument provides an alternate way to specify the above log component and artifact settings, in the format of a keyword args dictionary given as a single argument. There are two cases where this is useful (1) if a new log component or artifact has been registered but a keyword argument for it has not been added to this function and (2) if the log level for an unregistered module needs to be set. This can be done by providing the fully-qualified module name as the key, with the log level as the value. Default: None\n    Example: >>> import logging\n\n# The following changes the \"dynamo\" component to emit DEBUG-level\n# logs, and to emit \"graph_code\" artifacts.\n\n>>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n# The following enables the logs for a different module\n\n>>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n \n"}, {"name": "torch._logging.torch._logging.set_logs", "path": "generated/torch._logging.set_logs", "type": "Miscellaneous", "text": "torch._logging.set_logs  \ntorch._logging.set_logs(*, all=None, dynamo=None, aot=None, dynamic=None, inductor=None, distributed=None, onnx=None, bytecode=False, aot_graphs=False, aot_joint_graph=False, ddp_graphs=False, graph=False, graph_code=False, graph_breaks=False, graph_sizes=False, guards=False, recompiles=False, trace_source=False, trace_call=False, output_code=False, schedule=False, perf_hints=False, onnx_diagnostics=False, modules=None) [source]\n \nSets the log level for individual components and toggles individual log artifact types.  Warning This feature is a prototype and may have compatibility breaking changes in the future.   Note The TORCH_LOGS environment variable has complete precedence over this function, so if it was set, this function does nothing.  A component is a set of related features in PyTorch. All of the log messages emitted from a given component have their own log levels. If the log level of a particular message has priority greater than or equal to its component\u2019s log level setting, it is emitted. Otherwise, it is supressed. This allows you to, for instance, silence large groups of log messages that are not relevant to you and increase verbosity of logs for components that are relevant. The expected log level values, ordered from highest to lowest priority, are:  logging.CRITICAL logging.ERROR logging.WARNING logging.INFO logging.DEBUG logging.NOTSET  See documentation for the Python logging module for more information on log levels: https://docs.python.org/3/library/logging.html#logging-levels An artifact is a particular type of log message. Each artifact is assigned to a parent component. A component can emit many different kinds of artifacts. In general, an artifact is emitted if either its corresponding setting in the argument list below is turned on or if its parent component is set to a log level less than or equal to the log level of the artifact.  Keyword Arguments \n \nall (Optional[int]) \u2013 The default log level for all components. Default: logging.WARN\n \ndynamo (Optional[int]) \u2013 The log level for the TorchDynamo component. Default: logging.WARN\n \naot (Optional[int]) \u2013 The log level for the AOTAutograd component. Default: logging.WARN\n \ninductor (Optional[int]) \u2013 The log level for the TorchInductor component. Default: logging.WARN\n \ndynamic (Optional[int]) \u2013 The log level for dynamic shapes. Default: logging.WARN\n \ndistributed (Optional[int]) \u2013 Whether to log communication operations and other debug info from pytorch distributed components. Default: logging.WARN\n \nonnx (Optional[int]) \u2013 The log level for the ONNX exporter component. Default: logging.WARN\n \nbytecode (bool) \u2013 Whether to emit the original and generated bytecode from TorchDynamo. Default: False\n \naot_graphs (bool) \u2013 Whether to emit the graphs generated by AOTAutograd. Default: False\n \naot_joint_graph (bool) \u2013 Whether to emit the joint forward-backward graph generated by AOTAutograd. Default: False\n \nddp_graphs (bool) \u2013 Whether to emit graphs generated by DDPOptimizer. Default: False\n \ngraph (bool) \u2013 Whether to emit the graph captured by TorchDynamo in tabular format. Default: False\n \ngraph_code (bool) \u2013 Whether to emit the python source of the graph captured by TorchDynamo. Default: False\n \ngraph_breaks (bool) \u2013 Whether to emit the graph breaks encountered by TorchDynamo. Default: False\n \ngraph_sizes (bool) \u2013 Whether to emit tensor sizes of the graph captured by TorchDynamo. Default: False\n \nguards (bool) \u2013 Whether to emit the guards generated by TorchDynamo for each compiled function. Default: False\n \nrecompiles (bool) \u2013 Whether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default: False\n \ntrace_source (bool) \u2013 Whether to emit when TorchDynamo begins tracing a new line. Default: False\n \ntrace_call (bool) \u2013 Whether to emit detailed line location when TorchDynamo creates an FX node corresponding to function call. Python 3.11+ only. Default: False\n \noutput_code (bool) \u2013 Whether to emit the TorchInductor output code. Default: False\n \nschedule (bool) \u2013 Whether to emit the TorchInductor schedule. Default: False\n \nperf_hints (bool) \u2013 Whether to emit the TorchInductor perf hints. Default: False\n \nonnx_diagnostics (bool) \u2013 Whether to emit the ONNX exporter diagnostics in logging. Default: False\n \nmodules (dict) \u2013 This argument provides an alternate way to specify the above log component and artifact settings, in the format of a keyword args dictionary given as a single argument. There are two cases where this is useful (1) if a new log component or artifact has been registered but a keyword argument for it has not been added to this function and (2) if the log level for an unregistered module needs to be set. This can be done by providing the fully-qualified module name as the key, with the log level as the value. Default: None\n    Example: >>> import logging\n\n# The following changes the \"dynamo\" component to emit DEBUG-level\n# logs, and to emit \"graph_code\" artifacts.\n\n>>> torch._logging.set_logs(dynamo=logging.DEBUG, graph_code=True)\n\n# The following enables the logs for a different module\n\n>>> torch._logging.set_logs(modules={\"unregistered.module.name\": logging.DEBUG})\n \n\n"}, {"name": "torch.abs", "path": "generated/torch.abs", "type": "Torch", "text": "torch.abs  \ntorch.abs(input, *, out=None) \u2192 Tensor  \nComputes the absolute value of each element in input.  outi=\u2223inputi\u2223\\text{out}_{i} = |\\text{input}_{i}| \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n \n\n"}, {"name": "torch.absolute", "path": "generated/torch.absolute", "type": "Torch", "text": "torch.absolute  \ntorch.absolute(input, *, out=None) \u2192 Tensor  \nAlias for torch.abs() \n\n"}, {"name": "torch.acos", "path": "generated/torch.acos", "type": "Torch", "text": "torch.acos  \ntorch.acos(input, *, out=None) \u2192 Tensor  \nComputes the inverse cosine of each element in input.  outi=cos\u2061\u22121(inputi)\\text{out}_{i} = \\cos^{-1}(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n \n\n"}, {"name": "torch.acosh", "path": "generated/torch.acosh", "type": "Torch", "text": "torch.acosh  \ntorch.acosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic cosine of the elements of input.  outi=cosh\u2061\u22121(inputi)\\text{out}_{i} = \\cosh^{-1}(\\text{input}_{i}) \n\n Note The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\n \n\n"}, {"name": "torch.add", "path": "generated/torch.add", "type": "Torch", "text": "torch.add  \ntorch.add(input, other, *, alpha=1, out=None) \u2192 Tensor  \nAdds other, scaled by alpha, to input.  outi=inputi+alpha\u00d7otheri\\text{{out}}_i = \\text{{input}}_i + \\text{{alpha}} \\times \\text{{other}}_i \n\nSupports broadcasting to a common shape, type promotion, and integer, float, and complex inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor or Number) \u2013 the tensor or number to add to input.   Keyword Arguments \n \nalpha (Number) \u2013 the multiplier for other. \nout (Tensor, optional) \u2013 the output tensor.    Examples: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n\n>>> b = torch.randn(4)\n>>> b\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> c = torch.randn(4, 1)\n>>> c\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(b, c, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n \n\n"}, {"name": "torch.addbmm", "path": "generated/torch.addbmm", "type": "Torch", "text": "torch.addbmm  \ntorch.addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor.  out=\u03b2 input+\u03b1(\u2211i=0b\u22121batch1i@batch2i)out = \\beta\\ \\text{input} + \\alpha\\ (\\sum_{i=0}^{b-1} \\text{batch1}_i \\mathbin{@} \\text{batch2}_i) \n\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Parameters \n \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta) \ninput (Tensor) \u2013 matrix to be added \nalpha (Number, optional) \u2013 multiplier for batch1 @ batch2 (\u03b1\\alpha) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n \n\n"}, {"name": "torch.addcdiv", "path": "generated/torch.addcdiv", "type": "Torch", "text": "torch.addcdiv  \ntorch.addcdiv(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise division of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.  Warning Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.   outi=inputi+value\u00d7tensor1itensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i} \n\nThe shapes of input, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the numerator tensor \ntensor2 (Tensor) \u2013 the denominator tensor   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n \n\n"}, {"name": "torch.addcmul", "path": "generated/torch.addcmul", "type": "Torch", "text": "torch.addcmul  \ntorch.addcmul(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise multiplication of tensor1 by tensor2, multiplies the result by the scalar value and adds it to input.  outi=inputi+value\u00d7tensor1i\u00d7tensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\text{tensor1}_i \\times \\text{tensor2}_i \n\nThe shapes of tensor, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the tensor to be multiplied \ntensor2 (Tensor) \u2013 the tensor to be multiplied   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1.\u2217tensor2tensor1 .* tensor2\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n \n\n"}, {"name": "torch.addmm", "path": "generated/torch.addmm", "type": "Torch", "text": "torch.addmm  \ntorch.addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result. If mat1 is a (n\u00d7m)(n \\times m) tensor, mat2 is a (m\u00d7p)(m \\times p) tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p) tensor and out will be a (n\u00d7p)(n \\times p) tensor. alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(mat1i@mat2i)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i) \n\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operation has support for arguments with sparse layouts. If input is sparse the result will have the same layout and if out is provided it must have the same layout as input.  Warning Sparse support is a beta feature and some layout(s)/dtype/device combinations may not be supported, or may not have autograd support. If you notice missing functionality please open a feature request.  This operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nmat1 (Tensor) \u2013 the first matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second matrix to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2 (\u03b1\\alpha) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n \n\n"}, {"name": "torch.addmv", "path": "generated/torch.addmv", "type": "Torch", "text": "torch.addmv  \ntorch.addmv(input, mat, vec, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result. If mat is a (n\u00d7m)(n \\times m) tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n. alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.  out=\u03b2 input+\u03b1(mat@vec)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat} \\mathbin{@} \\text{vec}) \n\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.  Parameters \n \ninput (Tensor) \u2013 vector to be added \nmat (Tensor) \u2013 matrix to be matrix multiplied \nvec (Tensor) \u2013 vector to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta) \nalpha (Number, optional) \u2013 multiplier for mat@vecmat @ vec (\u03b1\\alpha) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n \n\n"}, {"name": "torch.addr", "path": "generated/torch.addr", "type": "Torch", "text": "torch.addr  \ntorch.addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms the outer-product of vectors vec1 and vec2 and adds it to the matrix input. Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(vec1\u2297vec2)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{vec1} \\otimes \\text{vec2}) \n\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m) and out will be a matrix of size (n\u00d7m)(n \\times m).  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nvec1 (Tensor) \u2013 the first vector of the outer product \nvec2 (Tensor) \u2013 the second vector of the outer product   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta) \nalpha (Number, optional) \u2013 multiplier for vec1\u2297vec2\\text{vec1} \\otimes \\text{vec2} (\u03b1\\alpha) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n \n\n"}, {"name": "torch.adjoint", "path": "generated/torch.adjoint", "type": "Torch", "text": "torch.adjoint  \ntorch.adjoint(Tensor) \u2192 Tensor  \nReturns a view of the tensor conjugated and with the last two dimensions transposed. x.adjoint() is equivalent to x.transpose(-2, -1).conj() for complex tensors and to x.transpose(-2, -1) for real tensors.  Example::\n\n>>> x = torch.arange(4, dtype=torch.float)\n>>> A = torch.complex(x, x).reshape(2, 2)\n>>> A\ntensor([[0.+0.j, 1.+1.j],\n        [2.+2.j, 3.+3.j]])\n>>> A.adjoint()\ntensor([[0.-0.j, 2.-2.j],\n        [1.-1.j, 3.-3.j]])\n>>> (A.adjoint() == A.mH).all()\ntensor(True)\n   \n\n"}, {"name": "torch.all", "path": "generated/torch.all", "type": "Torch", "text": "torch.all  \ntorch.all(input) \u2192 Tensor  \nTests if all elements in input evaluate to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)\n   torch.all(input, dim, keepdim=False, *, out=None) \u2192 Tensor\n\n For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)\n \n\n"}, {"name": "torch.allclose", "path": "generated/torch.allclose", "type": "Torch", "text": "torch.allclose  \ntorch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 bool  \nThis function checks if input and other satisfy the condition:  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert \n\nelementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose  Parameters \n \ninput (Tensor) \u2013 first tensor to compare \nother (Tensor) \u2013 second tensor to compare \natol (float, optional) \u2013 absolute tolerance. Default: 1e-08 \nrtol (float, optional) \u2013 relative tolerance. Default: 1e-05 \nequal_nan (bool, optional) \u2013 if True, then two NaN s will be considered equal. Default: False\n    Example: >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n \n\n"}, {"name": "torch.amax", "path": "generated/torch.amax", "type": "Torch", "text": "torch.amax  \ntorch.amax(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the maximum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\n \n\n"}, {"name": "torch.amin", "path": "generated/torch.amin", "type": "Torch", "text": "torch.amin  \ntorch.amin(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the minimum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\n \n\n"}, {"name": "torch.aminmax", "path": "generated/torch.aminmax", "type": "Torch", "text": "torch.aminmax  \ntorch.aminmax(input, *, dim=None, keepdim=False, out=None) -> (Tensor min, Tensor max)  \nComputes the minimum and maximum values of the input tensor.  Parameters \ninput (Tensor) \u2013 The input tensor  Keyword Arguments \n \ndim (Optional[int]) \u2013 The dimension along which to compute the values. If None, computes the values over the entire input tensor. Default is None. \nkeepdim (bool) \u2013 If True, the reduced dimensions will be kept in the output tensor as dimensions with size 1 for broadcasting, otherwise they will be removed, as if calling (torch.squeeze()). Default is False. \nout (Optional[Tuple[Tensor, Tensor]]) \u2013 Optional tensors on which to write the result. Must have the same shape and dtype as the expected output. Default is None.   Returns \nA named tuple (min, max) containing the minimum and maximum values.  Raises \nRuntimeError \u2013 If any of the dimensions to compute the values over has size 0.    Note NaN values are propagated to the output if at least one value is NaN.   See also torch.amin() computes just the minimum value torch.amax() computes just the maximum value  Example: >>> torch.aminmax(torch.tensor([1, -3, 5]))\ntorch.return_types.aminmax(\nmin=tensor(-3),\nmax=tensor(5))\n\n>>> # aminmax propagates NaNs\n>>> torch.aminmax(torch.tensor([1, -3, 5, torch.nan]))\ntorch.return_types.aminmax(\nmin=tensor(nan),\nmax=tensor(nan))\n\n>>> t = torch.arange(10).view(2, 5)\n>>> t\ntensor([[0, 1, 2, 3, 4],\n        [5, 6, 7, 8, 9]])\n>>> t.aminmax(dim=0, keepdim=True)\ntorch.return_types.aminmax(\nmin=tensor([[0, 1, 2, 3, 4]]),\nmax=tensor([[5, 6, 7, 8, 9]]))\n \n\n"}, {"name": "torch.angle", "path": "generated/torch.angle", "type": "Torch", "text": "torch.angle  \ntorch.angle(input, *, out=None) \u2192 Tensor  \nComputes the element-wise angle (in radians) of the given input tensor.  outi=angle(inputi)\\text{out}_{i} = angle(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Note Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.  Example: >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n \n\n"}, {"name": "torch.any", "path": "generated/torch.any", "type": "Torch", "text": "torch.any  \ntorch.any(input) \u2192 Tensor  \nTests if any element in input evaluates to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)\n   torch.any(input, dim, keepdim=False, *, out=None) \u2192 Tensor\n\n For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 2) < 0\n>>> a\ntensor([[ True,  True],\n        [False,  True],\n        [ True,  True],\n        [False, False]])\n>>> torch.any(a, 1)\ntensor([ True,  True,  True, False])\n>>> torch.any(a, 0)\ntensor([True, True])\n \n\n"}, {"name": "torch.ao.nn.intrinsic.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.bnrelu2d#torch.ao.nn.intrinsic.BNReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.BNReLU2d(batch_norm, relu) [source]\n \nThis is a sequential container which calls the BatchNorm 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.bnrelu3d#torch.ao.nn.intrinsic.BNReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.BNReLU3d(batch_norm, relu) [source]\n \nThis is a sequential container which calls the BatchNorm 3d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.convbn1d#torch.ao.nn.intrinsic.ConvBn1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBn1d(conv, bn) [source]\n \nThis is a sequential container which calls the Conv 1d and Batch Norm 1d modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.convbn2d#torch.ao.nn.intrinsic.ConvBn2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBn2d(conv, bn) [source]\n \nThis is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.convbn3d#torch.ao.nn.intrinsic.ConvBn3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBn3d(conv, bn) [source]\n \nThis is a sequential container which calls the Conv 3d and Batch Norm 3d modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu1d#torch.ao.nn.intrinsic.ConvBnReLU1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBnReLU1d(conv, bn, relu) [source]\n \nThis is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu2d#torch.ao.nn.intrinsic.ConvBnReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBnReLU2d(conv, bn, relu) [source]\n \nThis is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.convbnrelu3d#torch.ao.nn.intrinsic.ConvBnReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvBnReLU3d(conv, bn, relu) [source]\n \nThis is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.convrelu1d#torch.ao.nn.intrinsic.ConvReLU1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvReLU1d(conv, relu) [source]\n \nThis is a sequential container which calls the Conv1d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.convrelu2d#torch.ao.nn.intrinsic.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvReLU2d(conv, relu) [source]\n \nThis is a sequential container which calls the Conv2d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.convrelu3d#torch.ao.nn.intrinsic.ConvReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.ConvReLU3d(conv, relu) [source]\n \nThis is a sequential container which calls the Conv3d and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.linearrelu#torch.ao.nn.intrinsic.LinearReLU", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.LinearReLU(linear, relu) [source]\n \nThis is a sequential container which calls the Linear and ReLU modules. During quantization this will be replaced with the corresponding fused module. \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn1d#torch.ao.nn.intrinsic.qat.ConvBn1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBn1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBn1d module is a module fused from Conv1d and BatchNorm1d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d. Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default.  Variables \n \nfreeze_bn \u2013  \nweight_fake_quant \u2013 fake quant module for weight    \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn2d#torch.ao.nn.intrinsic.qat.ConvBn2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBn2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \n \nfreeze_bn \u2013  \nweight_fake_quant \u2013 fake quant module for weight    \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBn3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbn3d#torch.ao.nn.intrinsic.qat.ConvBn3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBn3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBn3d module is a module fused from Conv3d and BatchNorm3d, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d. Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.  Variables \n \nfreeze_bn \u2013  \nweight_fake_quant \u2013 fake quant module for weight    \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu1d#torch.ao.nn.intrinsic.qat.ConvBnReLU1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBnReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBnReLU1d module is a module fused from Conv1d, BatchNorm1d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv1d and torch.nn.BatchNorm1d and torch.nn.ReLU. Similar to torch.nn.Conv1d, with FakeQuantize modules initialized to default.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu2d#torch.ao.nn.intrinsic.qat.ConvBnReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBnReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convbnrelu3d#torch.ao.nn.intrinsic.qat.ConvBnReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvBnReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=None, padding_mode='zeros', eps=1e-05, momentum=0.1, freeze_bn=False, qconfig=None) [source]\n \nA ConvBnReLU3d module is a module fused from Conv3d, BatchNorm3d and ReLU, attached with FakeQuantize modules for weight, used in quantization aware training. We combined the interface of torch.nn.Conv3d and torch.nn.BatchNorm3d and torch.nn.ReLU. Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu2d#torch.ao.nn.intrinsic.qat.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None) [source]\n \nA ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. We combined the interface of Conv2d and BatchNorm2d.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.intrinsic.qat.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.qat.convrelu3d#torch.ao.nn.intrinsic.qat.ConvReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None) [source]\n \nA ConvReLU3d module is a fused module of Conv3d and ReLU, attached with FakeQuantize modules for weight for quantization aware training. We combined the interface of Conv3d and BatchNorm3d.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.intrinsic.qat.freeze_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.freeze_bn_stats#torch.ao.nn.intrinsic.qat.freeze_bn_stats", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.freeze_bn_stats(mod) [source]\n\n"}, {"name": "torch.ao.nn.intrinsic.qat.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.qat.linearrelu#torch.ao.nn.intrinsic.qat.LinearReLU", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.LinearReLU(in_features, out_features, bias=True, qconfig=None) [source]\n \nA LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for weight, used in quantization aware training. We adopt the same interface as torch.nn.Linear. Similar to torch.ao.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.  Variables \nweight (torch.Tensor) \u2013 fake quant module for weight   Examples: >>> m = nn.qat.LinearReLU(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.ao.nn.intrinsic.qat.update_bn_stats", "path": "generated/torch.ao.nn.intrinsic.qat.update_bn_stats#torch.ao.nn.intrinsic.qat.update_bn_stats", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.qat.update_bn_stats(mod) [source]\n\n"}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu2d#torch.ao.nn.intrinsic.quantized.BNReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.BNReLU2d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None) [source]\n \nA BNReLU2d module is a fused module of BatchNorm2d and ReLU We adopt the same interface as torch.ao.nn.quantized.BatchNorm2d.  Variables \ntorch.ao.nn.quantized.BatchNorm2d (Same as) \u2013    \n"}, {"name": "torch.ao.nn.intrinsic.quantized.BNReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.bnrelu3d#torch.ao.nn.intrinsic.quantized.BNReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.BNReLU3d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None) [source]\n \nA BNReLU3d module is a fused module of BatchNorm3d and ReLU We adopt the same interface as torch.ao.nn.quantized.BatchNorm3d.  Variables \ntorch.ao.nn.quantized.BatchNorm3d (Same as) \u2013    \n"}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU1d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu1d#torch.ao.nn.intrinsic.quantized.ConvReLU1d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.ConvReLU1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nA ConvReLU1d module is a fused module of Conv1d and ReLU We adopt the same interface as torch.ao.nn.quantized.Conv1d.  Variables \ntorch.ao.nn.quantized.Conv1d (Same as) \u2013    \n"}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU2d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu2d#torch.ao.nn.intrinsic.quantized.ConvReLU2d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.ConvReLU2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nA ConvReLU2d module is a fused module of Conv2d and ReLU We adopt the same interface as torch.ao.nn.quantized.Conv2d.  Variables \ntorch.ao.nn.quantized.Conv2d (Same as) \u2013    \n"}, {"name": "torch.ao.nn.intrinsic.quantized.ConvReLU3d", "path": "generated/torch.ao.nn.intrinsic.quantized.convrelu3d#torch.ao.nn.intrinsic.quantized.ConvReLU3d", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.ConvReLU3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nA ConvReLU3d module is a fused module of Conv3d and ReLU We adopt the same interface as torch.ao.nn.quantized.Conv3d. Attributes: Same as torch.ao.nn.quantized.Conv3d \n"}, {"name": "torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.dynamic.linearrelu#torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.dynamic.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8) [source]\n \nA LinearReLU module fused from Linear and ReLU modules that can be used for dynamic quantization. Supports both, FP16 and INT8 quantization. We adopt the same interface as torch.ao.nn.quantized.dynamic.Linear.  Variables \ntorch.ao.nn.quantized.dynamic.Linear (Same as) \u2013    Examples: >>> m = nn.intrinsic.quantized.dynamic.LinearReLU(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.ao.nn.intrinsic.quantized.LinearReLU", "path": "generated/torch.ao.nn.intrinsic.quantized.linearrelu#torch.ao.nn.intrinsic.quantized.LinearReLU", "type": "Quantization", "text": " \nclass torch.ao.nn.intrinsic.quantized.LinearReLU(in_features, out_features, bias=True, dtype=torch.qint8) [source]\n \nA LinearReLU module fused from Linear and ReLU modules We adopt the same interface as torch.ao.nn.quantized.Linear.  Variables \ntorch.ao.nn.quantized.Linear (Same as) \u2013    Examples: >>> m = nn.intrinsic.LinearReLU(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n \n"}, {"name": "torch.ao.nn.qat.Conv2d", "path": "generated/torch.ao.nn.qat.conv2d#torch.ao.nn.qat.Conv2d", "type": "Quantization", "text": " \nclass torch.ao.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None, device=None, dtype=None) [source]\n \nA Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.qat.Conv3d", "path": "generated/torch.ao.nn.qat.conv3d#torch.ao.nn.qat.Conv3d", "type": "Quantization", "text": " \nclass torch.ao.nn.qat.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None, device=None, dtype=None) [source]\n \nA Conv3d module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Conv3d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv3d#torch.nn.Conv3d for documentation. Similar to torch.nn.Conv3d, with FakeQuantize modules initialized to default.  Variables \nweight_fake_quant \u2013 fake quant module for weight   \n"}, {"name": "torch.ao.nn.qat.dynamic.Linear", "path": "generated/torch.ao.nn.qat.dynamic.linear#torch.ao.nn.qat.dynamic.Linear", "type": "Quantization", "text": " \nclass torch.ao.nn.qat.dynamic.Linear(in_features, out_features, bias=True, qconfig=None, device=None, dtype=None) [source]\n \nA linear module attached with FakeQuantize modules for weight, used for dynamic quantization aware training. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, with FakeQuantize modules initialized to default. \n"}, {"name": "torch.ao.nn.qat.Linear", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear", "type": "Quantization", "text": " \nclass torch.ao.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None, device=None, dtype=None) [source]\n \nA linear module attached with FakeQuantize modules for weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.  Variables \nweight (torch.Tensor) \u2013 fake quant module for weight    \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user \n \n"}, {"name": "torch.ao.nn.qat.Linear.from_float()", "path": "generated/torch.ao.nn.qat.linear#torch.ao.nn.qat.Linear.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.ao.quantization utilities or directly from user \n"}, {"name": "torch.ao.nn.quantizable.LSTM", "path": "generated/torch.ao.nn.quantizable.lstm#torch.ao.nn.quantizable.LSTM", "type": "Quantization", "text": " \nclass torch.ao.nn.quantizable.LSTM(input_size, hidden_size, num_layers=1, bias=True, batch_first=False, dropout=0.0, bidirectional=False, device=None, dtype=None) [source]\n \nA quantizable long short-term memory (LSTM). For the description and the argument types, please, refer to LSTM  Variables \nlayers \u2013 instances of the _LSTMLayer    Note To access the weights and biases, you need to access them per layer. See examples below.  Examples: >>> import torch.ao.nn.quantizable as nnqa\n>>> rnn = nnqa.LSTM(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> c0 = torch.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n>>> # To get the weights:\n>>> print(rnn.layers[0].weight_ih)\ntensor([[...]])\n>>> print(rnn.layers[0].weight_hh)\nAssertionError: There is no reverse path in the non-bidirectional layer\n \n"}, {"name": "torch.ao.nn.quantizable.MultiheadAttention", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention", "type": "Quantization", "text": " \nclass torch.ao.nn.quantizable.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None, batch_first=False, device=None, dtype=None) [source]\n \n \ndequantize() [source]\n \nUtility to convert the quantized MHA back to float. The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float. \n  \nforward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False) [source]\n \n Note::\n\nPlease, refer to forward() for more information    Parameters \n \nquery (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nvalue (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey_padding_mask (Optional[Tensor]) \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. \nneed_weights (bool) \u2013 output attn_output_weights. \nattn_mask (Optional[Tensor]) \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.   Return type \nTuple[Tensor, Optional[Tensor]]    Shape:\n\n Inputs: query: (L,N,E)(L, N, E) where L is the target sequence length, N is the batch size, E is the embedding dimension. (N,L,E)(N, L, E) if batch_first is True. key: (S,N,E)(S, N, E), where S is the source sequence length, N is the batch size, E is the embedding dimension. (N,S,E)(N, S, E) if batch_first is True. value: (S,N,E)(S, N, E) where S is the source sequence length, N is the batch size, E is the embedding dimension. (N,S,E)(N, S, E) if batch_first is True. key_padding_mask: (N,S)(N, S) where N is the batch size, S is the source sequence length. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask: 2D mask (L,S)(L, S) where L is the target sequence length, S is the source sequence length. 3D mask (N\u2217numheads,L,S)(N*num_heads, L, S) where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask. Default: False. average_attn_weights: If true, indicates that the returned attn_weights should be averaged across heads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an effect when need_weights=True.. Default: True (i.e. average weights across heads) Outputs: attn_output: (L,N,E)(L, N, E) where L is the target sequence length, N is the batch size, E is the embedding dimension. (N,L,E)(N, L, E) if batch_first is True. attn_output_weights: If average_attn_weights=True, returns attention weights averaged across heads of shape (N,L,S)(N, L, S), where N is the batch size, L is the target sequence length, S is the source sequence length. If average_attn_weights=False, returns attention weights per head of shape (N,numheads,L,S)(N, num_heads, L, S).    \n \n"}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.dequantize()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.dequantize", "type": "Quantization", "text": " \ndequantize() [source]\n \nUtility to convert the quantized MHA back to float. The motivation for this is that it is not trivial to conver the weights from the format that is used in the quantized version back to the float. \n"}, {"name": "torch.ao.nn.quantizable.MultiheadAttention.forward()", "path": "generated/torch.ao.nn.quantizable.multiheadattention#torch.ao.nn.quantizable.MultiheadAttention.forward", "type": "Quantization", "text": " \nforward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None, average_attn_weights=True, is_causal=False) [source]\n \n Note::\n\nPlease, refer to forward() for more information    Parameters \n \nquery (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nvalue (Tensor) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. \nkey_padding_mask (Optional[Tensor]) \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. \nneed_weights (bool) \u2013 output attn_output_weights. \nattn_mask (Optional[Tensor]) \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.   Return type \nTuple[Tensor, Optional[Tensor]]    Shape:\n\n Inputs: query: (L,N,E)(L, N, E) where L is the target sequence length, N is the batch size, E is the embedding dimension. (N,L,E)(N, L, E) if batch_first is True. key: (S,N,E)(S, N, E), where S is the source sequence length, N is the batch size, E is the embedding dimension. (N,S,E)(N, S, E) if batch_first is True. value: (S,N,E)(S, N, E) where S is the source sequence length, N is the batch size, E is the embedding dimension. (N,S,E)(N, S, E) if batch_first is True. key_padding_mask: (N,S)(N, S) where N is the batch size, S is the source sequence length. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask: 2D mask (L,S)(L, S) where L is the target sequence length, S is the source sequence length. 3D mask (N\u2217numheads,L,S)(N*num_heads, L, S) where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. is_causal: If specified, applies a causal mask as attention mask. Mutually exclusive with providing attn_mask. Default: False. average_attn_weights: If true, indicates that the returned attn_weights should be averaged across heads. Otherwise, attn_weights are provided separately per head. Note that this flag only has an effect when need_weights=True.. Default: True (i.e. average weights across heads) Outputs: attn_output: (L,N,E)(L, N, E) where L is the target sequence length, N is the batch size, E is the embedding dimension. (N,L,E)(N, L, E) if batch_first is True. attn_output_weights: If average_attn_weights=True, returns attention weights averaged across heads of shape (N,L,S)(N, L, S), where N is the batch size, L is the target sequence length, S is the source sequence length. If average_attn_weights=False, returns attention weights per head of shape (N,numheads,L,S)(N, num_heads, L, S).    \n"}, {"name": "torch.ao.nn.quantized.BatchNorm2d", "path": "generated/torch.ao.nn.quantized.batchnorm2d#torch.ao.nn.quantized.BatchNorm2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None) [source]\n \nThis is the quantized version of BatchNorm2d. \n"}, {"name": "torch.ao.nn.quantized.BatchNorm3d", "path": "generated/torch.ao.nn.quantized.batchnorm3d#torch.ao.nn.quantized.BatchNorm3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, device=None, dtype=None) [source]\n \nThis is the quantized version of BatchNorm3d. \n"}, {"name": "torch.ao.nn.quantized.Conv1d", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 1D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv1d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See Conv1d for other attributes. Examples: >>> m = nn.quantized.Conv1d(16, 33, 3, stride=2)\n>>> input = torch.randn(20, 16, 100)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,\n...                                     dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n \n"}, {"name": "torch.ao.nn.quantized.Conv1d.from_float()", "path": "generated/torch.ao.nn.quantized.conv1d#torch.ao.nn.quantized.Conv1d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n"}, {"name": "torch.ao.nn.quantized.Conv2d", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 2D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv2d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See Conv2d for other attributes. Examples: >>> # With square kernels and equal stride\n>>> m = nn.quantized.Conv2d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> # non-square kernels and unequal stride and with padding and dilation\n>>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n>>> input = torch.randn(20, 16, 50, 100)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n \n"}, {"name": "torch.ao.nn.quantized.Conv2d.from_float()", "path": "generated/torch.ao.nn.quantized.conv2d#torch.ao.nn.quantized.Conv2d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n"}, {"name": "torch.ao.nn.quantized.Conv3d", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 3D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv3d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See Conv3d for other attributes. Examples: >>> # With square kernels and equal stride\n>>> m = nn.quantized.Conv3d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n>>> # non-square kernels and unequal stride and with padding and dilation\n>>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))\n>>> input = torch.randn(20, 16, 56, 56, 56)\n>>> # quantize input to quint8\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n  \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n \n"}, {"name": "torch.ao.nn.quantized.Conv3d.from_float()", "path": "generated/torch.ao.nn.quantized.conv3d#torch.ao.nn.quantized.Conv3d.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreates a quantized module from a float module or qparams_dict.  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n"}, {"name": "torch.ao.nn.quantized.ConvTranspose1d", "path": "generated/torch.ao.nn.quantized.convtranspose1d#torch.ao.nn.quantized.ConvTranspose1d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 1D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose1d.  Note Currently only the QNNPACK engine is implemented. Please, set the torch.backends.quantized.engine = \u2018qnnpack\u2019  For special notes, please, see Conv1d  Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See ConvTranspose2d for other attributes. Examples: >>> torch.backends.quantized.engine = 'qnnpack'\n>>> from torch.ao.nn import quantized as nnq\n>>> # With square kernels and equal stride\n>>> m = nnq.ConvTranspose1d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nnq.ConvTranspose1d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> input = torch.randn(20, 16, 50)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n>>> # exact output size can be also specified as an argument\n>>> input = torch.randn(1, 16, 12)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> downsample = nnq.Conv1d(16, 16, 3, stride=2, padding=1)\n>>> upsample = nnq.ConvTranspose1d(16, 16, 3, stride=2, padding=1)\n>>> h = downsample(q_input)\n>>> h.size()\ntorch.Size([1, 16, 6])\n>>> output = upsample(h, output_size=input.size())\n>>> output.size()\ntorch.Size([1, 16, 12])\n \n"}, {"name": "torch.ao.nn.quantized.ConvTranspose2d", "path": "generated/torch.ao.nn.quantized.convtranspose2d#torch.ao.nn.quantized.ConvTranspose2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 2D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose2d. For special notes, please, see Conv2d  Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See ConvTranspose2d for other attributes. Examples: >>> # QNNPACK or FBGEMM as backend\n>>> torch.backends.quantized.engine = 'qnnpack'\n>>> # With square kernels and equal stride\n>>> import torch.ao.nn.quantized as nnq\n>>> m = nnq.ConvTranspose2d(16, 33, 3, stride=2)\n>>> # non-square kernels and unequal stride and with padding\n>>> m = nnq.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n>>> input = torch.randn(20, 16, 50, 100)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n>>> # exact output size can be also specified as an argument\n>>> input = torch.randn(1, 16, 12, 12)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> downsample = nnq.Conv2d(16, 16, 3, stride=2, padding=1)\n>>> upsample = nnq.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n>>> h = downsample(q_input)\n>>> h.size()\ntorch.Size([1, 16, 6, 6])\n>>> output = upsample(h, output_size=input.size())\n>>> output.size()\ntorch.Size([1, 16, 12, 12])\n \n"}, {"name": "torch.ao.nn.quantized.ConvTranspose3d", "path": "generated/torch.ao.nn.quantized.convtranspose3d#torch.ao.nn.quantized.ConvTranspose3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None) [source]\n \nApplies a 3D transposed convolution operator over an input image composed of several input planes. For details on input arguments, parameters, and implementation see ConvTranspose3d.  Note Currently only the FBGEMM engine is implemented. Please, set the torch.backends.quantized.engine = \u2018fbgemm\u2019  For special notes, please, see Conv3d  Variables \n \nweight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. \nscale (Tensor) \u2013 scalar for the output scale \nzero_point (Tensor) \u2013 scalar for the output zero point    See ConvTranspose3d for other attributes. Examples: >>> torch.backends.quantized.engine = 'fbgemm'\n>>> from torch.ao.nn import quantized as nnq\n>>> # With cubic kernels and equal stride\n>>> m = nnq.ConvTranspose3d(16, 33, 3, stride=2)\n>>> # non-cubic kernels and unequal stride and with padding\n>>> m = nnq.ConvTranspose3d(16, 33, (3, 3, 5), stride=(2, 1, 1), padding=(4, 2, 2))\n>>> input = torch.randn(20, 16, 50, 100, 100)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> output = m(q_input)\n>>> # exact output size can be also specified as an argument\n>>> input = torch.randn(1, 16, 12, 12, 12)\n>>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)\n>>> downsample = nnq.Conv3d(16, 16, 3, stride=2, padding=1)\n>>> upsample = nnq.ConvTranspose3d(16, 16, 3, stride=2, padding=1)\n>>> h = downsample(q_input)\n>>> h.size()\ntorch.Size([1, 16, 6, 6, 6])\n>>> output = upsample(h, output_size=input.size())\n>>> output.size()\ntorch.Size([1, 16, 12, 12, 12])\n \n"}, {"name": "torch.ao.nn.quantized.dynamic.GRU", "path": "generated/torch.ao.nn.quantized.dynamic.gru#torch.ao.nn.quantized.dynamic.GRU", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.GRU(*args, **kwargs) [source]\n \nApplies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  rt=\u03c3(Wirxt+bir+Whrh(t\u22121)+bhr)zt=\u03c3(Wizxt+biz+Whzh(t\u22121)+bhz)nt=tanh\u2061(Winxt+bin+rt\u2217(Whnh(t\u22121)+bhn))ht=(1\u2212zt)\u2217nt+zt\u2217h(t\u22121)\\begin{array}{ll} r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\ z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\ n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\ h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\end{array} \n\nwhere hth_t is the hidden state at time t, xtx_t is the input at time t, h(t\u22121)h_{(t-1)} is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_t, ztz_t, ntn_t are the reset, update, and new gates, respectively. \u03c3\\sigma is the sigmoid function, and \u2217* is the Hadamard product. In a multilayer GRU, the input xt(l)x^{(l)}_t of the ll -th layer (l>=2l >= 2) is the hidden state ht(l\u22121)h^{(l-1)}_t of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t where each \u03b4t(l\u22121)\\delta^{(l-1)}_t is a Bernoulli random variable which is 00 with probability dropout.  Parameters \n \ninput_size \u2013 The number of expected features in the input x\n \nhidden_size \u2013 The number of features in the hidden state h\n \nnum_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 \nbias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n \nbatch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n \ndropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 \nbidirectional \u2013 If True, becomes a bidirectional GRU. Default: False\n     Inputs: input, h_0\n\n \ninput of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. \nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n\n\n \noutput of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  \nh_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape:\n\n Input1: (L,N,Hin)(L, N, H_{in}) tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size} and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out}) tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size} Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions} If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all}) where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}\n Output2: (S,N,Hout)(S, N, H_{out}) tensor containing the next hidden state for each element in the batch     Variables \n \nweight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th} layer (W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0. Otherwise, the shape is (3*hidden_size, num_directions * hidden_size)\n \nweight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th} layer (W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)\n \nbias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th} layer (b_ir|b_iz|b_in), of shape (3*hidden_size)\n \nbias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th} layer (b_hr|b_hz|b_hn), of shape (3*hidden_size)\n     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}   Note The calculation of new gate ntn_t subtly differs from the original paper and other frameworks. In the original implementation, the Hadamard product (\u2217)(*) between rtr_t and the previous hidden state h(t\u22121)h_{(t-1)} is done before the multiplication with the weight matrix W and addition of bias:  nt=tanh\u2061(Winxt+bin+Whn(rt\u2217h(t\u22121))+bhn)\\begin{aligned} n_t = \\tanh(W_{in} x_t + b_{in} + W_{hn} ( r_t * h_{(t-1)} ) + b_{hn}) \\end{aligned} \n\nThis is in contrast to PyTorch implementation, which is done after Whnh(t\u22121)W_{hn} h_{(t-1)}  nt=tanh\u2061(Winxt+bin+rt\u2217(Whnh(t\u22121)+bhn))\\begin{aligned} n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\end{aligned} \n\nThis implementation differs on purpose for efficiency.   Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: >>> rnn = nn.GRU(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> output, hn = rnn(input, h0)\n \n"}, {"name": "torch.ao.nn.quantized.dynamic.GRUCell", "path": "generated/torch.ao.nn.quantized.dynamic.grucell#torch.ao.nn.quantized.dynamic.GRUCell", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8) [source]\n \nA gated recurrent unit (GRU) cell A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.GRUCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation. Examples: >>> rnn = nn.GRUCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n...     hx = rnn(input[i], hx)\n...     output.append(hx)\n \n"}, {"name": "torch.ao.nn.quantized.dynamic.Linear", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8) [source]\n \nA dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n \nweight (Tensor) \u2013 the non-learnable quantized weights of the module which are of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}). \nbias (Tensor) \u2013 the non-learnable floating point bias of the module of shape (out_features)(\\text{out\\_features}). If bias is True, the values are initialized to zero.    Examples: >>> m = nn.quantized.dynamic.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n  \nclassmethod from_float(mod) [source]\n \nCreate a dynamic quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n  \nclassmethod from_reference(ref_qlinear) [source]\n \nCreate a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user: \n \n"}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a dynamic quantized module from a float module or qparams_dict  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n"}, {"name": "torch.ao.nn.quantized.dynamic.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.dynamic.linear#torch.ao.nn.quantized.dynamic.Linear.from_reference", "type": "Quantization", "text": " \nclassmethod from_reference(ref_qlinear) [source]\n \nCreate a (fbgemm/qnnpack) dynamic quantized module from a reference quantized module :param ref_qlinear: a reference quantized module, either produced by :type ref_qlinear: Module :param torch.ao.quantization functions or provided by the user: \n"}, {"name": "torch.ao.nn.quantized.dynamic.LSTM", "path": "generated/torch.ao.nn.quantized.dynamic.lstm#torch.ao.nn.quantized.dynamic.LSTM", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.LSTM(*args, **kwargs) [source]\n \nA dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as torch.nn.LSTM, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation. Examples: >>> rnn = nn.LSTM(10, 20, 2)\n>>> input = torch.randn(5, 3, 10)\n>>> h0 = torch.randn(2, 3, 20)\n>>> c0 = torch.randn(2, 3, 20)\n>>> output, (hn, cn) = rnn(input, (h0, c0))\n \n"}, {"name": "torch.ao.nn.quantized.dynamic.LSTMCell", "path": "generated/torch.ao.nn.quantized.dynamic.lstmcell#torch.ao.nn.quantized.dynamic.LSTMCell", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.LSTMCell(*args, **kwargs) [source]\n \nA long short-term memory (LSTM) cell. A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.LSTMCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation. Examples: >>> rnn = nn.LSTMCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> cx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n...     hx, cx = rnn(input[i], (hx, cx))\n...     output.append(hx)\n \n"}, {"name": "torch.ao.nn.quantized.dynamic.RNNCell", "path": "generated/torch.ao.nn.quantized.dynamic.rnncell#torch.ao.nn.quantized.dynamic.RNNCell", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8) [source]\n \nAn Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as torch.nn.RNNCell, please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation. Examples: >>> rnn = nn.RNNCell(10, 20)\n>>> input = torch.randn(6, 3, 10)\n>>> hx = torch.randn(3, 20)\n>>> output = []\n>>> for i in range(6):\n...     hx = rnn(input[i], hx)\n...     output.append(hx)\n \n"}, {"name": "torch.ao.nn.quantized.ELU", "path": "generated/torch.ao.nn.quantized.elu#torch.ao.nn.quantized.ELU", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.ELU(scale, zero_point, alpha=1.0) [source]\n \nThis is the quantized equivalent of ELU.  Parameters \n \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor \nalpha (float) \u2013 the alpha constant    \n"}, {"name": "torch.ao.nn.quantized.Embedding", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, dtype=torch.quint8) [source]\n \nA quantized Embedding module with quantized packed weights as inputs. We adopt the same interface as torch.nn.Embedding, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding for documentation. Similar to Embedding, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \nweight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).    Examples::\n\n>>> m = nn.quantized.Embedding(num_embeddings=10, embedding_dim=12)\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8])\n>>> output = m(indices)\n>>> print(output.size())\ntorch.Size([9, 12])\n    \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user   \n \n"}, {"name": "torch.ao.nn.quantized.Embedding.from_float()", "path": "generated/torch.ao.nn.quantized.embedding#torch.ao.nn.quantized.Embedding.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user   \n"}, {"name": "torch.ao.nn.quantized.EmbeddingBag", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.EmbeddingBag(num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='sum', sparse=False, _weight=None, include_last_offset=False, dtype=torch.quint8) [source]\n \nA quantized EmbeddingBag module with quantized packed weights as inputs. We adopt the same interface as torch.nn.EmbeddingBag, please see https://pytorch.org/docs/stable/nn.html#torch.nn.EmbeddingBag for documentation. Similar to EmbeddingBag, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \nweight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (num_embeddings,embedding_dim)(\\text{num\\_embeddings}, \\text{embedding\\_dim}).    Examples::\n\n>>> m = nn.quantized.EmbeddingBag(num_embeddings=10, embedding_dim=12, include_last_offset=True, mode='sum')\n>>> indices = torch.tensor([9, 6, 5, 7, 8, 8, 9, 2, 8, 6, 6, 9, 1, 6, 8, 8, 3, 2, 3, 6, 3, 6, 5, 7, 0, 8, 4, 6, 5, 8, 2, 3])\n>>> offsets = torch.tensor([0, 19, 20, 28, 28, 32])\n>>> output = m(indices, offsets)\n>>> print(output.size())\ntorch.Size([5, 12])\n    \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding_bag module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user   \n \n"}, {"name": "torch.ao.nn.quantized.EmbeddingBag.from_float()", "path": "generated/torch.ao.nn.quantized.embeddingbag#torch.ao.nn.quantized.EmbeddingBag.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized embedding_bag module from a float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by user   \n"}, {"name": "torch.ao.nn.quantized.FloatFunctional", "path": "generated/torch.ao.nn.quantized.floatfunctional#torch.ao.nn.quantized.FloatFunctional", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.FloatFunctional [source]\n \nState collector class for float operations. The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: >>> f_add = FloatFunctional()\n>>> a = torch.tensor(3.0)\n>>> b = torch.tensor(4.0)\n>>> f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``\n  Valid operation names:\n\n add cat mul add_relu add_scalar mul_scalar    \n"}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool2d#torch.ao.nn.quantized.functional.adaptive_avg_pool2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.adaptive_avg_pool2d(input, output_size) [source]\n \nApplies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters propagate to the output.  See AdaptiveAvgPool2d for details and output shape.  Parameters \noutput_size (None) \u2013 the target output size (single integer or double-integer tuple)  Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.adaptive_avg_pool3d#torch.ao.nn.quantized.functional.adaptive_avg_pool3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.adaptive_avg_pool3d(input, output_size) [source]\n \nApplies a 3D adaptive average pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters propagate to the output.  See AdaptiveAvgPool3d for details and output shape.  Parameters \noutput_size (None) \u2013 the target output size (single integer or double-integer tuple)  Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.avg_pool2d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool2d#torch.ao.nn.quantized.functional.avg_pool2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) [source]\n \nApplies 2D average-pooling operation in kH\u00d7kWkH \\times kW regions by step size sH\u00d7sWsH \\times sW steps. The number of output features is equal to the number of input planes.  Note The input quantization parameters propagate to the output.  See AvgPool2d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)\n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n"}, {"name": "torch.ao.nn.quantized.functional.avg_pool3d", "path": "generated/torch.ao.nn.quantized.functional.avg_pool3d#torch.ao.nn.quantized.functional.avg_pool3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) [source]\n \nApplies 3D average-pooling operation in kDtimeskH\u00d7kWkD \\ times kH \\times kW regions by step size sD\u00d7sH\u00d7sWsD \\times sH \\times sW steps. The number of output features is equal to the number of input planes.  Note The input quantization parameters propagate to the output.   Parameters \n \ninput \u2013 quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)\n \nkernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kD, kH, kW)\n \nstride \u2013 stride of the pooling operation. Can be a single number or a tuple (sD, sH, sW). Default: kernel_size\n \npadding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padD, padH, padW). Default: 0 \nceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False\n \ncount_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True\n \ndivisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    \n"}, {"name": "torch.ao.nn.quantized.functional.celu", "path": "generated/torch.ao.nn.quantized.functional.celu#torch.ao.nn.quantized.functional.celu", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.celu(input, scale, zero_point, alpha=1.) [source]\n \nApplies the quantized CELU function element-wise.  CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x / \\alpha) - 1)) \n\n Parameters \n \ninput (Tensor) \u2013 quantized input \nalpha (float) \u2013 the \u03b1\\alpha value for the CELU formulation. Default: 1.0   Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.clamp", "path": "generated/torch.ao.nn.quantized.functional.clamp#torch.ao.nn.quantized.functional.clamp", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.clamp(input, min_, max_) [source]\n \nfloat(input, min_, max_) -> Tensor Applies the clamp function element-wise. See clamp for more details.  Parameters \n \ninput (Tensor) \u2013 quantized input \nmin \u2013 minimum value for clamping \nmax \u2013 maximum value for clamping   Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.conv1d", "path": "generated/torch.ao.nn.quantized.functional.conv1d#torch.ao.nn.quantized.functional.conv1d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.conv1d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 1D convolution over a quantized 1D input composed of several input planes. See Conv1d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)\n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,iW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , iW)\n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}). The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sW,). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dW,). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels} should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.ao.nn.quantized import functional as qF\n>>> filters = torch.randn(33, 16, 3, dtype=torch.float)\n>>> inputs = torch.randn(20, 16, 50, dtype=torch.float)\n>>> bias = torch.randn(33, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.ao.nn.quantized.functional.conv2d", "path": "generated/torch.ao.nn.quantized.functional.conv2d#torch.ao.nn.quantized.functional.conv2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.conv2d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 2D convolution over a quantized 2D input composed of several input planes. See Conv2d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)\n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)\n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}). The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels} should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.ao.nn.quantized import functional as qF\n>>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float)\n>>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)\n>>> bias = torch.randn(8, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.ao.nn.quantized.functional.conv3d", "path": "generated/torch.ao.nn.quantized.functional.conv3d#torch.ao.nn.quantized.functional.conv3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.conv3d(input, weight, bias, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros', scale=1.0, zero_point=0, dtype=torch.quint8) [source]\n \nApplies a 3D convolution over a quantized 3D input composed of several input planes. See Conv3d for details and output shape.  Parameters \n \ninput \u2013 quantized input tensor of shape (minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD , iH , iW)\n \nweight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kD,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kD , kH , kW)\n \nbias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels}). The tensor type must be torch.float. \nstride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1 \npadding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padD, padH, padW). Default: 0 \ndilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dD, dH, dW). Default: 1 \ngroups \u2013 split input into groups, in_channels\\text{in\\_channels} should be divisible by the number of groups. Default: 1 \npadding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d \nscale \u2013 quantization scale for the output. Default: 1.0 \nzero_point \u2013 quantization zero_point for the output. Default: 0 \ndtype \u2013 quantization data type to use. Default: torch.quint8\n    Examples: >>> from torch.ao.nn.quantized import functional as qF\n>>> filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)\n>>> inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)\n>>> bias = torch.randn(8, dtype=torch.float)\n>>>\n>>> scale, zero_point = 1.0, 0\n>>> dtype_inputs = torch.quint8\n>>> dtype_filters = torch.qint8\n>>>\n>>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)\n>>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)\n>>> qF.conv3d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)\n \n"}, {"name": "torch.ao.nn.quantized.functional.elu", "path": "generated/torch.ao.nn.quantized.functional.elu#torch.ao.nn.quantized.functional.elu", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.elu(input, scale, zero_point, alpha=1.0) [source]\n \nThis is the quantized version of elu().  Parameters \n \ninput (Tensor) \u2013 quantized input \nscale (float) \u2013 quantization scale of the output tensor \nzero_point (int) \u2013 quantization zero point of the output tensor \nalpha (float) \u2013 the alpha constant   Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.hardsigmoid", "path": "generated/torch.ao.nn.quantized.functional.hardsigmoid#torch.ao.nn.quantized.functional.hardsigmoid", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.hardsigmoid(input, inplace=False) [source]\n \nThis is the quantized version of hardsigmoid().  Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.hardswish", "path": "generated/torch.ao.nn.quantized.functional.hardswish#torch.ao.nn.quantized.functional.hardswish", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.hardswish(input, scale, zero_point) [source]\n \nThis is the quantized version of hardswish().  Parameters \n \ninput (Tensor) \u2013 quantized input \nscale (float) \u2013 quantization scale of the output tensor \nzero_point (int) \u2013 quantization zero point of the output tensor   Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.hardtanh", "path": "generated/torch.ao.nn.quantized.functional.hardtanh#torch.ao.nn.quantized.functional.hardtanh", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.hardtanh(input, min_val=-1.0, max_val=1.0, inplace=False) [source]\n \nThis is the quantized version of hardtanh().  Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.interpolate", "path": "generated/torch.ao.nn.quantized.functional.interpolate#torch.ao.nn.quantized.functional.interpolate", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nDown/up samples the input to either the given size or the given scale_factor See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D/3D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters \n \ninput (Tensor) \u2013 the input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. \nmode (str) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False\n    \n"}, {"name": "torch.ao.nn.quantized.functional.leaky_relu", "path": "generated/torch.ao.nn.quantized.functional.leaky_relu#torch.ao.nn.quantized.functional.leaky_relu", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.leaky_relu(input, negative_slope=0.01, inplace=False, scale=None, zero_point=None) [source]\n \nQuantized version of the. leaky_relu(input, negative_slope=0.01, inplace=False, scale, zero_point) -> Tensor Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  Parameters \n \ninput (Tensor) \u2013 Quantized input \nnegative_slope (float) \u2013 The slope of the negative input \ninplace (bool) \u2013 Inplace modification of the input tensor \nscale (Optional[float]) \u2013 Scale and zero point of the output tensor. \nzero_point (Optional[int]) \u2013 Scale and zero point of the output tensor.    See LeakyReLU for more details. \n"}, {"name": "torch.ao.nn.quantized.functional.linear", "path": "generated/torch.ao.nn.quantized.functional.linear#torch.ao.nn.quantized.functional.linear", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None) [source]\n \nApplies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + b. See Linear  Note Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear.   Parameters \n \ninput (Tensor) \u2013 Quantized input of type torch.quint8\n \nweight (Tensor) \u2013 Quantized weight of type torch.qint8\n \nbias (Tensor) \u2013 None or fp32 bias of type torch.float\n \nscale (double) \u2013 output scale. If None, derived from the input scale \nzero_point (python:long) \u2013 output zero point. If None, derived from the input zero_point   Return type \nTensor    Shape:\n\n Input: (N,\u2217,in_features)(N, *, in\\_features) where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features)\n Bias: (out_features)(out\\_features)\n Output: (N,\u2217,out_features)(N, *, out\\_features)\n    \n"}, {"name": "torch.ao.nn.quantized.functional.max_pool1d", "path": "generated/torch.ao.nn.quantized.functional.max_pool1d#torch.ao.nn.quantized.functional.max_pool1d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.max_pool1d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False) [source]\n \nApplies a 1D max pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters are propagated to the output.  See MaxPool1d for details. \n"}, {"name": "torch.ao.nn.quantized.functional.max_pool2d", "path": "generated/torch.ao.nn.quantized.functional.max_pool2d#torch.ao.nn.quantized.functional.max_pool2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1, ceil_mode=False, return_indices=False) [source]\n \nApplies a 2D max pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters are propagated to the output.  See MaxPool2d for details. \n"}, {"name": "torch.ao.nn.quantized.functional.threshold", "path": "generated/torch.ao.nn.quantized.functional.threshold#torch.ao.nn.quantized.functional.threshold", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.threshold(input, threshold, value) [source]\n \nApplies the quantized version of the threshold function element-wise:  x={xif x>thresholdvalueotherwisex = \\begin{cases} x & \\text{if~} x > \\text{threshold} \\\\ \\text{value} & \\text{otherwise} \\end{cases} \n\nSee Threshold for more details.  Return type \nTensor   \n"}, {"name": "torch.ao.nn.quantized.functional.upsample", "path": "generated/torch.ao.nn.quantized.functional.upsample#torch.ao.nn.quantized.functional.upsample", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None) [source]\n \nUpsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).  See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters \n \ninput (Tensor) \u2013 quantized input tensor \nsize (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (float or Tuple[float]) \u2013 multiplier for spatial size. Has to be an integer. \nmode (str) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear'\n \nalign_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False\n     Warning With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  \n"}, {"name": "torch.ao.nn.quantized.functional.upsample_bilinear", "path": "generated/torch.ao.nn.quantized.functional.upsample_bilinear#torch.ao.nn.quantized.functional.upsample_bilinear", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.upsample_bilinear(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters \n \ninput (Tensor) \u2013 quantized input \nsize (int or Tuple[int, int]) \u2013 output spatial size. \nscale_factor (int or Tuple[int, int]) \u2013 multiplier for spatial size    \n"}, {"name": "torch.ao.nn.quantized.functional.upsample_nearest", "path": "generated/torch.ao.nn.quantized.functional.upsample_nearest#torch.ao.nn.quantized.functional.upsample_nearest", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.functional.upsample_nearest(input, size=None, scale_factor=None) [source]\n \nUpsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.ao.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters \n \ninput (Tensor) \u2013 quantized input \nsize (int or Tuple[int, int] or Tuple[int, int, int]) \u2013 output spatial size. \nscale_factor (int) \u2013 multiplier for spatial size. Has to be an integer.    \n"}, {"name": "torch.ao.nn.quantized.FXFloatFunctional", "path": "generated/torch.ao.nn.quantized.fxfloatfunctional#torch.ao.nn.quantized.FXFloatFunctional", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.FXFloatFunctional(*args, **kwargs) [source]\n \nmodule to replace FloatFunctional module before FX graph mode quantization, since activation_post_process will be inserted in top level module directly  Valid operation names:\n\n add cat mul add_relu add_scalar mul_scalar    \n"}, {"name": "torch.ao.nn.quantized.GroupNorm", "path": "generated/torch.ao.nn.quantized.groupnorm#torch.ao.nn.quantized.GroupNorm", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.GroupNorm(num_groups, num_channels, weight, bias, scale, zero_point, eps=1e-05, affine=True, device=None, dtype=None) [source]\n \nThis is the quantized version of GroupNorm.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.ao.nn.quantized.Hardswish", "path": "generated/torch.ao.nn.quantized.hardswish#torch.ao.nn.quantized.Hardswish", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Hardswish(scale, zero_point, device=None, dtype=None) [source]\n \nThis is the quantized version of Hardswish.  Parameters \n \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor    \n"}, {"name": "torch.ao.nn.quantized.InstanceNorm1d", "path": "generated/torch.ao.nn.quantized.instancenorm1d#torch.ao.nn.quantized.InstanceNorm1d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.InstanceNorm1d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None) [source]\n \nThis is the quantized version of InstanceNorm1d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.ao.nn.quantized.InstanceNorm2d", "path": "generated/torch.ao.nn.quantized.instancenorm2d#torch.ao.nn.quantized.InstanceNorm2d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.InstanceNorm2d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None) [source]\n \nThis is the quantized version of InstanceNorm2d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.ao.nn.quantized.InstanceNorm3d", "path": "generated/torch.ao.nn.quantized.instancenorm3d#torch.ao.nn.quantized.InstanceNorm3d", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.InstanceNorm3d(num_features, weight, bias, scale, zero_point, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False, device=None, dtype=None) [source]\n \nThis is the quantized version of InstanceNorm3d.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.ao.nn.quantized.LayerNorm", "path": "generated/torch.ao.nn.quantized.layernorm#torch.ao.nn.quantized.LayerNorm", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.LayerNorm(normalized_shape, weight, bias, scale, zero_point, eps=1e-05, elementwise_affine=True, device=None, dtype=None) [source]\n \nThis is the quantized version of LayerNorm.  Additional args:\n\n \nscale - quantization scale of the output, type: double. \nzero_point - quantization zero point of the output, type: long.    \n"}, {"name": "torch.ao.nn.quantized.LeakyReLU", "path": "generated/torch.ao.nn.quantized.leakyrelu#torch.ao.nn.quantized.LeakyReLU", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.LeakyReLU(scale, zero_point, negative_slope=0.01, inplace=False, device=None, dtype=None) [source]\n \nThis is the quantized equivalent of LeakyReLU.  Parameters \n \nscale (float) \u2013 quantization scale of the output tensor \nzero_point (int) \u2013 quantization zero point of the output tensor \nnegative_slope (float) \u2013 Controls the angle of the negative slope. Default: 1e-2    \n"}, {"name": "torch.ao.nn.quantized.Linear", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Linear(in_features, out_features, bias_=True, dtype=torch.qint8) [source]\n \nA quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables \n \nweight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features}). \nbias (Tensor) \u2013 the non-learnable bias of the module of shape (out_features)(\\text{out\\_features}). If bias is True, the values are initialized to zero. \nscale \u2013 scale parameter of output Quantized Tensor, type: double \nzero_point \u2013 zero_point parameter for output Quantized Tensor, type: long    Examples: >>> m = nn.quantized.Linear(20, 30)\n>>> input = torch.randn(128, 20)\n>>> input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)\n>>> output = m(input)\n>>> print(output.size())\ntorch.Size([128, 30])\n  \nclassmethod from_float(mod) [source]\n \nCreate a quantized module from an observed float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n  \nclassmethod from_reference(ref_qlinear, output_scale, output_zero_point) [source]\n \nCreate a (fbgemm/qnnpack) quantized module from a reference quantized module  Parameters \n \nref_qlinear (Module) \u2013 a reference quantized linear module, either produced by torch.ao.quantization utilities or provided by the user \noutput_scale (float) \u2013 scale for output Tensor \noutput_zero_point (int) \u2013 zero point for output Tensor    \n \n"}, {"name": "torch.ao.nn.quantized.Linear.from_float()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_float", "type": "Quantization", "text": " \nclassmethod from_float(mod) [source]\n \nCreate a quantized module from an observed float module  Parameters \nmod (Module) \u2013 a float module, either produced by torch.ao.quantization utilities or provided by the user   \n"}, {"name": "torch.ao.nn.quantized.Linear.from_reference()", "path": "generated/torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear.from_reference", "type": "Quantization", "text": " \nclassmethod from_reference(ref_qlinear, output_scale, output_zero_point) [source]\n \nCreate a (fbgemm/qnnpack) quantized module from a reference quantized module  Parameters \n \nref_qlinear (Module) \u2013 a reference quantized linear module, either produced by torch.ao.quantization utilities or provided by the user \noutput_scale (float) \u2013 scale for output Tensor \noutput_zero_point (int) \u2013 zero point for output Tensor    \n"}, {"name": "torch.ao.nn.quantized.QFunctional", "path": "generated/torch.ao.nn.quantized.qfunctional#torch.ao.nn.quantized.QFunctional", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.QFunctional [source]\n \nWrapper class for quantized operations. The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: >>> q_add = QFunctional()\n>>> a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)\n>>> b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)\n>>> q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``\n  Valid operation names:\n\n add cat mul add_relu add_scalar mul_scalar    \n"}, {"name": "torch.ao.nn.quantized.ReLU6", "path": "generated/torch.ao.nn.quantized.relu6#torch.ao.nn.quantized.ReLU6", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.ReLU6(inplace=False) [source]\n \nApplies the element-wise function: ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6)), where x0x_0 is the zero_point, and q(6)q(6) is the quantized representation of number 6.  Parameters \ninplace (bool) \u2013 can optionally do the operation in-place. Default: False    Shape:\n\n Input: (N,\u2217)(N, *) where * means, any number of additional dimensions Output: (N,\u2217)(N, *), same shape as the input     Examples: >>> m = nn.quantized.ReLU6()\n>>> input = torch.randn(2)\n>>> input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)\n>>> output = m(input)\n \n"}, {"name": "torch.ao.nn.quantized.Sigmoid", "path": "generated/torch.ao.nn.quantized.sigmoid#torch.ao.nn.quantized.Sigmoid", "type": "Quantization", "text": " \nclass torch.ao.nn.quantized.Sigmoid(output_scale, output_zero_point) [source]\n \nThis is the quantized equivalent of Sigmoid.  Parameters \n \nscale \u2013 quantization scale of the output tensor \nzero_point \u2013 quantization zero point of the output tensor    \n"}, {"name": "torch.ao.ns._numeric_suite.compare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_outputs", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.compare_model_outputs(float_model, q_model, *data, logger_cls=<class 'torch.ao.ns._numeric_suite.OutputLogger'>, allow_list=None) [source]\n \nCompare output activations between float and quantized models at corresponding locations for the same input. Return a dict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the activations of quantized model and float model at matching locations. This dict can be used to compare and compute the propagation quantization error. Example usage: act_compare_dict = compare_model_outputs(float_model, qmodel, data)\nfor key in act_compare_dict:\n    print(\n        key,\n        compute_error(\n            act_compare_dict[key]['float'],\n            act_compare_dict[key]['quantized'].dequantize()\n        )\n    )\n  Parameters \n \nfloat_model (Module) \u2013 float model used to generate the q_model \nq_model (Module) \u2013 model quantized from float_model \ndata \u2013 input data used to run the prepared float_model and q_model \nlogger_cls \u2013 type of logger to be attached to float_module and q_module \nallow_list \u2013 list of module types to attach logger   Returns \ndict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations  Return type \nact_compare_dict   \n"}, {"name": "torch.ao.ns._numeric_suite.compare_model_stub()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_model_stub", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.compare_model_stub(float_model, q_model, module_swap_list, *data, logger_cls=<class 'torch.ao.ns._numeric_suite.ShadowLogger'>) [source]\n \nCompare quantized module in a model with its floating point counterpart, feeding both of them the same input. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the output tensors of quantized and its matching float shadow module. This dict can be used to compare and compute the module level quantization error. This function first call prepare_model_with_stubs() to swap the quantized module that we want to compare with the Shadow module, which takes quantized module, corresponding float module and logger as input, and creates a forward path inside to make the float module to shadow quantized module sharing the same input. The logger can be customizable, default logger is ShadowLogger and it will save the outputs of the quantized module and float module that can be used to compute the module level quantization error. Example usage: module_swap_list = [torchvision.models.quantization.resnet.QuantizableBasicBlock]\nob_dict = compare_model_stub(float_model,qmodel,module_swap_list, data)\nfor key in ob_dict:\n    print(key, compute_error(ob_dict[key]['float'], ob_dict[key]['quantized'].dequantize()))\n  Parameters \n \nfloat_model (Module) \u2013 float model used to generate the q_model \nq_model (Module) \u2013 model quantized from float_model \nmodule_swap_list (Set[type]) \u2013 list of float module types at which shadow modules will be attached. \ndata \u2013 input data used to run the prepared q_model \nlogger_cls \u2013 type of logger to be used in shadow module to process the outputs of quantized module and its float shadow module   Return type \nDict[str, Dict]   \n"}, {"name": "torch.ao.ns._numeric_suite.compare_weights()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.compare_weights", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.compare_weights(float_dict, quantized_dict) [source]\n \nCompare the weights of the float module with its corresponding quantized module. Return a dict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights. This dict can be used to compare and compute the quantization error of the weights of float and quantized models. Example usage: wt_compare_dict = compare_weights(\n    float_model.state_dict(), qmodel.state_dict())\nfor key in wt_compare_dict:\n    print(\n        key,\n        compute_error(\n            wt_compare_dict[key]['float'],\n            wt_compare_dict[key]['quantized'].dequantize()\n        )\n    )\n  Parameters \n \nfloat_dict (Dict[str, Any]) \u2013 state dict of the float model \nquantized_dict (Dict[str, Any]) \u2013 state dict of the quantized model   Returns \ndict with key corresponding to module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the float and quantized weights  Return type \nweight_dict   \n"}, {"name": "torch.ao.ns._numeric_suite.get_logger_dict()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_logger_dict", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.get_logger_dict(mod, prefix='') [source]\n \nTraverse the modules and save all logger stats into target dict. This is mainly used for quantization accuracy debug.  Type of loggers supported:\n\nShadowLogger: used to log the outputs of the quantized module and its matching float shadow module, OutputLogger: used to log the outputs of the modules    Parameters \n \nmod (Module) \u2013 module we want to save all logger stats \nprefix (str) \u2013 prefix for the current module   Returns \nthe dictionary used to save all logger stats  Return type \ntarget_dict   \n"}, {"name": "torch.ao.ns._numeric_suite.get_matching_activations()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.get_matching_activations", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.get_matching_activations(float_module, q_module) [source]\n \nFind the matching activation between float and quantized modules.  Parameters \n \nfloat_module (Module) \u2013 float module used to generate the q_module \nq_module (Module) \u2013 module quantized from float_module   Returns \ndict with key corresponding to quantized module names and each entry being a dictionary with two keys \u2018float\u2019 and \u2018quantized\u2019, containing the matching float and quantized activations  Return type \nact_dict   \n"}, {"name": "torch.ao.ns._numeric_suite.Logger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite.Logger [source]\n \nBase class for stats logging  \nforward(x) [source]\n\n \n"}, {"name": "torch.ao.ns._numeric_suite.Logger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Logger.forward", "type": "Quantization", "text": " \nforward(x) [source]\n\n"}, {"name": "torch.ao.ns._numeric_suite.OutputLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite.OutputLogger [source]\n \nClass used to log the outputs of the module  \nforward(x) [source]\n\n \n"}, {"name": "torch.ao.ns._numeric_suite.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.OutputLogger.forward", "type": "Quantization", "text": " \nforward(x) [source]\n\n"}, {"name": "torch.ao.ns._numeric_suite.prepare_model_outputs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_outputs", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.prepare_model_outputs(float_module, q_module, logger_cls=<class 'torch.ao.ns._numeric_suite.OutputLogger'>, allow_list=None) [source]\n \nPrepare the model by attaching the logger to both float module and quantized module if they are in the allow_list.  Parameters \n \nfloat_module (Module) \u2013 float module used to generate the q_module \nq_module (Module) \u2013 module quantized from float_module \nlogger_cls \u2013 type of logger to be attached to float_module and q_module \nallow_list \u2013 list of module types to attach logger    \n"}, {"name": "torch.ao.ns._numeric_suite.prepare_model_with_stubs()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.prepare_model_with_stubs", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite.prepare_model_with_stubs(float_module, q_module, module_swap_list, logger_cls) [source]\n \nPrepare the model by attaching the float module to its matching quantized module as the shadow if the float module type is in module_swap_list. Example usage: prepare_model_with_stubs(float_model, q_model, module_swap_list, Logger)\nq_model(data)\nob_dict = get_logger_dict(q_model)\n  Parameters \n \nfloat_module (Module) \u2013 float module used to generate the q_module \nq_module (Module) \u2013 module quantized from float_module \nmodule_swap_list (Set[type]) \u2013 list of float module types to attach the shadow \nlogger_cls (Callable) \u2013 type of logger to be used in shadow module to process the outputs of quantized module and its float shadow module    \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite.Shadow(q_module, float_module, logger_cls) [source]\n \nShadow module attaches the float module to its matching quantized module as the shadow. Then it uses Logger module to process the outputs of both modules.  Parameters \n \nq_module \u2013 module quantized from float_module that we want to shadow \nfloat_module \u2013 float module used to shadow q_module \nlogger_cls \u2013 type of logger used to process the outputs of q_module and float_module. ShadowLogger or custom loggers can be used.     \nforward(*x) [source]\n \n Return type \nTensor   \n  \nadd(x, y) [source]\n \n Return type \nTensor   \n  \nadd_scalar(x, y) [source]\n \n Return type \nTensor   \n  \nmul(x, y) [source]\n \n Return type \nTensor   \n  \nmul_scalar(x, y) [source]\n \n Return type \nTensor   \n  \ncat(x, dim=0) [source]\n \n Return type \nTensor   \n  \nadd_relu(x, y) [source]\n \n Return type \nTensor   \n \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.add()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add", "type": "Quantization", "text": " \nadd(x, y) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_relu()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_relu", "type": "Quantization", "text": " \nadd_relu(x, y) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.add_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.add_scalar", "type": "Quantization", "text": " \nadd_scalar(x, y) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.cat()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.cat", "type": "Quantization", "text": " \ncat(x, dim=0) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.forward", "type": "Quantization", "text": " \nforward(*x) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul", "type": "Quantization", "text": " \nmul(x, y) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.Shadow.mul_scalar()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.Shadow.mul_scalar", "type": "Quantization", "text": " \nmul_scalar(x, y) [source]\n \n Return type \nTensor   \n"}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite.ShadowLogger [source]\n \nClass used in Shadow module to record the outputs of the original and shadow modules.  \nforward(x, y) [source]\n\n \n"}, {"name": "torch.ao.ns._numeric_suite.ShadowLogger.forward()", "path": "torch.ao.ns._numeric_suite#torch.ao.ns._numeric_suite.ShadowLogger.forward", "type": "Quantization", "text": " \nforward(x, y) [source]\n\n"}, {"name": "torch.ao.ns._numeric_suite_fx.add_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_loggers", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.add_loggers(name_a, model_a, name_b, model_b, logger_cls, should_log_inputs=False, base_name_to_sets_of_related_ops=None, unmatchable_types_map=None) [source]\n \nInstrument model A and model B with loggers.  Parameters \n \nname_a (str) \u2013 string name of model A to use in results \nmodel_a (Module) \u2013 model A \nname_b (str) \u2013 string name of model B to use in results \nmodel_b (Module) \u2013 model B \nlogger_cls (Callable) \u2013 class of Logger to use \nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of subgraph base nodes, subject to change \nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of unmatchable types, subject to change   Returns \nReturns a tuple of (model_a_with_loggers, model_b_with_loggers). Modifies both models inplace.  Return type \nTuple[Module, Module]   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.add_shadow_loggers()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.add_shadow_loggers", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.add_shadow_loggers(name_a, model_a, name_b, model_b, logger_cls, should_log_inputs=False, base_name_to_sets_of_related_ops=None, node_type_to_io_type_map=None, unmatchable_types_map=None) [source]\n \nInstrument model A and model B with shadow loggers.  Parameters \n \nname_a (str) \u2013 string name of model A to use in results \nmodel_a (Module) \u2013 model A \nname_b (str) \u2013 string name of model B to use in results \nmodel_b (Module) \u2013 model B \nlogger_cls (Callable) \u2013 class of Logger to use \nshould_log_inputs (bool) \u2013 whether to log inputs \nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of subgraph base nodes, subject to change \nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of unmatchable types, subject to change   Return type \nModule   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.convert_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.convert_n_shadows_model", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.convert_n_shadows_model(model, custom_convert_fn=None, custom_convert_kwargs=None) [source]\n \nGiven a model from prepare_n_shadows_model, runs convert_fx on each shadow submodule.  Return type \nGraphModule   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.extend_logger_results_with_comparison(results, model_name_1, model_name_2, comparison_fn, comparison_name) [source]\n \nCompares the logged values from model_name_2 against the corresponding values in model_name_1, using comparison_fn. Records the result in model_name_2\u2019s results under comparison_name. Modifies results inplace.  Parameters \n \nresults (Dict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]) \u2013 the result data structure from extract_logger_info or extract_shadow_logger_info. \nmodel_name_1 (str) \u2013 string name of model 1 \nmodel_name_2 (str) \u2013 string name of model 2 \ncomparison_fn (Callable[[Tensor, Tensor], Tensor]) \u2013 function to compare two Tensors \ncomparison_name (str) \u2013 string name of model to use for layer names in the output    \n"}, {"name": "torch.ao.ns._numeric_suite_fx.extract_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_logger_info", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.extract_logger_info(model_a, model_b, logger_cls, model_name_to_use_for_layer_names) [source]\n \nTraverse all loggers in model_a and model_b, and extract the logged information.  Parameters \n \nmodel_a (Module) \u2013 model A \nmodel_b (Module) \u2013 model B \nlogger_cls (Callable) \u2013 class of Logger to use \nmodel_name_to_use_for_layer_names (str) \u2013 string name of model to use for layer names in the output   Returns \nNSResultsType, containing the logged comparisons  Return type \nDict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.extract_results_n_shadows_model(model) [source]\n \nExtracts logger results from model.  Return type \nDict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_shadow_logger_info", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.extract_shadow_logger_info(model_a_shadows_b, logger_cls, model_name_to_use_for_layer_names) [source]\n \nTraverse all loggers in a shadow model, and extract the logged information.  Parameters \n \nmodel_a_shadows_b (Module) \u2013 shadow model \nlogger_cls (Callable) \u2013 class of Logger to use \nmodel_name_to_use_for_layer_names (str) \u2013 string name of model to use for layer names in the output   Returns \nNSResultsType, containing the logged comparisons  Return type \nDict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.extract_weights()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.extract_weights", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.extract_weights(model_name_a, model_a, model_name_b, model_b, base_name_to_sets_of_related_ops=None, unmatchable_types_map=None, op_to_type_to_weight_extraction_fn=None) [source]\n \nExtract weights from model A and model B, and return a comparison.  Parameters \n \nmodel_name_a (str) \u2013 string name of model A to use in results \nmodel_a (Module) \u2013 model A \nmodel_name_b (str) \u2013 string name of model B to use in results \nmodel_b (Module) \u2013 model B \nbase_name_to_sets_of_related_ops (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of subgraph base nodes, subject to change \nunmatchable_types_map (Optional[Dict[str, Set[Union[Callable, str]]]]) \u2013 optional override of unmatchable types, subject to change \nop_to_type_to_weight_extraction_fn (Optional[Dict[str, Dict[Callable, Callable]]]) \u2013 optional override of function which extracts weight from a type, subject to change   Returns \nNSResultsType, containing the weight comparisons  Return type \nDict[str, Dict[str, Dict[str, List[Dict[str, Any]]]]]   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_enabled()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_enabled", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.loggers_set_enabled(model, enabled) [source]\n \nSets the enabled setting on a model\u2019s loggers \n"}, {"name": "torch.ao.ns._numeric_suite_fx.loggers_set_save_activations()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.loggers_set_save_activations", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.loggers_set_save_activations(model, save_activations) [source]\n \nSets the save_activations setting on a model\u2019s loggers \n"}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite_fx.NSTracer(skipped_module_names, skipped_module_classes) [source]\n \nJust like a regular FX quantization tracer, but treats observers and fake_quantize modules as leaf modules.  \nis_leaf_module(m, module_qualified_name) [source]\n \n Return type \nbool   \n \n"}, {"name": "torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.NSTracer.is_leaf_module", "type": "Quantization", "text": " \nis_leaf_module(m, module_qualified_name) [source]\n \n Return type \nbool   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite_fx.OutputComparisonLogger(*args, **kwargs) [source]\n \nSame as OutputLogger, but also requires the original activation in order to calculate the comparison at calibration time  \nforward(x, x_ref) [source]\n\n \n"}, {"name": "torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputComparisonLogger.forward", "type": "Quantization", "text": " \nforward(x, x_ref) [source]\n\n"}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger", "type": "Quantization", "text": " \nclass torch.ao.ns._numeric_suite_fx.OutputLogger(ref_node_name, prev_node_name, model_name, ref_name, prev_node_target_type, ref_node_target_type, results_type, index_within_arg, index_of_arg, fqn, qconfig_str='') [source]\n \nBase class for capturing intermediate values.  \nforward(x) [source]\n\n \n"}, {"name": "torch.ao.ns._numeric_suite_fx.OutputLogger.forward()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.OutputLogger.forward", "type": "Quantization", "text": " \nforward(x) [source]\n\n"}, {"name": "torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.prepare_n_shadows_model", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.prepare_n_shadows_model(model, example_inputs, qconfig_multi_mapping, backend_config, custom_prepare_fn=None, custom_prepare_kwargs=None, custom_tracer=None) [source]\n \nGiven a model with a graph with M ops such as args_kwargs_m -> op_m -> output_m And a set of N qconfigs for each op, creates a new model, with each of the subgraph of op_m transformed into      |---------> op_m_n -> log_m_n\n     |                     /\nargs_kwargs_m ---------> op_m -> log_m_0\n Where op_m_n is op_m wrapped in a submodule and transformed with qconfig_n, and its inner graph looks like args_m -------- op_m_prepared_with_qconfig_n -> out_m_n\n            /\nkwargs_m ---\n This is useful for testing different quantization of multiple layers in a single pass through the model. High level TODOs for future PRs: * figure out a better way to name the output structure * return a results data structure instead of printing it out * add examples to docblocks  Return type \nGraphModule   \n"}, {"name": "torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model", "type": "Quantization", "text": " \ntorch.ao.ns._numeric_suite_fx.print_comparisons_n_shadows_model(results) [source]\n \nPrints a summary of extracted results. \n"}, {"name": "torch.ao.ns.fx.utils.compute_cosine_similarity()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_cosine_similarity", "type": "Quantization", "text": " \ntorch.ao.ns.fx.utils.compute_cosine_similarity(x, y) [source]\n\n"}, {"name": "torch.ao.ns.fx.utils.compute_normalized_l2_error()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_normalized_l2_error", "type": "Quantization", "text": " \ntorch.ao.ns.fx.utils.compute_normalized_l2_error(x, y) [source]\n\n"}, {"name": "torch.ao.ns.fx.utils.compute_sqnr()", "path": "torch.ao.ns._numeric_suite_fx#torch.ao.ns.fx.utils.compute_sqnr", "type": "Quantization", "text": " \ntorch.ao.ns.fx.utils.compute_sqnr(x, y) [source]\n\n"}, {"name": "torch.ao.quantization.add_quant_dequant", "path": "generated/torch.ao.quantization.add_quant_dequant#torch.ao.quantization.add_quant_dequant", "type": "Quantization", "text": " \nclass torch.ao.quantization.add_quant_dequant(module) [source]\n \nWrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.  Parameters \n \nmodule \u2013 input module with qconfig attributes for all the leaf modules \nquantize (that we want to) \u2013    Returns \nEither the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.   \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.backend_config.BackendConfig(name='') [source]\n \nConfig that defines the set of patterns that can be quantized on a given backend, and how reference quantized models can be produced from these patterns. A pattern in this context refers to a module, a functional, an operator, or a directed acyclic graph of the above. Each pattern supported on the target backend can be individually configured through BackendPatternConfig in terms of:  The supported input/output activation, weight, and bias data types How observers and quant/dequant ops are inserted in order to construct the reference pattern, and (Optionally) Fusion, QAT, and reference module mappings.  The format of the patterns is described in: https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/backend_config/README.md Example usage: import torch\nfrom torch.ao.quantization.backend_config import (\n    BackendConfig,\n    BackendPatternConfig,\n    DTypeConfig,\n    ObservationType,\n)\n\nweighted_int8_dtype_config = DTypeConfig(\n    input_dtype=torch.quint8,\n    output_dtype=torch.quint8,\n    weight_dtype=torch.qint8,\n    bias_dtype=torch.float)\n\ndef fuse_conv2d_relu(is_qat, conv, relu):\n    return torch.ao.nn.intrinsic.ConvReLU2d(conv, relu)\n\n# For quantizing Linear\nlinear_config = BackendPatternConfig(torch.nn.Linear)             .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)             .add_dtype_config(weighted_int8_dtype_config)             .set_root_module(torch.nn.Linear)             .set_qat_module(torch.ao.nn.qat.Linear)             .set_reference_quantized_module(torch.ao.nn.quantized.reference.Linear)\n\n# For fusing Conv2d + ReLU into ConvReLU2d\nconv_relu_config = BackendPatternConfig((torch.nn.Conv2d, torch.nn.ReLU))             .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)             .add_dtype_config(weighted_int8_dtype_config)             .set_fused_module(torch.ao.nn.intrinsic.ConvReLU2d)             .set_fuser_method(fuse_conv2d_relu)\n\n# For quantizing ConvReLU2d\nfused_conv_relu_config = BackendPatternConfig(torch.ao.nn.intrinsic.ConvReLU2d)             .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT)             .add_dtype_config(weighted_int8_dtype_config)             .set_root_module(torch.nn.Conv2d)             .set_qat_module(torch.ao.nn.intrinsic.qat.ConvReLU2d)             .set_reference_quantized_module(torch.ao.nn.quantized.reference.Conv2d)\n\nbackend_config = BackendConfig(\"my_backend\")             .set_backend_pattern_config(linear_config)             .set_backend_pattern_config(conv_relu_config)             .set_backend_pattern_config(fused_conv_relu_config)\n  \nproperty configs: List[BackendPatternConfig]  \nReturn a copy of the list of configs set in this BackendConfig. \n  \nclassmethod from_dict(backend_config_dict) [source]\n \nCreate a BackendConfig from a dictionary with the following items: \u201cname\u201d: the name of the target backend \u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig  Return type \nBackendConfig   \n  \nset_backend_pattern_config(config) [source]\n \nSet the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.  Return type \nBackendConfig   \n  \nset_backend_pattern_configs(configs) [source]\n \nSet the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.  Return type \nBackendConfig   \n  \nset_name(name) [source]\n \nSet the name of the target backend.  Return type \nBackendConfig   \n  \nto_dict() [source]\n \nConvert this BackendConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.configs", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.configs", "type": "Quantization", "text": " \nproperty configs: List[BackendPatternConfig]  \nReturn a copy of the list of configs set in this BackendConfig. \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(backend_config_dict) [source]\n \nCreate a BackendConfig from a dictionary with the following items: \u201cname\u201d: the name of the target backend \u201cconfigs\u201d: a list of dictionaries that each represents a BackendPatternConfig  Return type \nBackendConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_config", "type": "Quantization", "text": " \nset_backend_pattern_config(config) [source]\n \nSet the config for an pattern that can be run on the target backend. This overrides any existing config for the given pattern.  Return type \nBackendConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_backend_pattern_configs", "type": "Quantization", "text": " \nset_backend_pattern_configs(configs) [source]\n \nSet the configs for patterns that can be run on the target backend. This overrides any existing config for a given pattern if it was previously registered already.  Return type \nBackendConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.set_name()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.set_name", "type": "Quantization", "text": " \nset_name(name) [source]\n \nSet the name of the target backend.  Return type \nBackendConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this BackendConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.backend_config.BackendPatternConfig(pattern=None) [source]\n \nConfig object that specifies quantization behavior for a given operator pattern. For a detailed example usage, see BackendConfig.  \nadd_dtype_config(dtype_config) [source]\n \nAdd a set of supported data types passed as arguments to quantize ops in the reference model spec.  Return type \nBackendPatternConfig   \n  \nclassmethod from_dict(backend_pattern_config_dict) [source]\n \nCreate a BackendPatternConfig from a dictionary with the following items: \u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)  Return type \nBackendPatternConfig   \n  \nset_dtype_configs(dtype_configs) [source]\n \nSet the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.  Return type \nBackendPatternConfig   \n  \nset_fused_module(fused_module) [source]\n \nSet the module that represents the fused implementation for this pattern.  Return type \nBackendPatternConfig   \n  \nset_fuser_method(fuser_method) [source]\n \nSet the function that specifies how to fuse this BackendPatternConfig\u2019s pattern. The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module. For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:  def fuse_linear_relu(is_qat, linear, relu):\n\nreturn torch.ao.nn.intrinsic.LinearReLU(linear, relu)   For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.  Return type \nBackendPatternConfig   \n  \nset_observation_type(observation_type) [source]\n \nSet how observers should be inserted in the graph for this pattern. Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output. There are two observation types: OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type. OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat. Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.  Return type \nBackendPatternConfig   \n  \nset_pattern(pattern) [source]\n \nSet the pattern to configure. The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.  Return type \nBackendPatternConfig   \n  \nset_qat_module(qat_module) [source]\n \nSet the module that represents the QAT implementation for this pattern.  Return type \nBackendPatternConfig   \n  \nset_reference_quantized_module(reference_quantized_module) [source]\n \nSet the module that represents the reference quantized implementation for this pattern\u2019s root module. For more detail, see set_root_module().  Return type \nBackendPatternConfig   \n  \nset_root_module(root_module) [source]\n \nSet the module that represents the root for this pattern. When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.  Return type \nBackendPatternConfig   \n  \nto_dict() [source]\n \nConvert this BackendPatternConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config", "type": "Quantization", "text": " \nadd_dtype_config(dtype_config) [source]\n \nAdd a set of supported data types passed as arguments to quantize ops in the reference model spec.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(backend_pattern_config_dict) [source]\n \nCreate a BackendPatternConfig from a dictionary with the following items: \u201cpattern\u201d: the pattern being configured \u201cobservation_type\u201d: the ObservationType that specifies how observers should be inserted for this pattern \u201cdtype_configs\u201d: a list of dictionaries that represents DTypeConfig s \u201croot_module\u201d: a torch.nn.Module that represents the root for this pattern \u201cqat_module\u201d: a torch.nn.Module that represents the QAT implementation for this pattern \u201creference_quantized_module\u201d: a torch.nn.Module that represents the reference quantized implementation for this pattern\u2019s root module. \u201cfused_module\u201d: a torch.nn.Module that represents the fused implementation for this pattern \u201cfuser_method\u201d: a function that specifies how to fuse the pattern for this pattern \u201cpattern_complex_format\u201d: the pattern specified in the reversed nested tuple format (deprecated)  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs", "type": "Quantization", "text": " \nset_dtype_configs(dtype_configs) [source]\n \nSet the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module", "type": "Quantization", "text": " \nset_fused_module(fused_module) [source]\n \nSet the module that represents the fused implementation for this pattern.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method", "type": "Quantization", "text": " \nset_fuser_method(fuser_method) [source]\n \nSet the function that specifies how to fuse this BackendPatternConfig\u2019s pattern. The first argument of this function should be is_qat, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module. For example, the fuser method for the pattern (torch.nn.Linear, torch.nn.ReLU) can be:  def fuse_linear_relu(is_qat, linear, relu):\n\nreturn torch.ao.nn.intrinsic.LinearReLU(linear, relu)   For a more complicated example, see https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type", "type": "Quantization", "text": " \nset_observation_type(observation_type) [source]\n \nSet how observers should be inserted in the graph for this pattern. Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output. There are two observation types: OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT (default): the output observer instance will be different from the input. This is the most common observation type. OUTPUT_SHARE_OBSERVER_WITH_INPUT: the output observer instance will be the same as the input. This is useful for operators like cat. Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern", "type": "Quantization", "text": " \nset_pattern(pattern) [source]\n \nSet the pattern to configure. The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module", "type": "Quantization", "text": " \nset_qat_module(qat_module) [source]\n \nSet the module that represents the QAT implementation for this pattern.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module", "type": "Quantization", "text": " \nset_reference_quantized_module(reference_quantized_module) [source]\n \nSet the module that represents the reference quantized implementation for this pattern\u2019s root module. For more detail, see set_root_module().  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module", "type": "Quantization", "text": " \nset_root_module(root_module) [source]\n \nSet the module that represents the root for this pattern. When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.  Return type \nBackendPatternConfig   \n"}, {"name": "torch.ao.quantization.backend_config.BackendPatternConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.backendpatternconfig#torch.ao.quantization.backend_config.BackendPatternConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this BackendPatternConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.backend_config.DTypeConfig", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.backend_config.DTypeConfig(input_dtype=None, output_dtype=None, weight_dtype=None, bias_dtype=None, is_dynamic=None) [source]\n \nConfig object that specifies the supported data types passed as arguments to quantize ops in the reference model spec, for input and output activations, weights, and biases. For example, consider the following reference model: quant1 - [dequant1 - fp32_linear - quant2] - dequant2 The pattern in the square brackets refers to the reference pattern of statically quantized linear. Setting the input dtype as torch.quint8 in the DTypeConfig means we pass in torch.quint8 as the dtype argument to the first quantize op (quant1). Similarly, setting the output dtype as torch.quint8 means we pass in torch.quint8 as the dtype argument to the second quantize op (quant2). Note that the dtype here does not refer to the interface dtypes of the op. For example, the \u201cinput dtype\u201d here is not the dtype of the input tensor passed to the quantized linear op. Though it can still be the same as the interface dtype, this is not always the case, e.g. the interface dtype is fp32 in dynamic quantization but the \u201cinput dtype\u201d specified in the DTypeConfig would still be quint8. The semantics of dtypes here are the same as the semantics of the dtypes specified in the observers. These dtypes are matched against the ones specified in the user\u2019s QConfig. If there is a match, and the QConfig satisfies the constraints specified in the DTypeConfig (if any), then we will quantize the given pattern using this DTypeConfig. Otherwise, the QConfig is ignored and the pattern will not be quantized. Example usage: >>> dtype_config1 = DTypeConfig(\n...     input_dtype=torch.quint8,\n...     output_dtype=torch.quint8,\n...     weight_dtype=torch.qint8,\n...     bias_dtype=torch.float)\n\n>>> dtype_config2 = DTypeConfig(\n...     input_dtype=DTypeWithConstraints(\n...         dtype=torch.quint8,\n...         quant_min_lower_bound=0,\n...         quant_max_upper_bound=255,\n...     ),\n...     output_dtype=DTypeWithConstraints(\n...         dtype=torch.quint8,\n...         quant_min_lower_bound=0,\n...         quant_max_upper_bound=255,\n...     ),\n...     weight_dtype=DTypeWithConstraints(\n...         dtype=torch.qint8,\n...         quant_min_lower_bound=-128,\n...         quant_max_upper_bound=127,\n...     ),\n...     bias_dtype=torch.float)\n\n>>> dtype_config1.input_dtype\ntorch.quint8\n\n>>> dtype_config2.input_dtype\ntorch.quint8\n\n>>> dtype_config2.input_dtype_with_constraints\nDTypeWithConstraints(dtype=torch.quint8, quant_min_lower_bound=0, quant_max_upper_bound=255, scale_min_lower_bound=None, scale_max_upper_bound=None)\n  \nclassmethod from_dict(dtype_config_dict) [source]\n \n \nCreate a DTypeConfig from a dictionary with the following items (all optional): \n\n\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool    Return type \nDTypeConfig   \n  \nto_dict() [source]\n \nConvert this DTypeConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.from_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(dtype_config_dict) [source]\n \n \nCreate a DTypeConfig from a dictionary with the following items (all optional): \n\n\u201cinput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201coutput_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cweight_dtype\u201d: torch.dtype or DTypeWithConstraints \u201cbias_type\u201d: torch.dtype \u201cis_dynamic\u201d: bool    Return type \nDTypeConfig   \n"}, {"name": "torch.ao.quantization.backend_config.DTypeConfig.to_dict()", "path": "generated/torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this DTypeConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.backend_config.DTypeWithConstraints", "path": "generated/torch.ao.quantization.backend_config.dtypewithconstraints#torch.ao.quantization.backend_config.DTypeWithConstraints", "type": "Quantization", "text": " \nclass torch.ao.quantization.backend_config.DTypeWithConstraints(dtype=None, quant_min_lower_bound=None, quant_max_upper_bound=None, scale_min_lower_bound=None, scale_max_upper_bound=None, scale_exact_match=None, zero_point_exact_match=None) [source]\n \nConfig for specifying additional constraints for a given dtype, such as quantization value ranges, scale value ranges, and fixed quantization params, to be used in DTypeConfig. The constraints currently supported are:  \nquant_min_lower_bound and quant_max_upper_bound: Lower and upper bounds for the minimum and maximum quantized values respectively. If the QConfig\u2019s quant_min and quant_max fall outside this range, then the QConfig will be ignored. \nscale_min_lower_bound and scale_max_upper_bound: Lower and upper bounds for the minimum and maximum scale values respectively. If the QConfig\u2019s minimum scale value (currently exposed as eps) falls below the lower bound, then the QConfig will be ignored. Note that the upper bound is currently not enforced. \nscale_exact_match and zero_point_exact_match: Exact match requirements for scale and zero point, to be used for operators with fixed quantization parameters such as sigmoid and tanh. If the observer specified in the QConfig is neither FixedQParamsObserver nor FixedQParamsFakeQuantize, or if the quantization parameters don\u2019t match, then the QConfig will be ignored.  \n"}, {"name": "torch.ao.quantization.backend_config.ObservationType", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType", "type": "Quantization", "text": " \nclass torch.ao.quantization.backend_config.ObservationType(value) [source]\n \nAn enum that represents different ways of how an operator/operator pattern should be observed  \nINPUT_OUTPUT_NOT_OBSERVED = 2  \nthis means the input and output are never observed example: x.shape, x.size \n  \nOUTPUT_SHARE_OBSERVER_WITH_INPUT = 1  \nthis means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool \n  \nOUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT = 0  \nthis means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax \n \n"}, {"name": "torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.INPUT_OUTPUT_NOT_OBSERVED", "type": "Quantization", "text": " \nINPUT_OUTPUT_NOT_OBSERVED = 2  \nthis means the input and output are never observed example: x.shape, x.size \n"}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_SHARE_OBSERVER_WITH_INPUT", "type": "Quantization", "text": " \nOUTPUT_SHARE_OBSERVER_WITH_INPUT = 1  \nthis means the output will use the same observer instance as input, based on qconfig.activation example: torch.cat, maxpool \n"}, {"name": "torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "path": "generated/torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT", "type": "Quantization", "text": " \nOUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT = 0  \nthis means input and output are observed with different observers, based on qconfig.activation example: conv, linear, softmax \n"}, {"name": "torch.ao.quantization.convert", "path": "generated/torch.ao.quantization.convert#torch.ao.quantization.convert", "type": "Quantization", "text": " \nclass torch.ao.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, is_reference=False, convert_custom_config_dict=None) [source]\n \nConverts submodules in input module to a different module according to mapping by calling from_float method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.  Parameters \n \nmodule \u2013 prepared and calibrated module \nmapping \u2013 a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nconvert_custom_config_dict \u2013 custom configuration dictionary for convert function    # Example of convert_custom_config_dict:\nconvert_custom_config_dict = {\n    # user will manually define the corresponding quantized\n    # module class which has a from_observed class method that converts\n    # observed custom module to quantized custom module\n    \"observed_to_quantized_custom_module_class\": {\n        ObservedCustomModule: QuantizedCustomModule\n    }\n}\n \n"}, {"name": "torch.ao.quantization.default_eval_fn", "path": "generated/torch.ao.quantization.default_eval_fn#torch.ao.quantization.default_eval_fn", "type": "Quantization", "text": " \nclass torch.ao.quantization.default_eval_fn(model, calib_data) [source]\n \nDefault evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset \n"}, {"name": "torch.ao.quantization.DeQuantStub", "path": "generated/torch.ao.quantization.dequantstub#torch.ao.quantization.DeQuantStub", "type": "Quantization", "text": " \nclass torch.ao.quantization.DeQuantStub(qconfig=None) [source]\n \nDequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert.  Parameters \nqconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules   \n"}, {"name": "torch.ao.quantization.fake_quantize.default_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fake_quant#torch.ao.quantization.fake_quantize.default_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_fake_quant  \nDefault fake_quant for activations. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_act_fake_quant#torch.ao.quantization.fake_quantize.default_fused_act_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_fused_act_fake_quant  \nFused version of default_fake_quant, with improved performance. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_fused_per_channel_wt_fake_quant  \nFused version of default_per_channel_weight_fake_quant, with improved performance. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant#torch.ao.quantization.fake_quantize.default_fused_wt_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_fused_wt_fake_quant  \nFused version of default_weight_fake_quant, with improved performance. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_histogram_fake_quant#torch.ao.quantization.fake_quantize.default_histogram_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_histogram_fake_quant  \nFake_quant for activations using a histogram.. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant#torch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_per_channel_weight_fake_quant  \nDefault fake_quant for per-channel weights. Observer is memoryless since averaging_constant is 1. \n"}, {"name": "torch.ao.quantization.fake_quantize.default_weight_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.default_weight_fake_quant#torch.ao.quantization.fake_quantize.default_weight_fake_quant", "type": "Quantization", "text": " \ntorch.ao.quantization.fake_quantize.default_weight_fake_quant  \nDefault fake_quant for weights. Observer is memoryless since averaging_constant is 1. \n"}, {"name": "torch.ao.quantization.fake_quantize.disable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.disable_fake_quant#torch.ao.quantization.fake_quantize.disable_fake_quant", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.disable_fake_quant(mod) [source]\n \nDisable fake quantization for this module, if applicable. Example usage: # model is any PyTorch model\nmodel.apply(torch.ao.quantization.disable_fake_quant)\n \n"}, {"name": "torch.ao.quantization.fake_quantize.disable_observer", "path": "generated/torch.ao.quantization.fake_quantize.disable_observer#torch.ao.quantization.fake_quantize.disable_observer", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.disable_observer(mod) [source]\n \nDisable observation for this module, if applicable. Example usage: # model is any PyTorch model\nmodel.apply(torch.ao.quantization.disable_observer)\n \n"}, {"name": "torch.ao.quantization.fake_quantize.enable_fake_quant", "path": "generated/torch.ao.quantization.fake_quantize.enable_fake_quant#torch.ao.quantization.fake_quantize.enable_fake_quant", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.enable_fake_quant(mod) [source]\n \nEnable fake quantization for this module, if applicable. Example usage: # model is any PyTorch model\nmodel.apply(torch.ao.quantization.enable_fake_quant)\n \n"}, {"name": "torch.ao.quantization.fake_quantize.enable_observer", "path": "generated/torch.ao.quantization.fake_quantize.enable_observer#torch.ao.quantization.fake_quantize.enable_observer", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.enable_observer(mod) [source]\n \nEnable observation for this module, if applicable. Example usage: # model is any PyTorch model\nmodel.apply(torch.ao.quantization.enable_observer)\n \n"}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fakequantize#torch.ao.quantization.fake_quantize.FakeQuantize", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.FakeQuantize(observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=None, quant_max=None, **observer_kwargs) [source]\n \nSimulate the quantize and dequantize operations in training time. The output of this module is given by: x_out = (\n  clamp(round(x/scale + zero_point), quant_min, quant_max) - zero_point\n) * scale\n  \nscale defines the scale factor used for quantization. \nzero_point specifies the quantized value to which 0 in floating point maps to \nfake_quant_enabled controls the application of fake quantization on tensors, note that statistics can still be updated. \nobserver_enabled controls statistics collection on tensors \n \ndtype specifies the quantized dtype that is being emulated with fake-quantization, \n\nallowable values are torch.qint8 and torch.quint8.      Parameters \n \nobserver (module) \u2013 Module for observing statistics on input tensors and calculating scale and zero-point. \nobserver_kwargs (optional) \u2013 Arguments for the observer module   Variables \nactivation_post_process (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.   \n"}, {"name": "torch.ao.quantization.fake_quantize.FakeQuantizeBase", "path": "generated/torch.ao.quantization.fake_quantize.fakequantizebase#torch.ao.quantization.fake_quantize.FakeQuantizeBase", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.FakeQuantizeBase [source]\n \nBase fake quantize module Any fake quantize implementation should derive from this class. Concrete fake quantize module should follow the same API. In forward, they will update the statistics of the observed Tensor and fake quantize the input. They should also provide a calculate_qparams function that computes the quantization parameters given the collected statistics. \n"}, {"name": "torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fixedqparamsfakequantize#torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize(observer) [source]\n \nSimulate quantize and dequantize with fixed quantization parameters in training time. Only per tensor quantization is supported. \n"}, {"name": "torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "path": "generated/torch.ao.quantization.fake_quantize.fusedmovingavgobsfakequantize#torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize", "type": "Quantization", "text": " \nclass torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize(observer=<class 'torch.ao.quantization.observer.MovingAverageMinMaxObserver'>, quant_min=0, quant_max=255, **observer_kwargs) [source]\n \nFused module that is used to observe the input tensor (compute min/max), compute scale/zero_point and fake_quantize the tensor. This module uses calculation similar MovingAverageMinMaxObserver for the inputs, to compute the min/max values in order to compute the scale/zero_point. The qscheme input in the observer is used to differentiate between symmetric/affine quantization scheme. The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale Similar to FakeQuantize, and accepts the same attributes as the base class. \n"}, {"name": "torch.ao.quantization.fuse_modules", "path": "generated/torch.ao.quantization.fuse_modules#torch.ao.quantization.fuse_modules", "type": "Quantization", "text": " \nclass torch.ao.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=<function fuse_known_modules>, fuse_custom_config_dict=None) [source]\n \nFuses a list of modules into a single module Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.  Parameters \n \nmodel \u2013 Model containing the modules to be fused \nmodules_to_fuse \u2013 list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse. \ninplace \u2013 bool specifying if fusion happens in place on the model, by default a new model is returned \nfuser_func \u2013 Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.ao.quantization.fuse_known_modules \nfuse_custom_config_dict \u2013 custom configuration for fusion    # Example of fuse_custom_config_dict\nfuse_custom_config_dict = {\n    # Additional fuser_method mapping\n    \"additional_fuser_method_mapping\": {\n        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn\n    },\n}\n  Returns \nmodel with fused modules. A new copy is created if inplace=True.   Examples: >>> m = M().eval()\n>>> # m is a module containing the sub-modules below\n>>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n>>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n\n>>> m = M().eval()\n>>> # Alternately provide a single list of modules to fuse\n>>> modules_to_fuse = ['conv1', 'bn1', 'relu1']\n>>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)\n>>> output = fused_m(input)\n \n"}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.fx.custom_config.ConvertCustomConfig [source]\n \nCustom configuration for convert_fx(). Example usage: convert_custom_config = ConvertCustomConfig()             .set_observed_to_quantized_mapping(ObservedCustomModule, QuantizedCustomModule)             .set_preserved_attributes([\"attr1\", \"attr2\"])\n  \nclassmethod from_dict(convert_custom_config_dict) [source]\n \nCreate a ConvertCustomConfig from a dictionary with the following items: \u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nConvertCustomConfig   \n  \nset_observed_to_quantized_mapping(observed_class, quantized_class, quant_type=QuantType.STATIC) [source]\n \nSet the mapping from a custom observed module class to a custom quantized module class. The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.  Return type \nConvertCustomConfig   \n  \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nConvertCustomConfig   \n  \nto_dict() [source]\n \nConvert this ConvertCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(convert_custom_config_dict) [source]\n \nCreate a ConvertCustomConfig from a dictionary with the following items: \u201cobserved_to_quantized_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from observed module classes to quantized module classes, e.g.:: { \u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cdynamic\u201d: {FloatCustomModule: ObservedCustomModule}, \u201cweight_only\u201d: {FloatCustomModule: ObservedCustomModule} } \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nConvertCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_observed_to_quantized_mapping", "type": "Quantization", "text": " \nset_observed_to_quantized_mapping(observed_class, quantized_class, quant_type=QuantType.STATIC) [source]\n \nSet the mapping from a custom observed module class to a custom quantized module class. The quantized module class must have a from_observed class method that converts the observed module class to the quantized module class.  Return type \nConvertCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.set_preserved_attributes", "type": "Quantization", "text": " \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nConvertCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.convertcustomconfig#torch.ao.quantization.fx.custom_config.ConvertCustomConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this ConvertCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.fx.custom_config.FuseCustomConfig [source]\n \nCustom configuration for fuse_fx(). Example usage: fuse_custom_config = FuseCustomConfig().set_preserved_attributes([\"attr1\", \"attr2\"])\n  \nclassmethod from_dict(fuse_custom_config_dict) [source]\n \nCreate a ConvertCustomConfig from a dictionary with the following items: \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nFuseCustomConfig   \n  \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nFuseCustomConfig   \n  \nto_dict() [source]\n \nConvert this FuseCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(fuse_custom_config_dict) [source]\n \nCreate a ConvertCustomConfig from a dictionary with the following items: \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nFuseCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.set_preserved_attributes", "type": "Quantization", "text": " \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nFuseCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.fusecustomconfig#torch.ao.quantization.fx.custom_config.FuseCustomConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this FuseCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.fx.custom_config.PrepareCustomConfig [source]\n \nCustom configuration for prepare_fx() and prepare_qat_fx(). Example usage: prepare_custom_config = PrepareCustomConfig()             .set_standalone_module_name(\"module1\", qconfig_mapping, example_inputs,                 child_prepare_custom_config, backend_config)             .set_standalone_module_class(MyStandaloneModule, qconfig_mapping, example_inputs,                 child_prepare_custom_config, backend_config)             .set_float_to_observed_mapping(FloatCustomModule, ObservedCustomModule)             .set_non_traceable_module_names([\"module2\", \"module3\"])             .set_non_traceable_module_classes([NonTraceableModule1, NonTraceableModule2])             .set_input_quantized_indexes([0])             .set_output_quantized_indexes([0])             .set_preserved_attributes([\"attr1\", \"attr2\"])\n  \nclassmethod from_dict(prepare_custom_config_dict) [source]\n \nCreate a PrepareCustomConfig from a dictionary with the following items: \u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples \u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples \u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}} \u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nPrepareCustomConfig   \n  \nset_float_to_observed_mapping(float_class, observed_class, quant_type=QuantType.STATIC) [source]\n \nSet the mapping from a custom float module class to a custom observed module class. The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.  Return type \nPrepareCustomConfig   \n  \nset_input_quantized_indexes(indexes) [source]\n \nSet the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.  Return type \nPrepareCustomConfig   \n  \nset_non_traceable_module_classes(module_classes) [source]\n \nSet the modules that are not symbolically traceable, identified by class.  Return type \nPrepareCustomConfig   \n  \nset_non_traceable_module_names(module_names) [source]\n \nSet the modules that are not symbolically traceable, identified by name.  Return type \nPrepareCustomConfig   \n  \nset_output_quantized_indexes(indexes) [source]\n \nSet the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.  Return type \nPrepareCustomConfig   \n  \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nPrepareCustomConfig   \n  \nset_standalone_module_class(module_class, qconfig_mapping, example_inputs, prepare_custom_config, backend_config) [source]\n \nSet the configuration for running a standalone module identified by module_class. If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.  Return type \nPrepareCustomConfig   \n  \nset_standalone_module_name(module_name, qconfig_mapping, example_inputs, prepare_custom_config, backend_config) [source]\n \nSet the configuration for running a standalone module identified by module_name. If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.  Return type \nPrepareCustomConfig   \n  \nto_dict() [source]\n \nConvert this PrepareCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(prepare_custom_config_dict) [source]\n \nCreate a PrepareCustomConfig from a dictionary with the following items: \u201cstandalone_module_name\u201d: a list of (module_name, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples \u201cstandalone_module_class\u201d a list of (module_class, qconfig_mapping, example_inputs, child_prepare_custom_config, backend_config) tuples \u201cfloat_to_observed_custom_module_class\u201d: a nested dictionary mapping from quantization mode to an inner mapping from float module classes to observed module classes, e.g. {\u201cstatic\u201d: {FloatCustomModule: ObservedCustomModule}} \u201cnon_traceable_module_name\u201d: a list of modules names that are not symbolically traceable \u201cnon_traceable_module_class\u201d: a list of module classes that are not symbolically traceable \u201cinput_quantized_idxs\u201d: a list of indexes of graph inputs that should be quantized \u201coutput_quantized_idxs\u201d: a list of indexes of graph outputs that should be quantized \u201cpreserved_attributes\u201d: a list of attributes that persist even if they are not used in forward This function is primarily for backward compatibility and may be removed in the future.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_float_to_observed_mapping", "type": "Quantization", "text": " \nset_float_to_observed_mapping(float_class, observed_class, quant_type=QuantType.STATIC) [source]\n \nSet the mapping from a custom float module class to a custom observed module class. The observed module class must have a from_float class method that converts the float module class to the observed module class. This is currently only supported for static quantization.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_input_quantized_indexes", "type": "Quantization", "text": " \nset_input_quantized_indexes(indexes) [source]\n \nSet the indexes of the inputs of the graph that should be quantized. Inputs are otherwise assumed to be in fp32 by default instead.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_classes", "type": "Quantization", "text": " \nset_non_traceable_module_classes(module_classes) [source]\n \nSet the modules that are not symbolically traceable, identified by class.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_non_traceable_module_names", "type": "Quantization", "text": " \nset_non_traceable_module_names(module_names) [source]\n \nSet the modules that are not symbolically traceable, identified by name.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_output_quantized_indexes", "type": "Quantization", "text": " \nset_output_quantized_indexes(indexes) [source]\n \nSet the indexes of the outputs of the graph that should be quantized. Outputs are otherwise assumed to be in fp32 by default instead.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_preserved_attributes", "type": "Quantization", "text": " \nset_preserved_attributes(attributes) [source]\n \nSet the names of the attributes that will persist in the graph module even if they are not used in the model\u2019s forward method.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_class", "type": "Quantization", "text": " \nset_standalone_module_class(module_class, qconfig_mapping, example_inputs, prepare_custom_config, backend_config) [source]\n \nSet the configuration for running a standalone module identified by module_class. If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.set_standalone_module_name", "type": "Quantization", "text": " \nset_standalone_module_name(module_name, qconfig_mapping, example_inputs, prepare_custom_config, backend_config) [source]\n \nSet the configuration for running a standalone module identified by module_name. If qconfig_mapping is None, the parent qconfig_mapping will be used instead. If prepare_custom_config is None, an empty PrepareCustomConfig will be used. If backend_config is None, the parent backend_config will be used instead.  Return type \nPrepareCustomConfig   \n"}, {"name": "torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict()", "path": "generated/torch.ao.quantization.fx.custom_config.preparecustomconfig#torch.ao.quantization.fx.custom_config.PrepareCustomConfig.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this PrepareCustomConfig to a dictionary with the items described in from_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "path": "generated/torch.ao.quantization.fx.custom_config.standalonemoduleconfigentry#torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry", "type": "Quantization", "text": " \nclass torch.ao.quantization.fx.custom_config.StandaloneModuleConfigEntry(qconfig_mapping: 'Optional[QConfigMapping]', example_inputs: 'Tuple[Any, ...]', prepare_custom_config: 'Optional[PrepareCustomConfig]', backend_config: 'Optional[BackendConfig]') [source]\n\n"}, {"name": "torch.ao.quantization.observer.default_debug_observer", "path": "generated/torch.ao.quantization.observer.default_debug_observer#torch.ao.quantization.observer.default_debug_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_debug_observer  \nDefault debug-only observer. \n"}, {"name": "torch.ao.quantization.observer.default_dynamic_quant_observer", "path": "generated/torch.ao.quantization.observer.default_dynamic_quant_observer#torch.ao.quantization.observer.default_dynamic_quant_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_dynamic_quant_observer  \nDefault observer for dynamic quantization. \n"}, {"name": "torch.ao.quantization.observer.default_float_qparams_observer", "path": "generated/torch.ao.quantization.observer.default_float_qparams_observer#torch.ao.quantization.observer.default_float_qparams_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_float_qparams_observer  \nDefault observer for a floating point zero-point. \n"}, {"name": "torch.ao.quantization.observer.default_histogram_observer", "path": "generated/torch.ao.quantization.observer.default_histogram_observer#torch.ao.quantization.observer.default_histogram_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_histogram_observer  \nDefault histogram observer, usually used for PTQ. \n"}, {"name": "torch.ao.quantization.observer.default_observer", "path": "generated/torch.ao.quantization.observer.default_observer#torch.ao.quantization.observer.default_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_observer  \nDefault observer for static quantization, usually used for debugging. \n"}, {"name": "torch.ao.quantization.observer.default_per_channel_weight_observer", "path": "generated/torch.ao.quantization.observer.default_per_channel_weight_observer#torch.ao.quantization.observer.default_per_channel_weight_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_per_channel_weight_observer  \nDefault per-channel weight observer, usually used on backends where per-channel weight quantization is supported, such as fbgemm. \n"}, {"name": "torch.ao.quantization.observer.default_placeholder_observer", "path": "generated/torch.ao.quantization.observer.default_placeholder_observer#torch.ao.quantization.observer.default_placeholder_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_placeholder_observer  \nDefault placeholder observer, usually used for quantization to torch.float16. \n"}, {"name": "torch.ao.quantization.observer.default_weight_observer", "path": "generated/torch.ao.quantization.observer.default_weight_observer#torch.ao.quantization.observer.default_weight_observer", "type": "Quantization", "text": " \ntorch.ao.quantization.observer.default_weight_observer  \nDefault weight observer. \n"}, {"name": "torch.ao.quantization.observer.get_observer_state_dict", "path": "generated/torch.ao.quantization.observer.get_observer_state_dict#torch.ao.quantization.observer.get_observer_state_dict", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.get_observer_state_dict(mod) [source]\n \nReturns the state dict corresponding to the observer stats. Traverse the model state_dict and extract out the stats. \n"}, {"name": "torch.ao.quantization.observer.HistogramObserver", "path": "generated/torch.ao.quantization.observer.histogramobserver#torch.ao.quantization.observer.HistogramObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07) [source]\n \nThe module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.  Parameters \n \nbins (int) \u2013 Number of bins to use for the histogram \nupsample_rate (int) \u2013 Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations \ndtype (dtype) \u2013 dtype argument to the quantize node needed to implement the reference model spec \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \neps (Tensor) \u2013 Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.    The scale and zero point are computed as follows:  \n Create the histogram of the incoming inputs.\n\nThe histogram is computed continuously, and the ranges per bin change with every new tensor observed.    \n Search the distribution in the histogram for optimal min/max values.\n\nThe search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.    \n Compute the scale and zero point the same way as in the\n\nMinMaxObserver     \n"}, {"name": "torch.ao.quantization.observer.load_observer_state_dict", "path": "generated/torch.ao.quantization.observer.load_observer_state_dict#torch.ao.quantization.observer.load_observer_state_dict", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.load_observer_state_dict(mod, obs_dict) [source]\n \nGiven input model and a state_dict containing model observer stats, load the stats back into the model. The observer state_dict can be saved using torch.ao.quantization.get_observer_state_dict \n"}, {"name": "torch.ao.quantization.observer.MinMaxObserver", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07) [source]\n \nObserver module for computing the quantization parameters based on the running min and max values. This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \ndtype \u2013 dtype argument to the quantize node needed to implement the reference model spec. \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup. \neps (Tensor) \u2013 Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.    Given running min/max as xminx_\\text{min} and xmaxx_\\text{max}, scale ss and zero point zz are computed as: The running minimum/maximum xmin/maxx_\\text{min/max} is computed as:  xmin={min\u2061(X)if xmin=Nonemin\u2061(xmin,min\u2061(X))otherwisexmax={max\u2061(X)if xmax=Nonemax\u2061(xmax,max\u2061(X))otherwise\\begin{array}{ll} x_\\text{min} &= \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ \\min\\left(x_\\text{min}, \\min(X)\\right) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} &= \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ \\max\\left(x_\\text{max}, \\max(X)\\right) & \\text{otherwise} \\end{cases}\\\\ \\end{array}\n\nwhere XX is the observed tensor. The scale ss and zero point zz are then computed as:  if Symmetric:s=2max\u2061(\u2223xmin\u2223,xmax)/(Qmax\u2212Qmin)z={0if dtype is qint8128otherwiseOtherwise:s=(xmax\u2212xmin)/(Qmax\u2212Qmin)z=Qmin\u2212round(xmin/s)\\begin{aligned} \\text{if Symmetric:}&\\\\ &s = 2 \\max(|x_\\text{min}|, x_\\text{max}) / \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\ &z = \\begin{cases} 0 & \\text{if dtype is qint8} \\\\ 128 & \\text{otherwise} \\end{cases}\\\\ \\text{Otherwise:}&\\\\ &s = \\left( x_\\text{max} - x_\\text{min} \\right ) / \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\ &z = Q_\\text{min} - \\text{round}(x_\\text{min} / s) \\end{aligned}\n\nwhere QminQ_\\text{min} and QmaxQ_\\text{max} are the minimum and maximum of the quantized data type.  Warning dtype can only take torch.qint8 or torch.quint8.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.   \ncalculate_qparams() [source]\n \nCalculates the quantization parameters. \n  \nforward(x_orig) [source]\n \nRecords the running minimum and maximum of x. \n  \nreset_min_max_vals() [source]\n \nResets the min/max values. \n \n"}, {"name": "torch.ao.quantization.observer.MinMaxObserver.calculate_qparams()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.calculate_qparams", "type": "Quantization", "text": " \ncalculate_qparams() [source]\n \nCalculates the quantization parameters. \n"}, {"name": "torch.ao.quantization.observer.MinMaxObserver.forward()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.forward", "type": "Quantization", "text": " \nforward(x_orig) [source]\n \nRecords the running minimum and maximum of x. \n"}, {"name": "torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.minmaxobserver#torch.ao.quantization.observer.MinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": " \nreset_min_max_vals() [source]\n \nResets the min/max values. \n"}, {"name": "torch.ao.quantization.observer.MovingAverageMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageminmaxobserver#torch.ao.quantization.observer.MovingAverageMinMaxObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None, eps=1.1920928955078125e-07, **kwargs) [source]\n \nObserver module for computing the quantization parameters based on the moving average of the min and max values. This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \ndtype \u2013 dtype argument to the quantize node needed to implement the reference model spec. \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup. \neps (Tensor) \u2013 Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.    The moving average min/max is computed as follows  xmin={min\u2061(X)if xmin=None(1\u2212c)xmin+cmin\u2061(X)otherwisexmax={max\u2061(X)if xmax=None(1\u2212c)xmax+cmax\u2061(X)otherwise\\begin{array}{ll} x_\\text{min} = \\begin{cases} \\min(X) & \\text{if~}x_\\text{min} = \\text{None} \\\\ (1 - c) x_\\text{min} + c \\min(X) & \\text{otherwise} \\end{cases}\\\\ x_\\text{max} = \\begin{cases} \\max(X) & \\text{if~}x_\\text{max} = \\text{None} \\\\ (1 - c) x_\\text{max} + c \\max(X) & \\text{otherwise} \\end{cases}\\\\ \\end{array}\n\nwhere xmin/maxx_\\text{min/max} is the running average min/max, XX is is the incoming tensor, and cc is the averaging_constant. The scale and zero point are then computed as in MinMaxObserver.  Note Only works with torch.per_tensor_affine quantization scheme.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  \n"}, {"name": "torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.movingaverageperchannelminmaxobserver#torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None, eps=1.1920928955078125e-07, **kwargs) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \naveraging_constant \u2013 Averaging constant for min/max. \nch_axis \u2013 Channel axis \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup. \neps (Tensor) \u2013 Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.    The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  \n"}, {"name": "torch.ao.quantization.observer.NoopObserver", "path": "generated/torch.ao.quantization.observer.noopobserver#torch.ao.quantization.observer.NoopObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.NoopObserver(dtype=torch.float16, custom_op_name='') [source]\n \nObserver that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float(). Primarily used for quantization to float16 which doesn\u2019t require determining ranges.  Parameters \n \ndtype \u2013 Quantized data type \ncustom_op_name \u2013 (temporary) specify this observer for an operator that doesn\u2019t require any observation (Can be used in Graph Mode Passes for special case ops).    \n"}, {"name": "torch.ao.quantization.observer.ObserverBase", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.ObserverBase(dtype) [source]\n \nBase observer Module. Any observer implementation should derive from this class. Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a calculate_qparams function that computes the quantization parameters given the collected statistics.  Parameters \ndtype \u2013 dtype argument to the quantize node needed to implement the reference model spec.    \nclassmethod with_args(**kwargs)  \nWrapper that allows creation of class factories. This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args Example: >>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n \n  \nclassmethod with_callable_args(**kwargs)  \nWrapper that allows creation of class factories args that need to be called at construction time. This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args Example: >>> Foo.with_callable_args = classmethod(_with_callable_args)\n>>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n>>> foo_instance1 = foo_builder()\n>>> # wait 50\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\nFalse\n \n \n"}, {"name": "torch.ao.quantization.observer.ObserverBase.with_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_args", "type": "Quantization", "text": " \nclassmethod with_args(**kwargs)  \nWrapper that allows creation of class factories. This can be useful when there is a need to create classes with the same constructor arguments, but different instances. Can be used in conjunction with _callable_args Example: >>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)\n>>> foo_instance1 = foo_builder()\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1) == id(foo_instance2)\nFalse\n \n"}, {"name": "torch.ao.quantization.observer.ObserverBase.with_callable_args()", "path": "generated/torch.ao.quantization.observer.observerbase#torch.ao.quantization.observer.ObserverBase.with_callable_args", "type": "Quantization", "text": " \nclassmethod with_callable_args(**kwargs)  \nWrapper that allows creation of class factories args that need to be called at construction time. This can be useful when there is a need to create classes with the same constructor arguments, but different instances and those arguments should only be calculated at construction time. Can be used in conjunction with _with_args Example: >>> Foo.with_callable_args = classmethod(_with_callable_args)\n>>> Foo.with_args = classmethod(_with_args)\n>>> foo_builder = Foo.with_callable_args(cur_time=get_time_func).with_args(name=\"dan\")\n>>> foo_instance1 = foo_builder()\n>>> # wait 50\n>>> foo_instance2 = foo_builder()\n>>> id(foo_instance1.creation_time) == id(foo_instance2.creation_time)\nFalse\n \n"}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None, factory_kwargs=None, eps=1.1920928955078125e-07) [source]\n \nObserver module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters \n \nch_axis \u2013 Channel axis \ndtype \u2013 dtype argument to the quantize node needed to implement the reference model spec. \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit \nquant_min \u2013 Minimum quantization value. If unspecified, it will follow the 8-bit setup. \nquant_max \u2013 Maximum quantization value. If unspecified, it will follow the 8-bit setup. \neps (Tensor) \u2013 Epsilon value for float32, Defaults to torch.finfo(torch.float32).eps.    The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.   \nreset_min_max_vals() [source]\n \nResets the min/max values. \n \n"}, {"name": "torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals()", "path": "generated/torch.ao.quantization.observer.perchannelminmaxobserver#torch.ao.quantization.observer.PerChannelMinMaxObserver.reset_min_max_vals", "type": "Quantization", "text": " \nreset_min_max_vals() [source]\n \nResets the min/max values. \n"}, {"name": "torch.ao.quantization.observer.PlaceholderObserver", "path": "generated/torch.ao.quantization.observer.placeholderobserver#torch.ao.quantization.observer.PlaceholderObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.PlaceholderObserver(dtype=torch.float32, custom_op_name='', compute_dtype=None, quant_min=None, quant_max=None, qscheme=None, eps=None, is_dynamic=False) [source]\n \nObserver that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float(). Can be used for quantization to float16 which doesn\u2019t require determining ranges.  Parameters \n \ndtype \u2013 dtype argument to the quantize node needed to implement the reference model spec. \nquant_min \u2013 minimum value in quantized domain (TODO: align behavior with other observers) \nquant_max \u2013 maximum value in quantized domain \ncustom_op_name \u2013 (temporary) specify this observer for an operator that doesn\u2019t require any observation (Can be used in Graph Mode Passes for special case ops). \ncompute_dtype (deprecated) \u2013 if set, marks the future quantize function to use dynamic quantization instead of static quantization. This field is deprecated, use is_dynamic=True instead. \nis_dynamic \u2013 if True, the quantize function in the reference model representation taking stats from this observer instance will use dynamic quantization.    \n"}, {"name": "torch.ao.quantization.observer.RecordingObserver", "path": "generated/torch.ao.quantization.observer.recordingobserver#torch.ao.quantization.observer.RecordingObserver", "type": "Quantization", "text": " \nclass torch.ao.quantization.observer.RecordingObserver(dtype=torch.quint8, **kwargs) [source]\n \nThe module is mainly for debug and records the tensor values during runtime.  Parameters \n \ndtype \u2013 Quantized data type \nqscheme \u2013 Quantization scheme to be used \nreduce_range \u2013 Reduces the range of the quantized data type by 1 bit    \n"}, {"name": "torch.ao.quantization.prepare", "path": "generated/torch.ao.quantization.prepare#torch.ao.quantization.prepare", "type": "Quantization", "text": " \nclass torch.ao.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute. The model will be attached with observer or fake quant modules, and qconfig will be propagated.  Parameters \n \nmodel \u2013 input model to be modified in-place \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nallow_list \u2013 list of quantizable modules \nobserver_non_leaf_module_list \u2013 list of non-leaf modules we want to add observer \nprepare_custom_config_dict \u2013 customization configuration dictionary for prepare function    # Example of prepare_custom_config_dict:\nprepare_custom_config_dict = {\n    # user will manually define the corresponding observed\n    # module class which has a from_float class method that converts\n    # float custom module to observed custom module\n    \"float_to_observed_custom_module_class\": {\n        CustomModule: ObservedCustomModule\n    }\n }\n \n"}, {"name": "torch.ao.quantization.prepare_qat", "path": "generated/torch.ao.quantization.prepare_qat#torch.ao.quantization.prepare_qat", "type": "Quantization", "text": " \nclass torch.ao.quantization.prepare_qat(model, mapping=None, inplace=False) [source]\n \nPrepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version. Quantization configuration should be assigned preemptively to individual submodules in .qconfig attribute.  Parameters \n \nmodel \u2013 input model to be modified in-place \nmapping \u2013 dictionary that maps float modules to quantized modules to be replaced. \ninplace \u2013 carry out model transformations in-place, the original module is mutated    \n"}, {"name": "torch.ao.quantization.propagate_qconfig_", "path": "generated/torch.ao.quantization.propagate_qconfig_#torch.ao.quantization.propagate_qconfig_", "type": "Quantization", "text": " \nclass torch.ao.quantization.propagate_qconfig_(module, qconfig_dict=None, prepare_custom_config_dict=None) [source]\n \nPropagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module  Parameters \n \nmodule \u2013 input module \nqconfig_dict \u2013 dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute) \nprepare_custom_config_dict \u2013 dictionary for custom handling of modules see docs for prepare_fx()\n   Returns \nNone, module is modified inplace with qconfig attached   \n"}, {"name": "torch.ao.quantization.qconfig.default_activation_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_activation_only_qconfig#torch.ao.quantization.qconfig.default_activation_only_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_activation_only_qconfig  \nDefault qconfig for quantizing activations only. \n"}, {"name": "torch.ao.quantization.qconfig.default_debug_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_debug_qconfig#torch.ao.quantization.qconfig.default_debug_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_debug_qconfig  \nDefault qconfig configuration for debugging. \n"}, {"name": "torch.ao.quantization.qconfig.default_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_dynamic_qconfig#torch.ao.quantization.qconfig.default_dynamic_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_dynamic_qconfig  \nDefault dynamic qconfig. \n"}, {"name": "torch.ao.quantization.qconfig.default_per_channel_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_per_channel_qconfig#torch.ao.quantization.qconfig.default_per_channel_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_per_channel_qconfig  \nDefault qconfig configuration for per channel weight quantization. \n"}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig#torch.ao.quantization.qconfig.default_qat_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_qat_qconfig  \nDefault qconfig for QAT. \n"}, {"name": "torch.ao.quantization.qconfig.default_qat_qconfig_v2", "path": "generated/torch.ao.quantization.qconfig.default_qat_qconfig_v2#torch.ao.quantization.qconfig.default_qat_qconfig_v2", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_qat_qconfig_v2  \nFused version of default_qat_config, has performance benefits. \n"}, {"name": "torch.ao.quantization.qconfig.default_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_qconfig#torch.ao.quantization.qconfig.default_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_qconfig  \nDefault qconfig configuration. \n"}, {"name": "torch.ao.quantization.qconfig.default_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.default_weight_only_qconfig#torch.ao.quantization.qconfig.default_weight_only_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.default_weight_only_qconfig  \nDefault qconfig for quantizing weights only. \n"}, {"name": "torch.ao.quantization.qconfig.float16_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_dynamic_qconfig#torch.ao.quantization.qconfig.float16_dynamic_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.float16_dynamic_qconfig  \nDynamic qconfig with weights quantized to torch.float16. \n"}, {"name": "torch.ao.quantization.qconfig.float16_static_qconfig", "path": "generated/torch.ao.quantization.qconfig.float16_static_qconfig#torch.ao.quantization.qconfig.float16_static_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.float16_static_qconfig  \nDynamic qconfig with both activations and weights quantized to torch.float16. \n"}, {"name": "torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "path": "generated/torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig#torch.ao.quantization.qconfig.float_qparams_weight_only_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.float_qparams_weight_only_qconfig  \nDynamic qconfig with weights quantized with a floating point zero_point. \n"}, {"name": "torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "path": "generated/torch.ao.quantization.qconfig.per_channel_dynamic_qconfig#torch.ao.quantization.qconfig.per_channel_dynamic_qconfig", "type": "Quantization", "text": " \ntorch.ao.quantization.qconfig.per_channel_dynamic_qconfig  \nDynamic qconfig with weights quantized per channel. \n"}, {"name": "torch.ao.quantization.qconfig.QConfig", "path": "generated/torch.ao.quantization.qconfig.qconfig#torch.ao.quantization.qconfig.QConfig", "type": "Quantization", "text": " \nclass torch.ao.quantization.qconfig.QConfig(activation, weight) [source]\n \nDescribes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively. Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial): my_qconfig = QConfig(\n    activation=MinMaxObserver.with_args(dtype=torch.qint8),\n    weight=default_observer.with_args(dtype=torch.qint8))\n \n"}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping", "type": "Quantization", "text": " \nclass torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping(backend='x86', version=1) [source]\n \nReturn the default QConfigMapping for quantization aware training.  Parameters \n \nbackend (*) \u2013 the quantization backend for the default qconfig mapping, should be one of [\u201cx86\u201d (default), \u201cfbgemm\u201d, \u201cqnnpack\u201d, \u201conednn\u201d] \nversion (*) \u2013 the version for the default qconfig mapping   Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "path": "generated/torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping#torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping", "type": "Quantization", "text": " \nclass torch.ao.quantization.qconfig_mapping.get_default_qconfig_mapping(backend='x86', version=0) [source]\n \nReturn the default QConfigMapping for post training quantization.  Parameters \n \nbackend (*) \u2013 the quantization backend for the default qconfig mapping, should be one of [\u201cx86\u201d (default), \u201cfbgemm\u201d, \u201cqnnpack\u201d, \u201conednn\u201d] \nversion (*) \u2013 the version for the default qconfig mapping   Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping", "type": "Quantization", "text": " \nclass torch.ao.quantization.qconfig_mapping.QConfigMapping [source]\n \nMapping from model ops to torch.ao.quantization.QConfig s. The user can specify QConfigs using the following methods (in increasing match priority): set_global : sets the global (default) QConfig set_object_type : sets the QConfig for a given module type, function, or method name set_module_name_regex : sets the QConfig for modules matching the given regex string set_module_name : sets the QConfig for modules matching the given module name set_module_name_object_type_order : sets the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears Example usage: qconfig_mapping = QConfigMapping()\n    .set_global(global_qconfig)\n    .set_object_type(torch.nn.Linear, qconfig1)\n    .set_object_type(torch.nn.ReLU, qconfig1)\n    .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig1)\n    .set_module_name_regex(\"foo.*\", qconfig2)\n    .set_module_name(\"module1\", qconfig1)\n    .set_module_name(\"module2\", qconfig2)\n    .set_module_name_object_type_order(\"foo.bar\", torch.nn.functional.linear, 0, qconfig3)\n  \nclassmethod from_dict(qconfig_dict) [source]\n \nCreate a QConfigMapping from a dictionary with the following keys (all optional): \u201c\u201d (for global QConfig) \u201cobject_type\u201d \u201cmodule_name_regex\u201d \u201cmodule_name\u201d \u201cmodule_name_object_type_order\u201d The values of this dictionary are expected to be lists of tuples.  Return type \nQConfigMapping   \n  \nset_global(global_qconfig) [source]\n \nSet the global (default) QConfig.  Return type \nQConfigMapping   \n  \nset_module_name(module_name, qconfig) [source]\n \nSet the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n  \nset_module_name_object_type_order(module_name, object_type, index, qconfig) [source]\n \nSet the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears. If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n  \nset_module_name_regex(module_name_regex, qconfig) [source]\n \nSet the QConfig for modules matching the given regex string. Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.: qconfig_mapping = QConfigMapping()\n    .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig1)\n    .set_module_name_regex(\"foo.*bar.*\", qconfig2)\n    .set_module_name_regex(\"foo.*\", qconfig3)\n In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3. If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.  Return type \nQConfigMapping   \n  \nset_object_type(object_type, qconfig) [source]\n \nSet the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n  \nto_dict() [source]\n \nConvert this QConfigMapping to a dictionary with the following keys: \u201c\u201d (for global QConfig) \u201cobject_type\u201d \u201cmodule_name_regex\u201d \u201cmodule_name\u201d \u201cmodule_name_object_type_order\u201d The values of this dictionary are lists of tuples.  Return type \nDict[str, Any]   \n \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.from_dict", "type": "Quantization", "text": " \nclassmethod from_dict(qconfig_dict) [source]\n \nCreate a QConfigMapping from a dictionary with the following keys (all optional): \u201c\u201d (for global QConfig) \u201cobject_type\u201d \u201cmodule_name_regex\u201d \u201cmodule_name\u201d \u201cmodule_name_object_type_order\u201d The values of this dictionary are expected to be lists of tuples.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_global", "type": "Quantization", "text": " \nset_global(global_qconfig) [source]\n \nSet the global (default) QConfig.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name", "type": "Quantization", "text": " \nset_module_name(module_name, qconfig) [source]\n \nSet the QConfig for modules matching the given module name. If the QConfig for an existing module name was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_object_type_order", "type": "Quantization", "text": " \nset_module_name_object_type_order(module_name, object_type, index, qconfig) [source]\n \nSet the QConfig for modules matching a combination of the given module name, object type, and the index at which the module appears. If the QConfig for an existing (module name, object type, index) was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_module_name_regex", "type": "Quantization", "text": " \nset_module_name_regex(module_name_regex, qconfig) [source]\n \nSet the QConfig for modules matching the given regex string. Regexes will be matched in the order in which they are registered through this method. Thus, the caller should register more specific patterns first, e.g.: qconfig_mapping = QConfigMapping()\n    .set_module_name_regex(\"foo.*bar.*conv[0-9]+\", qconfig1)\n    .set_module_name_regex(\"foo.*bar.*\", qconfig2)\n    .set_module_name_regex(\"foo.*\", qconfig3)\n In this example, \u201cfoo.bar.conv0\u201d would match qconfig1, \u201cfoo.bar.linear\u201d would match qconfig2, and \u201cfoo.baz.relu\u201d would match qconfig3. If the QConfig for an existing module name regex was already set, the new QConfig will override the old one while preserving the order in which the regexes were originally registered.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.set_object_type", "type": "Quantization", "text": " \nset_object_type(object_type, qconfig) [source]\n \nSet the QConfig for a given module type, function, or method name. If the QConfig for an existing object type was already set, the new QConfig will override the old one.  Return type \nQConfigMapping   \n"}, {"name": "torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict()", "path": "generated/torch.ao.quantization.qconfig_mapping.qconfigmapping#torch.ao.quantization.qconfig_mapping.QConfigMapping.to_dict", "type": "Quantization", "text": " \nto_dict() [source]\n \nConvert this QConfigMapping to a dictionary with the following keys: \u201c\u201d (for global QConfig) \u201cobject_type\u201d \u201cmodule_name_regex\u201d \u201cmodule_name\u201d \u201cmodule_name_object_type_order\u201d The values of this dictionary are lists of tuples.  Return type \nDict[str, Any]   \n"}, {"name": "torch.ao.quantization.quantize", "path": "generated/torch.ao.quantization.quantize#torch.ao.quantization.quantize", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False) [source]\n \nQuantize the input float model with post training static quantization. First it will prepare the model for calibration, then it calls run_fn which will run the calibration step, after that we will convert the model to a quantized model.  Parameters \n \nmodel \u2013 input float model \nrun_fn \u2013 a calibration function for calibrating the prepared model \nrun_args \u2013 positional arguments for run_fn\n \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 correspondence between original module types and quantized counterparts   Returns \nQuantized model.   \n"}, {"name": "torch.ao.quantization.quantize_dynamic", "path": "generated/torch.ao.quantization.quantize_dynamic#torch.ao.quantization.quantize_dynamic", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False) [source]\n \nConverts a float model to dynamic (i.e. weights-only) quantized model. Replaces specified modules with dynamic weight-only quantized versions and output the quantized model. For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants. Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.  Parameters \n \nmodel \u2013 input model \nqconfig_spec \u2013 \nEither:  A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfig instances. A set of types and/or submodule names to apply dynamic quantization to, in which case the dtype argument is used to specify the bit-width   \ninplace \u2013 carry out model transformations in-place, the original module is mutated \nmapping \u2013 maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced    \n"}, {"name": "torch.ao.quantization.quantize_fx.convert_fx", "path": "generated/torch.ao.quantization.quantize_fx.convert_fx#torch.ao.quantization.quantize_fx.convert_fx", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_fx.convert_fx(graph_module, convert_custom_config=None, _remove_qconfig=True, qconfig_mapping=None, backend_config=None) [source]\n \nConvert a calibrated or trained model to a quantized model  Parameters \n \ngraph_module (*) \u2013 A prepared and calibrated/trained model (GraphModule) \nconvert_custom_config (*) \u2013 custom configurations for convert function. See ConvertCustomConfig for more details \n_remove_qconfig (*) \u2013 Option to remove the qconfig attributes in the model after convert. \nqconfig_mapping (*) \u2013 \nconfig for specifying how to convert a model for quantization. The keys must include the ones in the qconfig_mapping passed to prepare_fx or prepare_qat_fx, with the same values or None. Additional keys can be specified with values set to None. For each entry whose value is set to None, we skip quantizing that entry in the model: qconfig_mapping = QConfigMapping\n    .set_global(qconfig_from_prepare)\n    .set_object_type(torch.nn.functional.add, None)  # skip quantizing torch.nn.functional.add\n    .set_object_type(torch.nn.functional.linear, qconfig_from_prepare)\n    .set_module_name(\"foo.bar\", None)  # skip quantizing module \"foo.bar\"\n  \n \nbackend_config (BackendConfig): A configuration for the backend which describes how \n\noperators should be quantized in the backend, this includes quantization mode support (static/dynamic/weight_only), dtype support (quint8/qint8 etc.), observer placement for each operators and fused operators. See BackendConfig for more details        Returns \nA quantized model (torch.nn.Module)  Return type \nGraphModule   Example: # prepared_model: the model after prepare_fx/prepare_qat_fx and calibration/training\n# convert_fx converts a calibrated/trained model to a quantized model for the\n# target hardware, this includes converting the model first to a reference\n# quantized model, and then lower the reference quantized model to a backend\n# Currently, the supported backends are fbgemm (onednn), qnnpack (xnnpack) and\n# they share the same set of quantized operators, so we are using the same\n# lowering procedure\n#\n# backend_config defines the corresponding reference quantized module for\n# the weighted modules in the model, e.g. nn.Linear\n# TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n# e.g. backend_config = get_default_backend_config(\"fbgemm\")\nquantized_model = convert_fx(prepared_model)\n \n"}, {"name": "torch.ao.quantization.quantize_fx.fuse_fx", "path": "generated/torch.ao.quantization.quantize_fx.fuse_fx#torch.ao.quantization.quantize_fx.fuse_fx", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_fx.fuse_fx(model, fuse_custom_config=None, backend_config=None) [source]\n \nFuse modules like conv+bn, conv+bn+relu etc, model must be in eval mode. Fusion rules are defined in torch.ao.quantization.fx.fusion_pattern.py  Parameters \n \nmodel (*) \u2013 a torch.nn.Module model \nfuse_custom_config (*) \u2013 custom configurations for fuse_fx. See FuseCustomConfig for more details   Return type \nGraphModule   Example: from torch.ao.quantization import fuse_fx\nm = Model().eval()\nm = fuse_fx(m)\n \n"}, {"name": "torch.ao.quantization.quantize_fx.prepare_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_fx#torch.ao.quantization.quantize_fx.prepare_fx", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_fx.prepare_fx(model, qconfig_mapping, example_inputs, prepare_custom_config=None, _equalization_config=None, backend_config=None) [source]\n \nPrepare a model for post training static quantization  Parameters \n \nmodel (*) \u2013 torch.nn.Module model \nqconfig_mapping (*) \u2013 QConfigMapping object to configure how a model is quantized, see QConfigMapping for more details \nexample_inputs (*) \u2013 Example inputs for forward function of the model, Tuple of positional args (keyword args can be passed as positional args as well) \nprepare_custom_config (*) \u2013 customization configuration for quantization tool. See PrepareCustomConfig for more details \n_equalization_config (*) \u2013 config for specifying how to perform equalization on the model \nbackend_config (*) \u2013 config that specifies how operators are quantized in a backend, this includes how the operators are observed, supported fusion patterns, how quantize/dequantize ops are inserted, supported dtypes etc. See BackendConfig for more details   Returns \nA GraphModule with observer (configured by qconfig_mapping), ready for calibration  Return type \nGraphModule   Example: import torch\nfrom torch.ao.quantization import get_default_qconfig_mapping\nfrom torch.ao.quantization.quantize_fx import prepare_fx\n\nclass Submodule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n    def forward(self, x):\n        x = self.linear(x)\n        return x\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n        self.sub = Submodule()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.sub(x) + x\n        return x\n\n# initialize a floating point model\nfloat_model = M().eval()\n\n# define calibration function\ndef calibrate(model, data_loader):\n    model.eval()\n    with torch.no_grad():\n        for image, target in data_loader:\n            model(image)\n\n# qconfig is the configuration for how we insert observers for a particular\n# operator\n# qconfig = get_default_qconfig(\"fbgemm\")\n# Example of customizing qconfig:\n# qconfig = torch.ao.quantization.QConfig(\n#    activation=MinMaxObserver.with_args(dtype=torch.qint8),\n#    weight=MinMaxObserver.with_args(dtype=torch.qint8))\n# `activation` and `weight` are constructors of observer module\n\n# qconfig_mapping is a collection of quantization configurations, user can\n# set the qconfig for each operator (torch op calls, functional calls, module calls)\n# in the model through qconfig_mapping\n# the following call will get the qconfig_mapping that works best for models\n# that target \"fbgemm\" backend\nqconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n\n# We can customize qconfig_mapping in different ways.\n# e.g. set the global qconfig, which means we will use the same qconfig for\n# all operators in the model, this can be overwritten by other settings\n# qconfig_mapping = QConfigMapping().set_global(qconfig)\n# e.g. quantize the linear submodule with a specific qconfig\n# qconfig_mapping = QConfigMapping().set_module_name(\"linear\", qconfig)\n# e.g. quantize all nn.Linear modules with a specific qconfig\n# qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\n# for a more complete list, please see the docstring for :class:`torch.ao.quantization.QConfigMapping`\n# argument\n\n# example_inputs is a tuple of inputs, that is used to infer the type of the\n# outputs in the model\n# currently it's not used, but please make sure model(*example_inputs) runs\nexample_inputs = (torch.randn(1, 3, 224, 224),)\n\n# TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n# e.g. backend_config = get_default_backend_config(\"fbgemm\")\n# `prepare_fx` inserts observers in the model based on qconfig_mapping and\n# backend_config. If the configuration for an operator in qconfig_mapping\n# is supported in the backend_config (meaning it's supported by the target\n# hardware), we'll insert observer modules according to the qconfig_mapping\n# otherwise the configuration in qconfig_mapping will be ignored\n#\n# Example:\n# in qconfig_mapping, user sets linear module to be quantized with quint8 for\n# activation and qint8 for weight:\n# qconfig = torch.ao.quantization.QConfig(\n#     observer=MinMaxObserver.with_args(dtype=torch.quint8),\n#     weight=MinMaxObserver.with-args(dtype=torch.qint8))\n# Note: current qconfig api does not support setting output observer, but\n# we may extend this to support these more fine grained control in the\n# future\n#\n# qconfig_mapping = QConfigMapping().set_object_type(torch.nn.Linear, qconfig)\n# in backend config, linear module also supports in this configuration:\n# weighted_int8_dtype_config = DTypeConfig(\n#   input_dtype=torch.quint8,\n#   output_dtype=torch.quint8,\n#   weight_dtype=torch.qint8,\n#   bias_type=torch.float)\n\n# linear_pattern_config = BackendPatternConfig(torch.nn.Linear) \\\n#    .set_observation_type(ObservationType.OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT) \\\n#    .add_dtype_config(weighted_int8_dtype_config) \\\n#    ...\n\n# backend_config = BackendConfig().set_backend_pattern_config(linear_pattern_config)\n# `prepare_fx` will check that the setting requested by suer in qconfig_mapping\n# is supported by the backend_config and insert observers and fake quant modules\n# in the model\nprepared_model = prepare_fx(float_model, qconfig_mapping, example_inputs)\n# Run calibration\ncalibrate(prepared_model, sample_inference_data)\n \n"}, {"name": "torch.ao.quantization.quantize_fx.prepare_qat_fx", "path": "generated/torch.ao.quantization.quantize_fx.prepare_qat_fx#torch.ao.quantization.quantize_fx.prepare_qat_fx", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_fx.prepare_qat_fx(model, qconfig_mapping, example_inputs, prepare_custom_config=None, backend_config=None) [source]\n \nPrepare a model for quantization aware training  Parameters \n \nmodel (*) \u2013 torch.nn.Module model \nqconfig_mapping (*) \u2013 see prepare_fx()\n \nexample_inputs (*) \u2013 see prepare_fx()\n \nprepare_custom_config (*) \u2013 see prepare_fx()\n \nbackend_config (*) \u2013 see prepare_fx()\n   Returns \nA GraphModule with fake quant modules (configured by qconfig_mapping and backend_config), ready for quantization aware training  Return type \nGraphModule   Example: import torch\nfrom torch.ao.quantization import get_default_qat_qconfig_mapping\nfrom torch.ao.quantization.quantize_fx import prepare_qat_fx\n\nclass Submodule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n    def forward(self, x):\n        x = self.linear(x)\n        return x\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(5, 5)\n        self.sub = Submodule()\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.sub(x) + x\n        return x\n\n# initialize a floating point model\nfloat_model = M().train()\n# (optional, but preferred) load the weights from pretrained model\n# float_model.load_weights(...)\n\n# define the training loop for quantization aware training\ndef train_loop(model, train_data):\n    model.train()\n    for image, target in data_loader:\n        ...\n\n# qconfig is the configuration for how we insert observers for a particular\n# operator\n# qconfig = get_default_qconfig(\"fbgemm\")\n# Example of customizing qconfig:\n# qconfig = torch.ao.quantization.QConfig(\n#    activation=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)),\n#    weight=FakeQuantize.with_args(observer=MinMaxObserver.with_args(dtype=torch.qint8)))\n# `activation` and `weight` are constructors of observer module\n\n# qconfig_mapping is a collection of quantization configurations, user can\n# set the qconfig for each operator (torch op calls, functional calls, module calls)\n# in the model through qconfig_mapping\n# the following call will get the qconfig_mapping that works best for models\n# that target \"fbgemm\" backend\nqconfig_mapping = get_default_qat_qconfig(\"fbgemm\")\n\n# We can customize qconfig_mapping in different ways, please take a look at\n# the docstring for :func:`~torch.ao.quantization.prepare_fx` for different ways\n# to configure this\n\n# example_inputs is a tuple of inputs, that is used to infer the type of the\n# outputs in the model\n# currently it's not used, but please make sure model(*example_inputs) runs\nexample_inputs = (torch.randn(1, 3, 224, 224),)\n\n# TODO: add backend_config after we split the backend_config for fbgemm and qnnpack\n# e.g. backend_config = get_default_backend_config(\"fbgemm\")\n# `prepare_qat_fx` inserts observers in the model based on qconfig_mapping and\n# backend_config, if the configuration for an operator in qconfig_mapping\n# is supported in the backend_config (meaning it's supported by the target\n# hardware), we'll insert fake_quantize modules according to the qconfig_mapping\n# otherwise the configuration in qconfig_mapping will be ignored\n# see :func:`~torch.ao.quantization.prepare_fx` for a detailed explanation of\n# how qconfig_mapping interacts with backend_config\nprepared_model = prepare_qat_fx(float_model, qconfig_mapping, example_inputs)\n# Run training\ntrain_loop(prepared_model, train_loop)\n \n"}, {"name": "torch.ao.quantization.quantize_qat", "path": "generated/torch.ao.quantization.quantize_qat#torch.ao.quantization.quantize_qat", "type": "Quantization", "text": " \nclass torch.ao.quantization.quantize_qat(model, run_fn, run_args, inplace=False) [source]\n \nDo quantization aware training and output a quantized model  Parameters \n \nmodel \u2013 input model \nrun_fn \u2013 a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop \nrun_args \u2013 positional arguments for run_fn\n   Returns \nQuantized model.   \n"}, {"name": "torch.ao.quantization.QuantStub", "path": "generated/torch.ao.quantization.quantstub#torch.ao.quantization.QuantStub", "type": "Quantization", "text": " \nclass torch.ao.quantization.QuantStub(qconfig=None) [source]\n \nQuantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.  Parameters \nqconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules   \n"}, {"name": "torch.ao.quantization.QuantWrapper", "path": "generated/torch.ao.quantization.quantwrapper#torch.ao.quantization.QuantWrapper", "type": "Quantization", "text": " \nclass torch.ao.quantization.QuantWrapper(module) [source]\n \nA wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules. This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub. \n"}, {"name": "torch.ao.quantization.swap_module", "path": "generated/torch.ao.quantization.swap_module#torch.ao.quantization.swap_module", "type": "Quantization", "text": " \nclass torch.ao.quantization.swap_module(mod, mapping, custom_module_class_mapping) [source]\n \nSwaps the module if it has a quantized counterpart and it has an observer attached.  Parameters \n \nmod \u2013 input module \nmapping \u2013 a dictionary that maps from nn module to nnq module   Returns \nThe corresponding quantized module of mod   \n"}, {"name": "torch.arange", "path": "generated/torch.arange", "type": "Torch", "text": "torch.arange  \ntorch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil with values from the interval [start, end) taken with common difference step beginning from start. Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise subtracting a small epsilon from end in such cases.  outi+1=outi+step\\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step} \n\n Parameters \n \nstart (Number) \u2013 the starting value for the set of points. Default: 0. \nend (Number) \u2013 the ending value for the set of points \nstep (Number) \u2013 the gap between each pair of adjacent points. Default: 1.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n \n\n"}, {"name": "torch.arccos", "path": "generated/torch.arccos", "type": "Torch", "text": "torch.arccos  \ntorch.arccos(input, *, out=None) \u2192 Tensor  \nAlias for torch.acos(). \n\n"}, {"name": "torch.arccosh", "path": "generated/torch.arccosh", "type": "Torch", "text": "torch.arccosh  \ntorch.arccosh(input, *, out=None) \u2192 Tensor  \nAlias for torch.acosh(). \n\n"}, {"name": "torch.arcsin", "path": "generated/torch.arcsin", "type": "Torch", "text": "torch.arcsin  \ntorch.arcsin(input, *, out=None) \u2192 Tensor  \nAlias for torch.asin(). \n\n"}, {"name": "torch.arcsinh", "path": "generated/torch.arcsinh", "type": "Torch", "text": "torch.arcsinh  \ntorch.arcsinh(input, *, out=None) \u2192 Tensor  \nAlias for torch.asinh(). \n\n"}, {"name": "torch.arctan", "path": "generated/torch.arctan", "type": "Torch", "text": "torch.arctan  \ntorch.arctan(input, *, out=None) \u2192 Tensor  \nAlias for torch.atan(). \n\n"}, {"name": "torch.arctan2", "path": "generated/torch.arctan2", "type": "Torch", "text": "torch.arctan2  \ntorch.arctan2(input, other, *, out=None) \u2192 Tensor  \nAlias for torch.atan2(). \n\n"}, {"name": "torch.arctanh", "path": "generated/torch.arctanh", "type": "Torch", "text": "torch.arctanh  \ntorch.arctanh(input, *, out=None) \u2192 Tensor  \nAlias for torch.atanh(). \n\n"}, {"name": "torch.are_deterministic_algorithms_enabled", "path": "generated/torch.are_deterministic_algorithms_enabled", "type": "Torch", "text": "torch.are_deterministic_algorithms_enabled  \ntorch.are_deterministic_algorithms_enabled() [source]\n \nReturns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details.  Return type \nbool   \n\n"}, {"name": "torch.argmax", "path": "generated/torch.argmax", "type": "Torch", "text": "torch.argmax  \ntorch.argmax(input) \u2192 LongTensor  \nReturns the indices of the maximum value of all elements in the input tensor. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Note If there are multiple maximal values then the indices of the first maximal value are returned.   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)\n   torch.argmax(input, dim, keepdim=False) \u2192 LongTensor\n\n Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmax of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n \n\n"}, {"name": "torch.argmin", "path": "generated/torch.argmin", "type": "Torch", "text": "torch.argmin  \ntorch.argmin(input, dim=None, keepdim=False) \u2192 LongTensor  \nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.  Note If there are multiple minimal values then the indices of the first minimal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmin of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not..    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])\n \n\n"}, {"name": "torch.argsort", "path": "generated/torch.argsort", "type": "Torch", "text": "torch.argsort  \ntorch.argsort(input, dim=-1, descending=False, stable=False) \u2192 Tensor  \nReturns the indices that sort a tensor along a given dimension in ascending order by value. This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method. If stable is True then the sorting routine becomes stable, preserving the order of equivalent elements. If False, the relative order of values which compare equal is not guaranteed. True is slower.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int, optional) \u2013 the dimension to sort along \ndescending (bool, optional) \u2013 controls the sorting order (ascending or descending) \nstable (bool, optional) \u2013 controls the relative order of equivalent elements    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n \n\n"}, {"name": "torch.argwhere", "path": "generated/torch.argwhere", "type": "Torch", "text": "torch.argwhere  \ntorch.argwhere(input) \u2192 Tensor  \nReturns a tensor containing the indices of all non-zero elements of input. Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style). If input has nn dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n), where zz is the total number of non-zero elements in the input tensor.  Note This function is similar to NumPy\u2019s argwhere. When input is on CUDA, this function causes host-device synchronization.   Parameters \n{input} \u2013    Example: >>> t = torch.tensor([1, 0, 1])\n>>> torch.argwhere(t)\ntensor([[0],\n        [2]])\n>>> t = torch.tensor([[1, 0, 1], [0, 1, 1]])\n>>> torch.argwhere(t)\ntensor([[0, 0],\n        [0, 2],\n        [1, 1],\n        [1, 2]])\n \n\n"}, {"name": "torch.as_strided", "path": "generated/torch.as_strided", "type": "Torch", "text": "torch.as_strided  \ntorch.as_strided(input, size, stride, storage_offset=None) \u2192 Tensor  \nCreate a view of an existing torch.Tensor input with specified size, stride and storage_offset.  Warning Prefer using other view functions, like torch.Tensor.expand(), to setting a view\u2019s strides manually with as_strided, as this function\u2019s behavior depends on the implementation of a tensor\u2019s storage. The constructed view of the storage must only refer to elements within the storage or a runtime error will be thrown, and if the view is \u201coverlapped\u201d (with multiple indices referring to the same element in memory) its behavior is undefined.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nsize (tuple or ints) \u2013 the shape of the output tensor \nstride (tuple or ints) \u2013 the stride of the output tensor \nstorage_offset (int, optional) \u2013 the offset in the underlying storage of the output tensor. If None, the storage_offset of the output tensor will match the input tensor.    Example: >>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n \n\n"}, {"name": "torch.as_tensor", "path": "generated/torch.as_tensor", "type": "Torch", "text": "torch.as_tensor  \ntorch.as_tensor(data, dtype=None, device=None) \u2192 Tensor  \nConverts data into a tensor, sharing data and preserving autograd history if possible. If data is already a tensor with the requested dtype and device then data itself is returned, but if data is a tensor with a different dtype or device then it\u2019s copied as if using data.to(dtype=dtype, device=device). If data is a NumPy array (an ndarray) with the same dtype and device then a tensor is constructed using torch.from_numpy().  See also torch.tensor() never shares its data and creates a new \u201cleaf tensor\u201d (see Autograd mechanics).   Parameters \n \ndata (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. \ndevice (torch.device, optional) \u2013 the device of the constructed tensor. If None and data is a tensor then the device of data is used. If None and data is not a tensor then the result tensor is constructed on the current device.    Example: >>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])\n \n\n"}, {"name": "torch.asarray", "path": "generated/torch.asarray", "type": "Torch", "text": "torch.asarray  \ntorch.asarray(obj, *, dtype=None, device=None, copy=None, requires_grad=False) \u2192 Tensor  \nConverts obj to a tensor. obj can be one of:  a tensor a NumPy array or a NumPy scalar a DLPack capsule an object that implements Python\u2019s buffer protocol a scalar a sequence of scalars  When obj is a tensor, NumPy array, or DLPack capsule the returned tensor will, by default, not require a gradient, have the same datatype as obj, be on the same device, and share memory with it. These properties can be controlled with the dtype, device, copy, and requires_grad keyword arguments. If the returned tensor is of a different datatype, on a different device, or a copy is requested then it will not share its memory with obj. If requires_grad is True then the returned tensor will require a gradient, and if obj is also a tensor with an autograd history then the returned tensor will have the same history. When obj is not a tensor, NumPy array, or DLPack capsule but implements Python\u2019s buffer protocol then the buffer is interpreted as an array of bytes grouped according to the size of the datatype passed to the dtype keyword argument. (If no datatype is passed then the default floating point datatype is used, instead.) The returned tensor will have the specified datatype (or default floating point datatype if none is specified) and, by default, be on the CPU device and share memory with the buffer. When obj is a NumPy scalar, the returned tensor will be a 0-dimensional tensor on the CPU and that doesn\u2019t share its memory (i.e. copy=True). By default datatype will be the PyTorch datatype corresponding to the NumPy\u2019s scalar\u2019s datatype. When obj is none of the above but a scalar, or a sequence of scalars then the returned tensor will, by default, infer its datatype from the scalar values, be on the current default device, and not share its memory.  See also torch.tensor() creates a tensor that always copies the data from the input object. torch.from_numpy() creates a tensor that always shares memory from NumPy arrays. torch.frombuffer() creates a tensor that always shares memory from objects that implement the buffer protocol. torch.from_dlpack() creates a tensor that always shares memory from DLPack capsules.   Parameters \nobj (object) \u2013 a tensor, NumPy array, DLPack Capsule, object that implements Python\u2019s buffer protocol, scalar, or sequence of scalars.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the datatype of the returned tensor. Default: None, which causes the datatype of the returned tensor to be inferred from obj. \ncopy (bool, optional) \u2013 controls whether the returned tensor shares memory with obj. Default: None, which causes the returned tensor to share memory with obj whenever possible. If True then the returned tensor does not share its memory. If False then the returned tensor shares its memory with obj and an error is thrown if it cannot. \ndevice (torch.device, optional) \u2013 the device of the returned tensor. Default: None, which causes the device of obj to be used. Or, if obj is a Python sequence, the current default device will be used. \nrequires_grad (bool, optional) \u2013 whether the returned tensor requires grad. Default: False, which causes the returned tensor not to require a gradient. If True, then the returned tensor will require a gradient, and if obj is also a tensor with an autograd history then the returned tensor will have the same history.    Example: >>> a = torch.tensor([1, 2, 3])\n>>> # Shares memory with tensor 'a'\n>>> b = torch.asarray(a)\n>>> a.data_ptr() == b.data_ptr()\nTrue\n>>> # Forces memory copy\n>>> c = torch.asarray(a, copy=True)\n>>> a.data_ptr() == c.data_ptr()\nFalse\n\n>>> a = torch.tensor([1., 2., 3.], requires_grad=True)\n>>> b = a + 2\n>>> b\ntensor([3., 4., 5.], grad_fn=<AddBackward0>)\n>>> # Shares memory with tensor 'b', with no grad\n>>> c = torch.asarray(b)\n>>> c\ntensor([3., 4., 5.])\n>>> # Shares memory with tensor 'b', retaining autograd history\n>>> d = torch.asarray(b, requires_grad=True)\n>>> d\ntensor([3., 4., 5.], grad_fn=<AddBackward0>)\n\n>>> array = numpy.array([1, 2, 3])\n>>> # Shares memory with array 'array'\n>>> t1 = torch.asarray(array)\n>>> array.__array_interface__['data'][0] == t1.data_ptr()\nTrue\n>>> # Copies memory due to dtype mismatch\n>>> t2 = torch.asarray(array, dtype=torch.float32)\n>>> array.__array_interface__['data'][0] == t2.data_ptr()\nFalse\n\n>>> scalar = numpy.float64(0.5)\n>>> torch.asarray(scalar)\ntensor(0.5000, dtype=torch.float64)\n \n\n"}, {"name": "torch.asin", "path": "generated/torch.asin", "type": "Torch", "text": "torch.asin  \ntorch.asin(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arcsine of the elements of input.  outi=sin\u2061\u22121(inputi)\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n \n\n"}, {"name": "torch.asinh", "path": "generated/torch.asinh", "type": "Torch", "text": "torch.asinh  \ntorch.asinh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic sine of the elements of input.  outi=sinh\u2061\u22121(inputi)\\text{out}_{i} = \\sinh^{-1}(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\n \n\n"}, {"name": "torch.atan", "path": "generated/torch.atan", "type": "Torch", "text": "torch.atan  \ntorch.atan(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arctangent of the elements of input.  outi=tan\u2061\u22121(inputi)\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n \n\n"}, {"name": "torch.atan2", "path": "generated/torch.atan2", "type": "Torch", "text": "torch.atan2  \ntorch.atan2(input, other, *, out=None) \u2192 Tensor  \nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i} with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i}) and vector (1,0)(1, 0). (Note that otheri\\text{other}_{i}, the second parameter, is the x-coordinate, while inputi\\text{input}_{i}, the first parameter, is the y-coordinate.) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n \n\n"}, {"name": "torch.atanh", "path": "generated/torch.atanh", "type": "Torch", "text": "torch.atanh  \ntorch.atanh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic tangent of the elements of input.  Note The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.   outi=tanh\u2061\u22121(inputi)\\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\n \n\n"}, {"name": "torch.atleast_1d", "path": "generated/torch.atleast_1d", "type": "Torch", "text": "torch.atleast_1d  \ntorch.atleast_1d(*tensors) [source]\n \nReturns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.  Parameters \ninput (Tensor or list of Tensors) \u2013   Returns \noutput (Tensor or tuple of Tensors)   Example: >>> x = torch.arange(2)\n>>> x\ntensor([0, 1])\n>>> torch.atleast_1d(x)\ntensor([0, 1])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x, y))\n(tensor([0.5000]), tensor([1.]))\n \n\n"}, {"name": "torch.atleast_2d", "path": "generated/torch.atleast_2d", "type": "Torch", "text": "torch.atleast_2d  \ntorch.atleast_2d(*tensors) [source]\n \nReturns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is.  Parameters \ninput (Tensor or list of Tensors) \u2013   Returns \noutput (Tensor or tuple of Tensors)   Example: >>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.atleast_2d(x)\ntensor([[0, 1],\n        [2, 3]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x, y))\n(tensor([[0.5000]]), tensor([[1.]]))\n \n\n"}, {"name": "torch.atleast_3d", "path": "generated/torch.atleast_3d", "type": "Torch", "text": "torch.atleast_3d  \ntorch.atleast_3d(*tensors) [source]\n \nReturns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is.  Parameters \ninput (Tensor or list of Tensors) \u2013   Returns \noutput (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.arange(4).view(2, 2)\n>>> y\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.atleast_3d(y)\ntensor([[[0],\n         [1]],\n\n        [[2],\n         [3]]])\n>>> x = torch.tensor(1).view(1, 1, 1)\n>>> x\ntensor([[[1]]])\n>>> torch.atleast_3d(x)\ntensor([[[1]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x, y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\n \n\n"}, {"name": "torch.autocast", "path": "amp#torch.autocast", "type": "Automatic Mixed Precision", "text": " \nclass torch.autocast(device_type, dtype=None, enabled=True, cache_enabled=None) [source]\n \nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. In these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details. When entering an autocast-enabled region, Tensors may be any type. You should not call half() or bfloat16() on your model(s) or inputs when using autocasting. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops. Example for CUDA Devices: # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with torch.autocast(device_type=\"cuda\"):\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n See the CUDA Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions). autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @torch.autocast(device_type=\"cuda\")\n    def forward(self, input):\n        ...\n Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. CUDA Example: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n CPU Training Example: # Creates model and optimizer in default precision\nmodel = Net()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor epoch in epochs:\n    for input, target in data:\n        optimizer.zero_grad()\n\n        # Runs the forward pass with autocasting.\n        with torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n            output = model(input)\n            loss = loss_fn(output, target)\n\n        loss.backward()\n        optimizer.step()\n CPU Inference Example: # Creates model in default precision\nmodel = Net().eval()\n\nwith torch.autocast(device_type=\"cpu\", dtype=torch.bfloat16):\n    for input in data:\n        # Runs the forward pass with autocasting.\n        output = model(input)\n CPU Inference Example with Jit Trace: class TestModel(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, num_classes)\n    def forward(self, x):\n        return self.fc1(x)\n\ninput_size = 2\nnum_classes = 2\nmodel = TestModel(input_size, num_classes).eval()\n\n# For now, we suggest to disable the Jit Autocast Pass,\n# As the issue: https://github.com/pytorch/pytorch/issues/75956\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.cpu.amp.autocast(cache_enabled=False):\n    model = torch.jit.trace(model, torch.randn(1, input_size))\nmodel = torch.jit.freeze(model)\n# Models Run\nfor _ in range(3):\n    model(torch.randn(1, input_size))\n Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue. autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith torch.autocast(device_type=\"cuda\"):\n    e_float16 = torch.mm(a_float32, b_float32)\n    with torch.autocast(device_type=\"cuda\", enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).  Parameters \n \ndevice_type (str, required) \u2013 Device type to use. Possible values are: \u2018cuda\u2019, \u2018cpu\u2019, \u2018xpu\u2019 and \u2018hpu\u2019. The type is the same as the type attribute of a torch.device. Thus, you may obtain the device type of a tensor using Tensor.device.type. \nenabled (bool, optional) \u2013 Whether autocasting should be enabled in the region. Default: True\n \ndtype (torch_dtype, optional) \u2013 Whether to use torch.float16 or torch.bfloat16. \ncache_enabled (bool, optional) \u2013 Whether the weight cache inside autocast should be enabled. Default: True\n    \n"}, {"name": "torch.autograd.backward()", "path": "generated/torch.autograd.backward#torch.autograd.backward", "type": "Automatic Differentiation", "text": " \ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None) [source]\n \nComputes the sum of gradients of given tensors with respect to graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.   Note If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Note When inputs are provided and a given input is not a leaf, the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.   Parameters \n \ntensors (Sequence[Tensor] or Tensor) \u2013 Tensors of which the derivative will be computed. \ngrad_tensors (Sequence[Tensor or None] or Tensor, optional) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (Sequence[Tensor] or Tensor, optional) \u2013 Inputs w.r.t. which the gradient be will accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors.    \n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.detect_anomaly(check_nan=True) [source]\n \nContext-manager that enable anomaly detection for the autograd engine. This does two things:  Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. If check_nan is True, any backward computation that generate \u201cnan\u201d value will raise an error. Default True.   Warning This mode should be enabled only for debugging as the different tests will slow down your program execution.  Example >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n \n"}, {"name": "torch.autograd.forward_ad.dual_level", "path": "generated/torch.autograd.forward_ad.dual_level#torch.autograd.forward_ad.dual_level", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.forward_ad.dual_level [source]\n \nContext-manager that enables forward AD. All forward AD computation must be performed in a dual_level context.  Note The dual_level context appropriately enters and exit the dual level to controls the current forward AD level, which is used by default by the other functions in this API. We currently don\u2019t plan to support nested dual_level contexts, however, so only a single forward AD level is supported. To compute higher-order forward grads, one can use torch.func.jvp().  Example: >>> x = torch.tensor([1])\n>>> x_t = torch.tensor([1])\n>>> with dual_level():\n...     inp = make_dual(x, x_t)\n...     # Do computations with inp\n...     out = your_fn(inp)\n...     _, grad = unpack_dual(out)\n>>> grad is None\nFalse\n>>> # After exiting the level, the grad is deleted\n>>> _, grad_after = unpack_dual(out)\n>>> grad is None\nTrue\n Please see the forward-mode AD tutorial for detailed steps on how to use this API. \n"}, {"name": "torch.autograd.forward_ad.make_dual()", "path": "generated/torch.autograd.forward_ad.make_dual#torch.autograd.forward_ad.make_dual", "type": "Automatic Differentiation", "text": " \ntorch.autograd.forward_ad.make_dual(tensor, tangent, *, level=None) [source]\n \nAssociates a tensor value with a forward gradient, the tangent, to create a \u201cdual tensor\u201d, which is used to compute forward AD gradients. The result is a new tensor aliased to tensor with tangent embedded as an attribute as-is if it has the same storage layout or copied otherwise. The tangent attribute can be recovered with unpack_dual(). This function is backward differentiable. Given a function f whose jacobian is J, it allows one to compute the Jacobian-vector product (jvp) between J and a given vector v as follows. Example: >>> with dual_level():\n...     inp = make_dual(x, v)\n...     out = f(inp)\n...     y, jvp = unpack_dual(out)\n Please see the forward-mode AD tutorial for detailed steps on how to use this API. \n"}, {"name": "torch.autograd.forward_ad.unpack_dual()", "path": "generated/torch.autograd.forward_ad.unpack_dual#torch.autograd.forward_ad.unpack_dual", "type": "Automatic Differentiation", "text": " \ntorch.autograd.forward_ad.unpack_dual(tensor, *, level=None) [source]\n \nUnpacks a \u201cdual tensor\u201d to get both its Tensor value and its forward AD gradient. The result is a namedtuple (primal, tangent) where primal is a view of tensor\u2019s primal and tangent is tensor\u2019s tangent as-is. Neither of these tensors can be dual tensor of level level. This function is backward differentiable. Example: >>> with dual_level():\n...     inp = make_dual(x, x_t)\n...     out = f(inp)\n...     y, jvp = unpack_dual(out)\n...     jvp = unpack_dual(out).tangent\n Please see the forward-mode AD tutorial for detailed steps on how to use this API. \n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.Function(*args, **kwargs) [source]\n \nBase class to create custom autograd.Function To create a custom autograd.Function, subclass this class and implement the forward() and backward() static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call forward() directly. To ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using torch.autograd.gradcheck(). See Extending torch.autograd for more details on how to use this class. Examples: >>> class Exp(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> # Use it by calling the apply method:\n>>> output = Exp.apply(input)\n \n"}, {"name": "torch.autograd.Function.backward()", "path": "generated/torch.autograd.function.backward#torch.autograd.Function.backward", "type": "Automatic Differentiation", "text": " \nstatic Function.backward(ctx, *grad_outputs)  \nDefines a formula for differentiating the operation with backward mode automatic differentiation (alias to the vjp function). This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs as the forward() returned (None will be passed in for non tensor outputs of the forward function), and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. If an input is not a Tensor or is a Tensor not requiring grads, you can just pass None as a gradient for that input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computed w.r.t. the output.  Return type \nAny   \n"}, {"name": "torch.autograd.Function.forward()", "path": "generated/torch.autograd.function.forward#torch.autograd.Function.forward", "type": "Automatic Differentiation", "text": " \nstatic Function.forward(ctx, *args, **kwargs)  \nThis function is to be overridden by all subclasses. There are two ways to define forward: Usage 1 (Combined forward and ctx): @staticmethod\ndef forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:\n    pass\n  It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). See Combined or separate forward() and setup_context() for more details  Usage 2 (Separate forward and ctx): @staticmethod\ndef forward(*args: Any, **kwargs: Any) -> Any:\n    pass\n\n@staticmethod\ndef setup_context(ctx: Any, inputs: Tuple[Any, ...], output: Any) -> None:\n    pass\n  The forward no longer accepts a ctx argument. Instead, you must also override the torch.autograd.Function.setup_context() staticmethod to handle setting up the ctx object. output is the output of the forward, inputs are a Tuple of inputs to the forward. See Extending torch.autograd for more details  The context can be used to store arbitrary data that can be then retrieved during the backward pass. Tensors should not be stored directly on ctx (though this is not currently enforced for backward compatibility). Instead, tensors should be saved either with ctx.save_for_backward() if they are intended to be used in backward (equivalently, vjp) or ctx.save_for_forward() if they are intended to be used for in jvp.  Return type \nAny   \n"}, {"name": "torch.autograd.function.FunctionCtx.mark_dirty()", "path": "generated/torch.autograd.function.functionctx.mark_dirty#torch.autograd.function.FunctionCtx.mark_dirty", "type": "Automatic Differentiation", "text": " \nFunctionCtx.mark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification.  Examples::\n\n>>> class Inplace(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         x_npy = x.numpy() # x_npy shares storage with x\n>>>         x_npy += 1\n>>>         ctx.mark_dirty(x)\n>>>         return x\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_output):\n>>>         return grad_output\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n>>> b = a * a\n>>> Inplace.apply(a)  # This would lead to wrong gradients!\n>>>                   # but the engine would not know unless we mark_dirty\n>>> b.backward() # RuntimeError: one of the variables needed for gradient\n>>>              # computation has been modified by an inplace operation\n   \n"}, {"name": "torch.autograd.function.FunctionCtx.mark_non_differentiable()", "path": "generated/torch.autograd.function.functionctx.mark_non_differentiable#torch.autograd.function.FunctionCtx.mark_non_differentiable", "type": "Automatic Differentiation", "text": " \nFunctionCtx.mark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be tensor outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output.  This is used e.g. for indices returned from a sort. See example::\n\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         sorted, idx = x.sort()\n>>>         ctx.mark_non_differentiable(idx)\n>>>         ctx.save_for_backward(x, idx)\n>>>         return sorted, idx\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):  # still need to accept g2\n>>>         x, idx = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         grad_input.index_add_(0, idx, g1)\n>>>         return grad_input\n   \n"}, {"name": "torch.autograd.function.FunctionCtx.save_for_backward()", "path": "generated/torch.autograd.function.functionctx.save_for_backward#torch.autograd.function.FunctionCtx.save_for_backward", "type": "Automatic Differentiation", "text": " \nFunctionCtx.save_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). save_for_backward should be called at most once, only from inside the forward() method, and only with tensors. All tensors intended to be used in the backward pass should be saved with save_for_backward (as opposed to directly on ctx) to prevent incorrect gradients and memory leaks, and enable the application of saved tensor hooks. See torch.autograd.graph.saved_tensors_hooks. Note that if intermediary tensors, tensors that are neither inputs nor outputs of forward(), are saved for backward, your custom Function may not support double backward. Custom Functions that do not support double backward should decorate their backward() method with @once_differentiable so that performing double backward raises an error. If you\u2019d like to support double backward, you can either recompute intermediaries based on the inputs during backward or return the intermediaries as the outputs of the custom Function. See the double backward tutorial for more details. In backward(), saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. This is a no-op. See Extending torch.autograd for more details on how to use this method.  Example::\n\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):\n>>>         w = x * z\n>>>         out = x * y + y * z + w * y\n>>>         ctx.save_for_backward(x, y, w, out)\n>>>         ctx.z = z  # z is not a tensor\n>>>         return out\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, grad_out):\n>>>         x, y, w, out = ctx.saved_tensors\n>>>         z = ctx.z\n>>>         gx = grad_out * (y + y * z)\n>>>         gy = grad_out * (x + z + w)\n>>>         gz = None\n>>>         return gx, gy, gz\n>>>\n>>> a = torch.tensor(1., requires_grad=True, dtype=torch.double)\n>>> b = torch.tensor(2., requires_grad=True, dtype=torch.double)\n>>> c = 4\n>>> d = Func.apply(a, b, c)\n   \n"}, {"name": "torch.autograd.function.FunctionCtx.set_materialize_grads()", "path": "generated/torch.autograd.function.functionctx.set_materialize_grads#torch.autograd.function.FunctionCtx.set_materialize_grads", "type": "Automatic Differentiation", "text": " \nFunctionCtx.set_materialize_grads(value) [source]\n \nSets whether to materialize grad tensors. Default is True. This should be called only from inside the forward() method If True, undefined grad tensors will be expanded to tensors full of zeros prior to calling the backward() and jvp() methods.  Example::\n\n>>> class SimpleFunc(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         return g1 + g2  # No check for None necessary\n>>>\n>>> # We modify SimpleFunc to handle non-materialized grad outputs\n>>> class Func(Function):\n>>>     @staticmethod\n>>>     def forward(ctx, x):\n>>>         ctx.set_materialize_grads(False)\n>>>         ctx.save_for_backward(x)\n>>>         return x.clone(), x.clone()\n>>>\n>>>     @staticmethod\n>>>     @once_differentiable\n>>>     def backward(ctx, g1, g2):\n>>>         x, = ctx.saved_tensors\n>>>         grad_input = torch.zeros_like(x)\n>>>         if g1 is not None:  # We must check for None now\n>>>             grad_input += g1\n>>>         if g2 is not None:\n>>>             grad_input += g2\n>>>         return grad_input\n>>>\n>>> a = torch.tensor(1., requires_grad=True)\n>>> b, _ = Func.apply(a)  # induces g2 to be undefined\n   \n"}, {"name": "torch.autograd.Function.jvp()", "path": "generated/torch.autograd.function.jvp#torch.autograd.Function.jvp", "type": "Automatic Differentiation", "text": " \nstatic Function.jvp(ctx, *grad_inputs)  \nDefines a formula for differentiating the operation with forward mode automatic differentiation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many inputs as the forward() got (None will be passed in for non tensor inputs of the forward function), and it should return as many tensors as there were outputs to forward(). Each argument is the gradient w.r.t the given input, and each returned value should be the gradient w.r.t. the corresponding output. If an output is not a Tensor or the function is not differentiable with respect to that output, you can just pass None as a gradient for that input. You can use the ctx object to pass any value from the forward to this functions.  Return type \nAny   \n"}, {"name": "torch.autograd.Function.vmap()", "path": "generated/torch.autograd.function.vmap#torch.autograd.Function.vmap", "type": "Automatic Differentiation", "text": " \nstatic Function.vmap(info, in_dims, *args) [source]\n \nDefines a rule for the behavior of this autograd.Function underneath torch.vmap(). For a torch.autograd.Function() to support torch.vmap(), you must either override this staticmethod, or set generate_vmap_rule to True (you may not do both). If you choose to override this staticmethod: it must accept  an info object as the first argument. info.batch_size specifies the size of the dimension being vmapped over, while info.randomness is the randomness option passed to torch.vmap(). an in_dims tuple as the second argument. For each arg in args, in_dims has a corresponding Optional[int]. It is None if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over. \n*args, which is the same as the args to forward().  The return of the vmap staticmethod is a tuple of (output, out_dims). Similar to in_dims, out_dims should be of the same structure as output and contain one out_dim per output that specifies if the output has the vmapped dimension and what index it is in. Please see Extending torch.func with autograd.Function for more details. \n"}, {"name": "torch.autograd.functional.hessian()", "path": "generated/torch.autograd.functional.hessian#torch.autograd.functional.hessian", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False, outer_jacobian_strategy='reverse-mode') [source]\n \nFunction that computes the Hessian of a given scalar function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Hessian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental. Please consider using torch.func.hessian() instead if you are looking for something less experimental and more performant. When computing the hessian, usually we invoke autograd.grad once per row of the hessian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False. \nouter_jacobian_strategy (str, optional) \u2013 The Hessian is computed by computing the Jacobian of a Jacobian. The inner Jacobian is always computed in reverse-mode AD. Setting strategy to \"forward-mode\" or \"reverse-mode\" determines whether the outer Jacobian will be computed with forward or reverse mode AD. Currently, computing the outer Jacobian in \"forward-mode\" requires vectorized=True. Defaults to \"reverse-mode\".   Returns \nif there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.  Return type \nHessian (Tensor or a tuple of tuple of Tensors)   Example >>> def pow_reducer(x):\n...     return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n >>> hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)\n >>> def pow_adder_reducer(x, y):\n...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n \n"}, {"name": "torch.autograd.functional.hvp()", "path": "generated/torch.autograd.functional.hvp#torch.autograd.functional.hvp", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Hessian vector product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...     return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n >>> hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=<SumBackward0>),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=<MulBackward0>))\n >>> def pow_adder_reducer(x, y):\n...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n  Note This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.  \n"}, {"name": "torch.autograd.functional.jacobian()", "path": "generated/torch.autograd.functional.jacobian#torch.autograd.functional.jacobian", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False, strategy='reverse-mode') [source]\n \nFunction that computes the Jacobian of a given function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Jacobian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental. Please consider using torch.func.jacrev() or torch.func.jacfwd() instead if you are looking for something less experimental and more performant. When computing the jacobian, usually we invoke autograd.grad once per row of the jacobian. If this flag is True, we perform only a single autograd.grad call with batched_grad=True which uses the vmap prototype feature. Though this should lead to performance improvements in many cases, because this feature is still experimental, there may be performance cliffs. See torch.autograd.grad()\u2019s batched_grad parameter for more information. \nstrategy (str, optional) \u2013 Set to \"forward-mode\" or \"reverse-mode\" to determine whether the Jacobian will be computed with forward or reverse mode AD. Currently, \"forward-mode\" requires vectorized=True. Defaults to \"reverse-mode\". If func has more outputs than inputs, \"forward-mode\" tends to be more performant. Otherwise, prefer to use \"reverse-mode\".   Returns \nif there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input. If strategy is forward-mode, the dtype will be that of the output; otherwise, the input.  Return type \nJacobian (Tensor or nested tuple of Tensors)   Example >>> def exp_reducer(x):\n...     return x.exp().sum(dim=1)\n>>> inputs = torch.rand(2, 2)\n>>> jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n >>> jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n >>> def exp_adder(x, y):\n...     return 2 * x.exp() + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n \n"}, {"name": "torch.autograd.functional.jvp()", "path": "generated/torch.autograd.functional.jvp#torch.autograd.functional.jvp", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Jacobian vector product is computed. Must be the same size as the input of func. This argument is optional when the input to func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.    Return type \noutput (tuple)    Note autograd.functional.jvp computes the jvp by using the backward of the backward (sometimes called the double backwards trick). This is not the most performant way of computing the jvp. Please consider using torch.func.jvp() or the low-level forward-mode AD API instead.  Example >>> def exp_reducer(x):\n...     return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4, 4)\n>>> jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n >>> jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))\n >>> def adder(x, y):\n...     return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.ones(2), torch.ones(2))\n>>> jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n \n"}, {"name": "torch.autograd.functional.vhp()", "path": "generated/torch.autograd.functional.vhp#torch.autograd.functional.vhp", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Hessian product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...     return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n>>> vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=<SumBackward0>),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=<MulBackward0>))\n>>> def pow_adder_reducer(x, y):\n...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n \n"}, {"name": "torch.autograd.functional.vjp()", "path": "generated/torch.autograd.functional.vjp#torch.autograd.functional.vjp", "type": "Automatic Differentiation", "text": " \ntorch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Jacobian product is computed. Must be the same size as the output of func. This argument is optional when the output of func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...     return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4)\n>>> vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n >>> vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))\n >>> def adder(x, y):\n...     return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = torch.ones(2)\n>>> vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n \n"}, {"name": "torch.autograd.grad()", "path": "generated/torch.autograd.grad#torch.autograd.grad", "type": "Automatic Differentiation", "text": " \ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False) [source]\n \nComputes and returns the sum of gradients of outputs with respect to the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in vector-Jacobian product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None).  Note If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.   Note only_inputs argument is deprecated and is ignored now (defaults to True). To accumulate gradient for other parts of the graph, please use torch.autograd.backward.   Parameters \n \noutputs (sequence of Tensor) \u2013 outputs of the differentiated function. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). \ngrad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the vector-Jacobian product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. \nallow_unused (Optional[bool], optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to the value of materialize_grads. \nis_grads_batched (bool, optional) \u2013 If True, the first dimension of each tensor in grad_outputs will be interpreted as the batch dimension. Instead of computing a single vector-Jacobian product, we compute a batch of vector-Jacobian products for each \u201cvector\u201d in the batch. We use the vmap prototype feature as the backend to vectorize calls to the autograd engine so that this computation can be performed in a single call. This should lead to performance improvements when compared to manually looping and performing backward multiple times. Note that due to this feature being experimental, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file an issue on github if warnings exist for your use case. Defaults to False. \nmaterialize_grads (bool, optional) \u2013 If True, set the gradient for unused inputs to zero instead of None. This is useful when computing higher-order derivatives. If materialize_grads is True and allow_unused is False, an error will be raised. Defaults to False.   Return type \nTuple[Tensor, \u2026]   \n"}, {"name": "torch.autograd.gradcheck()", "path": "generated/torch.autograd.gradcheck#torch.autograd.gradcheck", "type": "Automatic Differentiation", "text": " \ntorch.autograd.gradcheck(func, inputs, *, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=None, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_batched_forward_grad=False, check_forward_ad=False, check_backward_ad=True, fast_mode=False, masked=None) [source]\n \nCheck gradients computed via small finite differences against analytical gradients wrt tensors in inputs that are of floating point or complex type and with requires_grad=True. The check between numerical and analytical gradients uses allclose(). For most of the complex functions we consider for optimization purposes, no notion of Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient computation is done under the assumption that the overall function has a real-valued output, we treat functions with complex output in a special way. For these functions, gradcheck is applied to two real-valued functions corresponding to taking the real components of the complex outputs for the first, and taking the imaginary components of the complex outputs for the second. For more details, check out Autograd for Complex Numbers.  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Note Gradcheck may fail when evaluated on non-differentiable points because the numerically computed gradients via finite differencing may differ those computed analytically (not necessarily because either is incorrect). For more context, see Gradients for non-differentiable functions.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \ncheck_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor inputs, gradcheck will perform its check at nnz positions only. The check_sparse_nnz argument is deprecated, use the masked argument instead. If check_sparse_nnz != masked, an exception is raised. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros, for Tensor outputs. \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False. \ncheck_batched_forward_grad (bool, optional) \u2013 if True, checks if we can compute batched forward gradients using forward ad and prototype vmap support. Defaults to False. \ncheck_forward_ad (bool, optional) \u2013 if True, check that the gradients computed with forward mode AD match the numerical ones. Defaults to False. \ncheck_backward_ad (bool, optional) \u2013 if False, do not perform any checks that rely on backward mode AD to be implemented. Defaults to True. \nfast_mode (bool, optional) \u2013 Fast mode for gradcheck and gradgradcheck is currently only implemented for R to R functions. If none of the inputs and outputs are complex a faster implementation of gradcheck that no longer computes the entire jacobian is run; otherwise, we fall back to the slow implementation. \nmasked (bool, optional) \u2013 if True, the gradients of unspecified elements of sparse tensors are ignored. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition  Return type \nbool   \n"}, {"name": "torch.autograd.gradgradcheck()", "path": "generated/torch.autograd.gradgradcheck#torch.autograd.gradgradcheck", "type": "Automatic Differentiation", "text": " \ntorch.autograd.gradgradcheck(func, inputs, grad_outputs=None, *, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_fwd_over_rev=False, check_rev_over_rev=True, fast_mode=False, masked=False) [source]\n \nCheck gradients of gradients computed via small finite differences against analytical gradients wrt tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \ngrad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \ngen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False. \nfast_mode (bool, optional) \u2013 if True, run a faster implementation of gradgradcheck that no longer computes the entire jacobian. \nmasked (bool, optional) \u2013 if True, the gradients of unspecified elements of sparse tensors are ignored (default, False).   Returns \nTrue if all differences satisfy allclose condition  Return type \nbool   \n"}, {"name": "torch.autograd.graph.allow_mutation_on_saved_tensors", "path": "autograd#torch.autograd.graph.allow_mutation_on_saved_tensors", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.graph.allow_mutation_on_saved_tensors [source]\n \nContext manager under which mutating tensors saved for backward is allowed Under this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it\u2019s used during backward. To ensure the correct behavior, both the forward and backward should be run under the same context manager.  Returns \nAn _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.   Example: >>> import torch\n>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():\n...     # forward\n...     a = torch.ones(2, 3, requires_grad=True)\n...     b = a.clone()\n...     out = (b**2).sum()\n...     b.sin_()\n...     # backward\n...     out.sum().backward()\n...\ntensor([[0.8415, 0.8415, 0.8415],\n        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)\n \n"}, {"name": "torch.autograd.graph.disable_saved_tensors_hooks", "path": "autograd#torch.autograd.graph.disable_saved_tensors_hooks", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.graph.disable_saved_tensors_hooks(error_message) [source]\n \nContext-manager that disables the saved tensors default hooks feature. Useful for if you are creating a feature that does not work with saved tensors default hooks.  Parameters \nerror_message (str) \u2013 When saved tensors default hooks are used when they have been are disabled, a RuntimeError with this error message gets raised.   Example: >>> message = \"saved tensors default hooks are disabled\"\n>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):\n...     # Raises RuntimeError: saved tensors default hooks are disabled\n...     with torch.autograd.graph.save_on_cpu():\n...         pass\n \n"}, {"name": "torch.autograd.graph.Node.metadata()", "path": "generated/torch.autograd.graph.node.metadata#torch.autograd.graph.Node.metadata", "type": "Automatic Differentiation", "text": " \nabstract Node.metadata() [source]\n \nReturns the metadata.  Return type \ndict   \n"}, {"name": "torch.autograd.graph.Node.name()", "path": "generated/torch.autograd.graph.node.name#torch.autograd.graph.Node.name", "type": "Automatic Differentiation", "text": " \nabstract Node.name() [source]\n \nReturns the name. Example: >>> import torch\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.clone()\n>>> assert isinstance(b.grad_fn, torch.autograd.graph.Node)\n>>> print(b.grad_fn.name())\nCloneBackward0\n  Return type \nstr   \n"}, {"name": "torch.autograd.graph.Node.next_functions", "path": "generated/torch.autograd.graph.node.next_functions#torch.autograd.graph.Node.next_functions", "type": "Automatic Differentiation", "text": " \nabstract property Node.next_functions: Tuple[Tuple[Optional[Node], int], ...] \n"}, {"name": "torch.autograd.graph.Node.register_hook()", "path": "generated/torch.autograd.graph.node.register_hook#torch.autograd.graph.Node.register_hook", "type": "Automatic Differentiation", "text": " \nabstract Node.register_hook(fn) [source]\n \nRegisters a backward hook. The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature: hook(grad_inputs: Tuple[Tensor], grad_outputs: Tuple[Tensor]) -> Tuple[Tensor] or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_inputs. This function returns a handle with a method handle.remove() that removes the hook from the module.  Note See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.  Example: >>> import torch\n>>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.clone()\n>>> assert isinstance(b.grad_fn, torch.autograd.graph.Node)\n>>> handle = b.grad_fn.register_hook(lambda gI, gO: (gO[0] * 2,))\n>>> b.sum().backward(retain_graph=True)\n>>> print(a.grad)\ntensor([2., 2., 2.])\n>>> handle.remove() # Removes the hook\n>>> a.grad = None\n>>> b.sum().backward(retain_graph=True)\n>>> print(a.grad)\ntensor([1., 1., 1.])\n  Return type \nRemovableHandle   \n"}, {"name": "torch.autograd.graph.Node.register_prehook()", "path": "generated/torch.autograd.graph.node.register_prehook#torch.autograd.graph.Node.register_prehook", "type": "Automatic Differentiation", "text": " \nabstract Node.register_prehook(fn) [source]\n \nRegisters a backward pre-hook. The hook will be called every time a gradient with respect to the Node is computed. The hook should have the following signature: hook(grad_outputs: Tuple[Tensor]) -> Tuple[Tensor] or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad_outputs. This function returns a handle with a method handle.remove() that removes the hook from the module.  Note See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.  Example: >>> a = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> b = a.clone()\n>>> assert isinstance(b.grad_fn, torch.autograd.graph.Node)\n>>> handle = b.grad_fn.register_prehook(lambda gI: (gI[0] * 2,))\n>>> b.sum().backward(retain_graph=True)\n>>> print(a.grad)\ntensor([2., 2., 2.])\n>>> handle.remove()\n>>> a.grad = None\n>>> b.sum().backward(retain_graph=True)\n>>> print(a.grad)\ntensor([1., 1., 1.])\n  Return type \nRemovableHandle   \n"}, {"name": "torch.autograd.graph.register_multi_grad_hook", "path": "autograd#torch.autograd.graph.register_multi_grad_hook", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.graph.register_multi_grad_hook(tensors, fn) [source]\n \nRegisters a multi-grad backward hook. The hook will be called after gradients with respect to every tensor in tensors have been computed. If a tensor is in tensors but is not part of the graph, or if a tensor is not needed to compute the gradients for any inputs specified for the current .backward() or .grad() call, this tensor will be ignored and the hook will not wait for its gradient to be computed. After every non-ignored tensor\u2019s gradient has been computed, fn will be called with those gradients. None will be passed for tensors that did not have their gradients computed. The hook should not modify its arguments. This function returns a handle with a method handle.remove() that removes the hook.  Note See Backward Hooks execution for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.  Example: >>> import torch\n>>>\n>>> a = torch.rand(2, 3, requires_grad=True)\n>>> b = torch.rand(2, 3, requires_grad=True)\n>>> c = a * b\n>>> d = a * b\n>>>\n>>> def fn(grads):\n...     print([g is not None for g in grads])\n...\n>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)\n>>>\n>>> c.sum().backward(retain_graph=True)\n[True, True, True, False]\n>>> c.sum().backward(inputs=(a,), retain_graph=True)\n[True, False, True, False]\n>>>\n \n"}, {"name": "torch.autograd.graph.save_on_cpu", "path": "autograd#torch.autograd.graph.save_on_cpu", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.graph.save_on_cpu(pin_memory=False, device_type='cuda') [source]\n \nContext-manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward. When performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed. Use this context-manager to trade compute for GPU memory usage (e.g. when your model doesn\u2019t fit in GPU memory during training).  Parameters \npin_memory (bool) \u2013 If True tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to False. Also see Use pinned memory buffers.   Example: >>> a = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> b = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>> c = torch.randn(5, requires_grad=True, device=\"cuda\")\n>>>\n>>> def f(a, b, c):\n...     prod_1 = a * b           # a and b are saved on GPU\n...     with torch.autograd.graph.save_on_cpu():\n...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU\n...     y = prod_2 * a           # prod_2 and a are saved on GPU\n...     return y\n>>>\n>>> y = f(a, b, c)\n>>> del a, b, c  # for illustration only\n>>> # the content of a, b, and prod_2 are still alive on GPU\n>>> # the content of prod_1 and c only live on CPU\n>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward\n>>> # all intermediary tensors are released (deleted) after the call to backward\n \n"}, {"name": "torch.autograd.graph.saved_tensors_hooks", "path": "autograd#torch.autograd.graph.saved_tensors_hooks", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook) [source]\n \nContext-manager that sets a pair of pack / unpack hooks for saved tensors. Use this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval. In that context, the pack_hook function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using save_for_backward() but also those recorded by a PyTorch-defined operation). The output of pack_hook is then stored in the computation graph instead of the original tensor. The unpack_hook is called when the saved tensor needs to be accessed, namely when executing torch.Tensor.backward() or torch.autograd.grad(). It takes as argument the packed object returned by pack_hook and should return a tensor which has the same content as the original tensor (passed as input to the corresponding pack_hook). The hooks should have the following signatures: pack_hook(tensor: Tensor) -> Any unpack_hook(Any) -> Tensor where the return value of pack_hook is a valid input to unpack_hook. In general, you want unpack_hook(pack_hook(t)) to be equal to t in terms of value, size, dtype and device. Example: >>> def pack_hook(x):\n...     print(\"Packing\", x)\n...     return x\n>>>\n>>> def unpack_hook(x):\n...     print(\"Unpacking\", x)\n...     return x\n>>>\n>>> a = torch.ones(5, requires_grad=True)\n>>> b = torch.ones(5, requires_grad=True) * 2\n>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):\n...     y = a * b\nPacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nPacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n>>> y.sum().backward()\nUnpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)\nUnpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)\n  Warning Performing an inplace operation on the input to either hooks may lead to undefined behavior.   Warning Only one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied.  \n"}, {"name": "torch.autograd.profiler.emit_itt", "path": "autograd#torch.autograd.profiler.emit_itt", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.profiler.emit_itt(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an ITT range. It is useful when running the program under Intel(R) VTune Profiler: vtune <--vtune-flags> <regular command here>\n The Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI.  Parameters \n \nenabled (bool, optional) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional) \u2013 If record_shapes=True, the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: False\n    Example >>> with torch.autograd.profiler.emit_itt():\n...     model(x)\n \n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.  Parameters \n \nenabled (bool, optional) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional) \u2013 If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: False\n    Example >>> with torch.cuda.profiler.profile():\n...     model(x)  # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. \n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "generated/torch.autograd.profiler.load_nvprof#torch.autograd.profiler.load_nvprof", "type": "Automatic Differentiation", "text": " \ntorch.autograd.profiler.load_nvprof(path) [source]\n \nOpens an nvprof trace file and parses autograd annotations.  Parameters \npath (str) \u2013 path to nvprof trace   \n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, use_device=None, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, with_modules=False, use_kineto=False, use_cpu=True, use_mtia=False, experimental_config=None) [source]\n \nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks  Parameters \n \nenabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. \nuse_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. \nrecord_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection. \nwith_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operator\u2019s input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators. \nprofile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. \nwith_stack (bool, optional) \u2013 record source information (file and line number) for the ops. \nwith_modules (bool) \u2013 record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A\u2019s forward call\u2019s module B\u2019s forward which contains an aten::add op, then aten::add\u2019s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models. \nuse_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. \nuse_cpu (bool, optional) \u2013 profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling. \nexperimental_config (_ExperimentalConfig) \u2013 A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.    Example >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>>         y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n \n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "generated/torch.autograd.profiler.profile.export_chrome_trace#torch.autograd.profiler.profile.export_chrome_trace", "type": "Automatic Differentiation", "text": " \nprofile.export_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "generated/torch.autograd.profiler.profile.key_averages#torch.autograd.profiler.profile.key_averages", "type": "Automatic Differentiation", "text": " \nprofile.key_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by (event name, input shapes) rather than just event name. This is useful to see which input shapes contribute to the runtime the most and may help with size-specific optimizations or choosing the best candidates for quantization (aka fitting a roof line) \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total", "path": "generated/torch.autograd.profiler.profile.self_cpu_time_total#torch.autograd.profiler.profile.self_cpu_time_total", "type": "Automatic Differentiation", "text": " \nproperty profile.self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "generated/torch.autograd.profiler.profile.total_average#torch.autograd.profiler.profile.total_average", "type": "Automatic Differentiation", "text": " \nprofile.total_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "Automatic Differentiation", "text": " \nclass torch.autograd.set_detect_anomaly(mode, check_nan=True) [source]\n \nContext-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.  Parameters \n \nmode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False). \ncheck_nan (bool) \u2013 Flag whether to raise an error when the backward generate \u201cnan\u201d    \n"}, {"name": "torch.backends", "path": "backends", "type": "Backends", "text": "torch.backends torch.backends controls the behavior of various backends that PyTorch supports. These backends include:  torch.backends.cpu torch.backends.cuda torch.backends.cudnn torch.backends.mps torch.backends.mkl torch.backends.mkldnn torch.backends.openmp torch.backends.opt_einsum torch.backends.xeon  torch.backends.cpu  \ntorch.backends.cpu.get_cpu_capability() [source]\n \nReturns cpu capability as a string value. Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d  Return type \nstr   \n torch.backends.cuda  \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n  \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction  \nA bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs. \n  \ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction  \nA bool that controls whether reduced precision reductions are allowed with bf16 GEMMs. \n  \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].  \ntorch.backends.cuda.cufft_plan_cache.size  \nA readonly int that shows the number of plans currently in a cuFFT plan cache. \n  \ntorch.backends.cuda.cufft_plan_cache.max_size  \nA int that controls the capacity of a cuFFT plan cache. \n  \ntorch.backends.cuda.cufft_plan_cache.clear()  \nClears a cuFFT plan cache. \n \n  \ntorch.backends.cuda.preferred_linalg_library(backend=None) [source]\n \n Warning This flag is experimental and subject to change.  When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.  If \u201ccusolver\u201d is set then cuSOLVER will be used wherever possible. If \u201cmagma\u201d is set then MAGMA will be used wherever possible. If \u201cdefault\u201d (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available. When no input is given, this function returns the currently preferred library. User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.  Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs. Currently supported linalg operators:  torch.linalg.inv() torch.linalg.inv_ex() torch.linalg.cholesky() torch.linalg.cholesky_ex() torch.cholesky_solve() torch.cholesky_inverse() torch.linalg.lu_factor() torch.linalg.lu() torch.linalg.lu_solve() torch.linalg.qr() torch.linalg.eigh() torch.linalg.eighvals() torch.linalg.svd() torch.linalg.svdvals()   Return type \n_LinalgBackend   \n  \nclass torch.backends.cuda.SDPBackend(value) [source]\n \nEnum class for the scaled dot product attention backends.  Warning This class is in beta and subject to change.  This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h \n  \ntorch.backends.cuda.flash_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether flash scaled dot product attention is enabled or not. \n  \ntorch.backends.cuda.enable_mem_efficient_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables memory efficient scaled dot product attention. \n  \ntorch.backends.cuda.mem_efficient_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether memory efficient scaled dot product attention is enabled or not. \n  \ntorch.backends.cuda.enable_flash_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables flash scaled dot product attention. \n  \ntorch.backends.cuda.math_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether math scaled dot product attention is enabled or not. \n  \ntorch.backends.cuda.enable_math_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables math scaled dot product attention. \n  \ntorch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True) [source]\n \n Warning This flag is beta and subject to change.  This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored. \n torch.backends.cudnn  \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n  \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n  \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n  \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n  \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n  \ntorch.backends.cudnn.benchmark_limit  \nA int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API. \n torch.backends.mps  \ntorch.backends.mps.is_available() [source]\n \nReturns a bool indicating if MPS is currently available.  Return type \nbool   \n  \ntorch.backends.mps.is_built() [source]\n \nReturns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.  Return type \nbool   \n torch.backends.mkl  \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n  \nclass torch.backends.mkl.verbose(enable) [source]\n \nOn-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only. import torch\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n  Parameters \nlevel \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing   \n torch.backends.mkldnn  \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n  \nclass torch.backends.mkldnn.verbose(level) [source]\n \nOn-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only. import torch\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n  Parameters \nlevel \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation   \n torch.backends.openmp  \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n torch.backends.opt_einsum  \ntorch.backends.opt_einsum.is_available() [source]\n \nReturns a bool indicating if opt_einsum is currently available.  Return type \nbool   \n  \ntorch.backends.opt_einsum.get_opt_einsum() [source]\n \nReturns the opt_einsum package if opt_einsum is currently available, else None.  Return type \nAny   \n  \ntorch.backends.opt_einsum.enabled  \nA :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance. If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right. \n  \ntorch.backends.opt_einsum.strategy  \nA :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html). \n torch.backends.xeon\n"}, {"name": "torch.backends.cpu.get_cpu_capability()", "path": "backends#torch.backends.cpu.get_cpu_capability", "type": "Backends", "text": " \ntorch.backends.cpu.get_cpu_capability() [source]\n \nReturns cpu capability as a string value. Possible values: - \u201cDEFAULT\u201d - \u201cVSX\u201d - \u201cZ VECTOR\u201d - \u201cNO AVX\u201d - \u201cAVX2\u201d - \u201cAVX512\u201d  Return type \nstr   \n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "Backends", "text": " \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache contains the cuFFT plan caches for each CUDA device. Query a specific device i\u2019s cache via torch.backends.cuda.cufft_plan_cache[i].  \ntorch.backends.cuda.cufft_plan_cache.size  \nA readonly int that shows the number of plans currently in a cuFFT plan cache. \n  \ntorch.backends.cuda.cufft_plan_cache.max_size  \nA int that controls the capacity of a cuFFT plan cache. \n  \ntorch.backends.cuda.cufft_plan_cache.clear()  \nClears a cuFFT plan cache. \n \n"}, {"name": "torch.backends.cuda.cufft_plan_cache.clear()", "path": "backends#torch.backends.cuda.cufft_plan_cache.clear", "type": "Backends", "text": " \ntorch.backends.cuda.cufft_plan_cache.clear()  \nClears a cuFFT plan cache. \n"}, {"name": "torch.backends.cuda.cufft_plan_cache.max_size", "path": "backends#torch.backends.cuda.cufft_plan_cache.max_size", "type": "Backends", "text": " \ntorch.backends.cuda.cufft_plan_cache.max_size  \nA int that controls the capacity of a cuFFT plan cache. \n"}, {"name": "torch.backends.cuda.cufft_plan_cache.size", "path": "backends#torch.backends.cuda.cufft_plan_cache.size", "type": "Backends", "text": " \ntorch.backends.cuda.cufft_plan_cache.size  \nA readonly int that shows the number of plans currently in a cuFFT plan cache. \n"}, {"name": "torch.backends.cuda.enable_flash_sdp()", "path": "backends#torch.backends.cuda.enable_flash_sdp", "type": "Backends", "text": " \ntorch.backends.cuda.enable_flash_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables flash scaled dot product attention. \n"}, {"name": "torch.backends.cuda.enable_math_sdp()", "path": "backends#torch.backends.cuda.enable_math_sdp", "type": "Backends", "text": " \ntorch.backends.cuda.enable_math_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables math scaled dot product attention. \n"}, {"name": "torch.backends.cuda.enable_mem_efficient_sdp()", "path": "backends#torch.backends.cuda.enable_mem_efficient_sdp", "type": "Backends", "text": " \ntorch.backends.cuda.enable_mem_efficient_sdp(enabled) [source]\n \n Warning This flag is beta and subject to change.  Enables or disables memory efficient scaled dot product attention. \n"}, {"name": "torch.backends.cuda.flash_sdp_enabled()", "path": "backends#torch.backends.cuda.flash_sdp_enabled", "type": "Backends", "text": " \ntorch.backends.cuda.flash_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether flash scaled dot product attention is enabled or not. \n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "Backends", "text": " \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n"}, {"name": "torch.backends.cuda.math_sdp_enabled()", "path": "backends#torch.backends.cuda.math_sdp_enabled", "type": "Backends", "text": " \ntorch.backends.cuda.math_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether math scaled dot product attention is enabled or not. \n"}, {"name": "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction", "type": "Backends", "text": " \ntorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction  \nA bool that controls whether reduced precision reductions are allowed with bf16 GEMMs. \n"}, {"name": "torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "path": "backends#torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction", "type": "Backends", "text": " \ntorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction  \nA bool that controls whether reduced precision reductions (e.g., with fp16 accumulation type) are allowed with fp16 GEMMs. \n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "Backends", "text": " \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cuda.mem_efficient_sdp_enabled()", "path": "backends#torch.backends.cuda.mem_efficient_sdp_enabled", "type": "Backends", "text": " \ntorch.backends.cuda.mem_efficient_sdp_enabled() [source]\n \n Warning This flag is beta and subject to change.  Returns whether memory efficient scaled dot product attention is enabled or not. \n"}, {"name": "torch.backends.cuda.preferred_linalg_library()", "path": "backends#torch.backends.cuda.preferred_linalg_library", "type": "Backends", "text": " \ntorch.backends.cuda.preferred_linalg_library(backend=None) [source]\n \n Warning This flag is experimental and subject to change.  When PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a str) allows overriding those heuristics.  If \u201ccusolver\u201d is set then cuSOLVER will be used wherever possible. If \u201cmagma\u201d is set then MAGMA will be used wherever possible. If \u201cdefault\u201d (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available. When no input is given, this function returns the currently preferred library. User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.  Note: When a library is preferred other libraries may still be used if the preferred library doesn\u2019t implement the operation(s) called. This flag may achieve better performance if PyTorch\u2019s heuristic library selection is incorrect for your application\u2019s inputs. Currently supported linalg operators:  torch.linalg.inv() torch.linalg.inv_ex() torch.linalg.cholesky() torch.linalg.cholesky_ex() torch.cholesky_solve() torch.cholesky_inverse() torch.linalg.lu_factor() torch.linalg.lu() torch.linalg.lu_solve() torch.linalg.qr() torch.linalg.eigh() torch.linalg.eighvals() torch.linalg.svd() torch.linalg.svdvals()   Return type \n_LinalgBackend   \n"}, {"name": "torch.backends.cuda.sdp_kernel()", "path": "backends#torch.backends.cuda.sdp_kernel", "type": "Backends", "text": " \ntorch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=True) [source]\n \n Warning This flag is beta and subject to change.  This context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored. \n"}, {"name": "torch.backends.cuda.SDPBackend", "path": "backends#torch.backends.cuda.SDPBackend", "type": "Backends", "text": " \nclass torch.backends.cuda.SDPBackend(value) [source]\n \nEnum class for the scaled dot product attention backends.  Warning This class is in beta and subject to change.  This class needs to stay aligned with the enum defined in: pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h \n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "Backends", "text": " \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "Backends", "text": " \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n"}, {"name": "torch.backends.cudnn.benchmark_limit", "path": "backends#torch.backends.cudnn.benchmark_limit", "type": "Backends", "text": " \ntorch.backends.cudnn.benchmark_limit  \nA int that specifies the maximum number of cuDNN convolution algorithms to try when torch.backends.cudnn.benchmark is True. Set benchmark_limit to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API. \n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "Backends", "text": " \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "Backends", "text": " \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "Backends", "text": " \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "Backends", "text": " \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "Backends", "text": " \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n"}, {"name": "torch.backends.mkl.verbose", "path": "backends#torch.backends.mkl.verbose", "type": "Backends", "text": " \nclass torch.backends.mkl.verbose(enable) [source]\n \nOn-demand oneMKL verbosing functionality To make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named MKL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only. import torch\nmodel(data)\nwith torch.backends.mkl.verbose(torch.backends.mkl.VERBOSE_ON):\n    model(data)\n  Parameters \nlevel \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing   \n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "Backends", "text": " \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n"}, {"name": "torch.backends.mkldnn.verbose", "path": "backends#torch.backends.mkldnn.verbose", "type": "Backends", "text": " \nclass torch.backends.mkldnn.verbose(level) [source]\n \nOn-demand oneDNN (former MKL-DNN) verbosing functionality To make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named DNNL_VERBOSE. However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only. import torch\nmodel(data)\nwith torch.backends.mkldnn.verbose(torch.backends.mkldnn.VERBOSE_ON):\n    model(data)\n  Parameters \nlevel \u2013 Verbose level - VERBOSE_OFF: Disable verbosing - VERBOSE_ON: Enable verbosing - VERBOSE_ON_CREATION: Enable verbosing, including oneDNN kernel creation   \n"}, {"name": "torch.backends.mps.is_available()", "path": "backends#torch.backends.mps.is_available", "type": "Backends", "text": " \ntorch.backends.mps.is_available() [source]\n \nReturns a bool indicating if MPS is currently available.  Return type \nbool   \n"}, {"name": "torch.backends.mps.is_built()", "path": "backends#torch.backends.mps.is_built", "type": "Backends", "text": " \ntorch.backends.mps.is_built() [source]\n \nReturns whether PyTorch is built with MPS support. Note that this doesn\u2019t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.  Return type \nbool   \n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "Backends", "text": " \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n"}, {"name": "torch.backends.opt_einsum.enabled", "path": "backends#torch.backends.opt_einsum.enabled", "type": "Backends", "text": " \ntorch.backends.opt_einsum.enabled  \nA :class:bool that controls whether opt_einsum is enabled (True by default). If so, torch.einsum will use opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html) if available to calculate an optimal path of contraction for faster performance. If opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right. \n"}, {"name": "torch.backends.opt_einsum.get_opt_einsum()", "path": "backends#torch.backends.opt_einsum.get_opt_einsum", "type": "Backends", "text": " \ntorch.backends.opt_einsum.get_opt_einsum() [source]\n \nReturns the opt_einsum package if opt_einsum is currently available, else None.  Return type \nAny   \n"}, {"name": "torch.backends.opt_einsum.is_available()", "path": "backends#torch.backends.opt_einsum.is_available", "type": "Backends", "text": " \ntorch.backends.opt_einsum.is_available() [source]\n \nReturns a bool indicating if opt_einsum is currently available.  Return type \nbool   \n"}, {"name": "torch.backends.opt_einsum.strategy", "path": "backends#torch.backends.opt_einsum.strategy", "type": "Backends", "text": " \ntorch.backends.opt_einsum.strategy  \nA :class:str that specifies which strategies to try when torch.backends.opt_einsum.enabled is True. By default, torch.einsum will try the \u201cauto\u201d strategy, but the \u201cgreedy\u201d and \u201coptimal\u201d strategies are also supported. Note that the \u201coptimal\u201d strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum\u2019s docs (https://optimized-einsum.readthedocs.io/en/stable/path_finding.html). \n"}, {"name": "torch.baddbmm", "path": "generated/torch.baddbmm", "type": "Torch", "text": "torch.baddbmm  \ntorch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().  outi=\u03b2inputi+\u03b1(batch1i@batch2i)\\text{out}_i = \\beta\\ \\text{input}_i + \\alpha\\ (\\text{batch1}_i \\mathbin{@} \\text{batch2}_i) \n\nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta) \nalpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2} (\u03b1\\alpha) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n \n\n"}, {"name": "torch.bartlett_window", "path": "generated/torch.bartlett_window", "type": "Torch", "text": "torch.bartlett_window  \ntorch.bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBartlett window function.  w[n]=1\u2212\u22232nN\u22121\u22121\u2223={2nN\u22121if 0\u2264n\u2264N\u2212122\u22122nN\u22121if N\u221212<n<N,w[n] = 1 - \\left| \\frac{2n}{N-1} - 1 \\right| = \\begin{cases} \\frac{2n}{N - 1} & \\text{if } 0 \\leq n \\leq \\frac{N - 1}{2} \\\\ 2 - \\frac{2n}{N - 1} & \\text{if } \\frac{N - 1}{2} < n < N \\\\ \\end{cases}, \n\nwhere NN is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1, the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window  Return type \nTensor   \n\n"}, {"name": "torch.bernoulli", "path": "generated/torch.bernoulli", "type": "Torch", "text": "torch.bernoulli  \ntorch.bernoulli(input, *, generator=None, out=None) \u2192 Tensor  \nDraws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1. The ith\\text{i}^{th} element of the output tensor will draw a value 11 according to the ith\\text{i}^{th} probability value given in input.  outi\u223cBernoulli(p=inputi)\\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i}) \n\nThe returned out tensor only has values 0 or 1 and is of the same shape as input. out can have integral dtype, but input must have floating point dtype.  Parameters \ninput (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution  Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n \n\n"}, {"name": "torch.BFloat16Storage", "path": "storage#torch.BFloat16Storage", "type": "Storage", "text": " \nclass torch.BFloat16Storage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.bfloat16 [source]\n\n \n"}, {"name": "torch.BFloat16Storage.dtype", "path": "storage#torch.BFloat16Storage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.bfloat16 [source]\n\n"}, {"name": "torch.bincount", "path": "generated/torch.bincount", "type": "Torch", "text": "torch.bincount  \ntorch.bincount(input, weights=None, minlength=0) \u2192 Tensor  \nCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.  Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ninput (Tensor) \u2013 1-d int tensor \nweights (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input tensor. \nminlength (int) \u2013 optional, minimum number of bins. Should be non-negative.   Returns \na tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)  Return type \noutput (Tensor)   Example: >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n \n\n"}, {"name": "torch.bitwise_and", "path": "generated/torch.bitwise_and", "type": "Torch", "text": "torch.bitwise_and  \ntorch.bitwise_and(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])\n \n\n"}, {"name": "torch.bitwise_left_shift", "path": "generated/torch.bitwise_left_shift", "type": "Torch", "text": "torch.bitwise_left_shift  \ntorch.bitwise_left_shift(input, other, *, out=None) \u2192 Tensor  \nComputes the left arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion. The operation applied is:  outi=inputi<<otheri\\text{out}_i = \\text{input}_i << \\text{other}_i \n\n Parameters \n \ninput (Tensor or Scalar) \u2013 the first input tensor \nother (Tensor or Scalar) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2, 24], dtype=torch.int8)\n \n\n"}, {"name": "torch.bitwise_not", "path": "generated/torch.bitwise_not", "type": "Torch", "text": "torch.bitwise_not  \ntorch.bitwise_not(input, *, out=None) \u2192 Tensor  \nComputes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n \n\n"}, {"name": "torch.bitwise_or", "path": "generated/torch.bitwise_or", "type": "Torch", "text": "torch.bitwise_or  \ntorch.bitwise_or(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])\n \n\n"}, {"name": "torch.bitwise_right_shift", "path": "generated/torch.bitwise_right_shift", "type": "Torch", "text": "torch.bitwise_right_shift  \ntorch.bitwise_right_shift(input, other, *, out=None) \u2192 Tensor  \nComputes the right arithmetic shift of input by other bits. The input tensor must be of integral type. This operator supports broadcasting to a common shape and type promotion. In any case, if the value of the right operand is negative or is greater or equal to the number of bits in the promoted left operand, the behavior is undefined. The operation applied is:  outi=inputi>>otheri\\text{out}_i = \\text{input}_i >> \\text{other}_i \n\n Parameters \n \ninput (Tensor or Scalar) \u2013 the first input tensor \nother (Tensor or Scalar) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -7,  3], dtype=torch.int8)\n \n\n"}, {"name": "torch.bitwise_xor", "path": "generated/torch.bitwise_xor", "type": "Torch", "text": "torch.bitwise_xor  \ntorch.bitwise_xor(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n \n\n"}, {"name": "torch.blackman_window", "path": "generated/torch.blackman_window", "type": "Torch", "text": "torch.blackman_window  \ntorch.blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBlackman window function.  w[n]=0.42\u22120.5cos\u2061(2\u03c0nN\u22121)+0.08cos\u2061(4\u03c0nN\u22121)w[n] = 0.42 - 0.5 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right) + 0.08 \\cos \\left( \\frac{4 \\pi n}{N - 1} \\right) \n\nwhere NN is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN in above formula is in fact window_length+1\\text{window\\_length} + 1. Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1, the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},) containing the window  Return type \nTensor   \n\n"}, {"name": "torch.block_diag", "path": "generated/torch.block_diag", "type": "Torch", "text": "torch.block_diag  \ntorch.block_diag(*tensors) [source]\n \nCreate a block diagonal matrix from provided tensors.  Parameters \n*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.  Returns \nA 2 dimensional tensor with all the input tensors arranged in order such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.  Return type \nTensor   Example: >>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n \n\n"}, {"name": "torch.bmm", "path": "generated/torch.bmm", "type": "Torch", "text": "torch.bmm  \ntorch.bmm(input, mat2, *, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in input and mat2. input and mat2 must be 3-D tensors each containing the same number of matrices. If input is a (b\u00d7n\u00d7m)(b \\times n \\times m) tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p) tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p) tensor.  outi=inputi@mat2i\\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i \n\nThis operator supports TensorFloat32. On certain ROCm devices, when using float16 inputs this module will use different precision for backward.  Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().   Parameters \n \ninput (Tensor) \u2013 the first batch of matrices to be multiplied \nmat2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])\n \n\n"}, {"name": "torch.BoolStorage", "path": "storage#torch.BoolStorage", "type": "Storage", "text": " \nclass torch.BoolStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.bool [source]\n\n \n"}, {"name": "torch.BoolStorage.dtype", "path": "storage#torch.BoolStorage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.bool [source]\n\n"}, {"name": "torch.broadcast_shapes", "path": "generated/torch.broadcast_shapes", "type": "Torch", "text": "torch.broadcast_shapes  \ntorch.broadcast_shapes(*shapes) \u2192 Size [source]\n \nSimilar to broadcast_tensors() but for shapes. This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices. Example: >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])\n  Parameters \n*shapes (torch.Size) \u2013 Shapes of tensors.  Returns \nA shape compatible with all input shapes.  Return type \nshape (torch.Size)  Raises \nRuntimeError \u2013 If shapes are incompatible.   \n\n"}, {"name": "torch.broadcast_tensors", "path": "generated/torch.broadcast_tensors", "type": "Torch", "text": "torch.broadcast_tensors  \ntorch.broadcast_tensors(*tensors) \u2192 List of Tensors [source]\n \nBroadcasts the given tensors according to Broadcasting semantics.  Parameters \n*tensors \u2013 any number of tensors of the same type    Warning More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: >>> x = torch.arange(3).view(1, 3)\n>>> y = torch.arange(2).view(2, 1)\n>>> a, b = torch.broadcast_tensors(x, y)\n>>> a.size()\ntorch.Size([2, 3])\n>>> a\ntensor([[0, 1, 2],\n        [0, 1, 2]])\n \n\n"}, {"name": "torch.broadcast_to", "path": "generated/torch.broadcast_to", "type": "Torch", "text": "torch.broadcast_to  \ntorch.broadcast_to(input, shape) \u2192 Tensor  \nBroadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nshape (list, tuple, or torch.Size) \u2013 the new shape.    Example: >>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n \n\n"}, {"name": "torch.bucketize", "path": "generated/torch.bucketize", "type": "Torch", "text": "torch.bucketize  \ntorch.bucketize(input, boundaries, *, out_int32=False, right=False, out=None) \u2192 Tensor  \nReturns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is open. Note that this behavior is opposite the behavior of numpy.digitize. More formally, the returned index satisfies the following rules:   \nright returned index satisfies   \nFalse boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]  \nTrue boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]    Parameters \n \ninput (Tensor or Scalar) \u2013 N-D tensor or a Scalar containing the search value(s). \nboundaries (Tensor) \u2013 1-D tensor, must contain a strictly increasing sequence, or the return value is undefined.   Keyword Arguments \n \nout_int32 (bool, optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise. Default value is False, i.e. default output data type is torch.int64. \nright (bool, optional) \u2013 if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of boundaries (one pass the last index). In other words, if False, gets the lower bound index for each value in input from boundaries. If True, gets the upper bound index instead. Default value is False. \nout (Tensor, optional) \u2013 the output tensor, must be the same size as input if provided.    Example: >>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])\n \n\n"}, {"name": "torch.ByteStorage", "path": "storage#torch.ByteStorage", "type": "Storage", "text": " \nclass torch.ByteStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.uint8 [source]\n\n \n"}, {"name": "torch.ByteStorage.dtype", "path": "storage#torch.ByteStorage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.uint8 [source]\n\n"}, {"name": "torch.can_cast", "path": "generated/torch.can_cast", "type": "Torch", "text": "torch.can_cast  \ntorch.can_cast(from, to) \u2192 bool  \nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  Parameters \n \nfrom (dtype) \u2013 The original torch.dtype. \nto (dtype) \u2013 The target torch.dtype.    Example: >>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse\n \n\n"}, {"name": "torch.cartesian_prod", "path": "generated/torch.cartesian_prod", "type": "Torch", "text": "torch.cartesian_prod  \ntorch.cartesian_prod(*tensors) [source]\n \nDo cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.  Parameters \n*tensors (Tensor) \u2013 any number of 1 dimensional tensors.  Returns \nA tensor equivalent to converting all the input tensors into lists, do itertools.product on these lists, and finally convert the resulting list into tensor.  Return type \nTensor   Example: >>> import itertools\n>>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n \n\n"}, {"name": "torch.cat", "path": "generated/torch.cat", "type": "Torch", "text": "torch.cat  \ntorch.cat(tensors, dim=0, *, out=None) \u2192 Tensor  \nConcatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk(). torch.cat() can be best understood via examples.  See also torch.stack() concatenates the given sequence along a new dimension.   Parameters \n \ntensors (sequence of Tensors) \u2013 any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension. \ndim (int, optional) \u2013 the dimension over which the tensors are concatenated   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n \n\n"}, {"name": "torch.cdist", "path": "generated/torch.cdist", "type": "Torch", "text": "torch.cdist  \ntorch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary') [source]\n \nComputes batched the p-norm distance between each pair of the two collections of row vectors.  Parameters \n \nx1 (Tensor) \u2013 input tensor of shape B\u00d7P\u00d7MB \\times P \\times M. \nx2 (Tensor) \u2013 input tensor of shape B\u00d7R\u00d7MB \\times R \\times M. \np (float) \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty]. \ncompute_mode (str) \u2013 \u2018use_mm_for_euclid_dist_if_necessary\u2019 - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 \u2018use_mm_for_euclid_dist\u2019 - will always use matrix multiplication approach to calculate euclidean distance (p = 2) \u2018donot_use_mm_for_euclid_dist\u2019 - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.   Return type \nTensor   If x1 has shape B\u00d7P\u00d7MB \\times P \\times M and x2 has shape B\u00d7R\u00d7MB \\times R \\times M then the output will have shape B\u00d7P\u00d7RB \\times P \\times R. This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty). When p=0p = 0 it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty, the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). Example >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n \n\n"}, {"name": "torch.ceil", "path": "generated/torch.ceil", "type": "Torch", "text": "torch.ceil  \ntorch.ceil(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element. For integer inputs, follows the array-api convention of returning a copy of the input tensor.  outi=\u2308inputi\u2309\\text{out}_{i} = \\left\\lceil \\text{input}_{i} \\right\\rceil \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n \n\n"}, {"name": "torch.chain_matmul", "path": "generated/torch.chain_matmul", "type": "Torch", "text": "torch.chain_matmul  \ntorch.chain_matmul(*matrices, out=None) [source]\n \nReturns the matrix product of the NN 2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN is 1, then this is a no-op - the original matrix is returned as is.  Warning torch.chain_matmul() is deprecated and will be removed in a future PyTorch release. Use torch.linalg.multi_dot() instead, which accepts a list of two or more tensors rather than multiple arguments.   Parameters \n \nmatrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined. \nout (Tensor, optional) \u2013 the output tensor. Ignored if out = None.   Returns \nif the ithi^{th} tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}, then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}.  Return type \nTensor   Example: >>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> # will raise a deprecation warning\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n \n\n"}, {"name": "torch.CharStorage", "path": "storage#torch.CharStorage", "type": "Storage", "text": " \nclass torch.CharStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.int8 [source]\n\n \n"}, {"name": "torch.CharStorage.dtype", "path": "storage#torch.CharStorage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.int8 [source]\n\n"}, {"name": "torch.cholesky", "path": "generated/torch.cholesky", "type": "Torch", "text": "torch.cholesky  \ntorch.cholesky(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA or for batches of symmetric positive-definite matrices. If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:  A=UTUA = U^TU\n\nIf upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:  A=LLTA = LL^T\n\nIf upper is True, and AA is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.  Warning torch.cholesky() is deprecated in favor of torch.linalg.cholesky() and will be removed in a future PyTorch release. L = torch.cholesky(A) should be replaced with L = torch.linalg.cholesky(A)\n U = torch.cholesky(A, upper=True) should be replaced with U = torch.linalg.cholesky(A).mH\n This transform will produce equivalent results for all valid (symmetric positive definite) inputs.   Parameters \n \ninput (Tensor) \u2013 the input tensor AA of size (\u2217,n,n)(*, n, n) where * is zero or more batch dimensions consisting of symmetric positive-definite matrices. \nupper (bool, optional) \u2013 flag that indicates whether to return a upper or lower triangular matrix. Default: False\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output matrix   Example: >>> a = torch.randn(3, 3)\n>>> a = a @ a.mT + 1e-3 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> l @ l.mT\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2) # Example for batched input\n>>> a = a @ a.mT + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = l @ l.mT\n>>> torch.dist(z, a)\ntensor(2.3842e-07)\n \n\n"}, {"name": "torch.cholesky_inverse", "path": "generated/torch.cholesky_inverse", "type": "Torch", "text": "torch.cholesky_inverse  \ntorch.cholesky_inverse(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the inverse of a symmetric positive-definite matrix AA using its Cholesky factor uu: returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uu is lower triangular such that the returned tensor is  inv=(uuT)\u22121inv = (uu^{{T}})^{{-1}} \n\nIf upper is True or not provided, uu is upper triangular such that the returned tensor is  inv=(uTu)\u22121inv = (u^T u)^{{-1}} \n\nSupports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if AA is a batch of matrices then the output has the same batch dimensions.  Parameters \n \ninput (Tensor) \u2013 the input tensor AA of size (\u2217,n,n)(*, n, n), consisting of symmetric positive-definite matrices where \u2217* is zero or more batch dimensions. \nupper (bool, optional) \u2013 flag that indicates whether to return a upper or lower triangular matrix. Default: False   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for inv   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.linalg.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a = torch.randn(3, 2, 2) # Example for batched input\n>>> a = a @ a.mT + 1e-03 # make symmetric positive-definite\n>>> l = torch.linalg.cholesky(a)\n>>> z = l @ l.mT\n>>> torch.dist(z, a)\ntensor(3.5894e-07)\n \n\n"}, {"name": "torch.cholesky_solve", "path": "generated/torch.cholesky_solve", "type": "Torch", "text": "torch.cholesky_solve  \ntorch.cholesky_solve(input, input2, upper=False, *, out=None) \u2192 Tensor  \nSolves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu. If upper is False, uu is and lower triangular and c is returned such that:  c=(uuT)\u22121bc = (u u^T)^{{-1}} b \n\nIf upper is True or not provided, uu is upper triangular and c is returned such that:  c=(uTu)\u22121bc = (u^T u)^{{-1}} b \n\ntorch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.  Parameters \n \ninput (Tensor) \u2013 input matrix bb of size (\u2217,m,k)(*, m, k), where \u2217* is zero or more batch dimensions \ninput2 (Tensor) \u2013 input matrix uu of size (\u2217,m,m)(*, m, m), where \u2217* is zero of more batch dimensions composed of upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: False.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for c   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.linalg.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n \n\n"}, {"name": "torch.chunk", "path": "generated/torch.chunk", "type": "Torch", "text": "torch.chunk  \ntorch.chunk(input, chunks, dim=0) \u2192 List of Tensors  \nAttempts to split a tensor into the specified number of chunks. Each chunk is a view of the input tensor.  Note This function may return fewer than the specified number of chunks!   See also torch.tensor_split() a function that always returns exactly the specified number of chunks  If the tensor size along the given dimension dim is divisible by chunks, all returned chunks will be the same size. If the tensor size along the given dimension dim is not divisible by chunks, all returned chunks will be the same size, except the last one. If such division is not possible, this function may return fewer than the specified number of chunks.  Parameters \n \ninput (Tensor) \u2013 the tensor to split \nchunks (int) \u2013 number of chunks to return \ndim (int) \u2013 dimension along which to split the tensor    Example >>> torch.arange(11).chunk(6)\n(tensor([0, 1]),\n tensor([2, 3]),\n tensor([4, 5]),\n tensor([6, 7]),\n tensor([8, 9]),\n tensor([10]))\n>>> torch.arange(12).chunk(6)\n(tensor([0, 1]),\n tensor([2, 3]),\n tensor([4, 5]),\n tensor([6, 7]),\n tensor([8, 9]),\n tensor([10, 11]))\n>>> torch.arange(13).chunk(6)\n(tensor([0, 1, 2]),\n tensor([3, 4, 5]),\n tensor([6, 7, 8]),\n tensor([ 9, 10, 11]),\n tensor([12]))\n \n\n"}, {"name": "torch.clamp", "path": "generated/torch.clamp", "type": "Torch", "text": "torch.clamp  \ntorch.clamp(input, min=None, max=None, *, out=None) \u2192 Tensor  \nClamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:  yi=min\u2061(max\u2061(xi,min_valuei),max_valuei)y_i = \\min(\\max(x_i, \\text{min\\_value}_i), \\text{max\\_value}_i) \n\nIf min is None, there is no lower bound. Or, if max is None there is no upper bound.  Note If min is greater than max torch.clamp(..., min, max) sets all elements in input to the value of max.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nmin (Number or Tensor, optional) \u2013 lower-bound of the range to be clamped to \nmax (Number or Tensor, optional) \u2013 upper-bound of the range to be clamped to   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n>>> min = torch.linspace(-1, 1, steps=4)\n>>> torch.clamp(a, min=min)\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])\n \n\n"}, {"name": "torch.clip", "path": "generated/torch.clip", "type": "Torch", "text": "torch.clip  \ntorch.clip(input, min=None, max=None, *, out=None) \u2192 Tensor  \nAlias for torch.clamp(). \n\n"}, {"name": "torch.clone", "path": "generated/torch.clone", "type": "Torch", "text": "torch.clone  \ntorch.clone(input, *, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of input.  Note This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format.   \n\n"}, {"name": "torch.column_stack", "path": "generated/torch.column_stack", "type": "Torch", "text": "torch.column_stack  \ntorch.column_stack(tensors, *, out=None) \u2192 Tensor  \nCreates a new tensor by horizontally stacking the tensors in tensors. Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])\n \n\n"}, {"name": "torch.combinations", "path": "generated/torch.combinations", "type": "Torch", "text": "torch.combinations  \ntorch.combinations(input, r=2, with_replacement=False) \u2192 seq  \nCompute combinations of length rr of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters \n \ninput (Tensor) \u2013 1D vector. \nr (int, optional) \u2013 number of elements to combine \nwith_replacement (bool, optional) \u2013 whether to allow duplication in combination   Returns \nA tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n \n\n"}, {"name": "torch.compile", "path": "generated/torch.compile", "type": "Torch", "text": "torch.compile  \ntorch.compile(model=None, *, fullgraph=False, dynamic=None, backend='inductor', mode=None, options=None, disable=False) [source]\n \nOptimizes given model/function using TorchDynamo and specified backend. Concretely, for every frame executed within the compiled region, we will attempt to compile it and cache the compiled result on the code object for future use. A single frame may be compiled multiple times if previous compiled results are not applicable for subsequent calls (this is called a \u201cguard failure), you can use TORCH_LOGS=guards to debug these situations. Multiple compiled results can be associated with a frame up to torch._dynamo.config.cache_size_limit, which defaults to 64; at which point we will fall back to eager. Note that compile caches are per code object, not frame; if you dynamically create multiple copies of a function, they will all share the same code cache.  Parameters \n \nmodel (Callable) \u2013 Module/function to optimize \nfullgraph (bool) \u2013 Whether it is ok to break model into several subgraphs \ndynamic (bool or None) \u2013 Use dynamic shape tracing. When this is True, we will up-front attempt to generate a kernel that is as dynamic as possible to avoid recompilations when sizes change. This may not always work as some operations/optimizations will force specialization; use TORCH_LOGS=dynamic to debug overspecialization. When this is False, we will NEVER generate dynamic kernels, we will always specialize. By default (None), we automatically detect if dynamism has occurred and compile a more dynamic kernel upon recompile. \nbackend (str or Callable) \u2013 \nbackend to be used  \u201dinductor\u201d is the default backend, which is a good balance between performance and overhead Non experimental in-tree backends can be seen with torch._dynamo.list_backends()\n Experimental or debug in-tree backends can be seen with torch._dynamo.list_backends(None)\n To register an out-of-tree custom backend: https://pytorch.org/docs/main/compile/custom-backends.html\n   \nmode (str) \u2013 \nCan be either \u201cdefault\u201d, \u201creduce-overhead\u201d, \u201cmax-autotune\u201d or \u201cmax-autotune-no-cudagraphs\u201d  \u201ddefault\u201d is the default mode, which is a good balance between performance and overhead \u201dreduce-overhead\u201d is a mode that reduces the overhead of python with CUDA graphs, useful for small batches. Reduction of overhead can come at the cost of more memory usage, as we will cache the workspace memory required for the invocation so that we do not have to reallocate it on subsequent runs. Reduction of overhead is not guaranteed to work; today, we only reduce overhead for CUDA only graphs which do not mutate inputs. There are other circumstances where CUDA graphs are not applicable; use TORCH_LOG=perf_hints to debug. \u201dmax-autotune\u201d is a mode that leverages Triton based matrix multiplications and convolutions It enables CUDA graphs by default. \u201dmax-autotune-no-cudagraphs\u201d is a mode similar to \u201cmax-autotune\u201d but without CUDA graphs To see the exact configs that each mode sets you can call torch._inductor.list_mode_options()\n   \noptions (dict) \u2013 \nA dictionary of options to pass to the backend. Some notable ones to try out are  \nepilogue_fusion which fuses pointwise ops into templates. Requires max_autotune to also be set \nmax_autotune which will profile to pick the best matmul configuration \nfallback_random which is useful when debugging accuracy issues \nshape_padding which pads matrix shapes to better align loads on GPUs especially for tensor cores \ntriton.cudagraphs which will reduce the overhead of python with CUDA graphs \ntrace.enabled which is the most useful debugging flag to turn on \ntrace.graph_diagram which will show you a picture of your graph after fusion For inductor you can see the full list of configs that it supports by calling torch._inductor.list_options()\n   \ndisable (bool) \u2013 Turn torch.compile() into a no-op for testing   Return type \nCallable   Example: @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\ndef foo(x):\n    return torch.sin(x) + torch.cos(x)\n \n\n"}, {"name": "torch.compiled_with_cxx11_abi", "path": "generated/torch.compiled_with_cxx11_abi", "type": "Torch", "text": "torch.compiled_with_cxx11_abi  \ntorch.compiled_with_cxx11_abi() [source]\n \nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1  Return type \nbool   \n\n"}, {"name": "torch.compiler", "path": "torch.compiler", "type": "Miscellaneous", "text": "torch.compiler torch.compiler is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is torch.compile. torch.compile is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. torch.compile is written in Python and it marks the transition of PyTorch from C++ to Python. torch.compile leverages the following underlying technologies:  \nTorchDynamo (torch._dynamo) is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the torch.compiler namespace. \nTorchInductor is the default torch.compile deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through torch.compile possible. For NVIDIA and AMD GPUs, it leverages OpenAI Triton as the key building block. \nAOT Autograd captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass \u201cahead-of-time\u201d. This enables acceleration of both forwards and backwards pass using TorchInductor.   Note In some cases, the terms torch.compile, TorchDynamo, torch.compiler might be used interchangeably in this documentation.  As mentioned above, to run your workflows faster, torch.compile through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as inductor, TorchDynamo has a list of supported backends developed by our partners, which can be see by running torch.compiler.list_backends() each of which with its optional dependencies. Some of the most commonly used backends include: Training & inference backends   \nBackend Description   \ntorch.compile(m, backend=\"inductor\") Uses the TorchInductor backend. Read more  \ntorch.compile(m, backend=\"cudagraphs\") CUDA graphs with AOT Autograd. Read more  \ntorch.compile(m, backend=\"ipex\") Uses IPEX on CPU. Read more  \ntorch.compile(m, backend=\"onnxrt\") Uses ONNX Runtime for training on CPU/GPU. Read more   Inference-only backends   \nBackend Description   \ntorch.compile(m, backend=\"tensorrt\") Uses ONNX Runtime to run TensorRT for inference optimizations. Read more  \ntorch.compile(m, backend=\"ipex\") Uses IPEX for inference on CPU. Read more  \ntorch.compile(m, backend=\"tvm\") Uses Apache TVM for inference optimizations. Read more   Read More Getting Started for PyTorch Users  Getting Started torch.compiler API reference PyTorch 2.0 Performance Dashboard TorchDynamo APIs for fine-grained tracing TorchInductor GPU Profiling Profiling to understand torch.compile performance Frequently Asked Questions PyTorch 2.0 Troubleshooting  Deep Dive for PyTorch Developers  TorchDynamo Deep Dive Guards Overview Dynamic shapes PyTorch 2.0 NNModule Support Best Practices for Backends CUDAGraph Trees Fake tensor  HowTo for PyTorch Backend Vendors  Custom Backends Writing Graph Transformations on ATen IR IRs \n"}, {"name": "torch.compiler.allow_in_graph()", "path": "generated/torch.compiler.allow_in_graph#torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": " \ntorch.compiler.allow_in_graph(fn) [source]\n \nCustomize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions  Parameters \nfn \u2013 A callable representing the function to be included in the graph.    Warning allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues.  \n"}, {"name": "torch.compiler.assume_constant_result()", "path": "generated/torch.compiler.assume_constant_result#torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": " \ntorch.compiler.assume_constant_result(fn) [source]\n \nThis function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn  Parameters \nfn \u2013 The function to be marked as having a constant result.    Warning assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not  \n"}, {"name": "torch.compiler.Best Practices for Backends", "path": "torch.compiler_best_practices_for_backends", "type": "Miscellaneous", "text": "Best Practices for Backends x86 CPU Compiled workloads on modern x86 CPUs are usually optimized by Single Instruction Multiple Data (SIMD) instruction sets. SIMD is a typical parallel processing technique for high performance computing, such as deep learning model training and inference. With SIMD applied, each compute unit performs the same instruction with different allocated data at any given time slot. The most commonly deployed x86 instruction set architectures (ISAs) enabling SIMD include AVX, AVX2, AVX-512 and AMX. You can check supported ISAs for your machine by using the collect_env script. As the script provides complete environment information for PyTorch, we can use grep to extract the line containing ISA information: python collect_env.py | grep \"a[(v|m)]x\"\n Normally, if AVX-512 is supported, instructions start with \u201cavx512\u201d (like avx512f, avx512bw, avx512_vnni) should be observed. If AMX is supported, instructions start with \u201camx\u201d (like amx_tile, amx_bf16, amx_int8) should be observed. Specifically, with a server having AMX instructions enabled, workloads performance can be further boosted by leveraging AMX.\n"}, {"name": "torch.compiler.compile()", "path": "generated/torch.compiler.compile#torch.compiler.compile", "type": "Miscellaneous", "text": " \ntorch.compiler.compile(*args, **kwargs) [source]\n \nSee torch.compile() for details on the arguments for this function. \n"}, {"name": "torch.compiler.CUDAGraph Trees", "path": "torch.compiler_cudagraph_trees", "type": "Miscellaneous", "text": "CUDAGraph Trees CUDAGraph Background For a longer background on CUDAGraphs, read accelerating pytorch with CUDAGraphs. CUDA Graphs, which made its debut in CUDA 10, let a series of CUDA kernels to be defined and encapsulated as a single unit, i.e., a graph of operations, rather than a sequence of individually-launched operations. It provides a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduces the launching overheads. CUDA Graphs can give large speedups, especially for models with high CPU overhead or small compute. There are a number of limitations from requiring the same kernels to be run with the same arguments and dependencies, and memory addresses.  Control Flow is not possible Kernels which trigger host to device syncs (such as .item()) errors All input arguments to kernels are fixed to what they were recorded CUDA Memory addresses are fixed, however the values of the memory at those addresses can change No Essential CPU ops or CPU side effects  PyTorch CUDAGraph Integration PyTorch provides a convenience wrapper around CUDAGraphs that handles a couple of tricky interactions with PyTorch\u2019s caching allocator. The CachingAllocator uses a separate memory pool for all the new allocations. During CUDAGraph recording, memory is accounted for, allocated, and freed exactly as during eager run. On replay, just the kernels are invoked, and there are no changes to the allocator. Subsequent to initial recording, the allocator does not know which memory is actively being used in user programs. Using a separate memory pool between eager allocations and cudagraph allocations may increase the memory of your program if there is substantial memory allocated to both. Make Graphed Callables Make Graphed Callables is a PyTorch Abstraction to share a single memory pool over a series of callables. Graphed Callables takes advantage of the fact that on CUDA Graph recording, memory is exactly accounted for by the caching allocator to safely share memory between separate CUDA Graph recordings. In each invocation, outputs are preserved as live memory, preventing one callable from overwriting the live memory of another. Graphed Callables can only be invoked in a single order; memory addresses from the first run are burned into the second, and so forth. TorchDynamo Previous CUDA Graphs Integration Running with cudagraph_trees=False does not reuse memory across separate graph captures, which can lead to large memory regressions. Even for a model that has no graph breaks, this has issues. The forward and backward are separate graph captures, so the memory pools for forward and backward are not shared. In particular, memory for activations that are saved in the forward cannot be reclaimed in the backward. CUDAGraph Trees Integration Like Graph Callables, CUDA Graph Trees use a single memory pool across all graph captures. However, instead of requiring a single sequence of invocations, CUDA Graph Trees create separate trees of CUDA Graph captures. Let\u2019s take a look at an illustrative example: @torch.compile\ndef foo(x):\n    # GRAPH 1\n    y = x * x * x\n    # graph break triggered here\n    if y.sum() > 0:\n        # GRAPH 2\n        z = y ** y\n    else:\n        # GRAPH 3\n        z = (y.abs() ** y.abs())\n    torch._dynamo.graph_break()\n    # GRAPH 4\n    return z * torch.rand_like(z)\n\n    # the first run warms up each graph, which does things like CuBlas or Triton benchmarking\n    foo(torch.arange(0, 10), device=\"cuda\")\n    # The second run does a CUDA Graph recording, and replays it\n    foo(torch.arange(0, 10), device=\"cuda\")\n    # Finally we hit the optimized, CUDA Graph replay path\n    foo(torch.arange(0, 10), device=\"cuda\")\n In this example, there are two separate paths that we make through the function: 1 -> 2 -> 4, or 1 -> 3 -> 4. We share all of the memory in a single memory pool between separate recordings by building up a tape of CUDA Graph recordings, in this instance, 1 -> 2 -> 4. We add invariants to ensure that memory is always in the same location as it were recorded, and no live tensors exist in user programs that might be overwritten.  Same constraints from CUDA Graphs apply: same kernels must be invoked with the same arguments (static sizes, addresses, etc) The same pattern of memory must be observed between recording and replay: if a tensor output of one graph dies subsequent to another graph during recording, it must also do so during replay. Live memory in the CUDA pool forces a dependence between two recordings These recordings can only be invoked in a single order 1 - > 2 -> 4  All of the memory is shared in a single memory pool, so there is no additional memory overhead compared to eager. Now, what happens if we were to hit a new path and run Graph 3? Graph 1 gets replayed, and then we hit Graph 3, which we have not yet recorded. On graph replays, the private memory pool is not updated, so y is not reflected in the allocator. Without care, we would overwrite it. To support reusing the same memory pool after replaying other graphs, we checkpoint the memory pool back to its state at the end of graph 1. Now that our live tensors are reflected in the caching allocator, we are safe to run a new graph. First, we would hit the optimized, CUDAGraph.replay() path that we have already recorded in graph 1. Then we would hit Graph 3. Just as before, we will need to warm up the graph once before recording. On the warmup run, the memory addresses are not fixed, so graph 4 will also fallback to the inductor, non-cudagraph invocation. The second time we hit graph 3 we are warmed up and ready to record. We record graph 2 and then record graph 4 again since the input memory addresses have changed. This creates a tree of CUDA Graph recordings. A CUDA Graph Tree!   1\n / \\\\\n2   3\n \\\\   \\\\\n  4   4\n Limitations Because CUDA Graph fixes memory addresses, CUDA Graphs do not have a great way of handling live tensors from a previous invocation. Let\u2019s say we are benchmarking running inference with the following code: import torch\n\ndef my_model(x):\n    y = torch.matmul(x, x)\n    return y\n\nx = torch.randn(10, 10)\ny1 = my_model(x)\ny2 = my_model(x)\n In the Separate CUDA Graph implementation, the output from the first invocation will be overwritten by the second invocation. Similarly, in CUDA Graph Trees, naively, the live output of the first run would force a dependency between the first run and the second run, and we would never hit the optimized cudagraph replay invocation. CUDA Graph Trees will ignore outputs from a previous run of torch.compile and not force a memory dependency. In training, we will not ignore outputs from a previous run of torch.compile if we have pending backwards that have not been invoked. TODO - add API to increment generation manually, error on access of prior storage Comparisons   \nFootguns Separate CudaGraph CUDAGraph Trees   \nMemory Can Increase On each graph compilation (new sizes, etc.) If you are also running non-cudagraph memory  \nRecordings On any new invocation of a graph Will re-record on any new, unique path you take through your program  \nFootguns Invocation of one graph will overwrite prior invocation Cannot persist memory between separate runs through your model - one training loop training, or one run of inference  \n"}, {"name": "torch.compiler.Custom Backends", "path": "torch.compiler_custom_backends", "type": "Miscellaneous", "text": "Custom Backends Overview torch.compile provides a straightforward method to enable users to define custom backends. A backend function has the contract (gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]) -> Callable. Backend functions can be called by TorchDynamo, the graph tracing component of torch.compile, after tracing an FX graph and are expected to return a compiled function that is equivalent to the traced FX graph. The returned callable should have the same contract as the forward function of the original torch.fx.GraphModule passed into the backend: (*args: torch.Tensor) -> List[torch.Tensor]. In order for TorchDynamo to call your backend, pass your backend function as the backend kwarg in torch.compile. For example, import torch\n\ndef my_custom_backend(gm, example_inputs):\n    return gm.forward\n\ndef f(...):\n    ...\n\nf_opt = torch.compile(f, backend=my_custom_backend)\n\n@torch.compile(backend=my_custom_backend)\ndef g(...):\n    ...\n See below for more examples. Registering Custom Backends You can register your backend using the register_backend decorator, for example, from torch._dynamo.optimizations import register_backend\n\n@register_backend\ndef my_compiler(gm, example_inputs):\n    ...\n Besides the register_backend decorator, if your backend is in another python package, you could also register your backend through entry points of python package, which provides a way for a package to register a plugin for another one.  Hint You can learn more about entry_points in the python packaging documentation.  To register your backend through entry_points, you could add your backend function to the torch_dynamo_backends entry point group in the setup.py file of your package like: ...\nsetup(\n    ...\n    'torch_dynamo_backends': [\n        'my_compiler = your_module.submodule:my_compiler',\n    ]\n    ...\n)\n Please replace the my_compiler before = to the name of your backend\u2019s name and replace the part after = to the module and function name of your backend function. The entry point will be added to your python environment after the installation of the package. When you call torch.compile(model, backend=\"my_compiler\"), PyTorch would first search the backend named my_compiler that has been registered with register_backend. If not found, it will continue to search in all backends registered via entry_points. Registration serves two purposes:  You can pass a string containing your backend function\u2019s name to torch.compile instead of the function itself, for example, torch.compile(model, backend=\"my_compiler\"). It is required for use with the minifier. Any generated code from the minifier must call your code that registers your backend function, typically through an import statement.  Custom Backends after AOTAutograd It is possible to define custom backends that are called by AOTAutograd rather than TorchDynamo. This is useful for 2 main reasons:  Users can define backends that support model training, as AOTAutograd can generate the backward graph for compilation. AOTAutograd produces FX graphs consisting of canonical Aten ops. As a result, custom backends only need to support the canonical Aten opset, which is a significantly smaller opset than the entire torch/Aten opset.  Wrap your backend with torch._dynamo.optimizations.training.aot_autograd and use torch.compile with the backend kwarg as before. Backend functions wrapped by aot_autograd should have the same contract as before. Backend functions are passed to aot_autograd through the fw_compiler (forward compiler) or bw_compiler (backward compiler) kwargs. If bw_compiler is not specified, the backward compile function defaults to the forward compile function. One caveat is that AOTAutograd requires compiled functions returned by backends to be \u201cboxed\u201d. This can be done by wrapping the compiled function with functorch.compile.make_boxed_func. For example, from torch._dynamo.optimizations.training import aot_autograd\nfrom functorch.compile import make_boxed_func\n\ndef my_compiler(gm, example_inputs):\n    return make_boxed_func(gm.forward)\n\nmy_backend = aot_autograd(fw_compiler=my_compiler)  # bw_compiler=my_compiler\n\nmodel_opt = torch.compile(model, backend=my_backend)\n Examples Debugging Backend If you want to better understand what is going on during a compilation, you can create a custom compiler, which is referred to as backend in this section, that will print pretty print the fx GraphModule extracted from Dynamo\u2019s bytecode analysis and return a forward() callable. For example: from typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef fn(x, y):\n    a = torch.cos(x)\n    b = torch.sin(y)\n    return a + b\nfn(torch.randn(10), torch.randn(10))\n Running the above example produces the following output: my_compiler() called with FX graph:\nopcode         name    target                                                  args        kwargs\n-------------  ------  ------------------------------------------------------  ----------  --------\nplaceholder    x       x                                                       ()          {}\nplaceholder    y       y                                                       ()          {}\ncall_function  cos     <built-in method cos of type object at 0x7f1a894649a8>  (x,)        {}\ncall_function  sin     <built-in method sin of type object at 0x7f1a894649a8>  (y,)        {}\ncall_function  add     <built-in function add>                                 (cos, sin)  {}\noutput         output  output                                                  ((add,),)   {}\n This works for torch.nn.Module as well as shown below: from typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\nclass MockModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.relu = torch.nn.ReLU()\n    def forward(self, x):\n        return self.relu(torch.cos(x))\nmod = MockModule()\noptimized_mod = torch.compile(mod, backend=my_compiler)\noptimized_mod(torch.randn(10))\n Let\u2019s take a look at one more example with control flow: from typing import List\nimport torch\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n@torch.compile(backend=my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n Running this example produces the following output: my_compiler() called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    <built-in method abs of type object at 0x7f8d259298a0>  (a,)              {}\ncall_function  add      <built-in function add>                                 (abs_1, 1)        {}\ncall_function  truediv  <built-in function truediv>                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       <built-in function lt>                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     <built-in function mul>  (b, -1)      {}\ncall_function  mul_1   <built-in function mul>  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\n\nmy_compiler() called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     <built-in function mul>  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n The order of the last two graphs is nondeterministic depending on which one is encountered first by the just-in-time compiler. Speedy Backend Integrating a custom backend that offers superior performance is also easy and we\u2019ll integrate a real one with optimize_for_inference: def optimize_for_inference_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    scripted = torch.jit.script(gm)\n    return torch.jit.optimize_for_inference(scripted)\n And then you should be able to optimize any existing code with: @torch.compile(backend=optimize_for_inference_compiler)\ndef code_to_accelerate():\n    ...\n Composable Backends TorchDynamo includes many backends, which can be found in backends.py or torch._dynamo.list_backends(). You can combine these backends together with the following code: from torch._dynamo.optimizations import BACKENDS\n def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n     try:\n         trt_compiled = BACKENDS[\"tensorrt\"](gm, example_inputs)\n         if trt_compiled is not None:\n             return trt_compiled\n     except Exception:\n         pass\n     # first backend failed, try something else...\n     try:\n         inductor_compiled = BACKENDS[\"inductor\"](gm, example_inputs)\n         if inductor_compiled is not None:\n             return inductor_compiled\n     except Exception:\n         pass\n     return gm.forward\n\n"}, {"name": "torch.compiler.disable()", "path": "generated/torch.compiler.disable#torch.compiler.disable", "type": "Miscellaneous", "text": " \ntorch.compiler.disable(fn=None, recursive=True) [source]\n \nThis function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions  Parameters \n \nfn (optional) \u2013 The function to disable \nrecursive (optional) \u2013 A boolean value indicating whether the disabling should be recursive.    \n"}, {"name": "torch.compiler.Frequently Asked Questions", "path": "torch.compiler_faq", "type": "Miscellaneous", "text": "Frequently Asked Questions Author: Mark Saroufim Does torch.compile support training? torch.compile supports training, using AOTAutograd to capture backwards:  The .forward() graph and optimizer.step() is captured by TorchDynamo\u2019s python evalframe frontend. For each segment of .forward() that torchdynamo captures, it uses AOTAutograd to generate a backward graph segment. Each pair of forward and backward graph are (optionally) min-cut partitioned to save the minimal state between forward and backward. The forward and backward pairs are wrapped in autograd.function modules. Usercode calling.backward() still triggers eager\u2019s autograd engine, which runs each compiled backward graph as if it were one op, also running any non-compiled eager ops\u2019 .backward() functions.  Do you support Distributed code? torch.compile supports DistributedDataParallel (DDP). Support for other distributed training libraries is being considered. The main reason why Distributed code is challenging with dynamo is because AOTAutograd unrolls both the forward and backward pass and provides 2 graphs for backends to optimize. This is a problem for distributed code because we\u2019d like to ideally overlap communication operations with computations. Eager pytorch accomplishes this in different ways for DDP/FSDP- using autograd hooks, module hooks, and modifications/mutations of module states. In a naive application of dynamo, hooks that should run directly after an operation during backwards may be delayed until after the entire compiled region of backwards ops, due to how AOTAutograd compiled functions interact with dispatcher hooks. The basic strategy for optimizing DDP with Dynamo is outlined in distributed.py where the main idea will be to graph break on DDP bucket boundaries. When each node in DDP needs to synchronize its weights with the other nodes it organizes its gradients and parameters into buckets which reduces communication times and allows a node to broadcast a fraction of its gradients to other waiting nodes. Graph breaks in distributed code mean you can expect dynamo and its backends to optimize the compute overhead of a distributed program but not its communication overhead. Graph-breaks may interfere with compilation speedups, if the reduced graph-size robs the compiler of fusion opportunities. However, there are diminishing returns with increasing graph size since most of the current compute optimizations are local fusions. So in practice this approach may be sufficient. Do I still need to export whole graphs? For the vast majority of models you probably don\u2019t and you can use torch.compile() as is but there are a few situations where full graphs are necessary and you can can ensure a full graph by simply running torch.compile(..., nopython=True). These situations include:  Large scale training runs, such as $250K+ that require pipeline parallelism and other advanced sharding strategies. Inference optimizers like TensorRT or AITemplate that rely on fusing much more aggressively than training optimizers. Mobile training or inference.  Future work will include tracing communication operations into graphs, coordinating these operations with compute optimizations, and optimizing the communication operations. Why is my code crashing? If your code ran just fine without torch.compile and started to crash with it is enabled, then the most important first step is figuring out which part of the stack your failure occurred. To troubleshoot that, follow the steps below and only try the next step if the previous one succeeded.  \ntorch.compile(..., backend=\"eager\") which only runs TorchDynamo forward graph capture and then runs the captured graph with PyTorch. If this fails then there\u2019s an issue with TorchDynamo. \ntorch.compile(..., backend=\"aot_eager\") which runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph without any additional backend compiler steps. PyTorch eager will then be used to run the forward and backward graphs. If this fails then there\u2019s an issue with AOTAutograd. \ntorch.compile(..., backend=\"inductor\") which runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph with the TorchInductor compiler. If this fails then there\u2019s an issue with TorchInductor  Why is compilation slow?  \nDynamo Compilation\u2013 TorchDynamo has a builtin stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by calling torch._dynamo.utils.compile_times() after executing torch._dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name. \nInductor Compilation\u2013 TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump. env TORCH_COMPILE_DEBUG=1 python repro.py. This is a debugging tool designed to make it easier to debug/understand the internals of TorchInductor with an output that will look something like this Each file in that debug trace can be enabled/disabled via torch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate. See the example debug directory output for more examples. \nExcessive Recompilation When TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up to torch._dynamo.config.cache_size_limit times. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it. The recompilation profiler automates the process of setting TorchDynamo\u2019s cache limit to 1 and running your program under an observation-only \u2018compiler\u2019 that records the causes of any guard failures. You should be sure to run your program for at least as long (as many iterations) as you were running when you ran into trouble, and the profiler will accumulate statistics over this duration.  from torch._dynamo.utils import CompileProfiler\n\ndef my_model():\n    ...\n\nwith CompileProfiler() as prof:\n    profiler_model = torch.compile(my_model, backend=prof)\n    profiler_model()\n    print(prof.report())\n Why are you recompiling in production? In some cases, you may not want unexpected compiles after a program has warmed up. For example, if you are serving production traffic in a latency critical application. For this, TorchDynamo provides an alternate mode where prior compiled graphs are used, but no new ones are generated: frozen_toy_example = dynamo.run(toy_example)\nfrozen_toy_example(torch.randn(10), torch.randn(10))\n How are you speeding up my code? There are 3 major ways to accelerate PyTorch code:  Kernel fusion via vertical fusions which fuse sequential operations to avoid excessive read/writes. For example, fuse 2 subsequent cosines means you can can do 1 read 1 write instead 2 reads 2 writes 2. Horizontal fusion: the simplest example being batching where a single matrix is multiplied with a batch of examples but the more general scenario is a grouped GEMM where a group of matrix multiplications are scheduled together Out of order execution: A general optimization for compilers, by looking ahead at the exact data dependencies within a graph we can decide on the most opportune time to execute a node and which buffers can be reused Automatic work placement: Similar of the out of order execution point, but by matching nodes of a graph to resources like physical hardware or memory we can design an appropriate schedule  The above are general principles for accelerating PyTorch code but different backends will each make different tradeoffs on what to optimize. For example Inductor first takes care of fusing whatever it can and only then generates Triton kernels. It can also Triton in addition offers speedups because of automatic memory coalescing, memory management and scheduling within each Streaming Multiprocessor and has been designed to handle tiled computations. However, regardless of the backend you use it\u2019s best to use a benchmark and see approach so try out the PyTorch profiler, visually inspect the generated kernels and try to see what\u2019s going on for yourself. Why am I not seeing speedups? Graph Breaks The main reason you won\u2019t see the speedups you\u2019d like to by using dynamo is excessive graph breaks. So what\u2019s a graph break? Given a program like: def some_fun(x):\n    ...\n\ntorch.compile(some_fun)(x)\n...\n Torchdynamo will attempt to compile all of the torch/tensor operations within some_fun() into a single FX graph, but it may fail to capture everything into one graph. Some graph break reasons are insurmountable to TorchDynamo like calling into a C extension other than PyTorch is invisible to TorchDynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse. To maximize performance, it\u2019s important to have as few graph breaks as possible. Identifying the cause of a graph break To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage: import torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation, out_guards, graphs, ops_per_graph = dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\nprint(explanation)\n\"\"\"\nDynamo produced 3 graphs, with 2 graph break and 6 ops.\n Break reasons:\n1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n   File \"t2.py\", line 16, in toy_example\n    print(\"woo\")\n\n2. generic_jump\n   File \"t2.py\", line 17, in toy_example\n    if b.sum() < 0:\n \"\"\"\n To throw an error on the first graph break encountered you can use disable python fallback by using nopython=True, this should be familiar if you\u2019ve worked with export based compilers. def toy_example(a, b):\n   ...\n\ntorch.compile(toy_example, fullgraph=True, backend=<compiler>)\n Why didn\u2019t my code recompile when I changed it? If you enabled dynamic shapes by setting env TORCHDYNAMO_DYNAMIC_SHAPES=1 python model.py then your code won\u2019t recompile on shape changes. We\u2019ve added support for dynamic shapes which avoids recompilations in the case when shapes vary by less than a factor of 2. This is especially useful in scenarios like varying image sizes in CV or variable sequence length in NLP. In inference scenarios it\u2019s often not possible to know what a batch size will be beforehand because you take what you can get from different client apps. In general, TorchDynamo tries very hard not to recompile things unnecessarily so if for example TorchDynamo finds 3 graphs and your change only modified one graph then only that graph will recompile. So another tip to avoid potentially slow compilation times is to warmup a model by compiling it once after which subsequent compilations will be much faster. Cold start compile times is still a metric we track visibly. Why am I getting incorrect results? Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler. If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True Why am I getting OOMs? Dynamo is still an alpha product so there\u2019s a few sources of OOMs and if you\u2019re seeing an OOM try disabling the following configurations in this order and then open an issue on GitHub so we can solve the root problem 1. If you\u2019re using dynamic shapes try disabling them, we\u2019ve disabled them by default: env TORCHDYNAMO_DYNAMIC_SHAPES=0 python model.py 2. CUDA graphs with Triton are enabled by default in inductor but removing them may alleviate some OOM issues: torch._inductor.config.triton.cudagraphs = False. Does torch.func work with torch.compile (for grad and vmap transforms)? Applying a torch.func transform to a function that uses torch.compile does not work: import torch\n\n@torch.compile\ndef f(x):\n    return torch.sin(x)\n\ndef g(x):\n    return torch.grad(f)(x)\n\nx = torch.randn(2, 3)\ng(x)\n This code will not work. There is an issue that you can track for this. As a workaround, use torch.compile outside of the torch.func function:  Note This is an experimental feature and can be used by setting torch._dynamo.config.capture_func_transforms=True  import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef f(x):\n    return torch.sin(x)\n\n@torch.compile\ndef g(x):\n    return torch.vmap(f)(x)\n\nx = torch.randn(2, 3)\ng(x)\n Calling torch.func transform inside of a function handled with torch.compile\n Compiling torch.func.grad with torch.compile\n import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef wrapper_fn(x):\n    return torch.func.grad(lambda x: x.sin().sum())(x)\n\nx = torch.randn(3, 3, 3)\ngrad_x = torch.compile(wrapper_fn)(x)\n Compiling torch.vmap with torch.compile\n import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef my_fn(x):\n    return torch.vmap(lambda x: x.sum(1))(x)\n\nx = torch.randn(3, 3, 3)\noutput = torch.compile(my_fn)(x)\n Limitations There are currently a few cases which are not supported and lead to graph breaks (that is, torch.compile falls back to eager-mode PyTorch on these). We are working on improving the situation for the next release (PyTorch 2.2) 1. The inputs and outputs of the function being transformed over must be tensors. We do not yet support things like tuple of Tensors. import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef fn(x):\n    x1, x2 = x\n    return x1 + x2\n\ndef my_fn(x):\n    return torch.func.vmap(fn)(x)\n\nx1 = torch.randn(3, 3, 3)\nx2 = torch.randn(3, 3, 3)\n# Unsupported, falls back to eager-mode PyTorch\noutput = torch.compile(my_fn)((x1, x2))\n  Keyword arguments are not supported.  import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef fn(x, y):\n    return (x + y).sum()\n\ndef my_fn(x, y):\n    return torch.func.grad(fn)(x, y=y)\n\nx = torch.randn(3, 3)\ny = torch.randn(3, 3)\n# Unsupported, falls back to eager-mode PyTorch\noutput = torch.compile(my_fn)(x, y)\n 3. Functions with observable side effects. For example, it is OK to mutate a list created in the function, but not OK to mutate a list created outside of the function. import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\nsome_list = []\n\ndef f(x, y):\n    some_list.append(1)\n    return x + y\n\ndef my_fn(x, y):\n    return torch.func.vmap(f)(x, y)\n\nx = torch.ones(2, 3)\ny = torch.randn(2, 3)\n# Unsupported, falls back to eager-mode PyTorch\noutput = torch.compile(my_fn)(x, y)\n  \ntorch.vmap over a function that calls one or more operators in the following list.   Note \u2018stride\u2019, \u2018requires_grad\u2019, \u2018storage_offset\u2019, \u2018layout\u2019, \u2018data\u2019, \u2018is_coalesced\u2019, \u2018is_complex\u2019, \u2018is_conj\u2019, \u2018is_contiguous\u2019, \u2018is_cpu\u2019, \u2018is_cuda\u2019, \u2018is_distributed\u2019, \u2018is_floating_point\u2019, \u2018is_inference\u2019, \u2018is_ipu\u2019, \u2018is_leaf\u2019, \u2018is_meta\u2019, \u2018is_mkldnn\u2019, \u2018is_mps\u2019, \u2018is_neg\u2019, \u2018is_nested\u2019, \u2018is_nonzero\u2019, \u2018is_ort\u2019, \u2018is_pinned\u2019, \u2018is_quantized\u2019, \u2018is_same_size\u2019, \u2018is_set_to\u2019, \u2018is_shared\u2019, \u2018is_signed\u2019, \u2018is_sparse\u2019, \u2018is_sparse_csr\u2019, \u2018is_vulkan\u2019, \u2018is_xla\u2019, \u2018is_xpu\u2019  import torch\n\ntorch._dynamo.config.capture_func_transforms=True\n\ndef bad_fn(x):\n    x.stride()\n    return x\n\ndef my_fn(x):\n    return torch.func.vmap(bad_fn)(x)\n\nx = torch.randn(3, 3, 3)\n# Unsupported, falls back to eager-mode PyTorch\noutput = torch.compile(my_fn)(x)\n Compiling functions besides the ones which are supported (escape hatch) For other transforms, as a workaround, use torch._dynamo.allow_in_graph allow_in_graph is an escape hatch. If your code does not work with torch.compile, which introspects Python bytecode, but you believe it will work via a symbolic tracing approach (like jax.jit), then use allow_in_graph. By using allow_in_graph to annotate a function, you must make sure your code meets the following requirements:  All outputs in your function only depend on the inputs and do not depend on any captured Tensors. Your function is functional. That is, it does not mutate any state. This may be relaxed; we actually support functions that appear to be functional from the outside: they may have in-place PyTorch operations, but may not mutate global state or inputs to the function. Your function does not raise data-dependent errors.  import torch\n\n@torch.compile\ndef f(x):\n    return torch._dynamo.allow_in_graph(torch.vmap(torch.sum))(x)\n\nx = torch.randn(2, 3)\nf(x)\n A common pitfall is using allow_in_graph to annotate a function that invokes an nn.Module. This is because the outputs now depend on the parameters of the nn.Module. To get this to work, use torch.func.functional_call to extract the module state. Does NumPy work with torch.compile? Starting in 2.1, torch.compile understands native NumPy programs that work on NumPy arrays, and mixed PyTorch-NumPy programs that convert from PyTorch to NumPy and back via x.numpy(), torch.from_numpy, and related functions. Which NumPy features does torch.compile support? NumPy within torch.compile follows NumPy 2.0 pre-release. Generally, torch.compile is able to trace through most NumPy constructions, and when it cannot, it falls back to eager and lets NumPy execute that piece of code. Even then, there are a few features where torch.compile semantics slightly deviate from those of NumPy:  NumPy scalars: We model them as 0-D arrays. That is, np.float32(3) returns a 0-D array under torch.compile. To avoid a graph break, it is best to use this 0-D array. If this breaks your code, you can workaround this by casting the NumPy scalar to the relevant Python scalar type bool/int/float. Negative strides: np.flip and slicing with a negative step return a copy. Type promotion: NumPy\u2019s type promotion will change in NumPy 2.0. The new rules are described in NEP 50. torch.compile implements NEP 50 rather than the current soon-to-be deprecated rules. \n{tril,triu}_indices_from/{tril,triu}_indices return arrays rather than a tuple of arrays.  There are other features for which we do not support tracing and we gracefully fallback to NumPy for their execution:  Non-numeric dtypes like datetimes, strings, chars, void, structured dtypes and recarrays. Long dtypes np.float128/np.complex256 and some unsigned dtypes np.uint16/np.uint32/np.uint64. \nndarray subclasses. Masked arrays. Esoteric ufunc machinery like axes=[(n,k),(k,m)->(n,m)] and ufunc methods (e.g., np.add.reduce). Sorting / ordering complex64/complex128 arrays. NumPy np.poly1d and np.polynomial. Positional out1, out2 args in functions with 2 or more returns (out=tuple does work). \n__array_function__, __array_interface__ and __array_wrap__. \nndarray.ctypes attribute.  Can I execute NumPy code on CUDA via torch.compile? Yes you can! To do so, you may simply execute your code within a torch.device(\"cuda\") context. Consider the example import torch\nimport numpy as np\n\n@torch.compile\ndef numpy_fn(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    return np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n\nX = np.random.randn(1024, 64)\nY = np.random.randn(1024, 64)\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\n In this example, numpy_fn will be executed in CUDA. For this to be possible, torch.compile automatically moves X and Y from CPU to CUDA, and then it moves the result Z from CUDA to CPU. If we are executing this function several times in the same program run, we may want to avoid all these rather expensive memory copies. To do so, we just need to tweak our numpy_fn so that it accepts cuda Tensors and returns tensors: @torch.compile\ndef numpy_fn(X: torch.Tensor, Y: torch.Tensor) -> torch.Tensor:\n    X, Y = X.numpy(), Y.numpy()\n    Z = np.sum(X[:, :, None] * Y[:, None, :], axis=(-2, -1))\n    return torch.from_numpy(Z)\n\nX = torch.randn(1024, 64, device=\"cuda\")\nY = torch.randn(1024, 64, device=\"cuda\")\nwith torch.device(\"cuda\"):\n    Z = numpy_fn(X, Y)\n By doing this, we explicitly create the tensors in CUDA memory, and we keep them there. In this case X.numpy() and from_numpy() are hints to the compiler but no real data movement happens. Note that the original program would not run on eager mode now. If you want to run it in eager mode, you would need to call .numpy(force=True) doing Z = Z.cuda() before returning Z. Of course, doing this would execute the program on eager mode NumPy, and on CPU. How do I debug NumPy code under torch.compile? Debugging JIT compiled code is challenging, given the complexity of modern compilers and the daunting errors that they raise. The tutorial on how to diagnose runtime errors within torch.compile contains a few tips and tricks on how to tackle this task. If the above is not enough to pinpoint the origin of the issue, there are still a few other NumPy-specific tools we can use. We can discern whether the bug is entirely in the PyTorch code by disabling tracing through NumPy functions: from torch._dynamo import config\nconfig.trace_numpy = False\n If the bug lies in the traced NumPy code, we can execute the NumPy code eagerly (without torch.compile) using PyTorch as a backend by importing import torch._numpy as np. This should just be used for debugging purposes and is in no way a replacement for the PyTorch API, as it is much less performant and, as a private API, may change without notice. At any rate, torch._numpy is a Python implementation of NumPy in terms of PyTorch and it is used internally by torch.compile to transform NumPy code into Pytorch code. It is rather easy to read and modify, so if you find any bug in it feel free to submit a PR fixing it or simply open an issue. If the program does work when importing torch._numpy as np, chances are that the bug is in TorchDynamo. If this is the case, please feel open an issue with a minimal reproducer. I torch.compile some NumPy code and I did not see any speed-up. The best place to start is the tutorial with general advice for how to debug these sort of torch.compile issues. Some graph breaks may happen because of the use of unsupported features. See Which NumPy features does torch.compile support?. More generally, it is useful to keep in mind that some widely used NumPy features do not play well with compilers. For example, in-place modifications make reasoning difficult within the compiler and often yield worse performance than their out-of-place counterparts.As such, it is best to avoid them. Same goes for the use of the out= parameter. Instead, prefer out-of-place ops and let torch.compile optimize the memory use. Same goes for data-dependent ops like masked indexing through boolean masks, or data-dependent control flow like if or while constructions. Which API to use for fine grain tracing? In some cases, you might need to exclude small parts of your code from the torch.compile compilations. This section provides some of the answers and you can find more information in TorchDynamo APIs for fine-grained tracing. How do I graph break on a function? Graph break on a function is not enough to sufficiently express what you want PyTorch to do. You need to be more specific about your use case. Some of the most common use cases you might want to consider:  If you want to disable compilation on this function frame and the recursively invoked frames, use torch._dynamo.disable. If you want a particular operator, such as fbgemm to use the eager mode, use torch._dynamo.disallow_in_graph.  Some of the uncommon use cases include:  If you want to disable TorchDynamo on the function frame but enable it back on the recursively invoked frames \u2013 use torch._dynamo.disable(recursive=False). If you want to prevent inlining of a function frame \u2013 use torch._dynamo.graph_break at the beginning of the function you want to prevent inlining.  What\u2019s the difference between torch._dynamo.disable and torch._dynamo.disallow_in_graph\n Disallow-in-graph works at the level of operators, or more specifically, the operators that you see in the TorchDynamo extracted graphs. Disable works at the function frame level and decides if TorchDynamo should look into the function frame or not. What\u2019s the difference between torch._dynamo.disable and torch._dynamo_skip\n  Note torch._dynamo_skip is deprecated.  You most likely need torch._dynamo.disable. But in an unlikely scenario, you might need even finer control. Suppose you want to disable the tracing on just the a_fn function, but want to continue the tracing back in aa_fn and ab_fn. The image below demonstrates this use case:    In this case, you can use torch._dynamo.disable(recursive=False). In previous versions, this functionality was provided by torch._dynamo.skip. This is now supported by the recursive flag inside torch._dynamo.disable.\n"}, {"name": "torch.compiler.Getting Started", "path": "torch.compiler_get_started", "type": "Miscellaneous", "text": "Getting Started Before you read this section, make sure to read the torch.compiler. Let\u2019s start by looking at a simple torch.compile example that demonstrates how to use torch.compile for inference. This example demonstrates the torch.cos() and torch.sin() features which are examples of pointwise operators as they operate element by element on a vector. This example might not show significant performance gains but should help you form an intuitive understanding of how you can use torch.compile in your own programs.  Note To run this script, you need to have at least one GPU on your machine. If you do not have a GPU, you can remove the cuda() code in the snippet below and it will run on CPU.  import torch\ndef fn(x, y):\n    a = torch.cos(x).cuda()\n    b = torch.sin(y).cuda()\n    return a + b\nnew_fn = torch.compile(fn, backend=\"inductor\")\ninput_tensor = torch.randn(10000).to(device=\"cuda:0\")\na = new_fn(input_tensor, input_tensor)\n A more famous pointwise operator you might want to use would be something like torch.relu(). Pointwise ops in eager mode are suboptimal because each one would need to read a tensor from the memory, make some changes, and then write back those changes. The single most important optimization that inductor performs is fusion. In the example above we can turn 2 reads and 2 writes into 1 read and 1 write which is crucial especially for newer GPUs where the bottleneck is memory bandwidth (how quickly you can send data to a GPU) rather than compute (how quickly your GPU can crunch floating point operations). Another major optimization that inductor provides is automatic support for CUDA graphs. CUDA graphs help eliminate the overhead from launching individual kernels from a Python program which is especially relevant for newer GPUs. TorchDynamo supports many different backends, but TorchInductor specifically works by generating Triton kernels. Let\u2019s save our example above into a file called example.py. We can inspect the code generated Triton kernels by running TORCH_COMPILE_DEBUG=1 python example.py. As the script executes, you should see DEBUG messages printed to the terminal. Closer to the end of the log, you should see a path to a folder that contains torchinductor_<your_username>. In that folder, you can find the output_code.py file that contains the generated kernel code similar to the following: @pointwise(size_hints=[16384], filename=__file__, meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]})\n@triton.jit\ndef kernel(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):\n    xnumel = 10000\n    xoffset = tl.program_id(0) * XBLOCK\n    xindex = xoffset + tl.arange(0, XBLOCK)[:]\n    xmask = xindex < xnumel\n    x0 = xindex\n    tmp0 = tl.load(in_ptr0 + (x0), xmask)\n    tmp1 = tl.cos(tmp0)\n    tmp2 = tl.sin(tmp0)\n    tmp3 = tmp1 + tmp2\n    tl.store(out_ptr0 + (x0), tmp3, xmask)\n  Note The above code snippet is an example. Depending on your hardware, you might see different code generated.  And you can verify that fusing the cos and sin did actually occur because the cos and sin operations occur within a single Triton kernel and the temporary variables are held in registers with very fast access. Read more on Triton\u2019s performance here. Because the code is written in Python, it\u2019s fairly easy to understand even if you have not written all that many CUDA kernels. Next, let\u2019s try a real model like resnet50 from the PyTorch hub. import torch\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(1,3,64,64))\n And that is not the only available backend, you can run in a REPL torch.compiler.list_backends() to see all the available backends. Try out the cudagraphs next as inspiration. Using a pretrained model PyTorch users frequently leverage pretrained models from transformers or TIMM and one of the design goals is TorchDynamo and TorchInductor is to work out of the box with any model that people would like to author. Let\u2019s download a pretrained model directly from the HuggingFace hub and optimize it: import torch\nfrom transformers import BertTokenizer, BertModel\n# Copy pasted from here https://huggingface.co/bert-base-uncased\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\").to(device=\"cuda:0\")\nmodel = torch.compile(model, backend=\"inductor\") # This is the only line of code that we changed\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt').to(device=\"cuda:0\")\noutput = model(**encoded_input)\n If you remove the to(device=\"cuda:0\") from the model and encoded_input, then Triton will generate C++ kernels that will be optimized for running on your CPU. You can inspect both Triton or C++ kernels for BERT. They are more complex than the trigonometry example we tried above but you can similarly skim through it and see if you understand how PyTorch works. Similarly, let\u2019s try out a TIMM example: import timm\nimport torch\nmodel = timm.create_model('resnext101_32x8d', pretrained=True, num_classes=2)\nopt_model = torch.compile(model, backend=\"inductor\")\nopt_model(torch.randn(64,3,7,7))\n Next Steps In this section, we have reviewed a few inference examples and developed a basic understanding of how torch.compile works. Here is what you check out next:  torch.compile tutorial on training torch.compiler API reference TorchDynamo APIs for fine-grained tracing \n"}, {"name": "torch.compiler.Guards Overview", "path": "torch.compiler_guards_overview", "type": "Miscellaneous", "text": "Guards Overview From a UX perspective, TorchDynamo is very easy to use. The user invokes torchdynamo.optimize as an annotation: @torchdynamo.optimize(my_compiler)\ndef fn_foo(bar):\n Where a complete example looks like this: from typing import List\nimport torch\nfrom torch import _dynamo as torchdynamo\n\ndef my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    print(\"my_compiler() called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward  # return a python callable\n\n@torchdynamo.optimize(my_compiler)\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\n\nfor _ in range(100):\n    toy_example(torch.randn(10), torch.randn(10))\n This allows TorchDynamo to capture the interpreted Python frames, grab any and all relevant information, and speed things up wherever it can. The speedup comes from a few places, and can be rather dependent on the backend (my_compiler in the example above) provided, but the one speedup that is important in this section is caching. Caching itself is not a direct speedup but a critical enablement that prevents recompilation. We dig a hole with dynamo, and caching allows us to get out. It enables us to hold perf neutrality while then enabling backends - the true source of our speedups. With even a pass-through no-op backend provided: def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n    return gm.forward\n We can see TorchDynamo speeding up Python execution even on regular Python, not just PyTorch. Caching and Guards Overview TorchDynamo operates through caching transformed (by TorchDynamo) user bytecode. When TorchDynamo receives a frame for evaluation, it checks if the objects referenced in the frame have changed in certain ways, and if not, TorchDynamo reads the previously transformed user bytecode to evaluate it. In this section, we will focus on how we can identify whether or not the objects referenced in the frame have changed. This is a critical piece of functionality in TorchDynamo, because it drives the entire invalidation lifecycle. This functionality is called guards. At a very high level, the flow can be summarized like this:  TorchDynamo receives a Python frame. It converts the frame (1) passing it through instruction translation. \nFor the objects captured in (2), TorchDynamo creates tracking objects that are:  tracked on an output graph, which is an internal specialization of a torch.fx.Tracer\n guards   TorchDynamo processes the guard objects created in (3), turning them into a generated Python function, check_fn, associated with a piece of code. The check_fn is evaluated whenever we encounter this code a subsequent time - if a check_fn passes and evaluates to True, TorchDynamo identifies the code in the cache and the code encountered here as same, and can be safely used. If it fails and evaluates to False, TorchDynamo identifies the code in the cache as not valid, and can be thrown out in favor of a new entry, through recompilation or a graph break.  Python Frame Evaluation and PEP 523 The functionality of TorchDynamo is based on PEP 523. TorchDynamo installs a frame evaluation function on Python by using _PyInterpreterState_SetEvalFrameFunc. TorchDynamo has a hook where Python can hand control back to us during evaluation. The function we have installed is convert_frame or convert_frame_assert in the nopython=True case, but glossing over that nuance for now, let\u2019s take a look at convert_frame_assert, as convert_frame proxies to it. We can find it on line 222 of convert_frame.py, with a signature as follows: def  convert_frame_assert(compiler_fn: Callable, one_graph=True):\n This function wraps the entry point of where Python invokes TorchDynamo with a frame: def  _convert_frame_assert(frame: types.FrameType, cache_size: int):\n Here is what this function does:  Checks if it has seen this code(see: f_code here) before and exits early if it did. Checks if the code is an unsupported case. Checks if the cache_size (second arg above) crosses the limit defined in the config, cache_size_limit. If it has, the function drops the frame and logs warnings. This helps to avoid constant recompilation of a frame as it generally means that the frame is hot in an unexpected way and caching it produces needless overhead, as it is likely to get evicted the next time it is encountered. \nPasses the frame, alongside a function that creates an InstructionTranslator through bytecode transformation, via transform_code_object. A few crucial things happen under the hood here:  New code is produced through transform_code_object. An FX tracer named output is produced through InstructionTranslator. This can be a bit confusing, as InstructionTranslator is not an fx tracer, but its stored in a variable named tracer, and its output is an fx tracer. The function produces guards and stores them on output above. The function produces output_instructions and stores them on output above. The function maps the newly produced transformed code to the initial code it read off the frame. This mapping is worth remembering, we will refer to it much later on below where we cover guard failures.   Using the transformed code from 4.1 and the guards from 4.3, the function produces a GuardedCode.  Now that we have learned about frame evaluation, let\u2019s review InstructionTranslator, and see how it turns the frame we handed it over into TorchDynamo internal types. InstructionTranslator InstructionTranslator does a lot! We won\u2019t cover the details of everything it does, but most importantly for this document, it produces a mapping of symbolic_locals which maintains a mapping from the frame\u2019s f_locals to TorchDynamo internal Variable objects (more on these in a moment. symbolic_locals is filled via traversing the frame\u2019s locals: self.symbolic_locals = collections.OrderedDict(\n    (k, VariableBuilder(self, LocalSource(k))(f_locals[k]))\n    for k in vars\n    if k in f_locals\n)\n The important component here is the invocation of a call into VariableBuilder. VariableBuilder\u2019s call implementation proxies into a function called _wrap, which in turn both constructs instances of VariableTracker and calls make_guards on them. More on that later. This mapping, in turn, is critical as each Variable has associated guards, which are then passed to self.output, the instance of OutputGraph, an fx tracer, mentioned in 4.2 of the section above. If you recall, this OutputGraph, stored in a variable called output is where our guards are stored before being passed on to become GuardedCode How does InstructionTranslator do this? At the heart of it, there is a loop that is pumped, which drives a function step. step is just that - a single processing step, taking exactly one instruction and doing something with it.  Note These are real instructions processed by TorchDynamo\u2019s transform_code_object, and it is pretty cool.   Note This section purposely skips the details of dis.get_instructions.  For the example above, here is a snippet of a what a few Instruction's may look like: Instruction(opcode=124, opname='LOAD_FAST', arg=0, argval='b', offset=32, starts_line=8, is_jump_target=True, target=None)\nInstruction(opcode=100, opname='LOAD_CONST', arg=3, argval=-1, offset=34, starts_line=None, is_jump_target=False, target=None)\nInstruction(opcode=20, opname='BINARY_MULTIPLY', arg=None, argval=None, offset=36, starts_line=None, is_jump_target=False, target=None)\n This is the core functionality of this function. Take a look at the opname, and then take a look at this little snippet from inside step; if not hasattr(self, inst.opname):\n    unimplemented(f\"missing: {inst.opname}\")\ngetattr(self, inst.opname)(inst)\n As we can see, the function checks if the current class, the InstructionTranslator has an attribute set matching the operator name (for example, LOAD_CONST). If it does, the function invokes it, passing the whole instruction object in. If it does not, the function drops the frame as unimplemented. For the LOAD_CONST example, we can see that we do indeed support it, with a relatively straightforward definition: def LOAD_CONST(self, inst):\n    self.push(ConstantVariable(value=inst.argval))\n We can see that this function creates a new instance of the class ConstantVariable , with a value, in our example case, -1, and then pushes it onto the stack. There are dozens of such methods - see symbolic_convert.py for all of them. Generally, we implement as many matching methods to Python bytecode instructions as possible. Across both the logic downstream of step and the logic from invoking VariableBuilder - we now have a lot of VariableTrackers and of course, we\u2019ve spoken about creating guards quiet a bit. Let\u2019s dig into what Variables are, and get a little closer to understanding guards. Variables A ConstantVariable is an instance of VariableTracker. VariableTracker represents a tracked Python local or stack value. When it comes to representing an object inside TorchDynamo, a VariableTracker does exactly what it says - it tracks a given variable. It is an extremely flexible class, but there are a few points to keep in mind:  \nIt manages the guard relationship around the underlying object through:  make_guard replace_guards add_guard(s) \npropagate - propagate(*vars: List[List[\"VariableTracker\"]]) - Perhaps the most important of all, in that it combines guards from all the provided VariableTracker instances passed in. It visits the guards and combines the guards from these onto itself.   \nIt acts as a proxy on behalf of the underlying object, implementing methods for the rest of TorchDynamo to get information about the tracked object:  call_method call_function python_type as_proxy is/as_python_proxy   It stores the variable source of type Source, from torchdynamo/source.py. This source type is a relatively self contained class that helps us organize and bookkeep where the original source came from, and helps provide convenience methods for things like getting the name, and importantly for us, producing guards.  And this class (VariableTracker) is built around subclassing, somewhere between a full Abstract Base Class and fully fleshed out class - it leaves many methods raising NotImplementedError - with reliance on subclasses. See torchdynamo/variables/ for all subclasses to fulfill contracts and custom behaviors. Knowing what we know now, we can see an example of how an instruction from dis, BUILD_TUPLE: BUILD_TUPLE(count) Creates a tuple consuming count items from the stack, and pushes the resulting tuple onto the stack. In our case, our signature will be a little different due to the way we create Instruction objects, but the gist of it will be the same. Instead of passing in count, we pass in an object with a little extra bookkeeping, and of course, we deal with turning regular old python objects into TorchDynamo notions: def BUILD_TUPLE(self, inst):\n    items = self.popn(inst.argval)\n    options = VariableTracker.propagate(items)\n    self.push(TupleVariable(items, **options))\n Here is what this code does:  The function reads argval, which in this case, is analogous to counts in the pydoc for the equivalent instruction. The function popn the items, in this case, the signature is def\u00a0 popn(self, n: int) -> List[TensorVariable]: this hints at an underlying contract - we are returning TensorVariables. If we take a closer look at symbolic_convert.py and InstructionTranslatorBase/InstructionTranslatorwe see that the only thing pushed onto and popped from our stack are VariableTrackers.   The function calls VariableTracker.propagate. This takes the guards from every single item popped off the stack in 2, and recursively traverses it and combines all the guards into options: py\u00a0 return {\u00a0\u00a0\u00a0\u00a0\u00a0 \"guards\": guards,\u00a0 }\n The function then makes a new instance of a VariableTracker, TupleVariableout of the items and options. This then allows us to install all the appropriate guards from the items that make up the new TupleVariable\n   Note Where did the first guards come from? Propagation is a good technique, but we need something created before it can be propagated. VariableBuilder calls make_guards as it creates VariableTracker instances, from f_locals. This in turn calls into the source, to have it create guards.  After all this, bytecode translation is done and we are one step closer to producing GuardedCode. We now understand how locals become VariableTrackers, how instructions are handled, and where guards are called on for creation. Before we can go into seeing how code and guards are combined into a GuardedCode object, we need to dig a little bit into those make_guard and source.make_guard calls above. We can then understand, what was going on when we made guards alongside, and on, VariableTracker instances. Making Guards Guards are just Python objects, of the class Guard. Let\u2019s look at them in more detail. Looking at the definition of the dataclass (and therefore, ctor signature), we see that it has a name, a source, and a create function. @dataclasses.dataclass\nclass Guard:\n    name: str\n    source: GuardSource\n    create_fn: Callable\n The name should be the name of the variable. The source here is an enum indicating what kind of source the guard belongs to.  Note Not to be confused with Source and the other types in source.py, as stored on VariableTracker.  create_fn provides the main functionality to transition from a simple dataclass to actually producing valid Python code to be invoked for knowing whether or not things have changed in between invocations, and whether we can safely read from the code cache or not. The most common code paths for getting an instance of a guard are through make_guards on VariableTracker. make_guards -> source.make_guard -> return Guard(self.name(), self.guard_source(), fn) Or, in a concrete example: ...\nelif istype(value, range):\n    guards = self.make_guards(GuardBuilder.EQUALS_MATCH)\n    return RangeVariable(value=value, guards=guards)\n Since source was set at the construction time of this VariableTracker, all that was needed here was to provide the fn, GuardBuilder.EQUALS_MATCH to the create_fn field. This create_fn must be a method on GuardBuilder. The reason for this becomes apparent in our next step. Once we have all the guards created for a frame, we move on to CheckFunctionManager and compile_check_fn. Before the convert_frame function can produce a GuardedCode, it needs to run the CheckFunctionManager, with all the guards, to produce a check_fn which will then, in turn get passed in alongside the code into GuardedCode. This is the same check_fn that we store in our cache entry, and the same one we run to know whether or not to retrieve the code stored alongside. For reference, here is that code: static CacheEntry *create_cache_entry(CacheEntry *next,\n                                      PyObject *guarded_code) {\n  CacheEntry *e = (CacheEntry *)malloc(sizeof(CacheEntry));\n  DEBUG_NULL_CHECK(e);\n  e->check_fn = PyObject_GetAttrString(guarded_code, \"check_fn\");\n  NULL_CHECK(e->check_fn);\n  e->code = (PyCodeObject *)PyObject_GetAttrString(guarded_code, \"code\");\n  NULL_CHECK(e->code);\n  e->next = next;\n  return e;\n}\n We now know how a check_fn function is used, and who makes it, and what it is composed of, but what we do not yet know is how. How does a list of Guard objects become a function we can run later on? First, we iterate these guards: for guard in sorted(guards or [], key=Guard.sort_key):\n    if not config.guard_nn_modules and guard.is_nn_module():\n        continue\n    guard.create(local_builder, global_builder)\n Calling guard.create runs that create_fn we set on the Guard class above (don\u2019t confuse it with the check_fn we are working on producing, the names are similar, so it can get a little confusing). In our example above, our create_fn is GuardBuilder.EQUALS_MATCH. So we are now invoking it, passing in the self, the guard itself, in. The signature is: def EQUALS_MATCH(self, guard: Guard): And internally to that function, we can use the name on the guard to get back our original object, querying it for data and type information, which in turn gets us to the most important bit: appending code. At its simplest, EQUALS_MATCH appends just one line of code: self.code.append(f\"{ref} == {val!r}\"). Where ref is the name of the variable, and val is the value. It might produce code like this: y == 2\n This is a basic example. But if we append a few other kinds of GuardBuilder functions and then combine them all with and in between each statement (as we do), we might get something like this: ___guarded_code.valid and ___check_type_id(y, 94367738391392) and y == 2 and ___check_tensors(x)\n Here is what this code performs:  A check for .valid\n A type ID check A value check A tensor check  This becomes the heart of the code our check_fn, which in turn is evaluated the next time we encounter this code. It will then check:  Is this code still valid? If (1), Does y still have a type of 94367738391392? If (2), is y still 2? If (3), let\u2019s check on if tensor x changed in some specific ways.  If all of these are still true, then we can use the code cached alongside this check_fn.  Note For a deeper dive for how and where this happens you can read static PyCodeObject *lookup(CacheEntry *e, PyObject *f_locals) { of _eval_frame.c.  If not, then, we can move on to recompiling the code anew, and storing that in the cache alongside this code, and a whole new check_fn, again to be checked on yet another subsequent frame. There are lots of other such functions on GuardBuilder which get coalesced into, at times massive, strings which then get evaluated as Python code and stored into check_fn. The example above illustrates of a simple case. To understand this functionality better, read the other functions on GuardBuilder, or better yet, dump the code variable in compile_check_fn to see what is getting produced, especially on larger, real models. Summary In this section, we have reviewed:  The role of .valid and invalidation around weak references (and potentially soon to be NN Moduleinvalidations). How the C++ side of guard functions (___check_type_id, ___check_tensors, etc) operate. What happens when guards fail. What happens if we produce invalid guard code.  We covered how user provided code wrapped in a TorchDynamo context goes on to get traced and tracked internally, organized into VariableTrackers Sources and subsequently Guards, and how those Guards in turn guide cache entry selection and invalidation when handing Python code.\n"}, {"name": "torch.compiler.list_backends()", "path": "generated/torch.compiler.list_backends#torch.compiler.list_backends", "type": "Miscellaneous", "text": " \ntorch.compiler.list_backends(exclude_tags=('debug', 'experimental')) [source]\n \nReturn valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).  Parameters \nexclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.  Return type \nList[str]   \n"}, {"name": "torch.compiler.Profiling to understand torch.compile performance", "path": "torch.compiler_profiling_torch_compile", "type": "Miscellaneous", "text": "Profiling to understand torch.compile performance What to use torch.profiler for: torch.profiler is helpful for understanding the performance of your program at a kernel-level granularity - for example, it can show graph breaks and GPU utilization at the level of the program. The data provided by the profiler can often help users understand where to investigate further to understand model performance. To understand kernel-level performance, other toosl exist. NVIDIA\u2019s ncu tool can be used, or inductor\u2019s profiling tools. See also the general pytorch profiler guide. Basics of using torch.profiler and viewing traces Example program: We\u2019ll use this example of profiling resnet18. Notice the following parts of this example program:  Include a warm-up run to wait for compilation to complete (this will warm up systems like the CUDA caching allocator) Use torch.profiler.profile() context for profiling the section we are interested in Use prof.export_chrome_trace(\"trace.json\") to export the profiling artifact.  import torch\nfrom torchvision.models import resnet18\n\nmodel = resnet18().cuda()\ninputs = [torch.randn((5, 3, 224, 224), device='cuda') for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\n# warm up\nfwd_bwd(inputs[0])\n\nwith torch.profiler.profile() as prof:\n    for i in range(1, 4):\n        fwd_bwd(inputs[i])\n        prof.step()\n\nprof.export_chrome_trace(\"trace.json\")\n Viewing chrome traces: In the Chrome browser, open chrome://tracing and load the json file. Use the \u201cw\u201d and \u201cs\u201d keys to zoom in and out, and use \u201ca\u201d and \u201cd\u201d to scroll left and right. \u201c?\u201d will show a \u201chelp\u201d screen with a list of shortcuts.    Here, we observe: * CompiledFunction and CompiledFunctionBackward events, which correspond to the dynamo-compiled regions. * CPU events at the top, and GPU events at the bottom. Flows between CPU and GPU events Every kernel on the GPU occurs after being launched by code running on the CPU. The profiler can draw connections (i.e. \u201cflows\u201d) between the GPU and CPU events to show which CPU event launched a GPU kernel. This is particularly helpful because, with a few exceptions, GPU kernels are launched asynchronously. To view a flow connection, click on a GPU kernel and click \u201cac2g\u201d:    Alternatively, turn on all flows with the \u201cFlow events\u201d dropdown at the top. Working around CUDA Graph profiling issues When CUDA graphs are enabled, some cuda configurations (driver version under 525.85.12 or CUDA < 12) can encounter issues between the profiling tools and CUDA graphs. To fix these issues, add an empty profiling context at the top of your program: import torch\n\ntorch.profiler._utils._init_for_cuda_graphs()\n\n# ... rest of program\n Understanding compilation time To understand why compilation is taking a long time, you can profile the first invocation of a torch.compile-ed program. Keep in mind that profile traces of compilations can be distorted more than typical profiling, because compilation workloads can be quite different from typical PyTorch workloads. In some cases, trace files may also be quite large. Traces > 1GB can be difficult to open with the chrome tracing tool. Note: roughly the same information can also be obtained in non-graphical format with torch._dynamo.utils.compile_times(). This utility won\u2019t show when the compilation steps occur, but it will show the amount of time spent on each step - and times will not be affected by any profiling overhead. See an example below: import torch\nfrom torchvision.models import resnet18\n\nmodel = resnet18().cuda()\ninputs = [torch.randn((5, 3, 224, 224), device='cuda') for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\ndef warmup_compile():\n    def fn(x):\n        return x.sin().relu()\n\n    x = torch.rand((2, 2), device='cuda', requires_grad=True)\n    fn_c = torch.compile(fn)\n    out = fn_c(x)\n    out.sum().backward()\n\nwith torch.profiler.profile() as prof:\n    with torch.profiler.record_function(\"warmup compile\"):\n        warmup_compile()\n\n    with torch.profiler.record_function(\"resnet18 compile\"):\n        fwd_bwd(inputs[0])\n\nprof.export_chrome_trace(\"trace_compile.json\")\n    Note a few things:  The first invocation should occur during profiling in order to capture compilation Add a warm-up compilation in order to initialize any systems that need to be lazily initialized.  Finding graph breaks Although there are logging tools for identifying graph breaks, the profiler provides a quick visual method of identifying graph breaks. When gradients are required for any inputs, graph breaks are easy to identify: each graph break will interrupt a CompiledFunction block, splitting it in two. See the synthetic example below for a demonstration: import torch\nimport torch._dynamo\n\nclass ModelWithBreaks(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        def create_sequential():\n            return torch.nn.Sequential(\n                torch.nn.Linear(128, 128),\n                torch.nn.ReLU(),\n                torch.nn.Linear(128, 128),\n                torch.nn.ReLU(),\n            )\n        self.mod1 = create_sequential()\n        self.mod2 = create_sequential()\n        self.mod3 = create_sequential()\n        self.mod4 = create_sequential()\n\n    def forward(self, inp):\n        mod1 = self.mod1(inp)\n        torch._dynamo.graph_break()\n        mod2 = self.mod2(mod1)\n        torch._dynamo.graph_break()\n        mod3 = self.mod3(mod2)\n        torch._dynamo.graph_break()\n        mod4 = self.mod4(mod3)\n        return mod4\n\n\nmodel = ModelWithBreaks().cuda()\ninputs = [torch.randn((128, 128), device='cuda') for _ in range(10)]\n\nmodel_c = torch.compile(model)\n\ndef fwd_bwd(inp):\n    out = model_c(inp)\n    out.sum().backward()\n\n# warm up\nfwd_bwd(inputs[0])\n\nwith torch.profiler.profile() as prof:\n    for i in range(1, 4):\n        fwd_bwd(inputs[i])\n        prof.step()\n\nprof.export_chrome_trace(\"trace_break.json\")\n    Launch overhead One common issue is bad GPU utilization. A quick way to identify this is if there are large gaps between kernels on the GPU:    This is often the result of CPU overhead, e.g. if the amount of time spent on the CPU between kernel launches is larger than the amount of time spent by the GPU to process the kernels. The issue is more common for small batch sizes. When using inductor, enabling CUDA graphs can often help improve performance when launch overhead is a concern.\n"}, {"name": "torch.compiler.PyTorch 2.0 NNModule Support", "path": "torch.compiler_nn_module", "type": "Miscellaneous", "text": "PyTorch 2.0 NNModule Support Author: Will Constable torch.compile has special handling for torch.nn.Module objects, tracing them differently than it traces arbitrary python classes, with the intent of producing faster code by making assumptions about the structure. This doc describes some of the tradeoffs or edge cases that come up due to this specialization. NNModule Hooks Support Previously, torch.compile had no support for hooks on nn.Modules, and if hooks were registered they would simply be ignored in the compiled program. Indeed many users do not use nn.Module hooks at all, or only use them for debug workflows, but there are valid use cases for composing nn.Module hooks with torch.compile. Hooks that are orchestrated via nn.Module.__call__ implementation include _forward_pre_hooks, forward_hooks, _backward_pre_hooks, and _backward_hooks, and will be referred to as \u2018call hooks\u2019. These hooks are partially supported by torch.compile with limitations described below. Another category of hooks includes _state_dict_hooks and its pre and load_ variants, and are still unsupported by torch.compile. \nnn.Module.__call__ Hooks Usage and limitations By default, torch.compile will trace the contents of nn.Module.__call__ which means it will encounter and run forward/pre-forward hooks. If you install hooks before calling torch.compile and then do not remove or alter the hooks later, your use case should be supported by default. Backward/Pre-backward hooks are generally also supported, with similar caveats: currently graph-breaks in dynamo occur when accessing backward_hooks dicts, which is probably avoiable with some work. Graph-breaks also impact the timing of firing backward hooks, since graph-segments are run as autograd-functions which produce all their grads at the same time. Assuming it were possible for dynamo to not graph-break on the presence of backward-hooks, we would still expect the backward hooks for a series of modules to all fire together after the whole compiled graph\u2019s backward ran. hooks on \u2018allowed modules\u2019 torch.compile treats common modules such as torch.conv, as well as modules that are difficult to trace, specially by allowing them to be called opaquely in the dynamo graph instead of traced into by dynamo. For such modules, hooks currently trigger a graph-break so that the affected modules run outside of dynamo. Depending on the model, this could introduce a significant performance regression, and additional work is required to improve this support. skip_nnmodule_hook_guards By default, torch._dynamo.config.skip_nnmodule_hook_guards is set to True, meaning no guards will be installed on each nn.Module hook dictionary, improving runtime by reducing guard execution time, at the cost of not noticing if any hook dict is changed after compilation. If you want to be able to remove or modify hooks after compilation and have torch.compile react appropriately (by recompiling), then you need to set skip_nnmodule_hook_guards=False and expect a runtime penalty for the added guards. TODO: confirm if backward/pre_backward hooks are working or not and document accordingly state_dict Hooks State dict hooks have not yet been supported in torch.compile. TODO: warn_once if graph-breaking on hooks. warn_once to point to this doc if hooks are present.\n"}, {"name": "torch.compiler.PyTorch 2.0 Performance Dashboard", "path": "torch.compiler_performance_dashboard", "type": "Miscellaneous", "text": "PyTorch 2.0 Performance Dashboard Author: Bin Bao and Huy Do PyTorch 2.0\u2019s performance is tracked nightly on this dashboard. The performance collection runs on 12 GCP A100 nodes every night. Each node contains a 40GB A100 Nvidia GPU and a 6-core 2.2GHz Intel Xeon CPU. The corresponding CI workflow file can be found here. How to read the dashboard? The landing page shows tables for all three benchmark suites we measure, TorchBench, Huggingface, and TIMM, and graphs for one benchmark suite with the default setting. For example, the default graphs currently show the AMP training performance trend in the past 7 days for TorchBench. Droplists on the top of that page can be selected to view tables and graphs with different options. In addition to the pass rate, there are 3 key performance metrics reported there: Geometric mean speedup, Mean compilation time, and Peak memory footprint compression ratio. Both Geometric mean speedup and Peak memory footprint compression ratio are compared against the PyTorch eager performance, and the larger the better. Each individual performance number on those tables can be clicked, which will bring you to a view with detailed numbers for all the tests in that specific benchmark suite. What is measured on the dashboard? All the dashboard tests are defined in this function. The exact test configurations are subject to change, but at the moment, we measure both inference and training performance with AMP precision on the three benchmark suites. We also measure different settings of TorchInductor, including default, with_cudagraphs (default + cudagraphs), and dynamic (default + dynamic_shapes). Can I check if my PR affects TorchInductor\u2019s performance on the dashboard before merging? Individual dashboard runs can be triggered manually by clicking the Run workflow button here and submitting with your PR\u2019s branch selected. This will kick off a whole dashboard run with your PR\u2019s changes. Once it is done, you can check the results by selecting the corresponding branch name and commit ID on the performance dashboard UI. Be aware that this is an expensive CI run. With the limited resources, please use this functionality wisely. How can I run any performance test locally? The exact command lines used during a complete dashboard run can be found in any recent CI run logs. The workflow page is a good place to look for logs from some of the recent runs. In those logs, you can search for lines like python benchmarks/dynamo/huggingface.py --performance --cold-start-latency --inference --amp --backend inductor --disable-cudagraphs --device cuda and run them locally if you have a GPU working with PyTorch 2.0. python benchmarks/dynamo/huggingface.py -h will give you a detailed explanation on options of the benchmarking script.\n"}, {"name": "torch.compiler.PyTorch 2.0 Troubleshooting", "path": "torch.compiler_troubleshooting", "type": "Miscellaneous", "text": "PyTorch 2.0 Troubleshooting Author: Michael Lazos We are actively developing debug tools, profilers, and improving our error and warning messages. Below is a table of the available tools and their typical usage. For additional help see Diagnosing Runtime Errors.  Title  \nTool Purpose Usage   \nInfo logging View summarized steps of compilation torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\"  \nDebug logging View detailed steps of compilation (print every instruction traced) torch._logging.set_logs(dynamo = logging.DEBUG) and torch._dynamo.config.verbose = True, or TORCH_LOGS=\"+dynamo\" TORCHDYNAMO_VERBOSE=1  \nMinifier for any backend Find smallest subgraph which reproduces errors for any backend set environment variable TORCHDYNAMO_REPRO_AFTER=\"dynamo\"  \nMinifier for TorchInductor If the error is known to occur after AOTAutograd find smallest subgraph which reproduces errors during TorchInductor lowering set environment variable TORCHDYNAMO_REPRO_AFTER=\"aot\"  \nDynamo accuracy minifier Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in AOTAutograd TORCHDYNAMO_REPRO_AFTER=\"dynamo\" TORCHDYNAMO_REPRO_LEVEL=4  \nInductor accuracy minifier Finds the smallest subgraph which reproduces an accuracy issue between an eager mode model and optimized model, when you suspect the problem is in the backend (e.g., inductor). If this doesn\u2019t work, try the Dynamo accuracy minifier instead. TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4  \ntorch._dynamo.explain Find graph breaks and display reasoning for them torch._dynamo.explain(fn)(*inputs)  \nRecord/Replay Record and replay frames which to reproduce errors during graph capture torch._dynamo.config.replay_record_enabled = True  \nTorchDynamo function name filtering Only compile functions with the given name to reduce noise when debugging an issue set environment variable TORCHDYNAMO_DEBUG_FUNCTION=<name>  \nTorchInductor Debug logging Print general TorchInductor debug info and generated Triton/C++ code torch._inductor.config.debug = True  \nTorchInductor Tracing Show time taken in each TorchInductor stage + output code and graph visualization set the environment variable TORCH_COMPILE_DEBUG=1 or torch._inductor.config.trace.enabled = True   In addition to info and debug logging, you can use torch._logging for more fine-grained logging. Diagnosing Runtime Errors At a high level, the TorchDynamo stack consists of a graph capture from Python code (TorchDynamo) and a backend compiler. For example, a backend compiler may consist of backward graph tracing (AOTAutograd) and graph lowering (TorchInductor)*. Errors can occur in any component of the stack and will provide full stack traces. To determine in which component an error occurred, you may use info-level logging torch._logging.set_logs(dynamo = logging.INFO) or TORCH_LOGS=\"dynamo\" and look for Step #: ... outputs. Logs are made at the beginning and end of each step, so the step that an error should correspond to is the most recently logged step whose end has not yet been logged. The steps correspond to the following parts of the stack:   \nStep Component   \n1 TorchDynamo  \n2 Compiler Backend  \n3 TorchInductor   If info logging is insufficient, you can use available backend options. These options include:  \n\"eager\": only runs TorchDynamo forward graph capture and then runs the captured graph with PyTorch. This provides an indication as to whether TorchDynamo is raising the error. \n\"aot_eager\": runs TorchDynamo to capture a forward graph, and then AOTAutograd to trace the backward graph without any additional backend compiler steps. PyTorch eager will then be used to run the forward and backward graphs. This is useful to narrow down the issue to AOTAutograd.  The general procedure to narrow down an issue is the following:  Run your program with the \"eager\" backend. If the error no longer occurs, the issue is in the backend compiler that is being used (if using TorchInductor, proceed to step 2. If not, see this section). If the error still occurs with the \"eager\" backend, it is an error while running torchdynamo. This step is only necessary if TorchInductor is used as the backend compiler. Run the model with the \"aot_eager\" backend. If this backend raises an error then the error is occurring during AOTAutograd tracing. If the error no longer occurs with this backend, then the error is in TorchInductor*.  Each of these cases are analyzed in the following sections.  Note The TorchInductor backend consists of both AOTAutograd tracing and the TorchInductor compiler itself. We will disambiguate by referring to TorchInductor as the backend, and TorchInductor lowering as the phase which lowers the graph traced by AOTAutograd.  Torchdynamo Errors If the error that is generated occurs with the \"eager\" backend, then TorchDynamo is most likely the source of the error. Here is a sample code which will generate an error. import torch\n\nimport torch._dynamo as dynamo\n\n\ndef test_assertion_error():\n    y = torch.ones(200, 200)\n    z = {y: 5}\n    return z\n\ncompiled_test_assertion_error = torch.compile(test_assertion_error, backend=\"eager\")\n\ncompiled_test_assertion_error()\n The code above generates the following error: torch._dynamo.convert_frame: [ERROR] WON'T CONVERT test_assertion_error /scratch/mlazos/torchdynamo/../test/errors.py line 26\ndue to:\nTraceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchdynamo/symbolic_convert.py\", line 837, in BUILD_MAP\n    assert isinstance(k, ConstantVariable) or (\nAssertionError\n\nfrom user code:\n   File \"/scratch/mlazos/torchdynamo/../test/errors.py\", line 34, in test_assertion_error\n    z = {y: 5}\n\nSet torch._dynamo.config.verbose=True for more information\n==========\n As the message suggests you can set torch._dynamo.config.verbose=True to get a full stack trace to both the error in TorchDynamo and the user code. In addition to this flag, you can also set the log_level of TorchDynamo through torch._dynamo.config.log_level. These levels include:  \nlogging.DEBUG: Print every instruction that is encountered in addition to all the log levels listed below. \nlogging.INFO: Print each function that is compiled (original and modified bytecode) and the graph that is captured in addition to all the log levels listed below. \nlogging.WARNING (default): Print graph breaks in addition to all the log levels listed below. \nlogging.ERROR: Print errors only.  If a model is very large, the logs can become overwhelming. If an error occurs deep within a model\u2019s Python code, it can be useful to execute only the frame in which the error occurs to enable easier debugging. There are two tools available to enable this:  Setting the environment variable TORCHDYNAMO_DEBUG_FUNCTION to the desired function name will only run torchdynamo on functions with that name. Enabling the record/replay tool (set torch._dynamo.config.replay_record_enabled = True) which dumps an execution record when an error is encountered. This record can then be replayed to run only the frame where an error occurred.  Diagnosing TorchInductor Errors If the error does not occur with the \"eager\" backend, then the backend compiler is the source of the error (example error). There are different choices for backend compilers for TorchDynamo, with TorchInductor fitting the needs of most users. This section focuses on TorchInductor as the motivating example, but some tools can also be used with other backend compilers. Below is the portion of the stack which we are focusing on: With TorchInductor as the chosen backend, AOTAutograd is used to generate the backward graph from the forward graph captured by torchdynamo. It is important to note that errors can occur during this tracing and also while TorchInductor lowers the forward and backward graphs to GPU code or C++. A model can often consist of hundreds or thousands of FX nodes, so narrowing the exact nodes where this problem occurred can be very difficult. Fortunately, there are tools available to automatically minify these input graphs to the nodes which are causing the issue. The first step is to determine whether the error occurs during tracing of the backward graph with AOTAutograd or during TorchInductor lowering. As mentioned above in step 2, the \"aot_eager\" backend can be used to run only AOTAutograd in isolation without lowering. If the error still occurs with this backend, this indicates that the error is occurring during AOTAutograd tracing. Here is an example: import torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n\ndef test_backend_error():\n\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.ops.aten._foobar(z)  # dummy function which errors\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=\"inductor\")\ncompiled_test_backend_error()\n Running this should give you this error with a longer stack trace below it: Traceback (most recent call last):\n  File \"/scratch/mlazos/torchdynamo/torchinductor/graph.py\", line 246, in call_function\n    return lowerings[target](*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 185, in wrapped\n    return decomp_fn(*args, **kwargs)\n  File \"/scratch/mlazos/torchdynamo/torchinductor/lowering.py\", line 810, in _foobar\n    assert False\nAssertionError\n...\n error with full stack trace If you then change torch.compile(backend=\"inductor\") to torch.compile(backend=\"aot_eager\"), it will run without error, because the issue is in the TorchInductor lowering process, not in AOTAutograd. Minifying TorchInductor Errors From here, let\u2019s run the minifier to get a minimal repro. Setting the environment variable TORCHDYNAMO_REPRO_AFTER=\u201caot\u201d (or setting torch._dynamo.config.repro_after=\"aot\" directly) will generate a Python program which reduces the graph produced by AOTAutograd to the smallest subgraph which reproduces the error. (See below for an example where we minify the graph produced by TorchDynamo) Running the program with this environment variable should show nearly identical output, with an additional line indicating where minifier_launcher.py has been written to. The output directory is configurable by setting torch._dynamo.config.base_dir to a valid directory name. The final step is to run the minifier and check that it runs successfully. A successful run looks like this. If the minifier runs successfully, it generates runnable python code which reproduces the exact error. For our example this is the following code: import torch\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch.fx.experimental.proxy_tensor import make_fx\n\n# torch version: 1.13.0a0+gitfddfc44\n# torch cuda version: 11.6\n# torch git version: fddfc4488afb207971c54ad4bf58130fdc8a4dc5\n\n\n# CUDA Info:\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2022 NVIDIA Corporation\n# Built on Thu_Feb_10_18:23:41_PST_2022\n# Cuda compilation tools, release 11.6, V11.6.112\n# Build cuda_11.6.r11.6/compiler.30978841_0\n\n# GPU Hardware Info:\n# NVIDIA A100-SXM4-40GB : 8\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        _foobar = torch.ops.aten._foobar.default(add);  add = None\n        return (_foobar,)\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu')]\nargs = [rand_strided(shape, stride, dtype, device) for shape, stride, dtype, device in args]\nmod = make_fx(Repro())(*args)\nfrom torch._inductor.compile_fx import compile_fx_inner\n\ncompiled = compile_fx_inner(mod, args)\ncompiled(*args)\n The forward method of the Repro module contains the exact op which causes the issue. When filing an issue, please include any minified repros to aid in debugging. Minifying Backend Compiler Errors With backend compilers other than TorchInductor the process for finding the subgraph causing the error is nearly identical to the procedure in errors in TorchInductor with one important caveat. Namely, that the minifier will now be run on the graph that is traced by TorchDynamo, not the output graph of AOTAutograd. Let\u2019s walk through an example. import torch\n\nimport torch._dynamo as dynamo\n\nmodel = torch.nn.Sequential(*[torch.nn.Linear(200, 200) for _ in range(5)])\n# toy compiler which fails if graph contains relu\ndef toy_compiler(gm: torch.fx.GraphModule, _):\n    for node in gm.graph.nodes:\n        if node.target == torch.relu:\n            assert False\n\n    return gm\n\n\ndef test_backend_error():\n    y = torch.ones(200, 200)\n    x = torch.ones(200, 200)\n    z = x + y\n    a = torch.relu(z)\n    return model(a)\n\n\ncompiled_test_backend_error = torch.compile(test_backend_error, backend=toy_compiler)\ncompiled_test_backend_error()\n In order to run the code after TorchDynamo has traced the forward graph, you can use the TORCHDYNAMO_REPRO_AFTER environment variable. Running this program with TORCHDYNAMO_REPRO_AFTER=\u201cdynamo\u201d (or torch._dynamo.config.repro_after=\"dynamo\") should produce this outputand the following code in {torch._dynamo.config.base_dir}/repro.py.  Note The other option for TORCHDYNAMO_REPRO_AFTER is \"aot\", which will run the minifier after the backward graph has been generated.  import torch\nimport torch._dynamo as dynamo\nfrom torch import tensor, device\nimport torch.fx as fx\nfrom torch._dynamo.testing import rand_strided\nfrom math import inf\nfrom torch._dynamo.debug_utils import run_fwd_maybe_bwd\n\nfrom torch.nn import *\n\nclass Repro(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, add):\n        relu = torch.relu(add);  add = None\n        return (relu,)\n\n\nmod = Repro().cuda()\nopt_mod = torch.compile(mod, backend=\"None\")\n\n\nargs = [((200, 200), (200, 1), torch.float32, 'cpu', False)]\nargs = [rand_strided(sh, st, dt, dev).requires_grad_(rg) for (sh, st, dt, dev, rg) in args]\n\n\nwith torch.cuda.amp.autocast(enabled=False):\n    ref = run_fwd_maybe_bwd(mod, args)\n    res = run_fwd_maybe_bwd(opt_mod, args)\n The minifier successfully reduced the graph to the op that raises the error in toy_compiler. The other difference from the procedure in TorchInductor Errors is that the minifier is automatically run after encountering a backend compiler error. After a successful run, the minifier writes repro.py to torch._dynamo.config.base_dir. Performance Profiling Accessing TorchDynamo Profiler TorchDynamo has a built-in stats function for collecting and displaying the time spent in each compilation phase. These stats can be accessed by calling torch._dynamo.utils.compile_times() after executing Torch._Dynamo. By default, this returns a string representation of the compile times spent in each TorchDynamo function by name. TorchInductor Debugging using TORCH_COMPILE_DEBUG TorchInductor has a builtin stats and trace function for displaying time spent in each compilation phase, output code, output graph visualization and IR dump. This is a debugging tool designed to make it easier to understand and troubleshoot the internals of TorchInductor. Let\u2019s run an example with the following test program (repro.py): import torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n Setting the environment variable TORCH_COMPILE_DEBUG=1 will cause a debug trace directory to be created, by default this directory will be in the current directory and named torch_compile_debug (this can be overridden in the torchdynamo configuration field debug_dir_root and also the env var TORCH_COMPILE_DEBUG_DIR). Inside this directory, each run will have a separate folder named with the timestamp and process id of the run: $ env TORCH_COMPILE_DEBUG=1 python repro.py\n$ cd torch_compile_debug\n$ ls\nrun_2023_03_01_08_20_52_143510-pid_180167\n In the run folder there will be a torchdynamo directory which contains debug logs, and an torchinductor folder which contains a subfolder for each compiled kernel with inductor debug artifacts. $ cd\nrun_2023_03_01_08_20_52_143510-pid_180167\n$ ls\ntorchinductor  torchdynamo\n Moving further into the torchinductor directory, the \\*.log files are logs from the AOT Autograd phase of compilation, model__0_forward_1.0 contains the inductor debug artifacts. $ cd torchinductor\n$ ls\naot_model___0_debug.log  model__0_forward_1.0\n$ cd model__0_forward_1.0\n$ ls\ndebug.log  fx_graph_readable.py  fx_graph_runnable.py  fx_graph_transformed.py  ir_post_fusion.txt  ir_pre_fusion.txt  output_code.py\n Here is a summary of the contents:  \nfx_graph_readable.py and fx_graph_runnable.py are the readable and runnable versions of the fx_graph received by inductor. \nfx_graph_transformed.py is the fx graph after inductor has run all fx passes. \nir\\*.txt is the inductor ir pre and post fusion. \noutput_code.py is the compiled triton kernel for the subgraph.  Here are example debug directory contents for the test program: import torch\n\n@torch.compile()\ndef test_model(x):\n    model = torch.nn.Sequential(\n        torch.nn.Linear(10, 10),\n        torch.nn.LayerNorm(10),\n        torch.nn.ReLU(),\n    )\n    return model(x)\n\n\ny = test_model(torch.ones(10, 10))\n Each file in that debug trace can be enabled and disabled through torch._inductor.config.trace.*. The profile and the diagram are both disabled by default since they are expensive to generate. A single node in this new debug format looks like: buf1: SchedulerNode(ComputedBuffer)\nbuf1.writes =\n    {   MemoryDep(name='buf1', index=0, size=()),\n        MemoryDep(name='buf1', index=0, size=(s0,))}\nbuf1.unmet_dependencies = {MemoryDep(name='buf0', index=c0, size=(s0,))}\nbuf1.met_dependencies = {MemoryDep(name='primals_2', index=c0, size=(s0,))}\nbuf1.group.device = cuda:0\nbuf1.group.iteration = (1, s0)\nbuf1.sizes = ([], [s0])\nclass buf1_loop_body:\n    var_ranges = {z0: s0}\n    index0 = z0\n    index1 = 0\n    def body(self, ops):\n        get_index = self.get_index('index0')\n        load = ops.load('buf0', get_index, False)\n        get_index_1 = self.get_index('index0')\n        load_1 = ops.load('primals_2', get_index_1, False)\n        add = ops.add(load, load_1)\n        get_index_2 = self.get_index('index1')\n        reduction = ops.reduction('buf1', torch.float32, torch.float32, 'sum', get_index_2, add)\n        return reduction\n See the example debug directory output for more examples. Graph Breaks Given a program like this: def some_fun(x):\n    ...\n\ncompiled_fun = torch.compile(some_fun, ...)\n...\n TorchDynamo will attempt to compile all of the torch/tensor operations within some_fun into a single FX graph, but it may fail to capture everything into one graph. Some graph break reasons are insurmountable to TorchDynamo, and can\u2019t be easily fixed. - calling into a C extension other than torch is invisible to torchdynamo, and could do arbitrary things without TorchDynamo being able to introduce necessary guards to ensure that the compiled program would be safe to reuse. Graph breaks can hinder performance if the resulting fragments are small. To maximize performance, it\u2019s important to have as few graph breaks as possible. Identifying the Cause of a Graph Break To identify all graph breaks in a program and the associated reasons for the breaks, torch._dynamo.explain can be used. This tool runs TorchDynamo on the supplied function and aggregates the graph breaks that are encountered. Here is an example usage: import torch\nimport torch._dynamo as dynamo\ndef toy_example(a, b):\n    x = a / (torch.abs(a) + 1)\n    print(\"woo\")\n    if b.sum() < 0:\n        b = b * -1\n    return x * b\nexplanation, out_guards, graphs, ops_per_graph, break_reasons, explanation_verbose = (\n    dynamo.explain(toy_example, torch.randn(10), torch.randn(10))\n)\nprint(explanation_verbose)\n\"\"\"\nDynamo produced 3 graphs, with 2 graph breaks and 6 ops.\n Break reasons:\n1. call_function BuiltinVariable(print) [ConstantVariable(str)] {}\n   File \"t2.py\", line 16, in toy_example\n    print(\"woo\")\n\n2. generic_jump\n   File \"t2.py\", line 17, in toy_example\n    if b.sum() < 0:\n \"\"\"\n Outputs include:  \nout_guards - a list of lists where each sublist contains the guards that must pass to ensure the traced graphs are valid. \ngraphs - a list of graph modules which were successfully traced. \nops_per_graph - a list of lists where each sublist contains the ops that are run in the graph.  To throw an error on the first graph break encountered, use the nopython mode. This mode disables TorchDynamo\u2019s Python fallback, and only succeeds if the entire program is convertible into a single graph. Example usage: def toy_example(a, b):\n   ...\n\ncompiled_toy = torch.compile(toy_example, fullgraph=True, backend=<compiler>)\n Excessive Recompilation When TorchDynamo compiles a function (or part of one), it makes certain assumptions about locals and globals in order to allow compiler optimizations, and expresses these assumptions as guards that check particular values at runtime. If any of these guards fail, Dynamo will recompile that function (or part) up to torch._dynamo.config.cache_size_limit times. If your program is hitting the cache limit, you will first need to determine which guard is failing and what part of your program is triggering it. The compile profiler automates the process of setting TorchDynamo\u2019s cache limit to 1 and running your program under an observation-only \u2018compiler\u2019 that records the causes of any guard failures. You should be sure to run your program for at least as long (as many iterations) as you were running when you ran into trouble, and the profiler will accumulate statistics over this duration. If your program exhibits a bounded amount of dynamism, you may be able to tune the TorchDynamo cache limit to allow for each variation to be compiled and cached, but if the cache limit is too high you may find the cost of recompilation outweighs any optimization benefits. torch._dynamo.config.cache_size_limit = <your desired cache limit>\n TorchDynamo plans to support many common cases of dynamic tensor shapes, such as varying batch size or sequence length. It does not plan to support rank-dynamism. In the meantime, setting a specific cache limit can be used in coordination with bucketing techniques to achieve an acceptable number of recompilations for some dynamic models. from torch._dynamo.utils import CompileProfiler\n\ndef my_model():\n    ...\n\nwith CompileProfiler() as prof:\n    profiler_model = torch.compile(my_model, backend=prof)\n    profiler_model()\n    print(prof.report())\n Accuracy Debugging Accuracy issues can also be minified if you set the environment variable TORCHDYNAMO_REPRO_LEVEL=4, it operates with a similar git bisect model and a full repro might be something like TORCHDYNAMO_REPRO_AFTER=\"aot\" TORCHDYNAMO_REPRO_LEVEL=4 the reason we need this is downstream compilers will codegen code whether it\u2019s Triton code or the C++ backend, the numerics from those downstream compilers can be different in subtle ways yet have dramatic impact on your training stability. So the accuracy debugger is very useful for us to detect bugs in our codegen or with a backend compiler. If you\u2019d like to ensure that random number generation is the same across both torch and triton then you can enable torch._inductor.config.fallback_random = True\n"}, {"name": "torch.compiler.reset()", "path": "generated/torch.compiler.reset#torch.compiler.reset", "type": "Miscellaneous", "text": " \ntorch.compiler.reset() [source]\n \nThis function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation \n"}, {"name": "torch.compiler.torch.compiler API reference", "path": "torch.compiler_api", "type": "Miscellaneous", "text": "torch.compiler API reference For a quick overview of torch.compiler, see torch.compiler.  \n\ncompile\n See torch.compile() for details on the arguments for this function.  \n\nreset\n This function clears all compilation caches and restores the system to its initial state.  \n\nallow_in_graph\n Customize which functions compilation will include in the generated graph.  \n\nassume_constant_result\n This function is used to mark a function fn as having a constant result.  \n\nlist_backends\n Return valid strings that can be passed to torch.compile(..., backend=\"name\").  \n\ndisable\n This function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions  \n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.allow_in_graph", "path": "generated/torch.compiler.allow_in_graph", "type": "Miscellaneous", "text": "torch.compiler.allow_in_graph  \ntorch.compiler.allow_in_graph(fn) [source]\n \nCustomize which functions compilation will include in the generated graph. It bypasses all introspection of the symbolic python code in favor of directly writing it to the graph. If fn is a list or tuple of callables it recursively applies allow_in_graph() to each function and returns a new list or tuple containing the modified functions  Parameters \nfn \u2013 A callable representing the function to be included in the graph.    Warning allow_in_graph() skips TorchDynamo completely on the decorated function skipping all TorchDynamo safety checks (graph breaks, handling closures, etc). Therefore, one has to be very careful with allow_in_graph() since subsystems like AOT Autograd rely on torchdynamo If not careful, this could lead to soundness and really hard-to-debug issues.  \n\n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.assume_constant_result", "path": "generated/torch.compiler.assume_constant_result", "type": "Miscellaneous", "text": "torch.compiler.assume_constant_result  \ntorch.compiler.assume_constant_result(fn) [source]\n \nThis function is used to mark a function fn as having a constant result. This allows the compiler to optimize away your function Returns The same function fn  Parameters \nfn \u2013 The function to be marked as having a constant result.    Warning assume_constant_result can if invalid cause safety and soundness issues, torch.compile() will not attempt to validate whether the constant assumption is true or not  \n\n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.compile", "path": "generated/torch.compiler.compile", "type": "Miscellaneous", "text": "torch.compiler.compile  \ntorch.compiler.compile(*args, **kwargs) [source]\n \nSee torch.compile() for details on the arguments for this function. \n\n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.disable", "path": "generated/torch.compiler.disable", "type": "Miscellaneous", "text": "torch.compiler.disable  \ntorch.compiler.disable(fn=None, recursive=True) [source]\n \nThis function provides both a decorator and a context manager to disable compilation on a function It also provides the option of recursively disabling called functions  Parameters \n \nfn (optional) \u2013 The function to disable \nrecursive (optional) \u2013 A boolean value indicating whether the disabling should be recursive.    \n\n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.list_backends", "path": "generated/torch.compiler.list_backends", "type": "Miscellaneous", "text": "torch.compiler.list_backends  \ntorch.compiler.list_backends(exclude_tags=('debug', 'experimental')) [source]\n \nReturn valid strings that can be passed to torch.compile(\u2026, backend=\u201dname\u201d).  Parameters \nexclude_tags (optional) \u2013 A tuple of strings representing tags to exclude.  Return type \nList[str]   \n\n"}, {"name": "torch.compiler.torch.compiler API reference.torch.compiler.reset", "path": "generated/torch.compiler.reset", "type": "Miscellaneous", "text": "torch.compiler.reset  \ntorch.compiler.reset() [source]\n \nThis function clears all compilation caches and restores the system to its initial state. It is recommended to call this function, especially after using operations like torch.compile(\u2026) to ensure a clean state before another unrelated compilation \n\n"}, {"name": "torch.compiler.TorchDynamo APIs for fine-grained tracing", "path": "torch.compiler_fine_grain_apis", "type": "Miscellaneous", "text": "TorchDynamo APIs for fine-grained tracing  Note In this document torch.compiler.compile and torch.compile are used interchangeably. Both versions will work in your code.  torch.compile performs TorchDynamo tracing on the whole user model. However, it is possible that a small part of the model code cannot be handeled by torch.compiler. In this case, you might want to disable the compiler on that particular portion, while running compilation on the rest of the model. This section describe the existing APIs that use to define parts of your code in which you want to skip compilation and the relevant use cases. The API that you can use to define portions of the code on which you can disable compilation are listed in the following table:  TorchDynamo APIs to control fine-grained tracing  \nAPI Description When to use?   \ntorch.compiler.disable Disables Dynamo on the decorated function as well as recursively invoked functions. Excellent for unblocking a user, if a small portion of the model cannot be handeled with torch.compile.  \ntorch._dynamo.disallow_in_graph Disallows the marked op in the TorchDynamo graph. TorchDynamo causes graph break, and runs the op in the eager (no compile) mode.nnThis is suitable for the ops, while torch.compiler.disable is suitable for decorating functions. This API is excellent for both debugging and unblocking if a custom op like torch.ops.fbgemm.* is causing issues with the torch.compile function.  \ntorch.compile.allow_in_graph The annotated callable goes as is in the TorchDynamo graph. For example, a black-box for TorchDynamo Dynamo.nnNote that AOT Autograd will trace through it, so the allow_in_graph is only a Dynamo-level concept. This API is useful for portions of the model which have known TorchDynamo hard-to-support features, like hooks or autograd.Function. However, each usage of allow_in_graph must be carefully screened (no graph breaks, no closures).  \ntorch._dynamo.graph_break Adds a graph break. The code before and after the graph break goes through TorchDynamo. Rarely useful for deployment - If you think you need this, most probably you need either disable or disallow_in_graph.   torch.compiler.disable torch.compiler.disable disables compilation on the decorated function frame and all the function frames recursively invoked from the decorated function frame. TorchDynamo intercepts the execution of each Python function frame. So, suppose you have a code structure (image below) where the function fn calls functions a_fn and b_fn. And a_fn calls aa_fn and ab_fn. When you use the PyTorch eager mode rather than torch.compile, these function frames run as is. With torch.compile, TorchDynamo intercepts each of these function frames (indicated by the green color):    Let\u2019s imagine, that function a_fn is causing troubles with torch.compile. And this is a non-critical portion of the model. You can use compiler.disable on function a_fn. As shown above, TorchDynamo will stop looking at frames originating from the a_fn call (white color indicates original Python behavior). To skip compilation, you can decorate the offending function with @torch.compiler.disable. You can also use the non-decorator syntax if you don\u2019t want to change the source code However, we recommend that you avoid this style if possible. Here, you have to take care that all users of the original function are now using the patched version. torch._dynamo.disallow_in_graph torch._dynamo.disallow_in_graph disallows an operator but not the function to be present in the TorchDynamo extracted graph. Note that this is suitable for operators and not general functions as in the case of _dynamo.disable. Let\u2019s imagine you compile your model with PyTorch. TorchDynamo is able to extract a graph, but then you see the downstream compiler failing. For example, the meta kernel is missing, or some Autograd dispatch key is set incorrectly for a particular operator. Then you can mark that operator as disallow_in_graph, and TorchDynamo will cause a graph break and run that operator by using the PyTorch eager mode. The catch is that you will have to find the corresponding Dynamo level operator, and not the ATen level operator. See more in the Limitations section of the doc.  Warning torch._dynamo.disallow_in_graph is a global flag. If you are comparing different backend compilers, you might have to call allow_in_graph for the disallowed operator when switching to the other compiler.  torch.compiler.allow_in_graph torch.compiler.allow_in_graph is useful when the relevant function frame has some known hard-to-support TorchDynamo feature, such as hooks and autograd.Function, and you are confident that downstream PyTorch components such as AOTAutograd can safely trace through the decorated function. When a function is decorated with allow_in_graph, TorchDynamo treats it as a black-box and puts it as is in the generated graph.  Warning allow_in_graph skips TorchDynamo completely on the decorated function omitting all TorchDynamo safety checks, including graph breaks, handling closures, and others. Use allow_in_graph with caution. PyTorch downstream components, such as AOTAutograd rely on TorchDynamo to handle complex Python features, but allow_in_graph bypasses TorchDynamo. Using allow_in_graph could lead to soundness and hard-to-debug issues.  Limitations All the existing APIs are applied at the TorchDynamo level. Therefore, these APIs have visibility to only what TorchDynamo sees. This can lead to confusing scenarios. For example, torch._dynamo.disallow_in_graph will not work for ATen operators because they are visible to AOT Autograd. For example, torch._dynamo.disallow_in_graph(torch.ops.aten.add) will not work in the above example.\n"}, {"name": "torch.compiler.TorchInductor GPU Profiling", "path": "torch.compiler_inductor_profiling", "type": "Miscellaneous", "text": "TorchInductor GPU Profiling This section lists useful commands and workflows that can help you dive into a model\u2019s performance in TorchInductor. When a model is not running as fast as expected, you may want to check individual kernels of the model. Usually, those kernels taking the majority of the GPU time are the most interesting ones. After that, you may also want to run individual kernels directly and inspect its perf. PyTorch provides tools to cover everything mentioned above. Relevant Environment Variables You can use the following environment variables in your analysis:  \nTORCHINDUCTOR_UNIQUE_KERNEL_NAMES  By default, TorchInductor names a Triton kernel as \u2018triton\\_\u2019. When this environmental variable is enabled, inductor generates a more meaningful kernel name in the trace, for example, triton_poi_fused_cat_155 which contains the kernel category (poi for pointwise) and original ATen operator. This config is disabled by default to improve the chance of compilation cache hit.   \nTORCHINDUCTOR_BENCHMARK_KERNEL  Enabling this will make inductor codegen harness to benchmark individual triton kernels.   \nTORCHINDUCTOR_MAX_AUTOTUNE  Inductor autotuner will benchmark more triton.Configs and pick the one with the best performance results. This will increase compilation time with the hope to improve performance.    Breakdown Model GPU Time Below are the steps to breakdown execution time of a model into individual kernels. We take mixnet_l as an example.  \nRun the benchmark script for the model: TORCHINDUCTOR_UNIQUE_KERNEL_NAMES=1 TORCHINDUCTOR_BENCHMARK_KERNEL=1\npython -u benchmarks/dynamo/timm_models.py \u2013backend inductor \u2013amp\n\u2013performance \u2013dashboard \u2013only mixnet_l \u2013disable-cudagraphs \u2013training\n  Note The tool relies on kernel name to decide its category. Enabling TORCHINDUCTOR_UNIQUE_KERNEL_NAMES is crucial for that.   \nIn the output log, look for lines: **Compiled module path:\n/tmp/torchinductor_shunting/qz/cqz7hvhood7y3psp7fy6msjxsxyli7qiwiybizdwtjw6ffyq5wwd.py**\n   We have one line for each compiled module. If there are no extra graph breaks, we would see 2 such lines in the log, one for the forward graph and one for the backward graph. For our example command, we get the following compiled module for the forward and backward graphs respectively:  https://gist.github.com/shunting314/c2a4d8a28b00fcb5586d0e9d9bf77f9f https://gist.github.com/shunting314/48efc83b12ec3ead950052e4a0220b10   \nNow we can dive into the perf for each individual compiled module. Let\u2019s pick the one for the forward graph for illustration purposes. I\u2019ll name it fwd.py for convenience. Run it directly with the -p argument: **> python fwd.py -p**\n   See the full output log in this example gist. In the output, you can notice the following:  We write a chrome trace file for the profile so we can load the trace and interact with it. In the log, look for lines as follows to find the path of the trace file.  Chrome trace for the profile is written to /tmp/compiled_module_profile.json Loading the trace into Chrome (visit chrome://tracing in the chrome browser and load the file as the UI suggested) will show UI as follows:  You can zoom in and out to check the profile.  \nWe report the percent of GPU time regarding to the wall time by log line like: Percent of time when GPU is busy: 102.88% Sometimes you may see a value larger than 100%. The reason is because PyTorch uses the kernel execution time with profiling enabled while using wall time with profiling disabled. Profiling may distort the kernel execution time a bit. But overall it should not be a big deal. If we run the model like densenet121 with a small batch size, we would see low percent of time when GPU is busy: (Forward graph) Percent of time when GPU is busy: 32.69%\n This means the model has a lot of CPU overhead. This is consistent with the fact that enabling cudagraphs improve densenet121\u2019s perf a lot.  \nWe can break down the GPU time to different categories of kernels. In the mixnet_l example, we see  pointwise kernel takes 28.58% reduction kernel takes 13.85% persistent reduction kernel takes 3.89% the rest are cutlass/cudnn kernels for mm/conv which takes 56.57%  This information can be found in the summary line (last line) of the report for each kernel category.  \nWe also call zoom into a certain category of kernels. For example, let\u2019s check reduction kernels:  We can see an ordered table of execution time for each individual reduction kernel. We also see how many times a kernel is executed. This is helpful for a few reasons:  If a kernel only takes a tiny amount of time, for example, 0.1%, improving it will at most bring 0.1% overall gain. It is not worth spending a lot of effort on it. Ff a kernel takes 2% of time, improving it by 2x will bring in 1% overall gain which justifies the effort.    Benchmark Individual Triton Kernel Let\u2019s say we want to take a closer look at triton_red_fused\\__native_batch_norm_legit_functional_16 which is the most expensive reduction kernel and takes 2.19% of overall wall time for the forward graph. We can lookup the kernel name in the fwd.py, and find comment like: # kernel path: /tmp/torchinductor_shunting/jk/cjk2vm3446xrk7rth7hr6pun7xxo3dnzubwcn6ydrpifal4eykrz.py  I\u2019ll rename it k.py for convenience. Here is a paste for this file. k.py is a standalone Python module containing the kernel code and its benchmark. Run k.py directly will report its execution time and bandwidth:  We can check if max-autotune helps this kernel, by running: **TORCHINDUCTOR_MAX_AUTOTUNE=1 python /tmp/k.py**\n We may also temporarily add more reduction heuristics and run the script again to check how that helps with the kernel.\n"}, {"name": "torch.complex", "path": "generated/torch.complex", "type": "Torch", "text": "torch.complex  \ntorch.complex(real, imag, *, out=None) \u2192 Tensor  \nConstructs a complex tensor with its real part equal to real and its imaginary part equal to imag.  Parameters \n \nreal (Tensor) \u2013 The real part of the complex tensor. Must be half, float or double. \nimag (Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype as real.   Keyword Arguments \nout (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.   Example: >>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64\n \n\n"}, {"name": "torch.ComplexDoubleStorage", "path": "storage#torch.ComplexDoubleStorage", "type": "Storage", "text": " \nclass torch.ComplexDoubleStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.complex128 [source]\n\n \n"}, {"name": "torch.ComplexDoubleStorage.dtype", "path": "storage#torch.ComplexDoubleStorage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.complex128 [source]\n\n"}, {"name": "torch.ComplexFloatStorage", "path": "storage#torch.ComplexFloatStorage", "type": "Storage", "text": " \nclass torch.ComplexFloatStorage(*args, wrap_storage=None, dtype=None, device=None, _internal=False) [source]\n \n \ndtype: dtype = torch.complex64 [source]\n\n \n"}, {"name": "torch.ComplexFloatStorage.dtype", "path": "storage#torch.ComplexFloatStorage.dtype", "type": "Storage", "text": " \ndtype: dtype = torch.complex64 [source]\n\n"}, {"name": "torch.concat", "path": "generated/torch.concat", "type": "Torch", "text": "torch.concat  \ntorch.concat(tensors, dim=0, *, out=None) \u2192 Tensor  \nAlias of torch.cat(). \n\n"}, {"name": "torch.concatenate", "path": "generated/torch.concatenate", "type": "Torch", "text": "torch.concatenate  \ntorch.concatenate(tensors, axis=0, out=None) \u2192 Tensor  \nAlias of torch.cat(). \n\n"}, {"name": "torch.conj", "path": "generated/torch.conj", "type": "Torch", "text": "torch.conj  \ntorch.conj(input) \u2192 Tensor  \nReturns a view of input with a flipped conjugate bit. If input has a non-complex dtype, this function just returns input.  Note torch.conj() performs a lazy conjugation, but the actual conjugated tensor can be materialized at any time using torch.resolve_conj().   Warning In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> x = torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])\n>>> x.is_conj()\nFalse\n>>> y = torch.conj(x)\n>>> y.is_conj()\nTrue\n \n\n"}, {"name": "torch.conj_physical", "path": "generated/torch.conj_physical", "type": "Torch", "text": "torch.conj_physical  \ntorch.conj_physical(input, *, out=None) \u2192 Tensor  \nComputes the element-wise conjugate of the given input tensor. If input has a non-complex dtype, this function just returns input.  Note This performs the conjugate operation regardless of the fact conjugate bit is set or not.   Warning In the future, torch.conj_physical() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj_physical() when input is of non-complex dtype to be compatible with this change.   outi=conj(inputi)\\text{out}_{i} = conj(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.conj_physical(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n \n\n"}, {"name": "torch.copysign", "path": "generated/torch.copysign", "type": "Torch", "text": "torch.copysign  \ntorch.copysign(input, other, *, out=None) \u2192 Tensor  \nCreate a new floating-point tensor with the magnitude of input and the sign of other, elementwise.  outi={\u2212\u2223inputi\u2223if otheri\u2264\u22120.0\u2223inputi\u2223if otheri\u22650.0\\text{out}_{i} = \\begin{cases} -|\\text{input}_{i}| & \\text{if } \\text{other}_{i} \\leq -0.0 \\\\ |\\text{input}_{i}| & \\text{if } \\text{other}_{i} \\geq 0.0 \\\\ \\end{cases} \n\nSupports broadcasting to a common shape, and integer and float inputs.  Parameters \n \ninput (Tensor) \u2013 magnitudes. \nother (Tensor or Number) \u2013 contains value(s) whose signbit(s) are applied to the magnitudes in input.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\n>>> a = torch.tensor([1.])\n>>> b = torch.tensor([-0.])\n>>> torch.copysign(a, b)\ntensor([-1.])\n  Note copysign handles signed zeros. If the other argument has a negative zero (-0), the corresponding output value will be negative.  \n\n"}, {"name": "torch.corrcoef", "path": "generated/torch.corrcoef", "type": "Torch", "text": "torch.corrcoef  \ntorch.corrcoef(input) \u2192 Tensor  \nEstimates the Pearson product-moment correlation coefficient matrix of the variables given by the input matrix, where rows are the variables and columns are the observations.  Note The correlation coefficient matrix R is computed using the covariance matrix C as given by Rij=CijCii\u2217CjjR_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} * C_{jj} } }   Note Due to floating point rounding, the resulting array may not be Hermitian and its diagonal elements may not be 1. The real and imaginary values are clipped to the interval [-1, 1] in an attempt to improve this situation.   Parameters \ninput (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.  Returns \n(Tensor) The correlation coefficient matrix of the variables.    See also torch.cov() covariance matrix.  Example: >>> x = torch.tensor([[0, 1, 2], [2, 1, 0]])\n>>> torch.corrcoef(x)\ntensor([[ 1., -1.],\n        [-1.,  1.]])\n>>> x = torch.randn(2, 4)\n>>> x\ntensor([[-0.2678, -0.0908, -0.3766,  0.2780],\n        [-0.5812,  0.1535,  0.2387,  0.2350]])\n>>> torch.corrcoef(x)\ntensor([[1.0000, 0.3582],\n        [0.3582, 1.0000]])\n>>> torch.corrcoef(x[0])\ntensor(1.)\n \n\n"}, {"name": "torch.cos", "path": "generated/torch.cos", "type": "Torch", "text": "torch.cos  \ntorch.cos(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the cosine of the elements of input.  outi=cos\u2061(inputi)\\text{out}_{i} = \\cos(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n \n\n"}, {"name": "torch.cosh", "path": "generated/torch.cosh", "type": "Torch", "text": "torch.cosh  \ntorch.cosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the hyperbolic cosine of the elements of input.  outi=cosh\u2061(inputi)\\text{out}_{i} = \\cosh(\\text{input}_{i}) \n\n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n  Note When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details.  \n\n"}, {"name": "torch.count_nonzero", "path": "generated/torch.count_nonzero", "type": "Torch", "text": "torch.count_nonzero  \ntorch.count_nonzero(input, dim=None) \u2192 Tensor  \nCounts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros.    Example: >>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])\n \n\n"}, {"name": "torch.cov", "path": "generated/torch.cov", "type": "Torch", "text": "torch.cov  \ntorch.cov(input, *, correction=1, fweights=None, aweights=None) \u2192 Tensor  \nEstimates the covariance matrix of the variables given by the input matrix, where rows are the variables and columns are the observations. A covariance matrix is a square matrix giving the covariance of each pair of variables. The diagonal contains the variance of each variable (covariance of a variable with itself). By definition, if input represents a single variable (Scalar or 1D) then its variance is returned. The sample covariance of the variables xx and yy is given by:  cov(x,y)=\u2211i=1N(xi\u2212x\u02c9)(yi\u2212y\u02c9)max\u2061(0,N\u2212\u03b4N)\\text{cov}(x,y) = \\frac{\\sum^{N}_{i = 1}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\max(0,~N~-~\\delta N)} \n\nwhere x\u02c9\\bar{x} and y\u02c9\\bar{y} are the simple means of the xx and yy respectively, and \u03b4N\\delta N is the correction. If fweights and/or aweights are provided, the weighted covariance is calculated, which is given by:  covw(x,y)=\u2211i=1Nwi(xi\u2212\u03bcx\u2217)(yi\u2212\u03bcy\u2217)max\u2061(0,\u2211i=1Nwi\u2212\u2211i=1Nwiai\u2211i=1Nwi\u03b4N)\\text{cov}_w(x,y) = \\frac{\\sum^{N}_{i = 1}w_i(x_{i} - \\mu_x^*)(y_{i} - \\mu_y^*)} {\\max(0,~\\sum^{N}_{i = 1}w_i~-~\\frac{\\sum^{N}_{i = 1}w_ia_i}{\\sum^{N}_{i = 1}w_i}~\\delta N)} \n\nwhere ww denotes fweights or aweights (f and a for brevity) based on whichever is provided, or w=f\u00d7aw = f \\times a if both are provided, and \u03bcx\u2217=\u2211i=1Nwixi\u2211i=1Nwi\\mu_x^* = \\frac{\\sum^{N}_{i = 1}w_ix_{i} }{\\sum^{N}_{i = 1}w_i} is the weighted mean of the variable. If not provided, f and/or a can be seen as a 1\\mathbb{1} vector of appropriate size.  Parameters \ninput (Tensor) \u2013 A 2D matrix containing multiple variables and observations, or a Scalar or 1D vector representing a single variable.  Keyword Arguments \n \ncorrection (int, optional) \u2013 difference between the sample size and sample degrees of freedom. Defaults to Bessel\u2019s correction, correction = 1 which returns the unbiased estimate, even if both fweights and aweights are specified. correction = 0 will return the simple average. Defaults to 1. \nfweights (tensor, optional) \u2013 A Scalar or 1D tensor of observation vector frequencies representing the number of times each observation should be repeated. Its numel must equal the number of columns of input. Must have integral dtype. Ignored if None. Defaults to None. \naweights (tensor, optional) \u2013 A Scalar or 1D array of observation vector weights. These relative weights are typically large for observations considered \u201cimportant\u201d and smaller for observations considered less \u201cimportant\u201d. Its numel must equal the number of columns of input. Must have floating point dtype. Ignored if None. Defaults to None.   Returns \n(Tensor) The covariance matrix of the variables.    See also torch.corrcoef() normalized covariance matrix.   Example::\n\n>>> x = torch.tensor([[0, 2], [1, 1], [2, 0]]).T\n>>> x\ntensor([[0, 1, 2],\n        [2, 1, 0]])\n>>> torch.cov(x)\ntensor([[ 1., -1.],\n        [-1.,  1.]])\n>>> torch.cov(x, correction=0)\ntensor([[ 0.6667, -0.6667],\n        [-0.6667,  0.6667]])\n>>> fw = torch.randint(1, 10, (3,))\n>>> fw\ntensor([1, 6, 9])\n>>> aw = torch.rand(3)\n>>> aw\ntensor([0.4282, 0.0255, 0.4144])\n>>> torch.cov(x, fweights=fw, aweights=aw)\ntensor([[ 0.4169, -0.4169],\n        [-0.4169,  0.4169]])\n   \n\n"}, {"name": "torch.cpu", "path": "cpu", "type": "Miscellaneous", "text": "torch.cpu This package implements abstractions found in torch.cuda to facilitate writing device-agnostic code.  \n\ncurrent_stream\n Returns the currently selected Stream for a given device.  \n\nis_available\n Returns a bool indicating if CPU is currently available.  \n\nsynchronize\n Waits for all kernels in all streams on the CPU device to complete.  \n\nstream\n Wrapper around the Context-manager StreamContext that selects a given stream.  \n\ndevice_count\n Returns number of CPU devices (not cores).  \n\nStreamContext\n Context-manager that selects a given stream.   Streams and events  \n\nStream\n N.B.  \n"}, {"name": "torch.cpu.amp.autocast", "path": "amp#torch.cpu.amp.autocast", "type": "Automatic Mixed Precision", "text": " \nclass torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16, cache_enabled=True) [source]\n \nSee torch.autocast. torch.cpu.amp.autocast(args...) is equivalent to torch.autocast(\"cpu\", args...) \n"}, {"name": "torch.cpu.current_stream()", "path": "generated/torch.cpu.current_stream#torch.cpu.current_stream", "type": "Miscellaneous", "text": " \ntorch.cpu.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 Ignored.  Return type \nStream   N.B. This function only exists to facilitate device-agnostic code \n"}, {"name": "torch.cpu.device_count()", "path": "generated/torch.cpu.device_count#torch.cpu.device_count", "type": "Miscellaneous", "text": " \ntorch.cpu.device_count() [source]\n \nReturns number of CPU devices (not cores). Always 1. N.B. This function only exists to facilitate device-agnostic code  Return type \nint   \n"}, {"name": "torch.cpu.is_available()", "path": "generated/torch.cpu.is_available#torch.cpu.is_available", "type": "Miscellaneous", "text": " \ntorch.cpu.is_available() [source]\n \nReturns a bool indicating if CPU is currently available. N.B. This function only exists to facilitate device-agnostic code  Return type \nbool   \n"}, {"name": "torch.cpu.stream()", "path": "generated/torch.cpu.stream#torch.cpu.stream", "type": "Miscellaneous", "text": " \ntorch.cpu.stream(stream) [source]\n \nWrapper around the Context-manager StreamContext that selects a given stream. N.B. This function only exists to facilitate device-agnostic code  Return type \nAbstractContextManager   \n"}, {"name": "torch.cpu.StreamContext", "path": "generated/torch.cpu.streamcontext", "type": "Miscellaneous", "text": "StreamContext  \nclass torch.cpu.StreamContext(stream) [source]\n \nContext-manager that selects a given stream. N.B. This class only exists to facilitate device-agnostic code \n\n"}, {"name": "torch.cpu.synchronize()", "path": "generated/torch.cpu.synchronize#torch.cpu.synchronize", "type": "Miscellaneous", "text": " \ntorch.cpu.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on the CPU device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.   N.B. This function only exists to facilitate device-agnostic code. \n"}, {"name": "torch.cpu.torch.cpu.current_stream", "path": "generated/torch.cpu.current_stream", "type": "Miscellaneous", "text": "torch.cpu.current_stream  \ntorch.cpu.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 Ignored.  Return type \nStream   N.B. This function only exists to facilitate device-agnostic code \n\n"}, {"name": "torch.cpu.torch.cpu.device_count", "path": "generated/torch.cpu.device_count", "type": "Miscellaneous", "text": "torch.cpu.device_count  \ntorch.cpu.device_count() [source]\n \nReturns number of CPU devices (not cores). Always 1. N.B. This function only exists to facilitate device-agnostic code  Return type \nint   \n\n"}, {"name": "torch.cpu.torch.cpu.is_available", "path": "generated/torch.cpu.is_available", "type": "Miscellaneous", "text": "torch.cpu.is_available  \ntorch.cpu.is_available() [source]\n \nReturns a bool indicating if CPU is currently available. N.B. This function only exists to facilitate device-agnostic code  Return type \nbool   \n\n"}, {"name": "torch.cpu.torch.cpu.stream", "path": "generated/torch.cpu.stream", "type": "Miscellaneous", "text": "torch.cpu.stream  \ntorch.cpu.stream(stream) [source]\n \nWrapper around the Context-manager StreamContext that selects a given stream. N.B. This function only exists to facilitate device-agnostic code  Return type \nAbstractContextManager   \n\n"}, {"name": "torch.cpu.torch.cpu.synchronize", "path": "generated/torch.cpu.synchronize", "type": "Miscellaneous", "text": "torch.cpu.synchronize  \ntorch.cpu.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on the CPU device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 ignored, there\u2019s only one CPU device.   N.B. This function only exists to facilitate device-agnostic code. \n\n"}, {"name": "torch.cross", "path": "generated/torch.cross", "type": "Torch", "text": "torch.cross  \ntorch.cross(input, other, dim=None, *, out=None) \u2192 Tensor  \nReturns the cross product of vectors in dimension dim of input and other. Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of vectors, for which it computes the product along the dimension dim. In this case, the output has the same batch dimensions as the inputs. If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.  See also torch.linalg.cross() which requires specifying dim (defaulting to -1).   Warning This function may change in a future PyTorch release to match the default behaviour in torch.linalg.cross(). We recommend using torch.linalg.cross().   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor \ndim (int, optional) \u2013 the dimension to take the cross-product in.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n \n\n"}, {"name": "torch.cuda", "path": "cuda", "type": "CUDA", "text": "torch.cuda This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.  \n\nStreamContext\n Context-manager that selects a given stream.  \n\ncan_device_access_peer\n Checks if peer access between two devices is possible.  \n\ncurrent_blas_handle\n Returns cublasHandle_t pointer to current cuBLAS handle  \n\ncurrent_device\n Returns the index of a currently selected device.  \n\ncurrent_stream\n Returns the currently selected Stream for a given device.  \n\ndefault_stream\n Returns the default Stream for a given device.  \n\ndevice\n Context-manager that changes the selected device.  \n\ndevice_count\n Returns the number of GPUs available.  \n\ndevice_of\n Context-manager that changes the current device to that of given object.  \n\nget_arch_list\n Returns list CUDA architectures this library was compiled for.  \n\nget_device_capability\n Gets the cuda capability of a device.  \n\nget_device_name\n Gets the name of a device.  \n\nget_device_properties\n Gets the properties of a device.  \n\nget_gencode_flags\n Returns NVCC gencode flags this library was compiled with.  \n\nget_sync_debug_mode\n Returns current value of debug mode for cuda synchronizing operations.  \n\ninit\n Initialize PyTorch's CUDA state.  \n\nipc_collect\n Force collects GPU memory after it has been released by CUDA IPC.  \n\nis_available\n Returns a bool indicating if CUDA is currently available.  \n\nis_initialized\n Returns whether PyTorch's CUDA state has been initialized.  \n\nmemory_usage\n Returns the percent of time over the past sample period during which global (device) memory was being read or written.  \n\nset_device\n Sets the current device.  \n\nset_stream\n Sets the current stream.This is a wrapper API to set the stream.  \n\nset_sync_debug_mode\n Sets the debug mode for cuda synchronizing operations.  \n\nstream\n Wrapper around the Context-manager StreamContext that selects a given stream.  \n\nsynchronize\n Waits for all kernels in all streams on a CUDA device to complete.  \n\nutilization\n Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.  \n\ntemperature\n Returns the average temperature of the GPU sensor in Degrees C (Centigrades)  \n\npower_draw\n Returns the average power draw of the GPU sensor in mW (MilliWatts)  \n\nclock_rate\n Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.  \n\nOutOfMemoryError\n Exception raised when CUDA is out of memory   Random Number Generator  \n\nget_rng_state\n Returns the random number generator state of the specified GPU as a ByteTensor.  \n\nget_rng_state_all\n Returns a list of ByteTensor representing the random number states of all devices.  \n\nset_rng_state\n Sets the random number generator state of the specified GPU.  \n\nset_rng_state_all\n Sets the random number generator state of all devices.  \n\nmanual_seed\n Sets the seed for generating random numbers for the current GPU.  \n\nmanual_seed_all\n Sets the seed for generating random numbers on all GPUs.  \n\nseed\n Sets the seed for generating random numbers to a random number for the current GPU.  \n\nseed_all\n Sets the seed for generating random numbers to a random number on all GPUs.  \n\ninitial_seed\n Returns the current random seed of the current GPU.   Communication collectives  \ncomm.broadcast Broadcasts a tensor to specified GPU devices.  \ncomm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs.  \ncomm.reduce_add Sums tensors from multiple GPUs.  \ncomm.scatter Scatters tensor across multiple GPUs.  \ncomm.gather Gathers tensors from multiple GPU devices.   Streams and events  \n\nStream\n Wrapper around a CUDA stream.  \n\nExternalStream\n Wrapper around an externally allocated CUDA stream.  \n\nEvent\n Wrapper around a CUDA event.   Graphs (beta)  \n\nis_current_stream_capturing\n Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.  \n\ngraph_pool_handle\n Returns an opaque token representing the id of a graph memory pool.  \n\nCUDAGraph\n Wrapper around a CUDA graph.  \n\ngraph\n Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay.  \n\nmake_graphed_callables\n Accepts callables (functions or nn.Modules) and returns graphed versions.   Memory management  \n\nempty_cache\n Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  \n\nlist_gpu_processes\n Returns a human-readable printout of the running processes and their GPU memory use for a given device.  \n\nmem_get_info\n Returns the global free and total GPU memory for a given device using cudaMemGetInfo.  \n\nmemory_stats\n Returns a dictionary of CUDA memory allocator statistics for a given device.  \n\nmemory_summary\n Returns a human-readable printout of the current memory allocator statistics for a given device.  \n\nmemory_snapshot\n Returns a snapshot of the CUDA memory allocator state across all devices.  \n\nmemory_allocated\n Returns the current GPU memory occupied by tensors in bytes for a given device.  \n\nmax_memory_allocated\n Returns the maximum GPU memory occupied by tensors in bytes for a given device.  \n\nreset_max_memory_allocated\n Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.  \n\nmemory_reserved\n Returns the current GPU memory managed by the caching allocator in bytes for a given device.  \n\nmax_memory_reserved\n Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.  \n\nset_per_process_memory_fraction\n Set memory fraction for a process.  \n\nmemory_cached\n Deprecated; see memory_reserved().  \n\nmax_memory_cached\n Deprecated; see max_memory_reserved().  \n\nreset_max_memory_cached\n Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.  \n\nreset_peak_memory_stats\n Resets the \"peak\" stats tracked by the CUDA memory allocator.  \n\ncaching_allocator_alloc\n Performs a memory allocation using the CUDA memory allocator.  \n\ncaching_allocator_delete\n Deletes memory allocated using the CUDA memory allocator.  \n\nget_allocator_backend\n Returns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF.  \n\nCUDAPluggableAllocator\n CUDA memory allocator loaded from a so file.  \n\nchange_current_allocator\n Changes the currently used memory allocator to be the one provided.   NVIDIA Tools Extension (NVTX)  \nnvtx.mark Describe an instantaneous event that occurred at some point.  \nnvtx.range_push Pushes a range onto a stack of nested range span.  \nnvtx.range_pop Pops a range off of a stack of nested range spans.   Jiterator (beta)  \njiterator._create_jit_fn Create a jiterator-generated cuda kernel for an elementwise op.  \njiterator._create_multi_output_jit_fn Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.   Stream Sanitizer (prototype) CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the documentation for information on how to use it.\n"}, {"name": "torch.cuda._sanitizer.enable_cuda_sanitizer()", "path": "cuda._sanitizer#torch.cuda._sanitizer.enable_cuda_sanitizer", "type": "CUDA", "text": " \ntorch.cuda._sanitizer.enable_cuda_sanitizer() [source]\n \nEnables CUDA Sanitizer. The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program. \n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "Automatic Mixed Precision", "text": " \nclass torch.cuda.amp.autocast(enabled=True, dtype=torch.float16, cache_enabled=True) [source]\n \nSee torch.autocast. torch.cuda.amp.autocast(args...) is equivalent to torch.autocast(\"cuda\", args...) \n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "Automatic Mixed Precision", "text": " \ntorch.cuda.amp.custom_bwd(bwd) [source]\n \nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail. \n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "Automatic Mixed Precision", "text": " \ntorch.cuda.amp.custom_fwd(fwd=None, *, cast_inputs=None) [source]\n \nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.  Parameters \ncast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.    Note If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.  \n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "Automatic Mixed Precision", "text": " \nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True) [source]\n \n \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n  \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n  \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n  \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n  \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n  \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n  \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n  \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n  \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n  \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n  \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n  \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n  \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n  \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.   Warning For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges.  \n \n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "Automatic Mixed Precision", "text": " \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "Automatic Mixed Precision", "text": " \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "Automatic Mixed Precision", "text": " \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "Automatic Mixed Precision", "text": " \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "Automatic Mixed Precision", "text": " \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "Automatic Mixed Precision", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "Automatic Mixed Precision", "text": " \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "Automatic Mixed Precision", "text": " \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "Automatic Mixed Precision", "text": " \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "Automatic Mixed Precision", "text": " \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "Automatic Mixed Precision", "text": " \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "Automatic Mixed Precision", "text": " \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "Automatic Mixed Precision", "text": " \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "Automatic Mixed Precision", "text": " \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the new scale value manually. (new_scale is not used directly, it\u2019s used to fill GradScaler\u2019s internal scale tensor. So if new_scale was a tensor, later in-place changes to that tensor will not further affect the scale GradScaler uses internally.)  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.   Warning For performance reasons, we do not check the scale factor value to avoid synchronizations, so the scale factor is not guaranteed to be above 1. If the scale falls below 1 and/or you are seeing NaNs in your gradients or loss, something is likely wrong. For example, bf16-pretrained models are often incompatible with AMP/fp16 due to differing dynamic ranges.  \n"}, {"name": "torch.cuda.caching_allocator_alloc()", "path": "generated/torch.cuda.caching_allocator_alloc#torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": " \ntorch.cuda.caching_allocator_alloc(size, device=None, stream=None) [source]\n \nPerforms a memory allocation using the CUDA memory allocator. Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().  Parameters \n \nsize (int) \u2013 number of bytes to be allocated. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used. \nstream (torch.cuda.Stream or int, optional) \u2013 selected stream. If is None then the default stream for the selected device is used.     Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.caching_allocator_delete()", "path": "generated/torch.cuda.caching_allocator_delete#torch.cuda.caching_allocator_delete", "type": "CUDA", "text": " \ntorch.cuda.caching_allocator_delete(mem_ptr) [source]\n \nDeletes memory allocated using the CUDA memory allocator. Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.  Parameters \nmem_ptr (int) \u2013 memory address to be freed by the allocator.    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "generated/torch.cuda.can_device_access_peer#torch.cuda.can_device_access_peer", "type": "CUDA", "text": " \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible.  Return type \nbool   \n"}, {"name": "torch.cuda.change_current_allocator()", "path": "generated/torch.cuda.change_current_allocator#torch.cuda.change_current_allocator", "type": "CUDA", "text": " \ntorch.cuda.change_current_allocator(allocator) [source]\n \nChanges the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.  Parameters \nallocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.    Note See Memory management for details on creating and using a custom allocator  \n"}, {"name": "torch.cuda.clock_rate()", "path": "generated/torch.cuda.clock_rate#torch.cuda.clock_rate", "type": "CUDA", "text": " \ntorch.cuda.clock_rate(device=None) [source]\n \nReturns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n"}, {"name": "torch.cuda.comm.broadcast()", "path": "generated/torch.cuda.comm.broadcast#torch.cuda.comm.broadcast", "type": "CUDA", "text": " \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "generated/torch.cuda.comm.broadcast_coalesced#torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": " \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n"}, {"name": "torch.cuda.comm.gather()", "path": "generated/torch.cuda.comm.gather#torch.cuda.comm.gather", "type": "CUDA", "text": " \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "generated/torch.cuda.comm.reduce_add#torch.cuda.comm.reduce_add", "type": "CUDA", "text": " \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n"}, {"name": "torch.cuda.comm.scatter()", "path": "generated/torch.cuda.comm.scatter#torch.cuda.comm.scatter", "type": "CUDA", "text": " \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[torch.cuda.Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n"}, {"name": "torch.cuda.CUDA Stream Sanitizer", "path": "cuda._sanitizer", "type": "CUDA", "text": "CUDA Stream Sanitizer  Note This is a prototype feature, which means it is at an early stage for feedback and testing, and its components are subject to change.  Overview This module introduces CUDA Sanitizer, a tool for detecting synchronization errors between kernels ran on different streams. It stores information on accesses to tensors to determine if they are synchronized or not. When enabled in a python program and a possible data race is detected, a detailed warning will be printed and the program will exit. It can be enabled either by importing this module and calling enable_cuda_sanitizer() or by exporting the TORCH_CUDA_SANITIZER environment variable. Usage Here is an example of a simple synchronization error in PyTorch: import torch\n\na = torch.rand(4, 2, device=\"cuda\")\n\nwith torch.cuda.stream(torch.cuda.Stream()):\n    torch.mul(a, 5, out=a)\n The a tensor is initialized on the default stream and, without any synchronization methods, modified on a new stream. The two kernels will run concurrently on the same tensor, which might cause the second kernel to read uninitialized data before the first one was able to write it, or the first kernel might overwrite part of the result of the second. When this script is run on the commandline with: TORCH_CUDA_SANITIZER=1 python example_error.py\n the following output is printed by CSAN: ============================\nCSAN detected a possible data race on tensor with data pointer 139719969079296\nAccess by stream 94646435460352 during kernel:\naten::mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)\nwriting to argument(s) self, out, and to the output\nWith stack trace:\n  File \"example_error.py\", line 6, in <module>\n    torch.mul(a, 5, out=a)\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n    stack_trace = traceback.StackSummary.extract(\n\nPrevious access by stream 0 during kernel:\naten::rand(int[] size, *, int? dtype=None, Device? device=None) -> Tensor\nwriting to the output\nWith stack trace:\n  File \"example_error.py\", line 3, in <module>\n    a = torch.rand(10000, device=\"cuda\")\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 364, in _handle_kernel_launch\n    stack_trace = traceback.StackSummary.extract(\n\nTensor was allocated with stack trace:\n  File \"example_error.py\", line 3, in <module>\n    a = torch.rand(10000, device=\"cuda\")\n  ...\n  File \"pytorch/torch/cuda/_sanitizer.py\", line 420, in _handle_memory_allocation\n    traceback.StackSummary.extract(\n This gives extensive insight into the origin of the error:  A tensor was incorrectly accessed from streams with ids: 0 (default stream) and 94646435460352 (new stream) The tensor was allocated by invoking a = torch.rand(10000, device=\"cuda\")\n \n The faulty accesses were caused by operators\n\n \na = torch.rand(10000, device=\"cuda\") on stream 0 \ntorch.mul(a, 5, out=a) on stream 94646435460352     \nThe error message also displays the schemas of the invoked operators, along with a note showing which arguments of the operators correspond to the affected tensor.  In the example, it can be seen that tensor a corresponds to arguments self, out and the output value of the invoked operator torch.mul.     See also The list of supported torch operators and their schemas can be viewed here.  The bug can be fixed by forcing the new stream to wait for the default stream: with torch.cuda.stream(torch.cuda.Stream()):\n    torch.cuda.current_stream().wait_stream(torch.cuda.default_stream())\n    torch.mul(a, 5, out=a)\n When the script is run again, there are no errors reported. API Reference  \ntorch.cuda._sanitizer.enable_cuda_sanitizer() [source]\n \nEnables CUDA Sanitizer. The sanitizer will begin to analyze low-level CUDA calls invoked by torch functions for synchronization errors. All data races found will be printed to the standard error output along with stack traces of suspected causes. For best results, the sanitizer should be enabled at the very beginning of the program. \n\n"}, {"name": "torch.cuda.CUDAGraph", "path": "generated/torch.cuda.cudagraph", "type": "CUDA", "text": "CUDAGraph  \nclass torch.cuda.CUDAGraph [source]\n \nWrapper around a CUDA graph.  Warning This API is in beta and may change in future releases.   \ncapture_begin(pool=None, capture_error_mode='global') [source]\n \nBegins capturing CUDA work on the current stream. Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally.  Parameters \n \npool (optional) \u2013 Token (returned by graph_pool_handle() or other_Graph_instance.pool()) that hints this graph may share memory with the indicated pool. See Graph memory management. \ncapture_error_mode (str, optional) \u2013 specifies the cudaStreamCaptureMode for the graph capture stream. Can be \u201cglobal\u201d, \u201cthread_local\u201d or \u201crelaxed\u201d. During cuda graph capture, some actions, such as cudaMalloc, may be unsafe. \u201cglobal\u201d will error on actions in other threads, \u201cthread_local\u201d will only error for actions in the current thread, and \u201crelaxed\u201d will not error on these actions. Do NOT change this setting unless you\u2019re familiar with cudaStreamCaptureMode\n    \n  \ncapture_end() [source]\n \nEnds CUDA graph capture on the current stream. After capture_end, replay may be called on this instance. Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally. \n  \ndebug_dump(debug_path) [source]\n \n Parameters \ndebug_path (required) \u2013 Path to dump the graph to.   Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode() \n  \nenable_debug_mode() [source]\n \nEnables debugging mode for CUDAGraph.debug_dump. \n  \npool() [source]\n \nReturns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool. \n  \nreplay() [source]\n \nReplays the CUDA work captured by this graph. \n  \nreset() [source]\n \nDeletes the graph currently held by this instance. \n \n\n"}, {"name": "torch.cuda.CUDAGraph.capture_begin()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_begin", "type": "CUDA", "text": " \ncapture_begin(pool=None, capture_error_mode='global') [source]\n \nBegins capturing CUDA work on the current stream. Typically, you shouldn\u2019t call capture_begin yourself. Use graph or make_graphed_callables(), which call capture_begin internally.  Parameters \n \npool (optional) \u2013 Token (returned by graph_pool_handle() or other_Graph_instance.pool()) that hints this graph may share memory with the indicated pool. See Graph memory management. \ncapture_error_mode (str, optional) \u2013 specifies the cudaStreamCaptureMode for the graph capture stream. Can be \u201cglobal\u201d, \u201cthread_local\u201d or \u201crelaxed\u201d. During cuda graph capture, some actions, such as cudaMalloc, may be unsafe. \u201cglobal\u201d will error on actions in other threads, \u201cthread_local\u201d will only error for actions in the current thread, and \u201crelaxed\u201d will not error on these actions. Do NOT change this setting unless you\u2019re familiar with cudaStreamCaptureMode\n    \n"}, {"name": "torch.cuda.CUDAGraph.capture_end()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.capture_end", "type": "CUDA", "text": " \ncapture_end() [source]\n \nEnds CUDA graph capture on the current stream. After capture_end, replay may be called on this instance. Typically, you shouldn\u2019t call capture_end yourself. Use graph or make_graphed_callables(), which call capture_end internally. \n"}, {"name": "torch.cuda.CUDAGraph.debug_dump()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.debug_dump", "type": "CUDA", "text": " \ndebug_dump(debug_path) [source]\n \n Parameters \ndebug_path (required) \u2013 Path to dump the graph to.   Calls a debugging function to dump the graph if the debugging is enabled via CUDAGraph.enable_debug_mode() \n"}, {"name": "torch.cuda.CUDAGraph.enable_debug_mode()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.enable_debug_mode", "type": "CUDA", "text": " \nenable_debug_mode() [source]\n \nEnables debugging mode for CUDAGraph.debug_dump. \n"}, {"name": "torch.cuda.CUDAGraph.pool()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.pool", "type": "CUDA", "text": " \npool() [source]\n \nReturns an opaque token representing the id of this graph\u2019s memory pool. This id can optionally be passed to another graph\u2019s capture_begin, which hints the other graph may share the same memory pool. \n"}, {"name": "torch.cuda.CUDAGraph.replay()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.replay", "type": "CUDA", "text": " \nreplay() [source]\n \nReplays the CUDA work captured by this graph. \n"}, {"name": "torch.cuda.CUDAGraph.reset()", "path": "generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph.reset", "type": "CUDA", "text": " \nreset() [source]\n \nDeletes the graph currently held by this instance. \n"}, {"name": "torch.cuda.CUDAPluggableAllocator", "path": "generated/torch.cuda.cudapluggableallocator", "type": "CUDA", "text": "CUDAPluggableAllocator  \nclass torch.cuda.CUDAPluggableAllocator(path_to_so_file, alloc_fn_name, free_fn_name) [source]\n \nCUDA memory allocator loaded from a so file. Memory allocators are compiled in .so files and loaded dynamically using ctypes. To change the active allocator use the torch.memory.cuda.change_current_allocator() function.  Parameters \n \npath_to_so_file (str) \u2013 Path in the filesystem to the .so file containing the allocator functions \nalloc_fn_name (str) \u2013 Name of the function to perform the memory allocation in the so file. The signature must be: void* alloc_fn_name(ssize_t size, int device, cudaStream_t stream); \nfree_fn_name (str) \u2013 Name of the function to perform the memory release in the so file. The signature must be: void free_fn_name(void* ptr, size_t size, cudaStream_t stream);     Warning This is currently supported only in unix OSs   Note See Memory management for details on creating and using a custom allocator  \n\n"}, {"name": "torch.cuda.current_blas_handle()", "path": "generated/torch.cuda.current_blas_handle#torch.cuda.current_blas_handle", "type": "CUDA", "text": " \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n"}, {"name": "torch.cuda.current_device()", "path": "generated/torch.cuda.current_device#torch.cuda.current_device", "type": "CUDA", "text": " \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device.  Return type \nint   \n"}, {"name": "torch.cuda.current_stream()", "path": "generated/torch.cuda.current_stream#torch.cuda.current_stream", "type": "CUDA", "text": " \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).  Return type \nStream   \n"}, {"name": "torch.cuda.default_stream()", "path": "generated/torch.cuda.default_stream#torch.cuda.default_stream", "type": "CUDA", "text": " \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).  Return type \nStream   \n"}, {"name": "torch.cuda.device", "path": "generated/torch.cuda.device", "type": "CUDA", "text": "device  \nclass torch.cuda.device(device) [source]\n \nContext-manager that changes the selected device.  Parameters \ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   \n\n"}, {"name": "torch.cuda.device_count()", "path": "generated/torch.cuda.device_count#torch.cuda.device_count", "type": "CUDA", "text": " \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available.  Return type \nint   \n"}, {"name": "torch.cuda.device_of", "path": "generated/torch.cuda.device_of", "type": "CUDA", "text": "device_of  \nclass torch.cuda.device_of(obj) [source]\n \nContext-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters \nobj (Tensor or Storage) \u2013 object allocated on the selected device.   \n\n"}, {"name": "torch.cuda.empty_cache()", "path": "generated/torch.cuda.empty_cache#torch.cuda.empty_cache", "type": "CUDA", "text": " \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.Event", "path": "generated/torch.cuda.event", "type": "CUDA", "text": "Event  \nclass torch.cuda.Event(enable_timing=False, blocking=False, interprocess=False) [source]\n \nWrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters \n \nenable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) \nblocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) \ninterprocess (bool) \u2013 if True, the event can be shared between processes (default: False)     \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n  \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n  \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n  \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n  \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n  \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n  \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified.  Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info.  \n \n\n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "generated/torch.cuda.event#torch.cuda.Event.elapsed_time", "type": "CUDA", "text": " \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.from_ipc_handle", "type": "CUDA", "text": " \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "generated/torch.cuda.event#torch.cuda.Event.ipc_handle", "type": "CUDA", "text": " \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n"}, {"name": "torch.cuda.Event.query()", "path": "generated/torch.cuda.event#torch.cuda.Event.query", "type": "CUDA", "text": " \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n"}, {"name": "torch.cuda.Event.record()", "path": "generated/torch.cuda.event#torch.cuda.Event.record", "type": "CUDA", "text": " \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n"}, {"name": "torch.cuda.Event.synchronize()", "path": "generated/torch.cuda.event#torch.cuda.Event.synchronize", "type": "CUDA", "text": " \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n"}, {"name": "torch.cuda.Event.wait()", "path": "generated/torch.cuda.event#torch.cuda.Event.wait", "type": "CUDA", "text": " \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified.  Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Event documentation for more info.  \n"}, {"name": "torch.cuda.ExternalStream", "path": "generated/torch.cuda.externalstream", "type": "CUDA", "text": "ExternalStream  \nclass torch.cuda.ExternalStream(stream_ptr, device=None, **kwargs) [source]\n \nWrapper around an externally allocated CUDA stream. This class is used to wrap streams allocated in other libraries in order to facilitate data exchange and multi-library interactions.  Note This class doesn\u2019t manage the stream life-cycle, it is the user responsibility to keep the referenced stream alive while this class is being used.   Parameters \n \nstream_ptr (int) \u2013 Integer representation of the cudaStream_t value. allocated externally. \ndevice (torch.device or int, optional) \u2013 the device where the stream was originally allocated. if device is specified incorrectly, subsequent launches using this stream may fail.     \nquery()  \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n  \nrecord_event(event=None)  \nRecords an event.  Parameters \nevent (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n  \nsynchronize()  \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n  \nwait_event(event)  \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (torch.cuda.Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n  \nwait_stream(stream)  \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n \n\n"}, {"name": "torch.cuda.ExternalStream.query()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.query", "type": "CUDA", "text": " \nquery()  \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n"}, {"name": "torch.cuda.ExternalStream.record_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.record_event", "type": "CUDA", "text": " \nrecord_event(event=None)  \nRecords an event.  Parameters \nevent (torch.cuda.Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n"}, {"name": "torch.cuda.ExternalStream.synchronize()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.synchronize", "type": "CUDA", "text": " \nsynchronize()  \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n"}, {"name": "torch.cuda.ExternalStream.wait_event()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_event", "type": "CUDA", "text": " \nwait_event(event)  \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (torch.cuda.Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n"}, {"name": "torch.cuda.ExternalStream.wait_stream()", "path": "generated/torch.cuda.externalstream#torch.cuda.ExternalStream.wait_stream", "type": "CUDA", "text": " \nwait_stream(stream)  \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n"}, {"name": "torch.cuda.get_allocator_backend()", "path": "generated/torch.cuda.get_allocator_backend#torch.cuda.get_allocator_backend", "type": "CUDA", "text": " \ntorch.cuda.get_allocator_backend() [source]\n \nReturns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).  Note See Memory management for details on choosing the allocator backend.   Return type \nstr   \n"}, {"name": "torch.cuda.get_arch_list()", "path": "generated/torch.cuda.get_arch_list#torch.cuda.get_arch_list", "type": "CUDA", "text": " \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for.  Return type \nList[str]   \n"}, {"name": "torch.cuda.get_device_capability()", "path": "generated/torch.cuda.get_device_capability#torch.cuda.get_device_capability", "type": "CUDA", "text": " \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n"}, {"name": "torch.cuda.get_device_name()", "path": "generated/torch.cuda.get_device_name#torch.cuda.get_device_name", "type": "CUDA", "text": " \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n"}, {"name": "torch.cuda.get_device_properties()", "path": "generated/torch.cuda.get_device_properties#torch.cuda.get_device_properties", "type": "CUDA", "text": " \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "generated/torch.cuda.get_gencode_flags#torch.cuda.get_gencode_flags", "type": "CUDA", "text": " \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library was compiled with.  Return type \nstr   \n"}, {"name": "torch.cuda.get_rng_state()", "path": "generated/torch.cuda.get_rng_state#torch.cuda.get_rng_state", "type": "CUDA", "text": " \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).  Return type \nTensor    Warning This function eagerly initializes CUDA.  \n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "generated/torch.cuda.get_rng_state_all#torch.cuda.get_rng_state_all", "type": "CUDA", "text": " \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices.  Return type \nList[Tensor]   \n"}, {"name": "torch.cuda.get_sync_debug_mode()", "path": "generated/torch.cuda.get_sync_debug_mode#torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": " \ntorch.cuda.get_sync_debug_mode() [source]\n \nReturns current value of debug mode for cuda synchronizing operations.  Return type \nint   \n"}, {"name": "torch.cuda.graph", "path": "generated/torch.cuda.graph", "type": "CUDA", "text": "graph  \nclass torch.cuda.graph(cuda_graph, pool=None, stream=None, capture_error_mode='global') [source]\n \nContext-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay. See CUDA Graphs for a general introduction, detailed use, and constraints.  Parameters \n \ncuda_graph (torch.cuda.CUDAGraph) \u2013 Graph object used for capture. \npool (optional) \u2013 Opaque token (returned by a call to graph_pool_handle() or other_Graph_instance.pool()) hinting this graph\u2019s capture may share memory from the specified pool. See Graph memory management. \nstream (torch.cuda.Stream, optional) \u2013 If supplied, will be set as the current stream in the context. If not supplied, graph sets its own internal side stream as the current stream in the context. \ncapture_error_mode (str, optional) \u2013 specifies the cudaStreamCaptureMode for the graph capture stream. Can be \u201cglobal\u201d, \u201cthread_local\u201d or \u201crelaxed\u201d. During cuda graph capture, some actions, such as cudaMalloc, may be unsafe. \u201cglobal\u201d will error on actions in other threads, \u201cthread_local\u201d will only error for actions in the current thread, and \u201crelaxed\u201d will not error on actions. Do NOT change this setting unless you\u2019re familiar with cudaStreamCaptureMode\n     Note For effective memory sharing, if you pass a pool used by a previous capture and the previous capture used an explicit stream argument, you should pass the same stream argument to this capture.   Warning This API is in beta and may change in future releases.  \n\n"}, {"name": "torch.cuda.graph_pool_handle()", "path": "generated/torch.cuda.graph_pool_handle#torch.cuda.graph_pool_handle", "type": "CUDA", "text": " \ntorch.cuda.graph_pool_handle() [source]\n \nReturns an opaque token representing the id of a graph memory pool. See Graph memory management.  Warning This API is in beta and may change in future releases.  \n"}, {"name": "torch.cuda.init()", "path": "generated/torch.cuda.init#torch.cuda.init", "type": "CUDA", "text": " \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n"}, {"name": "torch.cuda.initial_seed()", "path": "generated/torch.cuda.initial_seed#torch.cuda.initial_seed", "type": "CUDA", "text": " \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.   Return type \nint   \n"}, {"name": "torch.cuda.ipc_collect()", "path": "generated/torch.cuda.ipc_collect#torch.cuda.ipc_collect", "type": "CUDA", "text": " \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n"}, {"name": "torch.cuda.is_available()", "path": "generated/torch.cuda.is_available#torch.cuda.is_available", "type": "CUDA", "text": " \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available.  Return type \nbool   \n"}, {"name": "torch.cuda.is_current_stream_capturing()", "path": "generated/torch.cuda.is_current_stream_capturing#torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": " \ntorch.cuda.is_current_stream_capturing() [source]\n \nReturns True if CUDA graph capture is underway on the current CUDA stream, False otherwise. If a CUDA context does not exist on the current device, returns False without initializing the context. \n"}, {"name": "torch.cuda.is_initialized()", "path": "generated/torch.cuda.is_initialized#torch.cuda.is_initialized", "type": "CUDA", "text": " \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n"}, {"name": "torch.cuda.jiterator._create_jit_fn()", "path": "generated/torch.cuda.jiterator._create_jit_fn#torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": " \ntorch.cuda.jiterator._create_jit_fn(code_string, **kwargs) [source]\n \nCreate a jiterator-generated cuda kernel for an elementwise op. The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir. Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.  Parameters \n \ncode_string (str) \u2013 CUDA code string to be compiled by jiterator. The entry functor must return by value. \nkwargs (Dict, optional) \u2013 Keyword arguments for generated function   Return type \nCallable   Example: code_string = \"template <typename T> T my_kernel(T x, T y, T alpha) { return -x + alpha * y; }\"\njitted_fn = create_jit_fn(code_string, alpha=1.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b, alpha=3.14)\n code_string also allows multiple function definitions, and the last function will be treated as the entry function. Example: code_string = \"template <typename T> T util_fn(T x, T y) { return ::sin(x) + ::cos(y); }\"\ncode_string += \"template <typename T> T my_kernel(T x, T y, T val) { return ::min(val, util_fn(x, y)); }\"\njitted_fn = create_jit_fn(code_string, val=0.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b)  # using default val=0.0\n Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu. Example: code_string = \"template <typename T> T my_gelu(T a) { return a > 0 ? a : 0; }\"\nmy_gelu = create_jit_fn(code_string)\nmy_lib = torch.library.Library(\"aten\", \"IMPL\")\nmy_lib.impl('aten::gelu', my_gelu, \"CUDA\")\n# torch.nn.GELU and torch.nn.function.gelu are now overridden\na = torch.rand(3, device='cuda')\ntorch.allclose(torch.nn.functional.gelu(a), torch.nn.functional.relu(a))\n  Warning This API is in beta and may change in future releases.   Warning This API only supports up to 8 inputs and 1 output   Warning All input tensors must live in CUDA device  \n"}, {"name": "torch.cuda.jiterator._create_multi_output_jit_fn()", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn#torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": " \ntorch.cuda.jiterator._create_multi_output_jit_fn(code_string, num_outputs, **kwargs) [source]\n \nCreate a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.  Parameters \n \ncode_string (str) \u2013 CUDA code string to be compiled by jiterator. The entry functor must return value by reference. \nnum_outputs (int) \u2013 number of outputs return by the kernel \nkwargs (Dict, optional) \u2013 Keyword arguments for generated function   Return type \nCallable   Example: code_string = \"template <typename T> void my_kernel(T x, T y, T alpha, T& out) { out = -x + alpha * y; }\"\njitted_fn = create_jit_fn(code_string, alpha=1.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b, alpha=3.14)\n  Warning This API is in beta and may change in future releases.   Warning This API only supports up to 8 inputs and 8 outputs  \n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "generated/torch.cuda.list_gpu_processes#torch.cuda.list_gpu_processes", "type": "CUDA", "text": " \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).  Return type \nstr   \n"}, {"name": "torch.cuda.make_graphed_callables()", "path": "generated/torch.cuda.make_graphed_callables#torch.cuda.make_graphed_callables", "type": "CUDA", "text": " \ntorch.cuda.make_graphed_callables(callables, sample_args, num_warmup_iters=3, allow_unused_input=False) [source]\n \nAccepts callables (functions or nn.Modules) and returns graphed versions. Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node. The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph. Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop. See Partial-network capture for detailed use and constraints. If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.  Parameters \n \ncallables (torch.nn.Module or Python function, or tuple of these) \u2013 Callable or callables to graph. See Graph memory management for when passing a tuple of callables is appropriate. If you pass a tuple of callables, their order in the tuple must be the same order they\u2019ll run in the live workload. \nsample_args (tuple of Tensors, or tuple of tuples of Tensors) \u2013 Samples args for each callable. If a single callable was passed, sample_args must be a single tuple of argument Tensors. If a tuple of callables was passed, sample_args must be tuple of tuples of argument Tensors. \nnum_warmup_iters (int) \u2013 The number of warmup iterations. Currently, DataDistributedParallel needs 11 iterations for warm up. Default: 3. \nallow_unused_input (bool) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.     Note The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.   Warning This API is in beta and may change in future releases.   Warning sample_args for each callable must contain only Tensors. Other types are not allowed.   Warning Returned callables do not support higher order differentiation (e.g., double backward).   Warning In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.   Warning After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.   Warning torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.   Warning When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.   Warning The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False.  \n"}, {"name": "torch.cuda.manual_seed()", "path": "generated/torch.cuda.manual_seed#torch.cuda.manual_seed", "type": "CUDA", "text": " \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n"}, {"name": "torch.cuda.manual_seed_all()", "path": "generated/torch.cuda.manual_seed_all#torch.cuda.manual_seed_all", "type": "CUDA", "text": " \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "generated/torch.cuda.max_memory_allocated#torch.cuda.max_memory_allocated", "type": "CUDA", "text": " \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.max_memory_cached()", "path": "generated/torch.cuda.max_memory_cached#torch.cuda.max_memory_cached", "type": "CUDA", "text": " \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved().  Return type \nint   \n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved", "type": "CUDA", "text": " \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.mem_get_info()", "path": "generated/torch.cuda.mem_get_info#torch.cuda.mem_get_info", "type": "CUDA", "text": " \ntorch.cuda.mem_get_info(device=None) [source]\n \nReturns the global free and total GPU memory for a given device using cudaMemGetInfo.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nTuple[int, int]    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory._dump_snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._dump_snapshot", "type": "Miscellaneous", "text": " \ntorch.cuda.memory._dump_snapshot(filename='dump_snapshot.pickle') [source]\n \nSaves a pickled version of the torch.memory._snapshot() dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz  Parameters \nfilename (str, optional) \u2013 Name of the file to create. Defaults to \u201cdump_snapshot.pickle\u201d.   \n"}, {"name": "torch.cuda.memory._record_memory_history()", "path": "torch_cuda_memory#torch.cuda.memory._record_memory_history", "type": "Miscellaneous", "text": " \ntorch.cuda.memory._record_memory_history(enabled='all', context='all', stacks='all', max_entries=9223372036854775807, device=None) [source]\n \nEnables recording of stack traces associated with memory allocations, so you can tell what allocated any piece of memory in torch.cuda.memory._snapshot(). In addition too keeping stack traces with each current allocation and free, this will also enable recording of a history of all alloc/free events. Use torch.cuda.memory._snapshot() to retrieve this information, and the tools in _memory_viz.py to visualize snapshots. The Python trace collection is fast (2us per trace), so you may consider enabling this on production jobs if you anticipate ever having to debug memory issues. C++ trace collection is also fast (~50ns/frame), which for many typical programs works out to ~2us per trace, but can vary depending on stack depth.  Parameters \n \nenabled (Literal[None, \"state\", \"all\"], optional) \u2013 None, disable recording memory history. \u201cstate\u201d, keep information for currenly allocated memory. \u201call\u201d, additionally keep a history of all alloc/free calls. Defaults to \u201call\u201d. \ncontext (Literal[None, \"state\", \"alloc\", \"all\"], optional) \u2013 None, Do not record any tracebacks. \u201cstate\u201d, Record tracebacks for currently allocated memory. \u201calloc\u201d, additionally keep tracebacks for alloc calls. \u201call\u201d, additionally keep tracebacks for free calls. Defaults to \u201call\u201d. \nstacks (Literal[\"python\", \"all\"], optional) \u2013 \u201cpython\u201d, include Python, TorchScript, and inductor frames in tracebacks \u201call\u201d, additionally include C++ frames Defaults to \u201call\u201d. \nmax_entries (int, optional) \u2013 Keep a maximum of max_entries alloc/free events in the recorded history recorded.    \n"}, {"name": "torch.cuda.memory._snapshot()", "path": "torch_cuda_memory#torch.cuda.memory._snapshot", "type": "Miscellaneous", "text": " \ntorch.cuda.memory._snapshot(device=None) [source]\n \nSaves a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure. class Snapshot(TypedDict):\n    segments : List[Segment]\n    device_traces: List[List[TraceEntry]]\n\nclass Segment(TypedDict):\n    # Segments are memory returned from a cudaMalloc call.\n    # The size of reserved memory is the sum of all Segments.\n    # Segments are cached and reused for future allocations.\n    # If the reuse is smaller than the segment, the segment\n    # is split into more then one Block.\n    # empty_cache() frees Segments that are entirely inactive.\n    address: int\n    total_size: int #  cudaMalloc'd size of segment\n    stream: int\n    segment_type: Literal['small', 'large'] # 'large' (>1MB)\n    allocated_size: int # size of memory in use\n    active_size: int # size of memory in use or in active_awaiting_free state\n    blocks : List[Block]\n\nclass Block(TypedDict):\n    # A piece of memory returned from the allocator, or\n    # current cached but inactive.\n    size: int\n    requested_size: int # size requested during malloc, may be smaller than\n                        # size due to rounding\n    address: int\n    state: Literal['active_allocated', # used by a tensor\n                'active_awaiting_free', # waiting for another stream to finish using\n                                        # this, then it will become free\n                'inactive',] # free for reuse\n    frames: List[Frame] # stack trace from where the allocation occurred\n\nclass Frame(TypedDict):\n        filename: str\n        line: int\n        name: str\n\nclass TraceEntry(TypedDict):\n    # When `torch.cuda.memory._record_memory_history()` is enabled,\n    # the snapshot will contain TraceEntry objects that record each\n    # action the allocator took.\n    action: Literal[\n    'alloc'  # memory allocated\n    'free_requested', # the allocated received a call to free memory\n    'free_completed', # the memory that was requested to be freed is now\n                    # able to be used in future allocation calls\n    'segment_alloc', # the caching allocator ask cudaMalloc for more memory\n                    # and added it as a segment in its cache\n    'segment_free',  # the caching allocator called cudaFree to return memory\n                    # to cuda possibly trying free up memory to\n                    # allocate more segments or because empty_caches was called\n    'oom',          # the allocator threw an OOM exception. 'size' is\n                    # the requested number of bytes that did not succeed\n    'snapshot'      # the allocator generated a memory snapshot\n                    # useful to coorelate a previously taken\n                    # snapshot with this trace\n    ]\n    addr: int # not present for OOM\n    frames: List[Frame]\n    size: int\n    stream: int\n    device_free: int # only present for OOM, the amount of\n                    # memory cuda still reports to be free\n  Returns \nThe Snapshot dictionary object   \n"}, {"name": "torch.cuda.memory_allocated()", "path": "generated/torch.cuda.memory_allocated#torch.cuda.memory_allocated", "type": "CUDA", "text": " \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_cached()", "path": "generated/torch.cuda.memory_cached#torch.cuda.memory_cached", "type": "CUDA", "text": " \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved().  Return type \nint   \n"}, {"name": "torch.cuda.memory_reserved()", "path": "generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved", "type": "CUDA", "text": " \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_snapshot()", "path": "generated/torch.cuda.memory_snapshot#torch.cuda.memory_snapshot", "type": "CUDA", "text": " \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_stats()", "path": "generated/torch.cuda.memory_stats#torch.cuda.memory_stats", "type": "CUDA", "text": " \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.  The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:  \n\"max_split_size\": blocks above this size will not be split. \n\"oversize_allocations.{current,peak,allocated,freed}\": number of over-size allocation requests received by the memory allocator. \n\"oversize_segments.{current,peak,allocated,freed}\": number of over-size reserved segments from cudaMalloc().  The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:  \n\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": memory requested by client code, compare this with allocated_bytes to check if allocation rounding adds too much overhead.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).  Return type \nDict[str, Any]    Note See Memory management for more details about GPU memory management.   Note With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero.  \n"}, {"name": "torch.cuda.memory_summary()", "path": "generated/torch.cuda.memory_summary#torch.cuda.memory_summary", "type": "CUDA", "text": " \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).   Return type \nstr    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_usage()", "path": "generated/torch.cuda.memory_usage#torch.cuda.memory_usage", "type": "CUDA", "text": " \ntorch.cuda.memory_usage(device=None) [source]\n \nReturns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n"}, {"name": "torch.cuda.nvtx.mark()", "path": "generated/torch.cuda.nvtx.mark#torch.cuda.nvtx.mark", "type": "CUDA", "text": " \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (str) \u2013 ASCII message to associate with the event.   \n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "generated/torch.cuda.nvtx.range_pop#torch.cuda.nvtx.range_pop", "type": "CUDA", "text": " \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "generated/torch.cuda.nvtx.range_push#torch.cuda.nvtx.range_push", "type": "CUDA", "text": " \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (str) \u2013 ASCII message to associate with range   \n"}, {"name": "torch.cuda.power_draw()", "path": "generated/torch.cuda.power_draw#torch.cuda.power_draw", "type": "CUDA", "text": " \ntorch.cuda.power_draw(device=None) [source]\n \n Returns the average power draw of the GPU sensor in mW (MilliWatts)\n\nover the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.    Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "generated/torch.cuda.reset_max_memory_allocated#torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": " \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "generated/torch.cuda.reset_max_memory_cached#torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": " \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.reset_peak_memory_stats()", "path": "generated/torch.cuda.reset_peak_memory_stats#torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": " \ntorch.cuda.reset_peak_memory_stats(device=None) [source]\n \nResets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.seed()", "path": "generated/torch.cuda.seed#torch.cuda.seed", "type": "CUDA", "text": " \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n"}, {"name": "torch.cuda.seed_all()", "path": "generated/torch.cuda.seed_all#torch.cuda.seed_all", "type": "CUDA", "text": " \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n"}, {"name": "torch.cuda.set_device()", "path": "generated/torch.cuda.set_device#torch.cuda.set_device", "type": "CUDA", "text": " \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "generated/torch.cuda.set_per_process_memory_fraction#torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": " \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n"}, {"name": "torch.cuda.set_rng_state()", "path": "generated/torch.cuda.set_rng_state#torch.cuda.set_rng_state", "type": "CUDA", "text": " \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "generated/torch.cuda.set_rng_state_all#torch.cuda.set_rng_state_all", "type": "CUDA", "text": " \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n"}, {"name": "torch.cuda.set_stream()", "path": "generated/torch.cuda.set_stream#torch.cuda.set_stream", "type": "CUDA", "text": " \ntorch.cuda.set_stream(stream) [source]\n \n Sets the current stream.This is a wrapper API to set the stream.\n\nUsage of this function is discouraged in favor of the stream context manager.    Parameters \nstream (Stream) \u2013 selected stream. This function is a no-op if this argument is None.   \n"}, {"name": "torch.cuda.set_sync_debug_mode()", "path": "generated/torch.cuda.set_sync_debug_mode#torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": " \ntorch.cuda.set_sync_debug_mode(debug_mode) [source]\n \nSets the debug mode for cuda synchronizing operations.  Parameters \ndebug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.    Warning This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet.  \n"}, {"name": "torch.cuda.stream()", "path": "generated/torch.cuda.stream#torch.cuda.stream", "type": "CUDA", "text": " \ntorch.cuda.stream(stream) [source]\n \nWrapper around the Context-manager StreamContext that selects a given stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.  Return type \nStreamContext   ..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream. \n"}, {"name": "torch.cuda.StreamContext", "path": "generated/torch.cuda.streamcontext", "type": "CUDA", "text": "StreamContext  \nclass torch.cuda.StreamContext(stream) [source]\n \nContext-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters \nStream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device.  \n\n"}, {"name": "torch.cuda.synchronize()", "path": "generated/torch.cuda.synchronize#torch.cuda.synchronize", "type": "CUDA", "text": " \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.temperature()", "path": "generated/torch.cuda.temperature#torch.cuda.temperature", "type": "CUDA", "text": " \ntorch.cuda.temperature(device=None) [source]\n \n Returns the average temperature of the GPU sensor in Degrees C (Centigrades)\n\nover the past sample period as given by nvidia-smi.    Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n"}, {"name": "torch.cuda.torch.cuda.caching_allocator_alloc", "path": "generated/torch.cuda.caching_allocator_alloc", "type": "CUDA", "text": "torch.cuda.caching_allocator_alloc  \ntorch.cuda.caching_allocator_alloc(size, device=None, stream=None) [source]\n \nPerforms a memory allocation using the CUDA memory allocator. Memory is allocated for a given device and a stream, this function is intended to be used for interoperability with other frameworks. Allocated memory is released through caching_allocator_delete().  Parameters \n \nsize (int) \u2013 number of bytes to be allocated. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used. \nstream (torch.cuda.Stream or int, optional) \u2013 selected stream. If is None then the default stream for the selected device is used.     Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.caching_allocator_delete", "path": "generated/torch.cuda.caching_allocator_delete", "type": "CUDA", "text": "torch.cuda.caching_allocator_delete  \ntorch.cuda.caching_allocator_delete(mem_ptr) [source]\n \nDeletes memory allocated using the CUDA memory allocator. Memory allocated with caching_allocator_alloc(). is freed here. The associated device and stream are tracked inside the allocator.  Parameters \nmem_ptr (int) \u2013 memory address to be freed by the allocator.    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.can_device_access_peer", "path": "generated/torch.cuda.can_device_access_peer", "type": "CUDA", "text": "torch.cuda.can_device_access_peer  \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible.  Return type \nbool   \n\n"}, {"name": "torch.cuda.torch.cuda.change_current_allocator", "path": "generated/torch.cuda.change_current_allocator", "type": "CUDA", "text": "torch.cuda.change_current_allocator  \ntorch.cuda.change_current_allocator(allocator) [source]\n \nChanges the currently used memory allocator to be the one provided. If the current allocator has already been used/initialized, this function will error.  Parameters \nallocator (torch.cuda.memory._CUDAAllocator) \u2013 allocator to be set as the active one.    Note See Memory management for details on creating and using a custom allocator  \n\n"}, {"name": "torch.cuda.torch.cuda.clock_rate", "path": "generated/torch.cuda.clock_rate", "type": "CUDA", "text": "torch.cuda.clock_rate  \ntorch.cuda.clock_rate(device=None) [source]\n \nReturns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n\n"}, {"name": "torch.cuda.torch.cuda.comm.broadcast", "path": "generated/torch.cuda.comm.broadcast", "type": "CUDA", "text": "torch.cuda.comm.broadcast  \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n\n"}, {"name": "torch.cuda.torch.cuda.comm.broadcast_coalesced", "path": "generated/torch.cuda.comm.broadcast_coalesced", "type": "CUDA", "text": "torch.cuda.comm.broadcast_coalesced  \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n\n"}, {"name": "torch.cuda.torch.cuda.comm.gather", "path": "generated/torch.cuda.comm.gather", "type": "CUDA", "text": "torch.cuda.comm.gather  \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n\n"}, {"name": "torch.cuda.torch.cuda.comm.reduce_add", "path": "generated/torch.cuda.comm.reduce_add", "type": "CUDA", "text": "torch.cuda.comm.reduce_add  \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n\n"}, {"name": "torch.cuda.torch.cuda.comm.scatter", "path": "generated/torch.cuda.comm.scatter", "type": "CUDA", "text": "torch.cuda.comm.scatter  \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[torch.cuda.Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n\n"}, {"name": "torch.cuda.torch.cuda.current_blas_handle", "path": "generated/torch.cuda.current_blas_handle", "type": "CUDA", "text": "torch.cuda.current_blas_handle  \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n\n"}, {"name": "torch.cuda.torch.cuda.current_device", "path": "generated/torch.cuda.current_device", "type": "CUDA", "text": "torch.cuda.current_device  \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device.  Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.current_stream", "path": "generated/torch.cuda.current_stream", "type": "CUDA", "text": "torch.cuda.current_stream  \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).  Return type \nStream   \n\n"}, {"name": "torch.cuda.torch.cuda.default_stream", "path": "generated/torch.cuda.default_stream", "type": "CUDA", "text": "torch.cuda.default_stream  \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).  Return type \nStream   \n\n"}, {"name": "torch.cuda.torch.cuda.device_count", "path": "generated/torch.cuda.device_count", "type": "CUDA", "text": "torch.cuda.device_count  \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available.  Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.empty_cache", "path": "generated/torch.cuda.empty_cache", "type": "CUDA", "text": "torch.cuda.empty_cache  \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.get_allocator_backend", "path": "generated/torch.cuda.get_allocator_backend", "type": "CUDA", "text": "torch.cuda.get_allocator_backend  \ntorch.cuda.get_allocator_backend() [source]\n \nReturns a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF. Currently available backends are native (PyTorch\u2019s native caching allocator) and cudaMallocAsync` (CUDA\u2019s built-in asynchronous allocator).  Note See Memory management for details on choosing the allocator backend.   Return type \nstr   \n\n"}, {"name": "torch.cuda.torch.cuda.get_arch_list", "path": "generated/torch.cuda.get_arch_list", "type": "CUDA", "text": "torch.cuda.get_arch_list  \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for.  Return type \nList[str]   \n\n"}, {"name": "torch.cuda.torch.cuda.get_device_capability", "path": "generated/torch.cuda.get_device_capability", "type": "CUDA", "text": "torch.cuda.get_device_capability  \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n\n"}, {"name": "torch.cuda.torch.cuda.get_device_name", "path": "generated/torch.cuda.get_device_name", "type": "CUDA", "text": "torch.cuda.get_device_name  \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n\n"}, {"name": "torch.cuda.torch.cuda.get_device_properties", "path": "generated/torch.cuda.get_device_properties", "type": "CUDA", "text": "torch.cuda.get_device_properties  \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n\n"}, {"name": "torch.cuda.torch.cuda.get_gencode_flags", "path": "generated/torch.cuda.get_gencode_flags", "type": "CUDA", "text": "torch.cuda.get_gencode_flags  \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library was compiled with.  Return type \nstr   \n\n"}, {"name": "torch.cuda.torch.cuda.get_rng_state", "path": "generated/torch.cuda.get_rng_state", "type": "CUDA", "text": "torch.cuda.get_rng_state  \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).  Return type \nTensor    Warning This function eagerly initializes CUDA.  \n\n"}, {"name": "torch.cuda.torch.cuda.get_rng_state_all", "path": "generated/torch.cuda.get_rng_state_all", "type": "CUDA", "text": "torch.cuda.get_rng_state_all  \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices.  Return type \nList[Tensor]   \n\n"}, {"name": "torch.cuda.torch.cuda.get_sync_debug_mode", "path": "generated/torch.cuda.get_sync_debug_mode", "type": "CUDA", "text": "torch.cuda.get_sync_debug_mode  \ntorch.cuda.get_sync_debug_mode() [source]\n \nReturns current value of debug mode for cuda synchronizing operations.  Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.graph_pool_handle", "path": "generated/torch.cuda.graph_pool_handle", "type": "CUDA", "text": "torch.cuda.graph_pool_handle  \ntorch.cuda.graph_pool_handle() [source]\n \nReturns an opaque token representing the id of a graph memory pool. See Graph memory management.  Warning This API is in beta and may change in future releases.  \n\n"}, {"name": "torch.cuda.torch.cuda.init", "path": "generated/torch.cuda.init", "type": "CUDA", "text": "torch.cuda.init  \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n\n"}, {"name": "torch.cuda.torch.cuda.initial_seed", "path": "generated/torch.cuda.initial_seed", "type": "CUDA", "text": "torch.cuda.initial_seed  \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.   Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.ipc_collect", "path": "generated/torch.cuda.ipc_collect", "type": "CUDA", "text": "torch.cuda.ipc_collect  \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n\n"}, {"name": "torch.cuda.torch.cuda.is_available", "path": "generated/torch.cuda.is_available", "type": "CUDA", "text": "torch.cuda.is_available  \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available.  Return type \nbool   \n\n"}, {"name": "torch.cuda.torch.cuda.is_current_stream_capturing", "path": "generated/torch.cuda.is_current_stream_capturing", "type": "CUDA", "text": "torch.cuda.is_current_stream_capturing  \ntorch.cuda.is_current_stream_capturing() [source]\n \nReturns True if CUDA graph capture is underway on the current CUDA stream, False otherwise. If a CUDA context does not exist on the current device, returns False without initializing the context. \n\n"}, {"name": "torch.cuda.torch.cuda.is_initialized", "path": "generated/torch.cuda.is_initialized", "type": "CUDA", "text": "torch.cuda.is_initialized  \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n\n"}, {"name": "torch.cuda.torch.cuda.jiterator._create_jit_fn", "path": "generated/torch.cuda.jiterator._create_jit_fn", "type": "CUDA", "text": "torch.cuda.jiterator._create_jit_fn  \ntorch.cuda.jiterator._create_jit_fn(code_string, **kwargs) [source]\n \nCreate a jiterator-generated cuda kernel for an elementwise op. The code string has to be a valid CUDA function that describes the computation for a single element. The code string has to follow the c++ template pattern, as shown in the example below. This function will be inlined into elementwise kernel template, and compiled on the fly. Compiled kernel will be cached in memory, as well as local temp dir. Jiterator-generated kernels accepts noncontiguous tensors, and supports broadcasting and type promotion.  Parameters \n \ncode_string (str) \u2013 CUDA code string to be compiled by jiterator. The entry functor must return by value. \nkwargs (Dict, optional) \u2013 Keyword arguments for generated function   Return type \nCallable   Example: code_string = \"template <typename T> T my_kernel(T x, T y, T alpha) { return -x + alpha * y; }\"\njitted_fn = create_jit_fn(code_string, alpha=1.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b, alpha=3.14)\n code_string also allows multiple function definitions, and the last function will be treated as the entry function. Example: code_string = \"template <typename T> T util_fn(T x, T y) { return ::sin(x) + ::cos(y); }\"\ncode_string += \"template <typename T> T my_kernel(T x, T y, T val) { return ::min(val, util_fn(x, y)); }\"\njitted_fn = create_jit_fn(code_string, val=0.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b)  # using default val=0.0\n Jiterator can be used together with python registration to override an operator\u2019s cuda kernel. Following example is overriding gelu\u2019s cuda kernel with relu. Example: code_string = \"template <typename T> T my_gelu(T a) { return a > 0 ? a : 0; }\"\nmy_gelu = create_jit_fn(code_string)\nmy_lib = torch.library.Library(\"aten\", \"IMPL\")\nmy_lib.impl('aten::gelu', my_gelu, \"CUDA\")\n# torch.nn.GELU and torch.nn.function.gelu are now overridden\na = torch.rand(3, device='cuda')\ntorch.allclose(torch.nn.functional.gelu(a), torch.nn.functional.relu(a))\n  Warning This API is in beta and may change in future releases.   Warning This API only supports up to 8 inputs and 1 output   Warning All input tensors must live in CUDA device  \n\n"}, {"name": "torch.cuda.torch.cuda.jiterator._create_multi_output_jit_fn", "path": "generated/torch.cuda.jiterator._create_multi_output_jit_fn", "type": "CUDA", "text": "torch.cuda.jiterator._create_multi_output_jit_fn  \ntorch.cuda.jiterator._create_multi_output_jit_fn(code_string, num_outputs, **kwargs) [source]\n \nCreate a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.  Parameters \n \ncode_string (str) \u2013 CUDA code string to be compiled by jiterator. The entry functor must return value by reference. \nnum_outputs (int) \u2013 number of outputs return by the kernel \nkwargs (Dict, optional) \u2013 Keyword arguments for generated function   Return type \nCallable   Example: code_string = \"template <typename T> void my_kernel(T x, T y, T alpha, T& out) { out = -x + alpha * y; }\"\njitted_fn = create_jit_fn(code_string, alpha=1.0)\na = torch.rand(3, device='cuda')\nb = torch.rand(3, device='cuda')\n# invoke jitted function like a regular python function\nresult = jitted_fn(a, b, alpha=3.14)\n  Warning This API is in beta and may change in future releases.   Warning This API only supports up to 8 inputs and 8 outputs  \n\n"}, {"name": "torch.cuda.torch.cuda.list_gpu_processes", "path": "generated/torch.cuda.list_gpu_processes", "type": "CUDA", "text": "torch.cuda.list_gpu_processes  \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).  Return type \nstr   \n\n"}, {"name": "torch.cuda.torch.cuda.make_graphed_callables", "path": "generated/torch.cuda.make_graphed_callables", "type": "CUDA", "text": "torch.cuda.make_graphed_callables  \ntorch.cuda.make_graphed_callables(callables, sample_args, num_warmup_iters=3, allow_unused_input=False) [source]\n \nAccepts callables (functions or nn.Modules) and returns graphed versions. Each graphed callable\u2019s forward pass runs its source callable\u2019s forward CUDA work as a CUDA graph inside a single autograd node. The graphed callable\u2019s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable\u2019s backward work as a CUDA graph. Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop. See Partial-network capture for detailed use and constraints. If you pass a tuple of several callables, their captures will use the same memory pool. See Graph memory management for when this is appropriate.  Parameters \n \ncallables (torch.nn.Module or Python function, or tuple of these) \u2013 Callable or callables to graph. See Graph memory management for when passing a tuple of callables is appropriate. If you pass a tuple of callables, their order in the tuple must be the same order they\u2019ll run in the live workload. \nsample_args (tuple of Tensors, or tuple of tuples of Tensors) \u2013 Samples args for each callable. If a single callable was passed, sample_args must be a single tuple of argument Tensors. If a tuple of callables was passed, sample_args must be tuple of tuples of argument Tensors. \nnum_warmup_iters (int) \u2013 The number of warmup iterations. Currently, DataDistributedParallel needs 11 iterations for warm up. Default: 3. \nallow_unused_input (bool) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.     Note The requires_grad state of each Tensor in sample_args must match the state that\u2019s expected for the corresponding real input in the training loop.   Warning This API is in beta and may change in future releases.   Warning sample_args for each callable must contain only Tensors. Other types are not allowed.   Warning Returned callables do not support higher order differentiation (e.g., double backward).   Warning In any Module passed to make_graphed_callables(), only parameters may be trainable. Buffers must have requires_grad=False.   Warning After you pass a torch.nn.Module through make_graphed_callables(), you may not add or remove any of that Module\u2019s parameters or buffers.   Warning torch.nn.Modules passed to make_graphed_callables() must not have module hooks registered on them at the time they are passed. However, registering hooks on modules after passing them through make_graphed_callables() is allowed.   Warning When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable\u2019s sample_args.   Warning The automatic mixed precision is supported in make_graphed_callables() only with disabled caching. The context manager torch.cuda.amp.autocast() must have cache_enabled=False.  \n\n"}, {"name": "torch.cuda.torch.cuda.manual_seed", "path": "generated/torch.cuda.manual_seed", "type": "CUDA", "text": "torch.cuda.manual_seed  \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n\n"}, {"name": "torch.cuda.torch.cuda.manual_seed_all", "path": "generated/torch.cuda.manual_seed_all", "type": "CUDA", "text": "torch.cuda.manual_seed_all  \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n\n"}, {"name": "torch.cuda.torch.cuda.max_memory_allocated", "path": "generated/torch.cuda.max_memory_allocated", "type": "CUDA", "text": "torch.cuda.max_memory_allocated  \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.max_memory_cached", "path": "generated/torch.cuda.max_memory_cached", "type": "CUDA", "text": "torch.cuda.max_memory_cached  \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved().  Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.max_memory_reserved", "path": "generated/torch.cuda.max_memory_reserved", "type": "CUDA", "text": "torch.cuda.max_memory_reserved  \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_memory_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.mem_get_info", "path": "generated/torch.cuda.mem_get_info", "type": "CUDA", "text": "torch.cuda.mem_get_info  \ntorch.cuda.mem_get_info(device=None) [source]\n \nReturns the global free and total GPU memory for a given device using cudaMemGetInfo.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nTuple[int, int]    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_allocated", "path": "generated/torch.cuda.memory_allocated", "type": "CUDA", "text": "torch.cuda.memory_allocated  \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_cached", "path": "generated/torch.cuda.memory_cached", "type": "CUDA", "text": "torch.cuda.memory_cached  \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved().  Return type \nint   \n\n"}, {"name": "torch.cuda.torch.cuda.memory_reserved", "path": "generated/torch.cuda.memory_reserved", "type": "CUDA", "text": "torch.cuda.memory_reserved  \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_snapshot", "path": "generated/torch.cuda.memory_snapshot", "type": "CUDA", "text": "torch.cuda.memory_snapshot  \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_stats", "path": "generated/torch.cuda.memory_stats", "type": "CUDA", "text": "torch.cuda.memory_stats  \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.  The caching allocator can be configured via ENV to not split blocks larger than a defined size (see Memory Management section of the Cuda Semantics documentation). This helps avoid memory fragmentation but may have a performance penalty. Additional outputs to assist with tuning and evaluating impact:  \n\"max_split_size\": blocks above this size will not be split. \n\"oversize_allocations.{current,peak,allocated,freed}\": number of over-size allocation requests received by the memory allocator. \n\"oversize_segments.{current,peak,allocated,freed}\": number of over-size reserved segments from cudaMalloc().  The caching allocator can be configured via ENV to round memory allocations in order to reduce fragmentation. Sometimes the overhead from rounding can be higher than the fragmentation it helps reduce. The following stat can be used to check if rounding adds too much overhead:  \n\"requested_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": memory requested by client code, compare this with allocated_bytes to check if allocation rounding adds too much overhead.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).  Return type \nDict[str, Any]    Note See Memory management for more details about GPU memory management.   Note With backend:cudaMallocAsync, some stats are not meaningful, and are always reported as zero.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_summary", "path": "generated/torch.cuda.memory_summary", "type": "CUDA", "text": "torch.cuda.memory_summary  \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).   Return type \nstr    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.memory_usage", "path": "generated/torch.cuda.memory_usage", "type": "CUDA", "text": "torch.cuda.memory_usage  \ntorch.cuda.memory_usage(device=None) [source]\n \nReturns the percent of time over the past sample period during which global (device) memory was being read or written. as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n\n"}, {"name": "torch.cuda.torch.cuda.nvtx.mark", "path": "generated/torch.cuda.nvtx.mark", "type": "CUDA", "text": "torch.cuda.nvtx.mark  \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (str) \u2013 ASCII message to associate with the event.   \n\n"}, {"name": "torch.cuda.torch.cuda.nvtx.range_pop", "path": "generated/torch.cuda.nvtx.range_pop", "type": "CUDA", "text": "torch.cuda.nvtx.range_pop  \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n\n"}, {"name": "torch.cuda.torch.cuda.nvtx.range_push", "path": "generated/torch.cuda.nvtx.range_push", "type": "CUDA", "text": "torch.cuda.nvtx.range_push  \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (str) \u2013 ASCII message to associate with range   \n\n"}, {"name": "torch.cuda.torch.cuda.OutOfMemoryError", "path": "generated/torch.cuda.outofmemoryerror", "type": "CUDA", "text": "torch.cuda.OutOfMemoryError  \nexception torch.cuda.OutOfMemoryError  \nException raised when CUDA is out of memory \n\n"}, {"name": "torch.cuda.torch.cuda.power_draw", "path": "generated/torch.cuda.power_draw", "type": "CUDA", "text": "torch.cuda.power_draw  \ntorch.cuda.power_draw(device=None) [source]\n \n Returns the average power draw of the GPU sensor in mW (MilliWatts)\n\nover the past sample period as given by nvidia-smi for Fermi or newer fully supported devices.    Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n\n"}, {"name": "torch.cuda.torch.cuda.reset_max_memory_allocated", "path": "generated/torch.cuda.reset_max_memory_allocated", "type": "CUDA", "text": "torch.cuda.reset_max_memory_allocated  \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.reset_max_memory_cached", "path": "generated/torch.cuda.reset_max_memory_cached", "type": "CUDA", "text": "torch.cuda.reset_max_memory_cached  \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.reset_peak_memory_stats", "path": "generated/torch.cuda.reset_peak_memory_stats", "type": "CUDA", "text": "torch.cuda.reset_peak_memory_stats  \ntorch.cuda.reset_peak_memory_stats(device=None) [source]\n \nResets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. See memory_stats() for details. Peak stats correspond to the \u201cpeak\u201d key in each individual stat dict.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n\n"}, {"name": "torch.cuda.torch.cuda.seed", "path": "generated/torch.cuda.seed", "type": "CUDA", "text": "torch.cuda.seed  \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n\n"}, {"name": "torch.cuda.torch.cuda.seed_all", "path": "generated/torch.cuda.seed_all", "type": "CUDA", "text": "torch.cuda.seed_all  \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n\n"}, {"name": "torch.cuda.torch.cuda.set_device", "path": "generated/torch.cuda.set_device", "type": "CUDA", "text": "torch.cuda.set_device  \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n\n"}, {"name": "torch.cuda.torch.cuda.set_per_process_memory_fraction", "path": "generated/torch.cuda.set_per_process_memory_fraction", "type": "CUDA", "text": "torch.cuda.set_per_process_memory_fraction  \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n\n"}, {"name": "torch.cuda.torch.cuda.set_rng_state", "path": "generated/torch.cuda.set_rng_state", "type": "CUDA", "text": "torch.cuda.set_rng_state  \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n\n"}, {"name": "torch.cuda.torch.cuda.set_rng_state_all", "path": "generated/torch.cuda.set_rng_state_all", "type": "CUDA", "text": "torch.cuda.set_rng_state_all  \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n\n"}, {"name": "torch.cuda.torch.cuda.set_stream", "path": "generated/torch.cuda.set_stream", "type": "CUDA", "text": "torch.cuda.set_stream  \ntorch.cuda.set_stream(stream) [source]\n \n Sets the current stream.This is a wrapper API to set the stream.\n\nUsage of this function is discouraged in favor of the stream context manager.    Parameters \nstream (Stream) \u2013 selected stream. This function is a no-op if this argument is None.   \n\n"}, {"name": "torch.cuda.torch.cuda.set_sync_debug_mode", "path": "generated/torch.cuda.set_sync_debug_mode", "type": "CUDA", "text": "torch.cuda.set_sync_debug_mode  \ntorch.cuda.set_sync_debug_mode(debug_mode) [source]\n \nSets the debug mode for cuda synchronizing operations.  Parameters \ndebug_mode (str or int) \u2013 if \u201cdefault\u201d or 0, don\u2019t error or warn on synchronizing operations, if \u201cwarn\u201d or 1, warn on synchronizing operations, if \u201cerror\u201d or 2, error out synchronizing operations.    Warning This is an experimental feature, and not all synchronizing operations will trigger warning or error. In particular, operations in torch.distributed and torch.sparse namespaces are not covered yet.  \n\n"}, {"name": "torch.cuda.torch.cuda.stream", "path": "generated/torch.cuda.stream", "type": "CUDA", "text": "torch.cuda.stream  \ntorch.cuda.stream(stream) [source]\n \nWrapper around the Context-manager StreamContext that selects a given stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.  Return type \nStreamContext   ..Note:: In eager mode stream is of type Stream class while in JIT it is an object of the custom class torch.classes.cuda.Stream. \n\n"}, {"name": "torch.cuda.torch.cuda.synchronize", "path": "generated/torch.cuda.synchronize", "type": "CUDA", "text": "torch.cuda.synchronize  \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n\n"}, {"name": "torch.cuda.torch.cuda.temperature", "path": "generated/torch.cuda.temperature", "type": "CUDA", "text": "torch.cuda.temperature  \ntorch.cuda.temperature(device=None) [source]\n \n Returns the average temperature of the GPU sensor in Degrees C (Centigrades)\n\nover the past sample period as given by nvidia-smi.    Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n\n"}, {"name": "torch.cuda.torch.cuda.utilization", "path": "generated/torch.cuda.utilization", "type": "CUDA", "text": "torch.cuda.utilization  \ntorch.cuda.utilization(device=None) [source]\n \nReturns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n\n"}, {"name": "torch.cuda.utilization()", "path": "generated/torch.cuda.utilization#torch.cuda.utilization", "type": "CUDA", "text": " \ntorch.cuda.utilization(device=None) [source]\n \nReturns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).  Return type \nint   Warning: Each sample period may be between 1 second and 1/6 second, depending on the product being queried. \n"}, {"name": "torch.cummax", "path": "generated/torch.cummax", "type": "Torch", "text": "torch.cummax  \ntorch.cummax(input, dim, *, out=None)  \nReturns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=max(x1,x2,x3,\u2026,xi)y_i = max(x_1, x_2, x_3, \\dots, x_i) \n\n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\n \n\n"}, {"name": "torch.cummin", "path": "generated/torch.cummin", "type": "Torch", "text": "torch.cummin  \ntorch.cummin(input, dim, *, out=None)  \nReturns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=min(x1,x2,x3,\u2026,xi)y_i = min(x_1, x_2, x_3, \\dots, x_i) \n\n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\n \n\n"}, {"name": "torch.cumprod", "path": "generated/torch.cumprod", "type": "Torch", "text": "torch.cumprod  \ntorch.cumprod(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative product of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1\u00d7x2\u00d7x3\u00d7\u22ef\u00d7xiy_i = x_1 \\times x_2\\times x_3\\times \\dots \\times x_i \n\n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n \n\n"}, {"name": "torch.cumsum", "path": "generated/torch.cumsum", "type": "Torch", "text": "torch.cumsum  \ntorch.cumsum(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative sum of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1+x2+x3+\u22ef+xiy_i = x_1 + x_2 + x_3 + \\dots + x_i \n\n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n \n\n"}, {"name": "torch.cumulative_trapezoid", "path": "generated/torch.cumulative_trapezoid", "type": "Torch", "text": "torch.cumulative_trapezoid  \ntorch.cumulative_trapezoid(y, x=None, *, dx=None, dim=-1) \u2192 Tensor  \nCumulatively computes the trapezoidal rule along dim. By default the spacing between elements is assumed to be 1, but dx can be used to specify a different constant spacing, and x can be used to specify arbitrary spacing along dim. For more details, please read torch.trapezoid(). The difference between torch.trapezoid() and this function is that, torch.trapezoid() returns a value for each integration, where as this function returns a cumulative value for every spacing within the integration. This is analogous to how .sum returns a value and .cumsum returns a cumulative sum.  Parameters \n \ny (Tensor) \u2013 Values to use when computing the trapezoidal rule. \nx (Tensor) \u2013 If specified, defines spacing between values as specified above.   Keyword Arguments \n \ndx (float) \u2013 constant spacing between values. If neither x or dx are specified then this defaults to 1. Effectively multiplies the result by its value. \ndim (int) \u2013 The dimension along which to compute the trapezoidal rule. The last (inner-most) dimension by default.    Examples: >>> # Cumulatively computes the trapezoidal rule in 1D, spacing is implicitly 1.\n>>> y = torch.tensor([1, 5, 10])\n>>> torch.cumulative_trapezoid(y)\ntensor([3., 10.5])\n\n>>> # Computes the same trapezoidal rule directly up to each element to verify\n>>> (1 + 5) / 2\n3.0\n>>> (1 + 10 + 10) / 2\n10.5\n\n>>> # Cumulatively computes the trapezoidal rule in 1D with constant spacing of 2\n>>> # NOTE: the result is the same as before, but multiplied by 2\n>>> torch.cumulative_trapezoid(y, dx=2)\ntensor([6., 21.])\n\n>>> # Cumulatively computes the trapezoidal rule in 1D with arbitrary spacing\n>>> x = torch.tensor([1, 3, 6])\n>>> torch.cumulative_trapezoid(y, x)\ntensor([6., 28.5])\n\n>>> # Computes the same trapezoidal rule directly up to each element to verify\n>>> ((3 - 1) * (1 + 5)) / 2\n6.0\n>>> ((3 - 1) * (1 + 5) + (6 - 3) * (5 + 10)) / 2\n28.5\n\n>>> # Cumulatively computes the trapezoidal rule for each row of a 3x3 matrix\n>>> y = torch.arange(9).reshape(3, 3)\ntensor([[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]])\n>>> torch.cumulative_trapezoid(y)\ntensor([[ 0.5,  2.],\n        [ 3.5,  8.],\n        [ 6.5, 14.]])\n\n>>> # Cumulatively computes the trapezoidal rule for each column of the matrix\n>>> torch.cumulative_trapezoid(y, dim=0)\ntensor([[ 1.5,  2.5,  3.5],\n        [ 6.0,  8.0, 10.0]])\n\n>>> # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix\n>>> #   with the same arbitrary spacing\n>>> y = torch.ones(3, 3)\n>>> x = torch.tensor([1, 3, 6])\n>>> torch.cumulative_trapezoid(y, x)\ntensor([[2., 5.],\n        [2., 5.],\n        [2., 5.]])\n\n>>> # Cumulatively computes the trapezoidal rule for each row of a 3x3 ones matrix\n>>> #   with different arbitrary spacing per row\n>>> y = torch.ones(3, 3)\n>>> x = torch.tensor([[1, 2, 3], [1, 3, 5], [1, 4, 7]])\n>>> torch.cumulative_trapezoid(y, x)\ntensor([[1., 2.],\n        [2., 4.],\n        [3., 6.]])\n \n\n"}, {"name": "torch.deg2rad", "path": "generated/torch.deg2rad", "type": "Torch", "text": "torch.deg2rad  \ntorch.deg2rad(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with each of the elements of input converted from angles in degrees to radians.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])\n \n\n"}, {"name": "torch.dequantize", "path": "generated/torch.dequantize", "type": "Torch", "text": "torch.dequantize  \ntorch.dequantize(tensor) \u2192 Tensor  \nReturns an fp32 Tensor by dequantizing a quantized Tensor  Parameters \ntensor (Tensor) \u2013 A quantized Tensor     torch.dequantize(tensors) \u2192 sequence of Tensors\n\n Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors  Parameters \ntensors (sequence of Tensors) \u2013 A list of quantized Tensors   \n\n"}, {"name": "torch.det", "path": "generated/torch.det", "type": "Torch", "text": "torch.det  \ntorch.det(input) \u2192 Tensor  \nAlias for torch.linalg.det() \n\n"}, {"name": "torch.device", "path": "tensor_attributes#torch.device", "type": "Miscellaneous", "text": " \nclass torch.device \n"}, {"name": "torch.diag", "path": "generated/torch.diag", "type": "Torch", "text": "torch.diag  \ntorch.diag(input, diagonal=0, *, out=None) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.  The argument diagonal controls which diagonal to consider:  If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndiagonal (int, optional) \u2013 the diagonal to consider   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    See also torch.diagonal() always returns the diagonal of its input. torch.diagflat() always constructs a tensor with diagonal elements specified by the input.  Examples: Get the square matrix where the input vector is the diagonal: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n Get the k-th diagonal of a given matrix: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n \n\n"}, {"name": "torch.diag_embed", "path": "generated/torch.diag_embed", "type": "Torch", "text": "torch.diag_embed  \ntorch.diag_embed(input, offset=0, dim1=-2, dim2=-1) \u2192 Tensor  \nCreates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00, the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset. Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 1-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: -2. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: -1.    Example: >>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n \n\n"}, {"name": "torch.diagflat", "path": "generated/torch.diagflat", "type": "Torch", "text": "torch.diagflat  \ntorch.diagflat(input, offset=0) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened input.  The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \noffset (int, optional) \u2013 the diagonal to consider. Default: 0 (main diagonal).    Examples: >>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n \n\n"}, {"name": "torch.diagonal", "path": "generated/torch.diagonal", "type": "Torch", "text": "torch.diagonal  \ntorch.diagonal(input, offset=0, dim1=0, dim2=1) \u2192 Tensor  \nReturns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 2-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: 0. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: 1.     Note To take a batch diagonal, pass in dim1=-2, dim2=-1.  Examples: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n \n\n"}, {"name": "torch.diagonal_scatter", "path": "generated/torch.diagonal_scatter", "type": "Torch", "text": "torch.diagonal_scatter  \ntorch.diagonal_scatter(input, src, offset=0, dim1=0, dim2=1) \u2192 Tensor  \nEmbeds the values of the src tensor into input along the diagonal elements of input, with respect to dim1 and dim2. This function returns a tensor with fresh storage; it does not return a view. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 2-dimensional. \nsrc (Tensor) \u2013 the tensor to embed into input. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: 0. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: 1.     Note src must be of the proper size in order to be embedded into input. Specifically, it should have the same shape as torch.diagonal(input, offset, dim1, dim2)  Examples: >>> a = torch.zeros(3, 3)\n>>> a\ntensor([[0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n\n>>> torch.diagonal_scatter(a, torch.ones(3), 0)\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n>>> torch.diagonal_scatter(a, torch.ones(2), 1)\ntensor([[0., 1., 0.],\n        [0., 0., 1.],\n        [0., 0., 0.]])\n \n\n"}, {"name": "torch.diff", "path": "generated/torch.diff", "type": "Torch", "text": "torch.diff  \ntorch.diff(input, n=1, dim=-1, prepend=None, append=None) \u2192 Tensor  \nComputes the n-th forward difference along the given dimension. The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.  Parameters \n \ninput (Tensor) \u2013 the tensor to compute the differences on \nn (int, optional) \u2013 the number of times to recursively compute the difference \ndim (int, optional) \u2013 the dimension to compute the difference along. Default is the last dimension. \nprepend (Tensor, optional) \u2013 values to prepend or append to input along dim before computing the difference. Their dimensions must be equivalent to that of input, and their shapes must match input\u2019s shape except on dim. \nappend (Tensor, optional) \u2013 values to prepend or append to input along dim before computing the difference. Their dimensions must be equivalent to that of input, and their shapes must match input\u2019s shape except on dim.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])\n \n\n"}, {"name": "torch.digamma", "path": "generated/torch.digamma", "type": "Torch", "text": "torch.digamma  \ntorch.digamma(input, *, out=None) \u2192 Tensor  \nAlias for torch.special.digamma(). \n\n"}, {"name": "torch.dist", "path": "generated/torch.dist", "type": "Torch", "text": "torch.dist  \ntorch.dist(input, other, p=2) \u2192 Tensor  \nReturns the p-norm of (input - other) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the Right-hand-side input tensor \np (float, optional) \u2013 the norm to be computed    Example: >>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(4.)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)\n \n\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.debugging_hooks.noop_hook(_, bucket) [source]\n \nThis DDP communication hook returns a future that wraps the input, so it is a noop that does not incur any communication overheads. This hook should only be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks.  Example::\n\n>>> ddp_model.register_comm_hook(None, noop_hook)\n    Return type \nFuture[Tensor]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket) [source]\n \nThis DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.  Example::\n\n>>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n    Return type \nFuture[Tensor]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_hook(process_group, bucket) [source]\n \nWarning: This API is experimental, and it requires NCCL version later than 2.9.6. This DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision Brain floating point format (torch.bfloat16) and then divides it by the process group size. It allreduces those bfloat16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).  Example::\n\n>>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)\n    Return type \nFuture[Tensor]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.bf16_compress_wrapper(hook) [source]\n \nWarning: This API is experimental, and it requires NCCL version later than 2.9.6. This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_ (``torch.bfloat16`), and casts the resulting tensor of the given hook back to the input data type, such as float32. Therefore, bf16_compress_hook is equivalent to bf16_compress_wrapper(allreduce_hook).  Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))\n    Return type \nCallable[[Any, GradBucket], Future[Tensor]]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket) [source]\n \nThis DDP communication hook implements a simple gradient compression approach that casts GradBucket tensor to half-precision floating-point format (torch.float16) and then divides it by the process group size. It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, the chained callback decompress casts it back to the input data type (such as float32).  Example::\n\n>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n    Return type \nFuture[Tensor]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_wrapper(hook) [source]\n \nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (torch.float16), and casts the resulting tensor of the given hook back to the input data type, such as float32. Therefore, fp16_compress_hook is equivalent to fp16_compress_wrapper(allreduce_hook).  Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))\n    Return type \nCallable[[Any, GradBucket], Future[Tensor]]   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.  Warning Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.  Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings; Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; Computes P, which is equal to MQ; Allreduces P; Orthogonalizes P; Computes Q, which is approximately equal to M^TP; Allreduces Q; Computes M, which is approximately equal to PQ^T. Truncates the input tensor to the original length.  Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank and start_powerSGD_iter. \nbucket (dist.GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.  Return type \nFuture[Tensor]    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)\n>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  \nViews the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups: 1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth. 1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).  \nHandles uncompressed tensors: 2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression; 2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.  \nHandles the tensors that should be compressed by PowerSGD compression: 3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; 3.2. Computes each P in Ps, which is equal to MQ; 3.3. Allreduces Ps as a batch; 3.4. Orthogonalizes each P in Ps; 3.5. Computes each Q in Qs, which is approximately equal to M^TP; 3.6. Allreduces Qs as a batch; 3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.   Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank, start_powerSGD_iter and min_compression_rate. \nbucket (dist.GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.  Return type \nFuture[Tensor]    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1,\n                          start_powerSGD_iter=10, min_compression_rate=0.5)\n>>> ddp_model.register_comm_hook(state, powerSGD_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": " \nclass torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=1000, min_compression_rate=2, use_error_feedback=True, warm_start=True, orthogonalization_epsilon=0, random_seed=0, compression_stats_logging_frequency=10000, batch_tensors_with_same_shape=False) [source]\n \nStores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.  \nmatrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression. 1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy. 1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain matrix_approximation_rank threshold.   To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.  \nstart_powerSGD_iter defers PowerSGD compression until step start_powerSGD_iter, and vanilla allreduce runs prior to step start_powerSGD_iter. This hybrid scheme of vanilla allreduce + PowerSGD can effectively improve the accuracy, even a relatively small matrix_approximation_rank is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.  To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, start_powerSGD_iter typically should be no less than the number of warm-up steps.  \nmin_compression_rate is the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where (num_rows + num_cols) * matrix_approximation_rank * min_compression_rate < num_rows * num_cols. If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.  Compression statistics are logged every compression_stats_logging_frequency iterations once PowerSGD compression starts.  \northogonalization_epsilon can be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy. \nbatch_tensors_with_same_shape controls whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e., bucket_cap_mb arg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set to True if the compression / decompression computation is a bottleneck.   Warning If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.  \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__getstate__", "type": "DDP Communication Hooks", "text": " \n__getstate__() [source]\n \nReturns a Dict[str, Any] which will be pickled and saved. process_group is not serializable and excluded from a returned state. \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState.__setstate__", "type": "DDP Communication Hooks", "text": " \n__setstate__(state) [source]\n \nTakes a provided state and retrieves PowerSGDState. process_group is set to default. \n"}, {"name": "torch.distributed.algorithms.Join", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join", "type": "Miscellaneous", "text": " \nclass torch.distributed.algorithms.Join(joinables, enable=True, throw_on_early_termination=False, **kwargs) [source]\n \nThis class defines the generic join context manager, which allows custom hooks to be called after a process joins. These hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to JoinHook for details about the hook definition.  Warning The context manager requires each participating Joinable to call the method notify_join_context() before its own per- iteration collective communications to ensure correctness.   Warning The context manager requires that all process_group attributes in the JoinHook objects are the same. If there are multiple JoinHook objects, then the device of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if throw_on_early_termination is enabled, both of which using an all- reduce.   Parameters \n \njoinables (List[Joinable]) \u2013 a list of the participating Joinable s; their hooks are iterated over in the given order. \nenable (bool) \u2013 a flag enabling uneven input detection; setting to False disables the context manager\u2019s functionality and should only be set when the user knows the inputs will not be uneven (default: True). \nthrow_on_early_termination (bool) \u2013 a flag controlling whether to throw an exception upon detecting uneven inputs (default: False).    Example: >>> import os\n>>> import torch\n>>> import torch.distributed as dist\n>>> import torch.multiprocessing as mp\n>>> import torch.nn.parallel.DistributedDataParallel as DDP\n>>> import torch.distributed.optim.ZeroRedundancyOptimizer as ZeRO\n>>> from torch.distributed.algorithms.join import Join\n>>>\n>>> # On each spawned worker\n>>> def worker(rank):\n>>>     dist.init_process_group(\"nccl\", rank=rank, world_size=2)\n>>>     model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n>>>     optim = ZeRO(model.parameters(), torch.optim.Adam, lr=0.01)\n>>>     # Rank 1 gets one more input than rank 0\n>>>     inputs = [torch.tensor([1.]).to(rank) for _ in range(10 + rank)]\n>>>     with Join([model, optim]):\n>>>         for input in inputs:\n>>>             loss = model(input).sum()\n>>>             loss.backward()\n>>>             optim.step()\n>>>     # All ranks reach here without hanging/erroring\n  \nstatic notify_join_context(joinable) [source]\n \nNotifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so. This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel. Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.  Parameters \njoinable (Joinable) \u2013 the Joinable object calling this method.  Returns \nAn async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise.   \n \n"}, {"name": "torch.distributed.algorithms.Join.notify_join_context()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Join.notify_join_context", "type": "Miscellaneous", "text": " \nstatic notify_join_context(joinable) [source]\n \nNotifies the join context manager that the calling process has not yet joined; then, if throw_on_early_termination=True, checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so. This method should be called from a Joinable object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in DistributedDataParallel. Only the first Joinable object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.  Parameters \njoinable (Joinable) \u2013 the Joinable object calling this method.  Returns \nAn async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if joinable is the first one passed into the context manager; None otherwise.   \n"}, {"name": "torch.distributed.algorithms.Joinable", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable", "type": "Miscellaneous", "text": " \nclass torch.distributed.algorithms.Joinable [source]\n \nThis defines an abstract base class for joinable classes. A joinable class (inheriting from Joinable) should implement join_hook(), which returns a JoinHook instance, in addition to join_device() and join_process_group() that return device and process group information, respectively.  \nabstract property join_device: device  \nReturns the device from which to perform collective communications needed by the join context manager implementation itself. \n  \nabstract join_hook(**kwargs) [source]\n \nReturns a JoinHook instance for the given Joinable.  Parameters \nkwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.  Return type \nJoinHook   \n  \nabstract property join_process_group: Any  \nReturns the process group for the collective communications needed by the join context manager itself. \n \n"}, {"name": "torch.distributed.algorithms.Joinable.join_device", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_device", "type": "Miscellaneous", "text": " \nabstract property join_device: device  \nReturns the device from which to perform collective communications needed by the join context manager implementation itself. \n"}, {"name": "torch.distributed.algorithms.Joinable.join_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_hook", "type": "Miscellaneous", "text": " \nabstract join_hook(**kwargs) [source]\n \nReturns a JoinHook instance for the given Joinable.  Parameters \nkwargs (dict) \u2013 a dict containing any keyword arguments to modify the behavior of the join hook at run time; all Joinable instances sharing the same join context manager are forwarded the same value for kwargs.  Return type \nJoinHook   \n"}, {"name": "torch.distributed.algorithms.Joinable.join_process_group", "path": "distributed.algorithms.join#torch.distributed.algorithms.Joinable.join_process_group", "type": "Miscellaneous", "text": " \nabstract property join_process_group: Any  \nReturns the process group for the collective communications needed by the join context manager itself. \n"}, {"name": "torch.distributed.algorithms.JoinHook", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook", "type": "Miscellaneous", "text": " \nclass torch.distributed.algorithms.JoinHook [source]\n \nThis defines a join hook, which provides two entry points in the join context manager: a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined. To implement a join hook for the generic join context manager, define a class that inherits from JoinHook and override main_hook() and post_hook() as appropriate.  \nmain_hook() [source]\n \nThis hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step). \n  \npost_hook(is_last_joiner) [source]\n \nThis hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.  Parameters \nis_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise.   \n \n"}, {"name": "torch.distributed.algorithms.JoinHook.main_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.main_hook", "type": "Miscellaneous", "text": " \nmain_hook() [source]\n \nThis hook is called repeatedly while there exists a non-joined process to shadow collective communications in one training iteration (i.e. in one forward pass, backward pass, and optimizer step). \n"}, {"name": "torch.distributed.algorithms.JoinHook.post_hook()", "path": "distributed.algorithms.join#torch.distributed.algorithms.JoinHook.post_hook", "type": "Miscellaneous", "text": " \npost_hook(is_last_joiner) [source]\n \nThis hook is called after all processes have joined. It is passed an additional bool argument is_last_joiner, which indicates if the rank is one of the last to join.  Parameters \nis_last_joiner (bool) \u2013 True if the rank is one of the last to join; False otherwise.   \n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "Distributed Communication", "text": " \ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Complex tensors are supported.  Parameters \n \ntensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. \ntensor (Tensor) \u2013 Tensor to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zeros(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n \n"}, {"name": "torch.distributed.all_gather_into_tensor()", "path": "distributed#torch.distributed.all_gather_into_tensor", "type": "Distributed Communication", "text": " \ntorch.distributed.all_gather_into_tensor(output_tensor, input_tensor, group=None, async_op=False) [source]\n \nGather tensors from all ranks and put them in a single output tensor.  Parameters \n \noutput_tensor (Tensor) \u2013 Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of \u201cconcatenation\u201d, see torch.cat(); (ii) a stack of all the input tensors along the primary dimension; for definition of \u201cstack\u201d, see torch.stack(). Examples below may better explain the supported output forms. \ninput_tensor (Tensor) \u2013 Tensor to be gathered from current rank. Different from the all_gather API, the input tensors in this API must have the same size across all ranks. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype and on CUDA devices.\n>>> # We have two ranks.\n>>> device = torch.device(f'cuda:{rank}')\n>>> tensor_in = torch.arange(2, dtype=torch.int64, device=device) + 1 + 2 * rank\n>>> tensor_in\ntensor([1, 2], device='cuda:0') # Rank 0\ntensor([3, 4], device='cuda:1') # Rank 1\n>>> # Output in concatenation form\n>>> tensor_out = torch.zeros(world_size * 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out, tensor_in)\n>>> tensor_out\ntensor([1, 2, 3, 4], device='cuda:0') # Rank 0\ntensor([1, 2, 3, 4], device='cuda:1') # Rank 1\n>>> # Output in stack form\n>>> tensor_out2 = torch.zeros(world_size, 2, dtype=torch.int64, device=device)\n>>> dist.all_gather_into_tensor(tensor_out2, tensor_in)\n>>> tensor_out2\ntensor([[1, 2],\n        [3, 4]], device='cuda:0') # Rank 0\ntensor([[1, 2],\n        [3, 4]], device='cuda:1') # Rank 1\n  Warning The Gloo backend does not support this API.  \n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "Distributed Communication", "text": " \ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors Complex tensors are supported.  Parameters \n \noutput_tensor_lists (List[List[Tensor]]) \u2013 \nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ninput_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "Distributed Communication", "text": " \ntorch.distributed.all_gather_object(object_list, obj, group=None) [source]\n \nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobject_list (list[Any]) \u2013 Output list. It should be correctly sized as the size of the group for this collective and will contain the output. \nobj (Any) \u2013 Pickable Python object to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.    Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Warning Calling all_gather_object() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using all_gather() instead.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "Distributed Communication", "text": " \ntorch.distributed.all_reduce(tensor, op=<RedOpType.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes. Complex tensors are supported.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n >>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n \n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "Distributed Communication", "text": " \ntorch.distributed.all_reduce_multigpu(tensor_list, op=<RedOpType.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Complex tensors are supported. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "Distributed Communication", "text": " \ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False) [source]\n \nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list. Complex tensors are supported.  Parameters \n \noutput_tensor_list (list[Tensor]) \u2013 List of tensors to be gathered one per rank. \ninput_tensor_list (list[Tensor]) \u2013 List of tensors to scatter one per rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src=i)\n >>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n >>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n \n"}, {"name": "torch.distributed.all_to_all_single()", "path": "distributed#torch.distributed.all_to_all_single", "type": "Distributed Communication", "text": " \ntorch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False) [source]\n \nEach process splits input tensor and then scatters the split list to all processes in a group. Then concatenate the received tensors from all the processes in the group and return single output tensor. Complex tensors are supported.  Parameters \n \noutput (Tensor) \u2013 Gathered concatenated output tensor. \ninput (Tensor) \u2013 Input tensor to scatter. \noutput_split_sizes \u2013 (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of output tensor must divide equally by world_size. \ninput_split_sizes \u2013 (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of input tensor must divide equally by world_size. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all_single is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\ntensor([8, 9, 10, 11])   # Rank 2\ntensor([12, 13, 14, 15]) # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\ntensor([2, 6, 10, 14])   # Rank 2\ntensor([3, 7, 11, 15])   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = list(input.chunk(world_size))\n>>> gather_list  = list(output.chunk(world_size))\n>>> for i in range(world_size):\n>>>     dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n >>> # Another example with uneven split\n>>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> output = ...\n>>> dist.all_to_all_single(output, input, output_splits, input_splits)\n>>> output\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n >>> # Another example with tensors of torch.cfloat type.\n>>> input = torch.tensor([1+1j, 2+2j, 3+3j, 4+4j], dtype=torch.cfloat) + 4 * rank * (1+1j)\n>>> input\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n>>> output = torch.empty([4], dtype=torch.int64)\n>>> dist.all_to_all_single(output, input)\n>>> output\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n \n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC", "text": " \ntorch.distributed.autograd.backward(context_id: int, roots: List[Tensor], retain_graph=False) \u2192 None  \nKicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass. We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done. We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API.  Parameters \n \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients. \nroots (list) \u2013 Tensors which represent the roots of the autograd computation. All the tensors should be scalars. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n   \n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC", "text": " \nclass torch.distributed.autograd.context [source]\n \nContext object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass.  Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>     dist_autograd.backward(context_id, [loss])\n   \n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC", "text": " \ntorch.distributed.autograd.get_gradients(context_id: int) \u2192 Dict[Tensor, Tensor]  \nRetrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.  Parameters \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients.  Returns \nA map where the key is the Tensor and the value is the associated gradient for that Tensor.    Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n   \n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "Distributed Communication", "text": " \nclass torch.distributed.Backend(name) [source]\n \nAn enum-like class of available backends: GLOO, NCCL, UCC, MPI, and other registered backends. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.   \nclassmethod register_backend(name, func, extended_api=False, devices=None) [source]\n \nRegisters a new backend with the given name and instantiating function. This class method is used by 3rd party ProcessGroup extension to register new backends.  Parameters \n \nname (str) \u2013 Backend name of the ProcessGroup extension. It should match the one in init_process_group(). \nfunc (function) \u2013 Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including store, rank, world_size, and timeout. \nextended_api (bool, optional) \u2013 Whether the backend supports extended argument structure. Default: False. If set to True, the backend will get an instance of c10d::DistributedBackendOptions, and a process group options object as defined by the backend implementation. \ndevice (str or list of str, optional) \u2013 device type this backend supports, e.g. \u201ccpu\u201d, \u201ccuda\u201d, etc. If None, assuming both \u201ccpu\u201d and \u201ccuda\u201d     Note This support of 3rd party backend is experimental and subject to change.  \n \n"}, {"name": "torch.distributed.Backend.register_backend()", "path": "distributed#torch.distributed.Backend.register_backend", "type": "Distributed Communication", "text": " \nclassmethod register_backend(name, func, extended_api=False, devices=None) [source]\n \nRegisters a new backend with the given name and instantiating function. This class method is used by 3rd party ProcessGroup extension to register new backends.  Parameters \n \nname (str) \u2013 Backend name of the ProcessGroup extension. It should match the one in init_process_group(). \nfunc (function) \u2013 Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including store, rank, world_size, and timeout. \nextended_api (bool, optional) \u2013 Whether the backend supports extended argument structure. Default: False. If set to True, the backend will get an instance of c10d::DistributedBackendOptions, and a process group options object as defined by the backend implementation. \ndevice (str or list of str, optional) \u2013 device type this backend supports, e.g. \u201ccpu\u201d, \u201ccuda\u201d, etc. If None, assuming both \u201ccpu\u201d and \u201ccuda\u201d     Note This support of 3rd party backend is experimental and subject to change.  \n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "Distributed Communication", "text": " \ntorch.distributed.barrier(group=None, async_op=False, device_ids=None) [source]\n \nSynchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters \n \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndevice_ids ([int], optional) \u2013 List of device/GPU ids.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.batch_isend_irecv()", "path": "distributed#torch.distributed.batch_isend_irecv", "type": "Distributed Communication", "text": " \ntorch.distributed.batch_isend_irecv(p2p_op_list) [source]\n \nSend or Receive a batch of tensors asynchronously and return a list of requests. Process each of the operations in p2p_op_list and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.  Parameters \np2p_op_list \u2013 A list of point-to-point operations(type of each operator is torch.distributed.P2POp). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.  Returns \nA list of distributed request objects returned by calling the corresponding op in the op_list.   Examples >>> send_tensor = torch.arange(2) + 2 * rank\n>>> recv_tensor = torch.randn(2)\n>>> send_op = dist.P2POp(dist.isend, send_tensor, (rank + 1)%world_size)\n>>> recv_op = dist.P2POp(dist.irecv, recv_tensor, (rank - 1 + world_size)%world_size)\n>>> reqs = batch_isend_irecv([send_op, recv_op])\n>>> for req in reqs:\n>>>     req.wait()\n>>> recv_tensor\ntensor([2, 3])     # Rank 0\ntensor([0, 1])     # Rank 1\n  Note Note that when this API is used with the NCCL PG backend, users must set the current GPU device with torch.cuda.set_device, otherwise it will lead to unexpected hang issues. In addition, if this API is the first collective call in the group passed to dist.P2POp, all ranks of the group must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the group, batched P2P operations involving only a subset of ranks of the group are allowed.  \n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "Distributed Communication", "text": " \ntorch.distributed.broadcast(tensor, src, group=None, async_op=False) [source]\n \nBroadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters \n \ntensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "Distributed Communication", "text": " \ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0) [source]\n \nBroadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \nsrc_tensor (int, optional) \u2013 Source tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "Distributed Communication", "text": " \ntorch.distributed.broadcast_object_list(object_list, src=0, group=None, device=None) [source]\n \nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.  Parameters \n \nobject_list (List[Any]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. \nsrc (int) \u2013 Source rank from which to broadcast object_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None. \ndevice (torch.device, optional) \u2013 If not None, the objects are serialized and converted to tensors which are moved to the device before broadcasting. Default is None.   Returns \nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.    Note For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Warning Calling broadcast_object_list() with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using broadcast() instead.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> # Assumes backend is not NCCL\n>>> device = torch.device(\"cpu\")\n>>> dist.broadcast_object_list(objects, src=0, device=device)\n>>> objects\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.DefaultLoadPlanner(flatten_state_dict=True, flatten_sharded_tensors=True) [source]\n \nDefaultLoadPlanner that adds multiple features on top of LoadPlanner. In particular it adds the following: flatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode  \nlookup_tensor(index) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner  Return type \nTensor   \n  \ntransform_tensor(read_item, tensor) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner \n \n"}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.lookup_tensor", "type": "Distributed Checkpoint", "text": " \nlookup_tensor(index) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner  Return type \nTensor   \n"}, {"name": "torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultLoadPlanner.transform_tensor", "type": "Distributed Checkpoint", "text": " \ntransform_tensor(read_item, tensor) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner \n"}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.DefaultSavePlanner(flatten_state_dict=True, flatten_sharded_tensors=True, dedup_replicated_tensors=True) [source]\n \n \nlookup_object(index) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner  Return type \nAny   \n  \ntransform_object(write_item, object) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner \n \n"}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.lookup_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.lookup_object", "type": "Distributed Checkpoint", "text": " \nlookup_object(index) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner  Return type \nAny   \n"}, {"name": "torch.distributed.checkpoint.DefaultSavePlanner.transform_object()", "path": "distributed.checkpoint#torch.distributed.checkpoint.DefaultSavePlanner.transform_object", "type": "Distributed Checkpoint", "text": " \ntransform_object(write_item, object) [source]\n \nThis is an extension from the planner interface to make it easy to extend the default planner \n"}, {"name": "torch.distributed.checkpoint.FileSystemReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemReader", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.FileSystemReader(path) [source]\n\n"}, {"name": "torch.distributed.checkpoint.FileSystemWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.FileSystemWriter", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.FileSystemWriter(path, single_file_per_rank=True, sync_files=True, thread_count=1, per_thread_copy_ahead=10000000) [source]\n \nBasic implementation of StorageWriter using file IO. This implementation makes the following assumptions and simplifications:  The checkpoint path is an empty or non-existing directory. File creation is atomic  The checkpoint consist of one file per write request plus a .metadata file with the serialized metadata. \n"}, {"name": "torch.distributed.checkpoint.load_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.load_state_dict", "type": "Distributed Checkpoint", "text": " \ntorch.distributed.checkpoint.load_state_dict(state_dict, storage_reader, process_group=None, coordinator_rank=0, no_dist=False, planner=None) [source]\n \nLoads a distributed state_dict in SPMD style. Each rank will try to read the least amount of data necessary to fullfill the requested state_dict. When loading ShardedTensor instances, each rank only reads data for their local shards.  Warning All tensors in state_dict must be allocated on their destination device prior to calling this function. All non-tensor data is loaded using torch.load() and modified in place on state_dict.   Warning Users must call load_state_dict on the root module to ensure load pos-processing and non-tensor data properly propagates.   Parameters \n \nstate_dict (Dict[str, Any]) \u2013 The state_dict to load. Note that this state dict will updated in place. \nstorage_reader (StorageReader) \u2013 StorageReader used to load data from. \nprocess_group (ProcessGroup) \u2013 ProcessGroup to be used for cross-rank synchronization. \ncoordinator_rank (int) \u2013 Rank to use to coordinate the checkpoint. rank0 is used by default. \nno_dist (bool) \u2013 If True, distributed checkpoint will not save in SPMD style. (Default: False)   Returns \nNone.  Return type \nNone    Examples\n\n>>> my_model = MyModule()\n>>> optimizer = Adagrad(my_model.parameters())\n>>> model_state_dict = my_model.state_dict()\n>>> fs_storage_reader = torch.distributed.checkpoint.FileSystemReader(\"/checkpoint/1\")\n >>> torch.distributed.checkpoint.load_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_reader=fs_storage_reader,\n>>> )\n >>> # module.load_state_dict() function might have customized steps\n>>> # to flush the state_dict, must call it to\n>>> # ensure correct behavior.\n>>> my_model.load_state_dict(model_state_dict)\n    Note load_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().  \n"}, {"name": "torch.distributed.checkpoint.LoadPlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlan", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.LoadPlan(items: List[torch.distributed.checkpoint.planner.ReadItem], storage_data: Any = None, planner_data: Any = None) [source]\n\n"}, {"name": "torch.distributed.checkpoint.LoadPlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.LoadPlanner [source]\n \nAbstract class defining the protocol used by load_state_dict to plan the load process. LoadPlanner are stateful objects that can be used to customize the whole load process. LoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process. A planner subclass can expect the following sequence of calls during load_state_dict:  \n set_up_planner - called on all ranks.\n\nSignals the start of loading a checkpoint.    \n create_local_plan - called on all ranks.\n\nProcess the state_dict and produces a LoadPlan that will be sent for global planning.    \n create_global_plan - called on the coordinator rank only.\n\nTakes the LoadPlan from all ranks and make any global decision.    \n load_bytes - called multiple times on each rank\n\nThis is called once per non-tensor value in state_dict.    \n resolve_tensor and commit_tensor - called multiple times on each rank\n\nThey are called in pair for each Tensor value in state_dict.     Users are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method. There are two usual patterns of extension: Rewriting state_dict. This is the simplest way to extend the load process as it doesn\u2019t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place >>> class RenamePlanner(DefaultLoadPlanner):\n>>>     def set_up_planner(self, state_dict, metadata, is_coordinator):\n>>>         self.original_state_dict = state_dict\n>>>         super().set_up_planner(self, {\"foo_\" + k: v for k, v in state_dict.items()}, is_coordinator)\n>>>\n>>>     def load_bytes(self, read_item, value):\n>>>         # Remove the \"foo_\" prefix\n>>>         self.original_state_dict[read_item.dest_index.fqn[4:]] = torch.load(value)\n Modifying resolve_tensor and commit_tensor to handle load time transformation. >>> class MetaModelMaterialize(DefaultSavePlanner):\n>>>     def resolve_tensor(self, read_item):\n>>>         tensor = super().resolve_tensor(read_item)\n>>>         return torch.empty_like(tensor, device=\"cpu\")\n>>>\n>>>     def commit_tensor(self, read_item, tensor):\n>>>         self.state_dict[read_item.dest_index.fqn] = tensor\n  \nabstract commit_tensor(read_item, tensor) [source]\n \nThis method is called once the StorageReader finished loading data into tensor. The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict. The contents of tensor will follow its device synchronization model. \n  \nabstract create_global_plan(global_plan) [source]\n \nCompute the global load plan and return plans for each rank. . N.B. This is called on the coordinator rank only  Return type \nList[LoadPlan]   \n  \nabstract create_local_plan() [source]\n \nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner. . N.B. This is called on every rank.  Return type \nLoadPlan   \n  \nabstract finish_plan(central_plan) [source]\n \nAccept the plan from coordinator and return final LoadPlan.  Return type \nLoadPlan   \n  \nabstract load_bytes(read_item, value) [source]\n \nLoad the item described by read_item``and ``value. This method is expected to modify in-place the underlying state_dict. The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded. \n  \nabstract resolve_tensor(read_item) [source]\n \nReturn the tensor described by read_item to be used by the StorageReader to load read_item. The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.  Return type \nTensor   \n  \nabstract set_up_planner(state_dict, metadata, is_coordinator) [source]\n \nInitialize this instance to load data into state_dict . N.B. This is called on every rank. \n \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.commit_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.commit_tensor", "type": "Distributed Checkpoint", "text": " \nabstract commit_tensor(read_item, tensor) [source]\n \nThis method is called once the StorageReader finished loading data into tensor. The provided tensor is the same one returned by the call to resolve_tensor. This method is only needed if this LoadPlanner needs to post process tensor prior to copying it back to the one in the state_dict. The contents of tensor will follow its device synchronization model. \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_global_plan", "type": "Distributed Checkpoint", "text": " \nabstract create_global_plan(global_plan) [source]\n \nCompute the global load plan and return plans for each rank. . N.B. This is called on the coordinator rank only  Return type \nList[LoadPlan]   \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.create_local_plan", "type": "Distributed Checkpoint", "text": " \nabstract create_local_plan() [source]\n \nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner. . N.B. This is called on every rank.  Return type \nLoadPlan   \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.finish_plan", "type": "Distributed Checkpoint", "text": " \nabstract finish_plan(central_plan) [source]\n \nAccept the plan from coordinator and return final LoadPlan.  Return type \nLoadPlan   \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.load_bytes()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.load_bytes", "type": "Distributed Checkpoint", "text": " \nabstract load_bytes(read_item, value) [source]\n \nLoad the item described by read_item``and ``value. This method is expected to modify in-place the underlying state_dict. The contents of value are defined by the SavePlanner used to produce the checkpoint being loaded. \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.resolve_tensor()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.resolve_tensor", "type": "Distributed Checkpoint", "text": " \nabstract resolve_tensor(read_item) [source]\n \nReturn the tensor described by read_item to be used by the StorageReader to load read_item. The tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that\u2019s not possible, the planner can use the commit_tensor method to copy the data back to the one in state_dict.  Return type \nTensor   \n"}, {"name": "torch.distributed.checkpoint.LoadPlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.LoadPlanner.set_up_planner", "type": "Distributed Checkpoint", "text": " \nabstract set_up_planner(state_dict, metadata, is_coordinator) [source]\n \nInitialize this instance to load data into state_dict . N.B. This is called on every rank. \n"}, {"name": "torch.distributed.checkpoint.ReadItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.ReadItem", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.ReadItem(type: torch.distributed.checkpoint.planner.LoadItemType, dest_index: torch.distributed.checkpoint.metadata.MetadataIndex, dest_offsets: torch.Size, storage_index: torch.distributed.checkpoint.metadata.MetadataIndex, storage_offsets: torch.Size, lengths: torch.Size) [source]\n\n"}, {"name": "torch.distributed.checkpoint.save_state_dict()", "path": "distributed.checkpoint#torch.distributed.checkpoint.save_state_dict", "type": "Distributed Checkpoint", "text": " \ntorch.distributed.checkpoint.save_state_dict(state_dict, storage_writer, process_group=None, coordinator_rank=0, no_dist=False, planner=None) [source]\n \nSaves a distributed model in SPMD style. This function is different from torch.save() as it handles ShardedTensor by having each rank only save their local shards.  Warning There is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.   Warning If using the process_group argument, make sure that only its ranks call save_state_dict and that all data in state_dict belong to it.   Note When saving checkpoint for FSDP\u2019s ShardingStrategy.HYBRID_SHARD, only one of the shard_group should be calling save_state_dict and the corresponding process group needs to be passed in.   Note This function can be used to save a state_dict without having a process group initialized by passing no_dist=True.   Parameters \n \nstate_dict (Dict[str, Any]) \u2013 The state_dict to save. \nstorage_writer (StorageWriter) \u2013 Instance of StorageWrite use to perform writes. \nprocess_group (ProcessGroup) \u2013 ProcessGroup to be used for cross-rank synchronization. \ncoordinator_rank (int) \u2013 Rank to use to coordinate the checkpoint. rank0 is used by default. \nno_dist (bool) \u2013 If True, distributed checkpoint will not save in SPMD style. (Default: False)   Returns \nMetadata object for the saved checkpoint.  Return type \nMetadata   Example >>> my_model = MyModule()\n >>> model_state_dict = my_model.state_dict()\n >>> fs_storage_writer = torch.distributed.checkpoint.FileSystemWriter(\"/checkpoint/1\")\n>>> torch.distributed.checkpoint.save_state_dict(\n>>>     state_dict=model_state_dict,\n>>>     storage_writer=fs_storage_writer,\n>>> )\n  Note save_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsibility to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().  \n"}, {"name": "torch.distributed.checkpoint.SavePlan", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlan", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.SavePlan(items: List[torch.distributed.checkpoint.planner.WriteItem], storage_data: Any = None, planner_data: Any = None) [source]\n\n"}, {"name": "torch.distributed.checkpoint.SavePlanner", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.SavePlanner [source]\n \nAbstract class defining the protocol used by save_state_dict to plan the save process. SavePlanners are stateful objects that can be used to customize the whole save process. SavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process. A planner subclass can expect the following sequence of calls during save_state_dict:  \n set_up_planner - called on all ranks.\n\nSignals the start of a checkpoint save.    \n create_local_plan - called on all ranks.\n\nProcess the state_dict and produces a SavePlan that will be sent for global planning.    \n create_global_plan - called on the coordinator rank only.\n\nTakes the SavePlan from all ranks and make any global decision.    \n finish_plan - called on all ranks.\n\nThis gives each rank a chance to adjust to global planning decisions.    \n resolve_data - called multiple times on each rank\n\nLookups a value on the state_dict for the storage layer to write.     Users are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method. There are 3 usual patterns of extension: Rewriting state_dict. This is the simplest way to extend the save process as it doesn\u2019t requite understanding the intrincacies of how SavePlan works: >>> class RenamePlanner(DefaultSavePlanner):\n>>>     def set_up_planner(self, state_dict, is_coordinator):\n>>>         # prefix all keys with `foo_``\n>>>         super().set_up_planner({\"foo_\" + k: v for k, v in state_dict.items()}, is_coordinator)\n Modifying local plan and lookup in tandem. This is useful when fine control of how data is persisted >>> class FP16Planner(DefaultSavePlanner):\n>>>     def create_local_plan(self):\n>>>         plan = super().create_local_plan()\n>>>         for p in plan:\n>>>             if p.tensor_data is not None:\n>>>                 p.tensor_data.properties.dtype = torch.float16\n>>>         return plan\n>>>\n>>>     def resolve_data(self, write_item):\n>>>         item = super().resolve_data(write_item)\n>>>         return item if write_item.type == WriteItemType.BYTE_IO else item.to(torch.float16)\n Using the global planning step to make central decisions that can\u2019t be made individually by each rank >>> from itertools import islice\n>>> from dataclasses import replace\n>>> class DDPLoadBalancingPlanner(DefaultSavePlanner):\n>>>     # This uses the default local plan behavior of having all non-sharded writes in rank 0\n>>>     # This sample doesn't handle ShardedTensors\n>>>     def create_global_plan(self, all_plans):\n>>>         def chunk(it, size):\n>>>             it = iter(it)\n>>>         return list(iter(lambda: tuple(islice(it, size)), ()))\n>>>         all_plans = [\n>>>             replace(plan, items=items) for plan, items in\n>>>                 zip(all_plans, chunk(all_plans[0].items, len(all_plans)))\n>>>         ]\n>>>         return super().create_global_plan(all_plans)\n Finally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them: >>> class SaveExtraDataPlanner(DefaultSavePlanner):\n>>>     def create_local_plan(self) -> SavePlan:\n>>>         plan = super().create_local_plan()\n>>>         return replace(plan, planner_data=\"per-rank-data\")\n>>>\n>>>     def create_global_plan(self, all_plans: List[SavePlan]) -> Tuple[List[SavePlan], Metadata]:\n>>>         global_plan, metadata = super().create_global_plan(all_plans)\n>>>         merged_data = [p.planner_data for p in global_plan]\n>>>         metadata = replace(metadata, planner_data=merged_data)\n>>>         return global_plan, metadata\n  \nabstract create_global_plan(all_plans) [source]\n \nCompute the global checkpoint plan and return the local plan of each rank. This is called on the coordinator rank only.  Return type \nTuple[List[SavePlan], Metadata]   \n  \nabstract create_local_plan() [source]\n \nCompute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data. This is called on all ranks.  Return type \nSavePlan   \n  \nabstract finish_plan(new_plan) [source]\n \nMerge the plan created by create_local_plan and the result of create_global_plan. This is called on all ranks.  Return type \nSavePlan   \n  \nabstract resolve_data(write_item) [source]\n \nLookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it. Called on each rank multiple times, at least once per WriteItem in the final SavePlan. This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need. Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing. When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.  Return type \nUnion[Tensor, BytesIO]   \n  \nabstract set_up_planner(state_dict, is_coordinator) [source]\n \nInitialize this planner to save state_dict. Implementations should save those values as they won\u2019t be provided lated in the save process. This is called on all ranks. \n \n"}, {"name": "torch.distributed.checkpoint.SavePlanner.create_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_global_plan", "type": "Distributed Checkpoint", "text": " \nabstract create_global_plan(all_plans) [source]\n \nCompute the global checkpoint plan and return the local plan of each rank. This is called on the coordinator rank only.  Return type \nTuple[List[SavePlan], Metadata]   \n"}, {"name": "torch.distributed.checkpoint.SavePlanner.create_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.create_local_plan", "type": "Distributed Checkpoint", "text": " \nabstract create_local_plan() [source]\n \nCompute the save plan for the current rank. This will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data. This is called on all ranks.  Return type \nSavePlan   \n"}, {"name": "torch.distributed.checkpoint.SavePlanner.finish_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.finish_plan", "type": "Distributed Checkpoint", "text": " \nabstract finish_plan(new_plan) [source]\n \nMerge the plan created by create_local_plan and the result of create_global_plan. This is called on all ranks.  Return type \nSavePlan   \n"}, {"name": "torch.distributed.checkpoint.SavePlanner.resolve_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.resolve_data", "type": "Distributed Checkpoint", "text": " \nabstract resolve_data(write_item) [source]\n \nLookup the object associated with write_item in state_dict and apply any transformation (such as serialization) prior to the storage layer consuming it. Called on each rank multiple times, at least once per WriteItem in the final SavePlan. This method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need. Any transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing. When returning tensors, they can be on any device or format, they can be views too. It\u2019s the storage layer responsibility to figure out how to save them.  Return type \nUnion[Tensor, BytesIO]   \n"}, {"name": "torch.distributed.checkpoint.SavePlanner.set_up_planner()", "path": "distributed.checkpoint#torch.distributed.checkpoint.SavePlanner.set_up_planner", "type": "Distributed Checkpoint", "text": " \nabstract set_up_planner(state_dict, is_coordinator) [source]\n \nInitialize this planner to save state_dict. Implementations should save those values as they won\u2019t be provided lated in the save process. This is called on all ranks. \n"}, {"name": "torch.distributed.checkpoint.StorageReader", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.StorageReader [source]\n \nInterface used by load_state_dict to read from storage. One StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role. A subclass should expected the following sequence of calls by load_state_dict:  (all ranks) read_metadata() (all ranks) set_up_storage_reader() (all ranks) prepare_local_plan() (coordinator) prepare_global_plan() (all ranks) read_data()   \nabstract prepare_global_plan(plans) [source]\n \nPerform centralized planning of storage loading. This method is only called on the coordinator instance. While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.  Parameters \nplans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.  Returns \nA list of transformed LoadPlan after storage global planning  Return type \nList[LoadPlan]   \n  \nabstract prepare_local_plan(plan) [source]\n \nPerform storage-specific local planning. While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.  Parameters \nplan (LoadPlan) \u2013 The local plan from the LoadPlan in use.  Returns \nA transformed LoadPlan after storage local planning  Return type \nLoadPlan   \n  \nabstract read_data(plan, planner) [source]\n \nReads all items from plan using planner to resolve the data. A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place. A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into. It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.  Parameters \n \nplan (LoadPlan) \u2013 The local plan to execute on \nplanner (LoadPlanner) \u2013 The planner object to use to resolve items.   Returns \nA future that completes once all reads are finished.  Return type \nFuture[None]   \n  \nabstract read_metadata() [source]\n \nReads the checkpoint metadata.  Returns \nThe metadata object associated with the checkpoint being loaded.  Return type \nMetadata   \n  \nabstract set_up_storage_reader(metadata, is_coordinator) [source]\n \nInitialize this instance.  Parameters \n \nmetadata (Metadata) \u2013 The metadata schema to use. \nis_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.    \n \n"}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_global_plan", "type": "Distributed Checkpoint", "text": " \nabstract prepare_global_plan(plans) [source]\n \nPerform centralized planning of storage loading. This method is only called on the coordinator instance. While this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.  Parameters \nplans (List[LoadPlan]) \u2013 A list of LoadPlan instances, one for each rank.  Returns \nA list of transformed LoadPlan after storage global planning  Return type \nList[LoadPlan]   \n"}, {"name": "torch.distributed.checkpoint.StorageReader.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.prepare_local_plan", "type": "Distributed Checkpoint", "text": " \nabstract prepare_local_plan(plan) [source]\n \nPerform storage-specific local planning. While this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.  Parameters \nplan (LoadPlan) \u2013 The local plan from the LoadPlan in use.  Returns \nA transformed LoadPlan after storage local planning  Return type \nLoadPlan   \n"}, {"name": "torch.distributed.checkpoint.StorageReader.read_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_data", "type": "Distributed Checkpoint", "text": " \nabstract read_data(plan, planner) [source]\n \nReads all items from plan using planner to resolve the data. A subclass should call LoadPlanner::load_bytes to deserialize a BytesIO object into the right place. A subclass should call LoadPlanner::resolve_tensor to get access to the tensors that in should load data into. It\u2019s the StorageLayer responsibility to properly schedule any cross device copies required.  Parameters \n \nplan (LoadPlan) \u2013 The local plan to execute on \nplanner (LoadPlanner) \u2013 The planner object to use to resolve items.   Returns \nA future that completes once all reads are finished.  Return type \nFuture[None]   \n"}, {"name": "torch.distributed.checkpoint.StorageReader.read_metadata()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.read_metadata", "type": "Distributed Checkpoint", "text": " \nabstract read_metadata() [source]\n \nReads the checkpoint metadata.  Returns \nThe metadata object associated with the checkpoint being loaded.  Return type \nMetadata   \n"}, {"name": "torch.distributed.checkpoint.StorageReader.set_up_storage_reader()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageReader.set_up_storage_reader", "type": "Distributed Checkpoint", "text": " \nabstract set_up_storage_reader(metadata, is_coordinator) [source]\n \nInitialize this instance.  Parameters \n \nmetadata (Metadata) \u2013 The metadata schema to use. \nis_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.    \n"}, {"name": "torch.distributed.checkpoint.StorageWriter", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.StorageWriter [source]\n \nInterface used by save_state_dict to write to storage. One StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role. A subclass should expect the following sequence of calls.  (all ranks) set_up_storage_writer() (all ranks) prepare_local_plan() (coordinator) prepare_global_plan() (all ranks) write_data() (coordinator) finish()   \nabstract finish(metadata, results) [source]\n \nWrites the metadata and marks the current checkpoint as successful. The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.  Parameters \n \nmetadata (Metadata) \u2013 metadata for the new checkpoint \nresults (List[List[WriteResult]]) \u2013 A list of WriteResults from all ranks.   Returns \nNone  Return type \nNone   \n  \nabstract prepare_global_plan(plans) [source]\n \nPerform centralized planning of storage. This method is only called on the coordinator instance. While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.  Parameters \nplans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.  Returns \nA list of transformed SavePlan after storage global planning  Return type \nList[SavePlan]   \n  \nabstract prepare_local_plan(plan) [source]\n \nPerform storage-specific local planning. While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.  Parameters \nplan (SavePlan) \u2013 The local plan from the SavePlanner in use.  Returns \nA transformed SavePlan after storage local planning  Return type \nSavePlan   \n  \nabstract set_up_storage_writer(is_coordinator) [source]\n \nInitialize this instance.  Parameters \nis_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.   \n  \nabstract write_data(plan, planner) [source]\n \nWrite all items from plan using planner to resolve the data. A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write. Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:  They might be on any device, including not matching the one on WriteItem::tensor_data\n They might be views or not contiguous. Only the projection needs to be saved.   Parameters \n \nplan (SavePlan) \u2013 The save plan to execute. \nplanner (SavePlanner) \u2013 Planner object to be used to resolve items to data.   Returns \nA future that completes to a list of WriteResult  Return type \nFuture[List[WriteResult]]   \n \n"}, {"name": "torch.distributed.checkpoint.StorageWriter.finish()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.finish", "type": "Distributed Checkpoint", "text": " \nabstract finish(metadata, results) [source]\n \nWrites the metadata and marks the current checkpoint as successful. The actual format/schema used for serializing metadata is an implementation detail. The only requirement is that it\u2019s recoverable in to the same object graph.  Parameters \n \nmetadata (Metadata) \u2013 metadata for the new checkpoint \nresults (List[List[WriteResult]]) \u2013 A list of WriteResults from all ranks.   Returns \nNone  Return type \nNone   \n"}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_global_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_global_plan", "type": "Distributed Checkpoint", "text": " \nabstract prepare_global_plan(plans) [source]\n \nPerform centralized planning of storage. This method is only called on the coordinator instance. While this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.  Parameters \nplans (List[SavePlan]) \u2013 A list of SavePlan instances, one for each rank.  Returns \nA list of transformed SavePlan after storage global planning  Return type \nList[SavePlan]   \n"}, {"name": "torch.distributed.checkpoint.StorageWriter.prepare_local_plan()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.prepare_local_plan", "type": "Distributed Checkpoint", "text": " \nabstract prepare_local_plan(plan) [source]\n \nPerform storage-specific local planning. While this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.  Parameters \nplan (SavePlan) \u2013 The local plan from the SavePlanner in use.  Returns \nA transformed SavePlan after storage local planning  Return type \nSavePlan   \n"}, {"name": "torch.distributed.checkpoint.StorageWriter.set_up_storage_writer()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.set_up_storage_writer", "type": "Distributed Checkpoint", "text": " \nabstract set_up_storage_writer(is_coordinator) [source]\n \nInitialize this instance.  Parameters \nis_coordinator (bool) \u2013 Whether this instance is responsible for coordinating the checkpoint.   \n"}, {"name": "torch.distributed.checkpoint.StorageWriter.write_data()", "path": "distributed.checkpoint#torch.distributed.checkpoint.StorageWriter.write_data", "type": "Distributed Checkpoint", "text": " \nabstract write_data(plan, planner) [source]\n \nWrite all items from plan using planner to resolve the data. A subclass should call SavePlanner::resolve_data on each item from the plan to get access to the underlying object to write. Subclasses should lazily call resolve_data as it can allocate memory. In case of tensors, make following assumptions:  They might be on any device, including not matching the one on WriteItem::tensor_data\n They might be views or not contiguous. Only the projection needs to be saved.   Parameters \n \nplan (SavePlan) \u2013 The save plan to execute. \nplanner (SavePlanner) \u2013 Planner object to be used to resolve items to data.   Returns \nA future that completes to a list of WriteResult  Return type \nFuture[List[WriteResult]]   \n"}, {"name": "torch.distributed.checkpoint.WriteItem", "path": "distributed.checkpoint#torch.distributed.checkpoint.WriteItem", "type": "Distributed Checkpoint", "text": " \nclass torch.distributed.checkpoint.WriteItem(index: torch.distributed.checkpoint.metadata.MetadataIndex, type: torch.distributed.checkpoint.planner.WriteItemType, tensor_data: Union[torch.distributed.checkpoint.planner.TensorWriteData, NoneType] = None) [source]\n\n"}, {"name": "torch.distributed.DistBackendError", "path": "distributed#torch.distributed.DistBackendError", "type": "Distributed Communication", "text": " \nclass torch.distributed.DistBackendError  \nException raised when a backend error occurs in distributed \n"}, {"name": "torch.distributed.elastic.agent.server.api.RunResult", "path": "elastic/agent#torch.distributed.elastic.agent.server.api.RunResult", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.api.RunResult(state, return_values=<factory>, failures=<factory>) [source]\n \nResults returned by the worker executions. Run results follow an \u201call-or-nothing\u201d policy where the run is successful if and only if ALL local workers managed by this agent complete successfully. If the result is successful (e.g. is_failed() = False) then the return_values field contains the outputs (return values) of the workers managed by THIS agent mapped by their GLOBAL ranks. That is result.return_values[0] is the return value of global rank 0.  Note return_values are only meaningful for when the worker entrypoint is a function. Workers specified as a binary entrypoint do not canonically have a return value and the return_values field is meaningless and may be empty.  If is_failed() returns True then the failures field contains the failure information, again, mapped by the GLOBAL rank of the worker that failed. The keys in return_values and failures are mutually exclusive, that is, a worker\u2019s final state can only be one of: succeeded, failed. Workers intentionally terminated by the agent according to the agent\u2019s restart policy, are not represented in either return_values nor failures. \n"}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.ElasticAgent [source]\n \nAgent process responsible for managing one or more worker processes. The worker processes are assumed to be regular distributed PyTorch scripts. When the worker process is created by the agent, the agent provides the necessary information for the worker processes to properly initialize a torch process group. The exact deployment topology and ratio of agent-to-worker is dependent on the specific implementation of the agent and the user\u2019s job placement preferences. For instance, to run a distributed training job on GPU with 8 trainers (one per GPU) one can:  Use 8 x single GPU instances, place an agent per instance, managing 1 worker per agent. Use 4 x double GPU instances, place an agent per instance, managing 2 workers per agent. Use 2 x quad GPU instances, place an agent per instance, managing 4 workers per agent. Use 1 x 8 GPU instance, place an agent per instance, managing 8 workers per agent.  Usage group_result = agent.run()\n if group_result.is_failed():\n   # workers failed\n   failure = group_result.failures[0]\n   log.exception(\"worker 0 failed with exit code : %s\", failure.exit_code)\n else:\n   return group_result.return_values[0] # return rank 0's results\n  \nabstract get_worker_group(role='default') [source]\n \n Returns \nThe WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.  Return type \nWorkerGroup   \n  \nabstract run(role='default') [source]\n \nRuns the agent, retrying the worker group on failures up to max_restarts.  Returns \nThe result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.  Raises \nException - any other failures NOT related to worker process \u2013   Return type \nRunResult   \n \n"}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.get_worker_group", "type": "Distributed Elastic", "text": " \nabstract get_worker_group(role='default') [source]\n \n Returns \nThe WorkerGroup for the given role. Note that the worker group is a mutable object and hence in a multi-threaded/process environment it may change state. Implementors are encouraged (but not required) to return a defensive read-only copy.  Return type \nWorkerGroup   \n"}, {"name": "torch.distributed.elastic.agent.server.ElasticAgent.run()", "path": "elastic/agent#torch.distributed.elastic.agent.server.ElasticAgent.run", "type": "Distributed Elastic", "text": " \nabstract run(role='default') [source]\n \nRuns the agent, retrying the worker group on failures up to max_restarts.  Returns \nThe result of the execution, containing the return values or failure details for each worker mapped by the worker\u2019s global rank.  Raises \nException - any other failures NOT related to worker process \u2013   Return type \nRunResult   \n"}, {"name": "torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.local_elastic_agent.LocalElasticAgent(spec, start_method='spawn', exit_barrier_timeout=300, log_dir=None) [source]\n \nAn implementation of torchelastic.agent.server.ElasticAgent that handles host-local workers. This agent is deployed per host and is configured to spawn n workers. When using GPUs, n maps to the number of GPUs available on the host. The local agent does not communicate to other local agents deployed on other hosts, even if the workers may communicate inter-host. The worker id is interpreted to be a local process. The agent starts and stops all worker processes as a single unit. The worker function and argument passed to the worker function must be python multiprocessing compatible. To pass multiprocessing data structures to the workers you may create the data structure in the same multiprocessing context as the specified start_method and pass it as a function argument. The exit_barrier_timeout specifies the amount of time (in seconds) to wait for other agents to finish. This acts as a safety net to handle cases where workers finish at different times, to prevent agents from viewing workers that finished early as a scale-down event. It is strongly advised that the user code deal with ensuring that workers are terminated in a synchronous manner rather than relying on the exit_barrier_timeout. A named pipe based watchdog can be enabled in `LocalElasticAgent` if an environment variable TORCHELASTIC_ENABLE_FILE_TIMER with value 1 has been defined in the `LocalElasticAgent` process. Optionally, another environment variable `TORCHELASTIC_TIMER_FILE` can be set with a unique file name for the named pipe. If the environment variable `TORCHELASTIC_TIMER_FILE` is not set, `LocalElasticAgent` will internally create a unique file name and set it to the environment variable `TORCHELASTIC_TIMER_FILE`, and this environment variable will be propagated to the worker processes to allow them to connect to the same named pipe that `LocalElasticAgent` uses. Example launching function def trainer(args) -> str:\n    return \"do train\"\n\ndef main():\n    start_method=\"spawn\"\n    shared_queue= multiprocessing.get_context(start_method).Queue()\n    spec = WorkerSpec(\n                role=\"trainer\",\n                local_world_size=nproc_per_process,\n                entrypoint=trainer,\n                args=(\"foobar\",),\n                ...<OTHER_PARAMS...>)\n    agent = LocalElasticAgent(spec, start_method)\n    results = agent.run()\n\n    if results.is_failed():\n        print(\"trainer failed\")\n    else:\n        print(f\"rank 0 return value: {results.return_values[0]}\")\n        # prints -> rank 0 return value: do train\n Example launching binary def main():\n    spec = WorkerSpec(\n                role=\"trainer\",\n                local_world_size=nproc_per_process,\n                entrypoint=\"/usr/local/bin/trainer\",\n                args=(\"--trainer-args\", \"foobar\"),\n                ...<OTHER_PARAMS...>)\n    agent = LocalElasticAgent(spec)\n    results = agent.run()\n\n    if not results.is_failed():\n        print(\"binary launches do not have return values\")\n \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.SimpleElasticAgent(spec, exit_barrier_timeout=300) [source]\n \nAn ElasticAgent that manages workers (WorkerGroup) for a single WorkerSpec (e.g. one particular type of worker role).  \n_assign_worker_ranks(store, group_rank, group_world_size, spec) [source]\n \nDetermines proper ranks for worker processes. The rank assignment is done according to the following algorithm:  Each agent writes its configuration(group_rank, group_world_size , num_workers) to the common store. Each agent retrieves configuration for all agents and performs two level sort using role and rank. Determine the global rank: the global rank of the workers for the current agent is the offset of the infos array up to group_rank of the agent. The offset is computed as a sum of local_world_size of all agents that have rank less than the group_rank. The workers would have the ranks: [offset, offset+local_world_size) Determine the role rank: The role rank is determined using the algorithms in the point 3 with the exception that the offset is done from the first agent that has the same role as current one and has the minimum group rank.   Return type \nList[Worker]   \n  \n_exit_barrier() [source]\n \nWait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish. \n  \n_initialize_workers(worker_group) [source]\n \nStarts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers. The caller should first call _stop_workers() to stop running workers prior to calling this method. Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method \n  \nabstract _monitor_workers(worker_group) [source]\n \nChecks on the workers for the worker_group and returns the new state of the worker group.  Return type \nRunResult   \n  \n_rendezvous(worker_group) [source]\n \nRuns rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group. \n  \n_restart_workers(worker_group) [source]\n \nRestarts (stops, rendezvous, starts) all local workers in the group. \n  \nabstract _shutdown(death_sig=Signals.SIGTERM) [source]\n \nCleans up any resources that were allocated during the agent\u2019s work.  Parameters \ndeath_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default   \n  \nabstract _start_workers(worker_group) [source]\n \nStarts worker_group.spec.local_world_size number of workers according to worker spec for the worker group . Returns a map of local_rank to worker id.  Return type \nDict[int, Any]   \n  \nabstract _stop_workers(worker_group) [source]\n \nStops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc. \n \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._assign_worker_ranks", "type": "Distributed Elastic", "text": " \n_assign_worker_ranks(store, group_rank, group_world_size, spec) [source]\n \nDetermines proper ranks for worker processes. The rank assignment is done according to the following algorithm:  Each agent writes its configuration(group_rank, group_world_size , num_workers) to the common store. Each agent retrieves configuration for all agents and performs two level sort using role and rank. Determine the global rank: the global rank of the workers for the current agent is the offset of the infos array up to group_rank of the agent. The offset is computed as a sum of local_world_size of all agents that have rank less than the group_rank. The workers would have the ranks: [offset, offset+local_world_size) Determine the role rank: The role rank is determined using the algorithms in the point 3 with the exception that the offset is done from the first agent that has the same role as current one and has the minimum group rank.   Return type \nList[Worker]   \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._exit_barrier", "type": "Distributed Elastic", "text": " \n_exit_barrier() [source]\n \nWait for exit_barrier_timeout seconds for all agents to finish executing their local workers (either successfully or not). This acts as a safety guard against user scripts that terminate at different times. This barrier keeps the agent process alive until all workers finish. \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._initialize_workers", "type": "Distributed Elastic", "text": " \n_initialize_workers(worker_group) [source]\n \nStarts a fresh set of workers for the worker_group. Essentially a rendezvous followed by a start_workers. The caller should first call _stop_workers() to stop running workers prior to calling this method. Optimistically sets the state of the worker group that just started as HEALTHY and delegates the actual monitoring of state to _monitor_workers() method \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._monitor_workers", "type": "Distributed Elastic", "text": " \nabstract _monitor_workers(worker_group) [source]\n \nChecks on the workers for the worker_group and returns the new state of the worker group.  Return type \nRunResult   \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._rendezvous", "type": "Distributed Elastic", "text": " \n_rendezvous(worker_group) [source]\n \nRuns rendezvous for the workers specified by worker spec. Assigns workers a new global rank and world size. Updates the rendezvous store for the worker group. \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._restart_workers", "type": "Distributed Elastic", "text": " \n_restart_workers(worker_group) [source]\n \nRestarts (stops, rendezvous, starts) all local workers in the group. \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._shutdown", "type": "Distributed Elastic", "text": " \nabstract _shutdown(death_sig=Signals.SIGTERM) [source]\n \nCleans up any resources that were allocated during the agent\u2019s work.  Parameters \ndeath_sig (Signals) \u2013 Signal to send to the child process, SIGTERM is default   \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._start_workers", "type": "Distributed Elastic", "text": " \nabstract _start_workers(worker_group) [source]\n \nStarts worker_group.spec.local_world_size number of workers according to worker spec for the worker group . Returns a map of local_rank to worker id.  Return type \nDict[int, Any]   \n"}, {"name": "torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers()", "path": "elastic/agent#torch.distributed.elastic.agent.server.SimpleElasticAgent._stop_workers", "type": "Distributed Elastic", "text": " \nabstract _stop_workers(worker_group) [source]\n \nStops all workers in the given worker group. Implementors must deal with workers in all states defined by WorkerState. That is, it must gracefully handle stopping non-existent workers, unhealthy (stuck) workers, etc. \n"}, {"name": "torch.distributed.elastic.agent.server.Worker", "path": "elastic/agent#torch.distributed.elastic.agent.server.Worker", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.Worker(local_rank, global_rank=-1, role_rank=-1, world_size=-1, role_world_size=-1) [source]\n \nRepresents a worker instance. Contrast this with WorkerSpec that represents the specifications of a worker. A Worker is created from a WorkerSpec. A Worker is to a WorkerSpec as an object is to a class. The id of the worker is interpreted by the specific implementation of ElasticAgent. For a local agent, it could be the pid (int) of the worker, for a remote agent it could be encoded as host:port (string).  Parameters \n \nid (Any) \u2013 uniquely identifies a worker (interpreted by the agent) \nlocal_rank (int) \u2013 local rank of the worker \nglobal_rank (int) \u2013 global rank of the worker \nrole_rank (int) \u2013 rank of the worker across all workers that have the same role \nworld_size (int) \u2013 number of workers (globally) \nrole_world_size (int) \u2013 number of workers that have the same role    \n"}, {"name": "torch.distributed.elastic.agent.server.WorkerGroup", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerGroup", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.WorkerGroup(spec) [source]\n \nRepresents the set of Worker instances for the given WorkerSpec managed by ElasticAgent. Whether the worker group contains cross instance workers or not depends on the implementation of the agent. \n"}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.WorkerSpec(role, local_world_size, rdzv_handler, fn=None, entrypoint=None, args=(), max_restarts=3, monitor_interval=30.0, master_port=None, master_addr=None, local_addr=None, redirects=Std.NONE, tee=Std.NONE) [source]\n \nContains blueprint information about a particular type of worker. For a given role, there must only exist a single worker spec. Worker spec is expected to be homogeneous across all nodes (machine), that is each node runs the same number of workers for a particular spec.  Parameters \n \nrole (str) \u2013 user-defined role for the workers with this spec \nlocal_world_size (int) \u2013 number local workers to run \nfn (Optional[Callable]) \u2013 (deprecated use entrypoint instead) \nentrypoint (Optional[Union[Callable, str]]) \u2013 worker function or command \nargs (Tuple) \u2013 arguments to pass to entrypoint\n \nrdzv_handler (RendezvousHandler) \u2013 handles rdzv for this set of workers \nmax_restarts (int) \u2013 number of max retries for the workers \nmonitor_interval (float) \u2013 monitor status of workers every n seconds \nmaster_port (Optional[int]) \u2013 fixed port to run the c10d store on rank 0 if not specified then will chose a random free port \nmaster_addr (Optional[str]) \u2013 fixed master_addr to run the c10d store on rank 0 if not specified then will chose hostname on agent rank 0 \nredirects (Union[Std, Dict[int, Std]]) \u2013 redirect std streams to a file, selectively redirect for a particular local rank by passing a map \ntee (Union[Std, Dict[int, Std]]) \u2013 tees the specified std stream(s) to console + file, selectively tee for a particular local rank by passing a map, takes precedence over redirects settings.     \nget_entrypoint_name() [source]\n \nIf the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name. \n \n"}, {"name": "torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerSpec.get_entrypoint_name", "type": "Distributed Elastic", "text": " \nget_entrypoint_name() [source]\n \nIf the entrypoint is a function (e.g. Callable) returns its __qualname__, else if the entrypoint is a binary (e.g. str), returns the binary name. \n"}, {"name": "torch.distributed.elastic.agent.server.WorkerState", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.agent.server.WorkerState(value) [source]\n \nState of the WorkerGroup. Workers in a worker group change state as a unit. If a single worker in a worker group fails the entire set is considered failed: UNKNOWN - agent lost track of worker group state, unrecoverable\nINIT - worker group object created not yet started\nHEALTHY - workers running and healthy\nUNHEALTHY - workers running and unhealthy\nSTOPPED - workers stopped (interrupted) by the agent\nSUCCEEDED - workers finished running (exit 0)\nFAILED - workers failed to successfully finish (exit !0)\n A worker group starts from an initial INIT state, then progresses to HEALTHY or UNHEALTHY states, and finally reaches a terminal SUCCEEDED or FAILED state. Worker groups can be interrupted and temporarily put into STOPPED state by the agent. Workers in STOPPED state are scheduled to be restarted in the near future by the agent. Some examples of workers being put into STOPPED state are:  Worker group failure|unhealthy observed Membership change detected  When actions (start, stop, rdzv, retry, etc) on worker group fails and results in the action being partially applied to the worker group the state will be UNKNOWN. Typically this happens on uncaught/unhandled exceptions during state change events on the agent. The agent is not expected to recover worker groups in UNKNOWN state and is better off self terminating and allowing the job manager to retry the node.  \nstatic is_running(state) [source]\n \n Returns \nTrue if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).  Return type \nbool   \n \n"}, {"name": "torch.distributed.elastic.agent.server.WorkerState.is_running()", "path": "elastic/agent#torch.distributed.elastic.agent.server.WorkerState.is_running", "type": "Distributed Elastic", "text": " \nstatic is_running(state) [source]\n \n Returns \nTrue if the worker state represents workers still running (e.g. that the process exists but not necessarily healthy).  Return type \nbool   \n"}, {"name": "torch.distributed.elastic.events.api.Event", "path": "elastic/events#torch.distributed.elastic.events.api.Event", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.events.api.Event(name, source, timestamp=0, metadata=<factory>) [source]\n \nThe class represents the generic event that occurs during the torchelastic job execution. The event can be any kind of meaningful action.  Parameters \n \nname (str) \u2013 event name. \nsource (EventSource) \u2013 the event producer, e.g. agent or worker \ntimestamp (int) \u2013 timestamp in milliseconds when event occurred. \nmetadata (Dict[str, Optional[Union[str, int, float, bool]]]) \u2013 additional data that is associated with the event.    \n"}, {"name": "torch.distributed.elastic.events.api.EventMetadataValue", "path": "elastic/events#torch.distributed.elastic.events.api.EventMetadataValue", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.events.api.EventMetadataValue  \nalias of Optional[Union[str, int, float, bool]] \n"}, {"name": "torch.distributed.elastic.events.api.EventSource", "path": "elastic/events#torch.distributed.elastic.events.api.EventSource", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.events.api.EventSource(value) [source]\n \nKnown identifiers of the event producers. \n"}, {"name": "torch.distributed.elastic.events.get_logging_handler()", "path": "elastic/events#torch.distributed.elastic.events.get_logging_handler", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.events.get_logging_handler(destination='null') [source]\n \n Return type \nHandler   \n"}, {"name": "torch.distributed.elastic.events.record()", "path": "elastic/events#torch.distributed.elastic.events.record", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.events.record(event, destination='null') [source]\n\n"}, {"name": "torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.ConsoleMetricHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.metrics.api.ConsoleMetricHandler [source]\n\n"}, {"name": "torch.distributed.elastic.metrics.api.MetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.MetricHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.metrics.api.MetricHandler [source]\n\n"}, {"name": "torch.distributed.elastic.metrics.api.NullMetricHandler", "path": "elastic/metrics#torch.distributed.elastic.metrics.api.NullMetricHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.metrics.api.NullMetricHandler [source]\n\n"}, {"name": "torch.distributed.elastic.metrics.configure()", "path": "elastic/metrics#torch.distributed.elastic.metrics.configure", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.metrics.configure(handler, group=None) [source]\n\n"}, {"name": "torch.distributed.elastic.metrics.prof()", "path": "elastic/metrics#torch.distributed.elastic.metrics.prof", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.metrics.prof(fn=None, group='torchelastic') [source]\n \n@profile decorator publishes duration.ms, count, success, failure metrics for the function that it decorates. The metric name defaults to the qualified name (class_name.def_name) of the function. If the function does not belong to a class, it uses the leaf module name instead. Usage @metrics.prof\ndef x():\n    pass\n\n@metrics.prof(group=\"agent\")\ndef y():\n    pass\n \n"}, {"name": "torch.distributed.elastic.metrics.put_metric()", "path": "elastic/metrics#torch.distributed.elastic.metrics.put_metric", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.metrics.put_metric(metric_name, metric_value, metric_group='torchelastic') [source]\n \nPublishes a metric data point. Usage put_metric(\"metric_name\", 1)\nput_metric(\"metric_name\", 1, \"metric_group_name\")\n \n"}, {"name": "torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.MultiprocessContext", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.api.MultiprocessContext(name, entrypoint, args, envs, stdouts, stderrs, tee_stdouts, tee_stderrs, error_files, start_method) [source]\n \nPContext holding worker processes invoked as a function. \n"}, {"name": "torch.distributed.elastic.multiprocessing.api.PContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.PContext", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.api.PContext(name, entrypoint, args, envs, stdouts, stderrs, tee_stdouts, tee_stderrs, error_files) [source]\n \nThe base class that standardizes operations over a set of processes that are launched via different mechanisms. The name PContext is intentional to disambiguate with torch.multiprocessing.ProcessContext.  Warning stdouts and stderrs should ALWAYS be a superset of tee_stdouts and tee_stderrs (respectively) this is b/c tee is implemented as a redirect + tail -f <stdout/stderr.log>  \n"}, {"name": "torch.distributed.elastic.multiprocessing.api.RunProcsResult", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.RunProcsResult", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.api.RunProcsResult(return_values=<factory>, failures=<factory>, stdouts=<factory>, stderrs=<factory>) [source]\n \nResults of a completed run of processes started with start_processes(). Returned by PContext. Note the following:  All fields are mapped by local rank \nreturn_values - only populated for functions (not the binaries). \nstdouts - path to stdout.log (empty string if no redirect) \nstderrs - path to stderr.log (empty string if no redirect)  \n"}, {"name": "torch.distributed.elastic.multiprocessing.api.SubprocessContext", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.api.SubprocessContext", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.api.SubprocessContext(name, entrypoint, args, envs, stdouts, stderrs, tee_stdouts, tee_stderrs, error_files) [source]\n \nPContext holding worker processes invoked as a binary. \n"}, {"name": "torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ChildFailedError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.errors.ChildFailedError(name, failures) [source]\n \nSpecial exception type that can be raised from a function annotated with the @record decorator to have the child process\u2019 (root exception) propagate up the stack as-is (e.g. without being wrapped in the parent\u2019s traceback). Useful in cases where the parent is a simple nanny process and the child (worker) processes are actually doing meaningful compute. In this case, errors typically occur on the child process as the parent is not doing anything non-trivial, and child errors should be propagated to the scheduler for accurate root cause diagnostics.  Note The propagation relies on error files rather than exception handling to support both function and binary launches.  Example: # process tree on a host (container)\n0: scheduler-init-process:\n           |- 1: torchelastic_agent:\n                    |- 2: trainer_0 (ok)\n                    |- 3: trainer_1 (fail) -> error.json\n                    |- ...\n                    |- n+2: trainer_n (ok)\n           |- n+3: other processes\n           |- ...\n In the example above, trainer 1\u2019s failure (written into error.json) is the root cause and should be reported to the scheduler\u2019s init process. The torchelastic agent raises a ChildFailedError(\"trainer\", {1: \"trainer_1/error.json\"}) upon detecting trainer 1\u2019s failure which would propagate the contents of trainer 1\u2019s error file to the scheduler\u2019s init process. \n"}, {"name": "torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ErrorHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.errors.ErrorHandler [source]\n \nWrites the provided exception object along with some other metadata about the error in a structured way in JSON format to an error file specified by the environment variable: TORCHELASTIC_ERROR_FILE. If this environment variable is not set, then simply logs the contents of what would have been written to the error file. This handler may be subclassed to customize the handling of the error. Subclasses should override initialize() and record_exception(). \n"}, {"name": "torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.ProcessFailure", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.multiprocessing.errors.ProcessFailure(local_rank, pid, exitcode, error_file) [source]\n \nRepresents the failed process result. When the worker process fails, it may record failure root cause into the file. Tries to read the failure timestamp from the provided error_file, if the error_file does not exist, the timestamp is the current timestamp (seconds since epoch). The message field is a concise explanation of the failure. If the error file exists then the message is obtained from the error file. Otherwise one is generated based on the failure signature.  Note It is assumed that the error_file is written by torch.distributed.elastic.multiprocessing.errors.error_handler.ErrorHandler. Otherwise the behavior is undefined.  \n"}, {"name": "torch.distributed.elastic.multiprocessing.errors.record()", "path": "elastic/errors#torch.distributed.elastic.multiprocessing.errors.record", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.multiprocessing.errors.record(fn, error_handler=None) [source]\n \nSyntactic sugar to record errors/exceptions that happened in the decorated function using the provided error_handler. Using this decorator is equivalent to: error_handler = get_error_handler()\nerror_handler.initialize()\ntry:\n   foobar()\nexcept ChildFailedError as e:\n   _, failure = e.get_first_failure()\n   error_handler.dump_error_file(failure.error_file, failure.exitcode)\n   raise\nexcept Exception as e:\n   error_handler.record(e)\n   raise\n  Important use this decorator once per process at the top level method, typically this is the main method.  Example @record\ndef main():\n    pass\n\nif __name__==\"__main__\":\n   main()\n  Return type \nCallable[[\u2026], T]   \n"}, {"name": "torch.distributed.elastic.multiprocessing.start_processes()", "path": "elastic/multiprocessing#torch.distributed.elastic.multiprocessing.start_processes", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.multiprocessing.start_processes(name, entrypoint, args, envs, log_dir, start_method='spawn', redirects=Std.NONE, tee=Std.NONE) [source]\n \nStarts n copies of entrypoint processes with the provided options. entrypoint is either a Callable (function) or a str (binary). The number of copies is determined by the number of entries for args and envs arguments, which need to have the same key set. args and env parameters are the arguments and environment variables to pass down to the entrypoint mapped by the replica index (local rank). All local ranks must be accounted for. That is, the keyset should be {0,1,...,(nprocs-1)}.  Note When the entrypoint is a binary (str), args can only be strings. If any other type is given, then it is casted to a string representation (e.g. str(arg1)). Furthermore, a binary failure will only write an error.json error file if the main function is annotated with torch.distributed.elastic.multiprocessing.errors.record. For function launches, this is done by default and there is no need to manually annotate with the @record annotation.  redirects and tee are bitmasks specifying which std stream(s) to redirect to a log file in the log_dir. Valid mask values are defined in Std. To redirect/tee only certain local ranks, pass redirects as a map with the key as the local rank to specify the redirect behavior for. Any missing local ranks will default to Std.NONE. tee acts like the unix \u201ctee\u201d command in that it redirects + prints to console. To avoid worker stdout/stderr from printing to console, use the redirects parameter. For each process, the log_dir will contain:  \n{local_rank}/error.json: if the process failed, a file with the error info \n{local_rank}/stdout.json: if redirect & STDOUT == STDOUT\n \n{local_rank}/stderr.json: if redirect & STDERR == STDERR\n   Note It is expected that the log_dir exists, is empty, and is a directory.  Example: log_dir = \"/tmp/test\"\n\n# ok; two copies of foo: foo(\"bar0\"), foo(\"bar1\")\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n)\n\n# invalid; envs missing for local rank 1\nstart_processes(\n   name=\"trainer\",\n   entrypoint=foo,\n   args:{0:(\"bar0\",), 1:(\"bar1\",),\n   envs:{0:{}},\n   log_dir=log_dir\n)\n\n# ok; two copies of /usr/bin/touch: touch file1, touch file2\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/touch\",\n   args:{0:(\"file1\",), 1:(\"file2\",),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n\n# caution; arguments casted to string, runs:\n# echo \"1\" \"2\" \"3\" and echo \"[1, 2, 3]\"\nstart_processes(\n   name=\"trainer\",\n   entrypoint=\"/usr/bin/echo\",\n   args:{0:(1,2,3), 1:([1,2,3],),\n   envs:{0:{}, 1:{}},\n   log_dir=log_dir\n )\n  Parameters \n \nname (str) \u2013 a human readable short name that describes what the processes are (used as header when tee\u2019ing stdout/stderr outputs) \nentrypoint (Union[Callable, str]) \u2013 either a Callable (function) or cmd (binary) \nargs (Dict[int, Tuple]) \u2013 arguments to each replica \nenvs (Dict[int, Dict[str, str]]) \u2013 env vars to each replica \nlog_dir (str) \u2013 directory used to write log files \nstart_method (str) \u2013 multiprocessing start method (spawn, fork, forkserver) ignored for binaries \nredirects (Union[Std, Dict[int, Std]]) \u2013 which std streams to redirect to a log file \ntee (Union[Std, Dict[int, Std]]) \u2013 which std streams to redirect + print to console   Return type \nPContext   \n"}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend(store, run_id) [source]\n \nRepresents a C10d-backed rendezvous backend.  Parameters \n \nstore (Store) \u2013 The torch.distributed.Store instance to use to communicate with the C10d store. \nrun_id (str) \u2013 The run id of the rendezvous.     \nget_state() [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any]]   \n  \nproperty name: str  \nSee base class. \n  \nset_state(state, token=None) [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any, bool]]   \n \n"}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.get_state", "type": "Distributed Elastic", "text": " \nget_state() [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.name", "type": "Distributed Elastic", "text": " \nproperty name: str  \nSee base class. \n"}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.C10dRendezvousBackend.set_state", "type": "Distributed Elastic", "text": " \nset_state(state, token=None) [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any, bool]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.rendezvous.c10d_rendezvous_backend.create_backend(params) [source]\n \nCreates a new C10dRendezvousBackend from the specified parameters.   \nParameter Description   \nstore_type The type of the C10d store. The currently supported types are \u201ctcp\u201d and \u201cfile\u201d which correspond to torch.distributed.TCPStore and torch.distributed.FileStore, respectively. Defaults to \u201ctcp\u201d.  \nread_timeout \nThe read timeout, in seconds, for store operations. Defaults to 60 seconds. Note this only applies to torch.distributed.TCPStore. It is not relevant to torch.distributed.FileStore which does not take in timeout as a parameter.   \nis_host \nA boolean value indicating whether this backend instance will host the C10d store. If not specified it will be inferred heuristically by matching the hostname or the IP address of this machine against the specified rendezvous endpoint. Defaults to None. Note that this configuration option only applies to torch.distributed.TCPStore. In normal circumstances you can safely skip it; the only time when it is needed is if its value cannot be correctly determined (e.g. the rendezvous endpoint has a CNAME as the hostname or does not match the FQDN of the machine).     Return type \nTuple[C10dRendezvousBackend, Store]   \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.rendezvous.dynamic_rendezvous.create_handler(store, backend, params) [source]\n \nCreates a new DynamicRendezvousHandler from the specified parameters.  Parameters \n \nstore (Store) \u2013 The C10d store to return as part of the rendezvous. \nbackend (RendezvousBackend) \u2013 The backend to use to hold the rendezvous state.   Return type \nDynamicRendezvousHandler     \nParameter Description   \njoin_timeout The total time, in seconds, within which the rendezvous is expected to complete. Defaults to 600 seconds.  \nlast_call_timeout An additional wait amount, in seconds, before completing the rendezvous once the minimum number of nodes has been reached. Defaults to 30 seconds.  \nclose_timeout The time, in seconds, within which the rendezvous is expected to close after a call to RendezvousHandler.set_closed() or RendezvousHandler.shutdown(). Defaults to 30 seconds.   \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler [source]\n \nRepresents a handler that sets up a rendezvous among a set of nodes.  \nclassmethod from_backend(run_id, store, backend, min_nodes, max_nodes, local_addr=None, timeout=None) [source]\n \nCreates a new DynamicRendezvousHandler.  Parameters \n \nrun_id (str) \u2013 The run id of the rendezvous. \nstore (Store) \u2013 The C10d store to return as part of the rendezvous. \nbackend (RendezvousBackend) \u2013 The backend to use to hold the rendezvous state. \nmin_nodes (int) \u2013 The minimum number of nodes to admit to the rendezvous. \nmax_nodes (int) \u2013 The maximum number of nodes to admit to the rendezvous. \nlocal_addr (Optional[str]) \u2013 The local node address. \ntimeout (Optional[RendezvousTimeout]) \u2013 The timeout configuration of the rendezvous.    \n \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.DynamicRendezvousHandler.from_backend", "type": "Distributed Elastic", "text": " \nclassmethod from_backend(run_id, store, backend, min_nodes, max_nodes, local_addr=None, timeout=None) [source]\n \nCreates a new DynamicRendezvousHandler.  Parameters \n \nrun_id (str) \u2013 The run id of the rendezvous. \nstore (Store) \u2013 The C10d store to return as part of the rendezvous. \nbackend (RendezvousBackend) \u2013 The backend to use to hold the rendezvous state. \nmin_nodes (int) \u2013 The minimum number of nodes to admit to the rendezvous. \nmax_nodes (int) \u2013 The maximum number of nodes to admit to the rendezvous. \nlocal_addr (Optional[str]) \u2013 The local node address. \ntimeout (Optional[RendezvousTimeout]) \u2013 The timeout configuration of the rendezvous.    \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend [source]\n \nRepresents a backend that holds the rendezvous state.  \nabstract get_state() [source]\n \nGets the rendezvous state.  Returns \nA tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.  Raises \n \nRendezvousConnectionError \u2013 The connection to the backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt.   Return type \nOptional[Tuple[bytes, Any]]   \n  \nabstract property name: str  \nGets the name of the backend. \n  \nabstract set_state(state, token=None) [source]\n \nSets the rendezvous state. The new rendezvous state is set conditionally:  If the specified token matches the fencing token stored in the backend, the state will be updated. The new state will be returned to the caller along with its fencing token. If the specified token does not match the fencing token stored in the backend, the state won\u2019t be updated; instead the existing state along with its fencing token will be returned to the caller. If the specified token is None, the new state will be set only if there is no existing state in the backend. Either the new state or the existing state along with its fencing token will be returned to the caller.   Parameters \n \nstate (bytes) \u2013 The encoded rendezvous state. \ntoken (Optional[Any]) \u2013 An optional fencing token that was retrieved by a previous call to get_state() or set_state().   Returns \nA tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.  Raises \n \nRendezvousConnectionError \u2013 The connection to the backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt.   Return type \nOptional[Tuple[bytes, Any, bool]]   \n \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.get_state", "type": "Distributed Elastic", "text": " \nabstract get_state() [source]\n \nGets the rendezvous state.  Returns \nA tuple of the encoded rendezvous state and its fencing token or None if no state is found in the backend.  Raises \n \nRendezvousConnectionError \u2013 The connection to the backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt.   Return type \nOptional[Tuple[bytes, Any]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.name", "type": "Distributed Elastic", "text": " \nabstract property name: str  \nGets the name of the backend. \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousBackend.set_state", "type": "Distributed Elastic", "text": " \nabstract set_state(state, token=None) [source]\n \nSets the rendezvous state. The new rendezvous state is set conditionally:  If the specified token matches the fencing token stored in the backend, the state will be updated. The new state will be returned to the caller along with its fencing token. If the specified token does not match the fencing token stored in the backend, the state won\u2019t be updated; instead the existing state along with its fencing token will be returned to the caller. If the specified token is None, the new state will be set only if there is no existing state in the backend. Either the new state or the existing state along with its fencing token will be returned to the caller.   Parameters \n \nstate (bytes) \u2013 The encoded rendezvous state. \ntoken (Optional[Any]) \u2013 An optional fencing token that was retrieved by a previous call to get_state() or set_state().   Returns \nA tuple of the serialized rendezvous state, its fencing token, and a boolean value indicating whether our set attempt succeeded.  Raises \n \nRendezvousConnectionError \u2013 The connection to the backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt.   Return type \nOptional[Tuple[bytes, Any, bool]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout(join=None, last_call=None, close=None, heartbeat=None) [source]\n \nHolds the timeout configuration of a rendezvous.  Parameters \n \njoin (Optional[timedelta]) \u2013 The time within which the rendezvous is expected to complete. \nlast_call (Optional[timedelta]) \u2013 An additional wait amount before completing the rendezvous once the rendezvous has the minimum number of required participants. \nclose (Optional[timedelta]) \u2013 The time within which the rendezvous is expected to close after a call to RendezvousHandler.set_closed() or RendezvousHandler.shutdown(). \nkeep_alive \u2013 The time within which a keep-alive heartbeat is expected to complete.     \nproperty close: timedelta  \nGets the close timeout. \n  \nproperty heartbeat: timedelta  \nGets the keep-alive heartbeat timeout. \n  \nproperty join: timedelta  \nGets the join timeout. \n  \nproperty last_call: timedelta  \nGets the last call timeout. \n \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.close", "type": "Distributed Elastic", "text": " \nproperty close: timedelta  \nGets the close timeout. \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.heartbeat", "type": "Distributed Elastic", "text": " \nproperty heartbeat: timedelta  \nGets the keep-alive heartbeat timeout. \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.join", "type": "Distributed Elastic", "text": " \nproperty join: timedelta  \nGets the join timeout. \n"}, {"name": "torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.dynamic_rendezvous.RendezvousTimeout.last_call", "type": "Distributed Elastic", "text": " \nproperty last_call: timedelta  \nGets the last call timeout. \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvousHandler(rdzv_impl) [source]\n \nImplements a torch.distributed.elastic.rendezvous.RendezvousHandler interface backed by torch.distributed.elastic.rendezvous.etcd_rendezvous.EtcdRendezvous. EtcdRendezvousHandler uses a URL to configure the type of rendezvous to use and to pass implementation specific configurations to the rendezvous module. The basic etcd rendezvous configuration URL looks like the following etcd://<etcd_address>:<port>/<job_id>?min_workers=<min_workers>&max_workers=<max_workers>  # noqa: W605\n\n-- example --\n\netcd://localhost:2379/1234?min_workers=1&max_workers=3\n The URL above is interpreted as follows:  Use the rendezvous handler that is registered with the etcd scheme The etcd endpoint to use is localhost:2379\n \njob_id == 1234 is used as the prefix in etcd (this allows one to share a common etcd server for multiple jobs so long as the job_ids are guaranteed to be unique). Note that the job id can be any string (e.g. does not need to be a number) as long as it is unique. \nmin_workers=1 and max_workers=3 specifies a range for membership size - Torch Distributed Elastic starts running the job as long as the cluster size is greater than or equal to min_workers and admits up to max_workers into the cluster.  Below are a full list of the parameters that can be passed to etcd rendezvous:   \nParameter Description   \nmin_workers minimum number of workers for the rendezvous to be valid  \nmax_workers maximum number of workers to admit  \ntimeout total timeout within which next_rendezvous is expected to succeed (default 600s)  \nlast_call_timeout additional wait amount (\u201clast call\u201d) after min number of workers has been reached (defaults to 30s)  \netcd_prefix path prefix (from etcd root), inside which all etcd nodes will be created (defaults to /torchelastic/p2p)   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.rendezvous.etcd_rendezvous_backend.create_backend(params) [source]\n \nCreates a new EtcdRendezvousBackend from the specified parameters.   \nParameter Description   \nread_timeout The read timeout, in seconds, for etcd operations. Defaults to 60 seconds.  \nprotocol The protocol to use to communicate with etcd. Valid values are \u201chttp\u201d and \u201chttps\u201d. Defaults to \u201chttp\u201d.  \nssl_cert The path to the SSL client certificate to use along with HTTPS. Defaults to None.  \nssl_cert_key The path to the private key of the SSL client certificate to use along with HTTPS. Defaults to None.  \nca_cert The path to the rool SSL authority certificate. Defaults to None.    Return type \nTuple[EtcdRendezvousBackend, Store]   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend(client, run_id, key_prefix=None, ttl=None) [source]\n \nRepresents an etcd-based rendezvous backend.  Parameters \n \nclient (Client) \u2013 The etcd.Client instance to use to communicate with etcd. \nrun_id (str) \u2013 The run id of the rendezvous. \nkey_prefix (Optional[str]) \u2013 The path under which to store the rendezvous state in etcd. \nttl (Optional[int]) \u2013 The TTL of the rendezvous state. If not specified, defaults to two hours.     \nget_state() [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any]]   \n  \nproperty name: str  \nSee base class. \n  \nset_state(state, token=None) [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any, bool]]   \n \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.get_state", "type": "Distributed Elastic", "text": " \nget_state() [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.name", "type": "Distributed Elastic", "text": " \nproperty name: str  \nSee base class. \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_rendezvous_backend.EtcdRendezvousBackend.set_state", "type": "Distributed Elastic", "text": " \nset_state(state, token=None) [source]\n \nSee base class.  Return type \nOptional[Tuple[bytes, Any, bool]]   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_server.EtcdServer", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.etcd_server.EtcdServer(data_dir=None) [source]\n \n Note tested on etcd server v3.4.3  Starts and stops a local standalone etcd server on a random free port. Useful for single node, multi-worker launches or testing, where a sidecar etcd server is more convenient than having to separately setup an etcd server. This class registers a termination handler to shutdown the etcd subprocess on exit. This termination handler is NOT a substitute for calling the stop() method. The following fallback mechanism is used to find the etcd binary:  Uses env var TORCHELASTIC_ETCD_BINARY_PATH Uses <this file root>/bin/etcd if one exists Uses etcd from PATH\n  Usage server = EtcdServer(\"/usr/bin/etcd\", 2379, \"/tmp/default.etcd\")\nserver.start()\nclient = server.get_client()\n# use client\nserver.stop()\n  Parameters \netcd_binary_path \u2013 path of etcd server binary (see above for fallback path)   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.etcd_store.EtcdStore(etcd_client, etcd_store_prefix, timeout=None) [source]\n \nImplements a c10 Store interface by piggybacking on the rendezvous etcd instance. This is the store object returned by EtcdRendezvous  \nadd(key, num) [source]\n \nAtomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.  Returns \nthe new (incremented) value  Return type \nint   \n  \ncheck(keys) [source]\n \nCheck if all of the keys are immediately present (without waiting).  Return type \nbool   \n  \nget(key) [source]\n \nGet a value by key, possibly doing a blocking wait. If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.  Returns \nvalue (bytes)  Raises \nLookupError - If key still not published after timeout \u2013   Return type \nbytes   \n  \nset(key, value) [source]\n \nWrite a key/value pair into EtcdStore. Both key and value may be either Python str or bytes. \n  \nwait(keys, override_timeout=None) [source]\n \nWaits until all of the keys are published, or until timeout.  Raises \nLookupError - if timeout occurs \u2013    \n \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.add", "type": "Distributed Elastic", "text": " \nadd(key, num) [source]\n \nAtomically increment a value by an integer amount. The integer is represented as a string using base 10. If key is not present, a default value of 0 will be assumed.  Returns \nthe new (incremented) value  Return type \nint   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.check", "type": "Distributed Elastic", "text": " \ncheck(keys) [source]\n \nCheck if all of the keys are immediately present (without waiting).  Return type \nbool   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.get", "type": "Distributed Elastic", "text": " \nget(key) [source]\n \nGet a value by key, possibly doing a blocking wait. If key is not immediately present, will do a blocking wait for at most timeout duration or until the key is published.  Returns \nvalue (bytes)  Raises \nLookupError - If key still not published after timeout \u2013   Return type \nbytes   \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.set", "type": "Distributed Elastic", "text": " \nset(key, value) [source]\n \nWrite a key/value pair into EtcdStore. Both key and value may be either Python str or bytes. \n"}, {"name": "torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.etcd_store.EtcdStore.wait", "type": "Distributed Elastic", "text": " \nwait(keys, override_timeout=None) [source]\n \nWaits until all of the keys are published, or until timeout.  Raises \nLookupError - if timeout occurs \u2013    \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousClosedError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousClosedError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousClosedError [source]\n \nRaised when a rendezvous is closed. \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousConnectionError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousConnectionError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousConnectionError [source]\n \nRaised when the connection to a rendezvous backend has failed. \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousError [source]\n \nRepresents the base type for rendezvous errors. \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousHandler [source]\n \nMain rendezvous interface.  Note Distributed Torch users normally do not need to implement their own RendezvousHandler. An implementation based on C10d Store is already provided, and is recommended for most users.   \nabstract get_backend() [source]\n \nReturns the name of the rendezvous backend.  Return type \nstr   \n  \nabstract get_run_id() [source]\n \nReturns the run id of the rendezvous. The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.  Return type \nstr   \n  \nabstract is_closed() [source]\n \nChecks whether the rendezvous has been closed. A closed rendezvous means all future attempts to re-rendezvous within same job will fail. is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.  Return type \nbool   \n  \nabstract next_rendezvous() [source]\n \nMain entry-point into the rendezvous barrier. Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.  Returns \nA tuple of torch.distributed.Store, rank, and world size.  Raises \n \nRendezvousClosedError \u2013 The rendezvous is closed. \nRendezvousConnectionError \u2013 The connection to the rendezvous backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt. \nRendezvousTimeoutError \u2013 The rendezvous did not complete on time.   Return type \nTuple[Store, int, int]   \n  \nabstract num_nodes_waiting() [source]\n \nReturns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group. Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).  Return type \nint   \n  \nabstract set_closed() [source]\n \nMarks the rendezvous as closed. \n  \nabstract shutdown() [source]\n \nCloses all resources that were open for the rendezvous. Example: rdzv_handler = ...\ntry:\n    store, rank, world_size = rdzv_handler.next_rendezvous()\nfinally:\n    rdzv_handler.shutdown()\n  Return type \nbool   \n \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_backend", "type": "Distributed Elastic", "text": " \nabstract get_backend() [source]\n \nReturns the name of the rendezvous backend.  Return type \nstr   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.get_run_id", "type": "Distributed Elastic", "text": " \nabstract get_run_id() [source]\n \nReturns the run id of the rendezvous. The run id is a user-defined id that uniquely identifies an instance of a distributed application. It typically maps to a job id and is used to allow nodes to join the correct distributed application.  Return type \nstr   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.is_closed", "type": "Distributed Elastic", "text": " \nabstract is_closed() [source]\n \nChecks whether the rendezvous has been closed. A closed rendezvous means all future attempts to re-rendezvous within same job will fail. is_closed() and set_closed() have semantics of eventual propagation and should not be used for synchronization. The intention is that if at least one node decides the job is finished, it will close the rendezvous, and other nodes will soon observe this and stop running as well.  Return type \nbool   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.next_rendezvous", "type": "Distributed Elastic", "text": " \nabstract next_rendezvous() [source]\n \nMain entry-point into the rendezvous barrier. Blocks until the rendezvous is complete and the current process is included in the formed worker group, or a timeout occurs, or the rendezvous was marked closed.  Returns \nA tuple of torch.distributed.Store, rank, and world size.  Raises \n \nRendezvousClosedError \u2013 The rendezvous is closed. \nRendezvousConnectionError \u2013 The connection to the rendezvous backend has failed. \nRendezvousStateError \u2013 The rendezvous state is corrupt. \nRendezvousTimeoutError \u2013 The rendezvous did not complete on time.   Return type \nTuple[Store, int, int]   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.num_nodes_waiting", "type": "Distributed Elastic", "text": " \nabstract num_nodes_waiting() [source]\n \nReturns the number of nodes who arrived late at the rendezvous barrier, hence were not included in the current worker group. Callers should periodically call this method to check whether new nodes are waiting to join the job and if so admit them by calling next_rendezvous() (re-rendezvous).  Return type \nint   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.set_closed", "type": "Distributed Elastic", "text": " \nabstract set_closed() [source]\n \nMarks the rendezvous as closed. \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandler.shutdown", "type": "Distributed Elastic", "text": " \nabstract shutdown() [source]\n \nCloses all resources that were open for the rendezvous. Example: rdzv_handler = ...\ntry:\n    store, rank, world_size = rdzv_handler.next_rendezvous()\nfinally:\n    rdzv_handler.shutdown()\n  Return type \nbool   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry [source]\n \nRepresents a registry of RendezvousHandler backends.  \ncreate_handler(params) [source]\n \nCreates a new RendezvousHandler.  Return type \nRendezvousHandler   \n  \nregister(backend, creator) [source]\n \nRegisters a new rendezvous backend.  Parameters \n \nbackend (str) \u2013 The name of the backend. \ncreator (Callable[[RendezvousParameters], RendezvousHandler]) \u2013 The callback to invoke to construct the RendezvousHandler.    \n \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.create_handler", "type": "Distributed Elastic", "text": " \ncreate_handler(params) [source]\n \nCreates a new RendezvousHandler.  Return type \nRendezvousHandler   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousHandlerRegistry.register", "type": "Distributed Elastic", "text": " \nregister(backend, creator) [source]\n \nRegisters a new rendezvous backend.  Parameters \n \nbackend (str) \u2013 The name of the backend. \ncreator (Callable[[RendezvousParameters], RendezvousHandler]) \u2013 The callback to invoke to construct the RendezvousHandler.    \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousParameters(backend, endpoint, run_id, min_nodes, max_nodes, local_addr=None, **kwargs) [source]\n \nHolds the parameters to construct a RendezvousHandler.  Parameters \n \nbackend (str) \u2013 The name of the backend to use to handle the rendezvous. \nendpoint (str) \u2013 The endpoint of the rendezvous, usually in form <hostname>[:<port>]. \nrun_id (str) \u2013 The id of the rendezvous. \nmin_nodes (int) \u2013 The minimum number of nodes to admit to the rendezvous. \nmax_nodes (int) \u2013 The maximum number of nodes to admit to the rendezvous. \nlocal_addr (Optional[str]) \u2013 The address of the local node. \n**kwargs \u2013 Additional parameters for the specified backend.     \nget(key, default=None) [source]\n \nReturns the value for key if key exists, else default.  Return type \nAny   \n  \nget_as_bool(key, default=None) [source]\n \nReturns the value for key as a bool.  Return type \nOptional[bool]   \n  \nget_as_int(key, default=None) [source]\n \nReturns the value for key as an int.  Return type \nOptional[int]   \n \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get", "type": "Distributed Elastic", "text": " \nget(key, default=None) [source]\n \nReturns the value for key if key exists, else default.  Return type \nAny   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_bool", "type": "Distributed Elastic", "text": " \nget_as_bool(key, default=None) [source]\n \nReturns the value for key as a bool.  Return type \nOptional[bool]   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int()", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousParameters.get_as_int", "type": "Distributed Elastic", "text": " \nget_as_int(key, default=None) [source]\n \nReturns the value for key as an int.  Return type \nOptional[int]   \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousStateError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousStateError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousStateError [source]\n \nRaised when the state of a rendezvous is corrupt. \n"}, {"name": "torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "path": "elastic/rendezvous#torch.distributed.elastic.rendezvous.RendezvousTimeoutError", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.rendezvous.RendezvousTimeoutError [source]\n \nRaised when a rendezvous did not complete on time. \n"}, {"name": "torch.distributed.elastic.timer.configure()", "path": "elastic/timer#torch.distributed.elastic.timer.configure", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.timer.configure(timer_client) [source]\n \nConfigures a timer client. Must be called before using expires. \n"}, {"name": "torch.distributed.elastic.timer.expires()", "path": "elastic/timer#torch.distributed.elastic.timer.expires", "type": "Distributed Elastic", "text": " \ntorch.distributed.elastic.timer.expires(after, scope=None, client=None) [source]\n \nAcquires a countdown timer that expires in after seconds from now, unless the code-block that it wraps is finished within the timeframe. When the timer expires, this worker is eligible to be reaped. The exact meaning of \u201creaped\u201d depends on the client implementation. In most cases, reaping means to terminate the worker process. Note that the worker is NOT guaranteed to be reaped at exactly time.now() + after, but rather the worker is \u201celigible\u201d for being reaped and the TimerServer that the client talks to will ultimately make the decision when and how to reap the workers with expired timers. Usage: torch.distributed.elastic.timer.configure(LocalTimerClient())\nwith expires(after=10):\n    torch.distributed.all_reduce(...)\n \n"}, {"name": "torch.distributed.elastic.timer.FileTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerClient", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.FileTimerClient(file_path, signal=Signals.SIGKILL) [source]\n \nClient side of FileTimerServer. This client is meant to be used on the same host that the FileTimerServer is running on and uses pid to uniquely identify a worker. This client uses a named_pipe to send timer requests to the FileTimerServer. This client is a producer while the FileTimerServer is a consumer. Multiple clients can work with the same FileTimerServer.  Parameters \n \nfile_path (str) \u2013 str, the path of a FIFO special file. FileTimerServer must have created it by calling os.mkfifo(). \nsignal \u2013 signal, the signal to use to kill the process. Using a negative or zero signal will not kill the process.    \n"}, {"name": "torch.distributed.elastic.timer.FileTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.FileTimerServer", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.FileTimerServer(file_path, max_interval=10, daemon=True, log_event=None) [source]\n \nServer that works with FileTimerClient. Clients are expected to be running on the same host as the process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host).  Parameters \n \nfile_path (str) \u2013 str, the path of a FIFO special file to be created. \nmax_interval (float) \u2013 float, max interval in seconds for each watchdog loop. \ndaemon (bool) \u2013 bool, running the watchdog thread in daemon mode or not. A daemon thread will not block a process to stop. \nlog_event (Optional[Callable[[str, Optional[FileTimerRequest]], None]]) \u2013 Callable[[Dict[str, str]], None], an optional callback for logging the events in JSON format.    \n"}, {"name": "torch.distributed.elastic.timer.LocalTimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerClient", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.LocalTimerClient(mp_queue) [source]\n \nClient side of LocalTimerServer. This client is meant to be used on the same host that the LocalTimerServer is running on and uses pid to uniquely identify a worker. This is particularly useful in situations where one spawns a subprocess (trainer) per GPU on a host with multiple GPU devices. \n"}, {"name": "torch.distributed.elastic.timer.LocalTimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.LocalTimerServer", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.LocalTimerServer(mp_queue, max_interval=60, daemon=True) [source]\n \nServer that works with LocalTimerClient. Clients are expected to be subprocesses to the parent process that is running this server. Each host in the job is expected to start its own timer server locally and each server instance manages timers for local workers (running on processes on the same host). \n"}, {"name": "torch.distributed.elastic.timer.TimerClient", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.TimerClient [source]\n \nClient library to acquire and release countdown timers by communicating with the TimerServer.  \nabstract acquire(scope_id, expiration_time) [source]\n \nAcquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer. \n  \nabstract release(scope_id) [source]\n \nReleases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect. \n \n"}, {"name": "torch.distributed.elastic.timer.TimerClient.acquire()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.acquire", "type": "Distributed Elastic", "text": " \nabstract acquire(scope_id, expiration_time) [source]\n \nAcquires a timer for the worker that holds this client object given the scope_id and expiration_time. Typically registers the timer with the TimerServer. \n"}, {"name": "torch.distributed.elastic.timer.TimerClient.release()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerClient.release", "type": "Distributed Elastic", "text": " \nabstract release(scope_id) [source]\n \nReleases the timer for the scope_id on the worker this client represents. After this method is called, the countdown timer on the scope is no longer in effect. \n"}, {"name": "torch.distributed.elastic.timer.TimerRequest", "path": "elastic/timer#torch.distributed.elastic.timer.TimerRequest", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.TimerRequest(worker_id, scope_id, expiration_time) [source]\n \nData object representing a countdown timer acquisition and release that is used between the TimerClient and TimerServer. A negative expiration_time should be interpreted as a \u201crelease\u201d request.  Note the type of worker_id is implementation specific. It is whatever the TimerServer and TimerClient implementations have on to uniquely identify a worker.  \n"}, {"name": "torch.distributed.elastic.timer.TimerServer", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer", "type": "Distributed Elastic", "text": " \nclass torch.distributed.elastic.timer.TimerServer(request_queue, max_interval, daemon=True) [source]\n \nEntity that monitors active timers and expires them in a timely fashion. This server is responsible for reaping workers that have expired timers.  \nabstract clear_timers(worker_ids) [source]\n \nClears all timers for the given worker_ids. \n  \nabstract get_expired_timers(deadline) [source]\n \nReturns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.  Return type \nDict[str, List[TimerRequest]]   \n  \nabstract register_timers(timer_requests) [source]\n \nProcesses the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request. \n \n"}, {"name": "torch.distributed.elastic.timer.TimerServer.clear_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.clear_timers", "type": "Distributed Elastic", "text": " \nabstract clear_timers(worker_ids) [source]\n \nClears all timers for the given worker_ids. \n"}, {"name": "torch.distributed.elastic.timer.TimerServer.get_expired_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.get_expired_timers", "type": "Distributed Elastic", "text": " \nabstract get_expired_timers(deadline) [source]\n \nReturns all expired timers for each worker_id. An expired timer is a timer for which the expiration_time is less than or equal to the provided deadline.  Return type \nDict[str, List[TimerRequest]]   \n"}, {"name": "torch.distributed.elastic.timer.TimerServer.register_timers()", "path": "elastic/timer#torch.distributed.elastic.timer.TimerServer.register_timers", "type": "Distributed Elastic", "text": " \nabstract register_timers(timer_requests) [source]\n \nProcesses the incoming timer requests and registers them with the server. The timer request can either be a acquire-timer or release-timer request. Timer requests with a negative expiration_time should be interpreted as a release-timer request. \n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "Distributed Communication", "text": " \nclass torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int, optional) \u2013 The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.fsdp.BackwardPrefetch", "path": "fsdp#torch.distributed.fsdp.BackwardPrefetch", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.BackwardPrefetch(value) [source]\n \nThis configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.  \nBACKWARD_PRE: This enables the most overlap but increases memory usage the most. This prefetches the next set of parameters before the current set of parameters\u2019 gradient computation. This overlaps the next all-gather and the current gradient computation, and at the peak, it holds the current set of parameters, next set of parameters, and current set of gradients in memory. \nBACKWARD_POST: This enables less overlap but requires less memory usage. This prefetches the next set of parameters after the current set of parameters\u2019 gradient computation. This overlaps the current reduce-scatter and the next gradient computation, and it frees the current set of parameters before allocating memory for the next set of parameters, only holding the next set of parameters and current set of gradients in memory at the peak. FSDP\u2019s backward_prefetch argument accepts None, which disables the backward prefetching altogether. This has no overlap and does not increase memory usage. In general, we do not recommend this setting since it may degrade throughput significantly.  For more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders. \n"}, {"name": "torch.distributed.fsdp.CPUOffload", "path": "fsdp#torch.distributed.fsdp.CPUOffload", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.CPUOffload(offload_params=False) [source]\n \nThis configures CPU offloading.  Variables \noffload_params (bool) \u2013 This specifies whether to offload parameters to CPU when not involved in computation. If True, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU.   \n"}, {"name": "torch.distributed.fsdp.FullOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.FullOptimStateDictConfig(offload_to_cpu=True, use_dtensor=False, rank0_only=False) [source]\n \n Variables \nrank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)   \n"}, {"name": "torch.distributed.fsdp.FullStateDictConfig", "path": "fsdp#torch.distributed.fsdp.FullStateDictConfig", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.FullStateDictConfig(offload_to_cpu=False, use_dtensor=False, rank0_only=False) [source]\n \nFullStateDictConfig is a config class meant to be used with StateDictType.FULL_STATE_DICT. We recommend enabling both offload_to_cpu=True and rank0_only=True when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the state_dict_type() context manager as follows: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> fsdp = FSDP(model, auto_wrap_policy=...)\n>>> cfg = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n>>> with FSDP.state_dict_type(fsdp, StateDictType.FULL_STATE_DICT, cfg):\n>>>     state = fsdp.state_dict()\n>>>     # `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n>>> # To reload checkpoint for inference, finetuning, transfer learning, etc:\n>>> model = model_fn() # Initialize model on CPU in preparation for wrapping with FSDP\n>>> if dist.get_rank() == 0:\n>>>     # Load checkpoint only on rank 0 to avoid memory redundancy\n>>>     state_dict = torch.load(\"my_checkpoint.pt\")\n>>>     model.load_state_dict(state_dict)\n>>> # All ranks initialize FSDP module as usual. `sync_module_states` argument\n>>> # communicates loaded checkpoint states from rank 0 to rest of the world.\n>>> fsdp = FSDP(model, device_id=torch.cuda.current_device(), auto_wrap_policy=..., sync_module_states=True)\n>>> # After this point, all ranks have FSDP model with loaded checkpoint.\n  Variables \nrank0_only (bool) \u2013 If True, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If False, then all ranks save the full state dict. (Default: False)   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.FullyShardedDataParallel(module, process_group=None, sharding_strategy=None, cpu_offload=None, auto_wrap_policy=None, backward_prefetch=BackwardPrefetch.BACKWARD_PRE, mixed_precision=None, ignored_modules=None, param_init_fn=None, device_id=None, sync_module_states=False, forward_prefetch=False, limit_all_gathers=True, use_orig_params=False, ignored_states=None) [source]\n \nA wrapper for sharding module parameters across data parallel workers. This is inspired by Xu et al. as well as the ZeRO Stage 3 from DeepSpeed. FullyShardedDataParallel is commonly shortened to FSDP. Example: >>> import torch\n>>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> torch.cuda.set_device(device_id)\n>>> sharded_module = FSDP(my_module)\n>>> optim = torch.optim.Adam(sharded_module.parameters(), lr=0.0001)\n>>> x = sharded_module(x, y=3, z=torch.Tensor([1]))\n>>> loss = x.sum()\n>>> loss.backward()\n>>> optim.step()\n  Warning The optimizer must be initialized after the module has been wrapped with FSDP since FSDP will shard and transform the module\u2019s parameters in a way that may not preserve the original parameter variables. Thus, the previously initialized optimizer may have stale references to the parameters.   Warning If the destination CUDA device has ID dev_id, either (1) module should already be placed on that device, (2) the device should be set using torch.cuda.set_device(dev_id), or (3) dev_id should be passed into the device_id constructor argument. This FSDP instance\u2019s compute device will be that destination device. For (1) and (3), the FSDP initialization always occurs on GPU. For (2), the FSDP initialization happens on module \u2018s current device, which may be CPU.   Warning FSDP currently does not support gradient accumulation outside no_sync() when using CPU offloading. Trying to do so yields incorrect results since FSDP will use the newly-reduced gradient instead of accumulating with any existing gradient.   Warning Changing the original parameter variable names after construction will lead to undefined behavior.   Warning Passing in the sync_module_states=True flag requires module to be on GPU or to use the device_id argument to specify a CUDA device that FSDP will move module to in the FSDP constructor. This is because sync_module_states=True requires GPU communication.   Warning As of PyTorch 1.12, FSDP only offers limited support for shared parameters (for example, setting one Linear layer\u2019s weight to another\u2019s). In particular, modules that share parameters must be wrapped as part of the same FSDP unit. If enhanced shared parameter support is needed for your use case, please ping https://github.com/pytorch/pytorch/issues/77724   Warning FSDP has some constraints on freezing parameters (i.e. setting param.requires_grad=False). For use_orig_params=False, each FSDP instance must manage parameters that are all frozen or all non-frozen. For use_orig_params=True, FSDP supports mixing frozen and non-frozen, but we recommend not doing so since then the gradient memory usage will be higher than expected (namely, equivalent to not freezing those parameters). This means that ideally, frozen parameters should be isolated into their own nn.Module s and wrapped separately with FSDP.   Note Attempting to run the forward pass of a submodule that is contained in an FSDP instance is not supported and will result in errors. This is because the submodule\u2019s parameters will be sharded, but it itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately. This could potentially happen when attempting to run only the encoder of a encoder-decoder model, and the encoder is not wrapped in its own FSDP instance. To resolve this, please wrap the submodule in its own FSDP unit.   Note FSDP moves input tensors to the forward method to the GPU compute device, so the user does not need to manually move them from CPU.   Warning The user should not modify the parameters between forward and backward without using the summon_full_params() context since the modifications may not persist. Moreover, for use_orig_params=False, accessing the original parameters between forward and backward may raise an illegal memory access.   Warning For use_orig_params=True, ShardingStrategy.SHARD_GRAD_OP exposes the unsharded parameters, not the sharded parameters, after forward since it does not free the unsharded ones, unlike ShardingStrategy.FULL_SHARD. One caveat is that, since gradients are always sharded or None, ShardingStrategy.SHARD_GRAD_OP will not expose the sharded gradients with the unsharded parameters after forward. If you want to inspect the gradients, try summon_full_params() with with_grads=True.   Warning FSDP replaces managed modules\u2019 parameters with torch.Tensor views during forward and backward computation for autograd-related reasons. If your module\u2019s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP\u2019s newly created views, and autograd will not work correctly.   Note With limit_all_gathers=True, you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.   Note When using sharding_strategy=ShardingStrategy.HYBRID_SHARD with the sharding process group being intra-node and the replication process group being inter-node, setting NCCL_CROSS_NIC=1 can help improve the all-reduce times over the replication process group for some cluster setups.   Parameters \n \nmodule (nn.Module) \u2013 This is the module to be wrapped with FSDP. \nprocess_group (Optional[Union[ProcessGroup, Tuple[ProcessGroup, ProcessGroup]]]) \u2013 This is the process group over which the model is sharded and thus the one used for FSDP\u2019s all-gather and reduce-scatter collective communications. If None, then FSDP uses the default process group. For hybrid sharding strategies such as ShardingStrategy.HYBRID_SHARD, users can pass in a tuple of process groups, representing the groups over which to shard and replicate, respectively. If None, then FSDP constructs process groups for the user to shard intra-node and replicate inter-node. (Default: None) \nsharding_strategy (Optional[ShardingStrategy]) \u2013 This configures the sharding strategy, which may trade off memory saving and communication overhead. See ShardingStrategy for details. (Default: FULL_SHARD) \ncpu_offload (Optional[CPUOffload]) \u2013 This configures CPU offloading. If this is set to None, then no CPU offloading happens. See CPUOffload for details. (Default: None) \nauto_wrap_policy (Optional[Union[Callable[[nn.Module, bool, int], bool], ModuleWrapPolicy]]) \u2013 \nThis specifies a policy to apply FSDP to submodules of module, which is needed for communication and computation overlap and thus affects performance. If None, then FSDP only applies to module, and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts ModuleWrapPolicy directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments module: nn.Module, recurse: bool, and nonwrapped_numel: int and should return a bool specifying whether the passed-in module should have FSDP applied if recurse=False or if the traversal should continue into the module\u2019s subtree if recurse=True. Users may add additional arguments to the callable. The size_based_auto_wrap_policy in torch.distributed.fsdp.wrap.py gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed. Example: >>> def custom_auto_wrap_policy(\n>>>     module: nn.Module,\n>>>     recurse: bool,\n>>>     nonwrapped_numel: int,\n>>>     # Additional custom arguments\n>>>     min_num_params: int = int(1e8),\n>>> ) -> bool:\n>>>     return nonwrapped_numel >= min_num_params\n>>> # Configure a custom `min_num_params`\n>>> my_auto_wrap_policy = functools.partial(custom_auto_wrap_policy, min_num_params=int(1e5))\n  \nbackward_prefetch (Optional[BackwardPrefetch]) \u2013 This configures explicit backward prefetching of all-gathers. If None, then FSDP does not backward prefetch, and there is no communication and computation overlap in the backward pass. See BackwardPrefetch for details. (Default: BACKWARD_PRE) \nmixed_precision (Optional[MixedPrecision]) \u2013 This configures native mixed precision for FSDP. If this is set to None, then no mixed precision is used. Otherwise, parameter, buffer, and gradient reduction dtypes can be set. See MixedPrecision for details. (Default: None) \nignored_modules (Optional[Iterable[torch.nn.Module]]) \u2013 Modules whose own parameters and child modules\u2019 parameters and buffers are ignored by this instance. None of the modules directly in ignored_modules should be FullyShardedDataParallel instances, and any child modules that are already-constructed FullyShardedDataParallel instances will not be ignored if they are nested under this instance. This argument may be used to avoid sharding specific parameters at module granularity when using an auto_wrap_policy or if parameters\u2019 sharding is not managed by FSDP. (Default: None) \nparam_init_fn (Optional[Callable[[nn.Module], None]]) \u2013 \nA Callable[torch.nn.Module] -> None that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via is_meta and either applies param_init_fn if specified or calls nn.Module.reset_parameters() otherwise. For both cases, the implementation should only initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX\u2019s (https://github.com/pytorch/torchdistX) deferred_init() API, where the deferred modules are initialized by calling param_init_fn if specified or torchdistX\u2019s default materialize_module() otherwise. If param_init_fn is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding. Example: >>> module = MyModule(device=\"meta\")\n>>> def my_init_fn(module: nn.Module):\n>>>     # E.g. initialize depending on the module type\n>>>     ...\n>>> fsdp_model = FSDP(module, param_init_fn=my_init_fn, auto_wrap_policy=size_based_auto_wrap_policy)\n>>> print(next(fsdp_model.parameters()).device) # current CUDA device\n>>> # With torchdistX\n>>> module = deferred_init.deferred_init(MyModule, device=\"cuda\")\n>>> # Will initialize via deferred_init.materialize_module().\n>>> fsdp_model = FSDP(module, auto_wrap_policy=size_based_auto_wrap_policy)\n  \ndevice_id (Optional[Union[int, torch.device]]) \u2013 An int or torch.device giving the CUDA device on which FSDP initialization takes place, including the module initialization if needed and the parameter sharding. This should be specified to improve initialization speed if module is on CPU. If the default CUDA device was set (e.g. via torch.cuda.set_device), then the user may pass torch.cuda.current_device to this. (Default: None) \nsync_module_states (bool) \u2013 If True, then each FSDP module will broadcast module parameters and buffers from rank 0 to ensure that they are replicated across ranks (adding communication overhead to this constructor). This can help load state_dict checkpoints via load_state_dict in a memory efficient way. See FullStateDictConfig for an example of this. (Default: False) \nforward_prefetch (bool) \u2013 If True, then FSDP explicitly prefetches the next forward-pass all-gather before the current forward computation. This is only useful for CPU-bound workloads, in which case issuing the next all-gather earlier may improve overlap. This should only be used for static-graph models since the prefetching follows the first iteration\u2019s execution order. (Default: False) \nlimit_all_gathers (bool) \u2013 If True, then FSDP explicitly synchronizes the CPU thread to ensure GPU memory usage from only two consecutive FSDP instances (the current instance running computation and the next instance whose all-gather is prefetched). If False, then FSDP allows the CPU thread to issue all-gathers without any extra synchronization. (Default: True) We often refer to this feature as the \u201crate limiter\u201d. This flag should only be set to False for specific CPU-bound workloads with low memory pressure in which case the CPU thread can aggressively issue all kernels without concern for the GPU memory usage. \nuse_orig_params (bool) \u2013 Setting this to True has FSDP use module \u2018s original parameters. FSDP exposes those original parameters to the user via nn.Module.named_parameters() instead of FSDP\u2019s internal FlatParameter s. This means that the optimizer step runs on the original parameters, enabling per-original-parameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded FlatParameter, respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size-0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. True is required to use torch.compile(). Setting this to False exposes FSDP\u2019s internal FlatParameter s to the user via nn.Module.named_parameters(). (Default: False) \nignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]) \u2013 Ignored parameters or modules that will not be managed by this FSDP instance, meaning that the parameters are not sharded and their gradients are not reduced across ranks. This argument unifies with the existing ignored_modules argument, and we may deprecate ignored_modules soon. For backward compatibility, we keep both ignored_states and ignored_modules`, but FSDP only allows one of them to be specified as not None.     \napply(fn) [source]\n \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init). Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   \n  \nclip_grad_norm_(max_norm, norm_type=2.0) [source]\n \nClips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.  Parameters \n \nmax_norm (float or int) \u2013 max norm of the gradients \nnorm_type (float or int) \u2013 type of the used p-norm. Can be 'inf' for infinity norm.   Returns \nTotal norm of the parameters (viewed as a single vector).  Return type \nTensor    Note If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().   Note If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.   Note The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.   Warning This needs to be called on all ranks since it uses collective communications.  \n  \nstatic flatten_sharded_optim_state_dict(sharded_optim_state_dict, model, optim) [source]\n \nThe API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.  Parameters \n \nsharded_optim_state_dict (Dict[str, Any]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the sharded optimizer state. \nmodel (torch.nn.Module) \u2013 Refer to shard_full_optim_state_dict(). \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters.   Returns \nRefer to shard_full_optim_state_dict().  Return type \nDict[str, Any]   \n  \nforward(*args, **kwargs) [source]\n \nRuns the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.  Return type \nAny   \n  \nstatic fsdp_modules(module, root_only=False) [source]\n \nReturns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module, which may or may not be an FSDP module. \nroot_only (bool) \u2013 Whether to return only FSDP root modules. (Default: False)   Returns \nFSDP modules that are nested in the input module.  Return type \nList[FullyShardedDataParallel]   \n  \nstatic full_optim_state_dict(model, optim, optim_input=None, rank0_only=True, group=None) [source]\n \nConsolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.  Warning This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.   Warning Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.   Note Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().   Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer optim representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \nrank0_only (bool) \u2013 If True, saves the populated dict only on rank 0; if False, saves it on all ranks. (Default: True) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group or None if using the default process group. (Default: None)   Returns \nA dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.  Return type \nDict[str, Any]   \n  \nstatic get_state_dict_type(module) [source]\n \nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.  Returns \nA StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.  Raises \n \nAssertionError` if the StateDictSettings for differen \u2013  \nFSDP submodules differ. \u2013    Return type \nStateDictSettings   \n  \nproperty module: Module  \nReturns the wrapped module (like DistributedDataParallel). \n  \nnamed_buffers(*args, **kwargs) [source]\n \nOverrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.  Return type \nIterator[Tuple[str, Tensor]]   \n  \nnamed_parameters(*args, **kwargs) [source]\n \nOverrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.  Return type \nIterator[Tuple[str, Parameter]]   \n  \nno_sync() [source]\n \nA context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.  Note This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.   Note When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.   Return type \nGenerator   \n  \nstatic optim_state_dict(model, optim, optim_state_dict=None, group=None) [source]\n \nTransforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict. For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM. For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory. For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n  Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_state_dict (Dict[str, Any]) \u2013 the target optimizer state_dict to transform. If the value is None, optim.state_dict() will be used. ( Default: None) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters are sharded or None if using the default process group. ( Default: None)   Returns \nA dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.  Return type \nDict[str, Any]   \n  \nstatic optim_state_dict_to_load(model, optim, optim_state_dict, is_named_optimizer=False, load_directly=False, group=None) [source]\n \nGiven a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel. >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n  Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_state_dict (Dict[str, Any]) \u2013 The optimizer states to be loaded. \nis_named_optimizer (bool) \u2013 Is this optimizer a NamedOptimizer or KeyedOptimizer. Only set to True if optim is TorchRec\u2019s KeyedOptimizer or torch.distributed\u2019s NamedOptimizer. \nload_directly (bool) \u2013 If this is set to True, this API will also call optim.load_state_dict(result) before returning the result. Otherwise, users are responsible to call optim.load_state_dict() (Default: False) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters are sharded or None if using the default process group. ( Default: None)   Return type \nDict[str, Any]   \n  \nregister_comm_hook(state, hook) [source]\n \nRegisters a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.  Warning FSDP communication hook should be registered before running an initial forward pass and only once.   Parameters \n \nstate (object) \u2013 \nPassed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.  \nhook (Callable) \u2013 Callable, which has one of the following signatures: 1) hook: Callable[torch.Tensor] -> None: This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns None; 2) hook: Callable[torch.Tensor, torch.Tensor] -> None: This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns None. Callables with signature 1 are expected to handle gradient communication for a NO_SHARD case. Callables with signature 2 are expected to handle gradient communication for sharded cases.    \n  \nstatic rekey_optim_state_dict(optim_state_dict, optim_state_key_type, model, optim_input=None, optim=None) [source]\n \nRe-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without. To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model: >>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd)\n To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model: >>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd)\n  Returns \nThe optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.  Return type \nDict[str, Any]   \n  \nstatic scatter_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None, group=None) [source]\n \nScatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict(). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd)\n  Note Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.   Parameters \n \nfull_optim_state_dict (Optional[Dict[str, Any]]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state if on rank 0; the argument is ignored on nonzero ranks. \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \noptim (Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group or None if using the default process group. (Default: None)   Returns \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.  Return type \nDict[str, Any]   \n  \nstatic set_state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None) [source]\n \nSet the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.  Note This API should be called for only the top-level (root) module.   Note This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:  Example: >>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module. \nstate_dict_type (StateDictType) \u2013 the desired state_dict_type to set. \nstate_dict_config (Optional[StateDictConfig]) \u2013 the configuration for the target state_dict_type.   Returns \nA StateDictSettings that include the previous state_dict type and configuration for the module.  Return type \nStateDictSettings   \n  \nstatic shard_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None) [source]\n \nShards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict(). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd)\n  Note Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.   Parameters \n \nfull_optim_state_dict (Dict[str, Any]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state. \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \noptim (Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None)   Returns \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.  Return type \nDict[str, Any]   \n  \nstatic sharded_optim_state_dict(model, optim, group=None) [source]\n \nThe API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):. For the detailed usage, refer to full_optim_state_dict().  Warning The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.   Return type \nDict[str, Any]   \n  \nstatic state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None) [source]\n \nA context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail. Example: >>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict()\n  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module. \nstate_dict_type (StateDictType) \u2013 the desired state_dict_type to set. \nstate_dict_config (Optional[StateDictConfig]) \u2013 the model state_dict configuration for the target state_dict_type. \noptim_state_dict_config (Optional[OptimStateDictConfig]) \u2013 the optimizer state_dict configuration for the target state_dict_type.   Return type \nGenerator   \n  \nstatic summon_full_params(module, recurse=True, writeback=True, rank0_only=False, offload_to_cpu=False, with_grads=False) [source]\n \nA context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.  Note This can be used on inner FSDPs.   Note This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.   Note Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.   Note The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.   Note This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.   Warning Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.   Warning Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.   Parameters \n \nrecurse (bool, Optional) \u2013 recursively summon all params for nested FSDP instances (default: True). \nwriteback (bool, Optional) \u2013 if False, modifications to params are discarded after the context manager exits; disabling this can be slightly more efficient (default: True) \nrank0_only (bool, Optional) \u2013 if True, full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting rank0_only=True with writeback=True is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited. \noffload_to_cpu (bool, Optional) \u2013 If True, full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or NO_SHARD config). It is recommended to use offload_to_cpu with rank0_only=True to avoid redundant copies of model parameters being offloaded to the same CPU memory. \nwith_grads (bool, Optional) \u2013 If True, gradients are also unsharded with the parameters. Currently, this is only supported when passing use_orig_params=True to the FSDP constructor and offload_to_cpu=False to this method. (Default: False)   Return type \nGenerator   \n \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.apply()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.apply", "type": "Fully Sharded Data Parallel", "text": " \napply(fn) [source]\n \nApplies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init). Compared to torch.nn.Module.apply, this version additionally gathers the full parameters before applying fn. It should not be called from within another summon_full_params context.  Parameters \nfn (Module -> None) \u2013 function to be applied to each submodule  Returns \nself  Return type \nModule   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.clip_grad_norm_", "type": "Fully Sharded Data Parallel", "text": " \nclip_grad_norm_(max_norm, norm_type=2.0) [source]\n \nClips the gradient norm of all parameters. The norm is computed over all parameters\u2019 gradients as viewed as a single vector, and the gradients are modified in-place.  Parameters \n \nmax_norm (float or int) \u2013 max norm of the gradients \nnorm_type (float or int) \u2013 type of the used p-norm. Can be 'inf' for infinity norm.   Returns \nTotal norm of the parameters (viewed as a single vector).  Return type \nTensor    Note If every FSDP instance uses NO_SHARD, meaning that no gradients are sharded across ranks, then you may directly use torch.nn.utils.clip_grad_norm_().   Note If at least some FSDP instance uses a sharded strategy (i.e. one other than NO_SHARD), then you should use this method instead of torch.nn.utils.clip_grad_norm_() since this method handles the fact that gradients are sharded across ranks.   Note The total norm returned will have the \u201clargest\u201d dtype across all parameters/gradients as defined by PyTorch\u2019s type promotion semantics. For example, if all parameters/gradients use a low precision dtype, then the returned norm\u2019s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm\u2019s dtype will be FP32.   Warning This needs to be called on all ranks since it uses collective communications.  \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.flatten_sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic flatten_sharded_optim_state_dict(sharded_optim_state_dict, model, optim) [source]\n \nThe API is similar to shard_full_optim_state_dict(). The only difference is that the input sharded_optim_state_dict should be returned from sharded_optim_state_dict(). Therefore, there will be all-gather calls on each rank to gather ShardedTensor s.  Parameters \n \nsharded_optim_state_dict (Dict[str, Any]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the sharded optimizer state. \nmodel (torch.nn.Module) \u2013 Refer to shard_full_optim_state_dict(). \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters.   Returns \nRefer to shard_full_optim_state_dict().  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.forward()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.forward", "type": "Fully Sharded Data Parallel", "text": " \nforward(*args, **kwargs) [source]\n \nRuns the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.  Return type \nAny   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.fsdp_modules", "type": "Fully Sharded Data Parallel", "text": " \nstatic fsdp_modules(module, root_only=False) [source]\n \nReturns all nested FSDP instances, possibly including module itself and only including FSDP root modules if root_only=True.  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module, which may or may not be an FSDP module. \nroot_only (bool) \u2013 Whether to return only FSDP root modules. (Default: False)   Returns \nFSDP modules that are nested in the input module.  Return type \nList[FullyShardedDataParallel]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic full_optim_state_dict(model, optim, optim_input=None, rank0_only=True, group=None) [source]\n \nConsolidates the full optimizer state on rank 0 and returns it as a dict following the convention of torch.optim.Optimizer.state_dict(), i.e. with keys \"state\" and \"param_groups\". The flattened parameters in FSDP modules contained in model are mapped back to their unflattened parameters.  Warning This needs to be called on all ranks since it uses collective communications. However, if rank0_only=True, then the state dict is only populated on rank 0, and all other ranks return an empty dict.   Warning Unlike torch.optim.Optimizer.state_dict(), this method uses full parameter names as keys instead of parameter IDs.   Note Like in torch.optim.Optimizer.state_dict(), the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using torch.save().   Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer optim representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \nrank0_only (bool) \u2013 If True, saves the populated dict only on rank 0; if False, saves it on all ranks. (Default: True) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group or None if using the default process group. (Default: None)   Returns \nA dict containing the optimizer state for model \u2018s original unflattened parameters and including keys \u201cstate\u201d and \u201cparam_groups\u201d following the convention of torch.optim.Optimizer.state_dict(). If rank0_only=True, then nonzero ranks return an empty dict.  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.get_state_dict_type", "type": "Fully Sharded Data Parallel", "text": " \nstatic get_state_dict_type(module) [source]\n \nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted at module. The target module does not have to be an FSDP module.  Returns \nA StateDictSettings containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.  Raises \n \nAssertionError` if the StateDictSettings for differen \u2013  \nFSDP submodules differ. \u2013    Return type \nStateDictSettings   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.module", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.module", "type": "Fully Sharded Data Parallel", "text": " \nproperty module: Module  \nReturns the wrapped module (like DistributedDataParallel). \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_buffers()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_buffers", "type": "Fully Sharded Data Parallel", "text": " \nnamed_buffers(*args, **kwargs) [source]\n \nOverrides named_buffers() to intercept buffer names and remove all occurrences of the FSDP-specific flattened buffer prefix when inside the summon_full_params() context manager.  Return type \nIterator[Tuple[str, Tensor]]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.named_parameters()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.named_parameters", "type": "Fully Sharded Data Parallel", "text": " \nnamed_parameters(*args, **kwargs) [source]\n \nOverrides named_parameters() to intercept parameter names and remove all occurrences of the FSDP-specific flattened parameter prefix when inside the summon_full_params() context manager.  Return type \nIterator[Tuple[str, Parameter]]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.no_sync()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.no_sync", "type": "Fully Sharded Data Parallel", "text": " \nno_sync() [source]\n \nA context manager to disable gradient synchronizations across FSDP instances. Within this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.  Note This likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.   Note When used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.   Return type \nGenerator   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic optim_state_dict(model, optim, optim_state_dict=None, group=None) [source]\n \nTransforms the state_dict of optim for the model that is sharded by FSDP to one of the three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict. For full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via state_dict_type() to avoid OOM. For sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via state_dict_type() to further save memory. For local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n  Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_state_dict (Dict[str, Any]) \u2013 the target optimizer state_dict to transform. If the value is None, optim.state_dict() will be used. ( Default: None) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters are sharded or None if using the default process group. ( Default: None)   Returns \nA dict containing the optimizer state for model. The sharding of the optimizer state is based on state_dict_type.  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.optim_state_dict_to_load", "type": "Fully Sharded Data Parallel", "text": " \nstatic optim_state_dict_to_load(model, optim, optim_state_dict, is_named_optimizer=False, load_directly=False, group=None) [source]\n \nGiven a optim_state_dict that is transformed through optim_state_dict(), converts it to the flattened optimizer state_dict that can be loaded to optim which is the optimizer for model. model must be sharded by FullyShardedDataParallel. >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> from torch.distributed.fsdp import StateDictType\n>>> from torch.distributed.fsdp import FullStateDictConfig\n>>> from torch.distributed.fsdp import FullOptimStateDictConfig\n>>> # Save a checkpoint\n>>> model, optim = ...\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> state_dict = model.state_dict()\n>>> original_osd = optim.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(\n>>>     model,\n>>>     optim,\n>>>     optim_state_dict=original_osd\n>>> )\n>>> save_a_checkpoint(state_dict, optim_state_dict)\n>>> # Load a checkpoint\n>>> model, optim = ...\n>>> state_dict, optim_state_dict = load_a_checkpoint()\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.FULL_STATE_DICT,\n>>>     FullStateDictConfig(rank0_only=False),\n>>>     FullOptimStateDictConfig(rank0_only=False),\n>>> )\n>>> model.load_state_dict(state_dict)\n>>> optim_state_dict = FSDP.optim_state_dict_to_load(\n>>>     optim_state_dict, model, optim\n>>> )\n>>> optim.load_state_dict(optim_state_dict)\n  Parameters \n \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters were passed into the optimizer optim. \noptim (torch.optim.Optimizer) \u2013 Optimizer for model \u2018s parameters. \noptim_state_dict (Dict[str, Any]) \u2013 The optimizer states to be loaded. \nis_named_optimizer (bool) \u2013 Is this optimizer a NamedOptimizer or KeyedOptimizer. Only set to True if optim is TorchRec\u2019s KeyedOptimizer or torch.distributed\u2019s NamedOptimizer. \nload_directly (bool) \u2013 If this is set to True, this API will also call optim.load_state_dict(result) before returning the result. Otherwise, users are responsible to call optim.load_state_dict() (Default: False) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group across which parameters are sharded or None if using the default process group. ( Default: None)   Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.register_comm_hook", "type": "Fully Sharded Data Parallel", "text": " \nregister_comm_hook(state, hook) [source]\n \nRegisters a communication hook which is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like GossipGrad and gradient compression which involve different communication strategies for parameter syncs while training with FullyShardedDataParallel.  Warning FSDP communication hook should be registered before running an initial forward pass and only once.   Parameters \n \nstate (object) \u2013 \nPassed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in GossipGrad, etc. It is locally stored by each worker and shared by all the gradient tensors on the worker.  \nhook (Callable) \u2013 Callable, which has one of the following signatures: 1) hook: Callable[torch.Tensor] -> None: This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns None; 2) hook: Callable[torch.Tensor, torch.Tensor] -> None: This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns None. Callables with signature 1 are expected to handle gradient communication for a NO_SHARD case. Callables with signature 2 are expected to handle gradient communication for sharded cases.    \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.rekey_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic rekey_optim_state_dict(optim_state_dict, optim_state_key_type, model, optim_input=None, optim=None) [source]\n \nRe-keys the optimizer state dict optim_state_dict to use the key type optim_state_key_type. This can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without. To re-key an FSDP full optimizer state dict (i.e. from full_optim_state_dict()) to use parameter IDs and be loadable to a non-wrapped model: >>> wrapped_model, wrapped_optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(wrapped_model, wrapped_optim)\n>>> nonwrapped_model, nonwrapped_optim = ...\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(full_osd, OptimStateKeyType.PARAM_ID, nonwrapped_model)\n>>> nonwrapped_optim.load_state_dict(rekeyed_osd)\n To re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model: >>> nonwrapped_model, nonwrapped_optim = ...\n>>> osd = nonwrapped_optim.state_dict()\n>>> rekeyed_osd = FSDP.rekey_optim_state_dict(osd, OptimStateKeyType.PARAM_NAME, nonwrapped_model)\n>>> wrapped_model, wrapped_optim = ...\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(rekeyed_osd, wrapped_model)\n>>> wrapped_optim.load_state_dict(sharded_osd)\n  Returns \nThe optimizer state dict re-keyed using the parameter keys specified by optim_state_key_type.  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.scatter_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic scatter_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None, group=None) [source]\n \nScatters the full optimizer state dict from rank 0 to all other ranks, returning the sharded optimizer state dict on each rank. The return value is the same as shard_full_optim_state_dict(), and on rank 0, the first argument should be the return value of full_optim_state_dict(). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)  # only non-empty on rank 0\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim, new_group = ...\n>>> sharded_osd = FSDP.scatter_full_optim_state_dict(full_osd, new_model, group=new_group)\n>>> new_optim.load_state_dict(sharded_osd)\n  Note Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.   Parameters \n \nfull_optim_state_dict (Optional[Dict[str, Any]]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state if on rank 0; the argument is ignored on nonzero ranks. \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \noptim (Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None) \ngroup (dist.ProcessGroup) \u2013 Model\u2019s process group or None if using the default process group. (Default: None)   Returns \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.set_state_dict_type", "type": "Fully Sharded Data Parallel", "text": " \nstatic set_state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None) [source]\n \nSet the state_dict_type and the corresponding (optional) configurations of all the descendant FSDP modules of the target module. The target module does not have to be a FSDP module. If the target module is a FSDP module, its state_dict_type will also be changed.  Note This API should be called for only the top-level (root) module.   Note This API enables users to transparently use the conventional state_dict API to take model checkpoints in cases where the root FSDP module is wrapped by another nn.Module. For example, the following will ensure state_dict is called on all non-FSDP instances, while dispatching into sharded_state_dict implementation for FSDP:  Example: >>> model = DDP(FSDP(...))\n>>> FSDP.set_state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>>     state_dict_config = ShardedStateDictConfig(offload_to_cpu=True),\n>>>     optim_state_dict_config = OptimStateDictConfig(offload_to_cpu=True),\n>>> )\n>>> param_state_dict = model.state_dict()\n>>> optim_state_dict = FSDP.optim_state_dict(model, optim)\n  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module. \nstate_dict_type (StateDictType) \u2013 the desired state_dict_type to set. \nstate_dict_config (Optional[StateDictConfig]) \u2013 the configuration for the target state_dict_type.   Returns \nA StateDictSettings that include the previous state_dict type and configuration for the module.  Return type \nStateDictSettings   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.shard_full_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic shard_full_optim_state_dict(full_optim_state_dict, model, optim_input=None, optim=None) [source]\n \nShards the full optimizer state dict full_optim_state_dict by remapping the state to flattened parameters instead of unflattened parameters and restricting to only this rank\u2019s part of the optimizer state. The first argument should be the return value of full_optim_state_dict(). Example: >>> from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n>>> model, optim = ...\n>>> full_osd = FSDP.full_optim_state_dict(model, optim)\n>>> torch.save(full_osd, PATH)\n>>> # Define new model with possibly different world size\n>>> new_model, new_optim = ...\n>>> full_osd = torch.load(PATH)\n>>> sharded_osd = FSDP.shard_full_optim_state_dict(full_osd, new_model)\n>>> new_optim.load_state_dict(sharded_osd)\n  Note Both shard_full_optim_state_dict() and scatter_full_optim_state_dict() may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.   Parameters \n \nfull_optim_state_dict (Dict[str, Any]) \u2013 Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state. \nmodel (torch.nn.Module) \u2013 Root module (which may or may not be a FullyShardedDataParallel instance) whose parameters correspond to the optimizer state in full_optim_state_dict. \noptim_input (Optional[Union[List[Dict[str, Any]], Iterable[torch.nn.Parameter]]]) \u2013 Input passed into the optimizer representing either a list of parameter groups or an iterable of parameters; if None, then this method assumes the input was model.parameters(). This argument is deprecated, and there is no need to pass it in anymore. (Default: None) \noptim (Optional[torch.optim.Optimizer]) \u2013 Optimizer that will load the state dict returned by this method. This is the preferred argument to use over optim_input. (Default: None)   Returns \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank\u2019s part of the optimizer state.  Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.sharded_optim_state_dict", "type": "Fully Sharded Data Parallel", "text": " \nstatic sharded_optim_state_dict(model, optim, group=None) [source]\n \nThe API is similar to full_optim_state_dict() but this API chunks all non-zero-dimension states to ShardedTensor to save memory. This API should only be used when the model state_dict is derived with the context manager with state_dict_type(SHARDED_STATE_DICT):. For the detailed usage, refer to full_optim_state_dict().  Warning The returned state dict contains ShardedTensor and cannot be directly used by the regular optim.load_state_dict.   Return type \nDict[str, Any]   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.state_dict_type", "type": "Fully Sharded Data Parallel", "text": " \nstatic state_dict_type(module, state_dict_type, state_dict_config=None, optim_state_dict_config=None) [source]\n \nA context manager to set the state_dict_type of all the descendant FSDP modules of the target module. This context manager has the same functions as set_state_dict_type(). Read the document of set_state_dict_type() for the detail. Example: >>> model = DDP(FSDP(...))\n>>> with FSDP.state_dict_type(\n>>>     model,\n>>>     StateDictType.SHARDED_STATE_DICT,\n>>> ):\n>>>     checkpoint = model.state_dict()\n  Parameters \n \nmodule (torch.nn.Module) \u2013 Root module. \nstate_dict_type (StateDictType) \u2013 the desired state_dict_type to set. \nstate_dict_config (Optional[StateDictConfig]) \u2013 the model state_dict configuration for the target state_dict_type. \noptim_state_dict_config (Optional[OptimStateDictConfig]) \u2013 the optimizer state_dict configuration for the target state_dict_type.   Return type \nGenerator   \n"}, {"name": "torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params()", "path": "fsdp#torch.distributed.fsdp.FullyShardedDataParallel.summon_full_params", "type": "Fully Sharded Data Parallel", "text": " \nstatic summon_full_params(module, recurse=True, writeback=True, rank0_only=False, offload_to_cpu=False, with_grads=False) [source]\n \nA context manager to expose full params for FSDP instances. Can be useful after forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the recurse argument.  Note This can be used on inner FSDPs.   Note This can not be used within a forward or backward pass. Nor can forward and backward be started from within this context.   Note Parameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.   Note The full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless writeback=False, in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when world_size == 1, or NO_SHARD config, the modification is persisted regardless of writeback.   Note This method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.   Warning Note that rank0_only=True in conjunction with writeback=True is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.   Warning Note that offload_to_cpu and rank0_only=False will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use offload_to_cpu with rank0_only=True.   Parameters \n \nrecurse (bool, Optional) \u2013 recursively summon all params for nested FSDP instances (default: True). \nwriteback (bool, Optional) \u2013 if False, modifications to params are discarded after the context manager exits; disabling this can be slightly more efficient (default: True) \nrank0_only (bool, Optional) \u2013 if True, full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting rank0_only=True with writeback=True is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited. \noffload_to_cpu (bool, Optional) \u2013 If True, full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or NO_SHARD config). It is recommended to use offload_to_cpu with rank0_only=True to avoid redundant copies of model parameters being offloaded to the same CPU memory. \nwith_grads (bool, Optional) \u2013 If True, gradients are also unsharded with the parameters. Currently, this is only supported when passing use_orig_params=True to the FSDP constructor and offload_to_cpu=False to this method. (Default: False)   Return type \nGenerator   \n"}, {"name": "torch.distributed.fsdp.LocalOptimStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalOptimStateDictConfig", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.LocalOptimStateDictConfig(offload_to_cpu: bool = False, use_dtensor: bool = False) [source]\n\n"}, {"name": "torch.distributed.fsdp.LocalStateDictConfig", "path": "fsdp#torch.distributed.fsdp.LocalStateDictConfig", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.LocalStateDictConfig(offload_to_cpu: bool = False, use_dtensor: bool = False) [source]\n\n"}, {"name": "torch.distributed.fsdp.MixedPrecision", "path": "fsdp#torch.distributed.fsdp.MixedPrecision", "type": "Fully Sharded Data Parallel", "text": " \nclass torch.distributed.fsdp.MixedPrecision(param_dtype=None, reduce_dtype=None, buffer_dtype=None, keep_low_precision_grads=False, cast_forward_inputs=False, cast_root_forward_inputs=True, _module_classes_to_ignore=(<class 'torch.nn.modules.batchnorm._BatchNorm'>, )) [source]\n \nThis configures FSDP-native mixed precision training.  Variables \n \nparam_dtype (Optional[torch.dtype]) \u2013 This specifies the dtype for model parameters during forward and backward and thus the dtype for forward and backward computation. Outside forward and backward, the sharded parameters are kept in full precision (e.g. for the optimizer step), and for model checkpointing, the parameters are always saved in full precision. (Default: None) \nreduce_dtype (Optional[torch.dtype]) \u2013 This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is None but param_dtype is not None, then this takes on the param_dtype value, still running gradient reduction in low precision. This is permitted to differ from param_dtype, e.g. to force gradient reduction to run in full precision. (Default: None) \nbuffer_dtype (Optional[torch.dtype]) \u2013 This specifies the dtype for buffers. FSDP does not shard buffers. Rather, FSDP casts them to buffer_dtype in the first forward pass and keeps them in that dtype thereafter. For model checkpointing, the buffers are saved in full precision except for LOCAL_STATE_DICT. (Default: None) \nkeep_low_precision_grads (bool) \u2013 If False, then FSDP upcasts gradients to full precision after the backward pass in preparation for the optimizer step. If True, then FSDP keeps the gradients in the dtype used for gradient reduction, which can save memory if using a custom optimizer that supports running in low precision. (Default: False) \ncast_forward_inputs (bool) \u2013 If True, then this FSDP module casts its forward args and kwargs to param_dtype. This is to ensure that parameter and input dtypes match for forward computation, as required by many ops. This may need to be set to True when only applying mixed precision to some but not all FSDP modules, in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default: False) \ncast_root_forward_inputs (bool) \u2013 If True, then the root FSDP module casts its forward args and kwargs to param_dtype, overriding the value of cast_forward_inputs. For non-root FSDP modules, this does not do anything. (Default: True) \n_module_classes_to_ignore (Sequence[Type[torch.nn.modules.module.Module]]) \u2013 (Sequence[Type[nn.Module]]): This specifies module classes to ignore for mixed precision when using an auto_wrap_policy: Modules of these classes will have FSDP applied to them separately with mixed precision disabled (meaning that the final FSDP construction would deviate from the specified policy). If auto_wrap_policy is not specified, then this does not do anything. This API is experimental and subject to change. (Default: (_BatchNorm,))     Note This API is experimental and subject to change.   Note Only floating point tensors are cast to their specified dtypes.   Note In summon_full_params, parameters are forced to full precision, but buffers are not.   Note Layer norm and batch norm accumulate in float32 even when their inputs are in a low precision like float16 or bfloat16. Disabling FSDP\u2019s mixed precision for those norm modules only means that the affine parameters are kept in flo