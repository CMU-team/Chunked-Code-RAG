[{"name": "10 minutes to pandas", "path": "user_guide/10min", "type": "Manual", "text": "\nThis is a short introduction to pandas, geared mainly for new users. You can\nsee more complex recipes in the Cookbook.\n\nCustomarily, we import as follows:\n\nSee the Intro to data structures section.\n\nCreating a `Series` by passing a list of values, letting pandas create a\ndefault integer index:\n\nCreating a `DataFrame` by passing a NumPy array, with a datetime index and\nlabeled columns:\n\nCreating a `DataFrame` by passing a dictionary of objects that can be\nconverted into a series-like structure:\n\nThe columns of the resulting `DataFrame` have different dtypes:\n\nIf you\u2019re using IPython, tab completion for column names (as well as public\nattributes) is automatically enabled. Here\u2019s a subset of the attributes that\nwill be completed:\n\nAs you can see, the columns `A`, `B`, `C`, and `D` are automatically tab\ncompleted. `E` and `F` are there as well; the rest of the attributes have been\ntruncated for brevity.\n\nSee the Basics section.\n\nHere is how to view the top and bottom rows of the frame:\n\nDisplay the index, columns:\n\n`DataFrame.to_numpy()` gives a NumPy representation of the underlying data.\nNote that this can be an expensive operation when your `DataFrame` has columns\nwith different data types, which comes down to a fundamental difference\nbetween pandas and NumPy: NumPy arrays have one dtype for the entire array,\nwhile pandas DataFrames have one dtype per column. When you call\n`DataFrame.to_numpy()`, pandas will find the NumPy dtype that can hold all of\nthe dtypes in the DataFrame. This may end up being `object`, which requires\ncasting every value to a Python object.\n\nFor `df`, our `DataFrame` of all floating-point values, `DataFrame.to_numpy()`\nis fast and doesn\u2019t require copying data:\n\nFor `df2`, the `DataFrame` with multiple dtypes, `DataFrame.to_numpy()` is\nrelatively expensive:\n\nNote\n\n`DataFrame.to_numpy()` does not include the index or column labels in the\noutput.\n\n`describe()` shows a quick statistic summary of your data:\n\nTransposing your data:\n\nSorting by an axis:\n\nSorting by values:\n\nNote\n\nWhile standard Python / NumPy expressions for selecting and setting are\nintuitive and come in handy for interactive work, for production code, we\nrecommend the optimized pandas data access methods, `.at`, `.iat`, `.loc` and\n`.iloc`.\n\nSee the indexing documentation Indexing and Selecting Data and MultiIndex /\nAdvanced Indexing.\n\nSelecting a single column, which yields a `Series`, equivalent to `df.A`:\n\nSelecting via `[]`, which slices the rows:\n\nSee more in Selection by Label.\n\nFor getting a cross section using a label:\n\nSelecting on a multi-axis by label:\n\nShowing label slicing, both endpoints are included:\n\nReduction in the dimensions of the returned object:\n\nFor getting a scalar value:\n\nFor getting fast access to a scalar (equivalent to the prior method):\n\nSee more in Selection by Position.\n\nSelect via the position of the passed integers:\n\nBy integer slices, acting similar to NumPy/Python:\n\nBy lists of integer position locations, similar to the NumPy/Python style:\n\nFor slicing rows explicitly:\n\nFor slicing columns explicitly:\n\nFor getting a value explicitly:\n\nFor getting fast access to a scalar (equivalent to the prior method):\n\nUsing a single column\u2019s values to select data:\n\nSelecting values from a DataFrame where a boolean condition is met:\n\nUsing the `isin()` method for filtering:\n\nSetting a new column automatically aligns the data by the indexes:\n\nSetting values by label:\n\nSetting values by position:\n\nSetting by assigning with a NumPy array:\n\nThe result of the prior setting operations:\n\nA `where` operation with setting:\n\npandas primarily uses the value `np.nan` to represent missing data. It is by\ndefault not included in computations. See the Missing Data section.\n\nReindexing allows you to change/add/delete the index on a specified axis. This\nreturns a copy of the data:\n\nTo drop any rows that have missing data:\n\nFilling missing data:\n\nTo get the boolean mask where values are `nan`:\n\nSee the Basic section on Binary Ops.\n\nOperations in general exclude missing data.\n\nPerforming a descriptive statistic:\n\nSame operation on the other axis:\n\nOperating with objects that have different dimensionality and need alignment.\nIn addition, pandas automatically broadcasts along the specified dimension:\n\nApplying functions to the data:\n\nSee more at Histogramming and Discretization.\n\nSeries is equipped with a set of string processing methods in the `str`\nattribute that make it easy to operate on each element of the array, as in the\ncode snippet below. Note that pattern-matching in `str` generally uses regular\nexpressions by default (and in some cases always uses them). See more at\nVectorized String Methods.\n\npandas provides various facilities for easily combining together Series and\nDataFrame objects with various kinds of set logic for the indexes and\nrelational algebra functionality in the case of join / merge-type operations.\n\nSee the Merging section.\n\nConcatenating pandas objects together with `concat()`:\n\nNote\n\nAdding a column to a `DataFrame` is relatively fast. However, adding a row\nrequires a copy, and may be expensive. We recommend passing a pre-built list\nof records to the `DataFrame` constructor instead of building a `DataFrame` by\niteratively appending records to it.\n\nSQL style merges. See the Database style joining section.\n\nAnother example that can be given is:\n\nBy \u201cgroup by\u201d we are referring to a process involving one or more of the\nfollowing steps:\n\nSplitting the data into groups based on some criteria\n\nApplying a function to each group independently\n\nCombining the results into a data structure\n\nSee the Grouping section.\n\nGrouping and then applying the `sum()` function to the resulting groups:\n\nGrouping by multiple columns forms a hierarchical index, and again we can\napply the `sum()` function:\n\nSee the sections on Hierarchical Indexing and Reshaping.\n\nThe `stack()` method \u201ccompresses\u201d a level in the DataFrame\u2019s columns:\n\nWith a \u201cstacked\u201d DataFrame or Series (having a `MultiIndex` as the `index`),\nthe inverse operation of `stack()` is `unstack()`, which by default unstacks\nthe last level:\n\nSee the section on Pivot Tables.\n\nWe can produce pivot tables from this data very easily:\n\npandas has simple, powerful, and efficient functionality for performing\nresampling operations during frequency conversion (e.g., converting secondly\ndata into 5-minutely data). This is extremely common in, but not limited to,\nfinancial applications. See the Time Series section.\n\nTime zone representation:\n\nConverting to another time zone:\n\nConverting between time span representations:\n\nConverting between period and timestamp enables some convenient arithmetic\nfunctions to be used. In the following example, we convert a quarterly\nfrequency with year ending in November to 9am of the end of the month\nfollowing the quarter end:\n\npandas can include categorical data in a `DataFrame`. For full docs, see the\ncategorical introduction and the API documentation.\n\nConverting the raw grades to a categorical data type:\n\nRename the categories to more meaningful names (assigning to\n`Series.cat.categories()` is in place!):\n\nReorder the categories and simultaneously add the missing categories (methods\nunder `Series.cat()` return a new `Series` by default):\n\nSorting is per order in the categories, not lexical order:\n\nGrouping by a categorical column also shows empty categories:\n\nSee the Plotting docs.\n\nWe use the standard convention for referencing the matplotlib API:\n\nThe `close()` method is used to close a figure window:\n\nIf running under Jupyter Notebook, the plot will appear on `plot()`. Otherwise\nuse matplotlib.pyplot.show to show it or matplotlib.pyplot.savefig to write it\nto a file.\n\nOn a DataFrame, the `plot()` method is a convenience to plot all of the\ncolumns with labels:\n\nWriting to a csv file:\n\nReading from a csv file:\n\nReading and writing to HDFStores.\n\nWriting to a HDF5 Store:\n\nReading from a HDF5 Store:\n\nReading and writing to MS Excel.\n\nWriting to an excel file:\n\nReading from an excel file:\n\nIf you are attempting to perform an operation you might see an exception like:\n\nSee Comparisons for an explanation and what to do.\n\nSee Gotchas as well.\n\n"}, {"name": "API reference", "path": "reference/index", "type": "General functions", "text": "\nThis page gives an overview of all public pandas objects, functions and\nmethods. All classes and functions exposed in `pandas.*` namespace are public.\n\nSome subpackages are public which include `pandas.errors`, `pandas.plotting`,\nand `pandas.testing`. Public functions in `pandas.io` and `pandas.tseries`\nsubmodules are mentioned in the documentation. `pandas.api.types` subpackage\nholds some public functions related to data types in pandas.\n\nWarning\n\nThe `pandas.core`, `pandas.compat`, and `pandas.util` top-level modules are\nPRIVATE. Stable functionality in such modules is not guaranteed.\n\n"}, {"name": "Categorical data", "path": "user_guide/categorical", "type": "Manual", "text": "\nThis is an introduction to pandas categorical data type, including a short\ncomparison with R\u2019s `factor`.\n\n`Categoricals` are a pandas data type corresponding to categorical variables\nin statistics. A categorical variable takes on a limited, and usually fixed,\nnumber of possible values (`categories`; `levels` in R). Examples are gender,\nsocial class, blood type, country affiliation, observation time or rating via\nLikert scales.\n\nIn contrast to statistical categorical variables, categorical data might have\nan order (e.g. \u2018strongly agree\u2019 vs \u2018agree\u2019 or \u2018first observation\u2019 vs. \u2018second\nobservation\u2019), but numerical operations (additions, divisions, \u2026) are not\npossible.\n\nAll values of categorical data are either in `categories` or `np.nan`. Order\nis defined by the order of `categories`, not lexical order of the values.\nInternally, the data structure consists of a `categories` array and an integer\narray of `codes` which point to the real value in the `categories` array.\n\nThe categorical data type is useful in the following cases:\n\nA string variable consisting of only a few different values. Converting such a\nstring variable to a categorical variable will save some memory, see here.\n\nThe lexical order of a variable is not the same as the logical order (\u201cone\u201d,\n\u201ctwo\u201d, \u201cthree\u201d). By converting to a categorical and specifying an order on the\ncategories, sorting and min/max will use the logical order instead of the\nlexical order, see here.\n\nAs a signal to other Python libraries that this column should be treated as a\ncategorical variable (e.g. to use suitable statistical methods or plot types).\n\nSee also the API docs on categoricals.\n\nCategorical `Series` or columns in a `DataFrame` can be created in several\nways:\n\nBy specifying `dtype=\"category\"` when constructing a `Series`:\n\nBy converting an existing `Series` or column to a `category` dtype:\n\nBy using special functions, such as `cut()`, which groups data into discrete\nbins. See the example on tiling in the docs.\n\nBy passing a `pandas.Categorical` object to a `Series` or assigning it to a\n`DataFrame`.\n\nCategorical data has a specific `category` dtype:\n\nSimilar to the previous section where a single column was converted to\ncategorical, all columns in a `DataFrame` can be batch converted to\ncategorical either during or after construction.\n\nThis can be done during construction by specifying `dtype=\"category\"` in the\n`DataFrame` constructor:\n\nNote that the categories present in each column differ; the conversion is done\ncolumn by column, so only labels present in a given column are categories:\n\nAnalogously, all columns in an existing `DataFrame` can be batch converted\nusing `DataFrame.astype()`:\n\nThis conversion is likewise done column by column:\n\nIn the examples above where we passed `dtype='category'`, we used the default\nbehavior:\n\nCategories are inferred from the data.\n\nCategories are unordered.\n\nTo control those behaviors, instead of passing `'category'`, use an instance\nof `CategoricalDtype`.\n\nSimilarly, a `CategoricalDtype` can be used with a `DataFrame` to ensure that\ncategories are consistent among all columns.\n\nNote\n\nTo perform table-wise conversion, where all labels in the entire `DataFrame`\nare used as categories for each column, the `categories` parameter can be\ndetermined programmatically by `categories =\npd.unique(df.to_numpy().ravel())`.\n\nIf you already have `codes` and `categories`, you can use the `from_codes()`\nconstructor to save the factorize step during normal constructor mode:\n\nTo get back to the original `Series` or NumPy array, use\n`Series.astype(original_dtype)` or `np.asarray(categorical)`:\n\nNote\n\nIn contrast to R\u2019s `factor` function, categorical data is not converting input\nvalues to strings; categories will end up the same data type as the original\nvalues.\n\nNote\n\nIn contrast to R\u2019s `factor` function, there is currently no way to\nassign/change labels at creation time. Use `categories` to change the\ncategories after creation time.\n\nA categorical\u2019s type is fully described by\n\n`categories`: a sequence of unique values and no missing values\n\n`ordered`: a boolean\n\nThis information can be stored in a `CategoricalDtype`. The `categories`\nargument is optional, which implies that the actual categories should be\ninferred from whatever is present in the data when the `pandas.Categorical` is\ncreated. The categories are assumed to be unordered by default.\n\nA `CategoricalDtype` can be used in any place pandas expects a `dtype`. For\nexample `pandas.read_csv()`, `pandas.DataFrame.astype()`, or in the `Series`\nconstructor.\n\nNote\n\nAs a convenience, you can use the string `'category'` in place of a\n`CategoricalDtype` when you want the default behavior of the categories being\nunordered, and equal to the set values present in the array. In other words,\n`dtype='category'` is equivalent to `dtype=CategoricalDtype()`.\n\nTwo instances of `CategoricalDtype` compare equal whenever they have the same\ncategories and order. When comparing two unordered categoricals, the order of\nthe `categories` is not considered.\n\nAll instances of `CategoricalDtype` compare equal to the string `'category'`.\n\nWarning\n\nSince `dtype='category'` is essentially `CategoricalDtype(None, False)`, and\nsince all instances `CategoricalDtype` compare equal to `'category'`, all\ninstances of `CategoricalDtype` compare equal to a `CategoricalDtype(None,\nFalse)`, regardless of `categories` or `ordered`.\n\nUsing `describe()` on categorical data will produce similar output to a\n`Series` or `DataFrame` of type `string`.\n\nCategorical data has a `categories` and a `ordered` property, which list their\npossible values and whether the ordering matters or not. These properties are\nexposed as `s.cat.categories` and `s.cat.ordered`. If you don\u2019t manually\nspecify categories and ordering, they are inferred from the passed arguments.\n\nIt\u2019s also possible to pass in the categories in a specific order:\n\nNote\n\nNew categorical data are not automatically ordered. You must explicitly pass\n`ordered=True` to indicate an ordered `Categorical`.\n\nNote\n\nThe result of `unique()` is not always the same as `Series.cat.categories`,\nbecause `Series.unique()` has a couple of guarantees, namely that it returns\ncategories in the order of appearance, and it only includes values that are\nactually present.\n\nRenaming categories is done by assigning new values to the\n`Series.cat.categories` property or by using the `rename_categories()` method:\n\nNote\n\nIn contrast to R\u2019s `factor`, categorical data can have categories of other\ntypes than string.\n\nNote\n\nBe aware that assigning new categories is an inplace operation, while most\nother operations under `Series.cat` per default return a new `Series` of dtype\n`category`.\n\nCategories must be unique or a `ValueError` is raised:\n\nCategories must also not be `NaN` or a `ValueError` is raised:\n\nAppending categories can be done by using the `add_categories()` method:\n\nRemoving categories can be done by using the `remove_categories()` method.\nValues which are removed are replaced by `np.nan`.:\n\nRemoving unused categories can also be done:\n\nIf you want to do remove and add new categories in one step (which has some\nspeed advantage), or simply set the categories to a predefined scale, use\n`set_categories()`.\n\nNote\n\nBe aware that `Categorical.set_categories()` cannot know whether some category\nis omitted intentionally or because it is misspelled or (under Python3) due to\na type difference (e.g., NumPy S1 dtype and Python strings). This can result\nin surprising behaviour!\n\nIf categorical data is ordered (`s.cat.ordered == True`), then the order of\nthe categories has a meaning and certain operations are possible. If the\ncategorical is unordered, `.min()/.max()` will raise a `TypeError`.\n\nYou can set categorical data to be ordered by using `as_ordered()` or\nunordered by using `as_unordered()`. These will by default return a new\nobject.\n\nSorting will use the order defined by categories, not any lexical order\npresent on the data type. This is even true for strings and numeric data:\n\nReordering the categories is possible via the\n`Categorical.reorder_categories()` and the `Categorical.set_categories()`\nmethods. For `Categorical.reorder_categories()`, all old categories must be\nincluded in the new categories and no new categories are allowed. This will\nnecessarily make the sort order the same as the categories order.\n\nNote\n\nNote the difference between assigning new categories and reordering the\ncategories: the first renames categories and therefore the individual values\nin the `Series`, but if the first position was sorted last, the renamed value\nwill still be sorted last. Reordering means that the way values are sorted is\ndifferent afterwards, but not that individual values in the `Series` are\nchanged.\n\nNote\n\nIf the `Categorical` is not ordered, `Series.min()` and `Series.max()` will\nraise `TypeError`. Numeric operations like `+`, `-`, `*`, `/` and operations\nbased on them (e.g. `Series.median()`, which would need to compute the mean\nbetween two values if the length of an array is even) do not work and raise a\n`TypeError`.\n\nA categorical dtyped column will participate in a multi-column sort in a\nsimilar manner to other columns. The ordering of the categorical is determined\nby the `categories` of that column.\n\nReordering the `categories` changes a future sort.\n\nComparing categorical data with other objects is possible in three cases:\n\nComparing equality (`==` and `!=`) to a list-like object (list, Series, array,\n\u2026) of the same length as the categorical data.\n\nAll comparisons (`==`, `!=`, `>`, `>=`, `<`, and `<=`) of categorical data to\nanother categorical Series, when `ordered==True` and the `categories` are the\nsame.\n\nAll comparisons of a categorical data to a scalar.\n\nAll other comparisons, especially \u201cnon-equality\u201d comparisons of two\ncategoricals with different categories or a categorical with any list-like\nobject, will raise a `TypeError`.\n\nNote\n\nAny \u201cnon-equality\u201d comparisons of categorical data with a `Series`,\n`np.array`, `list` or categorical data with different categories or ordering\nwill raise a `TypeError` because custom categories ordering could be\ninterpreted in two ways: one with taking into account the ordering and one\nwithout.\n\nComparing to a categorical with the same categories and ordering or to a\nscalar works:\n\nEquality comparisons work with any list-like object of same length and\nscalars:\n\nThis doesn\u2019t work because the categories are not the same:\n\nIf you want to do a \u201cnon-equality\u201d comparison of a categorical series with a\nlist-like object which is not categorical data, you need to be explicit and\nconvert the categorical data back to the original values:\n\nWhen you compare two unordered categoricals with the same categories, the\norder is not considered:\n\nApart from `Series.min()`, `Series.max()` and `Series.mode()`, the following\noperations are possible with categorical data:\n\n`Series` methods like `Series.value_counts()` will use all categories, even if\nsome categories are not present in the data:\n\n`DataFrame` methods like `DataFrame.sum()` also show \u201cunused\u201d categories.\n\nGroupby will also show \u201cunused\u201d categories:\n\nPivot tables:\n\nThe optimized pandas data access methods `.loc`, `.iloc`, `.at`, and `.iat`,\nwork as normal. The only difference is the return type (for getting) and that\nonly values already in `categories` can be assigned.\n\nIf the slicing operation returns either a `DataFrame` or a column of type\n`Series`, the `category` dtype is preserved.\n\nAn example where the category type is not preserved is if you take one single\nrow: the resulting `Series` is of dtype `object`:\n\nReturning a single item from categorical data will also return the value, not\na categorical of length \u201c1\u201d.\n\nNote\n\nThe is in contrast to R\u2019s `factor` function, where `factor(c(1,2,3))[1]`\nreturns a single value `factor`.\n\nTo get a single value `Series` of type `category`, you pass in a list with a\nsingle value:\n\nThe accessors `.dt` and `.str` will work if the `s.cat.categories` are of an\nappropriate type:\n\nNote\n\nThe returned `Series` (or `DataFrame`) is of the same type as if you used the\n`.str.<method>` / `.dt.<method>` on a `Series` of that type (and not of type\n`category`!).\n\nThat means, that the returned values from methods and properties on the\naccessors of a `Series` and the returned values from methods and properties on\nthe accessors of this `Series` transformed to one of type `category` will be\nequal:\n\nNote\n\nThe work is done on the `categories` and then a new `Series` is constructed.\nThis has some performance implication if you have a `Series` of type string,\nwhere lots of elements are repeated (i.e. the number of unique elements in the\n`Series` is a lot smaller than the length of the `Series`). In this case it\ncan be faster to convert the original `Series` to one of type `category` and\nuse `.str.<method>` or `.dt.<property>` on that.\n\nSetting values in a categorical column (or `Series`) works as long as the\nvalue is included in the `categories`:\n\nSetting values by assigning categorical data will also check that the\n`categories` match:\n\nAssigning a `Categorical` to parts of a column of other types will use the\nvalues:\n\nBy default, combining `Series` or `DataFrames` which contain the same\ncategories results in `category` dtype, otherwise results will depend on the\ndtype of the underlying categories. Merges that result in non-categorical\ndtypes will likely have higher memory usage. Use `.astype` or\n`union_categoricals` to ensure `category` results.\n\nThe following table summarizes the results of merging `Categoricals`:\n\narg1\n\narg2\n\nidentical\n\nresult\n\ncategory\n\ncategory\n\nTrue\n\ncategory\n\ncategory (object)\n\ncategory (object)\n\nFalse\n\nobject (dtype is inferred)\n\ncategory (int)\n\ncategory (float)\n\nFalse\n\nfloat (dtype is inferred)\n\nSee also the section on merge dtypes for notes about preserving merge dtypes\nand performance.\n\nIf you want to combine categoricals that do not necessarily have the same\ncategories, the `union_categoricals()` function will combine a list-like of\ncategoricals. The new categories will be the union of the categories being\ncombined.\n\nBy default, the resulting categories will be ordered as they appear in the\ndata. If you want the categories to be lexsorted, use `sort_categories=True`\nargument.\n\n`union_categoricals` also works with the \u201ceasy\u201d case of combining two\ncategoricals of the same categories and order information (e.g. what you could\nalso `append` for).\n\nThe below raises `TypeError` because the categories are ordered and not\nidentical.\n\nOrdered categoricals with different categories or orderings can be combined by\nusing the `ignore_ordered=True` argument.\n\n`union_categoricals()` also works with a `CategoricalIndex`, or `Series`\ncontaining categorical data, but note that the resulting array will always be\na plain `Categorical`:\n\nNote\n\n`union_categoricals` may recode the integer codes for categories when\ncombining categoricals. This is likely what you want, but if you are relying\non the exact numbering of the categories, be aware.\n\nYou can write data that contains `category` dtypes to a `HDFStore`. See here\nfor an example and caveats.\n\nIt is also possible to write data to and reading data from Stata format files.\nSee here for an example and caveats.\n\nWriting to a CSV file will convert the data, effectively removing any\ninformation about the categorical (categories and ordering). So if you read\nback the CSV file you have to convert the relevant columns back to `category`\nand assign the right categories and categories ordering.\n\nThe same holds for writing to a SQL database with `to_sql`.\n\npandas primarily uses the value `np.nan` to represent missing data. It is by\ndefault not included in computations. See the Missing Data section.\n\nMissing values should not be included in the Categorical\u2019s `categories`, only\nin the `values`. Instead, it is understood that NaN is different, and is\nalways a possibility. When working with the Categorical\u2019s `codes`, missing\nvalues will always have a code of `-1`.\n\nMethods for working with missing data, e.g. `isna()`, `fillna()`, `dropna()`,\nall work normally:\n\nThe following differences to R\u2019s factor functions can be observed:\n\nR\u2019s `levels` are named `categories`.\n\nR\u2019s `levels` are always of type string, while `categories` in pandas can be of\nany dtype.\n\nIt\u2019s not possible to specify labels at creation time. Use\n`s.cat.rename_categories(new_labels)` afterwards.\n\nIn contrast to R\u2019s `factor` function, using categorical data as the sole input\nto create a new categorical series will not remove unused categories but\ncreate a new categorical series which is equal to the passed in one!\n\nR allows for missing values to be included in its `levels` (pandas\u2019\n`categories`). pandas does not allow `NaN` categories, but missing values can\nstill be in the `values`.\n\nThe memory usage of a `Categorical` is proportional to the number of\ncategories plus the length of the data. In contrast, an `object` dtype is a\nconstant times the length of the data.\n\nNote\n\nIf the number of categories approaches the length of the data, the\n`Categorical` will use nearly the same or more memory than an equivalent\n`object` dtype representation.\n\nCurrently, categorical data and the underlying `Categorical` is implemented as\na Python object and not as a low-level NumPy array dtype. This leads to some\nproblems.\n\nNumPy itself doesn\u2019t know about the new `dtype`:\n\nDtype comparisons work:\n\nTo check if a Series contains Categorical data, use `hasattr(s, 'cat')`:\n\nUsing NumPy functions on a `Series` of type `category` should not work as\n`Categoricals` are not numeric data (even in the case that `.categories` is\nnumeric).\n\nNote\n\nIf such a function works, please file a bug at https://github.com/pandas-\ndev/pandas!\n\npandas currently does not preserve the dtype in apply functions: If you apply\nalong rows you get a `Series` of `object` `dtype` (same as getting a row ->\ngetting one element will return a basic type) and applying along columns will\nalso convert to object. `NaN` values are unaffected. You can use `fillna` to\nhandle missing values before applying a function.\n\n`CategoricalIndex` is a type of index that is useful for supporting indexing\nwith duplicates. This is a container around a `Categorical` and allows\nefficient indexing and storage of an index with a large number of duplicated\nelements. See the advanced indexing docs for a more detailed explanation.\n\nSetting the index will create a `CategoricalIndex`:\n\nConstructing a `Series` from a `Categorical` will not copy the input\n`Categorical`. This means that changes to the `Series` will in most cases\nchange the original `Categorical`:\n\nUse `copy=True` to prevent such a behaviour or simply don\u2019t reuse\n`Categoricals`:\n\nNote\n\nThis also happens in some cases when you supply a NumPy array instead of a\n`Categorical`: using an int array (e.g. `np.array([1,2,3,4])`) will exhibit\nthe same behavior, while using a string array (e.g.\n`np.array([\"a\",\"b\",\"c\",\"a\"])`) will not.\n\n"}, {"name": "Chart Visualization", "path": "user_guide/visualization", "type": "Manual", "text": "\nThis section demonstrates visualization through charting. For information on\nvisualization of tabular data please see the section on Table Visualization.\n\nWe use the standard convention for referencing the matplotlib API:\n\nWe provide the basics in pandas to easily create decent looking plots. See the\necosystem section for visualization libraries that go beyond the basics\ndocumented here.\n\nNote\n\nAll calls to `np.random` are seeded with 123456.\n\nWe will demonstrate the basics, see the cookbook for some advanced strategies.\n\nThe `plot` method on Series and DataFrame is just a simple wrapper around\n`plt.plot()`:\n\nIf the index consists of dates, it calls `gcf().autofmt_xdate()` to try to\nformat the x-axis nicely as per above.\n\nOn DataFrame, `plot()` is a convenience to plot all of the columns with\nlabels:\n\nYou can plot one column versus another using the `x` and `y` keywords in\n`plot()`:\n\nNote\n\nFor more formatting and styling options, see formatting below.\n\nPlotting methods allow for a handful of plot styles other than the default\nline plot. These methods can be provided as the `kind` keyword argument to\n`plot()`, and include:\n\n\u2018bar\u2019 or \u2018barh\u2019 for bar plots\n\n\u2018hist\u2019 for histogram\n\n\u2018box\u2019 for boxplot\n\n\u2018kde\u2019 or \u2018density\u2019 for density plots\n\n\u2018area\u2019 for area plots\n\n\u2018scatter\u2019 for scatter plots\n\n\u2018hexbin\u2019 for hexagonal bin plots\n\n\u2018pie\u2019 for pie plots\n\nFor example, a bar plot can be created the following way:\n\nYou can also create these other plots using the methods\n`DataFrame.plot.<kind>` instead of providing the `kind` keyword argument. This\nmakes it easier to discover plot methods and the specific arguments they use:\n\nIn addition to these `kind` s, there are the DataFrame.hist(), and\nDataFrame.boxplot() methods, which use a separate interface.\n\nFinally, there are several plotting functions in `pandas.plotting` that take a\n`Series` or `DataFrame` as an argument. These include:\n\nScatter Matrix\n\nAndrews Curves\n\nParallel Coordinates\n\nLag Plot\n\nAutocorrelation Plot\n\nBootstrap Plot\n\nRadViz\n\nPlots may also be adorned with errorbars or tables.\n\nFor labeled, non-time series data, you may wish to produce a bar plot:\n\nCalling a DataFrame\u2019s `plot.bar()` method produces a multiple bar plot:\n\nTo produce a stacked bar plot, pass `stacked=True`:\n\nTo get horizontal bar plots, use the `barh` method:\n\nHistograms can be drawn by using the `DataFrame.plot.hist()` and\n`Series.plot.hist()` methods.\n\nA histogram can be stacked using `stacked=True`. Bin size can be changed using\nthe `bins` keyword.\n\nYou can pass other keywords supported by matplotlib `hist`. For example,\nhorizontal and cumulative histograms can be drawn by\n`orientation='horizontal'` and `cumulative=True`.\n\nSee the `hist` method and the matplotlib hist documentation for more.\n\nThe existing interface `DataFrame.hist` to plot histogram still can be used.\n\n`DataFrame.hist()` plots the histograms of the columns on multiple subplots:\n\nThe `by` keyword can be specified to plot grouped histograms:\n\nIn addition, the `by` keyword can also be specified in\n`DataFrame.plot.hist()`.\n\nChanged in version 1.4.0.\n\nBoxplot can be drawn calling `Series.plot.box()` and `DataFrame.plot.box()`,\nor `DataFrame.boxplot()` to visualize the distribution of values within each\ncolumn.\n\nFor instance, here is a boxplot representing five trials of 10 observations of\na uniform random variable on [0,1).\n\nBoxplot can be colorized by passing `color` keyword. You can pass a `dict`\nwhose keys are `boxes`, `whiskers`, `medians` and `caps`. If some keys are\nmissing in the `dict`, default colors are used for the corresponding artists.\nAlso, boxplot has `sym` keyword to specify fliers style.\n\nWhen you pass other type of arguments via `color` keyword, it will be directly\npassed to matplotlib for all the `boxes`, `whiskers`, `medians` and `caps`\ncolorization.\n\nThe colors are applied to every boxes to be drawn. If you want more\ncomplicated colorization, you can get each drawn artists by passing\nreturn_type.\n\nAlso, you can pass other keywords supported by matplotlib `boxplot`. For\nexample, horizontal and custom-positioned boxplot can be drawn by `vert=False`\nand `positions` keywords.\n\nSee the `boxplot` method and the matplotlib boxplot documentation for more.\n\nThe existing interface `DataFrame.boxplot` to plot boxplot still can be used.\n\nYou can create a stratified boxplot using the `by` keyword argument to create\ngroupings. For instance,\n\nYou can also pass a subset of columns to plot, as well as group by multiple\ncolumns:\n\nYou could also create groupings with `DataFrame.plot.box()`, for instance:\n\nChanged in version 1.4.0.\n\nIn `boxplot`, the return type can be controlled by the `return_type`, keyword.\nThe valid choices are `{\"axes\", \"dict\", \"both\", None}`. Faceting, created by\n`DataFrame.boxplot` with the `by` keyword, will affect the output type as\nwell:\n\n`return_type`\n\nFaceted\n\nOutput type\n\n`None`\n\nNo\n\naxes\n\n`None`\n\nYes\n\n2-D ndarray of axes\n\n`'axes'`\n\nNo\n\naxes\n\n`'axes'`\n\nYes\n\nSeries of axes\n\n`'dict'`\n\nNo\n\ndict of artists\n\n`'dict'`\n\nYes\n\nSeries of dicts of artists\n\n`'both'`\n\nNo\n\nnamedtuple\n\n`'both'`\n\nYes\n\nSeries of namedtuples\n\n`Groupby.boxplot` always returns a `Series` of `return_type`.\n\nThe subplots above are split by the numeric columns first, then the value of\nthe `g` column. Below the subplots are first split by the value of `g`, then\nby the numeric columns.\n\nYou can create area plots with `Series.plot.area()` and\n`DataFrame.plot.area()`. Area plots are stacked by default. To produce stacked\narea plot, each column must be either all positive or all negative values.\n\nWhen input data contains `NaN`, it will be automatically filled by 0. If you\nwant to drop or fill by different values, use `dataframe.dropna()` or\n`dataframe.fillna()` before calling `plot`.\n\nTo produce an unstacked plot, pass `stacked=False`. Alpha value is set to 0.5\nunless otherwise specified:\n\nScatter plot can be drawn by using the `DataFrame.plot.scatter()` method.\nScatter plot requires numeric columns for the x and y axes. These can be\nspecified by the `x` and `y` keywords.\n\nTo plot multiple column groups in a single axes, repeat `plot` method\nspecifying target `ax`. It is recommended to specify `color` and `label`\nkeywords to distinguish each groups.\n\nThe keyword `c` may be given as the name of a column to provide colors for\neach point:\n\nIf a categorical column is passed to `c`, then a discrete colorbar will be\nproduced:\n\nNew in version 1.3.0.\n\nYou can pass other keywords supported by matplotlib `scatter`. The example\nbelow shows a bubble chart using a column of the `DataFrame` as the bubble\nsize.\n\nSee the `scatter` method and the matplotlib scatter documentation for more.\n\nYou can create hexagonal bin plots with `DataFrame.plot.hexbin()`. Hexbin\nplots can be a useful alternative to scatter plots if your data are too dense\nto plot each point individually.\n\nA useful keyword argument is `gridsize`; it controls the number of hexagons in\nthe x-direction, and defaults to 100. A larger `gridsize` means more, smaller\nbins.\n\nBy default, a histogram of the counts around each `(x, y)` point is computed.\nYou can specify alternative aggregations by passing values to the `C` and\n`reduce_C_function` arguments. `C` specifies the value at each `(x, y)` point\nand `reduce_C_function` is a function of one argument that reduces all the\nvalues in a bin to a single number (e.g. `mean`, `max`, `sum`, `std`). In this\nexample the positions are given by columns `a` and `b`, while the value is\ngiven by column `z`. The bins are aggregated with NumPy\u2019s `max` function.\n\nSee the `hexbin` method and the matplotlib hexbin documentation for more.\n\nYou can create a pie plot with `DataFrame.plot.pie()` or `Series.plot.pie()`.\nIf your data includes any `NaN`, they will be automatically filled with 0. A\n`ValueError` will be raised if there are any negative values in your data.\n\nFor pie plots it\u2019s best to use square figures, i.e. a figure aspect ratio 1.\nYou can create the figure with equal width and height, or force the aspect\nratio to be equal after plotting by calling `ax.set_aspect('equal')` on the\nreturned `axes` object.\n\nNote that pie plot with `DataFrame` requires that you either specify a target\ncolumn by the `y` argument or `subplots=True`. When `y` is specified, pie plot\nof selected column will be drawn. If `subplots=True` is specified, pie plots\nfor each column are drawn as subplots. A legend will be drawn in each pie\nplots by default; specify `legend=False` to hide it.\n\nYou can use the `labels` and `colors` keywords to specify the labels and\ncolors of each wedge.\n\nWarning\n\nMost pandas plots use the `label` and `color` arguments (note the lack of \u201cs\u201d\non those). To be consistent with `matplotlib.pyplot.pie()` you must use\n`labels` and `colors`.\n\nIf you want to hide wedge labels, specify `labels=None`. If `fontsize` is\nspecified, the value will be applied to wedge labels. Also, other keywords\nsupported by `matplotlib.pyplot.pie()` can be used.\n\nIf you pass values whose sum total is less than 1.0, matplotlib draws a\nsemicircle.\n\nSee the matplotlib pie documentation for more.\n\npandas tries to be pragmatic about plotting `DataFrames` or `Series` that\ncontain missing data. Missing values are dropped, left out, or filled\ndepending on the plot type.\n\nPlot Type\n\nNaN Handling\n\nLine\n\nLeave gaps at NaNs\n\nLine (stacked)\n\nFill 0\u2019s\n\nBar\n\nFill 0\u2019s\n\nScatter\n\nDrop NaNs\n\nHistogram\n\nDrop NaNs (column-wise)\n\nBox\n\nDrop NaNs (column-wise)\n\nArea\n\nFill 0\u2019s\n\nKDE\n\nDrop NaNs (column-wise)\n\nHexbin\n\nDrop NaNs\n\nPie\n\nFill 0\u2019s\n\nIf any of these defaults are not what you want, or if you want to be explicit\nabout how missing values are handled, consider using `fillna()` or `dropna()`\nbefore plotting.\n\nThese functions can be imported from `pandas.plotting` and take a `Series` or\n`DataFrame` as an argument.\n\nYou can create a scatter plot matrix using the `scatter_matrix` method in\n`pandas.plotting`:\n\nYou can create density plots using the `Series.plot.kde()` and\n`DataFrame.plot.kde()` methods.\n\nAndrews curves allow one to plot multivariate data as a large number of curves\nthat are created using the attributes of samples as coefficients for Fourier\nseries, see the Wikipedia entry for more information. By coloring these curves\ndifferently for each class it is possible to visualize data clustering. Curves\nbelonging to samples of the same class will usually be closer together and\nform larger structures.\n\nNote: The \u201cIris\u201d dataset is available here.\n\nParallel coordinates is a plotting technique for plotting multivariate data,\nsee the Wikipedia entry for an introduction. Parallel coordinates allows one\nto see clusters in data and to estimate other statistics visually. Using\nparallel coordinates points are represented as connected line segments. Each\nvertical line represents one attribute. One set of connected line segments\nrepresents one data point. Points that tend to cluster will appear closer\ntogether.\n\nLag plots are used to check if a data set or time series is random. Random\ndata should not exhibit any structure in the lag plot. Non-random structure\nimplies that the underlying data are not random. The `lag` argument may be\npassed, and when `lag=1` the plot is essentially `data[:-1]` vs. `data[1:]`.\n\nAutocorrelation plots are often used for checking randomness in time series.\nThis is done by computing autocorrelations for data values at varying time\nlags. If time series is random, such autocorrelations should be near zero for\nany and all time-lag separations. If time series is non-random then one or\nmore of the autocorrelations will be significantly non-zero. The horizontal\nlines displayed in the plot correspond to 95% and 99% confidence bands. The\ndashed line is 99% confidence band. See the Wikipedia entry for more about\nautocorrelation plots.\n\nBootstrap plots are used to visually assess the uncertainty of a statistic,\nsuch as mean, median, midrange, etc. A random subset of a specified size is\nselected from a data set, the statistic in question is computed for this\nsubset and the process is repeated a specified number of times. Resulting\nplots and histograms are what constitutes the bootstrap plot.\n\nRadViz is a way of visualizing multi-variate data. It is based on a simple\nspring tension minimization algorithm. Basically you set up a bunch of points\nin a plane. In our case they are equally spaced on a unit circle. Each point\nrepresents a single attribute. You then pretend that each sample in the data\nset is attached to each of these points by a spring, the stiffness of which is\nproportional to the numerical value of that attribute (they are normalized to\nunit interval). The point in the plane, where our sample settles to (where the\nforces acting on our sample are at an equilibrium) is where a dot representing\nour sample will be drawn. Depending on which class that sample belongs it will\nbe colored differently. See the R package Radviz for more information.\n\nNote: The \u201cIris\u201d dataset is available here.\n\nFrom version 1.5 and up, matplotlib offers a range of pre-configured plotting\nstyles. Setting the style can be used to easily give plots the general look\nthat you want. Setting the style is as easy as calling\n`matplotlib.style.use(my_plot_style)` before creating your plot. For example\nyou could write `matplotlib.style.use('ggplot')` for ggplot-style plots.\n\nYou can see the various available style names at `matplotlib.style.available`\nand it\u2019s very easy to try them out.\n\nMost plotting methods have a set of keyword arguments that control the layout\nand formatting of the returned plot:\n\nFor each kind of plot (e.g. `line`, `bar`, `scatter`) any additional arguments\nkeywords are passed along to the corresponding matplotlib function\n(`ax.plot()`, `ax.bar()`, `ax.scatter()`). These can be used to control\nadditional styling, beyond what pandas provides.\n\nYou may set the `legend` argument to `False` to hide the legend, which is\nshown by default.\n\nNew in version 1.1.0.\n\nYou may set the `xlabel` and `ylabel` arguments to give the plot custom labels\nfor x and y axis. By default, pandas will pick up index name as xlabel, while\nleaving it empty for ylabel.\n\nYou may pass `logy` to get a log-scale Y axis.\n\nSee also the `logx` and `loglog` keyword arguments.\n\nTo plot data on a secondary y-axis, use the `secondary_y` keyword:\n\nTo plot some columns in a `DataFrame`, give the column names to the\n`secondary_y` keyword:\n\nNote that the columns plotted on the secondary y-axis is automatically marked\nwith \u201c(right)\u201d in the legend. To turn off the automatic marking, use the\n`mark_right=False` keyword:\n\nChanged in version 1.0.0.\n\npandas provides custom formatters for timeseries plots. These change the\nformatting of the axis labels for dates and times. By default, the custom\nformatters are applied only to plots created by pandas with `DataFrame.plot()`\nor `Series.plot()`. To have them apply to all plots, including those made by\nmatplotlib, set the option `pd.options.plotting.matplotlib.register_converters\n= True` or use `pandas.plotting.register_matplotlib_converters()`.\n\npandas includes automatic tick resolution adjustment for regular frequency\ntime-series data. For limited cases where pandas cannot infer the frequency\ninformation (e.g., in an externally created `twinx`), you can choose to\nsuppress this behavior for alignment purposes.\n\nHere is the default behavior, notice how the x-axis tick labeling is\nperformed:\n\nUsing the `x_compat` parameter, you can suppress this behavior:\n\nIf you have more than one plot that needs to be suppressed, the `use` method\nin `pandas.plotting.plot_params` can be used in a `with` statement:\n\n`TimedeltaIndex` now uses the native matplotlib tick locator methods, it is\nuseful to call the automatic date tick adjustment from matplotlib for figures\nwhose ticklabels overlap.\n\nSee the `autofmt_xdate` method and the matplotlib documentation for more.\n\nEach `Series` in a `DataFrame` can be plotted on a different axis with the\n`subplots` keyword:\n\nThe layout of subplots can be specified by the `layout` keyword. It can accept\n`(rows, columns)`. The `layout` keyword can be used in `hist` and `boxplot`\nalso. If the input is invalid, a `ValueError` will be raised.\n\nThe number of axes which can be contained by rows x columns specified by\n`layout` must be larger than the number of required subplots. If layout can\ncontain more axes than required, blank axes are not drawn. Similar to a NumPy\narray\u2019s `reshape` method, you can use `-1` for one dimension to automatically\ncalculate the number of rows or columns needed, given the other.\n\nThe above example is identical to using:\n\nThe required number of columns (3) is inferred from the number of series to\nplot and the given number of rows (2).\n\nYou can pass multiple axes created beforehand as list-like via `ax` keyword.\nThis allows more complicated layouts. The passed axes must be the same number\nas the subplots being drawn.\n\nWhen multiple axes are passed via the `ax` keyword, `layout`, `sharex` and\n`sharey` keywords don\u2019t affect to the output. You should explicitly pass\n`sharex=False` and `sharey=False`, otherwise you will see a warning.\n\nAnother option is passing an `ax` argument to `Series.plot()` to plot on a\nparticular axis:\n\nPlotting with error bars is supported in `DataFrame.plot()` and\n`Series.plot()`.\n\nHorizontal and vertical error bars can be supplied to the `xerr` and `yerr`\nkeyword arguments to `plot()`. The error values can be specified using a\nvariety of formats:\n\nAs a `DataFrame` or `dict` of errors with column names matching the `columns`\nattribute of the plotting `DataFrame` or matching the `name` attribute of the\n`Series`.\n\nAs a `str` indicating which of the columns of plotting `DataFrame` contain the\nerror values.\n\nAs raw values (`list`, `tuple`, or `np.ndarray`). Must be the same length as\nthe plotting `DataFrame`/`Series`.\n\nHere is an example of one way to easily plot group means with standard\ndeviations from the raw data.\n\nAsymmetrical error bars are also supported, however raw error values must be\nprovided in this case. For a `N` length `Series`, a `2xN` array should be\nprovided indicating lower and upper (or left and right) errors. For a `MxN`\n`DataFrame`, asymmetrical errors should be in a `Mx2xN` array.\n\nHere is an example of one way to plot the min/max range using asymmetrical\nerror bars.\n\nPlotting with matplotlib table is now supported in `DataFrame.plot()` and\n`Series.plot()` with a `table` keyword. The `table` keyword can accept `bool`,\n`DataFrame` or `Series`. The simple way to draw a table is to specify\n`table=True`. Data will be transposed to meet matplotlib\u2019s default layout.\n\nAlso, you can pass a different `DataFrame` or `Series` to the `table` keyword.\nThe data will be drawn as displayed in print method (not transposed\nautomatically). If required, it should be transposed manually as seen in the\nexample below.\n\nThere also exists a helper function `pandas.plotting.table`, which creates a\ntable from `DataFrame` or `Series`, and adds it to an `matplotlib.Axes`\ninstance. This function can accept keywords which the matplotlib table has.\n\nNote: You can get table instances on the axes using `axes.tables` property for\nfurther decorations. See the matplotlib table documentation for more.\n\nA potential issue when plotting a large number of columns is that it can be\ndifficult to distinguish some series due to repetition in the default colors.\nTo remedy this, `DataFrame` plotting supports the use of the `colormap`\nargument, which accepts either a Matplotlib colormap or a string that is a\nname of a colormap registered with Matplotlib. A visualization of the default\nmatplotlib colormaps is available here.\n\nAs matplotlib does not directly support colormaps for line-based plots, the\ncolors are selected based on an even spacing determined by the number of\ncolumns in the `DataFrame`. There is no consideration made for background\ncolor, so some colormaps will produce lines that are not easily visible.\n\nTo use the cubehelix colormap, we can pass `colormap='cubehelix'`.\n\nAlternatively, we can pass the colormap itself:\n\nColormaps can also be used other plot types, like bar charts:\n\nParallel coordinates charts:\n\nAndrews curves charts:\n\nIn some situations it may still be preferable or necessary to prepare plots\ndirectly with matplotlib, for instance when a certain type of plot or\ncustomization is not (yet) supported by pandas. `Series` and `DataFrame`\nobjects behave like arrays and can therefore be passed directly to matplotlib\nfunctions without explicit casts.\n\npandas also automatically registers formatters and locators that recognize\ndate indices, thereby extending date and time support to practically all plot\ntypes available in matplotlib. Although this formatting does not provide the\nsame level of refinement you would get when plotting via pandas, it can be\nfaster when plotting a large number of points.\n\nStarting in version 0.25, pandas can be extended with third-party plotting\nbackends. The main idea is letting users select a plotting backend different\nthan the provided one based on Matplotlib.\n\nThis can be done by passing \u2018backend.module\u2019 as the argument `backend` in\n`plot` function. For example:\n\nAlternatively, you can also set this option globally, do you don\u2019t need to\nspecify the keyword in each `plot` call. For example:\n\nOr:\n\nThis would be more or less equivalent to:\n\nThe backend module can then use other visualization tools (Bokeh, Altair,\nhvplot,\u2026) to generate the plots. Some libraries implementing a backend for\npandas are listed on the ecosystem Visualization page.\n\nDevelopers guide can be found at\nhttps://pandas.pydata.org/docs/dev/development/extending.html#plotting-\nbackends\n\n"}, {"name": "Computational tools", "path": "user_guide/computation", "type": "Manual", "text": "\n`Series` and `DataFrame` have a method `pct_change()` to compute the percent\nchange over a given number of periods (using `fill_method` to fill NA/null\nvalues before computing the percent change).\n\n`Series.cov()` can be used to compute covariance between series (excluding\nmissing values).\n\nAnalogously, `DataFrame.cov()` to compute pairwise covariances among the\nseries in the DataFrame, also excluding NA/null values.\n\nNote\n\nAssuming the missing data are missing at random this results in an estimate\nfor the covariance matrix which is unbiased. However, for many applications\nthis estimate may not be acceptable because the estimated covariance matrix is\nnot guaranteed to be positive semi-definite. This could lead to estimated\ncorrelations having absolute values which are greater than one, and/or a non-\ninvertible covariance matrix. See Estimation of covariance matrices for more\ndetails.\n\n`DataFrame.cov` also supports an optional `min_periods` keyword that specifies\nthe required minimum number of observations for each column pair in order to\nhave a valid result.\n\nCorrelation may be computed using the `corr()` method. Using the `method`\nparameter, several methods for computing correlations are provided:\n\nMethod name\n\nDescription\n\n`pearson (default)`\n\nStandard correlation coefficient\n\n`kendall`\n\nKendall Tau correlation coefficient\n\n`spearman`\n\nSpearman rank correlation coefficient\n\nAll of these are currently computed using pairwise complete observations.\nWikipedia has articles covering the above correlation coefficients:\n\nPearson correlation coefficient\n\nKendall rank correlation coefficient\n\nSpearman\u2019s rank correlation coefficient\n\nNote\n\nPlease see the caveats associated with this method of calculating correlation\nmatrices in the covariance section.\n\nNote that non-numeric columns will be automatically excluded from the\ncorrelation calculation.\n\nLike `cov`, `corr` also supports the optional `min_periods` keyword:\n\nThe `method` argument can also be a callable for a generic correlation\ncalculation. In this case, it should be a single function that produces a\nsingle value from two ndarray inputs. Suppose we wanted to compute the\ncorrelation based on histogram intersection:\n\nA related method `corrwith()` is implemented on DataFrame to compute the\ncorrelation between like-labeled Series contained in different DataFrame\nobjects.\n\nThe `rank()` method produces a data ranking with ties being assigned the mean\nof the ranks (by default) for the group:\n\n`rank()` is also a DataFrame method and can rank either the rows (`axis=0`) or\nthe columns (`axis=1`). `NaN` values are excluded from the ranking.\n\n`rank` optionally takes a parameter `ascending` which by default is true; when\nfalse, data is reverse-ranked, with larger values assigned a smaller rank.\n\n`rank` supports different tie-breaking methods, specified with the `method`\nparameter:\n\n`average` : average rank of tied group\n\n`min` : lowest rank in the group\n\n`max` : highest rank in the group\n\n`first` : ranks assigned in the order they appear in the array\n\nSee the window operations user guide for an overview of windowing functions.\n\n"}, {"name": "Cookbook", "path": "user_guide/cookbook", "type": "Manual", "text": "\nThis is a repository for short and sweet examples and links for useful pandas\nrecipes. We encourage users to add to this documentation.\n\nAdding interesting links and/or inline examples to this section is a great\nFirst Pull Request.\n\nSimplified, condensed, new-user friendly, in-line examples have been inserted\nwhere possible to augment the Stack-Overflow and GitHub links. Many of the\nlinks contain expanded information, above what the in-line examples offer.\n\npandas (pd) and NumPy (np) are the only two abbreviated imported modules. The\nrest are kept explicitly imported for newer users.\n\nThese are some neat pandas `idioms`\n\nif-then/if-then-else on one column, and assignment to another one or more\ncolumns:\n\nAn if-then on one column\n\nAn if-then with assignment to 2 columns:\n\nAdd another line with different logic, to do the -else\n\nOr use pandas where after you\u2019ve set up a mask\n\nif-then-else using NumPy\u2019s where()\n\nSplit a frame with a boolean criterion\n\nSelect with multi-column criteria\n\n\u2026and (without assignment returns a Series)\n\n\u2026or (without assignment returns a Series)\n\n\u2026or (with assignment modifies the DataFrame.)\n\nSelect rows with data closest to certain value using argsort\n\nDynamically reduce a list of criteria using a binary operators\n\nOne could hard code:\n\n\u2026Or it can be done with a list of dynamically built criteria\n\nThe indexing docs.\n\nUsing both row labels and value conditionals\n\nUse loc for label-oriented slicing and iloc positional slicing GH2904\n\nThere are 2 explicit slicing methods, with a third general case\n\nPositional-oriented (Python slicing style : exclusive of end)\n\nLabel-oriented (Non-Python slicing style : inclusive of end)\n\nGeneral (Either slicing style : depends on if the slice contains labels or\npositions)\n\nAmbiguity arises when an index consists of integers with a non-zero start or\nnon-unit increment.\n\nUsing inverse operator (~) to take the complement of a mask\n\nEfficiently and dynamically creating new columns using applymap\n\nKeep other columns when using min() with groupby\n\nMethod 1 : idxmin() to get the index of the minimums\n\nMethod 2 : sort then take first of each\n\nNotice the same results, with the exception of the index.\n\nThe multindexing docs.\n\nCreating a MultiIndex from a labeled frame\n\nPerforming arithmetic with a MultiIndex that needs broadcasting\n\nSlicing a MultiIndex with xs\n\nTo take the cross section of the 1st level and 1st axis the index:\n\n\u2026and now the 2nd level of the 1st axis.\n\nSlicing a MultiIndex with xs, method #2\n\nSetting portions of a MultiIndex with xs\n\nSort by specific column or an ordered list of columns, with a MultiIndex\n\nPartial selection, the need for sortedness GH2995\n\nPrepending a level to a multiindex\n\nFlatten Hierarchical columns\n\nThe missing data docs.\n\nFill forward a reversed timeseries\n\ncumsum reset at NaN values\n\nUsing replace with backrefs\n\nThe grouping docs.\n\nBasic grouping with apply\n\nUnlike agg, apply\u2019s callable is passed a sub-DataFrame which gives you access\nto all the columns\n\nUsing get_group\n\nApply to different items in a group\n\nExpanding apply\n\nReplacing some values with mean of the rest of a group\n\nSort groups by aggregated data\n\nCreate multiple aggregated columns\n\nCreate a value counts column and reassign back to the DataFrame\n\nShift groups of the values in a column based on the index\n\nSelect row with maximum value from each group\n\nGrouping like Python\u2019s itertools.groupby\n\nAlignment and to-date\n\nRolling Computation window based on values instead of counts\n\nRolling Mean by Time Interval\n\nSplitting a frame\n\nCreate a list of dataframes, split using a delineation based on logic included\nin rows.\n\nThe Pivot docs.\n\nPartial sums and subtotals\n\nFrequency table like plyr in R\n\nPlot pandas DataFrame with year over year data\n\nTo create year and month cross tabulation:\n\nRolling apply to organize - Turning embedded lists into a MultiIndex frame\n\nRolling apply with a DataFrame returning a Series\n\nRolling Apply to multiple columns where function calculates a Series before a\nScalar from the Series is returned\n\nRolling apply with a DataFrame returning a Scalar\n\nRolling Apply to multiple columns where function returns a Scalar (Volume\nWeighted Average Price)\n\nBetween times\n\nUsing indexer between time\n\nConstructing a datetime range that excludes weekends and includes only certain\ntimes\n\nVectorized Lookup\n\nAggregation and plotting time series\n\nTurn a matrix with hours in columns and days in rows into a continuous row\nsequence in the form of a time series. How to rearrange a Python pandas\nDataFrame?\n\nDealing with duplicates when reindexing a timeseries to a specified frequency\n\nCalculate the first day of the month for each entry in a DatetimeIndex\n\nThe Resample docs.\n\nUsing Grouper instead of TimeGrouper for time grouping of values\n\nTime grouping with some missing values\n\nValid frequency arguments to Grouper Timeseries\n\nGrouping using a MultiIndex\n\nUsing TimeGrouper and another grouping to create subgroups, then apply a\ncustom function GH3791\n\nResampling with custom periods\n\nResample intraday frame without adding new days\n\nResample minute data\n\nResample with groupby\n\nThe Join docs.\n\nConcatenate two dataframes with overlapping index (emulate R rbind)\n\nDepending on df construction, `ignore_index` may be needed\n\nSelf Join of a DataFrame GH2996\n\nHow to set the index and join\n\nKDB like asof join\n\nJoin with a criteria based on the values\n\nUsing searchsorted to merge based on values inside a range\n\nThe Plotting docs.\n\nMake Matplotlib look like R\n\nSetting x-axis major and minor labels\n\nPlotting multiple charts in an IPython Jupyter notebook\n\nCreating a multi-line plot\n\nPlotting a heatmap\n\nAnnotate a time-series plot\n\nAnnotate a time-series plot #2\n\nGenerate Embedded plots in excel files using Pandas, Vincent and xlsxwriter\n\nBoxplot for each quartile of a stratifying variable\n\nPerformance comparison of SQL vs HDF5\n\nThe CSV docs\n\nread_csv in action\n\nappending to a csv\n\nReading a csv chunk-by-chunk\n\nReading only certain rows of a csv chunk-by-chunk\n\nReading the first few lines of a frame\n\nReading a file that is compressed but not by `gzip/bz2` (the native compressed\nformats which `read_csv` understands). This example shows a `WinZipped` file,\nbut is a general application of opening the file within a context manager and\nusing that handle to read. See here\n\nInferring dtypes from a file\n\nDealing with bad lines GH2886\n\nWrite a multi-row index CSV without writing duplicates\n\nThe best way to combine multiple files into a single DataFrame is to read the\nindividual frames one by one, put all of the individual frames into a list,\nand then combine the frames in the list using `pd.concat()`:\n\nYou can use the same approach to read all files matching a pattern. Here is an\nexample using `glob`:\n\nFinally, this strategy will work with the other `pd.read_*(...)` functions\ndescribed in the io docs.\n\nParsing date components in multi-columns is faster with a format\n\nThe SQL docs\n\nReading from databases with SQL\n\nThe Excel docs\n\nReading from a filelike handle\n\nModifying formatting in XlsxWriter output\n\nLoading only visible sheets GH19842#issuecomment-892150745\n\nReading HTML tables from a server that cannot handle the default request\nheader\n\nThe HDFStores docs\n\nSimple queries with a Timestamp Index\n\nManaging heterogeneous data using a linked multiple table hierarchy GH3032\n\nMerging on-disk tables with millions of rows\n\nAvoiding inconsistencies when writing to a store from multiple\nprocesses/threads\n\nDe-duplicating a large store by chunks, essentially a recursive reduction\noperation. Shows a function for taking in data from csv file and creating a\nstore by chunks, with date parsing as well. See here\n\nCreating a store chunk-by-chunk from a csv file\n\nAppending to a store, while creating a unique index\n\nLarge Data work flows\n\nReading in a sequence of files, then providing a global unique index to a\nstore while appending\n\nGroupby on a HDFStore with low group density\n\nGroupby on a HDFStore with high group density\n\nHierarchical queries on a HDFStore\n\nCounting with a HDFStore\n\nTroubleshoot HDFStore exceptions\n\nSetting min_itemsize with strings\n\nUsing ptrepack to create a completely-sorted-index on a store\n\nStoring Attributes to a group node\n\nYou can create or load a HDFStore in-memory by passing the `driver` parameter\nto PyTables. Changes are only written to disk when the HDFStore is closed.\n\npandas readily accepts NumPy record arrays, if you need to read in a binary\nfile consisting of an array of C structs. For example, given this C program in\na file called `main.c` compiled with `gcc main.c -std=gnu99` on a 64-bit\nmachine,\n\nthe following Python code will read the binary file `'binary.dat'` into a\npandas `DataFrame`, where each element of the struct corresponds to a column\nin the frame:\n\nNote\n\nThe offsets of the structure elements may be different depending on the\narchitecture of the machine on which the file was created. Using a raw binary\nfile format like this for general data storage is not recommended, as it is\nnot cross platform. We recommended either HDF5 or parquet, both of which are\nsupported by pandas\u2019 IO facilities.\n\nNumerical integration (sample-based) of a time series\n\nOften it\u2019s useful to obtain the lower (or upper) triangular form of a\ncorrelation matrix calculated from `DataFrame.corr()`. This can be achieved by\npassing a boolean mask to `where` as follows:\n\nThe `method` argument within `DataFrame.corr` can accept a callable in\naddition to the named correlation types. Here we compute the distance\ncorrelation matrix for a `DataFrame` object.\n\nThe Timedeltas docs.\n\nUsing timedeltas\n\nAdding and subtracting deltas and dates\n\nAnother example\n\nValues can be set to NaT using np.nan, similar to datetime\n\nTo create a dataframe from every combination of some given values, like R\u2019s\n`expand.grid()` function, we can create a dict where the keys are column names\nand the values are lists of the data values:\n\n"}, {"name": "DataFrame", "path": "reference/frame", "type": "DataFrame", "text": "\n`DataFrame`([data, index, columns, dtype, copy])\n\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nAxes\n\n`DataFrame.index`\n\nThe index (row labels) of the DataFrame.\n\n`DataFrame.columns`\n\nThe column labels of the DataFrame.\n\n`DataFrame.dtypes`\n\nReturn the dtypes in the DataFrame.\n\n`DataFrame.info`([verbose, buf, max_cols, ...])\n\nPrint a concise summary of a DataFrame.\n\n`DataFrame.select_dtypes`([include, exclude])\n\nReturn a subset of the DataFrame's columns based on the column dtypes.\n\n`DataFrame.values`\n\nReturn a Numpy representation of the DataFrame.\n\n`DataFrame.axes`\n\nReturn a list representing the axes of the DataFrame.\n\n`DataFrame.ndim`\n\nReturn an int representing the number of axes / array dimensions.\n\n`DataFrame.size`\n\nReturn an int representing the number of elements in this object.\n\n`DataFrame.shape`\n\nReturn a tuple representing the dimensionality of the DataFrame.\n\n`DataFrame.memory_usage`([index, deep])\n\nReturn the memory usage of each column in bytes.\n\n`DataFrame.empty`\n\nIndicator whether Series/DataFrame is empty.\n\n`DataFrame.set_flags`(*[, copy, ...])\n\nReturn a new object with updated flags.\n\n`DataFrame.astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`DataFrame.convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`DataFrame.infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`DataFrame.copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`DataFrame.bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`DataFrame.head`([n])\n\nReturn the first n rows.\n\n`DataFrame.at`\n\nAccess a single value for a row/column label pair.\n\n`DataFrame.iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`DataFrame.loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`DataFrame.iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`DataFrame.insert`(loc, column, value[, ...])\n\nInsert column into DataFrame at specified location.\n\n`DataFrame.__iter__`()\n\nIterate over info axis.\n\n`DataFrame.items`()\n\nIterate over (column name, Series) pairs.\n\n`DataFrame.iteritems`()\n\nIterate over (column name, Series) pairs.\n\n`DataFrame.keys`()\n\nGet the 'info axis' (see Indexing for more).\n\n`DataFrame.iterrows`()\n\nIterate over DataFrame rows as (index, Series) pairs.\n\n`DataFrame.itertuples`([index, name])\n\nIterate over DataFrame rows as namedtuples.\n\n`DataFrame.lookup`(row_labels, col_labels)\n\n(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.\n\n`DataFrame.pop`(item)\n\nReturn item and drop from frame.\n\n`DataFrame.tail`([n])\n\nReturn the last n rows.\n\n`DataFrame.xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\n`DataFrame.get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`DataFrame.isin`(values)\n\nWhether each element in the DataFrame is contained in values.\n\n`DataFrame.where`(cond[, other, inplace, ...])\n\nReplace values where the condition is False.\n\n`DataFrame.mask`(cond[, other, inplace, axis, ...])\n\nReplace values where the condition is True.\n\n`DataFrame.query`(expr[, inplace])\n\nQuery the columns of a DataFrame with a boolean expression.\n\nFor more information on `.at`, `.iat`, `.loc`, and `.iloc`, see the indexing\ndocumentation.\n\n`DataFrame.add`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator add).\n\n`DataFrame.sub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`DataFrame.mul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`DataFrame.div`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`DataFrame.truediv`(other[, axis, level, ...])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`DataFrame.floordiv`(other[, axis, level, ...])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\n`DataFrame.mod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\n`DataFrame.pow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\n`DataFrame.dot`(other)\n\nCompute the matrix multiplication between the DataFrame and other.\n\n`DataFrame.radd`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\n`DataFrame.rsub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\n`DataFrame.rmul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\n`DataFrame.rdiv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`DataFrame.rtruediv`(other[, axis, level, ...])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`DataFrame.rfloordiv`(other[, axis, level, ...])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\n`DataFrame.rmod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\n`DataFrame.rpow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\n`DataFrame.lt`(other[, axis, level])\n\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\n`DataFrame.gt`(other[, axis, level])\n\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\n`DataFrame.le`(other[, axis, level])\n\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\n`DataFrame.ge`(other[, axis, level])\n\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\n`DataFrame.ne`(other[, axis, level])\n\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\n`DataFrame.eq`(other[, axis, level])\n\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\n`DataFrame.combine`(other, func[, fill_value, ...])\n\nPerform column-wise combine with another DataFrame.\n\n`DataFrame.combine_first`(other)\n\nUpdate null elements with value in the same location in other.\n\n`DataFrame.apply`(func[, axis, raw, ...])\n\nApply a function along an axis of the DataFrame.\n\n`DataFrame.applymap`(func[, na_action])\n\nApply a function to a Dataframe elementwise.\n\n`DataFrame.pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`DataFrame.agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrame.aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrame.transform`(func[, axis])\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\n`DataFrame.groupby`([by, axis, level, ...])\n\nGroup DataFrame using a mapper or by a Series of columns.\n\n`DataFrame.rolling`(window[, min_periods, ...])\n\nProvide rolling window calculations.\n\n`DataFrame.expanding`([min_periods, center, ...])\n\nProvide expanding window calculations.\n\n`DataFrame.ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`DataFrame.abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`DataFrame.all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`DataFrame.any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`DataFrame.clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`DataFrame.corr`([method, min_periods])\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`DataFrame.corrwith`(other[, axis, drop, method])\n\nCompute pairwise correlation.\n\n`DataFrame.count`([axis, level, numeric_only])\n\nCount non-NA cells for each column or row.\n\n`DataFrame.cov`([min_periods, ddof])\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`DataFrame.cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`DataFrame.cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`DataFrame.cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`DataFrame.cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`DataFrame.describe`([percentiles, include, ...])\n\nGenerate descriptive statistics.\n\n`DataFrame.diff`([periods, axis])\n\nFirst discrete difference of element.\n\n`DataFrame.eval`(expr[, inplace])\n\nEvaluate a string describing operations on DataFrame columns.\n\n`DataFrame.kurt`([axis, skipna, level, ...])\n\nReturn unbiased kurtosis over requested axis.\n\n`DataFrame.kurtosis`([axis, skipna, level, ...])\n\nReturn unbiased kurtosis over requested axis.\n\n`DataFrame.mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`DataFrame.max`([axis, skipna, level, ...])\n\nReturn the maximum of the values over the requested axis.\n\n`DataFrame.mean`([axis, skipna, level, ...])\n\nReturn the mean of the values over the requested axis.\n\n`DataFrame.median`([axis, skipna, level, ...])\n\nReturn the median of the values over the requested axis.\n\n`DataFrame.min`([axis, skipna, level, ...])\n\nReturn the minimum of the values over the requested axis.\n\n`DataFrame.mode`([axis, numeric_only, dropna])\n\nGet the mode(s) of each element along the selected axis.\n\n`DataFrame.pct_change`([periods, fill_method, ...])\n\nPercentage change between the current and a prior element.\n\n`DataFrame.prod`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`DataFrame.product`([axis, skipna, level, ...])\n\nReturn the product of the values over the requested axis.\n\n`DataFrame.quantile`([q, axis, numeric_only, ...])\n\nReturn values at the given quantile over requested axis.\n\n`DataFrame.rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`DataFrame.round`([decimals])\n\nRound a DataFrame to a variable number of decimal places.\n\n`DataFrame.sem`([axis, skipna, level, ddof, ...])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`DataFrame.skew`([axis, skipna, level, ...])\n\nReturn unbiased skew over requested axis.\n\n`DataFrame.sum`([axis, skipna, level, ...])\n\nReturn the sum of the values over the requested axis.\n\n`DataFrame.std`([axis, skipna, level, ddof, ...])\n\nReturn sample standard deviation over requested axis.\n\n`DataFrame.var`([axis, skipna, level, ddof, ...])\n\nReturn unbiased variance over requested axis.\n\n`DataFrame.nunique`([axis, dropna])\n\nCount number of distinct elements in specified axis.\n\n`DataFrame.value_counts`([subset, normalize, ...])\n\nReturn a Series containing counts of unique rows in the DataFrame.\n\n`DataFrame.add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`DataFrame.add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`DataFrame.align`(other[, join, axis, level, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`DataFrame.at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`DataFrame.between_time`(start_time, end_time)\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`DataFrame.drop`([labels, axis, index, ...])\n\nDrop specified labels from rows or columns.\n\n`DataFrame.drop_duplicates`([subset, keep, ...])\n\nReturn DataFrame with duplicate rows removed.\n\n`DataFrame.duplicated`([subset, keep])\n\nReturn boolean Series denoting duplicate rows.\n\n`DataFrame.equals`(other)\n\nTest whether two objects contain the same elements.\n\n`DataFrame.filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`DataFrame.first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`DataFrame.head`([n])\n\nReturn the first n rows.\n\n`DataFrame.idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`DataFrame.idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`DataFrame.last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`DataFrame.reindex`([labels, index, columns, ...])\n\nConform Series/DataFrame to new index with optional filling logic.\n\n`DataFrame.reindex_like`(other[, method, ...])\n\nReturn an object with matching indices as other object.\n\n`DataFrame.rename`([mapper, index, columns, ...])\n\nAlter axes labels.\n\n`DataFrame.rename_axis`([mapper, index, ...])\n\nSet the name of the axis for the index or columns.\n\n`DataFrame.reset_index`([level, drop, ...])\n\nReset the index, or a level of it.\n\n`DataFrame.sample`([n, frac, replace, ...])\n\nReturn a random sample of items from an axis of object.\n\n`DataFrame.set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`DataFrame.set_index`(keys[, drop, append, ...])\n\nSet the DataFrame index using existing columns.\n\n`DataFrame.tail`([n])\n\nReturn the last n rows.\n\n`DataFrame.take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`DataFrame.truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`DataFrame.backfill`([axis, inplace, limit, ...])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`DataFrame.bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`DataFrame.dropna`([axis, how, thresh, ...])\n\nRemove missing values.\n\n`DataFrame.ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`DataFrame.fillna`([value, method, axis, ...])\n\nFill NA/NaN values using the specified method.\n\n`DataFrame.interpolate`([method, axis, limit, ...])\n\nFill NaN values using an interpolation method.\n\n`DataFrame.isna`()\n\nDetect missing values.\n\n`DataFrame.isnull`()\n\nDataFrame.isnull is an alias for DataFrame.isna.\n\n`DataFrame.notna`()\n\nDetect existing (non-missing) values.\n\n`DataFrame.notnull`()\n\nDataFrame.notnull is an alias for DataFrame.notna.\n\n`DataFrame.pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`DataFrame.replace`([to_replace, value, ...])\n\nReplace values given in to_replace with value.\n\n`DataFrame.droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`DataFrame.pivot`([index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`DataFrame.pivot_table`([values, index, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`DataFrame.reorder_levels`(order[, axis])\n\nRearrange index levels using input order.\n\n`DataFrame.sort_values`(by[, axis, ascending, ...])\n\nSort by the values along either axis.\n\n`DataFrame.sort_index`([axis, level, ...])\n\nSort object by labels (along an axis).\n\n`DataFrame.nlargest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in descending order.\n\n`DataFrame.nsmallest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in ascending order.\n\n`DataFrame.swaplevel`([i, j, axis])\n\nSwap levels i and j in a `MultiIndex`.\n\n`DataFrame.stack`([level, dropna])\n\nStack the prescribed level(s) from columns to index.\n\n`DataFrame.unstack`([level, fill_value])\n\nPivot a level of the (necessarily hierarchical) index labels.\n\n`DataFrame.swapaxes`(axis1, axis2[, copy])\n\nInterchange axes and swap values axes appropriately.\n\n`DataFrame.melt`([id_vars, value_vars, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`DataFrame.explode`(column[, ignore_index])\n\nTransform each element of a list-like to a row, replicating index values.\n\n`DataFrame.squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`DataFrame.to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`DataFrame.T`\n\n`DataFrame.transpose`(*args[, copy])\n\nTranspose index and columns.\n\n`DataFrame.append`(other[, ignore_index, ...])\n\nAppend rows of other to the end of caller, returning a new object.\n\n`DataFrame.assign`(**kwargs)\n\nAssign new columns to a DataFrame.\n\n`DataFrame.compare`(other[, align_axis, ...])\n\nCompare to another DataFrame and show the differences.\n\n`DataFrame.join`(other[, on, how, lsuffix, ...])\n\nJoin columns of another DataFrame.\n\n`DataFrame.merge`(right[, how, on, left_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`DataFrame.update`(other[, join, overwrite, ...])\n\nModify in place using non-NA values from another DataFrame.\n\n`DataFrame.asfreq`(freq[, method, how, ...])\n\nConvert time series to specified frequency.\n\n`DataFrame.asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`DataFrame.shift`([periods, freq, axis, ...])\n\nShift index by desired number of periods with an optional time freq.\n\n`DataFrame.slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\n`DataFrame.tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`DataFrame.first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`DataFrame.last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`DataFrame.resample`(rule[, axis, closed, ...])\n\nResample time-series data.\n\n`DataFrame.to_period`([freq, axis, copy])\n\nConvert DataFrame from DatetimeIndex to PeriodIndex.\n\n`DataFrame.to_timestamp`([freq, how, axis, copy])\n\nCast to DatetimeIndex of timestamps, at beginning of period.\n\n`DataFrame.tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`DataFrame.tz_localize`(tz[, axis, level, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\nFlags refer to attributes of the pandas object. Properties of the dataset\n(like the date is was recorded, the URL it was accessed from, etc.) should be\nstored in `DataFrame.attrs`.\n\n`Flags`(obj, *, allows_duplicate_labels)\n\nFlags that apply to pandas objects.\n\n`DataFrame.attrs` is a dictionary for storing global metadata for this\nDataFrame.\n\nWarning\n\n`DataFrame.attrs` is considered experimental and may change without warning.\n\n`DataFrame.attrs`\n\nDictionary of global attributes of this dataset.\n\n`DataFrame.plot` is both a callable method and a namespace attribute for\nspecific plotting methods of the form `DataFrame.plot.<kind>`.\n\n`DataFrame.plot`([x, y, kind, ax, ....])\n\nDataFrame plotting accessor and method\n\n`DataFrame.plot.area`([x, y])\n\nDraw a stacked area plot.\n\n`DataFrame.plot.bar`([x, y])\n\nVertical bar plot.\n\n`DataFrame.plot.barh`([x, y])\n\nMake a horizontal bar plot.\n\n`DataFrame.plot.box`([by])\n\nMake a box plot of the DataFrame columns.\n\n`DataFrame.plot.density`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`DataFrame.plot.hexbin`(x, y[, C, ...])\n\nGenerate a hexagonal binning plot.\n\n`DataFrame.plot.hist`([by, bins])\n\nDraw one histogram of the DataFrame's columns.\n\n`DataFrame.plot.kde`([bw_method, ind])\n\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\n`DataFrame.plot.line`([x, y])\n\nPlot Series or DataFrame as lines.\n\n`DataFrame.plot.pie`(**kwargs)\n\nGenerate a pie plot.\n\n`DataFrame.plot.scatter`(x, y[, s, c])\n\nCreate a scatter plot with varying marker point size and color.\n\n`DataFrame.boxplot`([column, by, ax, ...])\n\nMake a box plot from DataFrame columns.\n\n`DataFrame.hist`([column, by, grid, ...])\n\nMake a histogram of the DataFrame's columns.\n\nSparse-dtype specific methods and attributes are provided under the\n`DataFrame.sparse` accessor.\n\n`DataFrame.sparse.density`\n\nRatio of non-sparse points to total (dense) data points.\n\n`DataFrame.sparse.from_spmatrix`(data[, ...])\n\nCreate a new DataFrame from a scipy sparse matrix.\n\n`DataFrame.sparse.to_coo`()\n\nReturn the contents of the frame as a sparse SciPy COO matrix.\n\n`DataFrame.sparse.to_dense`()\n\nConvert a DataFrame with sparse values to dense.\n\n`DataFrame.from_dict`(data[, orient, dtype, ...])\n\nConstruct DataFrame from dict of array-like or dicts.\n\n`DataFrame.from_records`(data[, index, ...])\n\nConvert structured or record ndarray to DataFrame.\n\n`DataFrame.to_parquet`([path, engine, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`DataFrame.to_pickle`(path[, compression, ...])\n\nPickle (serialize) object to file.\n\n`DataFrame.to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`DataFrame.to_hdf`(path_or_buf, key[, mode, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`DataFrame.to_dict`([orient, into])\n\nConvert the DataFrame to a dictionary.\n\n`DataFrame.to_excel`(excel_writer[, ...])\n\nWrite object to an Excel sheet.\n\n`DataFrame.to_json`([path_or_buf, orient, ...])\n\nConvert the object to a JSON string.\n\n`DataFrame.to_html`([buf, columns, col_space, ...])\n\nRender a DataFrame as an HTML table.\n\n`DataFrame.to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`DataFrame.to_latex`([buf, columns, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`DataFrame.to_stata`(path[, convert_dates, ...])\n\nExport DataFrame object to Stata dta format.\n\n`DataFrame.to_gbq`(destination_table[, ...])\n\nWrite a DataFrame to a Google BigQuery table.\n\n`DataFrame.to_records`([index, column_dtypes, ...])\n\nConvert DataFrame to a NumPy record array.\n\n`DataFrame.to_string`([buf, columns, ...])\n\nRender a DataFrame to a console-friendly tabular output.\n\n`DataFrame.to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`DataFrame.to_markdown`([buf, mode, index, ...])\n\nPrint DataFrame in Markdown-friendly format.\n\n`DataFrame.style`\n\nReturns a Styler object.\n\n"}, {"name": "Date offsets", "path": "reference/offset_frequency", "type": "Data offsets", "text": "\n`DateOffset`\n\nStandard kind of date increment used for a date range.\n\n`DateOffset.freqstr`\n\n`DateOffset.kwds`\n\n`DateOffset.name`\n\n`DateOffset.nanos`\n\n`DateOffset.normalize`\n\n`DateOffset.rule_code`\n\n`DateOffset.n`\n\n`DateOffset.is_month_start`\n\n`DateOffset.is_month_end`\n\n`DateOffset.apply`\n\n`DateOffset.apply_index`(other)\n\n`DateOffset.copy`\n\n`DateOffset.isAnchored`\n\n`DateOffset.onOffset`\n\n`DateOffset.is_anchored`\n\n`DateOffset.is_on_offset`\n\n`DateOffset.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`DateOffset.is_month_start`\n\n`DateOffset.is_month_end`\n\n`DateOffset.is_quarter_start`\n\n`DateOffset.is_quarter_end`\n\n`DateOffset.is_year_start`\n\n`DateOffset.is_year_end`\n\n`BusinessDay`\n\nDateOffset subclass representing possibly n business days.\n\nAlias:\n\n`BDay`\n\nalias of `pandas._libs.tslibs.offsets.BusinessDay`\n\n`BusinessDay.freqstr`\n\n`BusinessDay.kwds`\n\n`BusinessDay.name`\n\n`BusinessDay.nanos`\n\n`BusinessDay.normalize`\n\n`BusinessDay.rule_code`\n\n`BusinessDay.n`\n\n`BusinessDay.weekmask`\n\n`BusinessDay.holidays`\n\n`BusinessDay.calendar`\n\n`BusinessDay.apply`\n\n`BusinessDay.apply_index`(other)\n\n`BusinessDay.copy`\n\n`BusinessDay.isAnchored`\n\n`BusinessDay.onOffset`\n\n`BusinessDay.is_anchored`\n\n`BusinessDay.is_on_offset`\n\n`BusinessDay.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessDay.is_month_start`\n\n`BusinessDay.is_month_end`\n\n`BusinessDay.is_quarter_start`\n\n`BusinessDay.is_quarter_end`\n\n`BusinessDay.is_year_start`\n\n`BusinessDay.is_year_end`\n\n`BusinessHour`\n\nDateOffset subclass representing possibly n business hours.\n\n`BusinessHour.freqstr`\n\n`BusinessHour.kwds`\n\n`BusinessHour.name`\n\n`BusinessHour.nanos`\n\n`BusinessHour.normalize`\n\n`BusinessHour.rule_code`\n\n`BusinessHour.n`\n\n`BusinessHour.start`\n\n`BusinessHour.end`\n\n`BusinessHour.weekmask`\n\n`BusinessHour.holidays`\n\n`BusinessHour.calendar`\n\n`BusinessHour.apply`\n\n`BusinessHour.apply_index`(other)\n\n`BusinessHour.copy`\n\n`BusinessHour.isAnchored`\n\n`BusinessHour.onOffset`\n\n`BusinessHour.is_anchored`\n\n`BusinessHour.is_on_offset`\n\n`BusinessHour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessHour.is_month_start`\n\n`BusinessHour.is_month_end`\n\n`BusinessHour.is_quarter_start`\n\n`BusinessHour.is_quarter_end`\n\n`BusinessHour.is_year_start`\n\n`BusinessHour.is_year_end`\n\n`CustomBusinessDay`\n\nDateOffset subclass representing custom business days excluding holidays.\n\nAlias:\n\n`CDay`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessDay`\n\n`CustomBusinessDay.freqstr`\n\n`CustomBusinessDay.kwds`\n\n`CustomBusinessDay.name`\n\n`CustomBusinessDay.nanos`\n\n`CustomBusinessDay.normalize`\n\n`CustomBusinessDay.rule_code`\n\n`CustomBusinessDay.n`\n\n`CustomBusinessDay.weekmask`\n\n`CustomBusinessDay.calendar`\n\n`CustomBusinessDay.holidays`\n\n`CustomBusinessDay.apply_index`\n\n`CustomBusinessDay.apply`\n\n`CustomBusinessDay.copy`\n\n`CustomBusinessDay.isAnchored`\n\n`CustomBusinessDay.onOffset`\n\n`CustomBusinessDay.is_anchored`\n\n`CustomBusinessDay.is_on_offset`\n\n`CustomBusinessDay.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessDay.is_month_start`\n\n`CustomBusinessDay.is_month_end`\n\n`CustomBusinessDay.is_quarter_start`\n\n`CustomBusinessDay.is_quarter_end`\n\n`CustomBusinessDay.is_year_start`\n\n`CustomBusinessDay.is_year_end`\n\n`CustomBusinessHour`\n\nDateOffset subclass representing possibly n custom business days.\n\n`CustomBusinessHour.freqstr`\n\n`CustomBusinessHour.kwds`\n\n`CustomBusinessHour.name`\n\n`CustomBusinessHour.nanos`\n\n`CustomBusinessHour.normalize`\n\n`CustomBusinessHour.rule_code`\n\n`CustomBusinessHour.n`\n\n`CustomBusinessHour.weekmask`\n\n`CustomBusinessHour.calendar`\n\n`CustomBusinessHour.holidays`\n\n`CustomBusinessHour.start`\n\n`CustomBusinessHour.end`\n\n`CustomBusinessHour.apply`\n\n`CustomBusinessHour.apply_index`(other)\n\n`CustomBusinessHour.copy`\n\n`CustomBusinessHour.isAnchored`\n\n`CustomBusinessHour.onOffset`\n\n`CustomBusinessHour.is_anchored`\n\n`CustomBusinessHour.is_on_offset`\n\n`CustomBusinessHour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessHour.is_month_start`\n\n`CustomBusinessHour.is_month_end`\n\n`CustomBusinessHour.is_quarter_start`\n\n`CustomBusinessHour.is_quarter_end`\n\n`CustomBusinessHour.is_year_start`\n\n`CustomBusinessHour.is_year_end`\n\n`MonthEnd`\n\nDateOffset of one month end.\n\n`MonthEnd.freqstr`\n\n`MonthEnd.kwds`\n\n`MonthEnd.name`\n\n`MonthEnd.nanos`\n\n`MonthEnd.normalize`\n\n`MonthEnd.rule_code`\n\n`MonthEnd.n`\n\n`MonthEnd.apply`\n\n`MonthEnd.apply_index`(other)\n\n`MonthEnd.copy`\n\n`MonthEnd.isAnchored`\n\n`MonthEnd.onOffset`\n\n`MonthEnd.is_anchored`\n\n`MonthEnd.is_on_offset`\n\n`MonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`MonthEnd.is_month_start`\n\n`MonthEnd.is_month_end`\n\n`MonthEnd.is_quarter_start`\n\n`MonthEnd.is_quarter_end`\n\n`MonthEnd.is_year_start`\n\n`MonthEnd.is_year_end`\n\n`MonthBegin`\n\nDateOffset of one month at beginning.\n\n`MonthBegin.freqstr`\n\n`MonthBegin.kwds`\n\n`MonthBegin.name`\n\n`MonthBegin.nanos`\n\n`MonthBegin.normalize`\n\n`MonthBegin.rule_code`\n\n`MonthBegin.n`\n\n`MonthBegin.apply`\n\n`MonthBegin.apply_index`(other)\n\n`MonthBegin.copy`\n\n`MonthBegin.isAnchored`\n\n`MonthBegin.onOffset`\n\n`MonthBegin.is_anchored`\n\n`MonthBegin.is_on_offset`\n\n`MonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`MonthBegin.is_month_start`\n\n`MonthBegin.is_month_end`\n\n`MonthBegin.is_quarter_start`\n\n`MonthBegin.is_quarter_end`\n\n`MonthBegin.is_year_start`\n\n`MonthBegin.is_year_end`\n\n`BusinessMonthEnd`\n\nDateOffset increments between the last business day of the month.\n\nAlias:\n\n`BMonthEnd`\n\nalias of `pandas._libs.tslibs.offsets.BusinessMonthEnd`\n\n`BusinessMonthEnd.freqstr`\n\n`BusinessMonthEnd.kwds`\n\n`BusinessMonthEnd.name`\n\n`BusinessMonthEnd.nanos`\n\n`BusinessMonthEnd.normalize`\n\n`BusinessMonthEnd.rule_code`\n\n`BusinessMonthEnd.n`\n\n`BusinessMonthEnd.apply`\n\n`BusinessMonthEnd.apply_index`(other)\n\n`BusinessMonthEnd.copy`\n\n`BusinessMonthEnd.isAnchored`\n\n`BusinessMonthEnd.onOffset`\n\n`BusinessMonthEnd.is_anchored`\n\n`BusinessMonthEnd.is_on_offset`\n\n`BusinessMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessMonthEnd.is_month_start`\n\n`BusinessMonthEnd.is_month_end`\n\n`BusinessMonthEnd.is_quarter_start`\n\n`BusinessMonthEnd.is_quarter_end`\n\n`BusinessMonthEnd.is_year_start`\n\n`BusinessMonthEnd.is_year_end`\n\n`BusinessMonthBegin`\n\nDateOffset of one month at the first business day.\n\nAlias:\n\n`BMonthBegin`\n\nalias of `pandas._libs.tslibs.offsets.BusinessMonthBegin`\n\n`BusinessMonthBegin.freqstr`\n\n`BusinessMonthBegin.kwds`\n\n`BusinessMonthBegin.name`\n\n`BusinessMonthBegin.nanos`\n\n`BusinessMonthBegin.normalize`\n\n`BusinessMonthBegin.rule_code`\n\n`BusinessMonthBegin.n`\n\n`BusinessMonthBegin.apply`\n\n`BusinessMonthBegin.apply_index`(other)\n\n`BusinessMonthBegin.copy`\n\n`BusinessMonthBegin.isAnchored`\n\n`BusinessMonthBegin.onOffset`\n\n`BusinessMonthBegin.is_anchored`\n\n`BusinessMonthBegin.is_on_offset`\n\n`BusinessMonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BusinessMonthBegin.is_month_start`\n\n`BusinessMonthBegin.is_month_end`\n\n`BusinessMonthBegin.is_quarter_start`\n\n`BusinessMonthBegin.is_quarter_end`\n\n`BusinessMonthBegin.is_year_start`\n\n`BusinessMonthBegin.is_year_end`\n\n`CustomBusinessMonthEnd`\n\nAttributes\n\nAlias:\n\n`CBMonthEnd`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthEnd`\n\n`CustomBusinessMonthEnd.freqstr`\n\n`CustomBusinessMonthEnd.kwds`\n\n`CustomBusinessMonthEnd.m_offset`\n\n`CustomBusinessMonthEnd.name`\n\n`CustomBusinessMonthEnd.nanos`\n\n`CustomBusinessMonthEnd.normalize`\n\n`CustomBusinessMonthEnd.rule_code`\n\n`CustomBusinessMonthEnd.n`\n\n`CustomBusinessMonthEnd.weekmask`\n\n`CustomBusinessMonthEnd.calendar`\n\n`CustomBusinessMonthEnd.holidays`\n\n`CustomBusinessMonthEnd.apply`\n\n`CustomBusinessMonthEnd.apply_index`(other)\n\n`CustomBusinessMonthEnd.copy`\n\n`CustomBusinessMonthEnd.isAnchored`\n\n`CustomBusinessMonthEnd.onOffset`\n\n`CustomBusinessMonthEnd.is_anchored`\n\n`CustomBusinessMonthEnd.is_on_offset`\n\n`CustomBusinessMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`CustomBusinessMonthEnd.is_month_start`\n\n`CustomBusinessMonthEnd.is_month_end`\n\n`CustomBusinessMonthEnd.is_quarter_start`\n\n`CustomBusinessMonthEnd.is_quarter_end`\n\n`CustomBusinessMonthEnd.is_year_start`\n\n`CustomBusinessMonthEnd.is_year_end`\n\n`CustomBusinessMonthBegin`\n\nAttributes\n\nAlias:\n\n`CBMonthBegin`\n\nalias of `pandas._libs.tslibs.offsets.CustomBusinessMonthBegin`\n\n`CustomBusinessMonthBegin.freqstr`\n\n`CustomBusinessMonthBegin.kwds`\n\n`CustomBusinessMonthBegin.m_offset`\n\n`CustomBusinessMonthBegin.name`\n\n`CustomBusinessMonthBegin.nanos`\n\n`CustomBusinessMonthBegin.normalize`\n\n`CustomBusinessMonthBegin.rule_code`\n\n`CustomBusinessMonthBegin.n`\n\n`CustomBusinessMonthBegin.weekmask`\n\n`CustomBusinessMonthBegin.calendar`\n\n`CustomBusinessMonthBegin.holidays`\n\n`CustomBusinessMonthBegin.apply`\n\n`CustomBusinessMonthBegin.apply_index`(other)\n\n`CustomBusinessMonthBegin.copy`\n\n`CustomBusinessMonthBegin.isAnchored`\n\n`CustomBusinessMonthBegin.onOffset`\n\n`CustomBusinessMonthBegin.is_anchored`\n\n`CustomBusinessMonthBegin.is_on_offset`\n\n`CustomBusinessMonthBegin.__call__`(*args, ...)\n\nCall self as a function.\n\n`CustomBusinessMonthBegin.is_month_start`\n\n`CustomBusinessMonthBegin.is_month_end`\n\n`CustomBusinessMonthBegin.is_quarter_start`\n\n`CustomBusinessMonthBegin.is_quarter_end`\n\n`CustomBusinessMonthBegin.is_year_start`\n\n`CustomBusinessMonthBegin.is_year_end`\n\n`SemiMonthEnd`\n\nTwo DateOffset's per month repeating on the last day of the month and\nday_of_month.\n\n`SemiMonthEnd.freqstr`\n\n`SemiMonthEnd.kwds`\n\n`SemiMonthEnd.name`\n\n`SemiMonthEnd.nanos`\n\n`SemiMonthEnd.normalize`\n\n`SemiMonthEnd.rule_code`\n\n`SemiMonthEnd.n`\n\n`SemiMonthEnd.day_of_month`\n\n`SemiMonthEnd.apply`\n\n`SemiMonthEnd.apply_index`(other)\n\n`SemiMonthEnd.copy`\n\n`SemiMonthEnd.isAnchored`\n\n`SemiMonthEnd.onOffset`\n\n`SemiMonthEnd.is_anchored`\n\n`SemiMonthEnd.is_on_offset`\n\n`SemiMonthEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`SemiMonthEnd.is_month_start`\n\n`SemiMonthEnd.is_month_end`\n\n`SemiMonthEnd.is_quarter_start`\n\n`SemiMonthEnd.is_quarter_end`\n\n`SemiMonthEnd.is_year_start`\n\n`SemiMonthEnd.is_year_end`\n\n`SemiMonthBegin`\n\nTwo DateOffset's per month repeating on the first day of the month and\nday_of_month.\n\n`SemiMonthBegin.freqstr`\n\n`SemiMonthBegin.kwds`\n\n`SemiMonthBegin.name`\n\n`SemiMonthBegin.nanos`\n\n`SemiMonthBegin.normalize`\n\n`SemiMonthBegin.rule_code`\n\n`SemiMonthBegin.n`\n\n`SemiMonthBegin.day_of_month`\n\n`SemiMonthBegin.apply`\n\n`SemiMonthBegin.apply_index`(other)\n\n`SemiMonthBegin.copy`\n\n`SemiMonthBegin.isAnchored`\n\n`SemiMonthBegin.onOffset`\n\n`SemiMonthBegin.is_anchored`\n\n`SemiMonthBegin.is_on_offset`\n\n`SemiMonthBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`SemiMonthBegin.is_month_start`\n\n`SemiMonthBegin.is_month_end`\n\n`SemiMonthBegin.is_quarter_start`\n\n`SemiMonthBegin.is_quarter_end`\n\n`SemiMonthBegin.is_year_start`\n\n`SemiMonthBegin.is_year_end`\n\n`Week`\n\nWeekly offset.\n\n`Week.freqstr`\n\n`Week.kwds`\n\n`Week.name`\n\n`Week.nanos`\n\n`Week.normalize`\n\n`Week.rule_code`\n\n`Week.n`\n\n`Week.weekday`\n\n`Week.apply`\n\n`Week.apply_index`(other)\n\n`Week.copy`\n\n`Week.isAnchored`\n\n`Week.onOffset`\n\n`Week.is_anchored`\n\n`Week.is_on_offset`\n\n`Week.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Week.is_month_start`\n\n`Week.is_month_end`\n\n`Week.is_quarter_start`\n\n`Week.is_quarter_end`\n\n`Week.is_year_start`\n\n`Week.is_year_end`\n\n`WeekOfMonth`\n\nDescribes monthly dates like \"the Tuesday of the 2nd week of each month\".\n\n`WeekOfMonth.freqstr`\n\n`WeekOfMonth.kwds`\n\n`WeekOfMonth.name`\n\n`WeekOfMonth.nanos`\n\n`WeekOfMonth.normalize`\n\n`WeekOfMonth.rule_code`\n\n`WeekOfMonth.n`\n\n`WeekOfMonth.week`\n\n`WeekOfMonth.apply`\n\n`WeekOfMonth.apply_index`(other)\n\n`WeekOfMonth.copy`\n\n`WeekOfMonth.isAnchored`\n\n`WeekOfMonth.onOffset`\n\n`WeekOfMonth.is_anchored`\n\n`WeekOfMonth.is_on_offset`\n\n`WeekOfMonth.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`WeekOfMonth.weekday`\n\n`WeekOfMonth.is_month_start`\n\n`WeekOfMonth.is_month_end`\n\n`WeekOfMonth.is_quarter_start`\n\n`WeekOfMonth.is_quarter_end`\n\n`WeekOfMonth.is_year_start`\n\n`WeekOfMonth.is_year_end`\n\n`LastWeekOfMonth`\n\nDescribes monthly dates in last week of month like \"the last Tuesday of each\nmonth\".\n\n`LastWeekOfMonth.freqstr`\n\n`LastWeekOfMonth.kwds`\n\n`LastWeekOfMonth.name`\n\n`LastWeekOfMonth.nanos`\n\n`LastWeekOfMonth.normalize`\n\n`LastWeekOfMonth.rule_code`\n\n`LastWeekOfMonth.n`\n\n`LastWeekOfMonth.weekday`\n\n`LastWeekOfMonth.week`\n\n`LastWeekOfMonth.apply`\n\n`LastWeekOfMonth.apply_index`(other)\n\n`LastWeekOfMonth.copy`\n\n`LastWeekOfMonth.isAnchored`\n\n`LastWeekOfMonth.onOffset`\n\n`LastWeekOfMonth.is_anchored`\n\n`LastWeekOfMonth.is_on_offset`\n\n`LastWeekOfMonth.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`LastWeekOfMonth.is_month_start`\n\n`LastWeekOfMonth.is_month_end`\n\n`LastWeekOfMonth.is_quarter_start`\n\n`LastWeekOfMonth.is_quarter_end`\n\n`LastWeekOfMonth.is_year_start`\n\n`LastWeekOfMonth.is_year_end`\n\n`BQuarterEnd`\n\nDateOffset increments between the last business day of each Quarter.\n\n`BQuarterEnd.freqstr`\n\n`BQuarterEnd.kwds`\n\n`BQuarterEnd.name`\n\n`BQuarterEnd.nanos`\n\n`BQuarterEnd.normalize`\n\n`BQuarterEnd.rule_code`\n\n`BQuarterEnd.n`\n\n`BQuarterEnd.startingMonth`\n\n`BQuarterEnd.apply`\n\n`BQuarterEnd.apply_index`(other)\n\n`BQuarterEnd.copy`\n\n`BQuarterEnd.isAnchored`\n\n`BQuarterEnd.onOffset`\n\n`BQuarterEnd.is_anchored`\n\n`BQuarterEnd.is_on_offset`\n\n`BQuarterEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BQuarterEnd.is_month_start`\n\n`BQuarterEnd.is_month_end`\n\n`BQuarterEnd.is_quarter_start`\n\n`BQuarterEnd.is_quarter_end`\n\n`BQuarterEnd.is_year_start`\n\n`BQuarterEnd.is_year_end`\n\n`BQuarterBegin`\n\nDateOffset increments between the first business day of each Quarter.\n\n`BQuarterBegin.freqstr`\n\n`BQuarterBegin.kwds`\n\n`BQuarterBegin.name`\n\n`BQuarterBegin.nanos`\n\n`BQuarterBegin.normalize`\n\n`BQuarterBegin.rule_code`\n\n`BQuarterBegin.n`\n\n`BQuarterBegin.startingMonth`\n\n`BQuarterBegin.apply`\n\n`BQuarterBegin.apply_index`(other)\n\n`BQuarterBegin.copy`\n\n`BQuarterBegin.isAnchored`\n\n`BQuarterBegin.onOffset`\n\n`BQuarterBegin.is_anchored`\n\n`BQuarterBegin.is_on_offset`\n\n`BQuarterBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BQuarterBegin.is_month_start`\n\n`BQuarterBegin.is_month_end`\n\n`BQuarterBegin.is_quarter_start`\n\n`BQuarterBegin.is_quarter_end`\n\n`BQuarterBegin.is_year_start`\n\n`BQuarterBegin.is_year_end`\n\n`QuarterEnd`\n\nDateOffset increments between Quarter end dates.\n\n`QuarterEnd.freqstr`\n\n`QuarterEnd.kwds`\n\n`QuarterEnd.name`\n\n`QuarterEnd.nanos`\n\n`QuarterEnd.normalize`\n\n`QuarterEnd.rule_code`\n\n`QuarterEnd.n`\n\n`QuarterEnd.startingMonth`\n\n`QuarterEnd.apply`\n\n`QuarterEnd.apply_index`(other)\n\n`QuarterEnd.copy`\n\n`QuarterEnd.isAnchored`\n\n`QuarterEnd.onOffset`\n\n`QuarterEnd.is_anchored`\n\n`QuarterEnd.is_on_offset`\n\n`QuarterEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`QuarterEnd.is_month_start`\n\n`QuarterEnd.is_month_end`\n\n`QuarterEnd.is_quarter_start`\n\n`QuarterEnd.is_quarter_end`\n\n`QuarterEnd.is_year_start`\n\n`QuarterEnd.is_year_end`\n\n`QuarterBegin`\n\nDateOffset increments between Quarter start dates.\n\n`QuarterBegin.freqstr`\n\n`QuarterBegin.kwds`\n\n`QuarterBegin.name`\n\n`QuarterBegin.nanos`\n\n`QuarterBegin.normalize`\n\n`QuarterBegin.rule_code`\n\n`QuarterBegin.n`\n\n`QuarterBegin.startingMonth`\n\n`QuarterBegin.apply`\n\n`QuarterBegin.apply_index`(other)\n\n`QuarterBegin.copy`\n\n`QuarterBegin.isAnchored`\n\n`QuarterBegin.onOffset`\n\n`QuarterBegin.is_anchored`\n\n`QuarterBegin.is_on_offset`\n\n`QuarterBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`QuarterBegin.is_month_start`\n\n`QuarterBegin.is_month_end`\n\n`QuarterBegin.is_quarter_start`\n\n`QuarterBegin.is_quarter_end`\n\n`QuarterBegin.is_year_start`\n\n`QuarterBegin.is_year_end`\n\n`BYearEnd`\n\nDateOffset increments between the last business day of the year.\n\n`BYearEnd.freqstr`\n\n`BYearEnd.kwds`\n\n`BYearEnd.name`\n\n`BYearEnd.nanos`\n\n`BYearEnd.normalize`\n\n`BYearEnd.rule_code`\n\n`BYearEnd.n`\n\n`BYearEnd.month`\n\n`BYearEnd.apply`\n\n`BYearEnd.apply_index`(other)\n\n`BYearEnd.copy`\n\n`BYearEnd.isAnchored`\n\n`BYearEnd.onOffset`\n\n`BYearEnd.is_anchored`\n\n`BYearEnd.is_on_offset`\n\n`BYearEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BYearEnd.is_month_start`\n\n`BYearEnd.is_month_end`\n\n`BYearEnd.is_quarter_start`\n\n`BYearEnd.is_quarter_end`\n\n`BYearEnd.is_year_start`\n\n`BYearEnd.is_year_end`\n\n`BYearBegin`\n\nDateOffset increments between the first business day of the year.\n\n`BYearBegin.freqstr`\n\n`BYearBegin.kwds`\n\n`BYearBegin.name`\n\n`BYearBegin.nanos`\n\n`BYearBegin.normalize`\n\n`BYearBegin.rule_code`\n\n`BYearBegin.n`\n\n`BYearBegin.month`\n\n`BYearBegin.apply`\n\n`BYearBegin.apply_index`(other)\n\n`BYearBegin.copy`\n\n`BYearBegin.isAnchored`\n\n`BYearBegin.onOffset`\n\n`BYearBegin.is_anchored`\n\n`BYearBegin.is_on_offset`\n\n`BYearBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`BYearBegin.is_month_start`\n\n`BYearBegin.is_month_end`\n\n`BYearBegin.is_quarter_start`\n\n`BYearBegin.is_quarter_end`\n\n`BYearBegin.is_year_start`\n\n`BYearBegin.is_year_end`\n\n`YearEnd`\n\nDateOffset increments between calendar year ends.\n\n`YearEnd.freqstr`\n\n`YearEnd.kwds`\n\n`YearEnd.name`\n\n`YearEnd.nanos`\n\n`YearEnd.normalize`\n\n`YearEnd.rule_code`\n\n`YearEnd.n`\n\n`YearEnd.month`\n\n`YearEnd.apply`\n\n`YearEnd.apply_index`(other)\n\n`YearEnd.copy`\n\n`YearEnd.isAnchored`\n\n`YearEnd.onOffset`\n\n`YearEnd.is_anchored`\n\n`YearEnd.is_on_offset`\n\n`YearEnd.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`YearEnd.is_month_start`\n\n`YearEnd.is_month_end`\n\n`YearEnd.is_quarter_start`\n\n`YearEnd.is_quarter_end`\n\n`YearEnd.is_year_start`\n\n`YearEnd.is_year_end`\n\n`YearBegin`\n\nDateOffset increments between calendar year begin dates.\n\n`YearBegin.freqstr`\n\n`YearBegin.kwds`\n\n`YearBegin.name`\n\n`YearBegin.nanos`\n\n`YearBegin.normalize`\n\n`YearBegin.rule_code`\n\n`YearBegin.n`\n\n`YearBegin.month`\n\n`YearBegin.apply`\n\n`YearBegin.apply_index`(other)\n\n`YearBegin.copy`\n\n`YearBegin.isAnchored`\n\n`YearBegin.onOffset`\n\n`YearBegin.is_anchored`\n\n`YearBegin.is_on_offset`\n\n`YearBegin.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`YearBegin.is_month_start`\n\n`YearBegin.is_month_end`\n\n`YearBegin.is_quarter_start`\n\n`YearBegin.is_quarter_end`\n\n`YearBegin.is_year_start`\n\n`YearBegin.is_year_end`\n\n`FY5253`\n\nDescribes 52-53 week fiscal year.\n\n`FY5253.freqstr`\n\n`FY5253.kwds`\n\n`FY5253.name`\n\n`FY5253.nanos`\n\n`FY5253.normalize`\n\n`FY5253.rule_code`\n\n`FY5253.n`\n\n`FY5253.startingMonth`\n\n`FY5253.variation`\n\n`FY5253.weekday`\n\n`FY5253.apply`\n\n`FY5253.apply_index`(other)\n\n`FY5253.copy`\n\n`FY5253.get_rule_code_suffix`\n\n`FY5253.get_year_end`\n\n`FY5253.isAnchored`\n\n`FY5253.onOffset`\n\n`FY5253.is_anchored`\n\n`FY5253.is_on_offset`\n\n`FY5253.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`FY5253.is_month_start`\n\n`FY5253.is_month_end`\n\n`FY5253.is_quarter_start`\n\n`FY5253.is_quarter_end`\n\n`FY5253.is_year_start`\n\n`FY5253.is_year_end`\n\n`FY5253Quarter`\n\nDateOffset increments between business quarter dates for 52-53 week fiscal\nyear (also known as a 4-4-5 calendar).\n\n`FY5253Quarter.freqstr`\n\n`FY5253Quarter.kwds`\n\n`FY5253Quarter.name`\n\n`FY5253Quarter.nanos`\n\n`FY5253Quarter.normalize`\n\n`FY5253Quarter.rule_code`\n\n`FY5253Quarter.n`\n\n`FY5253Quarter.qtr_with_extra_week`\n\n`FY5253Quarter.startingMonth`\n\n`FY5253Quarter.variation`\n\n`FY5253Quarter.weekday`\n\n`FY5253Quarter.apply`\n\n`FY5253Quarter.apply_index`(other)\n\n`FY5253Quarter.copy`\n\n`FY5253Quarter.get_rule_code_suffix`\n\n`FY5253Quarter.get_weeks`\n\n`FY5253Quarter.isAnchored`\n\n`FY5253Quarter.onOffset`\n\n`FY5253Quarter.is_anchored`\n\n`FY5253Quarter.is_on_offset`\n\n`FY5253Quarter.year_has_extra_week`\n\n`FY5253Quarter.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`FY5253Quarter.is_month_start`\n\n`FY5253Quarter.is_month_end`\n\n`FY5253Quarter.is_quarter_start`\n\n`FY5253Quarter.is_quarter_end`\n\n`FY5253Quarter.is_year_start`\n\n`FY5253Quarter.is_year_end`\n\n`Easter`\n\nDateOffset for the Easter holiday using logic defined in dateutil.\n\n`Easter.freqstr`\n\n`Easter.kwds`\n\n`Easter.name`\n\n`Easter.nanos`\n\n`Easter.normalize`\n\n`Easter.rule_code`\n\n`Easter.n`\n\n`Easter.apply`\n\n`Easter.apply_index`(other)\n\n`Easter.copy`\n\n`Easter.isAnchored`\n\n`Easter.onOffset`\n\n`Easter.is_anchored`\n\n`Easter.is_on_offset`\n\n`Easter.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Easter.is_month_start`\n\n`Easter.is_month_end`\n\n`Easter.is_quarter_start`\n\n`Easter.is_quarter_end`\n\n`Easter.is_year_start`\n\n`Easter.is_year_end`\n\n`Tick`\n\nAttributes\n\n`Tick.delta`\n\n`Tick.freqstr`\n\n`Tick.kwds`\n\n`Tick.name`\n\n`Tick.nanos`\n\n`Tick.normalize`\n\n`Tick.rule_code`\n\n`Tick.n`\n\n`Tick.copy`\n\n`Tick.isAnchored`\n\n`Tick.onOffset`\n\n`Tick.is_anchored`\n\n`Tick.is_on_offset`\n\n`Tick.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Tick.apply`\n\n`Tick.apply_index`(other)\n\n`Tick.is_month_start`\n\n`Tick.is_month_end`\n\n`Tick.is_quarter_start`\n\n`Tick.is_quarter_end`\n\n`Tick.is_year_start`\n\n`Tick.is_year_end`\n\n`Day`\n\nAttributes\n\n`Day.delta`\n\n`Day.freqstr`\n\n`Day.kwds`\n\n`Day.name`\n\n`Day.nanos`\n\n`Day.normalize`\n\n`Day.rule_code`\n\n`Day.n`\n\n`Day.copy`\n\n`Day.isAnchored`\n\n`Day.onOffset`\n\n`Day.is_anchored`\n\n`Day.is_on_offset`\n\n`Day.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Day.apply`\n\n`Day.apply_index`(other)\n\n`Day.is_month_start`\n\n`Day.is_month_end`\n\n`Day.is_quarter_start`\n\n`Day.is_quarter_end`\n\n`Day.is_year_start`\n\n`Day.is_year_end`\n\n`Hour`\n\nAttributes\n\n`Hour.delta`\n\n`Hour.freqstr`\n\n`Hour.kwds`\n\n`Hour.name`\n\n`Hour.nanos`\n\n`Hour.normalize`\n\n`Hour.rule_code`\n\n`Hour.n`\n\n`Hour.copy`\n\n`Hour.isAnchored`\n\n`Hour.onOffset`\n\n`Hour.is_anchored`\n\n`Hour.is_on_offset`\n\n`Hour.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Hour.apply`\n\n`Hour.apply_index`(other)\n\n`Hour.is_month_start`\n\n`Hour.is_month_end`\n\n`Hour.is_quarter_start`\n\n`Hour.is_quarter_end`\n\n`Hour.is_year_start`\n\n`Hour.is_year_end`\n\n`Minute`\n\nAttributes\n\n`Minute.delta`\n\n`Minute.freqstr`\n\n`Minute.kwds`\n\n`Minute.name`\n\n`Minute.nanos`\n\n`Minute.normalize`\n\n`Minute.rule_code`\n\n`Minute.n`\n\n`Minute.copy`\n\n`Minute.isAnchored`\n\n`Minute.onOffset`\n\n`Minute.is_anchored`\n\n`Minute.is_on_offset`\n\n`Minute.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Minute.apply`\n\n`Minute.apply_index`(other)\n\n`Minute.is_month_start`\n\n`Minute.is_month_end`\n\n`Minute.is_quarter_start`\n\n`Minute.is_quarter_end`\n\n`Minute.is_year_start`\n\n`Minute.is_year_end`\n\n`Second`\n\nAttributes\n\n`Second.delta`\n\n`Second.freqstr`\n\n`Second.kwds`\n\n`Second.name`\n\n`Second.nanos`\n\n`Second.normalize`\n\n`Second.rule_code`\n\n`Second.n`\n\n`Second.copy`\n\n`Second.isAnchored`\n\n`Second.onOffset`\n\n`Second.is_anchored`\n\n`Second.is_on_offset`\n\n`Second.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Second.apply`\n\n`Second.apply_index`(other)\n\n`Second.is_month_start`\n\n`Second.is_month_end`\n\n`Second.is_quarter_start`\n\n`Second.is_quarter_end`\n\n`Second.is_year_start`\n\n`Second.is_year_end`\n\n`Milli`\n\nAttributes\n\n`Milli.delta`\n\n`Milli.freqstr`\n\n`Milli.kwds`\n\n`Milli.name`\n\n`Milli.nanos`\n\n`Milli.normalize`\n\n`Milli.rule_code`\n\n`Milli.n`\n\n`Milli.copy`\n\n`Milli.isAnchored`\n\n`Milli.onOffset`\n\n`Milli.is_anchored`\n\n`Milli.is_on_offset`\n\n`Milli.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Milli.apply`\n\n`Milli.apply_index`(other)\n\n`Milli.is_month_start`\n\n`Milli.is_month_end`\n\n`Milli.is_quarter_start`\n\n`Milli.is_quarter_end`\n\n`Milli.is_year_start`\n\n`Milli.is_year_end`\n\n`Micro`\n\nAttributes\n\n`Micro.delta`\n\n`Micro.freqstr`\n\n`Micro.kwds`\n\n`Micro.name`\n\n`Micro.nanos`\n\n`Micro.normalize`\n\n`Micro.rule_code`\n\n`Micro.n`\n\n`Micro.copy`\n\n`Micro.isAnchored`\n\n`Micro.onOffset`\n\n`Micro.is_anchored`\n\n`Micro.is_on_offset`\n\n`Micro.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Micro.apply`\n\n`Micro.apply_index`(other)\n\n`Micro.is_month_start`\n\n`Micro.is_month_end`\n\n`Micro.is_quarter_start`\n\n`Micro.is_quarter_end`\n\n`Micro.is_year_start`\n\n`Micro.is_year_end`\n\n`Nano`\n\nAttributes\n\n`Nano.delta`\n\n`Nano.freqstr`\n\n`Nano.kwds`\n\n`Nano.name`\n\n`Nano.nanos`\n\n`Nano.normalize`\n\n`Nano.rule_code`\n\n`Nano.n`\n\n`Nano.copy`\n\n`Nano.isAnchored`\n\n`Nano.onOffset`\n\n`Nano.is_anchored`\n\n`Nano.is_on_offset`\n\n`Nano.__call__`(*args, **kwargs)\n\nCall self as a function.\n\n`Nano.apply`\n\n`Nano.apply_index`(other)\n\n`Nano.is_month_start`\n\n`Nano.is_month_end`\n\n`Nano.is_quarter_start`\n\n`Nano.is_quarter_end`\n\n`Nano.is_year_start`\n\n`Nano.is_year_end`\n\n"}, {"name": "Duplicate Labels", "path": "user_guide/duplicates", "type": "Manual", "text": "\n`Index` objects are not required to be unique; you can have duplicate row or\ncolumn labels. This may be a bit confusing at first. If you\u2019re familiar with\nSQL, you know that row labels are similar to a primary key on a table, and you\nwould never want duplicates in a SQL table. But one of pandas\u2019 roles is to\nclean messy, real-world data before it goes to some downstream system. And\nreal-world data has duplicates, even in fields that are supposed to be unique.\n\nThis section describes how duplicate labels change the behavior of certain\noperations, and how prevent duplicates from arising during operations, or to\ndetect them if they do.\n\nSome pandas methods (`Series.reindex()` for example) just don\u2019t work with\nduplicates present. The output can\u2019t be determined, and so pandas raises.\n\nOther methods, like indexing, can give very surprising results. Typically\nindexing with a scalar will reduce dimensionality. Slicing a `DataFrame` with\na scalar will return a `Series`. Slicing a `Series` with a scalar will return\na scalar. But with duplicates, this isn\u2019t the case.\n\nWe have duplicates in the columns. If we slice `'B'`, we get back a `Series`\n\nBut slicing `'A'` returns a `DataFrame`\n\nThis applies to row labels as well\n\nYou can check whether an `Index` (storing the row or column labels) is unique\nwith `Index.is_unique`:\n\nNote\n\nChecking whether an index is unique is somewhat expensive for large datasets.\npandas does cache this result, so re-checking on the same index is very fast.\n\n`Index.duplicated()` will return a boolean ndarray indicating whether a label\nis repeated.\n\nWhich can be used as a boolean filter to drop duplicate rows.\n\nIf you need additional logic to handle duplicate labels, rather than just\ndropping the repeats, using `groupby()` on the index is a common trick. For\nexample, we\u2019ll resolve duplicates by taking the average of all rows with the\nsame label.\n\nNew in version 1.2.0.\n\nAs noted above, handling duplicates is an important feature when reading in\nraw data. That said, you may want to avoid introducing duplicates as part of a\ndata processing pipeline (from methods like `pandas.concat()`, `rename()`,\netc.). Both `Series` and `DataFrame` disallow duplicate labels by calling\n`.set_flags(allows_duplicate_labels=False)`. (the default is to allow them).\nIf there are duplicate labels, an exception will be raised.\n\nThis applies to both row and column labels for a `DataFrame`\n\nThis attribute can be checked or set with `allows_duplicate_labels`, which\nindicates whether that object can have duplicate labels.\n\n`DataFrame.set_flags()` can be used to return a new `DataFrame` with\nattributes like `allows_duplicate_labels` set to some value\n\nThe new `DataFrame` returned is a view on the same data as the old\n`DataFrame`. Or the property can just be set directly on the same object\n\nWhen processing raw, messy data you might initially read in the messy data\n(which potentially has duplicate labels), deduplicate, and then disallow\nduplicates going forward, to ensure that your data pipeline doesn\u2019t introduce\nduplicates.\n\nSetting `allows_duplicate_labels=True` on a `Series` or `DataFrame` with\nduplicate labels or performing an operation that introduces duplicate labels\non a `Series` or `DataFrame` that disallows duplicates will raise an\n`errors.DuplicateLabelError`.\n\nThis error message contains the labels that are duplicated, and the numeric\npositions of all the duplicates (including the \u201coriginal\u201d) in the `Series` or\n`DataFrame`\n\nIn general, disallowing duplicates is \u201csticky\u201d. It\u2019s preserved through\noperations.\n\nWarning\n\nThis is an experimental feature. Currently, many methods fail to propagate the\n`allows_duplicate_labels` value. In future versions it is expected that every\nmethod taking or returning one or more DataFrame or Series objects will\npropagate `allows_duplicate_labels`.\n\n"}, {"name": "Enhancing performance", "path": "user_guide/enhancingperf", "type": "Manual", "text": "\nIn this part of the tutorial, we will investigate how to speed up certain\nfunctions operating on pandas `DataFrames` using three different techniques:\nCython, Numba and `pandas.eval()`. We will see a speed improvement of ~200\nwhen we use Cython and Numba on a test function operating row-wise on the\n`DataFrame`. Using `pandas.eval()` we will speed up a sum by an order of ~2.\n\nNote\n\nIn addition to following the steps in this tutorial, users interested in\nenhancing performance are highly encouraged to install the recommended\ndependencies for pandas. These dependencies are often not installed by\ndefault, but will offer speed improvements if present.\n\nFor many use cases writing pandas in pure Python and NumPy is sufficient. In\nsome computationally heavy applications however, it can be possible to achieve\nsizable speed-ups by offloading work to cython.\n\nThis tutorial assumes you have refactored as much as possible in Python, for\nexample by trying to remove for-loops and making use of NumPy vectorization.\nIt\u2019s always worth optimising in Python first.\n\nThis tutorial walks through a \u201ctypical\u201d process of cythonizing a slow\ncomputation. We use an example from the Cython documentation but in the\ncontext of pandas. Our final cythonized solution is around 100 times faster\nthan the pure Python solution.\n\nWe have a `DataFrame` to which we want to apply a function row-wise.\n\nHere\u2019s the function in pure Python:\n\nWe achieve our result by using `apply` (row-wise):\n\nBut clearly this isn\u2019t fast enough for us. Let\u2019s take a look and see where the\ntime is spent during this operation (limited to the most time consuming four\ncalls) using the prun ipython magic function:\n\nBy far the majority of time is spend inside either `integrate_f` or `f`, hence\nwe\u2019ll concentrate our efforts cythonizing these two functions.\n\nFirst we\u2019re going to need to import the Cython magic function to IPython:\n\nNow, let\u2019s simply copy our functions over to Cython as is (the suffix is here\nto distinguish between function versions):\n\nNote\n\nIf you\u2019re having trouble pasting the above into your ipython, you may need to\nbe using bleeding edge IPython for paste to play well with cell magics.\n\nAlready this has shaved a third off, not too bad for a simple copy and paste.\n\nWe get another huge improvement simply by providing type information:\n\nNow, we\u2019re talking! It\u2019s now over ten times faster than the original Python\nimplementation, and we haven\u2019t really modified the code. Let\u2019s have another\nlook at what\u2019s eating up time:\n\nIt\u2019s calling series\u2026 a lot! It\u2019s creating a Series from each row, and get-ting\nfrom both the index and the series (three times for each row). Function calls\nare expensive in Python, so maybe we could minimize these by cythonizing the\napply part.\n\nNote\n\nWe are now passing ndarrays into the Cython function, fortunately Cython plays\nvery nicely with NumPy.\n\nThe implementation is simple, it creates an array of zeros and loops over the\nrows, applying our `integrate_f_typed`, and putting this in the zeros array.\n\nWarning\n\nYou can not pass a `Series` directly as a `ndarray` typed parameter to a\nCython function. Instead pass the actual `ndarray` using the\n`Series.to_numpy()`. The reason is that the Cython definition is specific to\nan ndarray and not the passed `Series`.\n\nSo, do not do this:\n\nBut rather, use `Series.to_numpy()` to get the underlying `ndarray`:\n\nNote\n\nLoops like this would be extremely slow in Python, but in Cython looping over\nNumPy arrays is fast.\n\nWe\u2019ve gotten another big improvement. Let\u2019s check again where the time is\nspent:\n\nAs one might expect, the majority of the time is now spent in\n`apply_integrate_f`, so if we wanted to make anymore efficiencies we must\ncontinue to concentrate our efforts here.\n\nThere is still hope for improvement. Here\u2019s an example of using some more\nadvanced Cython techniques:\n\nEven faster, with the caveat that a bug in our Cython code (an off-by-one\nerror, for example) might cause a segfault because memory access isn\u2019t\nchecked. For more about `boundscheck` and `wraparound`, see the Cython docs on\ncompiler directives.\n\nAn alternative to statically compiling Cython code is to use a dynamic just-\nin-time (JIT) compiler with Numba.\n\nNumba allows you to write a pure Python function which can be JIT compiled to\nnative machine instructions, similar in performance to C, C++ and Fortran, by\ndecorating your function with `@jit`.\n\nNumba works by generating optimized machine code using the LLVM compiler\ninfrastructure at import time, runtime, or statically (using the included pycc\ntool). Numba supports compilation of Python to run on either CPU or GPU\nhardware and is designed to integrate with the Python scientific software\nstack.\n\nNote\n\nThe `@jit` compilation will add overhead to the runtime of the function, so\nperformance benefits may not be realized especially when using small data\nsets. Consider caching your function to avoid compilation overhead each time\nyour function is run.\n\nNumba can be used in 2 ways with pandas:\n\nSpecify the `engine=\"numba\"` keyword in select pandas methods\n\nDefine your own Python function decorated with `@jit` and pass the underlying\nNumPy array of `Series` or `Dataframe` (using `to_numpy()`) into the function\n\nIf Numba is installed, one can specify `engine=\"numba\"` in select pandas\nmethods to execute the method using Numba. Methods that support\n`engine=\"numba\"` will also have an `engine_kwargs` keyword that accepts a\ndictionary that allows one to specify `\"nogil\"`, `\"nopython\"` and `\"parallel\"`\nkeys with boolean values to pass into the `@jit` decorator. If `engine_kwargs`\nis not specified, it defaults to `{\"nogil\": False, \"nopython\": True,\n\"parallel\": False}` unless otherwise specified.\n\nIn terms of performance, the first time a function is run using the Numba\nengine will be slow as Numba will have some function compilation overhead.\nHowever, the JIT compiled functions are cached, and subsequent calls will be\nfast. In general, the Numba engine is performant with a larger amount of data\npoints (e.g. 1+ million).\n\nA custom Python function decorated with `@jit` can be used with pandas objects\nby passing their NumPy array representations with `to_numpy()`.\n\nIn this example, using Numba was faster than Cython.\n\nNumba can also be used to write vectorized functions that do not require the\nuser to explicitly loop over the observations of a vector; a vectorized\nfunction will be applied to each row automatically. Consider the following\nexample of doubling each observation:\n\nNumba is best at accelerating functions that apply numerical functions to\nNumPy arrays. If you try to `@jit` a function that contains unsupported Python\nor NumPy code, compilation will revert object mode which will mostly likely\nnot speed up your function. If you would prefer that Numba throw an error if\nit cannot compile a function in a way that speeds up your code, pass Numba the\nargument `nopython=True` (e.g. `@jit(nopython=True)`). For more on\ntroubleshooting Numba modes, see the Numba troubleshooting page.\n\nUsing `parallel=True` (e.g. `@jit(parallel=True)`) may result in a `SIGABRT`\nif the threading layer leads to unsafe behavior. You can first specify a safe\nthreading layer before running a JIT function with `parallel=True`.\n\nGenerally if the you encounter a segfault (`SIGSEGV`) while using Numba,\nplease report the issue to the Numba issue tracker.\n\nThe top-level function `pandas.eval()` implements expression evaluation of\n`Series` and `DataFrame` objects.\n\nNote\n\nTo benefit from using `eval()` you need to install `numexpr`. See the\nrecommended dependencies section for more details.\n\nThe point of using `eval()` for expression evaluation rather than plain Python\nis two-fold: 1) large `DataFrame` objects are evaluated more efficiently and\n2) large arithmetic and boolean expressions are evaluated all at once by the\nunderlying engine (by default `numexpr` is used for evaluation).\n\nNote\n\nYou should not use `eval()` for simple expressions or for expressions\ninvolving small DataFrames. In fact, `eval()` is many orders of magnitude\nslower for smaller expressions/objects than plain ol\u2019 Python. A good rule of\nthumb is to only use `eval()` when you have a `DataFrame` with more than\n10,000 rows.\n\n`eval()` supports all arithmetic expressions supported by the engine in\naddition to some extensions available only in pandas.\n\nNote\n\nThe larger the frame and the larger the expression the more speedup you will\nsee from using `eval()`.\n\nThese operations are supported by `pandas.eval()`:\n\nArithmetic operations except for the left shift (`<<`) and right shift (`>>`)\noperators, e.g., `df + 2 * pi / s ** 4 % 42 - the_golden_ratio`\n\nComparison operations, including chained comparisons, e.g., `2 < df < df2`\n\nBoolean operations, e.g., `df < df2 and df3 < df4 or not df_bool`\n\n`list` and `tuple` literals, e.g., `[1, 2]` or `(1, 2)`\n\nAttribute access, e.g., `df.a`\n\nSubscript expressions, e.g., `df[0]`\n\nSimple variable evaluation, e.g., `pd.eval(\"df\")` (this is not very useful)\n\nMath functions: `sin`, `cos`, `exp`, `log`, `expm1`, `log1p`, `sqrt`, `sinh`,\n`cosh`, `tanh`, `arcsin`, `arccos`, `arctan`, `arccosh`, `arcsinh`, `arctanh`,\n`abs`, `arctan2` and `log10`.\n\nThis Python syntax is not allowed:\n\nExpressions\n\nFunction calls other than math functions.\n\n`is`/`is not` operations\n\n`if` expressions\n\n`lambda` expressions\n\n`list`/`set`/`dict` comprehensions\n\nLiteral `dict` and `set` expressions\n\n`yield` expressions\n\nGenerator expressions\n\nBoolean expressions consisting of only scalar values\n\nStatements\n\nNeither simple nor compound statements are allowed. This includes things like\n`for`, `while`, and `if`.\n\n`pandas.eval()` works well with expressions containing large arrays.\n\nFirst let\u2019s create a few decent-sized arrays to play with:\n\nNow let\u2019s compare adding them together using plain ol\u2019 Python versus `eval()`:\n\nNow let\u2019s do the same thing but with comparisons:\n\n`eval()` also works with unaligned pandas objects:\n\nNote\n\nOperations such as\n\nshould be performed in Python. An exception will be raised if you try to\nperform any boolean/bitwise operations with scalar operands that are not of\ntype `bool` or `np.bool_`. Again, you should perform these kinds of operations\nin plain Python.\n\nIn addition to the top level `pandas.eval()` function you can also evaluate an\nexpression in the \u201ccontext\u201d of a `DataFrame`.\n\nAny expression that is a valid `pandas.eval()` expression is also a valid\n`DataFrame.eval()` expression, with the added benefit that you don\u2019t have to\nprefix the name of the `DataFrame` to the column(s) you\u2019re interested in\nevaluating.\n\nIn addition, you can perform assignment of columns within an expression. This\nallows for formulaic evaluation. The assignment target can be a new column\nname or an existing column name, and it must be a valid Python identifier.\n\nThe `inplace` keyword determines whether this assignment will performed on the\noriginal `DataFrame` or return a copy with the new column.\n\nWhen `inplace` is set to `False`, the default, a copy of the `DataFrame` with\nthe new or modified columns is returned and the original frame is unchanged.\n\nAs a convenience, multiple assignments can be performed by using a multi-line\nstring.\n\nThe equivalent in standard Python would be\n\nThe `query` method has a `inplace` keyword which determines whether the query\nmodifies the original frame.\n\nYou must explicitly reference any local variable that you want to use in an\nexpression by placing the `@` character in front of the name. For example,\n\nIf you don\u2019t prefix the local variable with `@`, pandas will raise an\nexception telling you the variable is undefined.\n\nWhen using `DataFrame.eval()` and `DataFrame.query()`, this allows you to have\na local variable and a `DataFrame` column with the same name in an expression.\n\nWith `pandas.eval()` you cannot use the `@` prefix at all, because it isn\u2019t\ndefined in that context. pandas will let you know this if you try to use `@`\nin a top-level call to `pandas.eval()`. For example,\n\nIn this case, you should simply refer to the variables like you would in\nstandard Python.\n\nThere are two different parsers and two different engines you can use as the\nbackend.\n\nThe default `'pandas'` parser allows a more intuitive syntax for expressing\nquery-like operations (comparisons, conjunctions and disjunctions). In\nparticular, the precedence of the `&` and `|` operators is made equal to the\nprecedence of the corresponding boolean operations `and` and `or`.\n\nFor example, the above conjunction can be written without parentheses.\nAlternatively, you can use the `'python'` parser to enforce strict Python\nsemantics.\n\nThe same expression can be \u201canded\u201d together with the word `and` as well:\n\nThe `and` and `or` operators here have the same precedence that they would in\nvanilla Python.\n\nThere\u2019s also the option to make `eval()` operate identical to plain ol\u2019\nPython.\n\nNote\n\nUsing the `'python'` engine is generally not useful, except for testing other\nevaluation engines against it. You will achieve no performance benefits using\n`eval()` with `engine='python'` and in fact may incur a performance hit.\n\nYou can see this by using `pandas.eval()` with the `'python'` engine. It is a\nbit slower (not by much) than evaluating the same expression in Python\n\n`eval()` is intended to speed up certain kinds of operations. In particular,\nthose operations involving complex expressions with large `DataFrame`/`Series`\nobjects should see a significant performance benefit. Here is a plot showing\nthe running time of `pandas.eval()` as function of the size of the frame\ninvolved in the computation. The two lines are two different engines.\n\nNote\n\nOperations with smallish objects (around 15k-20k rows) are faster using plain\nPython:\n\nThis plot was created using a `DataFrame` with 3 columns each containing\nfloating point values generated using `numpy.random.randn()`.\n\nExpressions that would result in an object dtype or involve datetime\noperations (because of `NaT`) must be evaluated in Python space. The main\nreason for this behavior is to maintain backwards compatibility with versions\nof NumPy < 1.7. In those versions of NumPy a call to `ndarray.astype(str)`\nwill truncate any strings that are more than 60 characters in length. Second,\nwe can\u2019t pass `object` arrays to `numexpr` thus string comparisons must be\nevaluated in Python space.\n\nThe upshot is that this only applies to object-dtype expressions. So, if you\nhave an expression\u2013for example\n\nthe numeric part of the comparison (`nums == 1`) will be evaluated by\n`numexpr`.\n\nIn general, `DataFrame.query()`/`pandas.eval()` will evaluate the\nsubexpressions that can be evaluated by `numexpr` and those that must be\nevaluated in Python space transparently to the user. This is done by inferring\nthe result type of an expression from its arguments and operators.\n\n"}, {"name": "Essential basic functionality", "path": "user_guide/basics", "type": "Manual", "text": "\nHere we discuss a lot of the essential functionality common to the pandas data\nstructures. To begin, let\u2019s create some example objects like we did in the 10\nminutes to pandas section:\n\nTo view a small sample of a Series or DataFrame object, use the `head()` and\n`tail()` methods. The default number of elements to display is five, but you\nmay pass a custom number.\n\npandas objects have a number of attributes enabling you to access the metadata\n\nshape: gives the axis dimensions of the object, consistent with ndarray\n\nSeries: index (only axis)\n\nDataFrame: index (rows) and columns\n\nNote, these attributes can be safely assigned to!\n\npandas objects (`Index`, `Series`, `DataFrame`) can be thought of as\ncontainers for arrays, which hold the actual data and do the actual\ncomputation. For many types, the underlying array is a `numpy.ndarray`.\nHowever, pandas and 3rd party libraries may extend NumPy\u2019s type system to add\nsupport for custom arrays (see dtypes).\n\nTo get the actual data inside a `Index` or `Series`, use the `.array` property\n\n`array` will always be an `ExtensionArray`. The exact details of what an\n`ExtensionArray` is and why pandas uses them are a bit beyond the scope of\nthis introduction. See dtypes for more.\n\nIf you know you need a NumPy array, use `to_numpy()` or `numpy.asarray()`.\n\nWhen the Series or Index is backed by an `ExtensionArray`, `to_numpy()` may\ninvolve copying data and coercing values. See dtypes for more.\n\n`to_numpy()` gives some control over the `dtype` of the resulting\n`numpy.ndarray`. For example, consider datetimes with timezones. NumPy doesn\u2019t\nhave a dtype to represent timezone-aware datetimes, so there are two possibly\nuseful representations:\n\nAn object-dtype `numpy.ndarray` with `Timestamp` objects, each with the\ncorrect `tz`\n\nA `datetime64[ns]` -dtype `numpy.ndarray`, where the values have been\nconverted to UTC and the timezone discarded\n\nTimezones may be preserved with `dtype=object`\n\nOr thrown away with `dtype='datetime64[ns]'`\n\nGetting the \u201craw data\u201d inside a `DataFrame` is possibly a bit more complex.\nWhen your `DataFrame` only has a single data type for all the columns,\n`DataFrame.to_numpy()` will return the underlying data:\n\nIf a DataFrame contains homogeneously-typed data, the ndarray can actually be\nmodified in-place, and the changes will be reflected in the data structure.\nFor heterogeneous data (e.g. some of the DataFrame\u2019s columns are not all the\nsame dtype), this will not be the case. The values attribute itself, unlike\nthe axis labels, cannot be assigned to.\n\nNote\n\nWhen working with heterogeneous data, the dtype of the resulting ndarray will\nbe chosen to accommodate all of the data involved. For example, if strings are\ninvolved, the result will be of object dtype. If there are only floats and\nintegers, the resulting array will be of float dtype.\n\nIn the past, pandas recommended `Series.values` or `DataFrame.values` for\nextracting the data from a Series or DataFrame. You\u2019ll still find references\nto these in old code bases and online. Going forward, we recommend avoiding\n`.values` and using `.array` or `.to_numpy()`. `.values` has the following\ndrawbacks:\n\nWhen your Series contains an extension type, it\u2019s unclear whether\n`Series.values` returns a NumPy array or the extension array. `Series.array`\nwill always return an `ExtensionArray`, and will never copy data.\n`Series.to_numpy()` will always return a NumPy array, potentially at the cost\nof copying / coercing values.\n\nWhen your DataFrame contains a mixture of data types, `DataFrame.values` may\ninvolve copying data and coercing values to a common dtype, a relatively\nexpensive operation. `DataFrame.to_numpy()`, being a method, makes it clearer\nthat the returned NumPy array may not be a view on the same data in the\nDataFrame.\n\npandas has support for accelerating certain types of binary numerical and\nboolean operations using the `numexpr` library and the `bottleneck` libraries.\n\nThese libraries are especially useful when dealing with large data sets, and\nprovide large speedups. `numexpr` uses smart chunking, caching, and multiple\ncores. `bottleneck` is a set of specialized cython routines that are\nespecially fast when dealing with arrays that have `nans`.\n\nHere is a sample (using 100 column x 100,000 row `DataFrames`):\n\nOperation\n\n0.11.0 (ms)\n\nPrior Version (ms)\n\nRatio to Prior\n\n`df1 > df2`\n\n13.32\n\n125.35\n\n0.1063\n\n`df1 * df2`\n\n21.71\n\n36.63\n\n0.5928\n\n`df1 + df2`\n\n22.04\n\n36.50\n\n0.6039\n\nYou are highly encouraged to install both libraries. See the section\nRecommended Dependencies for more installation info.\n\nThese are both enabled to be used by default, you can control this by setting\nthe options:\n\nWith binary operations between pandas data structures, there are two key\npoints of interest:\n\nBroadcasting behavior between higher- (e.g. DataFrame) and lower-dimensional\n(e.g. Series) objects.\n\nMissing data in computations.\n\nWe will demonstrate how to manage these issues independently, though they can\nbe handled simultaneously.\n\nDataFrame has the methods `add()`, `sub()`, `mul()`, `div()` and related\nfunctions `radd()`, `rsub()`, \u2026 for carrying out binary operations. For\nbroadcasting behavior, Series input is of primary interest. Using these\nfunctions, you can use to either match on the index or columns via the axis\nkeyword:\n\nFurthermore you can align a level of a MultiIndexed DataFrame with a Series.\n\nSeries and Index also support the `divmod()` builtin. This function takes the\nfloor division and modulo operation at the same time returning a two-tuple of\nthe same type as the left hand side. For example:\n\nWe can also do elementwise `divmod()`:\n\nIn Series and DataFrame, the arithmetic functions have the option of inputting\na fill_value, namely a value to substitute when at most one of the values at a\nlocation are missing. For example, when adding two DataFrame objects, you may\nwish to treat NaN as 0 unless both DataFrames are missing that value, in which\ncase the result will be NaN (you can later replace NaN with some other value\nusing `fillna` if you wish).\n\nSeries and DataFrame have the binary comparison methods `eq`, `ne`, `lt`,\n`gt`, `le`, and `ge` whose behavior is analogous to the binary arithmetic\noperations described above:\n\nThese operations produce a pandas object of the same type as the left-hand-\nside input that is of dtype `bool`. These `boolean` objects can be used in\nindexing operations, see the section on Boolean indexing.\n\nYou can apply the reductions: `empty`, `any()`, `all()`, and `bool()` to\nprovide a way to summarize a boolean result.\n\nYou can reduce to a final boolean value.\n\nYou can test if a pandas object is empty, via the `empty` property.\n\nTo evaluate single-element pandas objects in a boolean context, use the method\n`bool()`:\n\nWarning\n\nYou might be tempted to do the following:\n\nOr\n\nThese will both raise errors, as you are trying to compare multiple values.:\n\nSee gotchas for a more detailed discussion.\n\nOften you may find that there is more than one way to compute the same result.\nAs a simple example, consider `df + df` and `df * 2`. To test that these two\ncomputations produce the same result, given the tools shown above, you might\nimagine using `(df + df == df * 2).all()`. But in fact, this expression is\nFalse:\n\nNotice that the boolean DataFrame `df + df == df * 2` contains some False\nvalues! This is because NaNs do not compare as equals:\n\nSo, NDFrames (such as Series and DataFrames) have an `equals()` method for\ntesting equality, with NaNs in corresponding locations treated as equal.\n\nNote that the Series or DataFrame index needs to be in the same order for\nequality to be True:\n\nYou can conveniently perform element-wise comparisons when comparing a pandas\ndata structure with a scalar value:\n\npandas also handles element-wise comparisons between different array-like\nobjects of the same length:\n\nTrying to compare `Index` or `Series` objects of different lengths will raise\na ValueError:\n\nNote that this is different from the NumPy behavior where a comparison can be\nbroadcast:\n\nor it can return False if broadcasting can not be done:\n\nA problem occasionally arising is the combination of two similar data sets\nwhere values in one are preferred over the other. An example would be two data\nseries representing a particular economic indicator where one is considered to\nbe of \u201chigher quality\u201d. However, the lower quality series might extend further\nback in history or have more complete data coverage. As such, we would like to\ncombine two DataFrame objects where missing values in one DataFrame are\nconditionally filled with like-labeled values from the other DataFrame. The\nfunction implementing this operation is `combine_first()`, which we\nillustrate:\n\nThe `combine_first()` method above calls the more general\n`DataFrame.combine()`. This method takes another DataFrame and a combiner\nfunction, aligns the input DataFrame and then passes the combiner function\npairs of Series (i.e., columns whose names are the same).\n\nSo, for instance, to reproduce `combine_first()` as above:\n\nThere exists a large number of methods for computing descriptive statistics\nand other related operations on Series, DataFrame. Most of these are\naggregations (hence producing a lower-dimensional result) like `sum()`,\n`mean()`, and `quantile()`, but some of them, like `cumsum()` and `cumprod()`,\nproduce an object of the same size. Generally speaking, these methods take an\naxis argument, just like ndarray.{sum, std, \u2026}, but the axis can be specified\nby name or integer:\n\nSeries: no axis argument needed\n\nDataFrame: \u201cindex\u201d (axis=0, default), \u201ccolumns\u201d (axis=1)\n\nFor example:\n\nAll such methods have a `skipna` option signaling whether to exclude missing\ndata (`True` by default):\n\nCombined with the broadcasting / arithmetic behavior, one can describe various\nstatistical procedures, like standardization (rendering data zero mean and\nstandard deviation of 1), very concisely:\n\nNote that methods like `cumsum()` and `cumprod()` preserve the location of\n`NaN` values. This is somewhat different from `expanding()` and `rolling()`\nsince `NaN` behavior is furthermore dictated by a `min_periods` parameter.\n\nHere is a quick reference summary table of common functions. Each also takes\nan optional `level` parameter which applies only if the object has a\nhierarchical index.\n\nFunction\n\nDescription\n\n`count`\n\nNumber of non-NA observations\n\n`sum`\n\nSum of values\n\n`mean`\n\nMean of values\n\n`mad`\n\nMean absolute deviation\n\n`median`\n\nArithmetic median of values\n\n`min`\n\nMinimum\n\n`max`\n\nMaximum\n\n`mode`\n\nMode\n\n`abs`\n\nAbsolute Value\n\n`prod`\n\nProduct of values\n\n`std`\n\nBessel-corrected sample standard deviation\n\n`var`\n\nUnbiased variance\n\n`sem`\n\nStandard error of the mean\n\n`skew`\n\nSample skewness (3rd moment)\n\n`kurt`\n\nSample kurtosis (4th moment)\n\n`quantile`\n\nSample quantile (value at %)\n\n`cumsum`\n\nCumulative sum\n\n`cumprod`\n\nCumulative product\n\n`cummax`\n\nCumulative maximum\n\n`cummin`\n\nCumulative minimum\n\nNote that by chance some NumPy methods, like `mean`, `std`, and `sum`, will\nexclude NAs on Series input by default:\n\n`Series.nunique()` will return the number of unique non-NA values in a Series:\n\nThere is a convenient `describe()` function which computes a variety of\nsummary statistics about a Series or the columns of a DataFrame (excluding NAs\nof course):\n\nYou can select specific percentiles to include in the output:\n\nBy default, the median is always included.\n\nFor a non-numerical Series object, `describe()` will give a simple summary of\nthe number of unique values and most frequently occurring values:\n\nNote that on a mixed-type DataFrame object, `describe()` will restrict the\nsummary to include only numerical columns or, if none are, only categorical\ncolumns:\n\nThis behavior can be controlled by providing a list of types as\n`include`/`exclude` arguments. The special value `all` can also be used:\n\nThat feature relies on select_dtypes. Refer to there for details about\naccepted inputs.\n\nThe `idxmin()` and `idxmax()` functions on Series and DataFrame compute the\nindex labels with the minimum and maximum corresponding values:\n\nWhen there are multiple rows (or columns) matching the minimum or maximum\nvalue, `idxmin()` and `idxmax()` return the first matching index:\n\nNote\n\n`idxmin` and `idxmax` are called `argmin` and `argmax` in NumPy.\n\nThe `value_counts()` Series method and top-level function computes a histogram\nof a 1D array of values. It can also be used as a function on regular arrays:\n\nNew in version 1.1.0.\n\nThe `value_counts()` method can be used to count combinations across multiple\ncolumns. By default all columns are used but a subset can be selected using\nthe `subset` argument.\n\nSimilarly, you can get the most frequently occurring value(s), i.e. the mode,\nof the values in a Series or DataFrame:\n\nContinuous values can be discretized using the `cut()` (bins based on values)\nand `qcut()` (bins based on sample quantiles) functions:\n\n`qcut()` computes sample quantiles. For example, we could slice up some\nnormally distributed data into equal-size quartiles like so:\n\nWe can also pass infinite values to define the bins:\n\nTo apply your own or another library\u2019s functions to pandas objects, you should\nbe aware of the three methods below. The appropriate method to use depends on\nwhether your function expects to operate on an entire `DataFrame` or `Series`,\nrow- or column-wise, or elementwise.\n\nTablewise Function Application: `pipe()`\n\nRow or Column-wise Function Application: `apply()`\n\nAggregation API: `agg()` and `transform()`\n\nApplying Elementwise Functions: `applymap()`\n\n`DataFrames` and `Series` can be passed into functions. However, if the\nfunction needs to be called in a chain, consider using the `pipe()` method.\n\nFirst some setup:\n\n`extract_city_name` and `add_country_name` are functions taking and returning\n`DataFrames`.\n\nNow compare the following:\n\nIs equivalent to:\n\npandas encourages the second style, which is known as method chaining. `pipe`\nmakes it easy to use your own or another library\u2019s functions in method chains,\nalongside pandas\u2019 methods.\n\nIn the example above, the functions `extract_city_name` and `add_country_name`\neach expected a `DataFrame` as the first positional argument. What if the\nfunction you wish to apply takes its data as, say, the second argument? In\nthis case, provide `pipe` with a tuple of `(callable, data_keyword)`. `.pipe`\nwill route the `DataFrame` to the argument specified in the tuple.\n\nFor example, we can fit a regression using statsmodels. Their API expects a\nformula first and a `DataFrame` as the second argument, `data`. We pass in the\nfunction, keyword pair `(sm.ols, 'data')` to `pipe`:\n\nThe pipe method is inspired by unix pipes and more recently dplyr and\nmagrittr, which have introduced the popular `(%>%)` (read pipe) operator for\nR. The implementation of `pipe` here is quite clean and feels right at home in\nPython. We encourage you to view the source code of `pipe()`.\n\nArbitrary functions can be applied along the axes of a DataFrame using the\n`apply()` method, which, like the descriptive statistics methods, takes an\noptional `axis` argument:\n\nThe `apply()` method will also dispatch on a string method name.\n\nThe return type of the function passed to `apply()` affects the type of the\nfinal output from `DataFrame.apply` for the default behaviour:\n\nIf the applied function returns a `Series`, the final output is a `DataFrame`.\nThe columns match the index of the `Series` returned by the applied function.\n\nIf the applied function returns any other type, the final output is a\n`Series`.\n\nThis default behaviour can be overridden using the `result_type`, which\naccepts three options: `reduce`, `broadcast`, and `expand`. These will\ndetermine how list-likes return values expand (or not) to a `DataFrame`.\n\n`apply()` combined with some cleverness can be used to answer many questions\nabout a data set. For example, suppose we wanted to extract the date where the\nmaximum value for each column occurred:\n\nYou may also pass additional arguments and keyword arguments to the `apply()`\nmethod. For instance, consider the following function you would like to apply:\n\nYou may then apply this function as follows:\n\nAnother useful feature is the ability to pass Series methods to carry out some\nSeries operation on each column or row:\n\nFinally, `apply()` takes an argument `raw` which is False by default, which\nconverts each row or column into a Series before applying the function. When\nset to True, the passed function will instead receive an ndarray object, which\nhas positive performance implications if you do not need the indexing\nfunctionality.\n\nThe aggregation API allows one to express possibly multiple aggregation\noperations in a single concise way. This API is similar across pandas objects,\nsee groupby API, the window API, and the resample API. The entry point for\naggregation is `DataFrame.aggregate()`, or the alias `DataFrame.agg()`.\n\nWe will use a similar starting frame from above:\n\nUsing a single function is equivalent to `apply()`. You can also pass named\nmethods as strings. These will return a `Series` of the aggregated output:\n\nSingle aggregations on a `Series` this will return a scalar value:\n\nYou can pass multiple aggregation arguments as a list. The results of each of\nthe passed functions will be a row in the resulting `DataFrame`. These are\nnaturally named from the aggregation function.\n\nMultiple functions yield multiple rows:\n\nOn a `Series`, multiple functions return a `Series`, indexed by the function\nnames:\n\nPassing a `lambda` function will yield a `<lambda>` named row:\n\nPassing a named function will yield that name for the row:\n\nPassing a dictionary of column names to a scalar or a list of scalars, to\n`DataFrame.agg` allows you to customize which functions are applied to which\ncolumns. Note that the results are not in any particular order, you can use an\n`OrderedDict` instead to guarantee ordering.\n\nPassing a list-like will generate a `DataFrame` output. You will get a matrix-\nlike output of all of the aggregators. The output will consist of all unique\nfunctions. Those that are not noted for a particular column will be `NaN`:\n\nDeprecated since version 1.4.0: Attempting to determine which columns cannot\nbe aggregated and silently dropping them from the results is deprecated and\nwill be removed in a future version. If any porition of the columns or\noperations provided fail, the call to `.agg` will raise.\n\nWhen presented with mixed dtypes that cannot aggregate, `.agg` will only take\nthe valid aggregations. This is similar to how `.groupby.agg` works.\n\nWith `.agg()` it is possible to easily create a custom describe function,\nsimilar to the built in describe function.\n\nThe `transform()` method returns an object that is indexed the same (same\nsize) as the original. This API allows you to provide multiple operations at\nthe same time rather than one-by-one. Its API is quite similar to the `.agg`\nAPI.\n\nWe create a frame similar to the one used in the above sections.\n\nTransform the entire frame. `.transform()` allows input functions as: a NumPy\nfunction, a string function name or a user defined function.\n\nHere `transform()` received a single function; this is equivalent to a ufunc\napplication.\n\nPassing a single function to `.transform()` with a `Series` will yield a\nsingle `Series` in return.\n\nPassing multiple functions will yield a column MultiIndexed DataFrame. The\nfirst level will be the original frame column names; the second level will be\nthe names of the transforming functions.\n\nPassing multiple functions to a Series will yield a DataFrame. The resulting\ncolumn names will be the transforming functions.\n\nPassing a dict of functions will allow selective transforming per column.\n\nPassing a dict of lists will generate a MultiIndexed DataFrame with these\nselective transforms.\n\nSince not all functions can be vectorized (accept NumPy arrays and return\nanother array or value), the methods `applymap()` on DataFrame and analogously\n`map()` on Series accept any Python function taking a single value and\nreturning a single value. For example:\n\n`Series.map()` has an additional feature; it can be used to easily \u201clink\u201d or\n\u201cmap\u201d values defined by a secondary series. This is closely related to\nmerging/joining functionality:\n\n`reindex()` is the fundamental data alignment method in pandas. It is used to\nimplement nearly all other features relying on label-alignment functionality.\nTo reindex means to conform the data to match a given set of labels along a\nparticular axis. This accomplishes several things:\n\nReorders the existing data to match a new set of labels\n\nInserts missing value (NA) markers in label locations where no data for that\nlabel existed\n\nIf specified, fill data for missing labels using logic (highly relevant to\nworking with time series data)\n\nHere is a simple example:\n\nHere, the `f` label was not contained in the Series and hence appears as `NaN`\nin the result.\n\nWith a DataFrame, you can simultaneously reindex the index and columns:\n\nYou may also use `reindex` with an `axis` keyword:\n\nNote that the `Index` objects containing the actual axis labels can be shared\nbetween objects. So if we have a Series and a DataFrame, the following can be\ndone:\n\nThis means that the reindexed Series\u2019s index is the same Python object as the\nDataFrame\u2019s index.\n\n`DataFrame.reindex()` also supports an \u201caxis-style\u201d calling convention, where\nyou specify a single `labels` argument and the `axis` it applies to.\n\nSee also\n\nMultiIndex / Advanced Indexing is an even more concise way of doing\nreindexing.\n\nNote\n\nWhen writing performance-sensitive code, there is a good reason to spend some\ntime becoming a reindexing ninja: many operations are faster on pre-aligned\ndata. Adding two unaligned DataFrames internally triggers a reindexing step.\nFor exploratory analysis you will hardly notice the difference (because\n`reindex` has been heavily optimized), but when CPU cycles matter sprinkling a\nfew explicit `reindex` calls here and there can have an impact.\n\nYou may wish to take an object and reindex its axes to be labeled the same as\nanother object. While the syntax for this is straightforward albeit verbose,\nit is a common enough operation that the `reindex_like()` method is available\nto make this simpler:\n\nThe `align()` method is the fastest way to simultaneously align two objects.\nIt supports a `join` argument (related to joining and merging):\n\n`join='outer'`: take the union of the indexes (default)\n\n`join='left'`: use the calling object\u2019s index\n\n`join='right'`: use the passed object\u2019s index\n\n`join='inner'`: intersect the indexes\n\nIt returns a tuple with both of the reindexed Series:\n\nFor DataFrames, the join method will be applied to both the index and the\ncolumns by default:\n\nYou can also pass an `axis` option to only align on the specified axis:\n\nIf you pass a Series to `DataFrame.align()`, you can choose to align both\nobjects either on the DataFrame\u2019s index or columns using the `axis` argument:\n\n`reindex()` takes an optional parameter `method` which is a filling method\nchosen from the following table:\n\nMethod\n\nAction\n\npad / ffill\n\nFill values forward\n\nbfill / backfill\n\nFill values backward\n\nnearest\n\nFill from the nearest index value\n\nWe illustrate these fill methods on a simple Series:\n\nThese methods require that the indexes are ordered increasing or decreasing.\n\nNote that the same result could have been achieved using fillna (except for\n`method='nearest'`) or interpolate:\n\n`reindex()` will raise a ValueError if the index is not monotonically\nincreasing or decreasing. `fillna()` and `interpolate()` will not perform any\nchecks on the order of the index.\n\nThe `limit` and `tolerance` arguments provide additional control over filling\nwhile reindexing. Limit specifies the maximum count of consecutive matches:\n\nIn contrast, tolerance specifies the maximum distance between the index and\nindexer values:\n\nNotice that when used on a `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`,\n`tolerance` will coerced into a `Timedelta` if possible. This allows you to\nspecify tolerance with appropriate strings.\n\nA method closely related to `reindex` is the `drop()` function. It removes a\nset of labels from an axis:\n\nNote that the following also works, but is a bit less obvious / clean:\n\nThe `rename()` method allows you to relabel an axis based on some mapping (a\ndict or Series) or an arbitrary function.\n\nIf you pass a function, it must return a value when called with any of the\nlabels (and must produce a set of unique values). A dict or Series can also be\nused:\n\nIf the mapping doesn\u2019t include a column/index label, it isn\u2019t renamed. Note\nthat extra labels in the mapping don\u2019t throw an error.\n\n`DataFrame.rename()` also supports an \u201caxis-style\u201d calling convention, where\nyou specify a single `mapper` and the `axis` to apply that mapping to.\n\nThe `rename()` method also provides an `inplace` named parameter that is by\ndefault `False` and copies the underlying data. Pass `inplace=True` to rename\nthe data in place.\n\nFinally, `rename()` also accepts a scalar or list-like for altering the\n`Series.name` attribute.\n\nThe methods `DataFrame.rename_axis()` and `Series.rename_axis()` allow\nspecific names of a `MultiIndex` to be changed (as opposed to the labels).\n\nThe behavior of basic iteration over pandas objects depends on the type. When\niterating over a Series, it is regarded as array-like, and basic iteration\nproduces the values. DataFrames follow the dict-like convention of iterating\nover the \u201ckeys\u201d of the objects.\n\nIn short, basic iteration (`for i in object`) produces:\n\nSeries: values\n\nDataFrame: column labels\n\nThus, for example, iterating over a DataFrame gives you the column names:\n\npandas objects also have the dict-like `items()` method to iterate over the\n(key, value) pairs.\n\nTo iterate over the rows of a DataFrame, you can use the following methods:\n\n`iterrows()`: Iterate over the rows of a DataFrame as (index, Series) pairs.\nThis converts the rows to Series objects, which can change the dtypes and has\nsome performance implications.\n\n`itertuples()`: Iterate over the rows of a DataFrame as namedtuples of the\nvalues. This is a lot faster than `iterrows()`, and is in most cases\npreferable to use to iterate over the values of a DataFrame.\n\nWarning\n\nIterating through pandas objects is generally slow. In many cases, iterating\nmanually over the rows is not needed and can be avoided with one of the\nfollowing approaches:\n\nLook for a vectorized solution: many operations can be performed using built-\nin methods or NumPy functions, (boolean) indexing, \u2026\n\nWhen you have a function that cannot work on the full DataFrame/Series at\nonce, it is better to use `apply()` instead of iterating over the values. See\nthe docs on function application.\n\nIf you need to do iterative manipulations on the values but performance is\nimportant, consider writing the inner loop with cython or numba. See the\nenhancing performance section for some examples of this approach.\n\nWarning\n\nYou should never modify something you are iterating over. This is not\nguaranteed to work in all cases. Depending on the data types, the iterator\nreturns a copy and not a view, and writing to it will have no effect!\n\nFor example, in the following case setting the value has no effect:\n\nConsistent with the dict-like interface, `items()` iterates through key-value\npairs:\n\nSeries: (index, scalar value) pairs\n\nDataFrame: (column, Series) pairs\n\nFor example:\n\n`iterrows()` allows you to iterate through the rows of a DataFrame as Series\nobjects. It returns an iterator yielding each index value along with a Series\ncontaining the data in each row:\n\nNote\n\nBecause `iterrows()` returns a Series for each row, it does not preserve\ndtypes across the rows (dtypes are preserved across columns for DataFrames).\nFor example,\n\nAll values in `row`, returned as a Series, are now upcasted to floats, also\nthe original integer value in column `x`:\n\nTo preserve dtypes while iterating over the rows, it is better to use\n`itertuples()` which returns namedtuples of the values and which is generally\nmuch faster than `iterrows()`.\n\nFor instance, a contrived way to transpose the DataFrame would be:\n\nThe `itertuples()` method will return an iterator yielding a namedtuple for\neach row in the DataFrame. The first element of the tuple will be the row\u2019s\ncorresponding index value, while the remaining values are the row values.\n\nFor instance:\n\nThis method does not convert the row to a Series object; it merely returns the\nvalues inside a namedtuple. Therefore, `itertuples()` preserves the data type\nof the values and is generally faster as `iterrows()`.\n\nNote\n\nThe column names will be renamed to positional names if they are invalid\nPython identifiers, repeated, or start with an underscore. With a large number\nof columns (>255), regular tuples are returned.\n\n`Series` has an accessor to succinctly return datetime like properties for the\nvalues of the Series, if it is a datetime/period like Series. This will return\na Series, indexed like the existing Series.\n\nThis enables nice expressions like this:\n\nYou can easily produces tz aware transformations:\n\nYou can also chain these types of operations:\n\nYou can also format datetime values as strings with `Series.dt.strftime()`\nwhich supports the same format as the standard `strftime()`.\n\nThe `.dt` accessor works for period and timedelta dtypes.\n\nNote\n\n`Series.dt` will raise a `TypeError` if you access with a non-datetime-like\nvalues.\n\nSeries is equipped with a set of string processing methods that make it easy\nto operate on each element of the array. Perhaps most importantly, these\nmethods exclude missing/NA values automatically. These are accessed via the\nSeries\u2019s `str` attribute and generally have names matching the equivalent\n(scalar) built-in string methods. For example:\n\nPowerful pattern-matching methods are provided as well, but note that pattern-\nmatching generally uses regular expressions by default (and in some cases\nalways uses them).\n\nNote\n\nPrior to pandas 1.0, string methods were only available on `object` -dtype\n`Series`. pandas 1.0 added the `StringDtype` which is dedicated to strings.\nSee Text data types for more.\n\nPlease see Vectorized String Methods for a complete description.\n\npandas supports three kinds of sorting: sorting by index labels, sorting by\ncolumn values, and sorting by a combination of both.\n\nThe `Series.sort_index()` and `DataFrame.sort_index()` methods are used to\nsort a pandas object by its index levels.\n\nNew in version 1.1.0.\n\nSorting by index also supports a `key` parameter that takes a callable\nfunction to apply to the index being sorted. For `MultiIndex` objects, the key\nis applied per-level to the levels specified by `level`.\n\nFor information on key sorting by value, see value sorting.\n\nThe `Series.sort_values()` method is used to sort a `Series` by its values.\nThe `DataFrame.sort_values()` method is used to sort a `DataFrame` by its\ncolumn or row values. The optional `by` parameter to `DataFrame.sort_values()`\nmay used to specify one or more columns to use to determine the sorted order.\n\nThe `by` parameter can take a list of column names, e.g.:\n\nThese methods have special treatment of NA values via the `na_position`\nargument:\n\nNew in version 1.1.0.\n\nSorting also supports a `key` parameter that takes a callable function to\napply to the values being sorted.\n\n`key` will be given the `Series` of values and should return a `Series` or\narray of the same shape with the transformed values. For `DataFrame` objects,\nthe key is applied per column, so the key should still expect a Series and\nreturn a Series, e.g.\n\nThe name or type of each column can be used to apply different functions to\ndifferent columns.\n\nStrings passed as the `by` parameter to `DataFrame.sort_values()` may refer to\neither columns or index level names.\n\nSort by \u2018second\u2019 (index) and \u2018A\u2019 (column)\n\nNote\n\nIf a string matches both a column name and an index level name then a warning\nis issued and the column takes precedence. This will result in an ambiguity\nerror in a future version.\n\nSeries has the `searchsorted()` method, which works similarly to\n`numpy.ndarray.searchsorted()`.\n\n`Series` has the `nsmallest()` and `nlargest()` methods which return the\nsmallest or largest \\\\(n\\\\) values. For a large `Series` this can be much\nfaster than sorting the entire Series and calling `head(n)` on the result.\n\n`DataFrame` also has the `nlargest` and `nsmallest` methods.\n\nYou must be explicit about sorting when the column is a MultiIndex, and fully\nspecify all levels to `by`.\n\nThe `copy()` method on pandas objects copies the underlying data (though not\nthe axis indexes, since they are immutable) and returns a new object. Note\nthat it is seldom necessary to copy objects. For example, there are only a\nhandful of ways to alter a DataFrame in-place:\n\nInserting, deleting, or modifying a column.\n\nAssigning to the `index` or `columns` attributes.\n\nFor homogeneous data, directly modifying the values via the `values` attribute\nor advanced indexing.\n\nTo be clear, no pandas method has the side effect of modifying your data;\nalmost every method returns a new object, leaving the original object\nuntouched. If the data is modified, it is because you did so explicitly.\n\nFor the most part, pandas uses NumPy arrays and dtypes for Series or\nindividual columns of a DataFrame. NumPy provides support for `float`, `int`,\n`bool`, `timedelta64[ns]` and `datetime64[ns]` (note that NumPy does not\nsupport timezone-aware datetimes).\n\npandas and third-party libraries extend NumPy\u2019s type system in a few places.\nThis section describes the extensions pandas has made internally. See\nExtension types for how to write your own extension that works with pandas.\nSee Extension data types for a list of third-party libraries that have\nimplemented an extension.\n\nThe following table lists all of pandas extension types. For methods requiring\n`dtype` arguments, strings can be specified as indicated. See the respective\ndocumentation sections for more on each type.\n\nKind of Data\n\nData Type\n\nScalar\n\nArray\n\nString Aliases\n\ntz-aware datetime\n\n`DatetimeTZDtype`\n\n`Timestamp`\n\n`arrays.DatetimeArray`\n\n`'datetime64[ns, <tz>]'`\n\nCategorical\n\n`CategoricalDtype`\n\n(none)\n\n`Categorical`\n\n`'category'`\n\nperiod (time spans)\n\n`PeriodDtype`\n\n`Period`\n\n`arrays.PeriodArray` `'Period[<freq>]'`\n\n`'period[<freq>]'`,\n\nsparse\n\n`SparseDtype`\n\n(none)\n\n`arrays.SparseArray`\n\n`'Sparse'`, `'Sparse[int]'`, `'Sparse[float]'`\n\nintervals\n\n`IntervalDtype`\n\n`Interval`\n\n`arrays.IntervalArray`\n\n`'interval'`, `'Interval'`, `'Interval[<numpy_dtype>]'`,\n`'Interval[datetime64[ns, <tz>]]'`, `'Interval[timedelta64[<freq>]]'`\n\nnullable integer\n\n`Int64Dtype`, \u2026\n\n(none)\n\n`arrays.IntegerArray`\n\n`'Int8'`, `'Int16'`, `'Int32'`, `'Int64'`, `'UInt8'`, `'UInt16'`, `'UInt32'`,\n`'UInt64'`\n\nStrings\n\n`StringDtype`\n\n`str`\n\n`arrays.StringArray`\n\n`'string'`\n\nBoolean (with NA)\n\n`BooleanDtype`\n\n`bool`\n\n`arrays.BooleanArray`\n\n`'boolean'`\n\npandas has two ways to store strings.\n\n`object` dtype, which can hold any Python object, including strings.\n\n`StringDtype`, which is dedicated to strings.\n\nGenerally, we recommend using `StringDtype`. See Text data types for more.\n\nFinally, arbitrary objects may be stored using the `object` dtype, but should\nbe avoided to the extent possible (for performance and interoperability with\nother libraries and methods. See object conversion).\n\nA convenient `dtypes` attribute for DataFrame returns a Series with the data\ntype of each column.\n\nOn a `Series` object, use the `dtype` attribute.\n\nIf a pandas object contains data with multiple dtypes in a single column, the\ndtype of the column will be chosen to accommodate all of the data types\n(`object` is the most general).\n\nThe number of columns of each type in a `DataFrame` can be found by calling\n`DataFrame.dtypes.value_counts()`.\n\nNumeric dtypes will propagate and can coexist in DataFrames. If a dtype is\npassed (either directly via the `dtype` keyword, a passed `ndarray`, or a\npassed `Series`), then it will be preserved in DataFrame operations.\nFurthermore, different numeric dtypes will NOT be combined. The following\nexample will give you a taste.\n\nBy default integer types are `int64` and float types are `float64`, regardless\nof platform (32-bit or 64-bit). The following will all result in `int64`\ndtypes.\n\nNote that Numpy will choose platform-dependent types when creating arrays. The\nfollowing WILL result in `int32` on 32-bit platform.\n\nTypes can potentially be upcasted when combined with other types, meaning they\nare promoted from the current type (e.g. `int` to `float`).\n\n`DataFrame.to_numpy()` will return the lower-common-denominator of the dtypes,\nmeaning the dtype that can accommodate ALL of the types in the resulting\nhomogeneous dtyped NumPy array. This can force some upcasting.\n\nYou can use the `astype()` method to explicitly convert dtypes from one to\nanother. These will by default return a copy, even if the dtype was unchanged\n(pass `copy=False` to change this behavior). In addition, they will raise an\nexception if the astype operation is invalid.\n\nUpcasting is always according to the NumPy rules. If two different dtypes are\ninvolved in an operation, then the more general one will be used as the result\nof the operation.\n\nConvert a subset of columns to a specified type using `astype()`.\n\nConvert certain columns to a specific dtype by passing a dict to `astype()`.\n\nNote\n\nWhen trying to convert a subset of columns to a specified type using\n`astype()` and `loc()`, upcasting occurs.\n\n`loc()` tries to fit in what we are assigning to the current dtypes, while\n`[]` will overwrite them taking the dtype from the right hand side. Therefore\nthe following piece of code produces the unintended result.\n\npandas offers various functions to try to force conversion of types from the\n`object` dtype to other types. In cases where the data is already of the\ncorrect type, but stored in an `object` array, the `DataFrame.infer_objects()`\nand `Series.infer_objects()` methods can be used to soft convert to the\ncorrect type.\n\nBecause the data was transposed the original inference stored all columns as\nobject, which `infer_objects` will correct.\n\nThe following functions are available for one dimensional object arrays or\nscalars to perform hard conversion of objects to a specified type:\n\n`to_numeric()` (conversion to numeric dtypes)\n\n`to_datetime()` (conversion to datetime objects)\n\n`to_timedelta()` (conversion to timedelta objects)\n\nTo force a conversion, we can pass in an `errors` argument, which specifies\nhow pandas should deal with elements that cannot be converted to desired dtype\nor object. By default, `errors='raise'`, meaning that any errors encountered\nwill be raised during the conversion process. However, if `errors='coerce'`,\nthese errors will be ignored and pandas will convert problematic elements to\n`pd.NaT` (for datetime and timedelta) or `np.nan` (for numeric). This might be\nuseful if you are reading in data which is mostly of the desired dtype (e.g.\nnumeric, datetime), but occasionally has non-conforming elements intermixed\nthat you want to represent as missing:\n\nThe `errors` parameter has a third option of `errors='ignore'`, which will\nsimply return the passed in data if it encounters any errors with the\nconversion to a desired data type:\n\nIn addition to object conversion, `to_numeric()` provides another argument\n`downcast`, which gives the option of downcasting the newly (or already)\nnumeric data to a smaller dtype, which can conserve memory:\n\nAs these methods apply only to one-dimensional arrays, lists or scalars; they\ncannot be used directly on multi-dimensional objects such as DataFrames.\nHowever, with `apply()`, we can \u201capply\u201d the function over each column\nefficiently:\n\nPerforming selection operations on `integer` type data can easily upcast the\ndata to `floating`. The dtype of the input data will be preserved in cases\nwhere `nans` are not introduced. See also Support for integer NA.\n\nWhile float dtypes are unchanged.\n\nThe `select_dtypes()` method implements subsetting of columns based on their\n`dtype`.\n\nFirst, let\u2019s create a `DataFrame` with a slew of different dtypes:\n\nAnd the dtypes:\n\n`select_dtypes()` has two parameters `include` and `exclude` that allow you to\nsay \u201cgive me the columns with these dtypes\u201d (`include`) and/or \u201cgive the\ncolumns without these dtypes\u201d (`exclude`).\n\nFor example, to select `bool` columns:\n\nYou can also pass the name of a dtype in the NumPy dtype hierarchy:\n\n`select_dtypes()` also works with generic dtypes as well.\n\nFor example, to select all numeric and boolean columns while excluding\nunsigned integers:\n\nTo select string columns you must use the `object` dtype:\n\nTo see all the child dtypes of a generic `dtype` like `numpy.number` you can\ndefine a function that returns a tree of child dtypes:\n\nAll NumPy dtypes are subclasses of `numpy.generic`:\n\nNote\n\npandas also defines the types `category`, and `datetime64[ns, tz]`, which are\nnot integrated into the normal NumPy hierarchy and won\u2019t show up with the\nabove function.\n\n"}, {"name": "Extensions", "path": "reference/extensions", "type": "Extensions", "text": "\nThese are primarily intended for library authors looking to extend pandas\nobjects.\n\n`api.extensions.register_extension_dtype`(cls)\n\nRegister an ExtensionType with pandas as class decorator.\n\n`api.extensions.register_dataframe_accessor`(name)\n\nRegister a custom accessor on DataFrame objects.\n\n`api.extensions.register_series_accessor`(name)\n\nRegister a custom accessor on Series objects.\n\n`api.extensions.register_index_accessor`(name)\n\nRegister a custom accessor on Index objects.\n\n`api.extensions.ExtensionDtype`()\n\nA custom data type, to be paired with an ExtensionArray.\n\n`api.extensions.ExtensionArray`()\n\nAbstract base class for custom 1-D array types.\n\n`arrays.PandasArray`(values[, copy])\n\nA pandas ExtensionArray for NumPy data.\n\nAdditionally, we have some utility methods for ensuring your object behaves\ncorrectly.\n\n`api.indexers.check_array_indexer`(array, indexer)\n\nCheck if indexer is a valid array indexer for array.\n\nThe sentinel `pandas.api.extensions.no_default` is used as the default value\nin some methods. Use an `is` comparison to check if the user provides a non-\ndefault value.\n\n"}, {"name": "Frequently Asked Questions (FAQ)", "path": "user_guide/gotchas", "type": "Manual", "text": "\nThe memory usage of a `DataFrame` (including the index) is shown when calling\nthe `info()`. A configuration option, `display.memory_usage` (see the list of\noptions), specifies if the `DataFrame`\u2019s memory usage will be displayed when\ninvoking the `df.info()` method.\n\nFor example, the memory usage of the `DataFrame` below is shown when calling\n`info()`:\n\nThe `+` symbol indicates that the true memory usage could be higher, because\npandas does not count the memory used by values in columns with\n`dtype=object`.\n\nPassing `memory_usage='deep'` will enable a more accurate memory usage report,\naccounting for the full usage of the contained objects. This is optional as it\ncan be expensive to do this deeper introspection.\n\nBy default the display option is set to `True` but can be explicitly\noverridden by passing the `memory_usage` argument when invoking `df.info()`.\n\nThe memory usage of each column can be found by calling the `memory_usage()`\nmethod. This returns a `Series` with an index represented by column names and\nmemory usage of each column shown in bytes. For the `DataFrame` above, the\nmemory usage of each column and the total memory usage can be found with the\n`memory_usage` method:\n\nBy default the memory usage of the `DataFrame`\u2019s index is shown in the\nreturned `Series`, the memory usage of the index can be suppressed by passing\nthe `index=False` argument:\n\nThe memory usage displayed by the `info()` method utilizes the\n`memory_usage()` method to determine the memory usage of a `DataFrame` while\nalso formatting the output in human-readable units (base-2 representation;\ni.e. 1KB = 1024 bytes).\n\nSee also Categorical Memory Usage.\n\npandas follows the NumPy convention of raising an error when you try to\nconvert something to a `bool`. This happens in an `if`-statement or when using\nthe boolean operations: `and`, `or`, and `not`. It is not clear what the\nresult of the following code should be:\n\nShould it be `True` because it\u2019s not zero-length, or `False` because there are\n`False` values? It is unclear, so instead, pandas raises a `ValueError`:\n\nYou need to explicitly choose what you want to do with the `DataFrame`, e.g.\nuse `any()`, `all()` or `empty()`. Alternatively, you might want to compare if\nthe pandas object is `None`:\n\nBelow is how to check if any of the values are `True`:\n\nTo evaluate single-element pandas objects in a boolean context, use the method\n`bool()`:\n\nBitwise boolean operators like `==` and `!=` return a boolean `Series`, which\nis almost always what you want anyways.\n\nSee boolean comparisons for more examples.\n\nUsing the Python `in` operator on a `Series` tests for membership in the\nindex, not membership among the values.\n\nIf this behavior is surprising, keep in mind that using `in` on a Python\ndictionary tests keys, not values, and `Series` are dict-like. To test for\nmembership in the values, use the method `isin()`:\n\nFor `DataFrames`, likewise, `in` applies to the column axis, testing for\nmembership in the list of column names.\n\nThis section applies to pandas methods that take a UDF. In particular, the\nmethods `.apply`, `.aggregate`, `.transform`, and `.filter`.\n\nIt is a general rule in programming that one should not mutate a container\nwhile it is being iterated over. Mutation will invalidate the iterator,\ncausing unexpected behavior. Consider the example:\n\nOne probably would have expected that the result would be `[1, 3, 5]`. When\nusing a pandas method that takes a UDF, internally pandas is often iterating\nover the `DataFrame` or other pandas object. Therefore, if the UDF mutates\n(changes) the `DataFrame`, unexpected behavior can arise.\n\nHere is a similar example with `DataFrame.apply()`:\n\nTo resolve this issue, one can make a copy so that the mutation does not apply\nto the container being iterated over.\n\nFor lack of `NA` (missing) support from the ground up in NumPy and Python in\ngeneral, we were given the difficult choice between either:\n\nA masked array solution: an array of data and an array of boolean values\nindicating whether a value is there or is missing.\n\nUsing a special sentinel value, bit pattern, or set of sentinel values to\ndenote `NA` across the dtypes.\n\nFor many reasons we chose the latter. After years of production use it has\nproven, at least in my opinion, to be the best decision given the state of\naffairs in NumPy and Python in general. The special value `NaN` (Not-A-Number)\nis used everywhere as the `NA` value, and there are API functions `isna` and\n`notna` which can be used across the dtypes to detect NA values.\n\nHowever, it comes with it a couple of trade-offs which I most certainly have\nnot ignored.\n\nIn the absence of high performance `NA` support being built into NumPy from\nthe ground up, the primary casualty is the ability to represent NAs in integer\narrays. For example:\n\nThis trade-off is made largely for memory and performance reasons, and also so\nthat the resulting `Series` continues to be \u201cnumeric\u201d.\n\nIf you need to represent integers with possibly missing values, use one of the\nnullable-integer extension dtypes provided by pandas\n\n`Int8Dtype`\n\n`Int16Dtype`\n\n`Int32Dtype`\n\n`Int64Dtype`\n\nSee Nullable integer data type for more.\n\nWhen introducing NAs into an existing `Series` or `DataFrame` via `reindex()`\nor some other means, boolean and integer types will be promoted to a different\ndtype in order to store the NAs. The promotions are summarized in this table:\n\nTypeclass\n\nPromotion dtype for storing NAs\n\n`floating`\n\nno change\n\n`object`\n\nno change\n\n`integer`\n\ncast to `float64`\n\n`boolean`\n\ncast to `object`\n\nWhile this may seem like a heavy trade-off, I have found very few cases where\nthis is an issue in practice i.e. storing values greater than 2**53. Some\nexplanation for the motivation is in the next section.\n\nMany people have suggested that NumPy should simply emulate the `NA` support\npresent in the more domain-specific statistical programming language R. Part\nof the reason is the NumPy type hierarchy:\n\nTypeclass\n\nDtypes\n\n`numpy.floating`\n\n`float16, float32, float64, float128`\n\n`numpy.integer`\n\n`int8, int16, int32, int64`\n\n`numpy.unsignedinteger`\n\n`uint8, uint16, uint32, uint64`\n\n`numpy.object_`\n\n`object_`\n\n`numpy.bool_`\n\n`bool_`\n\n`numpy.character`\n\n`string_, unicode_`\n\nThe R language, by contrast, only has a handful of built-in data types:\n`integer`, `numeric` (floating-point), `character`, and `boolean`. `NA` types\nare implemented by reserving special bit patterns for each type to be used as\nthe missing value. While doing this with the full NumPy type hierarchy would\nbe possible, it would be a more substantial trade-off (especially for the 8-\nand 16-bit data types) and implementation undertaking.\n\nAn alternate approach is that of using masked arrays. A masked array is an\narray of data with an associated boolean mask denoting whether each value\nshould be considered `NA` or not. I am personally not in love with this\napproach as I feel that overall it places a fairly heavy burden on the user\nand the library implementer. Additionally, it exacts a fairly high performance\ncost when working with numerical data compared with the simple approach of\nusing `NaN`. Thus, I have chosen the Pythonic \u201cpracticality beats purity\u201d\napproach and traded integer `NA` capability for a much simpler approach of\nusing a special value in float and object arrays to denote `NA`, and promoting\ninteger arrays to floating when NAs must be introduced.\n\nFor `Series` and `DataFrame` objects, `var()` normalizes by `N-1` to produce\nunbiased estimates of the sample variance, while NumPy\u2019s `var` normalizes by\nN, which measures the variance of the sample. Note that `cov()` normalizes by\n`N-1` in both pandas and NumPy.\n\nAs of pandas 0.11, pandas is not 100% thread safe. The known issues relate to\nthe `copy()` method. If you are doing a lot of copying of `DataFrame` objects\nshared among threads, we recommend holding locks inside the threads where the\ndata copying occurs.\n\nSee this link for more information.\n\nOccasionally you may have to deal with data that were created on a machine\nwith a different byte order than the one on which you are running Python. A\ncommon symptom of this issue is an error like:\n\nTo deal with this issue you should convert the underlying NumPy array to the\nnative system byte order before passing it to `Series` or `DataFrame`\nconstructors using something similar to the following:\n\nSee the NumPy documentation on byte order for more details.\n\n"}, {"name": "General functions", "path": "reference/general_functions", "type": "Input/output", "text": "\n`melt`(frame[, id_vars, value_vars, var_name, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`pivot`(data[, index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`pivot_table`(data[, values, index, columns, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`crosstab`(index, columns[, values, rownames, ...])\n\nCompute a simple cross tabulation of two (or more) factors.\n\n`cut`(x, bins[, right, labels, retbins, ...])\n\nBin values into discrete intervals.\n\n`qcut`(x, q[, labels, retbins, precision, ...])\n\nQuantile-based discretization function.\n\n`merge`(left, right[, how, on, left_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`merge_ordered`(left, right[, on, left_on, ...])\n\nPerform a merge for ordered data with optional filling/interpolation.\n\n`merge_asof`(left, right[, on, left_on, ...])\n\nPerform a merge by key distance.\n\n`concat`(objs[, axis, join, ignore_index, ...])\n\nConcatenate pandas objects along a particular axis with optional set logic\nalong the other axes.\n\n`get_dummies`(data[, prefix, prefix_sep, ...])\n\nConvert categorical variable into dummy/indicator variables.\n\n`factorize`(values[, sort, na_sentinel, size_hint])\n\nEncode the object as an enumerated type or categorical variable.\n\n`unique`(values)\n\nReturn unique values based on a hash table.\n\n`wide_to_long`(df, stubnames, i, j[, sep, suffix])\n\nUnpivot a DataFrame from wide to long format.\n\n`isna`(obj)\n\nDetect missing values for an array-like object.\n\n`isnull`(obj)\n\nDetect missing values for an array-like object.\n\n`notna`(obj)\n\nDetect non-missing values for an array-like object.\n\n`notnull`(obj)\n\nDetect non-missing values for an array-like object.\n\n`to_numeric`(arg[, errors, downcast])\n\nConvert argument to a numeric type.\n\n`to_datetime`(arg[, errors, dayfirst, ...])\n\nConvert argument to datetime.\n\n`to_timedelta`(arg[, unit, errors])\n\nConvert argument to timedelta.\n\n`date_range`([start, end, periods, freq, tz, ...])\n\nReturn a fixed frequency DatetimeIndex.\n\n`bdate_range`([start, end, periods, freq, tz, ...])\n\nReturn a fixed frequency DatetimeIndex, with business day as the default\nfrequency.\n\n`period_range`([start, end, periods, freq, name])\n\nReturn a fixed frequency PeriodIndex.\n\n`timedelta_range`([start, end, periods, freq, ...])\n\nReturn a fixed frequency TimedeltaIndex, with day as the default frequency.\n\n`infer_freq`(index[, warn])\n\nInfer the most likely frequency given the input index.\n\n`interval_range`([start, end, periods, freq, ...])\n\nReturn a fixed frequency IntervalIndex.\n\n`eval`(expr[, parser, engine, truediv, ...])\n\nEvaluate a Python expression as a string using various backends.\n\n`util.hash_array`(vals[, encoding, hash_key, ...])\n\nGiven a 1d array, return an array of deterministic integers.\n\n`util.hash_pandas_object`(obj[, index, ...])\n\nReturn a data hash of the Index/Series/DataFrame.\n\n`test`([extra_args])\n\nRun the pandas test suite using pytest.\n\n"}, {"name": "General utility functions", "path": "reference/general_utility_functions", "type": "Input/output", "text": "\n`describe_option`(pat[, _print_desc])\n\nPrints the description for one or more registered options.\n\n`reset_option`(pat)\n\nReset one or more options to their default value.\n\n`get_option`(pat)\n\nRetrieves the value of the specified option.\n\n`set_option`(pat, value)\n\nSets the value of the specified option.\n\n`option_context`(*args)\n\nContext manager to temporarily set options in the with statement context.\n\n`testing.assert_frame_equal`(left, right[, ...])\n\nCheck that left and right DataFrame are equal.\n\n`testing.assert_series_equal`(left, right[, ...])\n\nCheck that left and right Series are equal.\n\n`testing.assert_index_equal`(left, right[, ...])\n\nCheck that left and right Index are equal.\n\n`testing.assert_extension_array_equal`(left, right)\n\nCheck that left and right ExtensionArrays are equal.\n\n`errors.AbstractMethodError`(class_instance[, ...])\n\nRaise this error instead of NotImplementedError for abstract methods while\nkeeping compatibility with Python 2 and Python 3.\n\n`errors.AccessorRegistrationWarning`\n\nWarning for attribute conflicts in accessor registration.\n\n`errors.DtypeWarning`\n\nWarning raised when reading different dtypes in a column from a file.\n\n`errors.DuplicateLabelError`\n\nError raised when an operation would introduce duplicate labels.\n\n`errors.EmptyDataError`\n\nException that is thrown in pd.read_csv (by both the C and Python engines)\nwhen empty data or header is encountered.\n\n`errors.InvalidIndexError`\n\nException raised when attempting to use an invalid index key.\n\n`errors.IntCastingNaNError`\n\nRaised when attempting an astype operation on an array with NaN to an integer\ndtype.\n\n`errors.MergeError`\n\nError raised when problems arise during merging due to problems with input\ndata.\n\n`errors.NullFrequencyError`\n\nError raised when a null freq attribute is used in an operation that needs a\nnon-null frequency, particularly DatetimeIndex.shift, TimedeltaIndex.shift,\nPeriodIndex.shift.\n\n`errors.NumbaUtilError`\n\nError raised for unsupported Numba engine routines.\n\n`errors.OptionError`\n\nException for pandas.options, backwards compatible with KeyError checks.\n\n`errors.OutOfBoundsDatetime`\n\n`errors.OutOfBoundsTimedelta`\n\nRaised when encountering a timedelta value that cannot be represented as a\ntimedelta64[ns].\n\n`errors.ParserError`\n\nException that is raised by an error encountered in parsing file contents.\n\n`errors.ParserWarning`\n\nWarning raised when reading a file that doesn't use the default 'c' parser.\n\n`errors.PerformanceWarning`\n\nWarning raised when there is a possible performance impact.\n\n`errors.UnsortedIndexError`\n\nError raised when attempting to get a slice of a MultiIndex, and the index has\nnot been lexsorted.\n\n`errors.UnsupportedFunctionCall`\n\nException raised when attempting to call a numpy function on a pandas object,\nbut that function is not supported by the object e.g.\n\n`api.types.union_categoricals`(to_union[, ...])\n\nCombine list-like of Categorical-like, unioning categories.\n\n`api.types.infer_dtype`\n\nEfficiently infer the type of a passed val, or list-like array of values.\n\n`api.types.pandas_dtype`(dtype)\n\nConvert input into a pandas only dtype object or a numpy dtype object.\n\n`api.types.is_bool_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a boolean dtype.\n\n`api.types.is_categorical_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Categorical dtype.\n\n`api.types.is_complex_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a complex dtype.\n\n`api.types.is_datetime64_any_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the datetime64 dtype.\n\n`api.types.is_datetime64_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the datetime64 dtype.\n\n`api.types.is_datetime64_ns_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the datetime64[ns] dtype.\n\n`api.types.is_datetime64tz_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\n`api.types.is_extension_type`(arr)\n\n(DEPRECATED) Check whether an array-like is of a pandas extension class\ninstance.\n\n`api.types.is_extension_array_dtype`(arr_or_dtype)\n\nCheck if an object is a pandas extension array type.\n\n`api.types.is_float_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a float dtype.\n\n`api.types.is_int64_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the int64 dtype.\n\n`api.types.is_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of an integer dtype.\n\n`api.types.is_interval_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Interval dtype.\n\n`api.types.is_numeric_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a numeric dtype.\n\n`api.types.is_object_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the object dtype.\n\n`api.types.is_period_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the Period dtype.\n\n`api.types.is_signed_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of a signed integer dtype.\n\n`api.types.is_string_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the string dtype.\n\n`api.types.is_timedelta64_dtype`(arr_or_dtype)\n\nCheck whether an array-like or dtype is of the timedelta64 dtype.\n\n`api.types.is_timedelta64_ns_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of the timedelta64[ns] dtype.\n\n`api.types.is_unsigned_integer_dtype`(arr_or_dtype)\n\nCheck whether the provided array or dtype is of an unsigned integer dtype.\n\n`api.types.is_sparse`(arr)\n\nCheck whether an array-like is a 1-D pandas sparse array.\n\n`api.types.is_dict_like`(obj)\n\nCheck if the object is dict-like.\n\n`api.types.is_file_like`(obj)\n\nCheck if the object is a file-like object.\n\n`api.types.is_list_like`\n\nCheck if the object is list-like.\n\n`api.types.is_named_tuple`(obj)\n\nCheck if the object is a named tuple.\n\n`api.types.is_iterator`\n\nCheck if the object is an iterator.\n\n`api.types.is_bool`\n\nReturn True if given object is boolean.\n\n`api.types.is_categorical`(arr)\n\nCheck whether an array-like is a Categorical instance.\n\n`api.types.is_complex`\n\nReturn True if given object is complex.\n\n`api.types.is_float`\n\nReturn True if given object is float.\n\n`api.types.is_hashable`(obj)\n\nReturn True if hash(obj) will succeed, False otherwise.\n\n`api.types.is_integer`\n\nReturn True if given object is integer.\n\n`api.types.is_interval`\n\n`api.types.is_number`(obj)\n\nCheck if the object is a number.\n\n`api.types.is_re`(obj)\n\nCheck if the object is a regex pattern instance.\n\n`api.types.is_re_compilable`(obj)\n\nCheck if the object can be compiled into a regex pattern instance.\n\n`api.types.is_scalar`\n\nReturn True if given object is scalar.\n\n`show_versions`([as_json])\n\nProvide useful information, important for bug reports.\n\n"}, {"name": "Group by: split-apply-combine", "path": "user_guide/groupby", "type": "Manual", "text": "\nBy \u201cgroup by\u201d we are referring to a process involving one or more of the\nfollowing steps:\n\nSplitting the data into groups based on some criteria.\n\nApplying a function to each group independently.\n\nCombining the results into a data structure.\n\nOut of these, the split step is the most straightforward. In fact, in many\nsituations we may wish to split the data set into groups and do something with\nthose groups. In the apply step, we might wish to do one of the following:\n\nAggregation: compute a summary statistic (or statistics) for each group. Some\nexamples:\n\nCompute group sums or means.\n\nCompute group sizes / counts.\n\nTransformation: perform some group-specific computations and return a like-\nindexed object. Some examples:\n\nStandardize data (zscore) within a group.\n\nFilling NAs within groups with a value derived from each group.\n\nFiltration: discard some groups, according to a group-wise computation that\nevaluates True or False. Some examples:\n\nDiscard data that belongs to groups with only a few members.\n\nFilter out data based on the group sum or mean.\n\nSome combination of the above: GroupBy will examine the results of the apply\nstep and try to return a sensibly combined result if it doesn\u2019t fit into\neither of the above two categories.\n\nSince the set of object instance methods on pandas data structures are\ngenerally rich and expressive, we often simply want to invoke, say, a\nDataFrame function on each group. The name GroupBy should be quite familiar to\nthose who have used a SQL-based tool (or `itertools`), in which you can write\ncode like:\n\nWe aim to make operations like this natural and easy to express using pandas.\nWe\u2019ll address each area of GroupBy functionality then provide some non-trivial\nexamples / use cases.\n\nSee the cookbook for some advanced strategies.\n\npandas objects can be split on any of their axes. The abstract definition of\ngrouping is to provide a mapping of labels to group names. To create a GroupBy\nobject (more on what the GroupBy object is later), you may do the following:\n\nThe mapping can be specified many different ways:\n\nA Python function, to be called on each of the axis labels.\n\nA list or NumPy array of the same length as the selected axis.\n\nA dict or `Series`, providing a `label -> group name` mapping.\n\nFor `DataFrame` objects, a string indicating either a column name or an index\nlevel name to be used to group.\n\n`df.groupby('A')` is just syntactic sugar for `df.groupby(df['A'])`.\n\nA list of any of the above things.\n\nCollectively we refer to the grouping objects as the keys. For example,\nconsider the following `DataFrame`:\n\nNote\n\nA string passed to `groupby` may refer to either a column or an index level.\nIf a string matches both a column name and an index level name, a `ValueError`\nwill be raised.\n\nOn a DataFrame, we obtain a GroupBy object by calling `groupby()`. We could\nnaturally group by either the `A` or `B` columns, or both:\n\nIf we also have a MultiIndex on columns `A` and `B`, we can group by all but\nthe specified columns\n\nThese will split the DataFrame on its index (rows). We could also split by the\ncolumns:\n\npandas `Index` objects support duplicate values. If a non-unique index is used\nas the group key in a groupby operation, all values for the same index value\nwill be considered to be in one group and thus the output of aggregation\nfunctions will only contain unique index values:\n\nNote that no splitting occurs until it\u2019s needed. Creating the GroupBy object\nonly verifies that you\u2019ve passed a valid mapping.\n\nNote\n\nMany kinds of complicated data manipulations can be expressed in terms of\nGroupBy operations (though can\u2019t be guaranteed to be the most efficient). You\ncan get quite creative with the label mapping functions.\n\nBy default the group keys are sorted during the `groupby` operation. You may\nhowever pass `sort=False` for potential speedups:\n\nNote that `groupby` will preserve the order in which observations are sorted\nwithin each group. For example, the groups created by `groupby()` below are in\nthe order they appeared in the original `DataFrame`:\n\nNew in version 1.1.0.\n\nBy default `NA` values are excluded from group keys during the `groupby`\noperation. However, in case you want to include `NA` values in group keys, you\ncould pass `dropna=False` to achieve it.\n\nThe default setting of `dropna` argument is `True` which means `NA` are not\nincluded in group keys.\n\nThe `groups` attribute is a dict whose keys are the computed unique groups and\ncorresponding values being the axis labels belonging to each group. In the\nabove example we have:\n\nCalling the standard Python `len` function on the GroupBy object just returns\nthe length of the `groups` dict, so it is largely just a convenience:\n\n`GroupBy` will tab complete column names (and other attributes):\n\nWith hierarchically-indexed data, it\u2019s quite natural to group by one of the\nlevels of the hierarchy.\n\nLet\u2019s create a Series with a two-level `MultiIndex`.\n\nWe can then group by one of the levels in `s`.\n\nIf the MultiIndex has names specified, these can be passed instead of the\nlevel number:\n\nGrouping with multiple levels is supported.\n\nIndex level names may be supplied as keys.\n\nMore on the `sum` function and aggregation later.\n\nA DataFrame may be grouped by a combination of columns and index levels by\nspecifying the column names as strings and the index levels as `pd.Grouper`\nobjects.\n\nThe following example groups `df` by the `second` index level and the `A`\ncolumn.\n\nIndex levels may also be specified by name.\n\nIndex level names may be specified as keys directly to `groupby`.\n\nOnce you have created the GroupBy object from a DataFrame, you might want to\ndo something different for each of the columns. Thus, using `[]` similar to\ngetting a column from a DataFrame, you can do:\n\nThis is mainly syntactic sugar for the alternative and much more verbose:\n\nAdditionally this method avoids recomputing the internal grouping information\nderived from the passed key.\n\nWith the GroupBy object in hand, iterating through the grouped data is very\nnatural and functions similarly to `itertools.groupby()`:\n\nIn the case of grouping by multiple keys, the group name will be a tuple:\n\nSee Iterating through groups.\n\nA single group can be selected using `get_group()`:\n\nOr for an object grouped on multiple columns:\n\nOnce the GroupBy object has been created, several methods are available to\nperform a computation on the grouped data. These operations are similar to the\naggregating API, window API, and resample API.\n\nAn obvious one is aggregation via the `aggregate()` or equivalently `agg()`\nmethod:\n\nAs you can see, the result of the aggregation will have the group names as the\nnew index along the grouped axis. In the case of multiple keys, the result is\na MultiIndex by default, though this can be changed by using the `as_index`\noption:\n\nNote that you could use the `reset_index` DataFrame function to achieve the\nsame result as the column names are stored in the resulting `MultiIndex`:\n\nAnother simple aggregation example is to compute the size of each group. This\nis included in GroupBy as the `size` method. It returns a Series whose index\nare the group names and whose values are the sizes of each group.\n\nAnother aggregation example is to compute the number of unique values of each\ngroup. This is similar to the `value_counts` function, except that it only\ncounts unique values.\n\nNote\n\nAggregation functions will not return the groups that you are aggregating over\nif they are named columns, when `as_index=True`, the default. The grouped\ncolumns will be the indices of the returned object.\n\nPassing `as_index=False` will return the groups that you are aggregating over,\nif they are named columns.\n\nAggregating functions are the ones that reduce the dimension of the returned\nobjects. Some common aggregating functions are tabulated below:\n\nFunction\n\nDescription\n\n`mean()`\n\nCompute mean of groups\n\n`sum()`\n\nCompute sum of group values\n\n`size()`\n\nCompute group sizes\n\n`count()`\n\nCompute count of group\n\n`std()`\n\nStandard deviation of groups\n\n`var()`\n\nCompute variance of groups\n\n`sem()`\n\nStandard error of the mean of groups\n\n`describe()`\n\nGenerates descriptive statistics\n\n`first()`\n\nCompute first of group values\n\n`last()`\n\nCompute last of group values\n\n`nth()`\n\nTake nth value, or a subset if n is a list\n\n`min()`\n\nCompute min of group values\n\n`max()`\n\nCompute max of group values\n\nThe aggregating functions above will exclude NA values. Any function which\nreduces a `Series` to a scalar value is an aggregation function and will work,\na trivial example is `df.groupby('A').agg(lambda ser: 1)`. Note that `nth()`\ncan act as a reducer or a filter, see here.\n\nWith grouped `Series` you can also pass a list or dict of functions to do\naggregation with, outputting a DataFrame:\n\nOn a grouped `DataFrame`, you can pass a list of functions to apply to each\ncolumn, which produces an aggregated result with a hierarchical index:\n\nThe resulting aggregations are named for the functions themselves. If you need\nto rename, then you can add in a chained operation for a `Series` like this:\n\nFor a grouped `DataFrame`, you can rename in a similar manner:\n\nNote\n\nIn general, the output column names should be unique. You can\u2019t apply the same\nfunction (or two functions with the same name) to the same column.\n\npandas does allow you to provide multiple lambdas. In this case, pandas will\nmangle the name of the (nameless) lambda functions, appending `_<i>` to each\nsubsequent lambda.\n\nNew in version 0.25.0.\n\nTo support column-specific aggregation with control over the output column\nnames, pandas accepts the special syntax in `GroupBy.agg()`, known as \u201cnamed\naggregation\u201d, where\n\nThe keywords are the output column names\n\nThe values are tuples whose first element is the column to select and the\nsecond element is the aggregation to apply to that column. pandas provides the\n`pandas.NamedAgg` namedtuple with the fields `['column', 'aggfunc']` to make\nit clearer what the arguments are. As usual, the aggregation can be a callable\nor a string alias.\n\n`pandas.NamedAgg` is just a `namedtuple`. Plain tuples are allowed as well.\n\nIf your desired output column names are not valid Python keywords, construct a\ndictionary and unpack the keyword arguments\n\nAdditional keyword arguments are not passed through to the aggregation\nfunctions. Only pairs of `(column, aggfunc)` should be passed as `**kwargs`.\nIf your aggregation functions requires additional arguments, partially apply\nthem with `functools.partial()`.\n\nNote\n\nFor Python 3.5 and earlier, the order of `**kwargs` in a functions was not\npreserved. This means that the output column ordering would not be consistent.\nTo ensure consistent ordering, the keys (and so output columns) will always be\nsorted for Python 3.5.\n\nNamed aggregation is also valid for Series groupby aggregations. In this case\nthere\u2019s no column selection, so the values are just the functions.\n\nBy passing a dict to `aggregate` you can apply a different aggregation to the\ncolumns of a DataFrame:\n\nThe function names can also be strings. In order for a string to be valid it\nmust be either implemented on GroupBy or available via dispatching:\n\nSome common aggregations, currently only `sum`, `mean`, `std`, and `sem`, have\noptimized Cython implementations:\n\nOf course `sum` and `mean` are implemented on pandas objects, so the above\ncode would work even without the special versions via dispatching (see below).\n\nUsers can also provide their own functions for custom aggregations. When\naggregating with a User-Defined Function (UDF), the UDF should not mutate the\nprovided `Series`, see Mutating with User Defined Function (UDF) methods for\nmore information.\n\nThe resulting dtype will reflect that of the aggregating function. If the\nresults from different groups have different dtypes, then a common dtype will\nbe determined in the same way as `DataFrame` construction.\n\nThe `transform` method returns an object that is indexed the same (same size)\nas the one being grouped. The transform function must:\n\nReturn a result that is either the same size as the group chunk or\nbroadcastable to the size of the group chunk (e.g., a scalar,\n`grouped.transform(lambda x: x.iloc[-1])`).\n\nOperate column-by-column on the group chunk. The transform is applied to the\nfirst group chunk using chunk.apply.\n\nNot perform in-place operations on the group chunk. Group chunks should be\ntreated as immutable, and changes to a group chunk may produce unexpected\nresults. For example, when using `fillna`, `inplace` must be `False`\n(`grouped.transform(lambda x: x.fillna(inplace=False))`).\n\n(Optionally) operates on the entire group chunk. If this is supported, a fast\npath is used starting from the second chunk.\n\nSimilar to Aggregations with User-Defined Functions, the resulting dtype will\nreflect that of the transformation function. If the results from different\ngroups have different dtypes, then a common dtype will be determined in the\nsame way as `DataFrame` construction.\n\nSuppose we wished to standardize the data within each group:\n\nWe would expect the result to now have mean 0 and standard deviation 1 within\neach group, which we can easily check:\n\nWe can also visually compare the original and transformed data sets.\n\nTransformation functions that have lower dimension outputs are broadcast to\nmatch the shape of the input array.\n\nAlternatively, the built-in methods could be used to produce the same outputs.\n\nAnother common data transform is to replace missing data with the group mean.\n\nWe can verify that the group means have not changed in the transformed data\nand that the transformed data contains no NAs.\n\nNote\n\nSome functions will automatically transform the input when applied to a\nGroupBy object, but returning an object of the same shape as the original.\nPassing `as_index=False` will not affect these transformation methods.\n\nFor example: `fillna, ffill, bfill, shift.`.\n\nIt is possible to use `resample()`, `expanding()` and `rolling()` as methods\non groupbys.\n\nThe example below will apply the `rolling()` method on the samples of the\ncolumn B based on the groups of column A.\n\nThe `expanding()` method will accumulate a given operation (`sum()` in the\nexample) for all the members of each particular group.\n\nSuppose you want to use the `resample()` method to get a daily frequency in\neach group of your dataframe and wish to complete the missing values with the\n`ffill()` method.\n\nThe `filter` method returns a subset of the original object. Suppose we want\nto take only elements that belong to groups with a group sum greater than 2.\n\nThe argument of `filter` must be a function that, applied to the group as a\nwhole, returns `True` or `False`.\n\nAnother useful operation is filtering out elements that belong to groups with\nonly a couple members.\n\nAlternatively, instead of dropping the offending groups, we can return a like-\nindexed objects where the groups that do not pass the filter are filled with\nNaNs.\n\nFor DataFrames with multiple columns, filters should explicitly specify a\ncolumn as the filter criterion.\n\nNote\n\nSome functions when applied to a groupby object will act as a filter on the\ninput, returning a reduced shape of the original (and potentially eliminating\ngroups), but with the index unchanged. Passing `as_index=False` will not\naffect these transformation methods.\n\nFor example: `head, tail`.\n\nWhen doing an aggregation or transformation, you might just want to call an\ninstance method on each data group. This is pretty easy to do by passing\nlambda functions:\n\nBut, it\u2019s rather verbose and can be untidy if you need to pass additional\narguments. Using a bit of metaprogramming cleverness, GroupBy now has the\nability to \u201cdispatch\u201d method calls to the groups:\n\nWhat is actually happening here is that a function wrapper is being generated.\nWhen invoked, it takes any passed arguments and invokes the function with any\narguments on each group (in the above example, the `std` function). The\nresults are then combined together much in the style of `agg` and `transform`\n(it actually uses `apply` to infer the gluing, documented next). This enables\nsome operations to be carried out rather succinctly:\n\nIn this example, we chopped the collection of time series into yearly chunks\nthen independently called fillna on the groups.\n\nThe `nlargest` and `nsmallest` methods work on `Series` style groupbys:\n\nSome operations on the grouped data might not fit into either the aggregate or\ntransform categories. Or, you may simply want GroupBy to infer how to combine\nthe results. For these, use the `apply` function, which can be substituted for\nboth `aggregate` and `transform` in many standard use cases. However, `apply`\ncan handle some exceptional use cases, for example:\n\nThe dimension of the returned result can also change:\n\n`apply` on a Series can operate on a returned value from the applied function,\nthat is itself a series, and possibly upcast the result to a DataFrame:\n\nNote\n\n`apply` can act as a reducer, transformer, or filter function, depending on\nexactly what is passed to it. So depending on the path taken, and exactly what\nyou are grouping. Thus the grouped columns(s) may be included in the output as\nwell as set the indices.\n\nSimilar to Aggregations with User-Defined Functions, the resulting dtype will\nreflect that of the apply function. If the results from different groups have\ndifferent dtypes, then a common dtype will be determined in the same way as\n`DataFrame` construction.\n\nNew in version 1.1.\n\nIf Numba is installed as an optional dependency, the `transform` and\n`aggregate` methods support `engine='numba'` and `engine_kwargs` arguments.\nSee enhancing performance with Numba for general usage of the arguments and\nperformance considerations.\n\nThe function signature must start with `values, index` exactly as the data\nbelonging to each group will be passed into `values`, and the group index will\nbe passed into `index`.\n\nWarning\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as NumPy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nAgain consider the example DataFrame we\u2019ve been looking at:\n\nSuppose we wish to compute the standard deviation grouped by the `A` column.\nThere is a slight problem, namely that we don\u2019t care about the data in column\n`B`. We refer to this as a \u201cnuisance\u201d column. If the passed aggregation\nfunction can\u2019t be applied to some columns, the troublesome columns will be\n(silently) dropped. Thus, this does not pose any problems:\n\nNote that `df.groupby('A').colname.std().` is more efficient than\n`df.groupby('A').std().colname`, so if the result of an aggregation function\nis only interesting over one column (here `colname`), it may be filtered\nbefore applying the aggregation function.\n\nNote\n\nAny object column, also if it contains numerical values such as `Decimal`\nobjects, is considered as a \u201cnuisance\u201d columns. They are excluded from\naggregate functions automatically in groupby.\n\nIf you do wish to include decimal or object columns in an aggregation with\nother non-nuisance data types, you must do so explicitly.\n\nWhen using a `Categorical` grouper (as a single grouper, or as part of\nmultiple groupers), the `observed` keyword controls whether to return a\ncartesian product of all possible groupers values (`observed=False`) or only\nthose that are observed groupers (`observed=True`).\n\nShow all values:\n\nShow only the observed values:\n\nThe returned dtype of the grouped will always include all of the categories\nthat were grouped.\n\nIf there are any NaN or NaT values in the grouping key, these will be\nautomatically excluded. In other words, there will never be an \u201cNA group\u201d or\n\u201cNaT group\u201d. This was not the case in older versions of pandas, but users were\ngenerally discarding the NA group anyway (and supporting it was an\nimplementation headache).\n\nCategorical variables represented as instance of pandas\u2019s `Categorical` class\ncan be used as group keys. If so, the order of the levels will be preserved:\n\nYou may need to specify a bit more data to properly group. You can use the\n`pd.Grouper` to provide this local control.\n\nGroupby a specific column with the desired frequency. This is like resampling.\n\nYou have an ambiguous specification in that you have a named index and a\ncolumn that could be potential groupers.\n\nJust like for a DataFrame or Series you can call head and tail on a groupby:\n\nThis shows the first or last n rows from each group.\n\nTo select from a DataFrame or Series the nth item, use `nth()`. This is a\nreduction method, and will return a single row (or no row) per group if you\npass an int for n:\n\nIf you want to select the nth not-null item, use the `dropna` kwarg. For a\nDataFrame this should be either `'any'` or `'all'` just like you would pass to\ndropna:\n\nAs with other methods, passing `as_index=False`, will achieve a filtration,\nwhich returns the grouped row.\n\nYou can also select multiple rows from each group by specifying multiple nth\nvalues as a list of ints.\n\nTo see the order in which each row appears within its group, use the\n`cumcount` method:\n\nTo see the ordering of the groups (as opposed to the order of rows within a\ngroup given by `cumcount`) you can use `ngroup()`.\n\nNote that the numbers given to the groups match the order in which the groups\nwould be seen when iterating over the groupby object, not the order they are\nfirst observed.\n\nGroupby also works with some plotting methods. For example, suppose we suspect\nthat some features in a DataFrame may differ by group, in this case, the\nvalues in column 1 where the group is \u201cB\u201d are 3 higher on average.\n\nWe can easily visualize this with a boxplot:\n\nThe result of calling `boxplot` is a dictionary whose keys are the values of\nour grouping column `g` (\u201cA\u201d and \u201cB\u201d). The values of the resulting dictionary\ncan be controlled by the `return_type` keyword of `boxplot`. See the\nvisualization documentation for more.\n\nWarning\n\nFor historical reasons, `df.groupby(\"g\").boxplot()` is not equivalent to\n`df.boxplot(by=\"g\")`. See here for an explanation.\n\nSimilar to the functionality provided by `DataFrame` and `Series`, functions\nthat take `GroupBy` objects can be chained together using a `pipe` method to\nallow for a cleaner, more readable syntax. To read about `.pipe` in general\nterms, see here.\n\nCombining `.groupby` and `.pipe` is often useful when you need to reuse\nGroupBy objects.\n\nAs an example, imagine having a DataFrame with columns for stores, products,\nrevenue and quantity sold. We\u2019d like to do a groupwise calculation of prices\n(i.e. revenue/quantity) per store and per product. We could do this in a\nmulti-step operation, but expressing it in terms of piping can make the code\nmore readable. First we set the data:\n\nNow, to find prices per store/product, we can simply do:\n\nPiping can also be expressive when you want to deliver a grouped object to\nsome arbitrary function, for example:\n\nwhere `mean` takes a GroupBy object and finds the mean of the Revenue and\nQuantity columns respectively for each Store-Product combination. The `mean`\nfunction can be any function that takes in a GroupBy object; the `.pipe` will\npass the GroupBy object as a parameter into the function you specify.\n\nRegroup columns of a DataFrame according to their sum, and sum the aggregated\nones.\n\nBy using `ngroup()`, we can extract information about the groups in a way\nsimilar to `factorize()` (as described further in the reshaping API) but which\napplies naturally to multiple columns of mixed type and different sources.\nThis can be useful as an intermediate categorical-like step in processing,\nwhen the relationships between the group rows are more important than their\ncontent, or as input to an algorithm which only accepts the integer encoding.\n(For more information about support in pandas for full categorical data, see\nthe Categorical introduction and the API documentation.)\n\nResampling produces new hypothetical samples (resamples) from already existing\nobserved data or from a model that generates data. These new samples are\nsimilar to the pre-existing samples.\n\nIn order to resample to work on indices that are non-datetimelike, the\nfollowing procedure can be utilized.\n\nIn the following examples, df.index // 5 returns a binary array which is used\nto determine what gets selected for the groupby operation.\n\nNote\n\nThe below example shows how we can downsample by consolidation of samples into\nfewer samples. Here by using df.index // 5, we are aggregating the samples in\nbins. By applying std() function, we aggregate the information contained in\nmany samples into a small subset of values which is their standard deviation\nthereby reducing the number of samples.\n\nGroup DataFrame columns, compute a set of metrics and return a named Series.\nThe Series name is used as the name for the column index. This is especially\nuseful in conjunction with reshaping operations such as stacking in which the\ncolumn index name will be used as the name of the inserted column:\n\n"}, {"name": "GroupBy", "path": "reference/groupby", "type": "GroupBy", "text": "\nGroupBy objects are returned by groupby calls: `pandas.DataFrame.groupby()`,\n`pandas.Series.groupby()`, etc.\n\n`GroupBy.__iter__`()\n\nGroupby iterator.\n\n`GroupBy.groups`\n\nDict {group name -> group labels}.\n\n`GroupBy.indices`\n\nDict {group name -> group indices}.\n\n`GroupBy.get_group`(name[, obj])\n\nConstruct DataFrame from group with provided name.\n\n`Grouper`(*args, **kwargs)\n\nA Grouper allows the user to specify a groupby instruction for an object.\n\n`GroupBy.apply`(func, *args, **kwargs)\n\nApply function `func` group-wise and combine the results together.\n\n`GroupBy.agg`(func, *args, **kwargs)\n\n`SeriesGroupBy.aggregate`([func, engine, ...])\n\nAggregate using one or more operations over the specified axis.\n\n`DataFrameGroupBy.aggregate`([func, engine, ...])\n\nAggregate using one or more operations over the specified axis.\n\n`SeriesGroupBy.transform`(func, *args[, ...])\n\nCall function producing a like-indexed Series on each group and return a\nSeries having the same indexes as the original object filled with the\ntransformed values.\n\n`DataFrameGroupBy.transform`(func, *args[, ...])\n\nCall function producing a like-indexed DataFrame on each group and return a\nDataFrame having the same indexes as the original object filled with the\ntransformed values.\n\n`GroupBy.pipe`(func, *args, **kwargs)\n\nApply a function func with arguments to this GroupBy object and return the\nfunction's result.\n\n`GroupBy.all`([skipna])\n\nReturn True if all values in the group are truthful, else False.\n\n`GroupBy.any`([skipna])\n\nReturn True if any value in the group is truthful, else False.\n\n`GroupBy.bfill`([limit])\n\nBackward fill the values.\n\n`GroupBy.backfill`([limit])\n\nBackward fill the values.\n\n`GroupBy.count`()\n\nCompute count of group, excluding missing values.\n\n`GroupBy.cumcount`([ascending])\n\nNumber each item in each group from 0 to the length of that group - 1.\n\n`GroupBy.cummax`([axis])\n\nCumulative max for each group.\n\n`GroupBy.cummin`([axis])\n\nCumulative min for each group.\n\n`GroupBy.cumprod`([axis])\n\nCumulative product for each group.\n\n`GroupBy.cumsum`([axis])\n\nCumulative sum for each group.\n\n`GroupBy.ffill`([limit])\n\nForward fill the values.\n\n`GroupBy.first`([numeric_only, min_count])\n\nCompute first of group values.\n\n`GroupBy.head`([n])\n\nReturn first n rows of each group.\n\n`GroupBy.last`([numeric_only, min_count])\n\nCompute last of group values.\n\n`GroupBy.max`([numeric_only, min_count])\n\nCompute max of group values.\n\n`GroupBy.mean`([numeric_only, engine, ...])\n\nCompute mean of groups, excluding missing values.\n\n`GroupBy.median`([numeric_only])\n\nCompute median of groups, excluding missing values.\n\n`GroupBy.min`([numeric_only, min_count])\n\nCompute min of group values.\n\n`GroupBy.ngroup`([ascending])\n\nNumber each group from 0 to the number of groups - 1.\n\n`GroupBy.nth`(n[, dropna])\n\nTake the nth row from each group if n is an int, otherwise a subset of rows.\n\n`GroupBy.ohlc`()\n\nCompute open, high, low and close values of a group, excluding missing values.\n\n`GroupBy.pad`([limit])\n\nForward fill the values.\n\n`GroupBy.prod`([numeric_only, min_count])\n\nCompute prod of group values.\n\n`GroupBy.rank`([method, ascending, na_option, ...])\n\nProvide the rank of values within each group.\n\n`GroupBy.pct_change`([periods, fill_method, ...])\n\nCalculate pct_change of each value to previous entry in group.\n\n`GroupBy.size`()\n\nCompute group sizes.\n\n`GroupBy.sem`([ddof])\n\nCompute standard error of the mean of groups, excluding missing values.\n\n`GroupBy.std`([ddof, engine, engine_kwargs])\n\nCompute standard deviation of groups, excluding missing values.\n\n`GroupBy.sum`([numeric_only, min_count, ...])\n\nCompute sum of group values.\n\n`GroupBy.var`([ddof, engine, engine_kwargs])\n\nCompute variance of groups, excluding missing values.\n\n`GroupBy.tail`([n])\n\nReturn last n rows of each group.\n\nThe following methods are available in both `SeriesGroupBy` and\n`DataFrameGroupBy` objects, but may differ slightly, usually in that the\n`DataFrameGroupBy` version usually permits the specification of an axis\nargument, and often an argument indicating whether to restrict application to\ncolumns of a specific data type.\n\n`DataFrameGroupBy.all`([skipna])\n\nReturn True if all values in the group are truthful, else False.\n\n`DataFrameGroupBy.any`([skipna])\n\nReturn True if any value in the group is truthful, else False.\n\n`DataFrameGroupBy.backfill`([limit])\n\nBackward fill the values.\n\n`DataFrameGroupBy.bfill`([limit])\n\nBackward fill the values.\n\n`DataFrameGroupBy.corr`\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`DataFrameGroupBy.count`()\n\nCompute count of group, excluding missing values.\n\n`DataFrameGroupBy.cov`\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`DataFrameGroupBy.cumcount`([ascending])\n\nNumber each item in each group from 0 to the length of that group - 1.\n\n`DataFrameGroupBy.cummax`([axis])\n\nCumulative max for each group.\n\n`DataFrameGroupBy.cummin`([axis])\n\nCumulative min for each group.\n\n`DataFrameGroupBy.cumprod`([axis])\n\nCumulative product for each group.\n\n`DataFrameGroupBy.cumsum`([axis])\n\nCumulative sum for each group.\n\n`DataFrameGroupBy.describe`(**kwargs)\n\nGenerate descriptive statistics.\n\n`DataFrameGroupBy.diff`\n\nFirst discrete difference of element.\n\n`DataFrameGroupBy.ffill`([limit])\n\nForward fill the values.\n\n`DataFrameGroupBy.fillna`\n\nFill NA/NaN values using the specified method.\n\n`DataFrameGroupBy.filter`(func[, dropna])\n\nReturn a copy of a DataFrame excluding filtered elements.\n\n`DataFrameGroupBy.hist`\n\nMake a histogram of the DataFrame's columns.\n\n`DataFrameGroupBy.idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`DataFrameGroupBy.idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`DataFrameGroupBy.mad`\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`DataFrameGroupBy.nunique`([dropna])\n\nReturn DataFrame with counts of unique elements in each position.\n\n`DataFrameGroupBy.pad`([limit])\n\nForward fill the values.\n\n`DataFrameGroupBy.pct_change`([periods, ...])\n\nCalculate pct_change of each value to previous entry in group.\n\n`DataFrameGroupBy.plot`\n\nClass implementing the .plot attribute for groupby objects.\n\n`DataFrameGroupBy.quantile`([q, interpolation])\n\nReturn group values at the given quantile, a la numpy.percentile.\n\n`DataFrameGroupBy.rank`([method, ascending, ...])\n\nProvide the rank of values within each group.\n\n`DataFrameGroupBy.resample`(rule, *args, **kwargs)\n\nProvide resampling when using a TimeGrouper.\n\n`DataFrameGroupBy.sample`([n, frac, replace, ...])\n\nReturn a random sample of items from each group.\n\n`DataFrameGroupBy.shift`([periods, freq, ...])\n\nShift each group by periods observations.\n\n`DataFrameGroupBy.size`()\n\nCompute group sizes.\n\n`DataFrameGroupBy.skew`\n\nReturn unbiased skew over requested axis.\n\n`DataFrameGroupBy.take`\n\nReturn the elements in the given positional indices along an axis.\n\n`DataFrameGroupBy.tshift`\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`DataFrameGroupBy.value_counts`([subset, ...])\n\nReturn a Series or DataFrame containing counts of unique rows.\n\nThe following methods are available only for `SeriesGroupBy` objects.\n\n`SeriesGroupBy.hist`\n\nDraw histogram of the input series using matplotlib.\n\n`SeriesGroupBy.nlargest`([n, keep])\n\nReturn the largest n elements.\n\n`SeriesGroupBy.nsmallest`([n, keep])\n\nReturn the smallest n elements.\n\n`SeriesGroupBy.nunique`([dropna])\n\nReturn number of unique elements in the group.\n\n`SeriesGroupBy.unique`\n\nReturn unique values of Series object.\n\n`SeriesGroupBy.value_counts`([normalize, ...])\n\n`SeriesGroupBy.is_monotonic_increasing`\n\nAlias for is_monotonic.\n\n`SeriesGroupBy.is_monotonic_decreasing`\n\nReturn boolean if values in the object are monotonic_decreasing.\n\nThe following methods are available only for `DataFrameGroupBy` objects.\n\n`DataFrameGroupBy.corrwith`\n\nCompute pairwise correlation.\n\n`DataFrameGroupBy.boxplot`([subplots, column, ...])\n\nMake box plots from DataFrameGroupBy data.\n\n"}, {"name": "Index objects", "path": "reference/indexing", "type": "Index Objects", "text": "\nMany of these methods or variants thereof are available on the objects that\ncontain an index (Series/DataFrame) and those should most likely be used\nbefore calling these methods directly.\n\n`Index`([data, dtype, copy, name, tupleize_cols])\n\nImmutable sequence used for indexing and alignment.\n\n`Index.values`\n\nReturn an array representing the data in the Index.\n\n`Index.is_monotonic`\n\nAlias for is_monotonic_increasing.\n\n`Index.is_monotonic_increasing`\n\nReturn if the index is monotonic increasing (only equal or increasing) values.\n\n`Index.is_monotonic_decreasing`\n\nReturn if the index is monotonic decreasing (only equal or decreasing) values.\n\n`Index.is_unique`\n\nReturn if the index has unique values.\n\n`Index.has_duplicates`\n\nCheck if the Index has duplicate values.\n\n`Index.hasnans`\n\nReturn True if there are any NaNs.\n\n`Index.dtype`\n\nReturn the dtype object of the underlying data.\n\n`Index.inferred_type`\n\nReturn a string of the type inferred from the values.\n\n`Index.is_all_dates`\n\nWhether or not the index values only consist of dates.\n\n`Index.shape`\n\nReturn a tuple of the shape of the underlying data.\n\n`Index.name`\n\nReturn Index or MultiIndex name.\n\n`Index.names`\n\n`Index.nbytes`\n\nReturn the number of bytes in the underlying data.\n\n`Index.ndim`\n\nNumber of dimensions of the underlying data, by definition 1.\n\n`Index.size`\n\nReturn the number of elements in the underlying data.\n\n`Index.empty`\n\n`Index.T`\n\nReturn the transpose, which is by definition self.\n\n`Index.memory_usage`([deep])\n\nMemory usage of the values.\n\n`Index.all`(*args, **kwargs)\n\nReturn whether all elements are Truthy.\n\n`Index.any`(*args, **kwargs)\n\nReturn whether any element is Truthy.\n\n`Index.argmin`([axis, skipna])\n\nReturn int position of the smallest value in the Series.\n\n`Index.argmax`([axis, skipna])\n\nReturn int position of the largest value in the Series.\n\n`Index.copy`([name, deep, dtype, names])\n\nMake a copy of this object.\n\n`Index.delete`(loc)\n\nMake new Index with passed location(-s) deleted.\n\n`Index.drop`(labels[, errors])\n\nMake new Index with passed list of labels deleted.\n\n`Index.drop_duplicates`([keep])\n\nReturn Index with duplicate values removed.\n\n`Index.duplicated`([keep])\n\nIndicate duplicate index values.\n\n`Index.equals`(other)\n\nDetermine if two Index object are equal.\n\n`Index.factorize`([sort, na_sentinel])\n\nEncode the object as an enumerated type or categorical variable.\n\n`Index.identical`(other)\n\nSimilar to equals, but checks that object attributes and types are also equal.\n\n`Index.insert`(loc, item)\n\nMake new Index inserting new item at location.\n\n`Index.is_`(other)\n\nMore flexible, faster check like `is` but that works through views.\n\n`Index.is_boolean`()\n\nCheck if the Index only consists of booleans.\n\n`Index.is_categorical`()\n\nCheck if the Index holds categorical data.\n\n`Index.is_floating`()\n\nCheck if the Index is a floating type.\n\n`Index.is_integer`()\n\nCheck if the Index only consists of integers.\n\n`Index.is_interval`()\n\nCheck if the Index holds Interval objects.\n\n`Index.is_mixed`()\n\nCheck if the Index holds data with mixed data types.\n\n`Index.is_numeric`()\n\nCheck if the Index only consists of numeric data.\n\n`Index.is_object`()\n\nCheck if the Index is of the object dtype.\n\n`Index.min`([axis, skipna])\n\nReturn the minimum value of the Index.\n\n`Index.max`([axis, skipna])\n\nReturn the maximum value of the Index.\n\n`Index.reindex`(target[, method, level, ...])\n\nCreate index with target's values.\n\n`Index.rename`(name[, inplace])\n\nAlter Index or MultiIndex name.\n\n`Index.repeat`(repeats[, axis])\n\nRepeat elements of a Index.\n\n`Index.where`(cond[, other])\n\nReplace values where the condition is False.\n\n`Index.take`(indices[, axis, allow_fill, ...])\n\nReturn a new Index of the values selected by the indices.\n\n`Index.putmask`(mask, value)\n\nReturn a new Index of the values set with the mask.\n\n`Index.unique`([level])\n\nReturn unique values in the index.\n\n`Index.nunique`([dropna])\n\nReturn number of unique elements in the object.\n\n`Index.value_counts`([normalize, sort, ...])\n\nReturn a Series containing counts of unique values.\n\n`Index.set_names`(names[, level, inplace])\n\nSet Index or MultiIndex name.\n\n`Index.droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`Index.fillna`([value, downcast])\n\nFill NA/NaN values with the specified value.\n\n`Index.dropna`([how])\n\nReturn Index without NA/NaN values.\n\n`Index.isna`()\n\nDetect missing values.\n\n`Index.notna`()\n\nDetect existing (non-missing) values.\n\n`Index.astype`(dtype[, copy])\n\nCreate an Index with values cast to dtypes.\n\n`Index.item`()\n\nReturn the first element of the underlying data as a Python scalar.\n\n`Index.map`(mapper[, na_action])\n\nMap values using an input mapping or function.\n\n`Index.ravel`([order])\n\nReturn an ndarray of the flattened values of the underlying data.\n\n`Index.to_list`()\n\nReturn a list of the values.\n\n`Index.to_native_types`([slicer])\n\n(DEPRECATED) Format specified values of self and return them.\n\n`Index.to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`Index.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`Index.view`([cls])\n\n`Index.argsort`(*args, **kwargs)\n\nReturn the integer indices that would sort the index.\n\n`Index.searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`Index.sort_values`([return_indexer, ...])\n\nReturn a sorted copy of the index.\n\n`Index.shift`([periods, freq])\n\nShift index by desired number of time frequency increments.\n\n`Index.append`(other)\n\nAppend a collection of Index options together.\n\n`Index.join`(other[, how, level, ...])\n\nCompute join_index and indexers to conform data structures to the new index.\n\n`Index.intersection`(other[, sort])\n\nForm the intersection of two Index objects.\n\n`Index.union`(other[, sort])\n\nForm the union of two Index objects.\n\n`Index.difference`(other[, sort])\n\nReturn a new Index with elements of index not in other.\n\n`Index.symmetric_difference`(other[, ...])\n\nCompute the symmetric difference of two Index objects.\n\n`Index.asof`(label)\n\nReturn the label from the index, or, if not present, the previous one.\n\n`Index.asof_locs`(where, mask)\n\nReturn the locations (indices) of labels in the index.\n\n`Index.get_indexer`(target[, method, limit, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`Index.get_indexer_for`(target)\n\nGuaranteed return of an indexer even when non-unique.\n\n`Index.get_indexer_non_unique`(target)\n\nCompute indexer and mask for new index given the current index.\n\n`Index.get_level_values`(level)\n\nReturn an Index of values for requested level.\n\n`Index.get_loc`(key[, method, tolerance])\n\nGet integer location, slice or boolean mask for requested label.\n\n`Index.get_slice_bound`(label, side[, kind])\n\nCalculate slice bound that corresponds to given label.\n\n`Index.get_value`(series, key)\n\nFast lookup of value from 1-dimensional ndarray.\n\n`Index.isin`(values[, level])\n\nReturn a boolean array where the index values are in values.\n\n`Index.slice_indexer`([start, end, step, kind])\n\nCompute the slice indexer for input labels and step.\n\n`Index.slice_locs`([start, end, step, kind])\n\nCompute slice locations for input labels.\n\n`RangeIndex`([start, stop, step, dtype, copy, ...])\n\nImmutable Index implementing a monotonic integer range.\n\n`Int64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`UInt64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`Float64Index`([data, dtype, copy, name])\n\n(DEPRECATED) Immutable sequence used for indexing and alignment.\n\n`RangeIndex.start`\n\nThe value of the start parameter (`0` if this was not supplied).\n\n`RangeIndex.stop`\n\nThe value of the stop parameter.\n\n`RangeIndex.step`\n\nThe value of the step parameter (`1` if this was not supplied).\n\n`RangeIndex.from_range`(data[, name, dtype])\n\nCreate RangeIndex from a range object.\n\n`CategoricalIndex`([data, categories, ...])\n\nIndex based on an underlying `Categorical`.\n\n`CategoricalIndex.codes`\n\nThe category codes of this categorical.\n\n`CategoricalIndex.categories`\n\nThe categories of this categorical.\n\n`CategoricalIndex.ordered`\n\nWhether the categories have an ordered relationship.\n\n`CategoricalIndex.rename_categories`(*args, ...)\n\nRename categories.\n\n`CategoricalIndex.reorder_categories`(*args, ...)\n\nReorder categories as specified in new_categories.\n\n`CategoricalIndex.add_categories`(*args, **kwargs)\n\nAdd new categories.\n\n`CategoricalIndex.remove_categories`(*args, ...)\n\nRemove the specified categories.\n\n`CategoricalIndex.remove_unused_categories`(...)\n\nRemove categories which are not used.\n\n`CategoricalIndex.set_categories`(*args, **kwargs)\n\nSet the categories to the specified new_categories.\n\n`CategoricalIndex.as_ordered`(*args, **kwargs)\n\nSet the Categorical to be ordered.\n\n`CategoricalIndex.as_unordered`(*args, **kwargs)\n\nSet the Categorical to be unordered.\n\n`CategoricalIndex.map`(mapper)\n\nMap values using input an input mapping or function.\n\n`CategoricalIndex.equals`(other)\n\nDetermine if two CategoricalIndex objects contain the same elements.\n\n`IntervalIndex`(data[, closed, dtype, copy, ...])\n\nImmutable index of intervals that are closed on the same side.\n\n`IntervalIndex.from_arrays`(left, right[, ...])\n\nConstruct from two arrays defining the left and right bounds.\n\n`IntervalIndex.from_tuples`(data[, closed, ...])\n\nConstruct an IntervalIndex from an array-like of tuples.\n\n`IntervalIndex.from_breaks`(breaks[, closed, ...])\n\nConstruct an IntervalIndex from an array of splits.\n\n`IntervalIndex.left`\n\n`IntervalIndex.right`\n\n`IntervalIndex.mid`\n\n`IntervalIndex.closed`\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n`IntervalIndex.length`\n\n`IntervalIndex.values`\n\nReturn an array representing the data in the Index.\n\n`IntervalIndex.is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`IntervalIndex.is_non_overlapping_monotonic`\n\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n`IntervalIndex.is_overlapping`\n\nReturn True if the IntervalIndex has overlapping intervals, else False.\n\n`IntervalIndex.get_loc`(key[, method, tolerance])\n\nGet integer location, slice or boolean mask for requested label.\n\n`IntervalIndex.get_indexer`(target[, method, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`IntervalIndex.set_closed`(*args, **kwargs)\n\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\n`IntervalIndex.contains`(*args, **kwargs)\n\nCheck elementwise if the Intervals contain the value.\n\n`IntervalIndex.overlaps`(*args, **kwargs)\n\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\n`IntervalIndex.to_tuples`(*args, **kwargs)\n\nReturn an ndarray of tuples of the form (left, right).\n\n`MultiIndex`([levels, codes, sortorder, ...])\n\nA multi-level, or hierarchical, index object for pandas objects.\n\n`IndexSlice`\n\nCreate an object to more easily perform multi-index slicing.\n\n`MultiIndex.from_arrays`(arrays[, sortorder, ...])\n\nConvert arrays to MultiIndex.\n\n`MultiIndex.from_tuples`(tuples[, sortorder, ...])\n\nConvert list of tuples to MultiIndex.\n\n`MultiIndex.from_product`(iterables[, ...])\n\nMake a MultiIndex from the cartesian product of multiple iterables.\n\n`MultiIndex.from_frame`(df[, sortorder, names])\n\nMake a MultiIndex from a DataFrame.\n\n`MultiIndex.names`\n\nNames of levels in MultiIndex.\n\n`MultiIndex.levels`\n\n`MultiIndex.codes`\n\n`MultiIndex.nlevels`\n\nInteger number of levels in this MultiIndex.\n\n`MultiIndex.levshape`\n\nA tuple with the length of each level.\n\n`MultiIndex.dtypes`\n\nReturn the dtypes as a Series for the underlying MultiIndex.\n\n`MultiIndex.set_levels`(levels[, level, ...])\n\nSet new levels on MultiIndex.\n\n`MultiIndex.set_codes`(codes[, level, ...])\n\nSet new codes on MultiIndex.\n\n`MultiIndex.to_flat_index`()\n\nConvert a MultiIndex to an Index of Tuples containing the level values.\n\n`MultiIndex.to_frame`([index, name])\n\nCreate a DataFrame with the levels of the MultiIndex as columns.\n\n`MultiIndex.sortlevel`([level, ascending, ...])\n\nSort MultiIndex at the requested level.\n\n`MultiIndex.droplevel`([level])\n\nReturn index with requested level(s) removed.\n\n`MultiIndex.swaplevel`([i, j])\n\nSwap level i with level j.\n\n`MultiIndex.reorder_levels`(order)\n\nRearrange levels using input order.\n\n`MultiIndex.remove_unused_levels`()\n\nCreate new MultiIndex from current that removes unused levels.\n\n`MultiIndex.get_loc`(key[, method])\n\nGet location for a label or a tuple of labels.\n\n`MultiIndex.get_locs`(seq)\n\nGet location for a sequence of labels.\n\n`MultiIndex.get_loc_level`(key[, level, ...])\n\nGet location and sliced index for requested label(s)/level(s).\n\n`MultiIndex.get_indexer`(target[, method, ...])\n\nCompute indexer and mask for new index given the current index.\n\n`MultiIndex.get_level_values`(level)\n\nReturn vector of label values for requested level.\n\n`DatetimeIndex`([data, freq, tz, normalize, ...])\n\nImmutable ndarray-like of datetime64 data.\n\n`DatetimeIndex.year`\n\nThe year of the datetime.\n\n`DatetimeIndex.month`\n\nThe month as January=1, December=12.\n\n`DatetimeIndex.day`\n\nThe day of the datetime.\n\n`DatetimeIndex.hour`\n\nThe hours of the datetime.\n\n`DatetimeIndex.minute`\n\nThe minutes of the datetime.\n\n`DatetimeIndex.second`\n\nThe seconds of the datetime.\n\n`DatetimeIndex.microsecond`\n\nThe microseconds of the datetime.\n\n`DatetimeIndex.nanosecond`\n\nThe nanoseconds of the datetime.\n\n`DatetimeIndex.date`\n\nReturns numpy array of python `datetime.date` objects.\n\n`DatetimeIndex.time`\n\nReturns numpy array of `datetime.time` objects.\n\n`DatetimeIndex.timetz`\n\nReturns numpy array of `datetime.time` objects with timezone information.\n\n`DatetimeIndex.dayofyear`\n\nThe ordinal day of the year.\n\n`DatetimeIndex.day_of_year`\n\nThe ordinal day of the year.\n\n`DatetimeIndex.weekofyear`\n\n(DEPRECATED) The week ordinal of the year.\n\n`DatetimeIndex.week`\n\n(DEPRECATED) The week ordinal of the year.\n\n`DatetimeIndex.dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`DatetimeIndex.quarter`\n\nThe quarter of the date.\n\n`DatetimeIndex.tz`\n\nReturn the timezone.\n\n`DatetimeIndex.freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`DatetimeIndex.freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`DatetimeIndex.is_month_start`\n\nIndicates whether the date is the first day of the month.\n\n`DatetimeIndex.is_month_end`\n\nIndicates whether the date is the last day of the month.\n\n`DatetimeIndex.is_quarter_start`\n\nIndicator for whether the date is the first day of a quarter.\n\n`DatetimeIndex.is_quarter_end`\n\nIndicator for whether the date is the last day of a quarter.\n\n`DatetimeIndex.is_year_start`\n\nIndicate whether the date is the first day of a year.\n\n`DatetimeIndex.is_year_end`\n\nIndicate whether the date is the last day of the year.\n\n`DatetimeIndex.is_leap_year`\n\nBoolean indicator if the date belongs to a leap year.\n\n`DatetimeIndex.inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\n`DatetimeIndex.indexer_at_time`(time[, asof])\n\nReturn index locations of values at particular time of day (e.g.\n\n`DatetimeIndex.indexer_between_time`(...[, ...])\n\nReturn index locations of values between particular times of day (e.g.,\n9:00-9:30AM).\n\n`DatetimeIndex.normalize`(*args, **kwargs)\n\nConvert times to midnight.\n\n`DatetimeIndex.strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`DatetimeIndex.snap`([freq])\n\nSnap time stamps to nearest occurring frequency.\n\n`DatetimeIndex.tz_convert`(tz)\n\nConvert tz-aware Datetime Array/Index from one time zone to another.\n\n`DatetimeIndex.tz_localize`(tz[, ambiguous, ...])\n\nLocalize tz-naive Datetime Array/Index to tz-aware Datetime Array/Index.\n\n`DatetimeIndex.round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`DatetimeIndex.floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`DatetimeIndex.ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`DatetimeIndex.month_name`(*args, **kwargs)\n\nReturn the month names of the DateTimeIndex with specified locale.\n\n`DatetimeIndex.day_name`(*args, **kwargs)\n\nReturn the day names of the DateTimeIndex with specified locale.\n\n`DatetimeIndex.to_period`(*args, **kwargs)\n\nCast to PeriodArray/Index at a particular frequency.\n\n`DatetimeIndex.to_perioddelta`(freq)\n\nCalculate TimedeltaArray of difference between index values and index\nconverted to PeriodArray at specified freq.\n\n`DatetimeIndex.to_pydatetime`(*args, **kwargs)\n\nReturn Datetime Array/Index as object ndarray of datetime.datetime objects.\n\n`DatetimeIndex.to_series`([keep_tz, index, name])\n\nCreate a Series with both index and values equal to the index keys useful with\nmap for returning an indexer based on an index.\n\n`DatetimeIndex.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`DatetimeIndex.mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n`DatetimeIndex.std`(*args, **kwargs)\n\nReturn sample standard deviation over requested axis.\n\n`TimedeltaIndex`([data, unit, freq, closed, ...])\n\nImmutable ndarray of timedelta64 data, represented internally as int64, and\nwhich can be boxed to timedelta objects.\n\n`TimedeltaIndex.days`\n\nNumber of days for each element.\n\n`TimedeltaIndex.seconds`\n\nNumber of seconds (>= 0 and less than 1 day) for each element.\n\n`TimedeltaIndex.microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second) for each element.\n\n`TimedeltaIndex.nanoseconds`\n\nNumber of nanoseconds (>= 0 and less than 1 microsecond) for each element.\n\n`TimedeltaIndex.components`\n\nReturn a dataframe of the components (days, hours, minutes, seconds,\nmilliseconds, microseconds, nanoseconds) of the Timedeltas.\n\n`TimedeltaIndex.inferred_freq`\n\nTries to return a string representing a frequency guess, generated by\ninfer_freq.\n\n`TimedeltaIndex.to_pytimedelta`(*args, **kwargs)\n\nReturn Timedelta Array/Index as object ndarray of datetime.timedelta objects.\n\n`TimedeltaIndex.to_series`([index, name])\n\nCreate a Series with both index and values equal to the index keys.\n\n`TimedeltaIndex.round`(*args, **kwargs)\n\nPerform round operation on the data to the specified freq.\n\n`TimedeltaIndex.floor`(*args, **kwargs)\n\nPerform floor operation on the data to the specified freq.\n\n`TimedeltaIndex.ceil`(*args, **kwargs)\n\nPerform ceil operation on the data to the specified freq.\n\n`TimedeltaIndex.to_frame`([index, name])\n\nCreate a DataFrame with a column containing the Index.\n\n`TimedeltaIndex.mean`(*args, **kwargs)\n\nReturn the mean value of the Array.\n\n`PeriodIndex`([data, ordinal, freq, dtype, ...])\n\nImmutable ndarray holding ordinal values indicating regular periods in time.\n\n`PeriodIndex.day`\n\nThe days of the period.\n\n`PeriodIndex.dayofweek`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.day_of_week`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.dayofyear`\n\nThe ordinal day of the year.\n\n`PeriodIndex.day_of_year`\n\nThe ordinal day of the year.\n\n`PeriodIndex.days_in_month`\n\nThe number of days in the month.\n\n`PeriodIndex.daysinmonth`\n\nThe number of days in the month.\n\n`PeriodIndex.end_time`\n\n`PeriodIndex.freq`\n\nReturn the frequency object if it is set, otherwise None.\n\n`PeriodIndex.freqstr`\n\nReturn the frequency object as a string if its set, otherwise None.\n\n`PeriodIndex.hour`\n\nThe hour of the period.\n\n`PeriodIndex.is_leap_year`\n\nLogical indicating if the date belongs to a leap year.\n\n`PeriodIndex.minute`\n\nThe minute of the period.\n\n`PeriodIndex.month`\n\nThe month as January=1, December=12.\n\n`PeriodIndex.quarter`\n\nThe quarter of the date.\n\n`PeriodIndex.qyear`\n\n`PeriodIndex.second`\n\nThe second of the period.\n\n`PeriodIndex.start_time`\n\n`PeriodIndex.week`\n\nThe week ordinal of the year.\n\n`PeriodIndex.weekday`\n\nThe day of the week with Monday=0, Sunday=6.\n\n`PeriodIndex.weekofyear`\n\nThe week ordinal of the year.\n\n`PeriodIndex.year`\n\nThe year of the period.\n\n`PeriodIndex.asfreq`([freq, how])\n\nConvert the PeriodArray to the specified frequency freq.\n\n`PeriodIndex.strftime`(*args, **kwargs)\n\nConvert to Index using specified date_format.\n\n`PeriodIndex.to_timestamp`([freq, how])\n\nCast to DatetimeArray/Index.\n\n"}, {"name": "Indexing and selecting data", "path": "user_guide/indexing", "type": "Manual", "text": "\nThe axis labeling information in pandas objects serves many purposes:\n\nIdentifies data (i.e. provides metadata) using known indicators, important for\nanalysis, visualization, and interactive console display.\n\nEnables automatic and explicit data alignment.\n\nAllows intuitive getting and setting of subsets of the data set.\n\nIn this section, we will focus on the final point: namely, how to slice, dice,\nand generally get and set subsets of pandas objects. The primary focus will be\non Series and DataFrame as they have received more development attention in\nthis area.\n\nNote\n\nThe Python and NumPy indexing operators `[]` and attribute operator `.`\nprovide quick and easy access to pandas data structures across a wide range of\nuse cases. This makes interactive work intuitive, as there\u2019s little new to\nlearn if you already know how to deal with Python dictionaries and NumPy\narrays. However, since the type of the data to be accessed isn\u2019t known in\nadvance, directly using standard operators has some optimization limits. For\nproduction code, we recommended that you take advantage of the optimized\npandas data access methods exposed in this chapter.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nSee the MultiIndex / Advanced Indexing for `MultiIndex` and more advanced\nindexing documentation.\n\nSee the cookbook for some advanced strategies.\n\nObject selection has had a number of user-requested additions in order to\nsupport more explicit location based indexing. pandas now supports three types\nof multi-axis indexing.\n\n`.loc` is primarily label based, but may also be used with a boolean array.\n`.loc` will raise `KeyError` when the items are not found. Allowed inputs are:\n\nA single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a label of\nthe index. This use is not an integer position along the index.).\n\nA list or array of labels `['a', 'b', 'c']`.\n\nA slice object with labels `'a':'f'` (Note that contrary to usual Python\nslices, both the start and the stop are included, when present in the index!\nSee Slicing with labels and Endpoints are inclusive.)\n\nA boolean array (any `NA` values will be treated as `False`).\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above).\n\nSee more at Selection by Label.\n\n`.iloc` is primarily integer position based (from `0` to `length-1` of the\naxis), but may also be used with a boolean array. `.iloc` will raise\n`IndexError` if a requested indexer is out-of-bounds, except slice indexers\nwhich allow out-of-bounds indexing. (this conforms with Python/NumPy slice\nsemantics). Allowed inputs are:\n\nAn integer e.g. `5`.\n\nA list or array of integers `[4, 3, 0]`.\n\nA slice object with ints `1:7`.\n\nA boolean array (any `NA` values will be treated as `False`).\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above).\n\nSee more at Selection by Position, Advanced Indexing and Advanced\nHierarchical.\n\n`.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer.\nSee more at Selection By Callable.\n\nGetting values from an object with multi-axes selection uses the following\nnotation (using `.loc` as an example, but the following applies to `.iloc` as\nwell). Any of the axes accessors may be the null slice `:`. Axes left out of\nthe specification are assumed to be `:`, e.g. `p.loc['a']` is equivalent to\n`p.loc['a', :, :]`.\n\nObject Type\n\nIndexers\n\nSeries\n\n`s.loc[indexer]`\n\nDataFrame\n\n`df.loc[row_indexer,column_indexer]`\n\nAs mentioned when introducing the data structures in the last section, the\nprimary function of indexing with `[]` (a.k.a. `__getitem__` for those\nfamiliar with implementing class behavior in Python) is selecting out lower-\ndimensional slices. The following table shows return type values when indexing\npandas objects with `[]`:\n\nObject Type\n\nSelection\n\nReturn Value Type\n\nSeries\n\n`series[label]`\n\nscalar value\n\nDataFrame\n\n`frame[colname]`\n\n`Series` corresponding to colname\n\nHere we construct a simple time series data set to use for illustrating the\nindexing functionality:\n\nNote\n\nNone of the indexing functionality is time series specific unless specifically\nstated.\n\nThus, as per above, we have the most basic indexing using `[]`:\n\nYou can pass a list of columns to `[]` to select columns in that order. If a\ncolumn is not contained in the DataFrame, an exception will be raised.\nMultiple columns can also be set in this manner:\n\nYou may find this useful for applying a transform (in-place) to a subset of\nthe columns.\n\nWarning\n\npandas aligns all AXES when setting `Series` and `DataFrame` from `.loc`, and\n`.iloc`.\n\nThis will not modify `df` because the column alignment is before value\nassignment.\n\nThe correct way to swap column values is by using raw values:\n\nYou may access an index on a `Series` or column on a `DataFrame` directly as\nan attribute:\n\nWarning\n\nYou can use this access only if the index element is a valid Python\nidentifier, e.g. `s.1` is not allowed. See here for an explanation of valid\nidentifiers.\n\nThe attribute will not be available if it conflicts with an existing method\nname, e.g. `s.min` is not allowed, but `s['min']` is possible.\n\nSimilarly, the attribute will not be available if it conflicts with any of the\nfollowing list: `index`, `major_axis`, `minor_axis`, `items`.\n\nIn any of these cases, standard indexing will still work, e.g. `s['1']`,\n`s['min']`, and `s['index']` will access the corresponding element or column.\n\nIf you are using the IPython environment, you may also use tab-completion to\nsee these accessible attributes.\n\nYou can also assign a `dict` to a row of a `DataFrame`:\n\nYou can use attribute access to modify an existing element of a Series or\ncolumn of a DataFrame, but be careful; if you try to use attribute access to\ncreate a new column, it creates a new attribute rather than a new column. In\n0.21.0 and later, this will raise a `UserWarning`:\n\nThe most robust and consistent way of slicing ranges along arbitrary axes is\ndescribed in the Selection by Position section detailing the `.iloc` method.\nFor now, we explain the semantics of slicing using the `[]` operator.\n\nWith Series, the syntax works exactly as with an ndarray, returning a slice of\nthe values and the corresponding labels:\n\nNote that setting works as well:\n\nWith DataFrame, slicing inside of `[]` slices the rows. This is provided\nlargely as a convenience since it is such a common operation.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nWarning\n\n`.loc` is strict when you present slicers that are not compatible (or\nconvertible) with the index type. For example using integers in a\n`DatetimeIndex`. These will raise a `TypeError`.\n\nString likes in slicing can be convertible to the type of the index and lead\nto natural slicing.\n\nWarning\n\nChanged in version 1.0.0.\n\npandas will raise a `KeyError` if indexing with a list with missing labels.\nSee list-like Using loc with missing keys in a list is Deprecated.\n\npandas provides a suite of methods in order to have purely label based\nindexing. This is a strict inclusion based protocol. Every label asked for\nmust be in the index, or a `KeyError` will be raised. When slicing, both the\nstart bound AND the stop bound are included, if present in the index. Integers\nare valid labels, but they refer to the label and not the position.\n\nThe `.loc` attribute is the primary access method. The following are valid\ninputs:\n\nA single label, e.g. `5` or `'a'` (Note that `5` is interpreted as a label of\nthe index. This use is not an integer position along the index.).\n\nA list or array of labels `['a', 'b', 'c']`.\n\nA slice object with labels `'a':'f'` (Note that contrary to usual Python\nslices, both the start and the stop are included, when present in the index!\nSee Slicing with labels.\n\nA boolean array.\n\nA `callable`, see Selection By Callable.\n\nNote that setting works as well:\n\nWith a DataFrame:\n\nAccessing via label slices:\n\nFor getting a cross section using a label (equivalent to `df.xs('a')`):\n\nFor getting values with a boolean array:\n\nNA values in a boolean array propagate as `False`:\n\nChanged in version 1.0.2.\n\nFor getting a value explicitly:\n\nWhen using `.loc` with slices, if both the start and the stop labels are\npresent in the index, then elements located between the two (including them)\nare returned:\n\nIf at least one of the two is absent, but the index is sorted, and can be\ncompared against start and stop labels, then slicing will still work as\nexpected, by selecting labels which rank between the two:\n\nHowever, if at least one of the two is absent and the index is not sorted, an\nerror will be raised (since doing otherwise would be computationally\nexpensive, as well as potentially ambiguous for mixed type indexes). For\ninstance, in the above example, `s.loc[1:6]` would raise `KeyError`.\n\nFor the rationale behind this behavior, see Endpoints are inclusive.\n\nAlso, if the index has duplicate labels and either the start or the stop label\nis duplicated, an error will be raised. For instance, in the above example,\n`s.loc[2:5]` would raise a `KeyError`.\n\nFor more information about duplicate labels, see Duplicate Labels.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation, may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\npandas provides a suite of methods in order to get purely integer based\nindexing. The semantics follow closely Python and NumPy slicing. These are\n`0-based` indexing. When slicing, the start bound is included, while the upper\nbound is excluded. Trying to use a non-integer, even a valid label will raise\nan `IndexError`.\n\nThe `.iloc` attribute is the primary access method. The following are valid\ninputs:\n\nAn integer e.g. `5`.\n\nA list or array of integers `[4, 3, 0]`.\n\nA slice object with ints `1:7`.\n\nA boolean array.\n\nA `callable`, see Selection By Callable.\n\nNote that setting works as well:\n\nWith a DataFrame:\n\nSelect via integer slicing:\n\nSelect via integer list:\n\nFor getting a cross section using an integer position (equiv to `df.xs(1)`):\n\nOut of range slice indexes are handled gracefully just as in Python/NumPy.\n\nNote that using slices that go out of bounds can result in an empty axis (e.g.\nan empty DataFrame being returned).\n\nA single indexer that is out of bounds will raise an `IndexError`. A list of\nindexers where any element is out of bounds will raise an `IndexError`.\n\n`.loc`, `.iloc`, and also `[]` indexing can accept a `callable` as indexer.\nThe `callable` must be a function with one argument (the calling Series or\nDataFrame) that returns valid output for indexing.\n\nYou can use callable indexing in `Series`.\n\nUsing these methods / indexers, you can chain data selection operations\nwithout using a temporary variable.\n\nIf you wish to get the 0th and the 2nd elements from the index in the \u2018A\u2019\ncolumn, you can do:\n\nThis can also be expressed using `.iloc`, by explicitly getting locations on\nthe indexers, and using positional indexing to select things.\n\nFor getting multiple indexers, using `.get_indexer`:\n\nWarning\n\nChanged in version 1.0.0.\n\nUsing `.loc` or `[]` with a list with one or more missing labels will no\nlonger reindex, in favor of `.reindex`.\n\nIn prior versions, using `.loc[list-of-labels]` would work as long as at least\n1 of the keys was found (otherwise it would raise a `KeyError`). This behavior\nwas changed and will now raise a `KeyError` if at least one label is missing.\nThe recommended alternative is to use `.reindex()`.\n\nFor example.\n\nSelection with all keys found is unchanged.\n\nPrevious behavior\n\nCurrent behavior\n\nThe idiomatic way to achieve selecting potentially not-found elements is via\n`.reindex()`. See also the section on reindexing.\n\nAlternatively, if you want to select only valid keys, the following is\nidiomatic and efficient; it is guaranteed to preserve the dtype of the\nselection.\n\nHaving a duplicated index will raise for a `.reindex()`:\n\nGenerally, you can intersect the desired labels with the current axis, and\nthen reindex.\n\nHowever, this would still raise if your resulting index is duplicated.\n\nA random selection of rows or columns from a Series or DataFrame with the\n`sample()` method. The method will sample rows by default, and accepts a\nspecific number of rows/columns to return, or a fraction of rows.\n\nBy default, `sample` will return each row at most once, but one can also\nsample with replacement using the `replace` option:\n\nBy default, each row has an equal probability of being selected, but if you\nwant rows to have different probabilities, you can pass the `sample` function\nsampling weights as `weights`. These weights can be a list, a NumPy array, or\na Series, but they must be of the same length as the object you are sampling.\nMissing values will be treated as a weight of zero, and inf values are not\nallowed. If weights do not sum to 1, they will be re-normalized by dividing\nall weights by the sum of the weights. For example:\n\nWhen applied to a DataFrame, you can use a column of the DataFrame as sampling\nweights (provided you are sampling rows and not columns) by simply passing the\nname of the column as a string.\n\n`sample` also allows users to sample columns instead of rows using the `axis`\nargument.\n\nFinally, one can also set a seed for `sample`\u2019s random number generator using\nthe `random_state` argument, which will accept either an integer (as a seed)\nor a NumPy RandomState object.\n\nThe `.loc/[]` operations can perform enlargement when setting a non-existent\nkey for that axis.\n\nIn the `Series` case this is effectively an appending operation.\n\nA `DataFrame` can be enlarged on either axis via `.loc`.\n\nThis is like an `append` operation on the `DataFrame`.\n\nSince indexing with `[]` must handle a lot of cases (single-label access,\nslicing, boolean indexing, etc.), it has a bit of overhead in order to figure\nout what you\u2019re asking for. If you only want to access a scalar value, the\nfastest way is to use the `at` and `iat` methods, which are implemented on all\nof the data structures.\n\nSimilarly to `loc`, `at` provides label based scalar lookups, while, `iat`\nprovides integer based lookups analogously to `iloc`\n\nYou can also set using these same indexers.\n\n`at` may enlarge the object in-place as above if the indexer is missing.\n\nAnother common operation is the use of boolean vectors to filter the data. The\noperators are: `|` for `or`, `&` for `and`, and `~` for `not`. These must be\ngrouped by using parentheses, since by default Python will evaluate an\nexpression such as `df['A'] > 2 & df['B'] < 3` as `df['A'] > (2 & df['B']) <\n3`, while the desired evaluation order is `(df['A'] > 2) & (df['B'] < 3)`.\n\nUsing a boolean vector to index a Series works exactly as in a NumPy ndarray:\n\nYou may select rows from a DataFrame using a boolean vector the same length as\nthe DataFrame\u2019s index (for example, something derived from one of the columns\nof the DataFrame):\n\nList comprehensions and the `map` method of Series can also be used to produce\nmore complex criteria:\n\nWith the choice methods Selection by Label, Selection by Position, and\nAdvanced Indexing you may select along more than one axis using boolean\nvectors combined with other indexing expressions.\n\nWarning\n\n`iloc` supports two kinds of boolean indexing. If the indexer is a boolean\n`Series`, an error will be raised. For instance, in the following example,\n`df.iloc[s.values, 1]` is ok. The boolean indexer is an array. But `df.iloc[s,\n1]` would raise `ValueError`.\n\nConsider the `isin()` method of `Series`, which returns a boolean vector that\nis true wherever the `Series` elements exist in the passed list. This allows\nyou to select rows where one or more columns have values you want:\n\nThe same method is available for `Index` objects and is useful for the cases\nwhen you don\u2019t know which of the sought labels are in fact present:\n\nIn addition to that, `MultiIndex` allows selecting a separate level to use in\nthe membership check:\n\nDataFrame also has an `isin()` method. When calling `isin`, pass a set of\nvalues as either an array or dict. If values is an array, `isin` returns a\nDataFrame of booleans that is the same shape as the original DataFrame, with\nTrue wherever the element is in the sequence of values.\n\nOftentimes you\u2019ll want to match certain values with certain columns. Just make\nvalues a `dict` where the key is the column, and the value is a list of items\nyou want to check for.\n\nTo return the DataFrame of booleans where the values are not in the original\nDataFrame, use the `~` operator:\n\nCombine DataFrame\u2019s `isin` with the `any()` and `all()` methods to quickly\nselect subsets of your data that meet a given criteria. To select a row where\neach column meets its own criterion:\n\nSelecting values from a Series with a boolean vector generally returns a\nsubset of the data. To guarantee that selection output has the same shape as\nthe original data, you can use the `where` method in `Series` and `DataFrame`.\n\nTo return only the selected rows:\n\nTo return a Series of the same shape as the original:\n\nSelecting values from a DataFrame with a boolean criterion now also preserves\ninput data shape. `where` is used under the hood as the implementation. The\ncode below is equivalent to `df.where(df < 0)`.\n\nIn addition, `where` takes an optional `other` argument for replacement of\nvalues where the condition is False, in the returned copy.\n\nYou may wish to set values based on some boolean criteria. This can be done\nintuitively like so:\n\nBy default, `where` returns a modified copy of the data. There is an optional\nparameter `inplace` so that the original data can be modified without creating\na copy:\n\nNote\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nAlignment\n\nFurthermore, `where` aligns the input boolean condition (ndarray or\nDataFrame), such that partial selection with setting is possible. This is\nanalogous to partial setting via `.loc` (but on the contents rather than the\naxis labels).\n\nWhere can also accept `axis` and `level` parameters to align the input when\nperforming the `where`.\n\nThis is equivalent to (but faster than) the following.\n\n`where` can accept a callable as condition and `other` arguments. The function\nmust be with one argument (the calling Series or DataFrame) and that returns\nvalid output as condition and `other` argument.\n\n`mask()` is the inverse boolean operation of `where`.\n\nAn alternative to `where()` is to use `numpy.where()`. Combined with setting a\nnew column, you can use it to enlarge a DataFrame where the values are\ndetermined conditionally.\n\nConsider you have two choices to choose from in the following DataFrame. And\nyou want to set a new column color to \u2018green\u2019 when the second column has \u2018Z\u2019.\nYou can do the following:\n\nIf you have multiple conditions, you can use `numpy.select()` to achieve that.\nSay corresponding to three conditions there are three choice of colors, with a\nfourth color as a fallback, you can do the following.\n\n`DataFrame` objects have a `query()` method that allows selection using an\nexpression.\n\nYou can get the value of the frame where column `b` has values between the\nvalues of columns `a` and `c`. For example:\n\nDo the same thing but fall back on a named index if there is no column with\nthe name `a`.\n\nIf instead you don\u2019t want to or cannot name your index, you can use the name\n`index` in your query expression:\n\nNote\n\nIf the name of your index overlaps with a column name, the column name is\ngiven precedence. For example,\n\nYou can still use the index in a query expression by using the special\nidentifier \u2018index\u2019:\n\nIf for some reason you have a column named `index`, then you can refer to the\nindex as `ilevel_0` as well, but at this point you should consider renaming\nyour columns to something less ambiguous.\n\nYou can also use the levels of a `DataFrame` with a `MultiIndex` as if they\nwere columns in the frame:\n\nIf the levels of the `MultiIndex` are unnamed, you can refer to them using\nspecial names:\n\nThe convention is `ilevel_0`, which means \u201cindex level 0\u201d for the 0th level of\nthe `index`.\n\nA use case for `query()` is when you have a collection of `DataFrame` objects\nthat have a subset of column names (or index levels/names) in common. You can\npass the same query to both frames without having to specify which frame\nyou\u2019re interested in querying\n\nFull numpy-like syntax:\n\nSlightly nicer by removing the parentheses (comparison operators bind tighter\nthan `&` and `|`):\n\nUse English instead of symbols:\n\nPretty close to how you might write it on paper:\n\n`query()` also supports special use of Python\u2019s `in` and `not in` comparison\noperators, providing a succinct syntax for calling the `isin` method of a\n`Series` or `DataFrame`.\n\nYou can combine this with other expressions for very succinct queries:\n\nNote\n\nNote that `in` and `not in` are evaluated in Python, since `numexpr` has no\nequivalent of this operation. However, only the `in`/`not in` expression\nitself is evaluated in vanilla Python. For example, in the expression\n\n`(b + c + d)` is evaluated by `numexpr` and then the `in` operation is\nevaluated in plain Python. In general, any operations that can be evaluated\nusing `numexpr` will be.\n\nComparing a `list` of values to a column using `==`/`!=` works similarly to\n`in`/`not in`.\n\nYou can negate boolean expressions with the word `not` or the `~` operator.\n\nOf course, expressions can be arbitrarily complex too:\n\n`DataFrame.query()` using `numexpr` is slightly faster than Python for large\nframes.\n\nNote\n\nYou will only see the performance benefits of using the `numexpr` engine with\n`DataFrame.query()` if your frame has more than approximately 200,000 rows.\n\nThis plot was created using a `DataFrame` with 3 columns each containing\nfloating point values generated using `numpy.random.randn()`.\n\nIf you want to identify and remove duplicate rows in a DataFrame, there are\ntwo methods that will help: `duplicated` and `drop_duplicates`. Each takes as\nan argument the columns to use to identify duplicated rows.\n\n`duplicated` returns a boolean vector whose length is the number of rows, and\nwhich indicates whether a row is duplicated.\n\n`drop_duplicates` removes duplicate rows.\n\nBy default, the first observed row of a duplicate set is considered unique,\nbut each method has a `keep` parameter to specify targets to be kept.\n\n`keep='first'` (default): mark / drop duplicates except for the first\noccurrence.\n\n`keep='last'`: mark / drop duplicates except for the last occurrence.\n\n`keep=False`: mark / drop all duplicates.\n\nAlso, you can pass a list of columns to identify duplications.\n\nTo drop duplicates by index value, use `Index.duplicated` then perform\nslicing. The same set of options are available for the `keep` parameter.\n\nEach of Series or DataFrame have a `get` method which can return a default\nvalue.\n\nSometimes you want to extract a set of values given a sequence of row labels\nand column labels, this can be achieved by `pandas.factorize` and NumPy\nindexing. For instance:\n\nFormerly this could be achieved with the dedicated `DataFrame.lookup` method\nwhich was deprecated in version 1.2.0.\n\nThe pandas `Index` class and its subclasses can be viewed as implementing an\nordered multiset. Duplicates are allowed. However, if you try to convert an\n`Index` object with duplicate entries into a `set`, an exception will be\nraised.\n\n`Index` also provides the infrastructure necessary for lookups, data\nalignment, and reindexing. The easiest way to create an `Index` directly is to\npass a `list` or other sequence to `Index`:\n\nYou can also pass a `name` to be stored in the index:\n\nThe name, if set, will be shown in the console display:\n\nIndexes are \u201cmostly immutable\u201d, but it is possible to set and change their\n`name` attribute. You can use the `rename`, `set_names` to set these\nattributes directly, and they default to returning a copy.\n\nSee Advanced Indexing for usage of MultiIndexes.\n\n`set_names`, `set_levels`, and `set_codes` also take an optional `level`\nargument\n\nThe two main operations are `union` and `intersection`. Difference is provided\nvia the `.difference()` method.\n\nAlso available is the `symmetric_difference` operation, which returns elements\nthat appear in either `idx1` or `idx2`, but not in both. This is equivalent to\nthe Index created by `idx1.difference(idx2).union(idx2.difference(idx1))`,\nwith duplicates dropped.\n\nNote\n\nThe resulting index from a set operation will be sorted in ascending order.\n\nWhen performing `Index.union()` between indexes with different dtypes, the\nindexes must be cast to a common dtype. Typically, though not always, this is\nobject dtype. The exception is when performing a union between integer and\nfloat data. In this case, the integer values are converted to float\n\nImportant\n\nEven though `Index` can hold missing values (`NaN`), it should be avoided if\nyou do not want any unexpected results. For example, some operations exclude\nmissing values implicitly.\n\n`Index.fillna` fills missing values with specified scalar value.\n\nOccasionally you will load or create a data set into a DataFrame and want to\nadd an index after you\u2019ve already done so. There are a couple of different\nways.\n\nDataFrame has a `set_index()` method which takes a column name (for a regular\n`Index`) or a list of column names (for a `MultiIndex`). To create a new, re-\nindexed DataFrame:\n\nThe `append` keyword option allow you to keep the existing index and append\nthe given columns to a MultiIndex:\n\nOther options in `set_index` allow you not drop the index columns or to add\nthe index in-place (without creating a new object):\n\nAs a convenience, there is a new function on DataFrame called `reset_index()`\nwhich transfers the index values into the DataFrame\u2019s columns and sets a\nsimple integer index. This is the inverse operation of `set_index()`.\n\nThe output is more similar to a SQL table or a record array. The names for the\ncolumns derived from the index are the ones stored in the `names` attribute.\n\nYou can use the `level` keyword to remove only a portion of the index:\n\n`reset_index` takes an optional parameter `drop` which if true simply discards\nthe index, instead of putting index values in the DataFrame\u2019s columns.\n\nIf you create an index yourself, you can just assign it to the `index` field:\n\nWhen setting values in a pandas object, care must be taken to avoid what is\ncalled `chained indexing`. Here is an example.\n\nCompare these two access methods:\n\nThese both yield the same results, so which should you use? It is instructive\nto understand the order of operations on these and why method 2 (`.loc`) is\nmuch preferred over method 1 (chained `[]`).\n\n`dfmi['one']` selects the first level of the columns and returns a DataFrame\nthat is singly-indexed. Then another Python operation\n`dfmi_with_one['second']` selects the series indexed by `'second'`. This is\nindicated by the variable `dfmi_with_one` because pandas sees these operations\nas separate events. e.g. separate calls to `__getitem__`, so it has to treat\nthem as linear operations, they happen one after another.\n\nContrast this to `df.loc[:,('one','second')]` which passes a nested tuple of\n`(slice(None),('one','second'))` to a single call to `__getitem__`. This\nallows pandas to deal with this as a single entity. Furthermore this order of\noperations can be significantly faster, and allows one to index both axes if\nso desired.\n\nThe problem in the previous section is just a performance issue. What\u2019s up\nwith the `SettingWithCopy` warning? We don\u2019t usually throw warnings around\nwhen you do something that might cost a few extra milliseconds!\n\nBut it turns out that assigning to the product of chained indexing has\ninherently unpredictable results. To see this, think about how the Python\ninterpreter executes this code:\n\nBut this code is handled differently:\n\nSee that `__getitem__` in there? Outside of simple cases, it\u2019s very hard to\npredict whether it will return a view or a copy (it depends on the memory\nlayout of the array, about which pandas makes no guarantees), and therefore\nwhether the `__setitem__` will modify `dfmi` or a temporary object that gets\nthrown out immediately afterward. That\u2019s what `SettingWithCopy` is warning you\nabout!\n\nNote\n\nYou may be wondering whether we should be concerned about the `loc` property\nin the first example. But `dfmi.loc` is guaranteed to be `dfmi` itself with\nmodified indexing behavior, so `dfmi.loc.__getitem__` / `dfmi.loc.__setitem__`\noperate on `dfmi` directly. Of course, `dfmi.loc.__getitem__(idx)` may be a\nview or a copy of `dfmi`.\n\nSometimes a `SettingWithCopy` warning will arise at times when there\u2019s no\nobvious chained indexing going on. These are the bugs that `SettingWithCopy`\nis designed to catch! pandas is probably trying to warn you that you\u2019ve done\nthis:\n\nYikes!\n\nWhen you use chained indexing, the order and type of the indexing operation\npartially determine whether the result is a slice into the original object, or\na copy of the slice.\n\npandas has the `SettingWithCopyWarning` because assigning to a copy of a slice\nis frequently not intentional, but a mistake caused by chained indexing\nreturning a copy where a slice was expected.\n\nIf you would like pandas to be more or less trusting about assignment to a\nchained indexing expression, you can set the option `mode.chained_assignment`\nto one of these values:\n\n`'warn'`, the default, means a `SettingWithCopyWarning` is printed.\n\n`'raise'` means pandas will raise a `SettingWithCopyException` you have to\ndeal with.\n\n`None` will suppress the warnings entirely.\n\nThis however is operating on a copy and will not work.\n\nA chained assignment can also crop up in setting in a mixed dtype frame.\n\nNote\n\nThese setting rules apply to all of `.loc/.iloc`.\n\nThe following is the recommended access method using `.loc` for multiple items\n(using `mask`) and a single item using a fixed index:\n\nThe following can work at times, but it is not guaranteed to, and therefore\nshould be avoided:\n\nLast, the subsequent example will not work at all, and so should be avoided:\n\nWarning\n\nThe chained assignment warnings / exceptions are aiming to inform the user of\na possibly invalid assignment. There may be false positives; situations where\na chained assignment is inadvertently reported.\n\n"}, {"name": "Input/output", "path": "reference/io", "type": "Input/output", "text": "\n`read_pickle`(filepath_or_buffer[, ...])\n\nLoad pickled pandas object (or any object) from file.\n\n`DataFrame.to_pickle`(path[, compression, ...])\n\nPickle (serialize) object to file.\n\n`read_table`(filepath_or_buffer[, sep, ...])\n\nRead general delimited file into DataFrame.\n\n`read_csv`(filepath_or_buffer[, sep, ...])\n\nRead a comma-separated values (csv) file into DataFrame.\n\n`DataFrame.to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`read_fwf`(filepath_or_buffer[, colspecs, ...])\n\nRead a table of fixed-width formatted lines into DataFrame.\n\n`read_clipboard`([sep])\n\nRead text from clipboard and pass to read_csv.\n\n`DataFrame.to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`read_excel`(io[, sheet_name, header, names, ...])\n\nRead an Excel file into a pandas DataFrame.\n\n`DataFrame.to_excel`(excel_writer[, ...])\n\nWrite object to an Excel sheet.\n\n`ExcelFile.parse`([sheet_name, header, names, ...])\n\nParse specified sheet(s) into a DataFrame.\n\n`Styler.to_excel`(excel_writer[, sheet_name, ...])\n\nWrite Styler to an Excel sheet.\n\n`ExcelWriter`(path[, engine, date_format, ...])\n\nClass for writing DataFrame objects into excel sheets.\n\n`read_json`([path_or_buf, orient, typ, dtype, ...])\n\nConvert a JSON string to pandas object.\n\n`json_normalize`(data[, record_path, meta, ...])\n\nNormalize semi-structured JSON data into a flat table.\n\n`DataFrame.to_json`([path_or_buf, orient, ...])\n\nConvert the object to a JSON string.\n\n`build_table_schema`(data[, index, ...])\n\nCreate a Table schema from `data`.\n\n`read_html`(io[, match, flavor, header, ...])\n\nRead HTML tables into a `list` of `DataFrame` objects.\n\n`DataFrame.to_html`([buf, columns, col_space, ...])\n\nRender a DataFrame as an HTML table.\n\n`Styler.to_html`([buf, table_uuid, ...])\n\nWrite Styler to a file, buffer or string in HTML-CSS format.\n\n`read_xml`(path_or_buffer[, xpath, ...])\n\nRead XML document into a `DataFrame` object.\n\n`DataFrame.to_xml`([path_or_buffer, index, ...])\n\nRender a DataFrame to an XML document.\n\n`DataFrame.to_latex`([buf, columns, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`Styler.to_latex`([buf, column_format, ...])\n\nWrite Styler to a file, buffer or string in LaTeX format.\n\n`read_hdf`(path_or_buf[, key, mode, errors, ...])\n\nRead from the store, close it if we opened it.\n\n`HDFStore.put`(key, value[, format, index, ...])\n\nStore object in HDFStore.\n\n`HDFStore.append`(key, value[, format, axes, ...])\n\nAppend to Table in file.\n\n`HDFStore.get`(key)\n\nRetrieve pandas object stored in file.\n\n`HDFStore.select`(key[, where, start, stop, ...])\n\nRetrieve pandas object stored in file, optionally based on where criteria.\n\n`HDFStore.info`()\n\nPrint detailed information on the store.\n\n`HDFStore.keys`([include])\n\nReturn a list of keys corresponding to objects stored in HDFStore.\n\n`HDFStore.groups`()\n\nReturn a list of all the top-level nodes.\n\n`HDFStore.walk`([where])\n\nWalk the pytables group hierarchy for pandas objects.\n\nWarning\n\nOne can store a subclass of `DataFrame` or `Series` to HDF5, but the type of\nthe subclass is lost upon storing.\n\n`read_feather`(path[, columns, use_threads, ...])\n\nLoad a feather-format object from the file path.\n\n`DataFrame.to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`read_parquet`(path[, engine, columns, ...])\n\nLoad a parquet object from the file path, returning a DataFrame.\n\n`DataFrame.to_parquet`([path, engine, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`read_orc`(path[, columns])\n\nLoad an ORC object from the file path, returning a DataFrame.\n\n`read_sas`(filepath_or_buffer[, format, ...])\n\nRead SAS files stored as either XPORT or SAS7BDAT format files.\n\n`read_spss`(path[, usecols, convert_categoricals])\n\nLoad an SPSS file from the file path, returning a DataFrame.\n\n`read_sql_table`(table_name, con[, schema, ...])\n\nRead SQL database table into a DataFrame.\n\n`read_sql_query`(sql, con[, index_col, ...])\n\nRead SQL query into a DataFrame.\n\n`read_sql`(sql, con[, index_col, ...])\n\nRead SQL query or database table into a DataFrame.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`read_gbq`(query[, project_id, index_col, ...])\n\nLoad data from Google BigQuery.\n\n`read_stata`(filepath_or_buffer[, ...])\n\nRead Stata file into DataFrame.\n\n`DataFrame.to_stata`(path[, convert_dates, ...])\n\nExport DataFrame object to Stata dta format.\n\n`StataReader.data_label`\n\nReturn data label of Stata file.\n\n`StataReader.value_labels`()\n\nReturn a dict, associating each variable name a dict, associating each value\nits corresponding label.\n\n`StataReader.variable_labels`()\n\nReturn variable labels as a dict, associating each variable name with\ncorresponding label.\n\n`StataWriter.write_file`()\n\nExport DataFrame object to Stata dta format.\n\n"}, {"name": "Intro to data structures", "path": "user_guide/dsintro", "type": "Manual", "text": "\nWe\u2019ll start with a quick, non-comprehensive overview of the fundamental data\nstructures in pandas to get you started. The fundamental behavior about data\ntypes, indexing, and axis labeling / alignment apply across all of the\nobjects. To get started, import NumPy and load pandas into your namespace:\n\nHere is a basic tenet to keep in mind: data alignment is intrinsic. The link\nbetween labels and data will not be broken unless done so explicitly by you.\n\nWe\u2019ll give a brief intro to the data structures, then consider all of the\nbroad categories of functionality and methods in separate sections.\n\n`Series` is a one-dimensional labeled array capable of holding any data type\n(integers, strings, floating point numbers, Python objects, etc.). The axis\nlabels are collectively referred to as the index. The basic method to create a\nSeries is to call:\n\nHere, `data` can be many different things:\n\na Python dict\n\nan ndarray\n\na scalar value (like 5)\n\nThe passed index is a list of axis labels. Thus, this separates into a few\ncases depending on what data is:\n\nFrom ndarray\n\nIf `data` is an ndarray, index must be the same length as data. If no index is\npassed, one will be created having values `[0, ..., len(data) - 1]`.\n\nNote\n\npandas supports non-unique index values. If an operation that does not support\nduplicate index values is attempted, an exception will be raised at that time.\nThe reason for being lazy is nearly all performance-based (there are many\ninstances in computations, like parts of GroupBy, where the index is not\nused).\n\nFrom dict\n\nSeries can be instantiated from dicts:\n\nNote\n\nWhen the data is a dict, and an index is not passed, the `Series` index will\nbe ordered by the dict\u2019s insertion order, if you\u2019re using Python version >=\n3.6 and pandas version >= 0.23.\n\nIf you\u2019re using Python < 3.6 or pandas < 0.23, and an index is not passed, the\n`Series` index will be the lexically ordered list of dict keys.\n\nIn the example above, if you were on a Python version lower than 3.6 or a\npandas version lower than 0.23, the `Series` would be ordered by the lexical\norder of the dict keys (i.e. `['a', 'b', 'c']` rather than `['b', 'a', 'c']`).\n\nIf an index is passed, the values in data corresponding to the labels in the\nindex will be pulled out.\n\nNote\n\nNaN (not a number) is the standard missing data marker used in pandas.\n\nFrom scalar value\n\nIf `data` is a scalar value, an index must be provided. The value will be\nrepeated to match the length of index.\n\n`Series` acts very similarly to a `ndarray`, and is a valid argument to most\nNumPy functions. However, operations such as slicing will also slice the\nindex.\n\nNote\n\nWe will address array-based indexing like `s[[4, 3, 1]]` in section on\nindexing.\n\nLike a NumPy array, a pandas Series has a `dtype`.\n\nThis is often a NumPy dtype. However, pandas and 3rd-party libraries extend\nNumPy\u2019s type system in a few places, in which case the dtype would be an\n`ExtensionDtype`. Some examples within pandas are Categorical data and\nNullable integer data type. See dtypes for more.\n\nIf you need the actual array backing a `Series`, use `Series.array`.\n\nAccessing the array can be useful when you need to do some operation without\nthe index (to disable automatic alignment, for example).\n\n`Series.array` will always be an `ExtensionArray`. Briefly, an ExtensionArray\nis a thin wrapper around one or more concrete arrays like a `numpy.ndarray`.\npandas knows how to take an `ExtensionArray` and store it in a `Series` or a\ncolumn of a `DataFrame`. See dtypes for more.\n\nWhile Series is ndarray-like, if you need an actual ndarray, then use\n`Series.to_numpy()`.\n\nEven if the Series is backed by a `ExtensionArray`, `Series.to_numpy()` will\nreturn a NumPy ndarray.\n\nA Series is like a fixed-size dict in that you can get and set values by index\nlabel:\n\nIf a label is not contained, an exception is raised:\n\nUsing the `get` method, a missing label will return None or specified default:\n\nSee also the section on attribute access.\n\nWhen working with raw NumPy arrays, looping through value-by-value is usually\nnot necessary. The same is true when working with Series in pandas. Series can\nalso be passed into most NumPy methods expecting an ndarray.\n\nA key difference between Series and ndarray is that operations between Series\nautomatically align the data based on label. Thus, you can write computations\nwithout giving consideration to whether the Series involved have the same\nlabels.\n\nThe result of an operation between unaligned Series will have the union of the\nindexes involved. If a label is not found in one Series or the other, the\nresult will be marked as missing `NaN`. Being able to write code without doing\nany explicit data alignment grants immense freedom and flexibility in\ninteractive data analysis and research. The integrated data alignment features\nof the pandas data structures set pandas apart from the majority of related\ntools for working with labeled data.\n\nNote\n\nIn general, we chose to make the default result of operations between\ndifferently indexed objects yield the union of the indexes in order to avoid\nloss of information. Having an index label, though the data is missing, is\ntypically important information as part of a computation. You of course have\nthe option of dropping labels with missing data via the dropna function.\n\nSeries can also have a `name` attribute:\n\nThe Series `name` will be assigned automatically in many cases, in particular\nwhen taking 1D slices of DataFrame as you will see below.\n\nYou can rename a Series with the `pandas.Series.rename()` method.\n\nNote that `s` and `s2` refer to different objects.\n\nDataFrame is a 2-dimensional labeled data structure with columns of\npotentially different types. You can think of it like a spreadsheet or SQL\ntable, or a dict of Series objects. It is generally the most commonly used\npandas object. Like Series, DataFrame accepts many different kinds of input:\n\nDict of 1D ndarrays, lists, dicts, or Series\n\n2-D numpy.ndarray\n\nStructured or record ndarray\n\nA `Series`\n\nAnother `DataFrame`\n\nAlong with the data, you can optionally pass index (row labels) and columns\n(column labels) arguments. If you pass an index and / or columns, you are\nguaranteeing the index and / or columns of the resulting DataFrame. Thus, a\ndict of Series plus a specific index will discard all data not matching up to\nthe passed index.\n\nIf axis labels are not passed, they will be constructed from the input data\nbased on common sense rules.\n\nNote\n\nWhen the data is a dict, and `columns` is not specified, the `DataFrame`\ncolumns will be ordered by the dict\u2019s insertion order, if you are using Python\nversion >= 3.6 and pandas >= 0.23.\n\nIf you are using Python < 3.6 or pandas < 0.23, and `columns` is not\nspecified, the `DataFrame` columns will be the lexically ordered list of dict\nkeys.\n\nThe resulting index will be the union of the indexes of the various Series. If\nthere are any nested dicts, these will first be converted to Series. If no\ncolumns are passed, the columns will be the ordered list of dict keys.\n\nThe row and column labels can be accessed respectively by accessing the index\nand columns attributes:\n\nNote\n\nWhen a particular set of columns is passed along with a dict of data, the\npassed columns override the keys in the dict.\n\nThe ndarrays must all be the same length. If an index is passed, it must\nclearly also be the same length as the arrays. If no index is passed, the\nresult will be `range(n)`, where `n` is the array length.\n\nThis case is handled identically to a dict of arrays.\n\nNote\n\nDataFrame is not intended to work exactly like a 2-dimensional NumPy ndarray.\n\nYou can automatically create a MultiIndexed frame by passing a tuples\ndictionary.\n\nThe result will be a DataFrame with the same index as the input Series, and\nwith one column whose name is the original name of the Series (only if no\nother column name provided).\n\nThe field names of the first `namedtuple` in the list determine the columns of\nthe `DataFrame`. The remaining namedtuples (or tuples) are simply unpacked and\ntheir values are fed into the rows of the `DataFrame`. If any of those tuples\nis shorter than the first `namedtuple` then the later columns in the\ncorresponding row are marked as missing values. If any are longer than the\nfirst `namedtuple`, a `ValueError` is raised.\n\nNew in version 1.1.0.\n\nData Classes as introduced in PEP557, can be passed into the DataFrame\nconstructor. Passing a list of dataclasses is equivalent to passing a list of\ndictionaries.\n\nPlease be aware, that all values in the list should be dataclasses, mixing\ntypes in the list would result in a TypeError.\n\nMissing data\n\nMuch more will be said on this topic in the Missing data section. To construct\na DataFrame with missing data, we use `np.nan` to represent missing values.\nAlternatively, you may pass a `numpy.MaskedArray` as the data argument to the\nDataFrame constructor, and its masked entries will be considered missing.\n\nDataFrame.from_dict\n\n`DataFrame.from_dict` takes a dict of dicts or a dict of array-like sequences\nand returns a DataFrame. It operates like the `DataFrame` constructor except\nfor the `orient` parameter which is `'columns'` by default, but which can be\nset to `'index'` in order to use the dict keys as row labels.\n\nIf you pass `orient='index'`, the keys will be the row labels. In this case,\nyou can also pass the desired column names:\n\nDataFrame.from_records\n\n`DataFrame.from_records` takes a list of tuples or an ndarray with structured\ndtype. It works analogously to the normal `DataFrame` constructor, except that\nthe resulting DataFrame index may be a specific field of the structured dtype.\nFor example:\n\nYou can treat a DataFrame semantically like a dict of like-indexed Series\nobjects. Getting, setting, and deleting columns works with the same syntax as\nthe analogous dict operations:\n\nColumns can be deleted or popped like with a dict:\n\nWhen inserting a scalar value, it will naturally be propagated to fill the\ncolumn:\n\nWhen inserting a Series that does not have the same index as the DataFrame, it\nwill be conformed to the DataFrame\u2019s index:\n\nYou can insert raw ndarrays but their length must match the length of the\nDataFrame\u2019s index.\n\nBy default, columns get inserted at the end. The `insert` function is\navailable to insert at a particular location in the columns:\n\nInspired by dplyr\u2019s `mutate` verb, DataFrame has an `assign()` method that\nallows you to easily create new columns that are potentially derived from\nexisting columns.\n\nIn the example above, we inserted a precomputed value. We can also pass in a\nfunction of one argument to be evaluated on the DataFrame being assigned to.\n\n`assign` always returns a copy of the data, leaving the original DataFrame\nuntouched.\n\nPassing a callable, as opposed to an actual value to be inserted, is useful\nwhen you don\u2019t have a reference to the DataFrame at hand. This is common when\nusing `assign` in a chain of operations. For example, we can limit the\nDataFrame to just those observations with a Sepal Length greater than 5,\ncalculate the ratio, and plot:\n\nSince a function is passed in, the function is computed on the DataFrame being\nassigned to. Importantly, this is the DataFrame that\u2019s been filtered to those\nrows with sepal length greater than 5. The filtering happens first, and then\nthe ratio calculations. This is an example where we didn\u2019t have a reference to\nthe filtered DataFrame available.\n\nThe function signature for `assign` is simply `**kwargs`. The keys are the\ncolumn names for the new fields, and the values are either a value to be\ninserted (for example, a `Series` or NumPy array), or a function of one\nargument to be called on the `DataFrame`. A copy of the original DataFrame is\nreturned, with the new values inserted.\n\nStarting with Python 3.6 the order of `**kwargs` is preserved. This allows for\ndependent assignment, where an expression later in `**kwargs` can refer to a\ncolumn created earlier in the same `assign()`.\n\nIn the second expression, `x['C']` will refer to the newly created column,\nthat\u2019s equal to `dfa['A'] + dfa['B']`.\n\nThe basics of indexing are as follows:\n\nOperation\n\nSyntax\n\nResult\n\nSelect column\n\n`df[col]`\n\nSeries\n\nSelect row by label\n\n`df.loc[label]`\n\nSeries\n\nSelect row by integer location\n\n`df.iloc[loc]`\n\nSeries\n\nSlice rows\n\n`df[5:10]`\n\nDataFrame\n\nSelect rows by boolean vector\n\n`df[bool_vec]`\n\nDataFrame\n\nRow selection, for example, returns a Series whose index is the columns of the\nDataFrame:\n\nFor a more exhaustive treatment of sophisticated label-based indexing and\nslicing, see the section on indexing. We will address the fundamentals of\nreindexing / conforming to new sets of labels in the section on reindexing.\n\nData alignment between DataFrame objects automatically align on both the\ncolumns and the index (row labels). Again, the resulting object will have the\nunion of the column and row labels.\n\nWhen doing an operation between DataFrame and Series, the default behavior is\nto align the Series index on the DataFrame columns, thus broadcasting row-\nwise. For example:\n\nFor explicit control over the matching and broadcasting behavior, see the\nsection on flexible binary operations.\n\nOperations with scalars are just as you would expect:\n\nBoolean operators work as well:\n\nTo transpose, access the `T` attribute (also the `transpose` function),\nsimilar to an ndarray:\n\nElementwise NumPy ufuncs (log, exp, sqrt, \u2026) and various other NumPy functions\ncan be used with no issues on Series and DataFrame, assuming the data within\nare numeric:\n\nDataFrame is not intended to be a drop-in replacement for ndarray as its\nindexing semantics and data model are quite different in places from an\nn-dimensional array.\n\n`Series` implements `__array_ufunc__`, which allows it to work with NumPy\u2019s\nuniversal functions.\n\nThe ufunc is applied to the underlying array in a Series.\n\nChanged in version 0.25.0: When multiple `Series` are passed to a ufunc, they\nare aligned before performing the operation.\n\nLike other parts of the library, pandas will automatically align labeled\ninputs as part of a ufunc with multiple inputs. For example, using\n`numpy.remainder()` on two `Series` with differently ordered labels will align\nbefore the operation.\n\nAs usual, the union of the two indices is taken, and non-overlapping values\nare filled with missing values.\n\nWhen a binary ufunc is applied to a `Series` and `Index`, the Series\nimplementation takes precedence and a Series is returned.\n\nNumPy ufuncs are safe to apply to `Series` backed by non-ndarray arrays, for\nexample `arrays.SparseArray` (see Sparse calculation). If possible, the ufunc\nis applied without converting the underlying data to an ndarray.\n\nVery large DataFrames will be truncated to display them in the console. You\ncan also get a summary using `info()`. (Here I am reading a CSV version of the\nbaseball dataset from the plyr R package):\n\nHowever, using `to_string` will return a string representation of the\nDataFrame in tabular form, though it won\u2019t always fit the console width:\n\nWide DataFrames will be printed across multiple rows by default:\n\nYou can change how much to print on a single row by setting the\n`display.width` option:\n\nYou can adjust the max width of the individual columns by setting\n`display.max_colwidth`\n\nYou can also disable this feature via the `expand_frame_repr` option. This\nwill print the table in one block.\n\nIf a DataFrame column label is a valid Python variable name, the column can be\naccessed like an attribute:\n\nThe columns are also connected to the IPython completion mechanism so they can\nbe tab-completed:\n\n"}, {"name": "IO tools (text, CSV, HDF5, \u2026)", "path": "user_guide/io", "type": "Manual", "text": "\nThe pandas I/O API is a set of top level `reader` functions accessed like\n`pandas.read_csv()` that generally return a pandas object. The corresponding\n`writer` functions are object methods that are accessed like\n`DataFrame.to_csv()`. Below is a table containing available `readers` and\n`writers`.\n\nFormat Type\n\nData Description\n\nReader\n\nWriter\n\ntext\n\nCSV\n\nread_csv\n\nto_csv\n\ntext\n\nFixed-Width Text File\n\nread_fwf\n\ntext\n\nJSON\n\nread_json\n\nto_json\n\ntext\n\nHTML\n\nread_html\n\nto_html\n\ntext\n\nLaTeX\n\nStyler.to_latex\n\ntext\n\nXML\n\nread_xml\n\nto_xml\n\ntext\n\nLocal clipboard\n\nread_clipboard\n\nto_clipboard\n\nbinary\n\nMS Excel\n\nread_excel\n\nto_excel\n\nbinary\n\nOpenDocument\n\nread_excel\n\nbinary\n\nHDF5 Format\n\nread_hdf\n\nto_hdf\n\nbinary\n\nFeather Format\n\nread_feather\n\nto_feather\n\nbinary\n\nParquet Format\n\nread_parquet\n\nto_parquet\n\nbinary\n\nORC Format\n\nread_orc\n\nbinary\n\nStata\n\nread_stata\n\nto_stata\n\nbinary\n\nSAS\n\nread_sas\n\nbinary\n\nSPSS\n\nread_spss\n\nbinary\n\nPython Pickle Format\n\nread_pickle\n\nto_pickle\n\nSQL\n\nSQL\n\nread_sql\n\nto_sql\n\nSQL\n\nGoogle BigQuery\n\nread_gbq\n\nto_gbq\n\nHere is an informal performance comparison for some of these IO methods.\n\nNote\n\nFor examples that use the `StringIO` class, make sure you import it with `from\nio import StringIO` for Python 3.\n\nThe workhorse function for reading text files (a.k.a. flat files) is\n`read_csv()`. See the cookbook for some advanced strategies.\n\n`read_csv()` accepts the following common arguments:\n\nEither a path to a file (a `str`, `pathlib.Path`, or\n`py:py._path.local.LocalPath`), URL (including http, ftp, and S3 locations),\nor any object with a `read()` method (such as an open file or `StringIO`).\n\nDelimiter to use. If sep is `None`, the C engine cannot automatically detect\nthe separator, but the Python parsing engine can, meaning the latter will be\nused and automatically detect the separator by Python\u2019s builtin sniffer tool,\n`csv.Sniffer`. In addition, separators longer than 1 character and different\nfrom `'\\s+'` will be interpreted as regular expressions and will also force\nthe use of the Python parsing engine. Note that regex delimiters are prone to\nignoring quoted data. Regex example: `'\\\\r\\\\t'`.\n\nAlternative argument name for sep.\n\nSpecifies whether or not whitespace (e.g. `' '` or `'\\t'`) will be used as the\ndelimiter. Equivalent to setting `sep='\\s+'`. If this option is set to `True`,\nnothing should be passed in for the `delimiter` parameter.\n\nRow number(s) to use as the column names, and the start of the data. Default\nbehavior is to infer the column names: if no names are passed the behavior is\nidentical to `header=0` and column names are inferred from the first line of\nthe file, if column names are passed explicitly then the behavior is identical\nto `header=None`. Explicitly pass `header=0` to be able to replace existing\nnames.\n\nThe header can be a list of ints that specify row locations for a MultiIndex\non the columns e.g. `[0,1,3]`. Intervening rows that are not specified will be\nskipped (e.g. 2 in this example is skipped). Note that this parameter ignores\ncommented lines and empty lines if `skip_blank_lines=True`, so header=0\ndenotes the first line of data rather than the first line of the file.\n\nList of column names to use. If file contains no header row, then you should\nexplicitly pass `header=None`. Duplicates in this list are not allowed.\n\nColumn(s) to use as the row labels of the `DataFrame`, either given as string\nname or column index. If a sequence of int / str is given, a MultiIndex is\nused.\n\nNote: `index_col=False` can be used to force pandas to not use the first\ncolumn as the index, e.g. when you have a malformed file with delimiters at\nthe end of each line.\n\nThe default value of `None` instructs pandas to guess. If the number of fields\nin the column header row is equal to the number of fields in the body of the\ndata file, then a default index is used. If it is larger, then the first\ncolumns are used as index so that the remaining number of fields in the body\nare equal to the number of fields in the header.\n\nThe first row after the header is used to determine the number of columns,\nwhich will go into the index. If the subsequent rows contain less columns than\nthe first row, they are filled with `NaN`.\n\nThis can be avoided through `usecols`. This ensures that the columns are taken\nas is and the trailing data are ignored.\n\nReturn a subset of the columns. If list-like, all elements must either be\npositional (i.e. integer indices into the document columns) or strings that\ncorrespond to column names provided either by the user in `names` or inferred\nfrom the document header row(s). If `names` are given, the document header\nrow(s) are not taken into account. For example, a valid list-like `usecols`\nparameter would be `[0, 1, 2]` or `['foo', 'bar', 'baz']`.\n\nElement order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`. To\ninstantiate a DataFrame from `data` with element order preserved use\n`pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]` for columns in\n`['foo', 'bar']` order or `pd.read_csv(data, usecols=['foo', 'bar'])[['bar',\n'foo']]` for `['bar', 'foo']` order.\n\nIf callable, the callable function will be evaluated against the column names,\nreturning names where the callable function evaluates to True:\n\nUsing this parameter results in much faster parsing time and lower memory\nusage when using the c engine. The Python engine loads the data first before\ndeciding which columns to drop.\n\nIf the parsed data only contains one column then return a `Series`.\n\nDeprecated since version 1.4.0: Append `.squeeze(\"columns\")` to the call to\n`{func_name}` to squeeze the data.\n\nPrefix to add to column numbers when no header, e.g. \u2018X\u2019 for X0, X1, \u2026\n\nDeprecated since version 1.4.0: Use a list comprehension on the DataFrame\u2019s\ncolumns after calling `read_csv`.\n\nDuplicate columns will be specified as \u2018X\u2019, \u2018X.1\u2019\u2026\u2019X.N\u2019, rather than \u2018X\u2019\u2026\u2019X\u2019.\nPassing in `False` will cause data to be overwritten if there are duplicate\nnames in the columns.\n\nData type for data or columns. E.g. `{'a': np.float64, 'b': np.int32}`\n(unsupported with `engine='python'`). Use `str` or `object` together with\nsuitable `na_values` settings to preserve and not interpret dtype.\n\nParser engine to use. The C and pyarrow engines are faster, while the python\nengine is currently more feature-complete. Multithreading is currently only\nsupported by the pyarrow engine.\n\nNew in version 1.4.0: The \u201cpyarrow\u201d engine was added as an experimental\nengine, and some features are unsupported, or may not work correctly, with\nthis engine.\n\nDict of functions for converting values in certain columns. Keys can either be\nintegers or column labels.\n\nValues to consider as `True`.\n\nValues to consider as `False`.\n\nSkip spaces after delimiter.\n\nLine numbers to skip (0-indexed) or number of lines to skip (int) at the start\nof the file.\n\nIf callable, the callable function will be evaluated against the row indices,\nreturning True if the row should be skipped and False otherwise:\n\nNumber of lines at bottom of file to skip (unsupported with engine=\u2019c\u2019).\n\nNumber of rows of file to read. Useful for reading pieces of large files.\n\nInternally process the file in chunks, resulting in lower memory use while\nparsing, but possibly mixed type inference. To ensure no mixed types either\nset `False`, or specify the type with the `dtype` parameter. Note that the\nentire file is read into a single `DataFrame` regardless, use the `chunksize`\nor `iterator` parameter to return the data in chunks. (Only valid with C\nparser)\n\nIf a filepath is provided for `filepath_or_buffer`, map the file object\ndirectly onto memory and access the data directly from there. Using this\noption can improve performance because there is no longer any I/O overhead.\n\nAdditional strings to recognize as NA/NaN. If dict passed, specific per-column\nNA values. See na values const below for a list of the values interpreted as\nNaN by default.\n\nWhether or not to include the default NaN values when parsing the data.\nDepending on whether `na_values` is passed in, the behavior is as follows:\n\nIf `keep_default_na` is `True`, and `na_values` are specified, `na_values` is\nappended to the default NaN values used for parsing.\n\nIf `keep_default_na` is `True`, and `na_values` are not specified, only the\ndefault NaN values are used for parsing.\n\nIf `keep_default_na` is `False`, and `na_values` are specified, only the NaN\nvalues specified `na_values` are used for parsing.\n\nIf `keep_default_na` is `False`, and `na_values` are not specified, no strings\nwill be parsed as NaN.\n\nNote that if `na_filter` is passed in as `False`, the `keep_default_na` and\n`na_values` parameters will be ignored.\n\nDetect missing value markers (empty strings and the value of na_values). In\ndata without any NAs, passing `na_filter=False` can improve the performance of\nreading a large file.\n\nIndicate number of NA values placed in non-numeric columns.\n\nIf `True`, skip over blank lines rather than interpreting as NaN values.\n\nIf `True` -> try parsing the index.\n\nIf `[1, 2, 3]` -> try parsing columns 1, 2, 3 each as a separate date column.\n\nIf `[[1, 3]]` -> combine columns 1 and 3 and parse as a single date column.\n\nIf `{'foo': [1, 3]}` -> parse columns 1, 3 as date and call result \u2018foo\u2019. A\nfast-path exists for iso8601-formatted dates.\n\nIf `True` and parse_dates is enabled for a column, attempt to infer the\ndatetime format to speed up the processing.\n\nIf `True` and parse_dates specifies combining multiple columns then keep the\noriginal columns.\n\nFunction to use for converting a sequence of string columns to an array of\ndatetime instances. The default uses `dateutil.parser.parser` to do the\nconversion. pandas will try to call date_parser in three different ways,\nadvancing to the next if an exception occurs: 1) Pass one or more arrays (as\ndefined by parse_dates) as arguments; 2) concatenate (row-wise) the string\nvalues from the columns defined by parse_dates into a single array and pass\nthat; and 3) call date_parser once for each row using one or more strings\n(corresponding to the columns defined by parse_dates) as arguments.\n\nDD/MM format dates, international and European format.\n\nIf True, use a cache of unique, converted dates to apply the datetime\nconversion. May produce significant speed-up when parsing duplicate date\nstrings, especially ones with timezone offsets.\n\nNew in version 0.25.0.\n\nReturn `TextFileReader` object for iteration or getting chunks with\n`get_chunk()`.\n\nReturn `TextFileReader` object for iteration. See iterating and chunking\nbelow.\n\nFor on-the-fly decompression of on-disk data. If \u2018infer\u2019, then use gzip, bz2,\nzip, xz, or zstandard if `filepath_or_buffer` is path-like ending in \u2018.gz\u2019,\n\u2018.bz2\u2019, \u2018.zip\u2019, \u2018.xz\u2019, \u2018.zst\u2019, respectively, and no decompression otherwise.\nIf using \u2018zip\u2019, the ZIP file must contain only one data file to be read in.\nSet to `None` for no decompression. Can also be a dict with key `'method'` set\nto one of {`'zip'`, `'gzip'`, `'bz2'`, `'zstd'`} and other key-value pairs are\nforwarded to `zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`. As an example, the following could be passed for\nfaster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.1.0: dict option extended to support `gzip` and `bz2`.\n\nChanged in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019\nto `gzip.open`.\n\nThousands separator.\n\nCharacter to recognize as decimal point. E.g. use `','` for European data.\n\nSpecifies which converter the C engine should use for floating-point values.\nThe options are `None` for the ordinary converter, `high` for the high-\nprecision converter, and `round_trip` for the round-trip converter.\n\nCharacter to break file into lines. Only valid with C parser.\n\nThe character used to denote the start and end of a quoted item. Quoted items\ncan include the delimiter and it will be ignored.\n\nControl field quoting behavior per `csv.QUOTE_*` constants. Use one of\n`QUOTE_MINIMAL` (0), `QUOTE_ALL` (1), `QUOTE_NONNUMERIC` (2) or `QUOTE_NONE`\n(3).\n\nWhen `quotechar` is specified and `quoting` is not `QUOTE_NONE`, indicate\nwhether or not to interpret two consecutive `quotechar` elements inside a\nfield as a single `quotechar` element.\n\nOne-character string used to escape delimiter when quoting is `QUOTE_NONE`.\n\nIndicates remainder of line should not be parsed. If found at the beginning of\na line, the line will be ignored altogether. This parameter must be a single\ncharacter. Like empty lines (as long as `skip_blank_lines=True`), fully\ncommented lines are ignored by the parameter `header` but not by `skiprows`.\nFor example, if `comment='#'`, parsing \u2018#empty\\na,b,c\\n1,2,3\u2019 with `header=0`\nwill result in \u2018a,b,c\u2019 being treated as the header.\n\nEncoding to use for UTF when reading/writing (e.g. `'utf-8'`). List of Python\nstandard encodings.\n\nIf provided, this parameter will override values (default or not) for the\nfollowing parameters: `delimiter`, `doublequote`, `escapechar`,\n`skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to override\nvalues, a ParserWarning will be issued. See `csv.Dialect` documentation for\nmore details.\n\nLines with too many fields (e.g. a csv line with too many commas) will by\ndefault cause an exception to be raised, and no `DataFrame` will be returned.\nIf `False`, then these \u201cbad lines\u201d will dropped from the `DataFrame` that is\nreturned. See bad lines below.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nIf error_bad_lines is `False`, and warn_bad_lines is `True`, a warning for\neach \u201cbad line\u201d will be output.\n\nDeprecated since version 1.3.0: The `on_bad_lines` parameter should be used\ninstead to specify behavior upon encountering a bad line instead.\n\nSpecifies what to do upon encountering a bad line (a line with too many\nfields). Allowed values are :\n\n\u2018error\u2019, raise an ParserError when a bad line is encountered.\n\n\u2018warn\u2019, print a warning when a bad line is encountered and skip that line.\n\n\u2018skip\u2019, skip bad lines without raising or warning when they are encountered.\n\nNew in version 1.3.0.\n\nYou can indicate the data type for the whole `DataFrame` or individual\ncolumns:\n\nFortunately, pandas offers more than one way to ensure that your column(s)\ncontain only one `dtype`. If you\u2019re unfamiliar with these concepts, you can\nsee here to learn more about dtypes, and here to learn more about `object`\nconversion in pandas.\n\nFor instance, you can use the `converters` argument of `read_csv()`:\n\nOr you can use the `to_numeric()` function to coerce the dtypes after reading\nin the data,\n\nwhich will convert all valid parsing to floats, leaving the invalid parsing as\n`NaN`.\n\nUltimately, how you deal with reading in columns containing mixed dtypes\ndepends on your specific needs. In the case above, if you wanted to `NaN` out\nthe data anomalies, then `to_numeric()` is probably your best option. However,\nif you wanted for all the data to be coerced, no matter the type, then using\nthe `converters` argument of `read_csv()` would certainly be worth trying.\n\nNote\n\nIn some cases, reading in abnormal data with columns containing mixed dtypes\nwill result in an inconsistent dataset. If you rely on pandas to infer the\ndtypes of your columns, the parsing engine will go and infer the dtypes for\ndifferent chunks of the data, rather than the whole dataset at once.\nConsequently, you can end up with column(s) with mixed dtypes. For example,\n\nwill result with `mixed_df` containing an `int` dtype for certain chunks of\nthe column, and `str` for others due to the mixed dtypes from the data that\nwas read in. It is important to note that the overall column will be marked\nwith a `dtype` of `object`, which is used for columns with mixed dtypes.\n\n`Categorical` columns can be parsed directly by specifying `dtype='category'`\nor `dtype=CategoricalDtype(categories, ordered)`.\n\nIndividual columns can be parsed as a `Categorical` using a dict\nspecification:\n\nSpecifying `dtype='category'` will result in an unordered `Categorical` whose\n`categories` are the unique values observed in the data. For more control on\nthe categories and order, create a `CategoricalDtype` ahead of time, and pass\nthat for that column\u2019s `dtype`.\n\nWhen using `dtype=CategoricalDtype`, \u201cunexpected\u201d values outside of\n`dtype.categories` are treated as missing values.\n\nThis matches the behavior of `Categorical.set_categories()`.\n\nNote\n\nWith `dtype='category'`, the resulting categories will always be parsed as\nstrings (object dtype). If the categories are numeric they can be converted\nusing the `to_numeric()` function, or as appropriate, another converter such\nas `to_datetime()`.\n\nWhen `dtype` is a `CategoricalDtype` with homogeneous `categories` ( all\nnumeric, all datetimes, etc.), the conversion is done automatically.\n\nA file may or may not have a header row. pandas assumes the first row should\nbe used as the column names:\n\nBy specifying the `names` argument in conjunction with `header` you can\nindicate other names to use and whether or not to throw away the header row\n(if any):\n\nIf the header is in a row other than the first, pass the row number to\n`header`. This will skip the preceding rows:\n\nNote\n\nDefault behavior is to infer the column names: if no names are passed the\nbehavior is identical to `header=0` and column names are inferred from the\nfirst non-blank line of the file, if column names are passed explicitly then\nthe behavior is identical to `header=None`.\n\nIf the file or header contains duplicate names, pandas will by default\ndistinguish between them so as to prevent overwriting data:\n\nThere is no more duplicate data because `mangle_dupe_cols=True` by default,\nwhich modifies a series of duplicate columns \u2018X\u2019, \u2026, \u2018X\u2019 to become \u2018X\u2019, \u2018X.1\u2019,\n\u2026, \u2018X.N\u2019. If `mangle_dupe_cols=False`, duplicate data can arise:\n\nTo prevent users from encountering this problem with duplicate data, a\n`ValueError` exception is raised if `mangle_dupe_cols != True`:\n\nThe `usecols` argument allows you to select any subset of the columns in a\nfile, either using the column names, position numbers or a callable:\n\nThe `usecols` argument can also be used to specify which columns not to use in\nthe final result:\n\nIn this case, the callable is specifying that we exclude the \u201ca\u201d and \u201cc\u201d\ncolumns from the output.\n\nIf the `comment` parameter is specified, then completely commented lines will\nbe ignored. By default, completely blank lines will be ignored as well.\n\nIf `skip_blank_lines=False`, then `read_csv` will not ignore blank lines:\n\nWarning\n\nThe presence of ignored lines might create ambiguities involving line numbers;\nthe parameter `header` uses row numbers (ignoring commented/empty lines),\nwhile `skiprows` uses line numbers (including commented/empty lines):\n\nIf both `header` and `skiprows` are specified, `header` will be relative to\nthe end of `skiprows`. For example:\n\nSometimes comments or meta data may be included in a file:\n\nBy default, the parser includes the comments in the output:\n\nWe can suppress the comments using the `comment` keyword:\n\nThe `encoding` argument should be used for encoded unicode data, which will\nresult in byte strings being decoded to unicode in the result:\n\nSome formats which encode all characters as multiple bytes, like UTF-16, won\u2019t\nparse correctly at all without specifying the encoding. Full list of Python\nstandard encodings.\n\nIf a file has one more column of data than the number of column names, the\nfirst column will be used as the `DataFrame`\u2019s row names:\n\nOrdinarily, you can achieve this behavior using the `index_col` option.\n\nThere are some exception cases when a file has been prepared with delimiters\nat the end of each data line, confusing the parser. To explicitly disable the\nindex column inference and discard the last column, pass `index_col=False`:\n\nIf a subset of data is being parsed using the `usecols` option, the\n`index_col` specification is based on that subset, not the original data.\n\nTo better facilitate working with datetime data, `read_csv()` uses the keyword\narguments `parse_dates` and `date_parser` to allow users to specify a variety\nof columns and date/time formats to turn the input text data into `datetime`\nobjects.\n\nThe simplest case is to just pass in `parse_dates=True`:\n\nIt is often the case that we may want to store date and time data separately,\nor store various date fields separately. the `parse_dates` keyword can be used\nto specify a combination of columns to parse the dates and/or times from.\n\nYou can specify a list of column lists to `parse_dates`, the resulting date\ncolumns will be prepended to the output (so as to not affect the existing\ncolumn order) and the new column names will be the concatenation of the\ncomponent column names:\n\nBy default the parser removes the component date columns, but you can choose\nto retain them via the `keep_date_col` keyword:\n\nNote that if you wish to combine multiple columns into a single date column, a\nnested list must be used. In other words, `parse_dates=[1, 2]` indicates that\nthe second and third columns should each be parsed as separate date columns\nwhile `parse_dates=[[1, 2]]` means the two columns should be parsed into a\nsingle column.\n\nYou can also use a dict to specify custom name columns:\n\nIt is important to remember that if multiple text columns are to be parsed\ninto a single date column, then a new column is prepended to the data. The\n`index_col` specification is based off of this new set of columns rather than\nthe original data columns:\n\nNote\n\nIf a column or index contains an unparsable date, the entire column or index\nwill be returned unaltered as an object data type. For non-standard datetime\nparsing, use `to_datetime()` after `pd.read_csv`.\n\nNote\n\nread_csv has a fast_path for parsing datetime strings in iso8601 format, e.g\n\u201c2000-01-01T00:01:02+00:00\u201d and similar variations. If you can arrange for\nyour data to store datetimes in this format, load times will be significantly\nfaster, ~20x has been observed.\n\nFinally, the parser allows you to specify a custom `date_parser` function to\ntake full advantage of the flexibility of the date parsing API:\n\npandas will try to call the `date_parser` function in three different ways. If\nan exception is raised, the next one is tried:\n\n`date_parser` is first called with one or more arrays as arguments, as defined\nusing `parse_dates` (e.g., `date_parser(['2013', '2013'], ['1', '2'])`).\n\nIf #1 fails, `date_parser` is called with all the columns concatenated row-\nwise into a single array (e.g., `date_parser(['2013 1', '2013 2'])`).\n\nNote that performance-wise, you should try these methods of parsing dates in\norder:\n\nTry to infer the format using `infer_datetime_format=True` (see section\nbelow).\n\nIf you know the format, use `pd.to_datetime()`: `date_parser=lambda x:\npd.to_datetime(x, format=...)`.\n\nIf you have a really non-standard format, use a custom `date_parser` function.\nFor optimal performance, this should be vectorized, i.e., it should accept\narrays as arguments.\n\npandas cannot natively represent a column or index with mixed timezones. If\nyour CSV file contains columns with a mixture of timezones, the default result\nwill be an object-dtype column with strings, even with `parse_dates`.\n\nTo parse the mixed-timezone values as a datetime column, pass a partially-\napplied `to_datetime()` with `utc=True` as the `date_parser`.\n\nIf you have `parse_dates` enabled for some or all of your columns, and your\ndatetime strings are all formatted the same way, you may get a large speed up\nby setting `infer_datetime_format=True`. If set, pandas will attempt to guess\nthe format of your datetime strings, and then use a faster means of parsing\nthe strings. 5-10x parsing speeds have been observed. pandas will fallback to\nthe usual parsing if either the format cannot be guessed or the format that\nwas guessed cannot properly parse the entire column of strings. So in general,\n`infer_datetime_format` should not have any negative consequences if enabled.\n\nHere are some examples of datetime strings that can be guessed (All\nrepresenting December 30th, 2011 at 00:00:00):\n\n\u201c20111230\u201d\n\n\u201c2011/12/30\u201d\n\n\u201c20111230 00:00:00\u201d\n\n\u201c12/30/2011 00:00:00\u201d\n\n\u201c30/Dec/2011 00:00:00\u201d\n\n\u201c30/December/2011 00:00:00\u201d\n\nNote that `infer_datetime_format` is sensitive to `dayfirst`. With\n`dayfirst=True`, it will guess \u201c01/12/2011\u201d to be December 1st. With\n`dayfirst=False` (default) it will guess \u201c01/12/2011\u201d to be January 12th.\n\nWhile US date formats tend to be MM/DD/YYYY, many international formats use\nDD/MM/YYYY instead. For convenience, a `dayfirst` keyword is provided:\n\nNew in version 1.2.0.\n\n`df.to_csv(..., mode=\"wb\")` allows writing a CSV to a file object opened\nbinary mode. In most cases, it is not necessary to specify `mode` as Pandas\nwill auto-detect whether the file object is opened in text or binary mode.\n\nThe parameter `float_precision` can be specified in order to use a specific\nfloating-point converter during parsing with the C engine. The options are the\nordinary converter, the high-precision converter, and the round-trip converter\n(which is guaranteed to round-trip values after writing to a file). For\nexample:\n\nFor large numbers that have been written with a thousands separator, you can\nset the `thousands` keyword to a string of length 1 so that integers will be\nparsed correctly:\n\nBy default, numbers with a thousands separator will be parsed as strings:\n\nThe `thousands` keyword allows integers to be parsed correctly:\n\nTo control which values are parsed as missing values (which are signified by\n`NaN`), specify a string in `na_values`. If you specify a list of strings,\nthen all values in it are considered to be missing values. If you specify a\nnumber (a `float`, like `5.0` or an `integer` like `5`), the corresponding\nequivalent values will also imply a missing value (in this case effectively\n`[5.0, 5]` are recognized as `NaN`).\n\nTo completely override the default values that are recognized as missing,\nspecify `keep_default_na=False`.\n\nThe default `NaN` recognized values are `['-1.#IND', '1.#QNAN', '1.#IND',\n'-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL',\n'null', 'NaN', '-NaN', 'nan', '-nan', '']`.\n\nLet us consider some examples:\n\nIn the example above `5` and `5.0` will be recognized as `NaN`, in addition to\nthe defaults. A string will first be interpreted as a numerical `5`, then as a\n`NaN`.\n\nAbove, only an empty field will be recognized as `NaN`.\n\nAbove, both `NA` and `0` as strings are `NaN`.\n\nThe default values, in addition to the string `\"Nope\"` are recognized as\n`NaN`.\n\n`inf` like values will be parsed as `np.inf` (positive infinity), and `-inf`\nas `-np.inf` (negative infinity). These will ignore the case of the value,\nmeaning `Inf`, will also be parsed as `np.inf`.\n\nUsing the `squeeze` keyword, the parser will return output with a single\ncolumn as a `Series`:\n\nDeprecated since version 1.4.0: Users should append `.squeeze(\"columns\")` to\nthe DataFrame returned by `read_csv` instead.\n\nThe common values `True`, `False`, `TRUE`, and `FALSE` are all recognized as\nboolean. Occasionally you might want to recognize other values as being\nboolean. To do this, use the `true_values` and `false_values` options as\nfollows:\n\nSome files may have malformed lines with too few fields or too many. Lines\nwith too few fields will have NA values filled in the trailing fields. Lines\nwith too many fields will raise an error by default:\n\nYou can elect to skip bad lines:\n\nOr pass a callable function to handle the bad line if `engine=\"python\"`. The\nbad line will be a list of strings that was split by the `sep`:\n\nYou can also use the `usecols` parameter to eliminate extraneous column data\nthat appear in some lines but not others:\n\nIn case you want to keep all data including the lines with too many fields,\nyou can specify a sufficient number of `names`. This ensures that lines with\nnot enough fields are filled with `NaN`.\n\nThe `dialect` keyword gives greater flexibility in specifying the file format.\nBy default it uses the Excel dialect but you can specify either the dialect\nname or a `csv.Dialect` instance.\n\nSuppose you had data with unenclosed quotes:\n\nBy default, `read_csv` uses the Excel dialect and treats the double quote as\nthe quote character, which causes it to fail when it finds a newline before it\nfinds the closing double quote.\n\nWe can get around this using `dialect`:\n\nAll of the dialect options can be specified separately by keyword arguments:\n\nAnother common dialect option is `skipinitialspace`, to skip any whitespace\nafter a delimiter:\n\nThe parsers make every attempt to \u201cdo the right thing\u201d and not be fragile.\nType inference is a pretty big deal. If a column can be coerced to integer\ndtype without altering the contents, the parser will do so. Any non-numeric\ncolumns will come through as object dtype as with the rest of pandas objects.\n\nQuotes (and other escape characters) in embedded fields can be handled in any\nnumber of ways. One way is to use backslashes; to properly parse this data,\nyou should pass the `escapechar` option:\n\nWhile `read_csv()` reads delimited data, the `read_fwf()` function works with\ndata files that have known and fixed column widths. The function parameters to\n`read_fwf` are largely the same as `read_csv` with two extra parameters, and a\ndifferent usage of the `delimiter` parameter:\n\n`colspecs`: A list of pairs (tuples) giving the extents of the fixed-width\nfields of each line as half-open intervals (i.e., [from, to[ ). String value\n\u2018infer\u2019 can be used to instruct the parser to try detecting the column\nspecifications from the first 100 rows of the data. Default behavior, if not\nspecified, is to infer.\n\n`widths`: A list of field widths which can be used instead of \u2018colspecs\u2019 if\nthe intervals are contiguous.\n\n`delimiter`: Characters to consider as filler characters in the fixed-width\nfile. Can be used to specify the filler character of the fields if it is not\nspaces (e.g., \u2018~\u2019).\n\nConsider a typical fixed-width data file:\n\nIn order to parse this file into a `DataFrame`, we simply need to supply the\ncolumn specifications to the `read_fwf` function along with the file name:\n\nNote how the parser automatically picks column names X.<column number> when\n`header=None` argument is specified. Alternatively, you can supply just the\ncolumn widths for contiguous columns:\n\nThe parser will take care of extra white spaces around the columns so it\u2019s ok\nto have extra separation between the columns in the file.\n\nBy default, `read_fwf` will try to infer the file\u2019s `colspecs` by using the\nfirst 100 rows of the file. It can do it only in cases when the columns are\naligned and correctly separated by the provided `delimiter` (default delimiter\nis whitespace).\n\n`read_fwf` supports the `dtype` parameter for specifying the types of parsed\ncolumns to be different from the inferred type.\n\nConsider a file with one less entry in the header than the number of data\ncolumn:\n\nIn this special case, `read_csv` assumes that the first column is to be used\nas the index of the `DataFrame`:\n\nNote that the dates weren\u2019t automatically parsed. In that case you would need\nto do as before:\n\nSuppose you have data indexed by two columns:\n\nThe `index_col` argument to `read_csv` can take a list of column numbers to\nturn multiple columns into a `MultiIndex` for the index of the returned\nobject:\n\nBy specifying list of row locations for the `header` argument, you can read in\na `MultiIndex` for the columns. Specifying non-consecutive rows will skip the\nintervening rows.\n\n`read_csv` is also able to interpret a more common format of multi-columns\nindices.\n\nNote: If an `index_col` is not specified (e.g. you don\u2019t have an index, or\nwrote it with `df.to_csv(..., index=False)`, then any `names` on the columns\nindex will be lost.\n\n`read_csv` is capable of inferring delimited (not necessarily comma-separated)\nfiles, as pandas uses the `csv.Sniffer` class of the csv module. For this, you\nhave to specify `sep=None`.\n\nIt\u2019s best to use `concat()` to combine multiple files. See the cookbook for an\nexample.\n\nSuppose you wish to iterate through a (potentially very large) file lazily\nrather than reading the entire file into memory, such as the following:\n\nBy specifying a `chunksize` to `read_csv`, the return value will be an\niterable object of type `TextFileReader`:\n\nChanged in version 1.2: `read_csv/json/sas` return a context-manager when\niterating through a file.\n\nSpecifying `iterator=True` will also return the `TextFileReader` object:\n\nPandas currently supports three engines, the C engine, the python engine, and\nan experimental pyarrow engine (requires the `pyarrow` package). In general,\nthe pyarrow engine is fastest on larger workloads and is equivalent in speed\nto the C engine on most other workloads. The python engine tends to be slower\nthan the pyarrow and C engines on most workloads. However, the pyarrow engine\nis much less robust than the C engine, which lacks a few features compared to\nthe Python engine.\n\nWhere possible, pandas uses the C parser (specified as `engine='c'`), but it\nmay fall back to Python if C-unsupported options are specified.\n\nCurrently, options unsupported by the C and pyarrow engines include:\n\n`sep` other than a single character (e.g. regex separators)\n\n`skipfooter`\n\n`sep=None` with `delim_whitespace=False`\n\nSpecifying any of the above options will produce a `ParserWarning` unless the\npython engine is selected explicitly using `engine='python'`.\n\nOptions that are unsupported by the pyarrow engine which are not covered by\nthe list above include:\n\n`float_precision`\n\n`chunksize`\n\n`comment`\n\n`nrows`\n\n`thousands`\n\n`memory_map`\n\n`dialect`\n\n`warn_bad_lines`\n\n`error_bad_lines`\n\n`on_bad_lines`\n\n`delim_whitespace`\n\n`quoting`\n\n`lineterminator`\n\n`converters`\n\n`decimal`\n\n`iterator`\n\n`dayfirst`\n\n`infer_datetime_format`\n\n`verbose`\n\n`skipinitialspace`\n\n`low_memory`\n\nSpecifying these options with `engine='pyarrow'` will raise a `ValueError`.\n\nYou can pass in a URL to read or write remote files to many of pandas\u2019 IO\nfunctions - the following example shows reading a CSV file:\n\nNew in version 1.3.0.\n\nA custom header can be sent alongside HTTP(s) requests by passing a dictionary\nof header key value mappings to the `storage_options` keyword argument as\nshown below:\n\nAll URLs which are not local files or HTTP(s) are handled by fsspec, if\ninstalled, and its various filesystem implementations (including Amazon S3,\nGoogle Cloud, SSH, FTP, webHDFS\u2026). Some of these implementations will require\nadditional packages to be installed, for example S3 URLs require the s3fs\nlibrary:\n\nWhen dealing with remote storage systems, you might need extra configuration\nwith environment variables or config files in special locations. For example,\nto access data in your S3 bucket, you will need to define credentials in one\nof the several ways listed in the S3Fs documentation. The same is true for\nseveral of the storage backends, and you should follow the links at fsimpl1\nfor implementations built into `fsspec` and fsimpl2 for those not included in\nthe main `fsspec` distribution.\n\nYou can also pass parameters directly to the backend driver. For example, if\nyou do not have S3 credentials, you can still access public data by specifying\nan anonymous connection, such as\n\nNew in version 1.2.0.\n\n`fsspec` also allows complex URLs, for accessing data in compressed archives,\nlocal caching of files, and more. To locally cache the above example, you\nwould modify the call to\n\nwhere we specify that the \u201canon\u201d parameter is meant for the \u201cs3\u201d part of the\nimplementation, not to the caching implementation. Note that this caches to a\ntemporary directory for the duration of the session only, but you can also\nspecify a permanent store.\n\nThe `Series` and `DataFrame` objects have an instance method `to_csv` which\nallows storing the contents of the object as a comma-separated-values file.\nThe function takes a number of arguments. Only the first is required.\n\n`path_or_buf`: A string path to the file to write or a file object. If a file\nobject it must be opened with `newline=''`\n\n`sep` : Field delimiter for the output file (default \u201c,\u201d)\n\n`na_rep`: A string representation of a missing value (default \u2018\u2019)\n\n`float_format`: Format string for floating point numbers\n\n`columns`: Columns to write (default None)\n\n`header`: Whether to write out the column names (default True)\n\n`index`: whether to write row (index) names (default True)\n\n`index_label`: Column label(s) for index column(s) if desired. If None\n(default), and `header` and `index` are True, then the index names are used.\n(A sequence should be given if the `DataFrame` uses MultiIndex).\n\n`mode` : Python write mode, default \u2018w\u2019\n\n`encoding`: a string representing the encoding to use if the contents are non-\nASCII, for Python versions prior to 3\n\n`line_terminator`: Character sequence denoting line end (default `os.linesep`)\n\n`quoting`: Set quoting rules as in csv module (default csv.QUOTE_MINIMAL).\nNote that if you have set a `float_format` then floats are converted to\nstrings and csv.QUOTE_NONNUMERIC will treat them as non-numeric\n\n`quotechar`: Character used to quote fields (default \u2018\u201d\u2019)\n\n`doublequote`: Control quoting of `quotechar` in fields (default True)\n\n`escapechar`: Character used to escape `sep` and `quotechar` when appropriate\n(default None)\n\n`chunksize`: Number of rows to write at a time\n\n`date_format`: Format string for datetime objects\n\nThe `DataFrame` object has an instance method `to_string` which allows control\nover the string representation of the object. All arguments are optional:\n\n`buf` default None, for example a StringIO object\n\n`columns` default None, which columns to write\n\n`col_space` default None, minimum width of each column.\n\n`na_rep` default `NaN`, representation of NA value\n\n`formatters` default None, a dictionary (by column) of functions each of which\ntakes a single argument and returns a formatted string\n\n`float_format` default None, a function which takes a single (float) argument\nand returns a formatted string; to be applied to floats in the `DataFrame`.\n\n`sparsify` default True, set to False for a `DataFrame` with a hierarchical\nindex to print every MultiIndex key at each row.\n\n`index_names` default True, will print the names of the indices\n\n`index` default True, will print the index (ie, row labels)\n\n`header` default True, will print the column labels\n\n`justify` default `left`, will print column headers left- or right-justified\n\nThe `Series` object also has a `to_string` method, but with only the `buf`,\n`na_rep`, `float_format` arguments. There is also a `length` argument which,\nif set to `True`, will additionally output the length of the Series.\n\nRead and write `JSON` format files and strings.\n\nA `Series` or `DataFrame` can be converted to a valid JSON string. Use\n`to_json` with optional parameters:\n\n`path_or_buf` : the pathname or buffer to write the output This can be `None`\nin which case a JSON string is returned\n\n`orient` :\n\ndefault is `index`\n\nallowed values are {`split`, `records`, `index`}\n\ndefault is `columns`\n\nallowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}\n\nThe format of the JSON string\n\n`split`\n\ndict like {index -> [index], columns -> [columns], data -> [values]}\n\n`records`\n\nlist like [{column -> value}, \u2026 , {column -> value}]\n\n`index`\n\ndict like {index -> {column -> value}}\n\n`columns`\n\ndict like {column -> {index -> value}}\n\n`values`\n\njust the values array\n\n`table`\n\nadhering to the JSON Table Schema\n\n`date_format` : string, type of date conversion, \u2018epoch\u2019 for timestamp, \u2018iso\u2019\nfor ISO8601.\n\n`double_precision` : The number of decimal places to use when encoding\nfloating point values, default 10.\n\n`force_ascii` : force encoded string to be ASCII, default True.\n\n`date_unit` : The time unit to encode to, governs timestamp and ISO8601\nprecision. One of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 for seconds, milliseconds,\nmicroseconds and nanoseconds respectively. Default \u2018ms\u2019.\n\n`default_handler` : The handler to call if an object cannot otherwise be\nconverted to a suitable format for JSON. Takes a single argument, which is the\nobject to convert, and returns a serializable object.\n\n`lines` : If `records` orient, then will write each record per line as json.\n\nNote `NaN`\u2019s, `NaT`\u2019s and `None` will be converted to `null` and `datetime`\nobjects will be converted based on the `date_format` and `date_unit`\nparameters.\n\nThere are a number of different options for the format of the resulting JSON\nfile / string. Consider the following `DataFrame` and `Series`:\n\nColumn oriented (the default for `DataFrame`) serializes the data as nested\nJSON objects with column labels acting as the primary index:\n\nIndex oriented (the default for `Series`) similar to column oriented but the\nindex labels are now primary:\n\nRecord oriented serializes the data to a JSON array of column -> value\nrecords, index labels are not included. This is useful for passing `DataFrame`\ndata to plotting libraries, for example the JavaScript library `d3.js`:\n\nValue oriented is a bare-bones option which serializes to nested JSON arrays\nof values only, column and index labels are not included:\n\nSplit oriented serializes to a JSON object containing separate entries for\nvalues, index and columns. Name is also included for `Series`:\n\nTable oriented serializes to the JSON Table Schema, allowing for the\npreservation of metadata including but not limited to dtypes and index names.\n\nNote\n\nAny orient option that encodes to a JSON object will not preserve the ordering\nof index and column labels during round-trip serialization. If you wish to\npreserve label ordering use the `split` option as it uses ordered containers.\n\nWriting in ISO date format:\n\nWriting in ISO date format, with microseconds:\n\nEpoch timestamps, in seconds:\n\nWriting to a file, with a date index and a date column:\n\nIf the JSON serializer cannot handle the container contents directly it will\nfall back in the following manner:\n\nif the dtype is unsupported (e.g. `np.complex_`) then the `default_handler`,\nif provided, will be called for each value, otherwise an exception is raised.\n\nif an object is unsupported it will attempt the following:\n\ncheck if the object has defined a `toDict` method and call it. A `toDict`\nmethod should return a `dict` which will then be JSON serialized.\n\ninvoke the `default_handler` if one was provided.\n\nconvert the object to a `dict` by traversing its contents. However this will\noften fail with an `OverflowError` or give unexpected results.\n\nIn general the best approach for unsupported objects or dtypes is to provide a\n`default_handler`. For example:\n\ncan be dealt with by specifying a simple `default_handler`:\n\nReading a JSON string to pandas object can take a number of parameters. The\nparser will try to parse a `DataFrame` if `typ` is not supplied or is `None`.\nTo explicitly force `Series` parsing, pass `typ=series`\n\n`filepath_or_buffer` : a VALID JSON string or file handle / StringIO. The\nstring could be a URL. Valid URL schemes include http, ftp, S3, and file. For\nfile URLs, a host is expected. For instance, a local file could be file\n://localhost/path/to/table.json\n\n`typ` : type of object to recover (series or frame), default \u2018frame\u2019\n\n`orient` :\n\ndefault is `index`\n\nallowed values are {`split`, `records`, `index`}\n\ndefault is `columns`\n\nallowed values are {`split`, `records`, `index`, `columns`, `values`, `table`}\n\nThe format of the JSON string\n\n`split`\n\ndict like {index -> [index], columns -> [columns], data -> [values]}\n\n`records`\n\nlist like [{column -> value}, \u2026 , {column -> value}]\n\n`index`\n\ndict like {index -> {column -> value}}\n\n`columns`\n\ndict like {column -> {index -> value}}\n\n`values`\n\njust the values array\n\n`table`\n\nadhering to the JSON Table Schema\n\n`dtype` : if True, infer dtypes, if a dict of column to dtype, then use those,\nif `False`, then don\u2019t infer dtypes at all, default is True, apply only to the\ndata.\n\n`convert_axes` : boolean, try to convert the axes to the proper dtypes,\ndefault is `True`\n\n`convert_dates` : a list of columns to parse for dates; If `True`, then try to\nparse date-like columns, default is `True`.\n\n`keep_default_dates` : boolean, default `True`. If parsing dates, then parse\nthe default date-like columns.\n\n`numpy` : direct decoding to NumPy arrays. default is `False`; Supports\nnumeric data only, although labels may be non-numeric. Also note that the JSON\nordering MUST be the same for each term if `numpy=True`.\n\n`precise_float` : boolean, default `False`. Set to enable usage of higher\nprecision (strtod) function when decoding string to double values. Default\n(`False`) is to use fast but less precise builtin functionality.\n\n`date_unit` : string, the timestamp unit to detect if converting dates.\nDefault None. By default the timestamp precision will be detected, if this is\nnot desired then pass one of \u2018s\u2019, \u2018ms\u2019, \u2018us\u2019 or \u2018ns\u2019 to force timestamp\nprecision to seconds, milliseconds, microseconds or nanoseconds respectively.\n\n`lines` : reads file as one json object per line.\n\n`encoding` : The encoding to use to decode py3 bytes.\n\n`chunksize` : when used in combination with `lines=True`, return a JsonReader\nwhich reads in `chunksize` lines per iteration.\n\nThe parser will raise one of `ValueError/TypeError/AssertionError` if the JSON\nis not parseable.\n\nIf a non-default `orient` was used when encoding to JSON be sure to pass the\nsame option here so that decoding produces sensible results, see Orient\nOptions for an overview.\n\nThe default of `convert_axes=True`, `dtype=True`, and `convert_dates=True`\nwill try to parse the axes, and all of the data into appropriate types,\nincluding dates. If you need to override specific dtypes, pass a dict to\n`dtype`. `convert_axes` should only be set to `False` if you need to preserve\nstring-like numbers (e.g. \u20181\u2019, \u20182\u2019) in an axes.\n\nNote\n\nLarge integer values may be converted to dates if `convert_dates=True` and the\ndata and / or column labels appear \u2018date-like\u2019. The exact threshold depends on\nthe `date_unit` specified. \u2018date-like\u2019 means that the column label meets one\nof the following criteria:\n\nit ends with `'_at'`\n\nit ends with `'_time'`\n\nit begins with `'timestamp'`\n\nit is `'modified'`\n\nit is `'date'`\n\nWarning\n\nWhen reading JSON data, automatic coercing into dtypes has some quirks:\n\nan index can be reconstructed in a different order from serialization, that\nis, the returned order is not guaranteed to be the same as before\nserialization\n\na column that was `float` data will be converted to `integer` if it can be\ndone safely, e.g. a column of `1.`\n\nbool columns will be converted to `integer` on reconstruction\n\nThus there are times where you may want to specify specific dtypes via the\n`dtype` keyword argument.\n\nReading from a JSON string:\n\nReading from a file:\n\nDon\u2019t convert any data (but still convert axes and dates):\n\nSpecify dtypes for conversion:\n\nPreserve string indices:\n\nDates written in nanoseconds need to be read back in nanoseconds:\n\nNote\n\nThis param has been deprecated as of version 1.0.0 and will raise a\n`FutureWarning`.\n\nThis supports numeric data only. Index and columns labels may be non-numeric,\ne.g. strings, dates etc.\n\nIf `numpy=True` is passed to `read_json` an attempt will be made to sniff an\nappropriate dtype during deserialization and to subsequently decode directly\nto NumPy arrays, bypassing the need for intermediate Python objects.\n\nThis can provide speedups if you are deserialising a large amount of numeric\ndata:\n\nThe speedup is less noticeable for smaller datasets:\n\nWarning\n\nDirect NumPy decoding makes a number of assumptions and may fail or produce\nunexpected output if these assumptions are not satisfied:\n\ndata is numeric.\n\ndata is uniform. The dtype is sniffed from the first value decoded. A\n`ValueError` may be raised, or incorrect output may be produced if this\ncondition is not satisfied.\n\nlabels are ordered. Labels are only read from the first container, it is\nassumed that each subsequent row / column has been encoded in the same order.\nThis should be satisfied if the data was encoded using `to_json` but may not\nbe the case if the JSON is from another source.\n\npandas provides a utility function to take a dict or list of dicts and\nnormalize this semi-structured data into a flat table.\n\nThe max_level parameter provides more control over which level to end\nnormalization. With max_level=1 the following snippet normalizes until 1st\nnesting level of the provided dict.\n\npandas is able to read and write line-delimited json files that are common in\ndata processing pipelines using Hadoop or Spark.\n\nFor line-delimited json files, pandas can also return an iterator which reads\nin `chunksize` lines at a time. This can be useful for large files or to read\nfrom a stream.\n\nTable Schema is a spec for describing tabular datasets as a JSON object. The\nJSON includes information on the field names, types, and other attributes. You\ncan use the orient `table` to build a JSON string with two fields, `schema`\nand `data`.\n\nThe `schema` field contains the `fields` key, which itself contains a list of\ncolumn name to type pairs, including the `Index` or `MultiIndex` (see below\nfor a list of types). The `schema` field also contains a `primaryKey` field if\nthe (Multi)index is unique.\n\nThe second field, `data`, contains the serialized data with the `records`\norient. The index is included, and any datetimes are ISO 8601 formatted, as\nrequired by the Table Schema spec.\n\nThe full list of types supported are described in the Table Schema spec. This\ntable shows the mapping from pandas types:\n\npandas type\n\nTable Schema type\n\nint64\n\ninteger\n\nfloat64\n\nnumber\n\nbool\n\nboolean\n\ndatetime64[ns]\n\ndatetime\n\ntimedelta64[ns]\n\nduration\n\ncategorical\n\nany\n\nobject\n\nstr\n\nA few notes on the generated table schema:\n\nThe `schema` object contains a `pandas_version` field. This contains the\nversion of pandas\u2019 dialect of the schema, and will be incremented with each\nrevision.\n\nAll dates are converted to UTC when serializing. Even timezone naive values,\nwhich are treated as UTC with an offset of 0.\n\ndatetimes with a timezone (before serializing), include an additional field\n`tz` with the time zone name (e.g. `'US/Central'`).\n\nPeriods are converted to timestamps before serialization, and so have the same\nbehavior of being converted to UTC. In addition, periods will contain and\nadditional field `freq` with the period\u2019s frequency, e.g. `'A-DEC'`.\n\nCategoricals use the `any` type and an `enum` constraint listing the set of\npossible values. Additionally, an `ordered` field is included:\n\nA `primaryKey` field, containing an array of labels, is included if the index\nis unique:\n\nThe `primaryKey` behavior is the same with MultiIndexes, but in this case the\n`primaryKey` is an array:\n\nThe default naming roughly follows these rules:\n\nFor series, the `object.name` is used. If that\u2019s none, then the name is\n`values`\n\nFor `DataFrames`, the stringified version of the column name is used\n\nFor `Index` (not `MultiIndex`), `index.name` is used, with a fallback to\n`index` if that is None.\n\nFor `MultiIndex`, `mi.names` is used. If any level has no name, then\n`level_<i>` is used.\n\n`read_json` also accepts `orient='table'` as an argument. This allows for the\npreservation of metadata such as dtypes and index names in a round-trippable\nmanner.\n\nPlease note that the literal string \u2018index\u2019 as the name of an `Index` is not\nround-trippable, nor are any names beginning with `'level_'` within a\n`MultiIndex`. These are used by default in `DataFrame.to_json()` to indicate\nmissing values and the subsequent read cannot distinguish the intent.\n\nWhen using `orient='table'` along with user-defined `ExtensionArray`, the\ngenerated schema will contain an additional `extDtype` key in the respective\n`fields` element. This extra key is not standard but does enable JSON\nroundtrips for extension types (e.g. `read_json(df.to_json(orient=\"table\"),\norient=\"table\")`).\n\nThe `extDtype` key carries the name of the extension, if you have properly\nregistered the `ExtensionDtype`, pandas will use said name to perform a lookup\ninto the registry and re-convert the serialized data into your custom dtype.\n\nWarning\n\nWe highly encourage you to read the HTML Table Parsing gotchas below regarding\nthe issues surrounding the BeautifulSoup4/html5lib/lxml parsers.\n\nThe top-level `read_html()` function can accept an HTML string/file/URL and\nwill parse HTML tables into list of pandas `DataFrames`. Let\u2019s look at a few\nexamples.\n\nNote\n\n`read_html` returns a `list` of `DataFrame` objects, even if there is only a\nsingle table contained in the HTML content.\n\nRead a URL with no options:\n\nNote\n\nThe data from the above URL changes every Monday so the resulting data above\nand the data below may be slightly different.\n\nRead in the content of the file from the above URL and pass it to `read_html`\nas a string:\n\nYou can even pass in an instance of `StringIO` if you so desire:\n\nNote\n\nThe following examples are not run by the IPython evaluator due to the fact\nthat having so many network-accessing functions slows down the documentation\nbuild. If you spot an error or an example that doesn\u2019t run, please do not\nhesitate to report it over on pandas GitHub issues page.\n\nRead a URL and match a table that contains specific text:\n\nSpecify a header row (by default `<th>` or `<td>` elements located within a\n`<thead>` are used to form the column index, if multiple rows are contained\nwithin `<thead>` then a MultiIndex is created); if specified, the header row\nis taken from the data minus the parsed header elements (`<th>` elements).\n\nSpecify an index column:\n\nSpecify a number of rows to skip:\n\nSpecify a number of rows to skip using a list (`range` works as well):\n\nSpecify an HTML attribute:\n\nSpecify values that should be converted to NaN:\n\nSpecify whether to keep the default set of NaN values:\n\nSpecify converters for columns. This is useful for numerical text data that\nhas leading zeros. By default columns that are numerical are cast to numeric\ntypes and the leading zeros are lost. To avoid this, we can convert these\ncolumns to strings.\n\nUse some combination of the above:\n\nRead in pandas `to_html` output (with some loss of floating point precision):\n\nThe `lxml` backend will raise an error on a failed parse if that is the only\nparser you provide. If you only have a single parser you can provide just a\nstring, but it is considered good practice to pass a list with one string if,\nfor example, the function expects a sequence of strings. You may use:\n\nOr you could pass `flavor='lxml'` without a list:\n\nHowever, if you have bs4 and html5lib installed and pass `None` or `['lxml',\n'bs4']` then the parse will most likely succeed. Note that as soon as a parse\nsucceeds, the function will return.\n\n`DataFrame` objects have an instance method `to_html` which renders the\ncontents of the `DataFrame` as an HTML table. The function arguments are as in\nthe method `to_string` described above.\n\nNote\n\nNot all of the possible options for `DataFrame.to_html` are shown here for\nbrevity\u2019s sake. See `to_html()` for the full set of options.\n\nHTML:\n\nThe `columns` argument will limit the columns shown:\n\nHTML:\n\n`float_format` takes a Python callable to control the precision of floating\npoint values:\n\nHTML:\n\n`bold_rows` will make the row labels bold by default, but you can turn that\noff:\n\nThe `classes` argument provides the ability to give the resulting HTML table\nCSS classes. Note that these classes are appended to the existing\n`'dataframe'` class.\n\nThe `render_links` argument provides the ability to add hyperlinks to cells\nthat contain URLs.\n\nHTML:\n\nFinally, the `escape` argument allows you to control whether the \u201c<\u201d, \u201c>\u201d and\n\u201c&\u201d characters escaped in the resulting HTML (by default it is `True`). So to\nget the HTML without escaped characters pass `escape=False`\n\nEscaped:\n\nNot escaped:\n\nNote\n\nSome browsers may not show a difference in the rendering of the previous two\nHTML tables.\n\nThere are some versioning issues surrounding the libraries that are used to\nparse HTML tables in the top-level pandas io function `read_html`.\n\nIssues with lxml\n\nBenefits\n\nlxml is very fast.\n\nlxml requires Cython to install correctly.\n\nDrawbacks\n\nlxml does not make any guarantees about the results of its parse unless it is\ngiven strictly valid markup.\n\nIn light of the above, we have chosen to allow you, the user, to use the lxml\nbackend, but this backend will use html5lib if lxml fails to parse\n\nIt is therefore highly recommended that you install both BeautifulSoup4 and\nhtml5lib, so that you will still get a valid result (provided everything else\nis valid) even if lxml fails.\n\nIssues with BeautifulSoup4 using lxml as a backend\n\nThe above issues hold here as well since BeautifulSoup4 is essentially just a\nwrapper around a parser backend.\n\nIssues with BeautifulSoup4 using html5lib as a backend\n\nBenefits\n\nhtml5lib is far more lenient than lxml and consequently deals with real-life\nmarkup in a much saner way rather than just, e.g., dropping an element without\nnotifying you.\n\nhtml5lib generates valid HTML5 markup from invalid markup automatically. This\nis extremely important for parsing HTML tables, since it guarantees a valid\ndocument. However, that does NOT mean that it is \u201ccorrect\u201d, since the process\nof fixing markup does not have a single definition.\n\nhtml5lib is pure Python and requires no additional build steps beyond its own\ninstallation.\n\nDrawbacks\n\nThe biggest drawback to using html5lib is that it is slow as molasses. However\nconsider the fact that many tables on the web are not big enough for the\nparsing algorithm runtime to matter. It is more likely that the bottleneck\nwill be in the process of reading the raw text from the URL over the web,\ni.e., IO (input-output). For very large tables, this might not be true.\n\nNew in version 1.3.0.\n\nCurrently there are no methods to read from LaTeX, only output methods.\n\nNote\n\nDataFrame and Styler objects currently have a `to_latex` method. We recommend\nusing the Styler.to_latex() method over DataFrame.to_latex() due to the\nformer\u2019s greater flexibility with conditional styling, and the latter\u2019s\npossible future deprecation.\n\nReview the documentation for Styler.to_latex, which gives examples of\nconditional styling and explains the operation of its keyword arguments.\n\nFor simple application the following pattern is sufficient.\n\nTo format values before output, chain the Styler.format method.\n\nNew in version 1.3.0.\n\nThe top-level `read_xml()` function can accept an XML string/file/URL and will\nparse nodes and attributes into a pandas `DataFrame`.\n\nNote\n\nSince there is no standard XML structure where design types can vary in many\nways, `read_xml` works best with flatter, shallow versions. If an XML document\nis deeply nested, use the `stylesheet` feature to transform XML into a flatter\nversion.\n\nLet\u2019s look at a few examples.\n\nRead an XML string:\n\nRead a URL with no options:\n\nRead in the content of the \u201cbooks.xml\u201d file and pass it to `read_xml` as a\nstring:\n\nRead in the content of the \u201cbooks.xml\u201d as instance of `StringIO` or `BytesIO`\nand pass it to `read_xml`:\n\nEven read XML from AWS S3 buckets such as Python Software Foundation\u2019s IRS 990\nForm:\n\nWith lxml as default `parser`, you access the full-featured XML library that\nextends Python\u2019s ElementTree API. One powerful tool is ability to query nodes\nselectively or conditionally with more expressive XPath:\n\nSpecify only elements or only attributes to parse:\n\nXML documents can have namespaces with prefixes and default namespaces without\nprefixes both of which are denoted with a special attribute `xmlns`. In order\nto parse by node under a namespace context, `xpath` must reference a prefix.\n\nFor example, below XML contains a namespace with prefix, `doc`, and URI at\n`https://example.com`. In order to parse `doc:row` nodes, `namespaces` must be\nused.\n\nSimilarly, an XML document can have a default namespace without prefix.\nFailing to assign a temporary prefix will return no nodes and raise a\n`ValueError`. But assigning any temporary name to correct URI allows parsing\nby nodes.\n\nHowever, if XPath does not reference node names such as default, `/*`, then\n`namespaces` is not required.\n\nWith lxml as parser, you can flatten nested XML documents with an XSLT script\nwhich also can be string/file/URL types. As background, XSLT is a special-\npurpose language written in a special XML file that can transform original XML\ndocuments into other XML, HTML, even text (CSV, JSON, etc.) using an XSLT\nprocessor.\n\nFor example, consider this somewhat nested structure of Chicago \u201cL\u201d Rides\nwhere station and rides elements encapsulate data in their own sections. With\nbelow XSLT, `lxml` can transform original nested document into a flatter\noutput (as shown below for demonstration) for easier parse into `DataFrame`:\n\nNew in version 1.3.0.\n\n`DataFrame` objects have an instance method `to_xml` which renders the\ncontents of the `DataFrame` as an XML document.\n\nNote\n\nThis method does not support special properties of XML including DTD, CData,\nXSD schemas, processing instructions, comments, and others. Only namespaces at\nthe root level is supported. However, `stylesheet` allows design changes after\ninitial output.\n\nLet\u2019s look at a few examples.\n\nWrite an XML without options:\n\nWrite an XML with new root and row name:\n\nWrite an attribute-centric XML:\n\nWrite a mix of elements and attributes:\n\nAny `DataFrames` with hierarchical columns will be flattened for XML element\nnames with levels delimited by underscores:\n\nWrite an XML with default namespace:\n\nWrite an XML with namespace prefix:\n\nWrite an XML without declaration or pretty print:\n\nWrite an XML and transform with stylesheet:\n\nAll XML documents adhere to W3C specifications. Both `etree` and `lxml`\nparsers will fail to parse any markup document that is not well-formed or\nfollows XML syntax rules. Do be aware HTML is not an XML document unless it\nfollows XHTML specs. However, other popular markup types including KML, XAML,\nRSS, MusicML, MathML are compliant XML schemas.\n\nFor above reason, if your application builds XML prior to pandas operations,\nuse appropriate DOM libraries like `etree` and `lxml` to build the necessary\ndocument and not by string concatenation or regex adjustments. Always remember\nXML is a special text file with markup rules.\n\nWith very large XML files (several hundred MBs to GBs), XPath and XSLT can\nbecome memory-intensive operations. Be sure to have enough available RAM for\nreading and writing to large XML files (roughly about 5 times the size of\ntext).\n\nBecause XSLT is a programming language, use it with caution since such scripts\ncan pose a security risk in your environment and can run large or infinite\nrecursive operations. Always test scripts on small fragments before full run.\n\nThe etree parser supports all functionality of both `read_xml` and `to_xml`\nexcept for complex XPath and any XSLT. Though limited in features, `etree` is\nstill a reliable and capable parser and tree builder. Its performance may\ntrail `lxml` to a certain degree for larger files but relatively unnoticeable\non small to medium size files.\n\nThe `read_excel()` method can read Excel 2007+ (`.xlsx`) files using the\n`openpyxl` Python module. Excel 2003 (`.xls`) files can be read using `xlrd`.\nBinary Excel (`.xlsb`) files can be read using `pyxlsb`. The `to_excel()`\ninstance method is used for saving a `DataFrame` to Excel. Generally the\nsemantics are similar to working with csv data. See the cookbook for some\nadvanced strategies.\n\nWarning\n\nThe xlwt package for writing old-style `.xls` excel files is no longer\nmaintained. The xlrd package is now only for reading old-style `.xls` files.\n\nBefore pandas 1.3.0, the default argument `engine=None` to `read_excel()`\nwould result in using the `xlrd` engine in many cases, including new Excel\n2007+ (`.xlsx`) files. pandas will now default to using the openpyxl engine.\n\nIt is strongly encouraged to install `openpyxl` to read Excel 2007+ (`.xlsx`)\nfiles. Please do not report issues when using ``xlrd`` to read ``.xlsx``\nfiles. This is no longer supported, switch to using `openpyxl` instead.\n\nAttempting to use the the `xlwt` engine will raise a `FutureWarning` unless\nthe option `io.excel.xls.writer` is set to `\"xlwt\"`. While this option is now\ndeprecated and will also raise a `FutureWarning`, it can be globally set and\nthe warning suppressed. Users are recommended to write `.xlsx` files using the\n`openpyxl` engine instead.\n\nIn the most basic use-case, `read_excel` takes a path to an Excel file, and\nthe `sheet_name` indicating which sheet to parse.\n\nTo facilitate working with multiple sheets from the same file, the `ExcelFile`\nclass can be used to wrap the file and can be passed into `read_excel` There\nwill be a performance benefit for reading multiple sheets as the file is read\ninto memory only once.\n\nThe `ExcelFile` class can also be used as a context manager.\n\nThe `sheet_names` property will generate a list of the sheet names in the\nfile.\n\nThe primary use-case for an `ExcelFile` is parsing multiple sheets with\ndifferent parameters:\n\nNote that if the same parsing parameters are used for all sheets, a list of\nsheet names can simply be passed to `read_excel` with no loss in performance.\n\n`ExcelFile` can also be called with a `xlrd.book.Book` object as a parameter.\nThis allows the user to control how the excel file is read. For example,\nsheets can be loaded on demand by calling `xlrd.open_workbook()` with\n`on_demand=True`.\n\nNote\n\nThe second argument is `sheet_name`, not to be confused with\n`ExcelFile.sheet_names`.\n\nNote\n\nAn ExcelFile\u2019s attribute `sheet_names` provides access to a list of sheets.\n\nThe arguments `sheet_name` allows specifying the sheet or sheets to read.\n\nThe default value for `sheet_name` is 0, indicating to read the first sheet\n\nPass a string to refer to the name of a particular sheet in the workbook.\n\nPass an integer to refer to the index of a sheet. Indices follow Python\nconvention, beginning at 0.\n\nPass a list of either strings or integers, to return a dictionary of specified\nsheets.\n\nPass a `None` to return a dictionary of all available sheets.\n\nUsing the sheet index:\n\nUsing all default values:\n\nUsing None to get all sheets:\n\nUsing a list to get multiple sheets:\n\n`read_excel` can read more than one sheet, by setting `sheet_name` to either a\nlist of sheet names, a list of sheet positions, or `None` to read all sheets.\nSheets can be specified by sheet index or sheet name, using an integer or\nstring, respectively.\n\n`read_excel` can read a `MultiIndex` index, by passing a list of columns to\n`index_col` and a `MultiIndex` column by passing a list of rows to `header`.\nIf either the `index` or `columns` have serialized level names those will be\nread in as well by specifying the rows/columns that make up the levels.\n\nFor example, to read in a `MultiIndex` index without names:\n\nIf the index has level names, they will parsed as well, using the same\nparameters.\n\nIf the source file has both `MultiIndex` index and columns, lists specifying\neach should be passed to `index_col` and `header`:\n\nIt is often the case that users will insert columns to do temporary\ncomputations in Excel and you may not want to read in those columns.\n`read_excel` takes a `usecols` keyword to allow you to specify a subset of\ncolumns to parse.\n\nChanged in version 1.0.0.\n\nPassing in an integer for `usecols` will no longer work. Please pass in a list\nof ints from 0 to `usecols` inclusive instead.\n\nYou can specify a comma-delimited set of Excel columns and ranges as a string:\n\nIf `usecols` is a list of integers, then it is assumed to be the file column\nindices to be parsed.\n\nElement order is ignored, so `usecols=[0, 1]` is the same as `[1, 0]`.\n\nIf `usecols` is a list of strings, it is assumed that each string corresponds\nto a column name provided either by the user in `names` or inferred from the\ndocument header row(s). Those strings define which columns will be parsed:\n\nElement order is ignored, so `usecols=['baz', 'joe']` is the same as `['joe',\n'baz']`.\n\nIf `usecols` is callable, the callable function will be evaluated against the\ncolumn names, returning names where the callable function evaluates to `True`.\n\nDatetime-like values are normally automatically converted to the appropriate\ndtype when reading the excel file. But if you have a column of strings that\nlook like dates (but are not actually formatted as dates in excel), you can\nuse the `parse_dates` keyword to parse those strings to datetimes:\n\nIt is possible to transform the contents of Excel cells via the `converters`\noption. For instance, to convert a column to boolean:\n\nThis options handles missing values and treats exceptions in the converters as\nmissing data. Transformations are applied cell by cell rather than to the\ncolumn as a whole, so the array dtype is not guaranteed. For instance, a\ncolumn of integers with missing values cannot be transformed to an array with\ninteger dtype, because NaN is strictly a float. You can manually mask missing\ndata to recover integer dtype:\n\nAs an alternative to converters, the type for an entire column can be\nspecified using the `dtype` keyword, which takes a dictionary mapping column\nnames to types. To interpret data with no type inference, use the type `str`\nor `object`.\n\nTo write a `DataFrame` object to a sheet of an Excel file, you can use the\n`to_excel` instance method. The arguments are largely the same as `to_csv`\ndescribed above, the first argument being the name of the excel file, and the\noptional second argument the name of the sheet to which the `DataFrame` should\nbe written. For example:\n\nFiles with a `.xls` extension will be written using `xlwt` and those with a\n`.xlsx` extension will be written using `xlsxwriter` (if available) or\n`openpyxl`.\n\nThe `DataFrame` will be written in a way that tries to mimic the REPL output.\nThe `index_label` will be placed in the second row instead of the first. You\ncan place it in the first row by setting the `merge_cells` option in\n`to_excel()` to `False`:\n\nIn order to write separate `DataFrames` to separate sheets in a single Excel\nfile, one can pass an `ExcelWriter`.\n\npandas supports writing Excel files to buffer-like objects such as `StringIO`\nor `BytesIO` using `ExcelWriter`.\n\nNote\n\n`engine` is optional but recommended. Setting the engine determines the\nversion of workbook produced. Setting `engine='xlrd'` will produce an Excel\n2003-format workbook (xls). Using either `'openpyxl'` or `'xlsxwriter'` will\nproduce an Excel 2007-format workbook (xlsx). If omitted, an Excel\n2007-formatted workbook is produced.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed from a future version of pandas. This is the\nonly engine in pandas that supports writing to `.xls` files.\n\npandas chooses an Excel writer via two methods:\n\nthe `engine` keyword argument\n\nthe filename extension (via the default specified in config options)\n\nBy default, pandas uses the XlsxWriter for `.xlsx`, openpyxl for `.xlsm`, and\nxlwt for `.xls` files. If you have multiple engines installed, you can set the\ndefault engine through setting the config options `io.excel.xlsx.writer` and\n`io.excel.xls.writer`. pandas will fall back on openpyxl for `.xlsx` files if\nXlsxwriter is not available.\n\nTo specify which writer you want to use, you can pass an engine keyword\nargument to `to_excel` and to `ExcelWriter`. The built-in engines are:\n\n`openpyxl`: version 2.4 or higher is required\n\n`xlsxwriter`\n\n`xlwt`\n\nThe look and feel of Excel worksheets created from pandas can be modified\nusing the following parameters on the `DataFrame`\u2019s `to_excel` method.\n\n`float_format` : Format string for floating point numbers (default `None`).\n\n`freeze_panes` : A tuple of two integers representing the bottommost row and\nrightmost column to freeze. Each of these parameters is one-based, so (1, 1)\nwill freeze the first row and first column (default `None`).\n\nUsing the Xlsxwriter engine provides many options for controlling the format\nof an Excel worksheet created with the `to_excel` method. Excellent examples\ncan be found in the Xlsxwriter documentation here:\nhttps://xlsxwriter.readthedocs.io/working_with_pandas.html\n\nNew in version 0.25.\n\nThe `read_excel()` method can also read OpenDocument spreadsheets using the\n`odfpy` module. The semantics and features for reading OpenDocument\nspreadsheets match what can be done for Excel files using `engine='odf'`.\n\nNote\n\nCurrently pandas only supports reading OpenDocument spreadsheets. Writing is\nnot implemented.\n\nNew in version 1.0.0.\n\nThe `read_excel()` method can also read binary Excel files using the `pyxlsb`\nmodule. The semantics and features for reading binary Excel files mostly match\nwhat can be done for Excel files using `engine='pyxlsb'`. `pyxlsb` does not\nrecognize datetime types in files and will return floats instead.\n\nNote\n\nCurrently pandas only supports reading binary Excel files. Writing is not\nimplemented.\n\nA handy way to grab data is to use the `read_clipboard()` method, which takes\nthe contents of the clipboard buffer and passes them to the `read_csv` method.\nFor instance, you can copy the following text to the clipboard (CTRL-C on many\noperating systems):\n\nAnd then import the data directly to a `DataFrame` by calling:\n\nThe `to_clipboard` method can be used to write the contents of a `DataFrame`\nto the clipboard. Following which you can paste the clipboard contents into\nother applications (CTRL-V on many operating systems). Here we illustrate\nwriting a `DataFrame` into clipboard and reading it back.\n\nWe can see that we got the same content back, which we had earlier written to\nthe clipboard.\n\nNote\n\nYou may need to install xclip or xsel (with PyQt5, PyQt4 or qtpy) on Linux to\nuse these methods.\n\nAll pandas objects are equipped with `to_pickle` methods which use Python\u2019s\n`cPickle` module to save data structures to disk using the pickle format.\n\nThe `read_pickle` function in the `pandas` namespace can be used to load any\npickled pandas object (or any other pickled object) from file:\n\nWarning\n\nLoading pickled data received from untrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html\n\nWarning\n\n`read_pickle()` is only guaranteed backwards compatible back to pandas version\n0.20.3\n\n`read_pickle()`, `DataFrame.to_pickle()` and `Series.to_pickle()` can read and\nwrite compressed pickle files. The compression types of `gzip`, `bz2`, `xz`,\n`zstd` are supported for reading and writing. The `zip` file format only\nsupports reading and must contain only one data file to be read.\n\nThe compression type can be an explicit parameter or be inferred from the file\nextension. If \u2018infer\u2019, then use `gzip`, `bz2`, `zip`, `xz`, `zstd` if filename\nends in `'.gz'`, `'.bz2'`, `'.zip'`, `'.xz'`, or `'.zst'`, respectively.\n\nThe compression parameter can also be a `dict` in order to pass options to the\ncompression protocol. It must have a `'method'` key set to the name of the\ncompression protocol, which must be one of {`'zip'`, `'gzip'`, `'bz2'`,\n`'xz'`, `'zstd'`}. All other key-value pairs are passed to the underlying\ncompression library.\n\nUsing an explicit compression type:\n\nInferring compression type from the extension:\n\nThe default is to \u2018infer\u2019:\n\nPassing options to the compression protocol in order to speed up compression:\n\npandas support for `msgpack` has been removed in version 1.0.0. It is\nrecommended to use pickle instead.\n\nAlternatively, you can also the Arrow IPC serialization format for on-the-wire\ntransmission of pandas objects. For documentation on pyarrow, see here.\n\n`HDFStore` is a dict-like object which reads and writes pandas using the high\nperformance HDF5 format using the excellent PyTables library. See the cookbook\nfor some advanced strategies\n\nWarning\n\npandas uses PyTables for reading and writing HDF5 files, which allows\nserializing object-dtype data with pickle. Loading pickled data received from\nuntrusted sources can be unsafe.\n\nSee: https://docs.python.org/3/library/pickle.html for more.\n\nObjects can be written to the file just like adding key-value pairs to a dict:\n\nIn a current or later Python session, you can retrieve stored objects:\n\nDeletion of the object specified by the key:\n\nClosing a Store and using a context manager:\n\n`HDFStore` supports a top-level API using `read_hdf` for reading and `to_hdf`\nfor writing, similar to how `read_csv` and `to_csv` work.\n\nHDFStore will by default not drop rows that are all missing. This behavior can\nbe changed by setting `dropna=True`.\n\nThe examples above show storing using `put`, which write the HDF5 to\n`PyTables` in a fixed array format, called the `fixed` format. These types of\nstores are not appendable once written (though you can simply remove them and\nrewrite). Nor are they queryable; they must be retrieved in their entirety.\nThey also do not support dataframes with non-unique column names. The `fixed`\nformat stores offer very fast writing and slightly faster reading than `table`\nstores. This format is specified by default when using `put` or `to_hdf` or by\n`format='fixed'` or `format='f'`.\n\nWarning\n\nA `fixed` format will raise a `TypeError` if you try to retrieve using a\n`where`:\n\n`HDFStore` supports another `PyTables` format on disk, the `table` format.\nConceptually a `table` is shaped very much like a DataFrame, with rows and\ncolumns. A `table` may be appended to in the same or other sessions. In\naddition, delete and query type operations are supported. This format is\nspecified by `format='table'` or `format='t'` to `append` or `put` or\n`to_hdf`.\n\nThis format can be set as an option as well\n`pd.set_option('io.hdf.default_format','table')` to enable `put/append/to_hdf`\nto by default store in the `table` format.\n\nNote\n\nYou can also create a `table` by passing `format='table'` or `format='t'` to a\n`put` operation.\n\nKeys to a store can be specified as a string. These can be in a hierarchical\npath-name like format (e.g. `foo/bar/bah`), which will generate a hierarchy of\nsub-stores (or `Groups` in PyTables parlance). Keys can be specified without\nthe leading \u2018/\u2019 and are always absolute (e.g. \u2018foo\u2019 refers to \u2018/foo\u2019). Removal\noperations can remove everything in the sub-store and below, so be careful.\n\nYou can walk through the group hierarchy using the `walk` method which will\nyield a tuple for each group key along with the relative keys of its contents.\n\nWarning\n\nHierarchical keys cannot be retrieved as dotted (attribute) access as\ndescribed above for items stored under the root node.\n\nInstead, use explicit string based keys:\n\nStoring mixed-dtype data is supported. Strings are stored as a fixed-width\nusing the maximum size of the appended column. Subsequent attempts at\nappending longer strings will raise a `ValueError`.\n\nPassing `min_itemsize={`values`: size}` as a parameter to append will set a\nlarger minimum for the string columns. Storing `floats, strings, ints, bools,\ndatetime64` are currently supported. For string columns, passing `nan_rep =\n'nan'` to append will change the default nan representation on disk (which\nconverts to/from `np.nan`), this defaults to `nan`.\n\nStoring MultiIndex `DataFrames` as tables is very similar to storing/selecting\nfrom homogeneous index `DataFrames`.\n\nNote\n\nThe `index` keyword is reserved and cannot be use as a level name.\n\n`select` and `delete` operations have an optional criterion that can be\nspecified to select/delete only a subset of the data. This allows one to have\na very large on-disk table and retrieve only a portion of the data.\n\nA query is specified using the `Term` class under the hood, as a boolean\nexpression.\n\n`index` and `columns` are supported indexers of `DataFrames`.\n\nif `data_columns` are specified, these can be used as additional indexers.\n\nlevel name in a MultiIndex, with default name `level_0`, `level_1`, \u2026 if not\nprovided.\n\nValid comparison operators are:\n\n`=, ==, !=, >, >=, <, <=`\n\nValid boolean expressions are combined with:\n\n`|` : or\n\n`&` : and\n\n`(` and `)` : for grouping\n\nThese rules are similar to how boolean expressions are used in pandas for\nindexing.\n\nNote\n\n`=` will be automatically expanded to the comparison operator `==`\n\n`~` is the not operator, but can only be used in very limited circumstances\n\nIf a list/tuple of expressions is passed they will be combined via `&`\n\nThe following are valid expressions:\n\n`'index >= date'`\n\n`\"columns = ['A', 'D']\"`\n\n`\"columns in ['A', 'D']\"`\n\n`'columns = A'`\n\n`'columns == A'`\n\n`\"~(columns = ['A', 'B'])\"`\n\n`'index > df.index[3] & string = \"bar\"'`\n\n`'(index > df.index[3] & index <= df.index[6]) | string = \"bar\"'`\n\n`\"ts >= Timestamp('2012-02-01')\"`\n\n`\"major_axis>=20130101\"`\n\nThe `indexers` are on the left-hand side of the sub-expression:\n\n`columns`, `major_axis`, `ts`\n\nThe right-hand side of the sub-expression (after a comparison operator) can\nbe:\n\nfunctions that will be evaluated, e.g. `Timestamp('2012-02-01')`\n\nstrings, e.g. `\"bar\"`\n\ndate-like, e.g. `20130101`, or `\"20130101\"`\n\nlists, e.g. `\"['A', 'B']\"`\n\nvariables that are defined in the local names space, e.g. `date`\n\nNote\n\nPassing a string to a query by interpolating it into the query expression is\nnot recommended. Simply assign the string of interest to a variable and use\nthat variable in an expression. For example, do this\n\ninstead of this\n\nThe latter will not work and will raise a `SyntaxError`.Note that there\u2019s a\nsingle quote followed by a double quote in the `string` variable.\n\nIf you must interpolate, use the `'%r'` format specifier\n\nwhich will quote `string`.\n\nHere are some examples:\n\nUse boolean expressions, with in-line function evaluation.\n\nUse inline column reference.\n\nThe `columns` keyword can be supplied to select a list of columns to be\nreturned, this is equivalent to passing a\n`'columns=list_of_columns_to_filter'`:\n\n`start` and `stop` parameters can be specified to limit the total search\nspace. These are in terms of the total number of rows in a table.\n\nNote\n\n`select` will raise a `ValueError` if the query expression has an unknown\nvariable reference. Usually this means that you are trying to select on a\ncolumn that is not a data_column.\n\n`select` will raise a `SyntaxError` if the query expression is not valid.\n\nYou can store and query using the `timedelta64[ns]` type. Terms can be\nspecified in the format: `<float>(<unit>)`, where float may be signed (and\nfractional), and unit can be `D,s,ms,us,ns` for the timedelta. Here\u2019s an\nexample:\n\nSelecting from a `MultiIndex` can be achieved by using the name of the level.\n\nIf the `MultiIndex` levels names are `None`, the levels are automatically made\navailable via the `level_n` keyword with `n` the level of the `MultiIndex` you\nwant to select from.\n\nYou can create/modify an index for a table with `create_table_index` after\ndata is already in the table (after and `append/put` operation). Creating a\ntable index is highly encouraged. This will speed your queries a great deal\nwhen you use a `select` with the indexed dimension as the `where`.\n\nNote\n\nIndexes are automagically created on the indexables and any data columns you\nspecify. This behavior can be turned off by passing `index=False` to `append`.\n\nOftentimes when appending large amounts of data to a store, it is useful to\nturn off index creation for each append, then recreate at the end.\n\nThen create the index when finished appending.\n\nSee here for how to create a completely-sorted-index (CSI) on an existing\nstore.\n\nYou can designate (and index) certain columns that you want to be able to\nperform queries (other than the `indexable` columns, which you can always\nquery). For instance say you want to perform this common operation, on-disk,\nand return just the frame that matches this query. You can specify\n`data_columns = True` to force all columns to be `data_columns`.\n\nThere is some performance degradation by making lots of columns into `data\ncolumns`, so it is up to the user to designate these. In addition, you cannot\nchange data columns (nor indexables) after the first append/put operation (Of\ncourse you can simply read in the data and create a new table!).\n\nYou can pass `iterator=True` or `chunksize=number_in_a_chunk` to `select` and\n`select_as_multiple` to return an iterator on the results. The default is\n50,000 rows returned in a chunk.\n\nNote\n\nYou can also use the iterator with `read_hdf` which will open, then\nautomatically close the store when finished iterating.\n\nNote, that the chunksize keyword applies to the source rows. So if you are\ndoing a query, then the chunksize will subdivide the total rows in the table\nand the query applied, returning an iterator on potentially unequal sized\nchunks.\n\nHere is a recipe for generating a query and using it to create equal sized\nreturn chunks.\n\nTo retrieve a single indexable or data column, use the method `select_column`.\nThis will, for example, enable you to get the index very quickly. These return\na `Series` of the result, indexed by the row number. These do not currently\naccept the `where` selector.\n\nSometimes you want to get the coordinates (a.k.a the index locations) of your\nquery. This returns an `Int64Index` of the resulting locations. These\ncoordinates can also be passed to subsequent `where` operations.\n\nSometime your query can involve creating a list of rows to select. Usually\nthis `mask` would be a resulting `index` from an indexing operation. This\nexample selects the months of a datetimeindex which are 5.\n\nIf you want to inspect the stored object, retrieve via `get_storer`. You could\nuse this programmatically to say get the number of rows in an object.\n\nThe methods `append_to_multiple` and `select_as_multiple` can perform\nappending/selecting from multiple tables at once. The idea is to have one\ntable (call it the selector table) that you index most/all of the columns, and\nperform your queries. The other table(s) are data tables with an index\nmatching the selector table\u2019s index. You can then perform a very fast query on\nthe selector table, yet get lots of data back. This method is similar to\nhaving a very wide table, but enables more efficient queries.\n\nThe `append_to_multiple` method splits a given single DataFrame into multiple\ntables according to `d`, a dictionary that maps the table names to a list of\n\u2018columns\u2019 you want in that table. If `None` is used in place of a list, that\ntable will have the remaining unspecified columns of the given DataFrame. The\nargument `selector` defines which table is the selector table (which you can\nmake queries from). The argument `dropna` will drop rows from the input\n`DataFrame` to ensure tables are synchronized. This means that if a row for\none of the tables being written to is entirely `np.NaN`, that row will be\ndropped from all tables.\n\nIf `dropna` is False, THE USER IS RESPONSIBLE FOR SYNCHRONIZING THE TABLES.\nRemember that entirely `np.Nan` rows are not written to the HDFStore, so if\nyou choose to call `dropna=False`, some tables may have more rows than others,\nand therefore `select_as_multiple` may not work or it may return unexpected\nresults.\n\nYou can delete from a table selectively by specifying a `where`. In deleting\nrows, it is important to understand the `PyTables` deletes rows by erasing the\nrows, then moving the following data. Thus deleting can potentially be a very\nexpensive operation depending on the orientation of your data. To get optimal\nperformance, it\u2019s worthwhile to have the dimension you are deleting be the\nfirst of the `indexables`.\n\nData is ordered (on the disk) in terms of the `indexables`. Here\u2019s a simple\nuse case. You store panel-type data, with dates in the `major_axis` and ids in\nthe `minor_axis`. The data is then interleaved like this:\n\nid_1\n\nid_2\n\n.\n\nid_n\n\nid_1\n\n.\n\nid_n\n\nIt should be clear that a delete operation on the `major_axis` will be fairly\nquick, as one chunk is removed, then the following data moved. On the other\nhand a delete operation on the `minor_axis` will be very expensive. In this\ncase it would almost certainly be faster to rewrite the table using a `where`\nthat selects all but the missing data.\n\nWarning\n\nPlease note that HDF5 DOES NOT RECLAIM SPACE in the h5 files automatically.\nThus, repeatedly deleting (or removing nodes) and adding again, WILL TEND TO\nINCREASE THE FILE SIZE.\n\nTo repack and clean the file, use ptrepack.\n\n`PyTables` allows the stored data to be compressed. This applies to all kinds\nof stores, not just tables. Two parameters are used to control compression:\n`complevel` and `complib`.\n\n`complevel` specifies if and how hard data is to be compressed. `complevel=0`\nand `complevel=None` disables compression and `0<complevel<10` enables\ncompression.\n\n`complib` specifies which compression library to use. If nothing is specified\nthe default library `zlib` is used. A compression library usually optimizes\nfor either good compression rates or speed and the results will depend on the\ntype of data. Which type of compression to choose depends on your specific\nneeds and data. The list of supported compression libraries:\n\nzlib: The default compression library. A classic in terms of compression,\nachieves good compression rates but is somewhat slow.\n\nlzo: Fast compression and decompression.\n\nbzip2: Good compression rates.\n\nblosc: Fast compression and decompression.\n\nSupport for alternative blosc compressors:\n\nblosc:blosclz This is the default compressor for `blosc`\n\nblosc:lz4: A compact, very popular and fast compressor.\n\nblosc:lz4hc: A tweaked version of LZ4, produces better compression ratios at\nthe expense of speed.\n\nblosc:snappy: A popular compressor used in many places.\n\nblosc:zlib: A classic; somewhat slower than the previous ones, but achieving\nbetter compression ratios.\n\nblosc:zstd: An extremely well balanced codec; it provides the best compression\nratios among the others above, and at reasonably fast speed.\n\nIf `complib` is defined as something other than the listed libraries a\n`ValueError` exception is issued.\n\nNote\n\nIf the library specified with the `complib` option is missing on your\nplatform, compression defaults to `zlib` without further ado.\n\nEnable compression for all objects within the file:\n\nOr on-the-fly compression (this only applies to tables) in stores where\ncompression is not enabled:\n\n`PyTables` offers better write performance when tables are compressed after\nthey are written, as opposed to turning on compression at the very beginning.\nYou can use the supplied `PyTables` utility `ptrepack`. In addition,\n`ptrepack` can change compression levels after the fact.\n\nFurthermore `ptrepack in.h5 out.h5` will repack the file to allow you to reuse\npreviously deleted space. Alternatively, one can simply remove the file and\nwrite again, or use the `copy` method.\n\nWarning\n\n`HDFStore` is not-threadsafe for writing. The underlying `PyTables` only\nsupports concurrent reads (via threading or processes). If you need reading\nand writing at the same time, you need to serialize these operations in a\nsingle thread in a single process. You will corrupt your data otherwise. See\nthe (GH2397) for more information.\n\nIf you use locks to manage write access between multiple processes, you may\nwant to use `fsync()` before releasing write locks. For convenience you can\nuse `store.flush(fsync=True)` to do this for you.\n\nOnce a `table` is created columns (DataFrame) are fixed; only exactly the same\ncolumns can be appended\n\nBe aware that timezones (e.g., `pytz.timezone('US/Eastern')`) are not\nnecessarily equal across timezone versions. So if data is localized to a\nspecific timezone in the HDFStore using one version of a timezone library and\nthat data is updated with another version, the data will be converted to UTC\nsince these timezones are not considered equal. Either use the same version of\ntimezone library or use `tz_convert` with the updated timezone definition.\n\nWarning\n\n`PyTables` will show a `NaturalNameWarning` if a column name cannot be used as\nan attribute selector. Natural identifiers contain only letters, numbers, and\nunderscores, and may not begin with a number. Other identifiers cannot be used\nin a `where` clause and are generally a bad idea.\n\n`HDFStore` will map an object dtype to the `PyTables` underlying dtype. This\nmeans the following types are known to work:\n\nType\n\nRepresents missing values\n\nfloating : `float64, float32, float16`\n\n`np.nan`\n\ninteger : `int64, int32, int8, uint64,uint32, uint8`\n\nboolean\n\n`datetime64[ns]`\n\n`NaT`\n\n`timedelta64[ns]`\n\n`NaT`\n\ncategorical : see the section below\n\nobject : `strings`\n\n`np.nan`\n\n`unicode` columns are not supported, and WILL FAIL.\n\nYou can write data that contains `category` dtypes to a `HDFStore`. Queries\nwork the same as if it was an object array. However, the `category` dtyped\ndata is stored in a more efficient manner.\n\nmin_itemsize\n\nThe underlying implementation of `HDFStore` uses a fixed column width\n(itemsize) for string columns. A string column itemsize is calculated as the\nmaximum of the length of data (for that column) that is passed to the\n`HDFStore`, in the first append. Subsequent appends, may introduce a string\nfor a column larger than the column can hold, an Exception will be raised\n(otherwise you could have a silent truncation of these columns, leading to\nloss of information). In the future we may relax this and allow a user-\nspecified truncation to occur.\n\nPass `min_itemsize` on the first table creation to a-priori specify the\nminimum length of a particular string column. `min_itemsize` can be an\ninteger, or a dict mapping a column name to an integer. You can pass `values`\nas a key to allow all indexables or data_columns to have this min_itemsize.\n\nPassing a `min_itemsize` dict will cause all passed columns to be created as\ndata_columns automatically.\n\nNote\n\nIf you are not passing any `data_columns`, then the `min_itemsize` will be the\nmaximum of the length of any string passed\n\nnan_rep\n\nString columns will serialize a `np.nan` (a missing value) with the `nan_rep`\nstring representation. This defaults to the string value `nan`. You could\ninadvertently turn an actual `nan` value into a missing value.\n\n`HDFStore` writes `table` format objects in specific formats suitable for\nproducing loss-less round trips to pandas objects. For external compatibility,\n`HDFStore` can read native `PyTables` format tables.\n\nIt is possible to write an `HDFStore` object that can easily be imported into\n`R` using the `rhdf5` library (Package website). Create a table format store\nlike this:\n\nIn R this file can be read into a `data.frame` object using the `rhdf5`\nlibrary. The following example function reads the corresponding column names\nand data values from the values and assembles them into a `data.frame`:\n\nNow you can import the `DataFrame` into R:\n\nNote\n\nThe R function lists the entire HDF5 file\u2019s contents and assembles the\n`data.frame` object from all matching nodes, so use this only as a starting\npoint if you have stored multiple `DataFrame` objects to a single HDF5 file.\n\n`tables` format come with a writing performance penalty as compared to `fixed`\nstores. The benefit is the ability to append/delete and query (potentially\nvery large amounts of data). Write times are generally longer as compared with\nregular stores. Query times can be quite fast, especially on an indexed axis.\n\nYou can pass `chunksize=<int>` to `append`, specifying the write chunksize\n(default is 50000). This will significantly lower your memory usage on\nwriting.\n\nYou can pass `expectedrows=<int>` to the first `append`, to set the TOTAL\nnumber of rows that `PyTables` will expect. This will optimize read/write\nperformance.\n\nDuplicate rows can be written to tables, but are filtered out in selection\n(with the last items being selected; thus a table is unique on major, minor\npairs)\n\nA `PerformanceWarning` will be raised if you are attempting to store types\nthat will be pickled by PyTables (rather than stored as endemic types). See\nHere for more information and some solutions.\n\nFeather provides binary columnar serialization for data frames. It is designed\nto make reading and writing data frames efficient, and to make sharing data\nacross data analysis languages easy.\n\nFeather is designed to faithfully serialize and de-serialize DataFrames,\nsupporting all of the pandas dtypes, including extension dtypes such as\ncategorical and datetime with tz.\n\nSeveral caveats:\n\nThe format will NOT write an `Index`, or `MultiIndex` for the `DataFrame` and\nwill raise an error if a non-default one is provided. You can `.reset_index()`\nto store the index or `.reset_index(drop=True)` to ignore it.\n\nDuplicate column names and non-string columns names are not supported\n\nActual Python objects in object dtype columns are not supported. These will\nraise a helpful error message on an attempt at serialization.\n\nSee the Full Documentation.\n\nWrite to a feather file.\n\nRead from a feather file.\n\nApache Parquet provides a partitioned binary columnar serialization for data\nframes. It is designed to make reading and writing data frames efficient, and\nto make sharing data across data analysis languages easy. Parquet can use a\nvariety of compression techniques to shrink the file size as much as possible\nwhile still maintaining good read performance.\n\nParquet is designed to faithfully serialize and de-serialize `DataFrame` s,\nsupporting all of the pandas dtypes, including extension dtypes such as\ndatetime with tz.\n\nSeveral caveats.\n\nDuplicate column names and non-string columns names are not supported.\n\nThe `pyarrow` engine always writes the index to the output, but `fastparquet`\nonly writes non-default indexes. This extra column can cause problems for non-\npandas consumers that are not expecting it. You can force including or\nomitting indexes with the `index` argument, regardless of the underlying\nengine.\n\nIndex level names, if specified, must be strings.\n\nIn the `pyarrow` engine, categorical dtypes for non-string types can be\nserialized to parquet, but will de-serialize as their primitive dtype.\n\nThe `pyarrow` engine preserves the `ordered` flag of categorical dtypes with\nstring types. `fastparquet` does not preserve the `ordered` flag.\n\nNon supported types include `Interval` and actual Python object types. These\nwill raise a helpful error message on an attempt at serialization. `Period`\ntype is supported with pyarrow >= 0.16.0.\n\nThe `pyarrow` engine preserves extension data types such as the nullable\ninteger and string data type (requiring pyarrow >= 0.16.0, and requiring the\nextension type to implement the needed protocols, see the extension types\ndocumentation).\n\nYou can specify an `engine` to direct the serialization. This can be one of\n`pyarrow`, or `fastparquet`, or `auto`. If the engine is NOT specified, then\nthe `pd.options.io.parquet.engine` option is checked; if this is also `auto`,\nthen `pyarrow` is tried, and falling back to `fastparquet`.\n\nSee the documentation for pyarrow and fastparquet.\n\nNote\n\nThese engines are very similar and should read/write nearly identical parquet\nformat files. Currently `pyarrow` does not support timedelta data,\n`fastparquet>=0.1.4` supports timezone aware datetimes. These libraries differ\nby having different underlying dependencies (`fastparquet` by using `numba`,\nwhile `pyarrow` uses a c-library).\n\nWrite to a parquet file.\n\nRead from a parquet file.\n\nRead only certain columns of a parquet file.\n\nSerializing a `DataFrame` to parquet may include the implicit index as one or\nmore columns in the output file. Thus, this code:\n\ncreates a parquet file with three columns if you use `pyarrow` for\nserialization: `a`, `b`, and `__index_level_0__`. If you\u2019re using\n`fastparquet`, the index may or may not be written to the file.\n\nThis unexpected extra column causes some databases like Amazon Redshift to\nreject the file, because that column doesn\u2019t exist in the target table.\n\nIf you want to omit a dataframe\u2019s indexes when writing, pass `index=False` to\n`to_parquet()`:\n\nThis creates a parquet file with just the two expected columns, `a` and `b`.\nIf your `DataFrame` has a custom index, you won\u2019t get it back when you load\nthis file into a `DataFrame`.\n\nPassing `index=True` will always write the index, even if that\u2019s not the\nunderlying engine\u2019s default behavior.\n\nParquet supports partitioning of data based on the values of one or more\ncolumns.\n\nThe `path` specifies the parent directory to which data will be saved. The\n`partition_cols` are the column names by which the dataset will be\npartitioned. Columns are partitioned in the order they are given. The\npartition splits are determined by the unique values in the partition columns.\nThe above example creates a partitioned dataset that may look like:\n\nNew in version 1.0.0.\n\nSimilar to the parquet format, the ORC Format is a binary columnar\nserialization for data frames. It is designed to make reading data frames\nefficient. pandas provides only a reader for the ORC format, `read_orc()`.\nThis requires the pyarrow library.\n\nWarning\n\nIt is highly recommended to install pyarrow using conda due to some issues\noccurred by pyarrow.\n\n`read_orc()` is not supported on Windows yet, you can find valid environments\non install optional dependencies.\n\nThe `pandas.io.sql` module provides a collection of query wrappers to both\nfacilitate data retrieval and to reduce dependency on DB-specific API.\nDatabase abstraction is provided by SQLAlchemy if installed. In addition you\nwill need a driver library for your database. Examples of such drivers are\npsycopg2 for PostgreSQL or pymysql for MySQL. For SQLite this is included in\nPython\u2019s standard library by default. You can find an overview of supported\ndrivers for each SQL dialect in the SQLAlchemy docs.\n\nIf SQLAlchemy is not installed, a fallback is only provided for sqlite (and\nfor mysql for backwards compatibility, but this is deprecated and will be\nremoved in a future version). This mode requires a Python database adapter\nwhich respect the Python DB-API.\n\nSee also some cookbook examples for some advanced strategies.\n\nThe key functions are:\n\n`read_sql_table`(table_name, con[, schema, ...])\n\nRead SQL database table into a DataFrame.\n\n`read_sql_query`(sql, con[, index_col, ...])\n\nRead SQL query into a DataFrame.\n\n`read_sql`(sql, con[, index_col, ...])\n\nRead SQL query or database table into a DataFrame.\n\n`DataFrame.to_sql`(name, con[, schema, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\nNote\n\nThe function `read_sql()` is a convenience wrapper around `read_sql_table()`\nand `read_sql_query()` (and for backward compatibility) and will delegate to\nspecific function depending on the provided input (database table name or sql\nquery). Table names do not need to be quoted if they have special characters.\n\nIn the following example, we use the SQlite SQL database engine. You can use a\ntemporary SQLite database where data are stored in \u201cmemory\u201d.\n\nTo connect with SQLAlchemy you use the `create_engine()` function to create an\nengine object from database URI. You only need to create the engine once per\ndatabase you are connecting to. For more information on `create_engine()` and\nthe URI formatting, see the examples below and the SQLAlchemy documentation\n\nIf you want to manage your own connections you can pass one of those instead.\nThe example below opens a connection to the database using a Python context\nmanager that automatically closes the connection after the block has\ncompleted. See the SQLAlchemy docs for an explanation of how the database\nconnection is handled.\n\nWarning\n\nWhen you open a connection to a database you are also responsible for closing\nit. Side effects of leaving a connection open may include locking the database\nor other breaking behaviour.\n\nAssuming the following data is in a `DataFrame` `data`, we can insert it into\nthe database using `to_sql()`.\n\nid\n\nDate\n\nCol_1\n\nCol_2\n\nCol_3\n\n26\n\n2012-10-18\n\nX\n\n25.7\n\nTrue\n\n42\n\n2012-10-19\n\nY\n\n-12.4\nFalse\n\n63\n\n2012-10-20\n\nZ\n\n5.73\n\nTrue\n\nWith some databases, writing large DataFrames can result in errors due to\npacket size limitations being exceeded. This can be avoided by setting the\n`chunksize` parameter when calling `to_sql`. For example, the following writes\n`data` to the database in batches of 1000 rows at a time:\n\n`to_sql()` will try to map your data to an appropriate SQL data type based on\nthe dtype of the data. When you have columns of dtype `object`, pandas will\ntry to infer the data type.\n\nYou can always override the default type by specifying the desired SQL type of\nany of the columns by using the `dtype` argument. This argument needs a\ndictionary mapping column names to SQLAlchemy types (or strings for the\nsqlite3 fallback mode). For example, specifying to use the sqlalchemy `String`\ntype instead of the default `Text` type for string columns:\n\nNote\n\nDue to the limited support for timedelta\u2019s in the different database flavors,\ncolumns with type `timedelta64` will be written as integer values as\nnanoseconds to the database and a warning will be raised.\n\nNote\n\nColumns of `category` dtype will be converted to the dense representation as\nyou would get with `np.asarray(categorical)` (e.g. for string categories this\ngives an array of strings). Because of this, reading the database table back\nin does not generate a categorical.\n\nUsing SQLAlchemy, `to_sql()` is capable of writing datetime data that is\ntimezone naive or timezone aware. However, the resulting data stored in the\ndatabase ultimately depends on the supported data type for datetime data of\nthe database system being used.\n\nThe following table lists supported data types for datetime data for some\ncommon databases. Other database dialects may have different data types for\ndatetime data.\n\nDatabase\n\nSQL Datetime Types\n\nTimezone Support\n\nSQLite\n\n`TEXT`\n\nNo\n\nMySQL\n\n`TIMESTAMP` or `DATETIME`\n\nNo\n\nPostgreSQL\n\n`TIMESTAMP` or `TIMESTAMP WITH TIME ZONE`\n\nYes\n\nWhen writing timezone aware data to databases that do not support timezones,\nthe data will be written as timezone naive timestamps that are in local time\nwith respect to the timezone.\n\n`read_sql_table()` is also capable of reading datetime data that is timezone\naware or naive. When reading `TIMESTAMP WITH TIME ZONE` types, pandas will\nconvert the data to UTC.\n\nThe parameter `method` controls the SQL insertion clause used. Possible values\nare:\n\n`None`: Uses standard SQL `INSERT` clause (one per row).\n\n`'multi'`: Pass multiple values in a single `INSERT` clause. It uses a special\nSQL syntax not supported by all backends. This usually provides better\nperformance for analytic databases like Presto and Redshift, but has worse\nperformance for traditional SQL backend if the table contains many columns.\nFor more information check the SQLAlchemy documentation.\n\ncallable with signature `(pd_table, conn, keys, data_iter)`: This can be used\nto implement a more performant insertion method based on specific backend\ndialect features.\n\nExample of a callable using PostgreSQL COPY clause:\n\n`read_sql_table()` will read a database table given the table name and\noptionally a subset of columns to read.\n\nNote\n\nIn order to use `read_sql_table()`, you must have the SQLAlchemy optional\ndependency installed.\n\nNote\n\nNote that pandas infers column dtypes from query outputs, and not by looking\nup data types in the physical database schema. For example, assume `userid` is\nan integer column in a table. Then, intuitively, `select userid ...` will\nreturn integer-valued series, while `select cast(userid as text) ...` will\nreturn object-valued (str) series. Accordingly, if the query output is empty,\nthen all resulting columns will be returned as object-valued (since they are\nmost general). If you foresee that your query will sometimes generate an empty\nresult, you may want to explicitly typecast afterwards to ensure dtype\nintegrity.\n\nYou can also specify the name of the column as the `DataFrame` index, and\nspecify a subset of columns to be read.\n\nAnd you can explicitly force columns to be parsed as dates:\n\nIf needed you can explicitly specify a format string, or a dict of arguments\nto pass to `pandas.to_datetime()`:\n\nYou can check if a table exists using `has_table()`\n\nReading from and writing to different schema\u2019s is supported through the\n`schema` keyword in the `read_sql_table()` and `to_sql()` functions. Note\nhowever that this depends on the database flavor (sqlite does not have\nschema\u2019s). For example:\n\nYou can query using raw SQL in the `read_sql_query()` function. In this case\nyou must use the SQL variant appropriate for your database. When using\nSQLAlchemy, you can also pass SQLAlchemy Expression language constructs, which\nare database-agnostic.\n\nOf course, you can specify a more \u201ccomplex\u201d query.\n\nThe `read_sql_query()` function supports a `chunksize` argument. Specifying\nthis will return an iterator through chunks of the query result:\n\nYou can also run a plain query without creating a `DataFrame` with\n`execute()`. This is useful for queries that don\u2019t return values, such as\nINSERT. This is functionally equivalent to calling `execute` on the SQLAlchemy\nengine or db connection object. Again, you must use the SQL syntax variant\nappropriate for your database.\n\nTo connect with SQLAlchemy you use the `create_engine()` function to create an\nengine object from database URI. You only need to create the engine once per\ndatabase you are connecting to.\n\nFor more information see the examples the SQLAlchemy documentation\n\nYou can use SQLAlchemy constructs to describe your query.\n\nUse `sqlalchemy.text()` to specify query parameters in a backend-neutral way\n\nIf you have an SQLAlchemy description of your database you can express where\nconditions using SQLAlchemy expressions\n\nYou can combine SQLAlchemy expressions with parameters passed to `read_sql()`\nusing `sqlalchemy.bindparam()`\n\nThe use of sqlite is supported without using SQLAlchemy. This mode requires a\nPython database adapter which respect the Python DB-API.\n\nYou can create connections like so:\n\nAnd then issue the following queries:\n\nWarning\n\nStarting in 0.20.0, pandas has split off Google BigQuery support into the\nseparate package `pandas-gbq`. You can `pip install pandas-gbq` to get it.\n\nThe `pandas-gbq` package provides functionality to read/write from Google\nBigQuery.\n\npandas integrates with this external package. if `pandas-gbq` is installed,\nyou can use the pandas methods `pd.read_gbq` and `DataFrame.to_gbq`, which\nwill call the respective functions from `pandas-gbq`.\n\nFull documentation can be found here.\n\nThe method `to_stata()` will write a DataFrame into a .dta file. The format\nversion of this file is always 115 (Stata 12).\n\nStata data files have limited data type support; only strings with 244 or\nfewer characters, `int8`, `int16`, `int32`, `float32` and `float64` can be\nstored in `.dta` files. Additionally, Stata reserves certain values to\nrepresent missing data. Exporting a non-missing value that is outside of the\npermitted range in Stata for a particular data type will retype the variable\nto the next larger size. For example, `int8` values are restricted to lie\nbetween -127 and 100 in Stata, and so variables with values above 100 will\ntrigger a conversion to `int16`. `nan` values in floating points data types\nare stored as the basic missing data type (`.` in Stata).\n\nNote\n\nIt is not possible to export missing data values for integer data types.\n\nThe Stata writer gracefully handles other data types including `int64`,\n`bool`, `uint8`, `uint16`, `uint32` by casting to the smallest supported type\nthat can represent the data. For example, data with a type of `uint8` will be\ncast to `int8` if all values are less than 100 (the upper bound for non-\nmissing `int8` data in Stata), or, if values are outside of this range, the\nvariable is cast to `int16`.\n\nWarning\n\nConversion from `int64` to `float64` may result in a loss of precision if\n`int64` values are larger than 2**53.\n\nWarning\n\n`StataWriter` and `to_stata()` only support fixed width strings containing up\nto 244 characters, a limitation imposed by the version 115 dta file format.\nAttempting to write Stata dta files with strings longer than 244 characters\nraises a `ValueError`.\n\nThe top-level function `read_stata` will read a dta file and return either a\n`DataFrame` or a `StataReader` that can be used to read the file\nincrementally.\n\nSpecifying a `chunksize` yields a `StataReader` instance that can be used to\nread `chunksize` lines from the file at a time. The `StataReader` object can\nbe used as an iterator.\n\nFor more fine-grained control, use `iterator=True` and specify `chunksize`\nwith each call to `read()`.\n\nCurrently the `index` is retrieved as a column.\n\nThe parameter `convert_categoricals` indicates whether value labels should be\nread and used to create a `Categorical` variable from them. Value labels can\nalso be retrieved by the function `value_labels`, which requires `read()` to\nbe called before use.\n\nThe parameter `convert_missing` indicates whether missing value\nrepresentations in Stata should be preserved. If `False` (the default),\nmissing values are represented as `np.nan`. If `True`, missing values are\nrepresented using `StataMissingValue` objects, and columns containing missing\nvalues will have `object` data type.\n\nNote\n\n`read_stata()` and `StataReader` support .dta formats 113-115 (Stata 10-12),\n117 (Stata 13), and 118 (Stata 14).\n\nNote\n\nSetting `preserve_dtypes=False` will upcast to the standard pandas data types:\n`int64` for all integer types and `float64` for floating point data. By\ndefault, the Stata data types are preserved when importing.\n\n`Categorical` data can be exported to Stata data files as value labeled data.\nThe exported data consists of the underlying category codes as integer data\nvalues and the categories as value labels. Stata does not have an explicit\nequivalent to a `Categorical` and information about whether the variable is\nordered is lost when exporting.\n\nWarning\n\nStata only supports string value labels, and so `str` is called on the\ncategories when exporting data. Exporting `Categorical` variables with non-\nstring categories produces a warning, and can result a loss of information if\nthe `str` representations of the categories are not unique.\n\nLabeled data can similarly be imported from Stata data files as `Categorical`\nvariables using the keyword argument `convert_categoricals` (`True` by\ndefault). The keyword argument `order_categoricals` (`True` by default)\ndetermines whether imported `Categorical` variables are ordered.\n\nNote\n\nWhen importing categorical data, the values of the variables in the Stata data\nfile are not preserved since `Categorical` variables always use integer data\ntypes between `-1` and `n-1` where `n` is the number of categories. If the\noriginal values in the Stata data file are required, these can be imported by\nsetting `convert_categoricals=False`, which will import original data (but not\nthe variable labels). The original values can be matched to the imported\ncategorical data since there is a simple mapping between the original Stata\ndata values and the category codes of imported Categorical variables: missing\nvalues are assigned code `-1`, and the smallest original value is assigned\n`0`, the second smallest is assigned `1` and so on until the largest original\nvalue is assigned the code `n-1`.\n\nNote\n\nStata supports partially labeled series. These series have value labels for\nsome but not all data values. Importing a partially labeled series will\nproduce a `Categorical` with string categories for the values that are labeled\nand numeric categories for values with no label.\n\nThe top-level function `read_sas()` can read (but not write) SAS XPORT (.xpt)\nand (since v0.18.0) SAS7BDAT (.sas7bdat) format files.\n\nSAS files only contain two value types: ASCII text and floating point values\n(usually 8 bytes but sometimes truncated). For xport files, there is no\nautomatic type conversion to integers, dates, or categoricals. For SAS7BDAT\nfiles, the format codes may allow date variables to be automatically converted\nto dates. By default the whole file is read and returned as a `DataFrame`.\n\nSpecify a `chunksize` or use `iterator=True` to obtain reader objects\n(`XportReader` or `SAS7BDATReader`) for incrementally reading the file. The\nreader objects also have attributes that contain additional information about\nthe file and its variables.\n\nRead a SAS7BDAT file:\n\nObtain an iterator and read an XPORT file 100,000 lines at a time:\n\nThe specification for the xport file format is available from the SAS web\nsite.\n\nNo official documentation is available for the SAS7BDAT format.\n\nNew in version 0.25.0.\n\nThe top-level function `read_spss()` can read (but not write) SPSS SAV (.sav)\nand ZSAV (.zsav) format files.\n\nSPSS files contain column names. By default the whole file is read,\ncategorical columns are converted into `pd.Categorical`, and a `DataFrame`\nwith all columns is returned.\n\nSpecify the `usecols` parameter to obtain a subset of columns. Specify\n`convert_categoricals=False` to avoid converting categorical columns into\n`pd.Categorical`.\n\nRead an SPSS file:\n\nExtract a subset of columns contained in `usecols` from an SPSS file and avoid\nconverting categorical columns into `pd.Categorical`:\n\nMore information about the SAV and ZSAV file formats is available here.\n\npandas itself only supports IO with a limited set of file formats that map\ncleanly to its tabular data model. For reading and writing other file formats\ninto and from pandas, we recommend these packages from the broader community.\n\nxarray provides data structures inspired by the pandas `DataFrame` for working\nwith multi-dimensional datasets, with a focus on the netCDF file format and\neasy conversion to and from pandas.\n\nThis is an informal comparison of various IO methods, using pandas 0.24.2.\nTimings are machine dependent and small differences should be ignored.\n\nThe following test functions will be used below to compare the performance of\nseveral IO methods:\n\nWhen writing, the top three functions in terms of speed are\n`test_feather_write`, `test_hdf_fixed_write` and\n`test_hdf_fixed_write_compress`.\n\nWhen reading, the top three functions in terms of speed are\n`test_feather_read`, `test_pickle_read` and `test_hdf_fixed_read`.\n\nThe files `test.pkl.compress`, `test.parquet` and `test.feather` took the\nleast space on disk (in bytes).\n\n"}, {"name": "Merge, join, concatenate and compare", "path": "user_guide/merging", "type": "Manual", "text": "\npandas provides various facilities for easily combining together Series or\nDataFrame with various kinds of set logic for the indexes and relational\nalgebra functionality in the case of join / merge-type operations.\n\nIn addition, pandas also provides utilities to compare two Series or DataFrame\nand summarize their differences.\n\nThe `concat()` function (in the main pandas namespace) does all of the heavy\nlifting of performing concatenation operations along an axis while performing\noptional set logic (union or intersection) of the indexes (if any) on the\nother axes. Note that I say \u201cif any\u201d because there is only a single possible\naxis of concatenation for Series.\n\nBefore diving into all of the details of `concat` and what it can do, here is\na simple example:\n\nLike its sibling function on ndarrays, `numpy.concatenate`, `pandas.concat`\ntakes a list or dict of homogeneously-typed objects and concatenates them with\nsome configurable handling of \u201cwhat to do with the other axes\u201d:\n\n`objs` : a sequence or mapping of Series or DataFrame objects. If a dict is\npassed, the sorted keys will be used as the `keys` argument, unless it is\npassed, in which case the values will be selected (see below). Any None\nobjects will be dropped silently unless they are all None in which case a\nValueError will be raised.\n\n`axis` : {0, 1, \u2026}, default 0. The axis to concatenate along.\n\n`join` : {\u2018inner\u2019, \u2018outer\u2019}, default \u2018outer\u2019. How to handle indexes on other\naxis(es). Outer for union and inner for intersection.\n\n`ignore_index` : boolean, default False. If True, do not use the index values\non the concatenation axis. The resulting axis will be labeled 0, \u2026, n - 1.\nThis is useful if you are concatenating objects where the concatenation axis\ndoes not have meaningful indexing information. Note the index values on the\nother axes are still respected in the join.\n\n`keys` : sequence, default None. Construct hierarchical index using the passed\nkeys as the outermost level. If multiple levels passed, should contain tuples.\n\n`levels` : list of sequences, default None. Specific levels (unique values) to\nuse for constructing a MultiIndex. Otherwise they will be inferred from the\nkeys.\n\n`names` : list, default None. Names for the levels in the resulting\nhierarchical index.\n\n`verify_integrity` : boolean, default False. Check whether the new\nconcatenated axis contains duplicates. This can be very expensive relative to\nthe actual data concatenation.\n\n`copy` : boolean, default True. If False, do not copy data unnecessarily.\n\nWithout a little bit of context many of these arguments don\u2019t make much sense.\nLet\u2019s revisit the above example. Suppose we wanted to associate specific keys\nwith each of the pieces of the chopped up DataFrame. We can do this using the\n`keys` argument:\n\nAs you can see (if you\u2019ve read the rest of the documentation), the resulting\nobject\u2019s index has a hierarchical index. This means that we can now select out\neach chunk by key:\n\nIt\u2019s not a stretch to see how this can be very useful. More detail on this\nfunctionality below.\n\nNote\n\nIt is worth noting that `concat()` (and therefore `append()`) makes a full\ncopy of the data, and that constantly reusing this function can create a\nsignificant performance hit. If you need to use the operation over several\ndatasets, use a list comprehension.\n\nNote\n\nWhen concatenating DataFrames with named axes, pandas will attempt to preserve\nthese index/column names whenever possible. In the case where all inputs share\na common name, this name will be assigned to the result. When the input names\ndo not all agree, the result will be unnamed. The same is true for\n`MultiIndex`, but the logic is applied separately on a level-by-level basis.\n\nWhen gluing together multiple DataFrames, you have a choice of how to handle\nthe other axes (other than the one being concatenated). This can be done in\nthe following two ways:\n\nTake the union of them all, `join='outer'`. This is the default option as it\nresults in zero information loss.\n\nTake the intersection, `join='inner'`.\n\nHere is an example of each of these methods. First, the default `join='outer'`\nbehavior:\n\nHere is the same thing with `join='inner'`:\n\nLastly, suppose we just wanted to reuse the exact index from the original\nDataFrame:\n\nSimilarly, we could index before the concatenation:\n\nFor `DataFrame` objects which don\u2019t have a meaningful index, you may wish to\nappend them and ignore the fact that they may have overlapping indexes. To do\nthis, use the `ignore_index` argument:\n\nYou can concatenate a mix of `Series` and `DataFrame` objects. The `Series`\nwill be transformed to `DataFrame` with the column name as the name of the\n`Series`.\n\nNote\n\nSince we\u2019re concatenating a `Series` to a `DataFrame`, we could have achieved\nthe same result with `DataFrame.assign()`. To concatenate an arbitrary number\nof pandas objects (`DataFrame` or `Series`), use `concat`.\n\nIf unnamed `Series` are passed they will be numbered consecutively.\n\nPassing `ignore_index=True` will drop all name references.\n\nA fairly common use of the `keys` argument is to override the column names\nwhen creating a new `DataFrame` based on existing `Series`. Notice how the\ndefault behaviour consists on letting the resulting `DataFrame` inherit the\nparent `Series`\u2019 name, when these existed.\n\nThrough the `keys` argument we can override the existing column names.\n\nLet\u2019s consider a variation of the very first example presented:\n\nYou can also pass a dict to `concat` in which case the dict keys will be used\nfor the `keys` argument (unless other keys are specified):\n\nThe MultiIndex created has levels that are constructed from the passed keys\nand the index of the `DataFrame` pieces:\n\nIf you wish to specify other levels (as will occasionally be the case), you\ncan do so using the `levels` argument:\n\nThis is fairly esoteric, but it is actually necessary for implementing things\nlike GroupBy where the order of a categorical variable is meaningful.\n\nIf you have a series that you want to append as a single row to a `DataFrame`,\nyou can convert the row into a `DataFrame` and use `concat`\n\nYou should use `ignore_index` with this method to instruct DataFrame to\ndiscard its index. If you wish to preserve the index, you should construct an\nappropriately-indexed DataFrame and append or concatenate those objects.\n\npandas has full-featured, high performance in-memory join operations\nidiomatically very similar to relational databases like SQL. These methods\nperform significantly better (in some cases well over an order of magnitude\nbetter) than other open source implementations (like `base::merge.data.frame`\nin R). The reason for this is careful algorithmic design and the internal\nlayout of the data in `DataFrame`.\n\nSee the cookbook for some advanced strategies.\n\nUsers who are familiar with SQL but new to pandas might be interested in a\ncomparison with SQL.\n\npandas provides a single function, `merge()`, as the entry point for all\nstandard database join operations between `DataFrame` or named `Series`\nobjects:\n\n`left`: A DataFrame or named Series object.\n\n`right`: Another DataFrame or named Series object.\n\n`on`: Column or index level names to join on. Must be found in both the left\nand right DataFrame and/or Series objects. If not passed and `left_index` and\n`right_index` are `False`, the intersection of the columns in the DataFrames\nand/or Series will be inferred to be the join keys.\n\n`left_on`: Columns or index levels from the left DataFrame or Series to use as\nkeys. Can either be column names, index level names, or arrays with length\nequal to the length of the DataFrame or Series.\n\n`right_on`: Columns or index levels from the right DataFrame or Series to use\nas keys. Can either be column names, index level names, or arrays with length\nequal to the length of the DataFrame or Series.\n\n`left_index`: If `True`, use the index (row labels) from the left DataFrame or\nSeries as its join key(s). In the case of a DataFrame or Series with a\nMultiIndex (hierarchical), the number of levels must match the number of join\nkeys from the right DataFrame or Series.\n\n`right_index`: Same usage as `left_index` for the right DataFrame or Series\n\n`how`: One of `'left'`, `'right'`, `'outer'`, `'inner'`, `'cross'`. Defaults\nto `inner`. See below for more detailed description of each method.\n\n`sort`: Sort the result DataFrame by the join keys in lexicographical order.\nDefaults to `True`, setting to `False` will improve performance substantially\nin many cases.\n\n`suffixes`: A tuple of string suffixes to apply to overlapping columns.\nDefaults to `('_x', '_y')`.\n\n`copy`: Always copy data (default `True`) from the passed DataFrame or named\nSeries objects, even when reindexing is not necessary. Cannot be avoided in\nmany cases but may improve performance / memory usage. The cases where copying\ncan be avoided are somewhat pathological but this option is provided\nnonetheless.\n\n`indicator`: Add a column to the output DataFrame called `_merge` with\ninformation on the source of each row. `_merge` is Categorical-type and takes\non a value of `left_only` for observations whose merge key only appears in\n`'left'` DataFrame or Series, `right_only` for observations whose merge key\nonly appears in `'right'` DataFrame or Series, and `both` if the observation\u2019s\nmerge key is found in both.\n\n`validate` : string, default None. If specified, checks if merge is of\nspecified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: checks if merge keys are unique in both left and right\ndatasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: checks if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: checks if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nNote\n\nSupport for specifying index levels as the `on`, `left_on`, and `right_on`\nparameters was added in version 0.23.0. Support for merging named `Series`\nobjects was added in version 0.24.0.\n\nThe return type will be the same as `left`. If `left` is a `DataFrame` or\nnamed `Series` and `right` is a subclass of `DataFrame`, the return type will\nstill be `DataFrame`.\n\n`merge` is a function in the pandas namespace, and it is also available as a\n`DataFrame` instance method `merge()`, with the calling `DataFrame` being\nimplicitly considered the left object in the join.\n\nThe related `join()` method, uses `merge` internally for the index-on-index\n(by default) and column(s)-on-index join. If you are joining on index only,\nyou may wish to use `DataFrame.join` to save yourself some typing.\n\nExperienced users of relational databases like SQL will be familiar with the\nterminology used to describe join operations between two SQL-table like\nstructures (`DataFrame` objects). There are several cases to consider which\nare very important to understand:\n\none-to-one joins: for example when joining two `DataFrame` objects on their\nindexes (which must contain unique values).\n\nmany-to-one joins: for example when joining an index (unique) to one or more\ncolumns in a different `DataFrame`.\n\nmany-to-many joins: joining columns on columns.\n\nNote\n\nWhen joining columns on columns (potentially a many-to-many join), any indexes\non the passed `DataFrame` objects will be discarded.\n\nIt is worth spending some time understanding the result of the many-to-many\njoin case. In SQL / standard relational algebra, if a key combination appears\nmore than once in both tables, the resulting table will have the Cartesian\nproduct of the associated data. Here is a very basic example with one unique\nkey combination:\n\nHere is a more complicated example with multiple join keys. Only the keys\nappearing in `left` and `right` are present (the intersection), since\n`how='inner'` by default.\n\nThe `how` argument to `merge` specifies how to determine which keys are to be\nincluded in the resulting table. If a key combination does not appear in\neither the left or right tables, the values in the joined table will be `NA`.\nHere is a summary of the `how` options and their SQL equivalent names:\n\nMerge method\n\nSQL Join Name\n\nDescription\n\n`left`\n\n`LEFT OUTER JOIN`\n\nUse keys from left frame only\n\n`right`\n\n`RIGHT OUTER JOIN`\n\nUse keys from right frame only\n\n`outer`\n\n`FULL OUTER JOIN`\n\nUse union of keys from both frames\n\n`inner`\n\n`INNER JOIN`\n\nUse intersection of keys from both frames\n\n`cross`\n\n`CROSS JOIN`\n\nCreate the cartesian product of rows of both frames\n\nYou can merge a mult-indexed Series and a DataFrame, if the names of the\nMultiIndex correspond to the columns from the DataFrame. Transform the Series\nto a DataFrame using `Series.reset_index()` before merging, as shown in the\nfollowing example.\n\nHere is another example with duplicate join keys in DataFrames:\n\nWarning\n\nJoining / merging on duplicate keys can cause a returned frame that is the\nmultiplication of the row dimensions, which may result in memory overflow. It\nis the user\u2019 s responsibility to manage duplicate values in keys before\njoining large DataFrames.\n\nUsers can use the `validate` argument to automatically check whether there are\nunexpected duplicates in their merge keys. Key uniqueness is checked before\nmerge operations and so should protect against memory overflows. Checking key\nuniqueness is also a good way to ensure user data structures are as expected.\n\nIn the following example, there are duplicate values of `B` in the right\n`DataFrame`. As this is not a one-to-one merge \u2013 as specified in the\n`validate` argument \u2013 an exception will be raised.\n\nIf the user is aware of the duplicates in the right `DataFrame` but wants to\nensure there are no duplicates in the left DataFrame, one can use the\n`validate='one_to_many'` argument instead, which will not raise an exception.\n\n`merge()` accepts the argument `indicator`. If `True`, a Categorical-type\ncolumn called `_merge` will be added to the output object that takes on\nvalues:\n\nObservation Origin\n\n`_merge` value\n\nMerge key only in `'left'` frame\n\n`left_only`\n\nMerge key only in `'right'` frame\n\n`right_only`\n\nMerge key in both frames\n\n`both`\n\nThe `indicator` argument will also accept string arguments, in which case the\nindicator function will use the value of the passed string as the name for the\nindicator column.\n\nMerging will preserve the dtype of the join keys.\n\nWe are able to preserve the join keys:\n\nOf course if you have missing values that are introduced, then the resulting\ndtype will be upcast.\n\nMerging will preserve `category` dtypes of the mergands. See also the section\non categoricals.\n\nThe left frame.\n\nThe right frame.\n\nThe merged result:\n\nNote\n\nThe category dtypes must be exactly the same, meaning the same categories and\nthe ordered attribute. Otherwise the result will coerce to the categories\u2019\ndtype.\n\nNote\n\nMerging on `category` dtypes that are the same can be quite performant\ncompared to `object` dtype merging.\n\n`DataFrame.join()` is a convenient method for combining the columns of two\npotentially differently-indexed `DataFrames` into a single result `DataFrame`.\nHere is a very basic example:\n\nThe same as above, but with `how='inner'`.\n\nThe data alignment here is on the indexes (row labels). This same behavior can\nbe achieved using `merge` plus additional arguments instructing it to use the\nindexes:\n\n`join()` takes an optional `on` argument which may be a column or multiple\ncolumn names, which specifies that the passed `DataFrame` is to be aligned on\nthat column in the `DataFrame`. These two function calls are completely\nequivalent:\n\nObviously you can choose whichever form you find more convenient. For many-to-\none joins (where one of the `DataFrame`\u2019s is already indexed by the join key),\nusing `join` may be more convenient. Here is a simple example:\n\nTo join on multiple keys, the passed DataFrame must have a `MultiIndex`:\n\nNow this can be joined by passing the two key column names:\n\nThe default for `DataFrame.join` is to perform a left join (essentially a\n\u201cVLOOKUP\u201d operation, for Excel users), which uses only the keys found in the\ncalling DataFrame. Other join types, for example inner join, can be just as\neasily performed:\n\nAs you can see, this drops any rows where there was no match.\n\nYou can join a singly-indexed `DataFrame` with a level of a MultiIndexed\n`DataFrame`. The level will match on the name of the index of the singly-\nindexed frame against a level name of the MultiIndexed frame.\n\nThis is equivalent but less verbose and more memory efficient / faster than\nthis.\n\nThis is supported in a limited way, provided that the index for the right\nargument is completely used in the join, and is a subset of the indices in the\nleft argument, as in this example:\n\nIf that condition is not satisfied, a join with two multi-indexes can be done\nusing the following code.\n\nStrings passed as the `on`, `left_on`, and `right_on` parameters may refer to\neither column names or index level names. This enables merging `DataFrame`\ninstances on a combination of index levels and columns without resetting\nindexes.\n\nNote\n\nWhen DataFrames are merged on a string that matches an index level in both\nframes, the index level is preserved as an index level in the resulting\nDataFrame.\n\nNote\n\nWhen DataFrames are merged using only some of the levels of a `MultiIndex`,\nthe extra levels will be dropped from the resulting merge. In order to\npreserve those levels, use `reset_index` on those level names to move those\nlevels to columns prior to doing the merge.\n\nNote\n\nIf a string matches both a column name and an index level name, then a warning\nis issued and the column takes precedence. This will result in an ambiguity\nerror in a future version.\n\nThe merge `suffixes` argument takes a tuple of list of strings to append to\noverlapping column names in the input `DataFrame`s to disambiguate the result\ncolumns:\n\n`DataFrame.join()` has `lsuffix` and `rsuffix` arguments which behave\nsimilarly.\n\nA list or tuple of `DataFrames` can also be passed to `join()` to join them\ntogether on their indexes.\n\nAnother fairly common situation is to have two like-indexed (or similarly\nindexed) `Series` or `DataFrame` objects and wanting to \u201cpatch\u201d values in one\nobject from values for matching indices in the other. Here is an example:\n\nFor this, use the `combine_first()` method:\n\nNote that this method only takes values from the right `DataFrame` if they are\nmissing in the left `DataFrame`. A related method, `update()`, alters non-NA\nvalues in place:\n\nA `merge_ordered()` function allows combining time series and other ordered\ndata. In particular it has an optional `fill_method` keyword to\nfill/interpolate missing data:\n\nA `merge_asof()` is similar to an ordered left-join except that we match on\nnearest key rather than equal keys. For each row in the `left` `DataFrame`, we\nselect the last row in the `right` `DataFrame` whose `on` key is less than the\nleft\u2019s key. Both DataFrames must be sorted by the key.\n\nOptionally an asof merge can perform a group-wise merge. This matches the `by`\nkey equally, in addition to the nearest match on the `on` key.\n\nFor example; we might have `trades` and `quotes` and we want to `asof` merge\nthem.\n\nBy default we are taking the asof of the quotes.\n\nWe only asof within `2ms` between the quote time and the trade time.\n\nWe only asof within `10ms` between the quote time and the trade time and we\nexclude exact matches on time. Note that though we exclude the exact matches\n(of the quotes), prior quotes do propagate to that point in time.\n\nThe `compare()` and `compare()` methods allow you to compare two DataFrame or\nSeries, respectively, and summarize their differences.\n\nThis feature was added in V1.1.0.\n\nFor example, you might want to compare two `DataFrame` and stack their\ndifferences side by side.\n\nBy default, if two corresponding values are equal, they will be shown as\n`NaN`. Furthermore, if all values in an entire row / column, the row / column\nwill be omitted from the result. The remaining differences will be aligned on\ncolumns.\n\nIf you wish, you may choose to stack the differences on rows.\n\nIf you wish to keep all original rows and columns, set `keep_shape` argument\nto `True`.\n\nYou may also keep all the original values even if they are equal.\n\n"}, {"name": "MultiIndex / advanced indexing", "path": "user_guide/advanced", "type": "Manual", "text": "\nThis section covers indexing with a MultiIndex and other advanced indexing\nfeatures.\n\nSee the Indexing and Selecting Data for general indexing documentation.\n\nWarning\n\nWhether a copy or a reference is returned for a setting operation may depend\non the context. This is sometimes called `chained assignment` and should be\navoided. See Returning a View versus Copy.\n\nSee the cookbook for some advanced strategies.\n\nHierarchical / Multi-level indexing is very exciting as it opens the door to\nsome quite sophisticated data analysis and manipulation, especially for\nworking with higher dimensional data. In essence, it enables you to store and\nmanipulate data with an arbitrary number of dimensions in lower dimensional\ndata structures like `Series` (1d) and `DataFrame` (2d).\n\nIn this section, we will show what exactly we mean by \u201chierarchical\u201d indexing\nand how it integrates with all of the pandas indexing functionality described\nabove and in prior sections. Later, when discussing group by and pivoting and\nreshaping data, we\u2019ll show non-trivial applications to illustrate how it aids\nin structuring data for analysis.\n\nSee the cookbook for some advanced strategies.\n\nThe `MultiIndex` object is the hierarchical analogue of the standard `Index`\nobject which typically stores the axis labels in pandas objects. You can think\nof `MultiIndex` as an array of tuples where each tuple is unique. A\n`MultiIndex` can be created from a list of arrays (using\n`MultiIndex.from_arrays()`), an array of tuples (using\n`MultiIndex.from_tuples()`), a crossed set of iterables (using\n`MultiIndex.from_product()`), or a `DataFrame` (using\n`MultiIndex.from_frame()`). The `Index` constructor will attempt to return a\n`MultiIndex` when it is passed a list of tuples. The following examples\ndemonstrate different ways to initialize MultiIndexes.\n\nWhen you want every pairing of the elements in two iterables, it can be easier\nto use the `MultiIndex.from_product()` method:\n\nYou can also construct a `MultiIndex` from a `DataFrame` directly, using the\nmethod `MultiIndex.from_frame()`. This is a complementary method to\n`MultiIndex.to_frame()`.\n\nAs a convenience, you can pass a list of arrays directly into `Series` or\n`DataFrame` to construct a `MultiIndex` automatically:\n\nAll of the `MultiIndex` constructors accept a `names` argument which stores\nstring names for the levels themselves. If no names are provided, `None` will\nbe assigned:\n\nThis index can back any axis of a pandas object, and the number of levels of\nthe index is up to you:\n\nWe\u2019ve \u201csparsified\u201d the higher levels of the indexes to make the console output\na bit easier on the eyes. Note that how the index is displayed can be\ncontrolled using the `multi_sparse` option in `pandas.set_options()`:\n\nIt\u2019s worth keeping in mind that there\u2019s nothing preventing you from using\ntuples as atomic labels on an axis:\n\nThe reason that the `MultiIndex` matters is that it can allow you to do\ngrouping, selection, and reshaping operations as we will describe below and in\nsubsequent areas of the documentation. As you will see in later sections, you\ncan find yourself working with hierarchically-indexed data without creating a\n`MultiIndex` explicitly yourself. However, when loading data from a file, you\nmay wish to generate your own `MultiIndex` when preparing the data set.\n\nThe method `get_level_values()` will return a vector of the labels for each\nlocation at a particular level:\n\nOne of the important features of hierarchical indexing is that you can select\ndata by a \u201cpartial\u201d label identifying a subgroup in the data. Partial\nselection \u201cdrops\u201d levels of the hierarchical index in the result in a\ncompletely analogous way to selecting a column in a regular DataFrame:\n\nSee Cross-section with hierarchical index for how to select on a deeper level.\n\nThe `MultiIndex` keeps all the defined levels of an index, even if they are\nnot actually used. When slicing an index, you may notice this. For example:\n\nThis is done to avoid a recomputation of the levels in order to make slicing\nhighly performant. If you want to see only the used levels, you can use the\n`get_level_values()` method.\n\nTo reconstruct the `MultiIndex` with only the used levels, the\n`remove_unused_levels()` method may be used.\n\nOperations between differently-indexed objects having `MultiIndex` on the axes\nwill work as you expect; data alignment will work the same as an Index of\ntuples:\n\nThe `reindex()` method of `Series`/`DataFrames` can be called with another\n`MultiIndex`, or even a list or array of tuples:\n\nSyntactically integrating `MultiIndex` in advanced indexing with `.loc` is a\nbit challenging, but we\u2019ve made every effort to do so. In general, MultiIndex\nkeys take the form of tuples. For example, the following works as you would\nexpect:\n\nNote that `df.loc['bar', 'two']` would also work in this example, but this\nshorthand notation can lead to ambiguity in general.\n\nIf you also want to index a specific column with `.loc`, you must use a tuple\nlike this:\n\nYou don\u2019t have to specify all levels of the `MultiIndex` by passing only the\nfirst elements of the tuple. For example, you can use \u201cpartial\u201d indexing to\nget all elements with `bar` in the first level as follows:\n\nThis is a shortcut for the slightly more verbose notation `df.loc[('bar',),]`\n(equivalent to `df.loc['bar',]` in this example).\n\n\u201cPartial\u201d slicing also works quite nicely.\n\nYou can slice with a \u2018range\u2019 of values, by providing a slice of tuples.\n\nPassing a list of labels or tuples works similar to reindexing:\n\nNote\n\nIt is important to note that tuples and lists are not treated identically in\npandas when it comes to indexing. Whereas a tuple is interpreted as one multi-\nlevel key, a list is used to specify several keys. Or in other words, tuples\ngo horizontally (traversing levels), lists go vertically (scanning levels).\n\nImportantly, a list of tuples indexes several complete `MultiIndex` keys,\nwhereas a tuple of lists refer to several values within a level:\n\nYou can slice a `MultiIndex` by providing multiple indexers.\n\nYou can provide any of the selectors as if you are indexing by label, see\nSelection by Label, including slices, lists of labels, labels, and boolean\nindexers.\n\nYou can use `slice(None)` to select all the contents of that level. You do not\nneed to specify all the deeper levels, they will be implied as `slice(None)`.\n\nAs usual, both sides of the slicers are included as this is label indexing.\n\nWarning\n\nYou should specify all axes in the `.loc` specifier, meaning the indexer for\nthe index and for the columns. There are some ambiguous cases where the passed\nindexer could be mis-interpreted as indexing both axes, rather than into say\nthe `MultiIndex` for the rows.\n\nYou should do this:\n\nYou should not do this:\n\nBasic MultiIndex slicing using slices, lists, and labels.\n\nYou can use `pandas.IndexSlice` to facilitate a more natural syntax using `:`,\nrather than using `slice(None)`.\n\nIt is possible to perform quite complicated selections using this method on\nmultiple axes at the same time.\n\nUsing a boolean indexer you can provide selection related to the values.\n\nYou can also specify the `axis` argument to `.loc` to interpret the passed\nslicers on a single axis.\n\nFurthermore, you can set the values using the following methods.\n\nYou can use a right-hand-side of an alignable object as well.\n\nThe `xs()` method of `DataFrame` additionally takes a level argument to make\nselecting data at a particular level of a `MultiIndex` easier.\n\nYou can also select on the columns with `xs`, by providing the axis argument.\n\n`xs` also allows selection with multiple keys.\n\nYou can pass `drop_level=False` to `xs` to retain the level that was selected.\n\nCompare the above with the result using `drop_level=True` (the default value).\n\nUsing the parameter `level` in the `reindex()` and `align()` methods of pandas\nobjects is useful to broadcast values across a level. For instance:\n\nThe `swaplevel()` method can switch the order of two levels:\n\nThe `reorder_levels()` method generalizes the `swaplevel` method, allowing you\nto permute the hierarchical index levels in one step:\n\nThe `rename()` method is used to rename the labels of a `MultiIndex`, and is\ntypically used to rename the columns of a `DataFrame`. The `columns` argument\nof `rename` allows a dictionary to be specified that includes only the columns\nyou wish to rename.\n\nThis method can also be used to rename specific labels of the main index of\nthe `DataFrame`.\n\nThe `rename_axis()` method is used to rename the name of a `Index` or\n`MultiIndex`. In particular, the names of the levels of a `MultiIndex` can be\nspecified, which is useful if `reset_index()` is later used to move the values\nfrom the `MultiIndex` to a column.\n\nNote that the columns of a `DataFrame` are an index, so that using\n`rename_axis` with the `columns` argument will change the name of that index.\n\nBoth `rename` and `rename_axis` support specifying a dictionary, `Series` or a\nmapping function to map labels/names to new values.\n\nWhen working with an `Index` object directly, rather than via a `DataFrame`,\n`Index.set_names()` can be used to change the names.\n\nYou cannot set the names of the MultiIndex via a level.\n\nUse `Index.set_names()` instead.\n\nFor `MultiIndex`-ed objects to be indexed and sliced effectively, they need to\nbe sorted. As with any index, you can use `sort_index()`.\n\nYou may also pass a level name to `sort_index` if the `MultiIndex` levels are\nnamed.\n\nOn higher dimensional objects, you can sort any of the other axes by level if\nthey have a `MultiIndex`:\n\nIndexing will work even if the data are not sorted, but will be rather\ninefficient (and show a `PerformanceWarning`). It will also return a copy of\nthe data rather than a view:\n\nFurthermore, if you try to index something that is not fully lexsorted, this\ncan raise:\n\nThe `is_monotonic_increasing()` method on a `MultiIndex` shows if the index is\nsorted:\n\nAnd now selection works as expected.\n\nSimilar to NumPy ndarrays, pandas `Index`, `Series`, and `DataFrame` also\nprovides the `take()` method that retrieves elements along a given axis at the\ngiven indices. The given indices must be either a list or an ndarray of\ninteger index positions. `take` will also accept negative integers as relative\npositions to the end of the object.\n\nFor DataFrames, the given indices should be a 1d list or ndarray that\nspecifies row or column positions.\n\nIt is important to note that the `take` method on pandas objects are not\nintended to work on boolean indices and may return unexpected results.\n\nFinally, as a small note on performance, because the `take` method handles a\nnarrower range of inputs, it can offer performance that is a good deal faster\nthan fancy indexing.\n\nWe have discussed `MultiIndex` in the previous sections pretty extensively.\nDocumentation about `DatetimeIndex` and `PeriodIndex` are shown here, and\ndocumentation about `TimedeltaIndex` is found here.\n\nIn the following sub-sections we will highlight some other index types.\n\n`CategoricalIndex` is a type of index that is useful for supporting indexing\nwith duplicates. This is a container around a `Categorical` and allows\nefficient indexing and storage of an index with a large number of duplicated\nelements.\n\nSetting the index will create a `CategoricalIndex`.\n\nIndexing with `__getitem__/.iloc/.loc` works similarly to an `Index` with\nduplicates. The indexers must be in the category or the operation will raise a\n`KeyError`.\n\nThe `CategoricalIndex` is preserved after indexing:\n\nSorting the index will sort by the order of the categories (recall that we\ncreated the index with `CategoricalDtype(list('cab'))`, so the sorted order is\n`cab`).\n\nGroupby operations on the index will preserve the index nature as well.\n\nReindexing operations will return a resulting index based on the type of the\npassed indexer. Passing a list will return a plain-old `Index`; indexing with\na `Categorical` will return a `CategoricalIndex`, indexed according to the\ncategories of the passed `Categorical` dtype. This allows one to arbitrarily\nindex these even with values not in the categories, similarly to how you can\nreindex any pandas index.\n\nWarning\n\nReshaping and Comparison operations on a `CategoricalIndex` must have the same\ncategories or a `TypeError` will be raised.\n\nDeprecated since version 1.4.0: In pandas 2.0, `Index` will become the default\nindex type for numeric types instead of `Int64Index`, `Float64Index` and\n`UInt64Index` and those index types are therefore deprecated and will be\nremoved in a futire version. `RangeIndex` will not be removed, as it\nrepresents an optimized version of an integer index.\n\n`Int64Index` is a fundamental basic index in pandas. This is an immutable\narray implementing an ordered, sliceable set.\n\n`RangeIndex` is a sub-class of `Int64Index` that provides the default index\nfor all `NDFrame` objects. `RangeIndex` is an optimized version of\n`Int64Index` that can represent a monotonic ordered set. These are analogous\nto Python range types.\n\nDeprecated since version 1.4.0: `Index` will become the default index type for\nnumeric types in the future instead of `Int64Index`, `Float64Index` and\n`UInt64Index` and those index types are therefore deprecated and will be\nremoved in a future version of Pandas. `RangeIndex` will not be removed as it\nrepresents an optimized version of an integer index.\n\nBy default a `Float64Index` will be automatically created when passing\nfloating, or mixed-integer-floating values in index creation. This enables a\npure label-based slicing paradigm that makes `[],ix,loc` for scalar indexing\nand slicing work exactly the same.\n\nScalar selection for `[],.loc` will always be label based. An integer will\nmatch an equal float index (e.g. `3` is equivalent to `3.0`).\n\nThe only positional indexing is via `iloc`.\n\nA scalar index that is not found will raise a `KeyError`. Slicing is primarily\non the values of the index when using `[],ix,loc`, and always positional when\nusing `iloc`. The exception is when the slice is boolean, in which case it\nwill always be positional.\n\nIn float indexes, slicing using floats is allowed.\n\nIn non-float indexes, slicing using floats will raise a `TypeError`.\n\nHere is a typical use-case for using this type of indexing. Imagine that you\nhave a somewhat irregular timedelta-like indexing scheme, but the data is\nrecorded as floats. This could, for example, be millisecond offsets.\n\nSelection operations then will always work on a value basis, for all selection\noperators.\n\nYou could retrieve the first 1 second (1000 ms) of data as such:\n\nIf you need integer based selection, you should use `iloc`:\n\n`IntervalIndex` together with its own dtype, `IntervalDtype` as well as the\n`Interval` scalar type, allow first-class support in pandas for interval\nnotation.\n\nThe `IntervalIndex` allows some unique indexing and is also used as a return\ntype for the categories in `cut()` and `qcut()`.\n\nAn `IntervalIndex` can be used in `Series` and in `DataFrame` as the index.\n\nLabel based indexing via `.loc` along the edges of an interval works as you\nwould expect, selecting that particular interval.\n\nIf you select a label contained within an interval, this will also select the\ninterval.\n\nSelecting using an `Interval` will only return exact matches (starting from\npandas 0.25.0).\n\nTrying to select an `Interval` that is not exactly contained in the\n`IntervalIndex` will raise a `KeyError`.\n\nSelecting all `Intervals` that overlap a given `Interval` can be performed\nusing the `overlaps()` method to create a boolean indexer.\n\n`cut()` and `qcut()` both return a `Categorical` object, and the bins they\ncreate are stored as an `IntervalIndex` in its `.categories` attribute.\n\n`cut()` also accepts an `IntervalIndex` for its `bins` argument, which enables\na useful pandas idiom. First, We call `cut()` with some data and `bins` set to\na fixed number, to generate the bins. Then, we pass the values of\n`.categories` as the `bins` argument in subsequent calls to `cut()`, supplying\nnew data which will be binned into the same bins.\n\nAny value which falls outside all bins will be assigned a `NaN` value.\n\nIf we need intervals on a regular frequency, we can use the `interval_range()`\nfunction to create an `IntervalIndex` using various combinations of `start`,\n`end`, and `periods`. The default frequency for `interval_range` is a 1 for\nnumeric intervals, and calendar day for datetime-like intervals:\n\nThe `freq` parameter can used to specify non-default frequencies, and can\nutilize a variety of frequency aliases with datetime-like intervals:\n\nAdditionally, the `closed` parameter can be used to specify which side(s) the\nintervals are closed on. Intervals are closed on the right side by default.\n\nSpecifying `start`, `end`, and `periods` will generate a range of evenly\nspaced intervals from `start` to `end` inclusively, with `periods` number of\nelements in the resulting `IntervalIndex`:\n\nLabel-based indexing with integer axis labels is a thorny topic. It has been\ndiscussed heavily on mailing lists and among various members of the scientific\nPython community. In pandas, our general viewpoint is that labels matter more\nthan integer locations. Therefore, with an integer axis index only label-based\nindexing is possible with the standard tools like `.loc`. The following code\nwill generate exceptions:\n\nThis deliberate decision was made to prevent ambiguities and subtle bugs (many\nusers reported finding bugs when the API change was made to stop \u201cfalling\nback\u201d on position-based indexing).\n\nIf the index of a `Series` or `DataFrame` is monotonically increasing or\ndecreasing, then the bounds of a label-based slice can be outside the range of\nthe index, much like slice indexing a normal Python `list`. Monotonicity of an\nindex can be tested with the `is_monotonic_increasing()` and\n`is_monotonic_decreasing()` attributes.\n\nOn the other hand, if the index is not monotonic, then both slice bounds must\nbe unique members of the index.\n\n`Index.is_monotonic_increasing` and `Index.is_monotonic_decreasing` only check\nthat an index is weakly monotonic. To check for strict monotonicity, you can\ncombine one of those with the `is_unique()` attribute.\n\nCompared with standard Python sequence slicing in which the slice endpoint is\nnot inclusive, label-based slicing in pandas is inclusive. The primary reason\nfor this is that it is often not possible to easily determine the \u201csuccessor\u201d\nor next element after a particular label in an index. For example, consider\nthe following `Series`:\n\nSuppose we wished to slice from `c` to `e`, using integers this would be\naccomplished as such:\n\nHowever, if you only had `c` and `e`, determining the next element in the\nindex can be somewhat complicated. For example, the following does not work:\n\nA very common use case is to limit a time series to start and end at two\nspecific dates. To enable this, we made the design choice to make label-based\nslicing include both endpoints:\n\nThis is most definitely a \u201cpracticality beats purity\u201d sort of thing, but it is\nsomething to watch out for if you expect label-based slicing to behave exactly\nin the way that standard Python integer slicing works.\n\nThe different indexing operation can potentially change the dtype of a\n`Series`.\n\nThis is because the (re)indexing operations above silently inserts `NaNs` and\nthe `dtype` changes accordingly. This can cause some issues when using `numpy`\n`ufuncs` such as `numpy.logical_and`.\n\nSee the GH2388 for a more detailed discussion.\n\n"}, {"name": "Nullable Boolean data type", "path": "user_guide/boolean", "type": "Manual", "text": "\nNote\n\nBooleanArray is currently experimental. Its API or implementation may change\nwithout warning.\n\nNew in version 1.0.0.\n\npandas allows indexing with `NA` values in a boolean array, which are treated\nas `False`.\n\nChanged in version 1.0.2.\n\nIf you would prefer to keep the `NA` values you can manually fill them with\n`fillna(True)`.\n\n`arrays.BooleanArray` implements Kleene Logic (sometimes called three-value\nlogic) for logical operations like `&` (and), `|` (or) and `^` (exclusive-or).\n\nThis table demonstrates the results for every combination. These operations\nare symmetrical, so flipping the left- and right-hand side makes no difference\nin the result.\n\nExpression\n\nResult\n\n`True & True`\n\n`True`\n\n`True & False`\n\n`False`\n\n`True & NA`\n\n`NA`\n\n`False & False`\n\n`False`\n\n`False & NA`\n\n`False`\n\n`NA & NA`\n\n`NA`\n\n`True | True`\n\n`True`\n\n`True | False`\n\n`True`\n\n`True | NA`\n\n`True`\n\n`False | False`\n\n`False`\n\n`False | NA`\n\n`NA`\n\n`NA | NA`\n\n`NA`\n\n`True ^ True`\n\n`False`\n\n`True ^ False`\n\n`True`\n\n`True ^ NA`\n\n`NA`\n\n`False ^ False`\n\n`False`\n\n`False ^ NA`\n\n`NA`\n\n`NA ^ NA`\n\n`NA`\n\nWhen an `NA` is present in an operation, the output value is `NA` only if the\nresult cannot be determined solely based on the other input. For example,\n`True | NA` is `True`, because both `True | True` and `True | False` are\n`True`. In that case, we don\u2019t actually need to consider the value of the\n`NA`.\n\nOn the other hand, `True & NA` is `NA`. The result depends on whether the `NA`\nreally is `True` or `False`, since `True & True` is `True`, but `True & False`\nis `False`, so we can\u2019t determine the output.\n\nThis differs from how `np.nan` behaves in logical operations. pandas treated\n`np.nan` is always false in the output.\n\nIn `or`\n\nIn `and`\n\n"}, {"name": "Nullable integer data type", "path": "user_guide/integer_na", "type": "Manual", "text": "\nNote\n\nIntegerArray is currently experimental. Its API or implementation may change\nwithout warning.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as the missing value rather\nthan `numpy.nan`.\n\nIn Working with missing data, we saw that pandas primarily uses `NaN` to\nrepresent missing data. Because `NaN` is a float, this forces an array of\nintegers with any missing values to become floating point. In some cases, this\nmay not matter much. But if your integer column is, say, an identifier,\ncasting to float can be problematic. Some integers cannot even be represented\nas floating point numbers.\n\npandas can represent integer data with possibly missing values using\n`arrays.IntegerArray`. This is an extension types implemented within pandas.\n\nOr the string alias `\"Int64\"` (note the capital `\"I\"`, to differentiate from\nNumPy\u2019s `'int64'` dtype:\n\nAll NA-like values are replaced with `pandas.NA`.\n\nThis array can be stored in a `DataFrame` or `Series` like any NumPy array.\n\nYou can also pass the list-like object to the `Series` constructor with the\ndtype.\n\nWarning\n\nCurrently `pandas.array()` and `pandas.Series()` use different rules for dtype\ninference. `pandas.array()` will infer a nullable- integer dtype\n\nFor backwards-compatibility, `Series` infers these as either integer or float\ndtype\n\nWe recommend explicitly providing the dtype to avoid confusion.\n\nIn the future, we may provide an option for `Series` to infer a nullable-\ninteger dtype.\n\nOperations involving an integer array will behave similar to NumPy arrays.\nMissing values will be propagated, and the data will be coerced to another\ndtype if needed.\n\nThese dtypes can operate as part of `DataFrame`.\n\nThese dtypes can be merged & reshaped & casted.\n\nReduction and groupby operations such as \u2018sum\u2019 work as well.\n\n`arrays.IntegerArray` uses `pandas.NA` as its scalar missing value. Slicing a\nsingle element that\u2019s missing will return `pandas.NA`\n\n"}, {"name": "Options and settings", "path": "user_guide/options", "type": "Manual", "text": "\npandas has an options system that lets you customize some aspects of its\nbehaviour, display-related options being those the user is most likely to\nadjust.\n\nOptions have a full \u201cdotted-style\u201d, case-insensitive name (e.g.\n`display.max_rows`). You can get/set options directly as attributes of the\ntop-level `options` attribute:\n\nThe API is composed of 5 relevant functions, available directly from the\n`pandas` namespace:\n\n`get_option()` / `set_option()` \\- get/set the value of a single option.\n\n`reset_option()` \\- reset one or more options to their default value.\n\n`describe_option()` \\- print the descriptions of one or more options.\n\n`option_context()` \\- execute a codeblock with a set of options that revert to\nprior settings after execution.\n\nNote: Developers can check out pandas/core/config_init.py for more\ninformation.\n\nAll of the functions above accept a regexp pattern (`re.search` style) as an\nargument, and so passing in a substring will work - as long as it is\nunambiguous:\n\nThe following will not work because it matches multiple option names, e.g.\n`display.max_colwidth`, `display.max_rows`, `display.max_columns`:\n\nNote: Using this form of shorthand may cause your code to break if new options\nwith similar names are added in future versions.\n\nYou can get a list of available options and their descriptions with\n`describe_option`. When called with no argument `describe_option` will print\nout the descriptions for all available options.\n\nAs described above, `get_option()` and `set_option()` are available from the\npandas namespace. To change an option, call `set_option('option regex',\nnew_value)`.\n\nNote: The option \u2018mode.sim_interactive\u2019 is mostly used for debugging purposes.\n\nAll options also have a default value, and you can use `reset_option` to do\njust that:\n\nIt\u2019s also possible to reset multiple options at once (using a regex):\n\n`option_context` context manager has been exposed through the top-level API,\nallowing you to execute code with given option values. Option values are\nrestored automatically when you exit the `with` block:\n\nUsing startup scripts for the Python/IPython environment to import pandas and\nset options makes working with pandas more efficient. To do this, create a .py\nor .ipy script in the startup directory of the desired profile. An example\nwhere the startup folder is in a default IPython profile can be found at:\n\nMore information can be found in the IPython documentation. An example startup\nscript for pandas is displayed below:\n\nThe following is a walk-through of the more frequently used display options.\n\n`display.max_rows` and `display.max_columns` sets the maximum number of rows\nand columns displayed when a frame is pretty-printed. Truncated lines are\nreplaced by an ellipsis.\n\nOnce the `display.max_rows` is exceeded, the `display.min_rows` options\ndetermines how many rows are shown in the truncated repr.\n\n`display.expand_frame_repr` allows for the representation of dataframes to\nstretch across pages, wrapped over the full column vs row-wise.\n\n`display.large_repr` lets you select whether to display dataframes that exceed\n`max_columns` or `max_rows` as a truncated frame, or as a summary.\n\n`display.max_colwidth` sets the maximum width of columns. Cells of this length\nor longer will be truncated with an ellipsis.\n\n`display.max_info_columns` sets a threshold for when by-column info will be\ngiven.\n\n`display.max_info_rows`: `df.info()` will usually show null-counts for each\ncolumn. For large frames this can be quite slow. `max_info_rows` and\n`max_info_cols` limit this null check only to frames with smaller dimensions\nthen specified. Note that you can specify the option\n`df.info(null_counts=True)` to override on showing a particular frame.\n\n`display.precision` sets the output display precision in terms of decimal\nplaces. This is only a suggestion.\n\n`display.chop_threshold` sets at what level pandas rounds to zero when it\ndisplays a Series of DataFrame. This setting does not change the precision at\nwhich the number is stored.\n\n`display.colheader_justify` controls the justification of the headers. The\noptions are \u2018right\u2019, and \u2018left\u2019.\n\nOption\n\nDefault\n\nFunction\n\ndisplay.chop_threshold\n\nNone\n\nIf set to a float value, all float values smaller then the given threshold\nwill be displayed as exactly 0 by repr and friends.\n\ndisplay.colheader_justify\n\nright\n\nControls the justification of column headers. used by DataFrameFormatter.\n\ndisplay.column_space\n\n12\n\nNo description available.\n\ndisplay.date_dayfirst\n\nFalse\n\nWhen True, prints and parses dates with the day first, eg 20/01/2005\n\ndisplay.date_yearfirst\n\nFalse\n\nWhen True, prints and parses dates with the year first, eg 2005/01/20\n\ndisplay.encoding\n\nUTF-8\n\nDefaults to the detected encoding of the console. Specifies the encoding to be\nused for strings returned by to_string, these are generally strings meant to\nbe displayed on the console.\n\ndisplay.expand_frame_repr\n\nTrue\n\nWhether to print out the full DataFrame repr for wide DataFrames across\nmultiple lines, `max_columns` is still respected, but the output will wrap-\naround across multiple \u201cpages\u201d if its width exceeds `display.width`.\n\ndisplay.float_format\n\nNone\n\nThe callable should accept a floating point number and return a string with\nthe desired format of the number. This is used in some places like\nSeriesFormatter. See core.format.EngFormatter for an example.\n\ndisplay.large_repr\n\ntruncate\n\nFor DataFrames exceeding max_rows/max_cols, the repr (and HTML repr) can show\na truncated table (the default), or switch to the view from df.info() (the\nbehaviour in earlier versions of pandas). allowable settings, [\u2018truncate\u2019,\n\u2018info\u2019]\n\ndisplay.latex.repr\n\nFalse\n\nWhether to produce a latex DataFrame representation for Jupyter frontends that\nsupport it.\n\ndisplay.latex.escape\n\nTrue\n\nEscapes special characters in DataFrames, when using the to_latex method.\n\ndisplay.latex.longtable\n\nFalse\n\nSpecifies if the to_latex method of a DataFrame uses the longtable format.\n\ndisplay.latex.multicolumn\n\nTrue\n\nCombines columns when using a MultiIndex\n\ndisplay.latex.multicolumn_format\n\n\u2018l\u2019\n\nAlignment of multicolumn labels\n\ndisplay.latex.multirow\n\nFalse\n\nCombines rows when using a MultiIndex. Centered instead of top-aligned,\nseparated by clines.\n\ndisplay.max_columns\n\n0 or 20\n\nmax_rows and max_columns are used in __repr__() methods to decide if\nto_string() or info() is used to render an object to a string. In case\nPython/IPython is running in a terminal this is set to 0 by default and pandas\nwill correctly auto-detect the width of the terminal and switch to a smaller\nformat in case all columns would not fit vertically. The IPython notebook,\nIPython qtconsole, or IDLE do not run in a terminal and hence it is not\npossible to do correct auto-detection, in which case the default is set to 20.\n\u2018None\u2019 value means unlimited.\n\ndisplay.max_colwidth\n\n50\n\nThe maximum width in characters of a column in the repr of a pandas data\nstructure. When the column overflows, a \u201c\u2026\u201d placeholder is embedded in the\noutput. \u2018None\u2019 value means unlimited.\n\ndisplay.max_info_columns\n\n100\n\nmax_info_columns is used in DataFrame.info method to decide if per column\ninformation will be printed.\n\ndisplay.max_info_rows\n\n1690785\n\ndf.info() will usually show null-counts for each column. For large frames this\ncan be quite slow. max_info_rows and max_info_cols limit this null check only\nto frames with smaller dimensions then specified.\n\ndisplay.max_rows\n\n60\n\nThis sets the maximum number of rows pandas should output when printing out\nvarious output. For example, this value determines whether the repr() for a\ndataframe prints out fully or just a truncated or summary repr. \u2018None\u2019 value\nmeans unlimited.\n\ndisplay.min_rows\n\n10\n\nThe numbers of rows to show in a truncated repr (when `max_rows` is exceeded).\nIgnored when `max_rows` is set to None or 0. When set to None, follows the\nvalue of `max_rows`.\n\ndisplay.max_seq_items\n\n100\n\nwhen pretty-printing a long sequence, no more then `max_seq_items` will be\nprinted. If items are omitted, they will be denoted by the addition of \u201c\u2026\u201d to\nthe resulting string. If set to None, the number of items to be printed is\nunlimited.\n\ndisplay.memory_usage\n\nTrue\n\nThis specifies if the memory usage of a DataFrame should be displayed when the\ndf.info() method is invoked.\n\ndisplay.multi_sparse\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display (don\u2019t display repeated elements in outer levels\nwithin groups)\n\ndisplay.notebook_repr_html\n\nTrue\n\nWhen True, IPython notebook will use html representation for pandas objects\n(if it is available).\n\ndisplay.pprint_nest_depth\n\n3\n\nControls the number of nested levels to process when pretty-printing\n\ndisplay.precision\n\n6\n\nFloating point output precision in terms of number of places after the\ndecimal, for regular formatting as well as scientific notation. Similar to\nnumpy\u2019s `precision` print option\n\ndisplay.show_dimensions\n\ntruncate\n\nWhether to print out dimensions at the end of DataFrame repr. If \u2018truncate\u2019 is\nspecified, only print out the dimensions if the frame is truncated (e.g. not\ndisplay all rows and/or columns)\n\ndisplay.width\n\n80\n\nWidth of the display in characters. In case Python/IPython is running in a\nterminal this can be set to None and pandas will correctly auto-detect the\nwidth. Note that the IPython notebook, IPython qtconsole, or IDLE do not run\nin a terminal and hence it is not possible to correctly detect the width.\n\ndisplay.html.table_schema\n\nFalse\n\nWhether to publish a Table Schema representation for frontends that support\nit.\n\ndisplay.html.border\n\n1\n\nA `border=value` attribute is inserted in the `<table>` tag for the DataFrame\nHTML repr.\n\ndisplay.html.use_mathjax\n\nTrue\n\nWhen True, Jupyter notebook will process table contents using MathJax,\nrendering mathematical expressions enclosed by the dollar symbol.\n\ndisplay.max_dir_items\n\n100\n\nThe number of columns from a dataframe that are added to dir. These columns\ncan then be suggested by tab completion. \u2018None\u2019 value means unlimited.\n\nio.excel.xls.writer\n\nxlwt\n\nThe default Excel writer engine for \u2018xls\u2019 files.\n\nDeprecated since version 1.2.0: As xlwt package is no longer maintained, the\n`xlwt` engine will be removed in a future version of pandas. Since this is the\nonly engine in pandas that supports writing to `.xls` files, this option will\nalso be removed.\n\nio.excel.xlsm.writer\n\nopenpyxl\n\nThe default Excel writer engine for \u2018xlsm\u2019 files. Available options:\n\u2018openpyxl\u2019 (the default).\n\nio.excel.xlsx.writer\n\nopenpyxl\n\nThe default Excel writer engine for \u2018xlsx\u2019 files.\n\nio.hdf.default_format\n\nNone\n\ndefault format writing format, if None, then put will default to \u2018fixed\u2019 and\nappend will default to \u2018table\u2019\n\nio.hdf.dropna_table\n\nTrue\n\ndrop ALL nan rows when appending to a table\n\nio.parquet.engine\n\nNone\n\nThe engine to use as a default for parquet reading and writing. If None then\ntry \u2018pyarrow\u2019 and \u2018fastparquet\u2019\n\nio.sql.engine\n\nNone\n\nThe engine to use as a default for sql reading and writing, with SQLAlchemy as\na higher level interface. If None then try \u2018sqlalchemy\u2019\n\nmode.chained_assignment\n\nwarn\n\nControls `SettingWithCopyWarning`: \u2018raise\u2019, \u2018warn\u2019, or None. Raise an\nexception, warn, or no action if trying to use chained assignment.\n\nmode.sim_interactive\n\nFalse\n\nWhether to simulate interactive mode for purposes of testing.\n\nmode.use_inf_as_na\n\nFalse\n\nTrue means treat None, NaN, -INF, INF as NA (old way), False means None and\nNaN are null, but INF, -INF are not NA (new way).\n\ncompute.use_bottleneck\n\nTrue\n\nUse the bottleneck library to accelerate computation if it is installed.\n\ncompute.use_numexpr\n\nTrue\n\nUse the numexpr library to accelerate computation if it is installed.\n\nplotting.backend\n\nmatplotlib\n\nChange the plotting backend to a different backend than the current matplotlib\none. Backends can be implemented as third-party libraries implementing the\npandas plotting API. They can use other plotting libraries like Bokeh, Altair,\netc.\n\nplotting.matplotlib.register_converters\n\nTrue\n\nRegister custom converters with matplotlib. Set to False to de-register.\n\nstyler.sparse.index\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display for rows in Styler output (don\u2019t display\nrepeated elements in outer levels within groups).\n\nstyler.sparse.columns\n\nTrue\n\n\u201cSparsify\u201d MultiIndex display for columns in Styler output.\n\nstyler.render.repr\n\nhtml\n\nStandard output format for Styler rendered in Jupyter Notebook. Should be one\nof \u201chtml\u201d or \u201clatex\u201d.\n\nstyler.render.max_elements\n\n262144\n\nMaximum number of datapoints that Styler will render trimming either rows,\ncolumns or both to fit.\n\nstyler.render.max_rows\n\nNone\n\nMaximum number of rows that Styler will render. By default this is dynamic\nbased on `max_elements`.\n\nstyler.render.max_columns\n\nNone\n\nMaximum number of columns that Styler will render. By default this is dynamic\nbased on `max_elements`.\n\nstyler.render.encoding\n\nutf-8\n\nDefault encoding for output HTML or LaTeX files.\n\nstyler.format.formatter\n\nNone\n\nObject to specify formatting functions to `Styler.format`.\n\nstyler.format.na_rep\n\nNone\n\nString representation for missing data.\n\nstyler.format.precision\n\n6\n\nPrecision to display floating point and complex numbers.\n\nstyler.format.decimal\n\n.\n\nString representation for decimal point separator for floating point and\ncomplex numbers.\n\nstyler.format.thousands\n\nNone\n\nString representation for thousands separator for integers, and floating point\nand complex numbers.\n\nstyler.format.escape\n\nNone\n\nWhether to escape \u201chtml\u201d or \u201clatex\u201d special characters in the display\nrepresentation.\n\nstyler.html.mathjax\n\nTrue\n\nIf set to False will render specific CSS classes to table attributes that will\nprevent Mathjax from rendering in Jupyter Notebook.\n\nstyler.latex.multicol_align\n\nr\n\nAlignment of headers in a merged column due to sparsification. Can be in {\u201cr\u201d,\n\u201cc\u201d, \u201cl\u201d}.\n\nstyler.latex.multirow_align\n\nc\n\nAlignment of index labels in a merged row due to sparsification. Can be in\n{\u201cc\u201d, \u201ct\u201d, \u201cb\u201d}.\n\nstyler.latex.environment\n\nNone\n\nIf given will replace the default `\\\\begin{table}` environment. If \u201clongtable\u201d\nis specified this will render with a specific \u201clongtable\u201d template with\nlongtable features.\n\nstyler.latex.hrules\n\nFalse\n\nIf set to True will render `\\\\toprule`, `\\\\midrule`, and `\\bottomrule` by\ndefault.\n\npandas also allows you to set how numbers are displayed in the console. This\noption is not set through the `set_options` API.\n\nUse the `set_eng_float_format` function to alter the floating-point formatting\nof pandas objects to produce a particular format.\n\nFor instance:\n\nTo round floats on a case-by-case basis, you can also use `round()` and\n`round()`.\n\nWarning\n\nEnabling this option will affect the performance for printing of DataFrame and\nSeries (about 2 times slower). Use only when it is actually required.\n\nSome East Asian countries use Unicode characters whose width corresponds to\ntwo Latin characters. If a DataFrame or Series contains these characters, the\ndefault output mode may not align them properly.\n\nNote\n\nScreen captures are attached for each output to show the actual results.\n\nEnabling `display.unicode.east_asian_width` allows pandas to check each\ncharacter\u2019s \u201cEast Asian Width\u201d property. These characters can be aligned\nproperly by setting this option to `True`. However, this will result in longer\nrender times than the standard `len` function.\n\nIn addition, Unicode characters whose width is \u201cAmbiguous\u201d can either be 1 or\n2 characters wide depending on the terminal setting or encoding. The option\n`display.unicode.ambiguous_as_wide` can be used to handle the ambiguity.\n\nBy default, an \u201cAmbiguous\u201d character\u2019s width, such as \u201c\u00a1\u201d (inverted\nexclamation) in the example below, is taken to be 1.\n\nEnabling `display.unicode.ambiguous_as_wide` makes pandas interpret these\ncharacters\u2019 widths to be 2. (Note that this option will only be effective when\n`display.unicode.east_asian_width` is enabled.)\n\nHowever, setting this option incorrectly for your terminal will cause these\ncharacters to be aligned incorrectly:\n\n`DataFrame` and `Series` will publish a Table Schema representation by\ndefault. False by default, this can be enabled globally with the\n`display.html.table_schema` option:\n\nOnly `'display.max_rows'` are serialized and published.\n\n"}, {"name": "pandas arrays, scalars, and data types", "path": "reference/arrays", "type": "Pandas arrays", "text": "\nFor most data types, pandas uses NumPy arrays as the concrete objects\ncontained with a `Index`, `Series`, or `DataFrame`.\n\nFor some data types, pandas extends NumPy\u2019s type system. String aliases for\nthese types can be found at dtypes.\n\nKind of Data\n\npandas Data Type\n\nScalar\n\nArray\n\nTZ-aware datetime\n\n`DatetimeTZDtype`\n\n`Timestamp`\n\nDatetime data\n\nTimedeltas\n\n(none)\n\n`Timedelta`\n\nTimedelta data\n\nPeriod (time spans)\n\n`PeriodDtype`\n\n`Period`\n\nTimespan data\n\nIntervals\n\n`IntervalDtype`\n\n`Interval`\n\nInterval data\n\nNullable Integer\n\n`Int64Dtype`, \u2026\n\n(none)\n\nNullable integer\n\nCategorical\n\n`CategoricalDtype`\n\n(none)\n\nCategorical data\n\nSparse\n\n`SparseDtype`\n\n(none)\n\nSparse data\n\nStrings\n\n`StringDtype`\n\n`str`\n\nText data\n\nBoolean (with NA)\n\n`BooleanDtype`\n\n`bool`\n\nBoolean data with missing values\n\npandas and third-party libraries can extend NumPy\u2019s type system (see Extension\ntypes). The top-level `array()` method can be used to create a new array,\nwhich may be stored in a `Series`, `Index`, or as a column in a `DataFrame`.\n\n`array`(data[, dtype, copy])\n\nCreate an array.\n\nNumPy cannot natively represent timezone-aware datetimes. pandas supports this\nwith the `arrays.DatetimeArray` extension array, which can hold timezone-naive\nor timezone-aware values.\n\n`Timestamp`, a subclass of `datetime.datetime`, is pandas\u2019 scalar type for\ntimezone-naive or timezone-aware datetime data.\n\n`Timestamp`([ts_input, freq, tz, unit, year, ...])\n\nPandas replacement for python datetime.datetime object.\n\n`Timestamp.asm8`\n\nReturn numpy datetime64 format in nanoseconds.\n\n`Timestamp.day`\n\n`Timestamp.dayofweek`\n\nReturn day of the week.\n\n`Timestamp.day_of_week`\n\nReturn day of the week.\n\n`Timestamp.dayofyear`\n\nReturn the day of the year.\n\n`Timestamp.day_of_year`\n\nReturn the day of the year.\n\n`Timestamp.days_in_month`\n\nReturn the number of days in the month.\n\n`Timestamp.daysinmonth`\n\nReturn the number of days in the month.\n\n`Timestamp.fold`\n\n`Timestamp.hour`\n\n`Timestamp.is_leap_year`\n\nReturn True if year is a leap year.\n\n`Timestamp.is_month_end`\n\nReturn True if date is last day of month.\n\n`Timestamp.is_month_start`\n\nReturn True if date is first day of month.\n\n`Timestamp.is_quarter_end`\n\nReturn True if date is last day of the quarter.\n\n`Timestamp.is_quarter_start`\n\nReturn True if date is first day of the quarter.\n\n`Timestamp.is_year_end`\n\nReturn True if date is last day of the year.\n\n`Timestamp.is_year_start`\n\nReturn True if date is first day of the year.\n\n`Timestamp.max`\n\n`Timestamp.microsecond`\n\n`Timestamp.min`\n\n`Timestamp.minute`\n\n`Timestamp.month`\n\n`Timestamp.nanosecond`\n\n`Timestamp.quarter`\n\nReturn the quarter of the year.\n\n`Timestamp.resolution`\n\n`Timestamp.second`\n\n`Timestamp.tz`\n\nAlias for tzinfo.\n\n`Timestamp.tzinfo`\n\n`Timestamp.value`\n\n`Timestamp.week`\n\nReturn the week number of the year.\n\n`Timestamp.weekofyear`\n\nReturn the week number of the year.\n\n`Timestamp.year`\n\n`Timestamp.astimezone`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`Timestamp.ceil`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp ceiled to this resolution.\n\n`Timestamp.combine`(date, time)\n\nCombine date, time into datetime with same date and time fields.\n\n`Timestamp.ctime`\n\nReturn ctime() style string.\n\n`Timestamp.date`\n\nReturn date object with same year, month and day.\n\n`Timestamp.day_name`\n\nReturn the day name of the Timestamp with specified locale.\n\n`Timestamp.dst`\n\nReturn self.tzinfo.dst(self).\n\n`Timestamp.floor`(freq[, ambiguous, nonexistent])\n\nReturn a new Timestamp floored to this resolution.\n\n`Timestamp.freq`\n\n`Timestamp.freqstr`\n\nReturn the total number of days in the month.\n\n`Timestamp.fromordinal`(ordinal[, freq, tz])\n\nPassed an ordinal, translate and convert to a ts.\n\n`Timestamp.fromtimestamp`(ts)\n\nTransform timestamp[, tz] to tz's local time from POSIX timestamp.\n\n`Timestamp.isocalendar`\n\nReturn a 3-tuple containing ISO year, week number, and weekday.\n\n`Timestamp.isoformat`\n\nReturn the time formatted according to ISO 8610.\n\n`Timestamp.isoweekday`()\n\nReturn the day of the week represented by the date.\n\n`Timestamp.month_name`\n\nReturn the month name of the Timestamp with specified locale.\n\n`Timestamp.normalize`\n\nNormalize Timestamp to midnight, preserving tz information.\n\n`Timestamp.now`([tz])\n\nReturn new Timestamp object representing current time local to tz.\n\n`Timestamp.replace`([year, month, day, hour, ...])\n\nImplements datetime.replace, handles nanoseconds.\n\n`Timestamp.round`(freq[, ambiguous, nonexistent])\n\nRound the Timestamp to the specified resolution.\n\n`Timestamp.strftime`(format)\n\nReturn a string representing the given POSIX timestamp controlled by an\nexplicit format string.\n\n`Timestamp.strptime`(string, format)\n\nFunction is not implemented.\n\n`Timestamp.time`\n\nReturn time object with same time but with tzinfo=None.\n\n`Timestamp.timestamp`\n\nReturn POSIX timestamp as float.\n\n`Timestamp.timetuple`\n\nReturn time tuple, compatible with time.localtime().\n\n`Timestamp.timetz`\n\nReturn time object with same time and tzinfo.\n\n`Timestamp.to_datetime64`\n\nReturn a numpy.datetime64 object with 'ns' precision.\n\n`Timestamp.to_numpy`\n\nConvert the Timestamp to a NumPy datetime64.\n\n`Timestamp.to_julian_date`()\n\nConvert TimeStamp to a Julian Date.\n\n`Timestamp.to_period`\n\nReturn an period of which this timestamp is an observation.\n\n`Timestamp.to_pydatetime`\n\nConvert a Timestamp object to a native Python datetime object.\n\n`Timestamp.today`(cls[, tz])\n\nReturn the current time in the local timezone.\n\n`Timestamp.toordinal`\n\nReturn proleptic Gregorian ordinal.\n\n`Timestamp.tz_convert`(tz)\n\nConvert timezone-aware Timestamp to another time zone.\n\n`Timestamp.tz_localize`(tz[, ambiguous, ...])\n\nConvert naive Timestamp to local time zone, or remove timezone from timezone-\naware Timestamp.\n\n`Timestamp.tzname`\n\nReturn self.tzinfo.tzname(self).\n\n`Timestamp.utcfromtimestamp`(ts)\n\nConstruct a naive UTC datetime from a POSIX timestamp.\n\n`Timestamp.utcnow`()\n\nReturn a new Timestamp representing UTC day and time.\n\n`Timestamp.utcoffset`\n\nReturn self.tzinfo.utcoffset(self).\n\n`Timestamp.utctimetuple`\n\nReturn UTC time tuple, compatible with time.localtime().\n\n`Timestamp.weekday`()\n\nReturn the day of the week represented by the date.\n\nA collection of timestamps may be stored in a `arrays.DatetimeArray`. For\ntimezone-aware data, the `.dtype` of a `arrays.DatetimeArray` is a\n`DatetimeTZDtype`. For timezone-naive data, `np.dtype(\"datetime64[ns]\")` is\nused.\n\nIf the data are timezone-aware, then every value in the array must have the\nsame timezone.\n\n`arrays.DatetimeArray`(values[, dtype, freq, copy])\n\nPandas ExtensionArray for tz-naive or tz-aware datetime data.\n\n`DatetimeTZDtype`([unit, tz])\n\nAn ExtensionDtype for timezone-aware datetime data.\n\nNumPy can natively represent timedeltas. pandas provides `Timedelta` for\nsymmetry with `Timestamp`.\n\n`Timedelta`([value, unit])\n\nRepresents a duration, the difference between two dates or times.\n\n`Timedelta.asm8`\n\nReturn a numpy timedelta64 array scalar view.\n\n`Timedelta.components`\n\nReturn a components namedtuple-like.\n\n`Timedelta.days`\n\nNumber of days.\n\n`Timedelta.delta`\n\nReturn the timedelta in nanoseconds (ns), for internal compatibility.\n\n`Timedelta.freq`\n\n`Timedelta.is_populated`\n\n`Timedelta.max`\n\n`Timedelta.microseconds`\n\nNumber of microseconds (>= 0 and less than 1 second).\n\n`Timedelta.min`\n\n`Timedelta.nanoseconds`\n\nReturn the number of nanoseconds (n), where 0 <= n < 1 microsecond.\n\n`Timedelta.resolution`\n\n`Timedelta.seconds`\n\nNumber of seconds (>= 0 and less than 1 day).\n\n`Timedelta.value`\n\n`Timedelta.view`\n\nArray view compatibility.\n\n`Timedelta.ceil`(freq)\n\nReturn a new Timedelta ceiled to this resolution.\n\n`Timedelta.floor`(freq)\n\nReturn a new Timedelta floored to this resolution.\n\n`Timedelta.isoformat`\n\nFormat Timedelta as ISO 8601 Duration like `P[n]Y[n]M[n]DT[n]H[n]M[n]S`, where\nthe `[n]` s are replaced by the values.\n\n`Timedelta.round`(freq)\n\nRound the Timedelta to the specified resolution.\n\n`Timedelta.to_pytimedelta`\n\nConvert a pandas Timedelta object into a python `datetime.timedelta` object.\n\n`Timedelta.to_timedelta64`\n\nReturn a numpy.timedelta64 object with 'ns' precision.\n\n`Timedelta.to_numpy`\n\nConvert the Timedelta to a NumPy timedelta64.\n\n`Timedelta.total_seconds`\n\nTotal seconds in the duration.\n\nA collection of `Timedelta` may be stored in a `TimedeltaArray`.\n\n`arrays.TimedeltaArray`(values[, dtype, freq, ...])\n\nPandas ExtensionArray for timedelta data.\n\npandas represents spans of times as `Period` objects.\n\n`Period`([value, freq, ordinal, year, month, ...])\n\nRepresents a period of time.\n\n`Period.day`\n\nGet day of the month that a Period falls on.\n\n`Period.dayofweek`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.day_of_week`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.dayofyear`\n\nReturn the day of the year.\n\n`Period.day_of_year`\n\nReturn the day of the year.\n\n`Period.days_in_month`\n\nGet the total number of days in the month that this period falls on.\n\n`Period.daysinmonth`\n\nGet the total number of days of the month that the Period falls in.\n\n`Period.end_time`\n\nGet the Timestamp for the end of the period.\n\n`Period.freq`\n\n`Period.freqstr`\n\nReturn a string representation of the frequency.\n\n`Period.hour`\n\nGet the hour of the day component of the Period.\n\n`Period.is_leap_year`\n\nReturn True if the period's year is in a leap year.\n\n`Period.minute`\n\nGet minute of the hour component of the Period.\n\n`Period.month`\n\nReturn the month this Period falls on.\n\n`Period.ordinal`\n\n`Period.quarter`\n\nReturn the quarter this Period falls on.\n\n`Period.qyear`\n\nFiscal year the Period lies in according to its starting-quarter.\n\n`Period.second`\n\nGet the second component of the Period.\n\n`Period.start_time`\n\nGet the Timestamp for the start of the period.\n\n`Period.week`\n\nGet the week of the year on the given Period.\n\n`Period.weekday`\n\nDay of the week the period lies in, with Monday=0 and Sunday=6.\n\n`Period.weekofyear`\n\nGet the week of the year on the given Period.\n\n`Period.year`\n\nReturn the year this Period falls on.\n\n`Period.asfreq`\n\nConvert Period to desired frequency, at the start or end of the interval.\n\n`Period.now`\n\nReturn the period of now's date.\n\n`Period.strftime`\n\nReturns the string representation of the `Period`, depending on the selected\n`fmt`.\n\n`Period.to_timestamp`\n\nReturn the Timestamp representation of the Period.\n\nA collection of `Period` may be stored in a `arrays.PeriodArray`. Every period\nin a `arrays.PeriodArray` must have the same `freq`.\n\n`arrays.PeriodArray`(values[, dtype, freq, copy])\n\nPandas ExtensionArray for storing Period data.\n\n`PeriodDtype`([freq])\n\nAn ExtensionDtype for Period data.\n\nArbitrary intervals can be represented as `Interval` objects.\n\n`Interval`\n\nImmutable object implementing an Interval, a bounded slice-like interval.\n\n`Interval.closed`\n\nWhether the interval is closed on the left-side, right-side, both or neither.\n\n`Interval.closed_left`\n\nCheck if the interval is closed on the left side.\n\n`Interval.closed_right`\n\nCheck if the interval is closed on the right side.\n\n`Interval.is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`Interval.left`\n\nLeft bound for the interval.\n\n`Interval.length`\n\nReturn the length of the Interval.\n\n`Interval.mid`\n\nReturn the midpoint of the Interval.\n\n`Interval.open_left`\n\nCheck if the interval is open on the left side.\n\n`Interval.open_right`\n\nCheck if the interval is open on the right side.\n\n`Interval.overlaps`\n\nCheck whether two Interval objects overlap.\n\n`Interval.right`\n\nRight bound for the interval.\n\nA collection of intervals may be stored in an `arrays.IntervalArray`.\n\n`arrays.IntervalArray`(data[, closed, dtype, ...])\n\nPandas array for interval data that are closed on the same side.\n\n`IntervalDtype`([subtype, closed])\n\nAn ExtensionDtype for Interval data.\n\n`numpy.ndarray` cannot natively represent integer-data with missing values.\npandas provides this through `arrays.IntegerArray`.\n\n`arrays.IntegerArray`(values, mask[, copy])\n\nArray of integer (optional missing) values.\n\n`Int8Dtype`()\n\nAn ExtensionDtype for int8 integer data.\n\n`Int16Dtype`()\n\nAn ExtensionDtype for int16 integer data.\n\n`Int32Dtype`()\n\nAn ExtensionDtype for int32 integer data.\n\n`Int64Dtype`()\n\nAn ExtensionDtype for int64 integer data.\n\n`UInt8Dtype`()\n\nAn ExtensionDtype for uint8 integer data.\n\n`UInt16Dtype`()\n\nAn ExtensionDtype for uint16 integer data.\n\n`UInt32Dtype`()\n\nAn ExtensionDtype for uint32 integer data.\n\n`UInt64Dtype`()\n\nAn ExtensionDtype for uint64 integer data.\n\npandas defines a custom data type for representing data that can take only a\nlimited, fixed set of values. The dtype of a `Categorical` can be described by\na `CategoricalDtype`.\n\n`CategoricalDtype`([categories, ordered])\n\nType for categorical data with the categories and orderedness.\n\n`CategoricalDtype.categories`\n\nAn `Index` containing the unique categories allowed.\n\n`CategoricalDtype.ordered`\n\nWhether the categories have an ordered relationship.\n\nCategorical data can be stored in a `pandas.Categorical`\n\n`Categorical`(values[, categories, ordered, ...])\n\nRepresent a categorical variable in classic R / S-plus fashion.\n\nThe alternative `Categorical.from_codes()` constructor can be used when you\nhave the categories and integer codes already:\n\n`Categorical.from_codes`(codes[, categories, ...])\n\nMake a Categorical type from codes and categories or dtype.\n\nThe dtype information is available on the `Categorical`\n\n`Categorical.dtype`\n\nThe `CategoricalDtype` for this instance.\n\n`Categorical.categories`\n\nThe categories of this categorical.\n\n`Categorical.ordered`\n\nWhether the categories have an ordered relationship.\n\n`Categorical.codes`\n\nThe category codes of this categorical.\n\n`np.asarray(categorical)` works by implementing the array interface. Be aware,\nthat this converts the `Categorical` back to a NumPy array, so categories and\norder information is not preserved!\n\n`Categorical.__array__`([dtype])\n\nThe numpy array interface.\n\nA `Categorical` can be stored in a `Series` or `DataFrame`. To create a Series\nof dtype `category`, use `cat = s.astype(dtype)` or `Series(..., dtype=dtype)`\nwhere `dtype` is either\n\nthe string `'category'`\n\nan instance of `CategoricalDtype`.\n\nIf the `Series` is of dtype `CategoricalDtype`, `Series.cat` can be used to\nchange the categorical data. See Categorical accessor for more.\n\nData where a single value is repeated many times (e.g. `0` or `NaN`) may be\nstored efficiently as a `arrays.SparseArray`.\n\n`arrays.SparseArray`(data[, sparse_index, ...])\n\nAn ExtensionArray for storing sparse data.\n\n`SparseDtype`([dtype, fill_value])\n\nDtype for data stored in `SparseArray`.\n\nThe `Series.sparse` accessor may be used to access sparse-specific attributes\nand methods if the `Series` contains sparse values. See Sparse accessor for\nmore.\n\nWhen working with text data, where each valid element is a string or missing,\nwe recommend using `StringDtype` (with the alias `\"string\"`).\n\n`arrays.StringArray`(values[, copy])\n\nExtension array for string data.\n\n`arrays.ArrowStringArray`(values)\n\nExtension array for string data in a `pyarrow.ChunkedArray`.\n\n`StringDtype`([storage])\n\nExtension dtype for string data.\n\nThe `Series.str` accessor is available for `Series` backed by a\n`arrays.StringArray`. See String handling for more.\n\nThe boolean dtype (with the alias `\"boolean\"`) provides support for storing\nboolean data (`True`, `False`) with missing values, which is not possible with\na bool `numpy.ndarray`.\n\n`arrays.BooleanArray`(values, mask[, copy])\n\nArray of boolean (True/False) data with missing values.\n\n`BooleanDtype`()\n\nExtension dtype for boolean data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray", "path": "reference/api/pandas.api.extensions.extensionarray", "type": "Extensions", "text": "\nAbstract base class for custom 1-D array types.\n\npandas will recognize instances of this class as proper arrays with a custom\ntype and will not attempt to coerce them to objects. They may be stored\ndirectly inside a `DataFrame` or `Series`.\n\nNotes\n\nThe interface includes the following abstract methods that must be implemented\nby subclasses:\n\n_from_sequence\n\n_from_factorized\n\n__getitem__\n\n__len__\n\n__eq__\n\ndtype\n\nnbytes\n\nisna\n\ntake\n\ncopy\n\n_concat_same_type\n\nA default repr displaying the type, (truncated) data, length, and dtype is\nprovided. It can be customized or replaced by by overriding:\n\n__repr__ : A default repr for the ExtensionArray.\n\n_formatter : Print scalars inside a Series or DataFrame.\n\nSome methods require casting the ExtensionArray to an ndarray of Python\nobjects with `self.astype(object)`, which may be expensive. When performance\nis a concern, we highly recommend overriding the following methods:\n\nfillna\n\ndropna\n\nunique\n\nfactorize / _values_for_factorize\n\nargsort / _values_for_argsort\n\nsearchsorted\n\nThe remaining methods implemented on this class should be performant, as they\nonly compose abstract methods. Still, a more efficient implementation may be\navailable, and these methods can be overridden.\n\nOne can implement methods to handle array reductions.\n\n_reduce\n\nOne can implement methods to handle parsing from strings that will be used in\nmethods such as `pandas.io.parsers.read_csv`.\n\n_from_sequence_of_strings\n\nThis class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons.\nMethods and properties required by the interface raise\n`pandas.errors.AbstractMethodError` and no `register` method is provided for\nregistering virtual subclasses.\n\nExtensionArrays are limited to 1 dimension.\n\nThey may be backed by none, one, or many NumPy arrays. For example,\n`pandas.Categorical` is an extension array backed by two arrays, one for codes\nand one for categories. An array of IPv6 address may be backed by a NumPy\nstructured array with two fields, one for the lower 64 bits and one for the\nupper 64 bits. Or they may be backed by some other storage type, like Python\nlists. Pandas makes no assumptions on how the data are stored, just that it\ncan be converted to a NumPy array. The ExtensionArray interface does not\nimpose any rules on how this data is stored. However, currently, the backing\ndata cannot be stored in attributes called `.values` or `._values` to ensure\nfull compatibility with pandas internals. But other names as `.data`,\n`._data`, `._items`, \u2026 can be freely used.\n\nIf implementing NumPy\u2019s `__array_ufunc__` interface, pandas expects that\n\nYou defer by returning `NotImplemented` when any Series are present in inputs.\nPandas will extract the arrays and call the ufunc again.\n\nYou define a `_HANDLED_TYPES` tuple as an attribute on the class. Pandas\ninspect this to determine whether the ufunc is valid for the types present.\n\nSee NumPy universal functions for more.\n\nBy default, ExtensionArrays are not hashable. Immutable subclasses may\noverride this behavior.\n\nAttributes\n\n`dtype`\n\nAn instance of 'ExtensionDtype'.\n\n`nbytes`\n\nThe number of bytes needed to store this object in memory.\n\n`ndim`\n\nExtension Arrays are only allowed to be 1-dimensional.\n\n`shape`\n\nReturn a tuple of the array dimensions.\n\nMethods\n\n`argsort`([ascending, kind, na_position])\n\nReturn the indices that would sort this array.\n\n`astype`(dtype[, copy])\n\nCast to a NumPy array or ExtensionArray with 'dtype'.\n\n`copy`()\n\nReturn a copy of the array.\n\n`dropna`()\n\nReturn ExtensionArray without NA values.\n\n`factorize`([na_sentinel])\n\nEncode the extension array as an enumerated type.\n\n`fillna`([value, method, limit])\n\nFill NA/NaN values using the specified method.\n\n`equals`(other)\n\nReturn if another array is equivalent to this array.\n\n`insert`(loc, item)\n\nInsert an item at the given position.\n\n`isin`(values)\n\nPointwise comparison for set containment in the given values.\n\n`isna`()\n\nA 1-D array indicating if each value is missing.\n\n`ravel`([order])\n\nReturn a flattened view on this array.\n\n`repeat`(repeats[, axis])\n\nRepeat elements of a ExtensionArray.\n\n`searchsorted`(value[, side, sorter])\n\nFind indices where elements should be inserted to maintain order.\n\n`shift`([periods, fill_value])\n\nShift values by desired number.\n\n`take`(indices, *[, allow_fill, fill_value])\n\nTake elements from an array.\n\n`tolist`()\n\nReturn a list of the values.\n\n`unique`()\n\nCompute the ExtensionArray of unique values.\n\n`view`([dtype])\n\nReturn a view on the array.\n\n`_concat_same_type`(to_concat)\n\nConcatenate multiple array of this dtype.\n\n`_formatter`([boxed])\n\nFormatting function for scalar values.\n\n`_from_factorized`(values, original)\n\nReconstruct an ExtensionArray after factorization.\n\n`_from_sequence`(scalars, *[, dtype, copy])\n\nConstruct a new ExtensionArray from a sequence of scalars.\n\n`_from_sequence_of_strings`(strings, *[, ...])\n\nConstruct a new ExtensionArray from a sequence of strings.\n\n`_reduce`(name, *[, skipna])\n\nReturn a scalar result of performing the reduction operation.\n\n`_values_for_argsort`()\n\nReturn values for sorting.\n\n`_values_for_factorize`()\n\nReturn an array and missing value suitable for factorization.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._concat_same_type", "path": "reference/api/pandas.api.extensions.extensionarray._concat_same_type", "type": "Extensions", "text": "\nConcatenate multiple array of this dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._formatter", "path": "reference/api/pandas.api.extensions.extensionarray._formatter", "type": "Extensions", "text": "\nFormatting function for scalar values.\n\nThis is used in the default \u2018__repr__\u2019. The returned formatting function\nreceives instances of your scalar type.\n\nAn indicated for whether or not your array is being printed within a Series,\nDataFrame, or Index (True), or just by itself (False). This may be useful if\nyou want scalar values to appear differently within a Series versus on its own\n(e.g. quoted or not).\n\nA callable that gets instances of the scalar type and returns a string. By\ndefault, `repr()` is used when `boxed=False` and `str()` is used when\n`boxed=True`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_factorized", "path": "reference/api/pandas.api.extensions.extensionarray._from_factorized", "type": "Extensions", "text": "\nReconstruct an ExtensionArray after factorization.\n\nAn integer ndarray with the factorized values.\n\nThe original ExtensionArray that factorize was called on.\n\nSee also\n\nTop-level factorize method that dispatches here.\n\nEncode the extension array as an enumerated type.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence", "type": "Extensions", "text": "\nConstruct a new ExtensionArray from a sequence of scalars.\n\nEach element will be an instance of the scalar type for this array,\n`cls.dtype.type` or be converted into this type in this method.\n\nConstruct for this particular dtype. This should be a Dtype compatible with\nthe ExtensionArray.\n\nIf True, copy the underlying data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._from_sequence_of_strings", "path": "reference/api/pandas.api.extensions.extensionarray._from_sequence_of_strings", "type": "Extensions", "text": "\nConstruct a new ExtensionArray from a sequence of strings.\n\nEach element will be an instance of the scalar type for this array,\n`cls.dtype.type`.\n\nConstruct for this particular dtype. This should be a Dtype compatible with\nthe ExtensionArray.\n\nIf True, copy the underlying data.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._reduce", "path": "reference/api/pandas.api.extensions.extensionarray._reduce", "type": "Extensions", "text": "\nReturn a scalar result of performing the reduction operation.\n\nName of the function, supported values are: { any, all, min, max, sum, mean,\nmedian, prod, std, var, sem, kurt, skew }.\n\nIf True, skip NaN values.\n\nAdditional keyword arguments passed to the reduction function. Currently, ddof\nis the only supported kwarg.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._values_for_argsort", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_argsort", "type": "Extensions", "text": "\nReturn values for sorting.\n\nThe transformed values should maintain the ordering between values within the\narray.\n\nSee also\n\nReturn the indices that would sort this array.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray._values_for_factorize", "path": "reference/api/pandas.api.extensions.extensionarray._values_for_factorize", "type": "Extensions", "text": "\nReturn an array and missing value suitable for factorization.\n\nAn array suitable for factorization. This should maintain order and be a\nsupported dtype (Float64, Int64, UInt64, String, Object). By default, the\nextension array is cast to object dtype.\n\nThe value in values to consider missing. This will be treated as NA in the\nfactorization routines, so it will be coded as na_sentinel and not included in\nuniques. By default, `np.nan` is used.\n\nNotes\n\nThe values returned by this method are also used in\n`pandas.util.hash_pandas_object()`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.argsort", "path": "reference/api/pandas.api.extensions.extensionarray.argsort", "type": "Extensions", "text": "\nReturn the indices that would sort this array.\n\nWhether the indices should result in an ascending or descending sort.\n\nSorting algorithm.\n\nPassed through to `numpy.argsort()`.\n\nArray of indices that sort `self`. If NaN values are contained, NaN values are\nplaced at the end.\n\nSee also\n\nSorting implementation used internally.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.astype", "path": "reference/api/pandas.api.extensions.extensionarray.astype", "type": "Extensions", "text": "\nCast to a NumPy array or ExtensionArray with \u2018dtype\u2019.\n\nTypecode or data-type to which the array is cast.\n\nWhether to copy the data, even if not necessary. If False, a copy is made only\nif the old dtype does not match the new dtype.\n\nAn ExtensionArray if dtype is ExtensionDtype, Otherwise a NumPy ndarray with\n\u2018dtype\u2019 for its dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.copy", "path": "reference/api/pandas.api.extensions.extensionarray.copy", "type": "Extensions", "text": "\nReturn a copy of the array.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.dropna", "path": "reference/api/pandas.api.extensions.extensionarray.dropna", "type": "Extensions", "text": "\nReturn ExtensionArray without NA values.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.dtype", "path": "reference/api/pandas.api.extensions.extensionarray.dtype", "type": "Extensions", "text": "\nAn instance of \u2018ExtensionDtype\u2019.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.equals", "path": "reference/api/pandas.api.extensions.extensionarray.equals", "type": "Extensions", "text": "\nReturn if another array is equivalent to this array.\n\nEquivalent means that both arrays have the same shape and dtype, and all\nvalues compare equal. Missing values in the same location are considered equal\n(in contrast with normal equality).\n\nArray to compare to this Array.\n\nWhether the arrays are equivalent.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.factorize", "path": "reference/api/pandas.api.extensions.extensionarray.factorize", "type": "Extensions", "text": "\nEncode the extension array as an enumerated type.\n\nValue to use in the codes array to indicate missing values.\n\nAn integer NumPy array that\u2019s an indexer into the original ExtensionArray.\n\nAn ExtensionArray containing the unique values of self.\n\nNote\n\nuniques will not contain an entry for the NA value of the ExtensionArray if\nthere are any missing values present in self.\n\nSee also\n\nTop-level factorize method that dispatches here.\n\nNotes\n\n`pandas.factorize()` offers a sort keyword as well.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.fillna", "path": "reference/api/pandas.api.extensions.extensionarray.fillna", "type": "Extensions", "text": "\nFill NA/NaN values using the specified method.\n\nIf a scalar value is passed it is used to fill all missing values.\nAlternatively, an array-like \u2018value\u2019 can be given. It\u2019s expected that the\narray-like have the same length as \u2018self\u2019.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use NEXT valid\nobservation to fill gap.\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled.\n\nWith NA/NaN filled.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.insert", "path": "reference/api/pandas.api.extensions.extensionarray.insert", "type": "Extensions", "text": "\nInsert an item at the given position.\n\nNotes\n\nThis method should be both type and dtype-preserving. If the item cannot be\nheld in an array of this type/dtype, either ValueError or TypeError should be\nraised.\n\nThe default implementation relies on _from_sequence to raise on invalid items.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.isin", "path": "reference/api/pandas.api.extensions.extensionarray.isin", "type": "Extensions", "text": "\nPointwise comparison for set containment in the given values.\n\nRoughly equivalent to np.array([x in values for x in self])\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.isna", "path": "reference/api/pandas.api.extensions.extensionarray.isna", "type": "Extensions", "text": "\nA 1-D array indicating if each value is missing.\n\nIn most cases, this should return a NumPy ndarray. For exceptional cases like\n`SparseArray`, where returning an ndarray would be expensive, an\nExtensionArray may be returned.\n\nNotes\n\nIf returning an ExtensionArray, then\n\n`na_values._is_boolean` should be True\n\nna_values should implement `ExtensionArray._reduce()`\n\n`na_values.any` and `na_values.all` should be implemented\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.nbytes", "path": "reference/api/pandas.api.extensions.extensionarray.nbytes", "type": "Extensions", "text": "\nThe number of bytes needed to store this object in memory.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.ndim", "path": "reference/api/pandas.api.extensions.extensionarray.ndim", "type": "Extensions", "text": "\nExtension Arrays are only allowed to be 1-dimensional.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.ravel", "path": "reference/api/pandas.api.extensions.extensionarray.ravel", "type": "Extensions", "text": "\nReturn a flattened view on this array.\n\nNotes\n\nBecause ExtensionArrays are 1D-only, this is a no-op.\n\nThe \u201corder\u201d argument is ignored, is for compatibility with NumPy.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.repeat", "path": "reference/api/pandas.api.extensions.extensionarray.repeat", "type": "Extensions", "text": "\nRepeat elements of a ExtensionArray.\n\nReturns a new ExtensionArray where each element of the current ExtensionArray\nis repeated consecutively a given number of times.\n\nThe number of repetitions for each element. This should be a non-negative\ninteger. Repeating 0 times will return an empty ExtensionArray.\n\nMust be `None`. Has no effect but is accepted for compatibility with numpy.\n\nNewly created ExtensionArray with repeated elements.\n\nSee also\n\nEquivalent function for Series.\n\nEquivalent function for Index.\n\nSimilar method for `numpy.ndarray`.\n\nTake arbitrary positions.\n\nExamples\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.searchsorted", "path": "reference/api/pandas.api.extensions.extensionarray.searchsorted", "type": "Extensions", "text": "\nFind indices where elements should be inserted to maintain order.\n\nFind the indices into a sorted array self (a) such that, if the corresponding\nelements in value were inserted before the indices, the order of self would be\npreserved.\n\nAssuming that self is sorted:\n\nside\n\nreturned index i satisfies\n\nleft\n\n`self[i-1] < value <= self[i]`\n\nright\n\n`self[i-1] <= value < self[i]`\n\nValue(s) to insert into self.\n\nIf \u2018left\u2019, the index of the first suitable location found is given. If\n\u2018right\u2019, return the last such index. If there is no suitable index, return\neither 0 or N (where N is the length of self).\n\nOptional array of integer indices that sort array a into ascending order. They\nare typically the result of argsort.\n\nIf value is array-like, array of insertion points. If value is scalar, a\nsingle integer.\n\nSee also\n\nSimilar method from NumPy.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.shape", "path": "reference/api/pandas.api.extensions.extensionarray.shape", "type": "Extensions", "text": "\nReturn a tuple of the array dimensions.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.shift", "path": "reference/api/pandas.api.extensions.extensionarray.shift", "type": "Extensions", "text": "\nShift values by desired number.\n\nNewly introduced missing values are filled with `self.dtype.na_value`.\n\nThe number of periods to shift. Negative values are allowed for shifting\nbackwards.\n\nThe scalar value to use for newly introduced missing values. The default is\n`self.dtype.na_value`.\n\nShifted.\n\nNotes\n\nIf `self` is empty or `periods` is 0, a copy of `self` is returned.\n\nIf `periods > len(self)`, then an array of size len(self) is returned, with\nall values filled with `self.dtype.na_value`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.take", "path": "reference/api/pandas.api.extensions.extensionarray.take", "type": "Extensions", "text": "\nTake elements from an array.\n\nIndices to be taken.\n\nHow to handle negative values in indices.\n\nFalse: negative values in indices indicate positional indices from the right\n(the default). This is similar to `numpy.take()`.\n\nTrue: negative values in indices indicate missing values. These values are set\nto fill_value. Any other other negative values raise a `ValueError`.\n\nFill value to use for NA-indices when allow_fill is True. This may be `None`,\nin which case the default NA value for the type, `self.dtype.na_value`, is\nused.\n\nFor many ExtensionArrays, there will be two representations of fill_value: a\nuser-facing \u201cboxed\u201d scalar, and a low-level physical NA value. fill_value\nshould be the user-facing version, and the implementation should handle\ntranslating that to the physical version for processing the take if necessary.\n\nWhen the indices are out of bounds for the array.\n\nWhen indices contains negative values other than `-1` and allow_fill is True.\n\nSee also\n\nTake elements from an array along an axis.\n\nTake elements from an array.\n\nNotes\n\nExtensionArray.take is called by `Series.__getitem__`, `.loc`, `iloc`, when\nindices is a sequence of values. Additionally, it\u2019s called by\n`Series.reindex()`, or any other method that causes realignment, with a\nfill_value.\n\nExamples\n\nHere\u2019s an example implementation, which relies on casting the extension array\nto object dtype. This uses the helper method `pandas.api.extensions.take()`.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.tolist", "path": "reference/api/pandas.api.extensions.extensionarray.tolist", "type": "Extensions", "text": "\nReturn a list of the values.\n\nThese are each a scalar type, which is a Python scalar (for str, int, float)\nor a pandas scalar (for Timestamp/Timedelta/Interval/Period)\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.unique", "path": "reference/api/pandas.api.extensions.extensionarray.unique", "type": "Extensions", "text": "\nCompute the ExtensionArray of unique values.\n\n"}, {"name": "pandas.api.extensions.ExtensionArray.view", "path": "reference/api/pandas.api.extensions.extensionarray.view", "type": "Extensions", "text": "\nReturn a view on the array.\n\nDefault None.\n\nA view on the `ExtensionArray`\u2019s data.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype", "path": "reference/api/pandas.api.extensions.extensiondtype", "type": "Extensions", "text": "\nA custom data type, to be paired with an ExtensionArray.\n\nSee also\n\nRegister an ExtensionType with pandas as class decorator.\n\nAbstract base class for custom 1-D array types.\n\nNotes\n\nThe interface includes the following abstract methods that must be implemented\nby subclasses:\n\ntype\n\nname\n\nconstruct_array_type\n\nThe following attributes and methods influence the behavior of the dtype in\npandas operations\n\n_is_numeric\n\n_is_boolean\n\n_get_common_dtype\n\nThe na_value class attribute can be used to set the default NA value for this\ntype. `numpy.nan` is used by default.\n\nExtensionDtypes are required to be hashable. The base class provides a default\nimplementation, which relies on the `_metadata` class attribute. `_metadata`\nshould be a tuple containing the strings that define your data type. For\nexample, with `PeriodDtype` that\u2019s the `freq` attribute.\n\nIf you have a parametrized dtype you should set the ``_metadata`` class\nproperty.\n\nIdeally, the attributes in `_metadata` will match the parameters to your\n`ExtensionDtype.__init__` (if any). If any of the attributes in `_metadata`\ndon\u2019t implement the standard `__eq__` or `__hash__`, the default\nimplementations here will not work.\n\nFor interaction with Apache Arrow (pyarrow), a `__from_arrow__` method can be\nimplemented: this method receives a pyarrow Array or ChunkedArray as only\nargument and is expected to return the appropriate pandas ExtensionArray for\nthis dtype and the passed values:\n\nThis class does not inherit from \u2018abc.ABCMeta\u2019 for performance reasons.\nMethods and properties required by the interface raise\n`pandas.errors.AbstractMethodError` and no `register` method is provided for\nregistering virtual subclasses.\n\nAttributes\n\n`kind`\n\nA character code (one of 'biufcmMOSUV'), default 'O'\n\n`na_value`\n\nDefault NA value to use for this type.\n\n`name`\n\nA string identifying the data type.\n\n`names`\n\nOrdered list of field names, or None if there are no fields.\n\n`type`\n\nThe scalar type for the array, e.g.\n\nMethods\n\n`construct_array_type`()\n\nReturn the array type associated with this dtype.\n\n`construct_from_string`(string)\n\nConstruct this type from a string.\n\n`empty`(shape)\n\nConstruct an ExtensionArray of this dtype with the given shape.\n\n`is_dtype`(dtype)\n\nCheck if we match 'dtype'.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.construct_array_type", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_array_type", "type": "Extensions", "text": "\nReturn the array type associated with this dtype.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.construct_from_string", "path": "reference/api/pandas.api.extensions.extensiondtype.construct_from_string", "type": "Extensions", "text": "\nConstruct this type from a string.\n\nThis is useful mainly for data types that accept parameters. For example, a\nperiod dtype accepts a frequency parameter that can be set as `period[H]`\n(where H means hourly frequency).\n\nBy default, in the abstract class, just the name of the type is expected. But\nsubclasses can overwrite this method to accept parameters.\n\nThe name of the type, for example `category`.\n\nInstance of the dtype.\n\nIf a class cannot be constructed from this \u2018string\u2019.\n\nExamples\n\nFor extension dtypes with arguments the following may be an adequate\nimplementation.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.empty", "path": "reference/api/pandas.api.extensions.extensiondtype.empty", "type": "Extensions", "text": "\nConstruct an ExtensionArray of this dtype with the given shape.\n\nAnalogous to numpy.empty.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.is_dtype", "path": "reference/api/pandas.api.extensions.extensiondtype.is_dtype", "type": "Extensions", "text": "\nCheck if we match \u2018dtype\u2019.\n\nThe object to check.\n\nNotes\n\nThe default implementation is True if\n\n`cls.construct_from_string(dtype)` is an instance of `cls`.\n\n`dtype` is an object and is an instance of `cls`\n\n`dtype` has a `dtype` attribute, and any of the above conditions is true for\n`dtype.dtype`.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.kind", "path": "reference/api/pandas.api.extensions.extensiondtype.kind", "type": "Extensions", "text": "\nA character code (one of \u2018biufcmMOSUV\u2019), default \u2018O\u2019\n\nThis should match the NumPy dtype used when the array is converted to an\nndarray, which is probably \u2018O\u2019 for object if the extension type cannot be\nrepresented as a built-in NumPy type.\n\nSee also\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.na_value", "path": "reference/api/pandas.api.extensions.extensiondtype.na_value", "type": "Extensions", "text": "\nDefault NA value to use for this type.\n\nThis is used in e.g. ExtensionArray.take. This should be the user-facing\n\u201cboxed\u201d version of the NA value, not the physical NA value for storage. e.g.\nfor JSONArray, this is an empty dictionary.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.name", "path": "reference/api/pandas.api.extensions.extensiondtype.name", "type": "Extensions", "text": "\nA string identifying the data type.\n\nWill be used for display in, e.g. `Series.dtype`\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.names", "path": "reference/api/pandas.api.extensions.extensiondtype.names", "type": "Extensions", "text": "\nOrdered list of field names, or None if there are no fields.\n\nThis is for compatibility with NumPy arrays, and may be removed in the future.\n\n"}, {"name": "pandas.api.extensions.ExtensionDtype.type", "path": "reference/api/pandas.api.extensions.extensiondtype.type", "type": "Extensions", "text": "\nThe scalar type for the array, e.g. `int`\n\nIt\u2019s expected `ExtensionArray[item]` returns an instance of\n`ExtensionDtype.type` for scalar `item`, assuming that value is valid (not\nNA). NA values do not need to be instances of type.\n\n"}, {"name": "pandas.api.extensions.register_dataframe_accessor", "path": "reference/api/pandas.api.extensions.register_dataframe_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on DataFrame objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.extensions.register_extension_dtype", "path": "reference/api/pandas.api.extensions.register_extension_dtype", "type": "Extensions", "text": "\nRegister an ExtensionType with pandas as class decorator.\n\nThis enables operations like `.astype(name)` for the name of the\nExtensionDtype.\n\nA class decorator.\n\nExamples\n\n"}, {"name": "pandas.api.extensions.register_index_accessor", "path": "reference/api/pandas.api.extensions.register_index_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on Index objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.extensions.register_series_accessor", "path": "reference/api/pandas.api.extensions.register_series_accessor", "type": "Extensions", "text": "\nRegister a custom accessor on Series objects.\n\nName under which the accessor should be registered. A warning is issued if\nthis name conflicts with a preexisting attribute.\n\nA class decorator.\n\nSee also\n\nRegister a custom accessor on DataFrame objects.\n\nRegister a custom accessor on Series objects.\n\nRegister a custom accessor on Index objects.\n\nNotes\n\nWhen accessed, your accessor will be initialized with the pandas object the\nuser is interacting with. So the signature must be\n\nFor consistency with pandas methods, you should raise an `AttributeError` if\nthe data passed to your accessor has an incorrect dtype.\n\nExamples\n\nIn your library code:\n\nBack in an interactive IPython session:\n\n"}, {"name": "pandas.api.indexers.BaseIndexer", "path": "reference/api/pandas.api.indexers.baseindexer", "type": "Window", "text": "\nBase class for window bounds calculations.\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.BaseIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.baseindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.indexers.check_array_indexer", "path": "reference/api/pandas.api.indexers.check_array_indexer", "type": "Extensions", "text": "\nCheck if indexer is a valid array indexer for array.\n\nFor a boolean mask, array and indexer are checked to have the same length. The\ndtype is validated, and if it is an integer or boolean ExtensionArray, it is\nchecked if there are missing values present, and it is converted to the\nappropriate numpy array. Other dtypes will raise an error.\n\nNon-array indexers (integer, slice, Ellipsis, tuples, ..) are passed through\nas is.\n\nNew in version 1.0.0.\n\nThe array that is being indexed (only used for the length).\n\nThe array-like that\u2019s used to index. List-like input that is not yet a numpy\narray or an ExtensionArray is converted to one. Other input types are passed\nthrough as is.\n\nThe validated indexer as a numpy array that can be used to index.\n\nWhen the lengths don\u2019t match.\n\nWhen indexer cannot be converted to a numpy ndarray to index (e.g. presence of\nmissing values).\n\nSee also\n\nCheck if key is of boolean dtype.\n\nExamples\n\nWhen checking a boolean mask, a boolean ndarray is returned when the arguments\nare all valid.\n\nAn IndexError is raised when the lengths don\u2019t match.\n\nNA values in a boolean array are treated as False.\n\nA numpy boolean mask will get passed through (if the length is correct):\n\nSimilarly for integer indexers, an integer ndarray is returned when it is a\nvalid indexer, otherwise an error is (for integer indexers, a matching length\nis not required):\n\nFor non-integer/boolean dtypes, an appropriate error is raised:\n\n"}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer", "type": "Window", "text": "\nCreates window boundaries for fixed-length windows that include the current\nrow.\n\nExamples\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.FixedForwardWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.fixedforwardwindowindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer", "type": "Window", "text": "\nCalculate window boundaries based on a non-fixed offset such as a BusinessDay.\n\nMethods\n\n`get_window_bounds`([num_values, min_periods, ...])\n\nComputes the bounds of a window.\n\n"}, {"name": "pandas.api.indexers.VariableOffsetWindowIndexer.get_window_bounds", "path": "reference/api/pandas.api.indexers.variableoffsetwindowindexer.get_window_bounds", "type": "Window", "text": "\nComputes the bounds of a window.\n\nnumber of values that will be aggregated over\n\nthe number of rows in a window\n\nmin_periods passed from the top level rolling API\n\ncenter passed from the top level rolling API\n\nclosed passed from the top level rolling API\n\nwin_type passed from the top level rolling API\n\n"}, {"name": "pandas.api.types.infer_dtype", "path": "reference/api/pandas.api.types.infer_dtype", "type": "General utility functions", "text": "\nEfficiently infer the type of a passed val, or list-like array of values.\nReturn a string describing the type.\n\nIgnore NaN values when inferring the type.\n\nDescribing the common type of the input data.\n\nIf ndarray-like but cannot infer the dtype\n\nNotes\n\n\u2018mixed\u2019 is the catchall for anything that is not otherwise specialized\n\n\u2018mixed-integer-float\u2019 are floats and integers\n\n\u2018mixed-integer\u2019 are integers mixed with non-integers\n\n\u2018unknown-array\u2019 is the catchall for something that is an array (has a dtype\nattribute), but has a dtype unknown to pandas (e.g. external extension array)\n\nExamples\n\n"}, {"name": "pandas.api.types.is_bool", "path": "reference/api/pandas.api.types.is_bool", "type": "General utility functions", "text": "\nReturn True if given object is boolean.\n\n"}, {"name": "pandas.api.types.is_bool_dtype", "path": "reference/api/pandas.api.types.is_bool_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a boolean dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a boolean dtype.\n\nNotes\n\nAn ExtensionArray is considered boolean when the `_is_boolean` attribute is\nset to True.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_categorical", "path": "reference/api/pandas.api.types.is_categorical", "type": "General utility functions", "text": "\nCheck whether an array-like is a Categorical instance.\n\nThe array-like to check.\n\nWhether or not the array-like is of a Categorical instance.\n\nExamples\n\nCategoricals, Series Categoricals, and CategoricalIndex will return True.\n\n"}, {"name": "pandas.api.types.is_categorical_dtype", "path": "reference/api/pandas.api.types.is_categorical_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Categorical dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Categorical dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_complex", "path": "reference/api/pandas.api.types.is_complex", "type": "General utility functions", "text": "\nReturn True if given object is complex.\n\n"}, {"name": "pandas.api.types.is_complex_dtype", "path": "reference/api/pandas.api.types.is_complex_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a complex dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a complex dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_any_dtype", "path": "reference/api/pandas.api.types.is_datetime64_any_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the datetime64 dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the datetime64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_dtype", "path": "reference/api/pandas.api.types.is_datetime64_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the datetime64 dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the datetime64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64_ns_dtype", "path": "reference/api/pandas.api.types.is_datetime64_ns_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the datetime64[ns] dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the datetime64[ns] dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_datetime64tz_dtype", "path": "reference/api/pandas.api.types.is_datetime64tz_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of a DatetimeTZDtype dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of a DatetimeTZDtype dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_dict_like", "path": "reference/api/pandas.api.types.is_dict_like", "type": "General utility functions", "text": "\nCheck if the object is dict-like.\n\nWhether obj has dict-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_extension_array_dtype", "path": "reference/api/pandas.api.types.is_extension_array_dtype", "type": "General utility functions", "text": "\nCheck if an object is a pandas extension array type.\n\nSee the Use Guide for more.\n\nFor array-like input, the `.dtype` attribute will be extracted.\n\nWhether the arr_or_dtype is an extension array type.\n\nNotes\n\nThis checks whether an object implements the pandas extension array interface.\nIn pandas, this includes:\n\nCategorical\n\nSparse\n\nInterval\n\nPeriod\n\nDatetimeArray\n\nTimedeltaArray\n\nThird-party libraries may implement arrays or types satisfying this interface\nas well.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_extension_type", "path": "reference/api/pandas.api.types.is_extension_type", "type": "General utility functions", "text": "\nCheck whether an array-like is of a pandas extension class instance.\n\nDeprecated since version 1.0.0: Use `is_extension_array_dtype` instead.\n\nExtension classes include categoricals, pandas sparse objects (i.e. classes\nrepresented within the pandas library and not ones external to it like scipy\nsparse matrices), and datetime-like arrays.\n\nThe array-like to check.\n\nWhether or not the array-like is of a pandas extension class instance.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_file_like", "path": "reference/api/pandas.api.types.is_file_like", "type": "General utility functions", "text": "\nCheck if the object is a file-like object.\n\nFor objects to be considered file-like, they must be an iterator AND have\neither a read and/or write method as an attribute.\n\nNote: file-like objects must be iterable, but iterable objects need not be\nfile-like.\n\nWhether obj has file-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_float", "path": "reference/api/pandas.api.types.is_float", "type": "General utility functions", "text": "\nReturn True if given object is float.\n\n"}, {"name": "pandas.api.types.is_float_dtype", "path": "reference/api/pandas.api.types.is_float_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a float dtype.\n\nThis function is internal and should not be exposed in the public API.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a float dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_hashable", "path": "reference/api/pandas.api.types.is_hashable", "type": "General utility functions", "text": "\nReturn True if hash(obj) will succeed, False otherwise.\n\nSome types will pass a test against collections.abc.Hashable but fail when\nthey are actually hashed with hash().\n\nDistinguish between these and other types by trying the call to hash() and\nseeing if they raise TypeError.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_int64_dtype", "path": "reference/api/pandas.api.types.is_int64_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the int64 dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the int64 dtype.\n\nNotes\n\nDepending on system architecture, the return value of is_int64_dtype( int)\nwill be True if the OS uses 64-bit integers and False if the OS uses 32-bit\nintegers.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_integer", "path": "reference/api/pandas.api.types.is_integer", "type": "General utility functions", "text": "\nReturn True if given object is integer.\n\n"}, {"name": "pandas.api.types.is_integer_dtype", "path": "reference/api/pandas.api.types.is_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of an integer dtype.\n\nUnlike in is_any_int_dtype, timedelta64 instances will return False.\n\nThe nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of an integer dtype and not an instance\nof timedelta64.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_interval", "path": "reference/api/pandas.api.types.is_interval", "type": "General utility functions", "text": "\n\n"}, {"name": "pandas.api.types.is_interval_dtype", "path": "reference/api/pandas.api.types.is_interval_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Interval dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Interval dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_iterator", "path": "reference/api/pandas.api.types.is_iterator", "type": "General utility functions", "text": "\nCheck if the object is an iterator.\n\nThis is intended for generators, not list-like objects.\n\nWhether obj is an iterator.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_list_like", "path": "reference/api/pandas.api.types.is_list_like", "type": "General utility functions", "text": "\nCheck if the object is list-like.\n\nObjects that are considered list-like are for example Python lists, tuples,\nsets, NumPy arrays, and Pandas Series.\n\nStrings and datetime objects, however, are not considered list-like.\n\nObject to check.\n\nIf this parameter is False, sets will not be considered list-like.\n\nWhether obj has list-like properties.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_named_tuple", "path": "reference/api/pandas.api.types.is_named_tuple", "type": "General utility functions", "text": "\nCheck if the object is a named tuple.\n\nWhether obj is a named tuple.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_number", "path": "reference/api/pandas.api.types.is_number", "type": "General utility functions", "text": "\nCheck if the object is a number.\n\nReturns True when the object is a number, and False if is not.\n\nThe object to check if is a number.\n\nWhether obj is a number or not.\n\nSee also\n\nChecks a subgroup of numbers.\n\nExamples\n\nBooleans are valid because they are int subclass.\n\n"}, {"name": "pandas.api.types.is_numeric_dtype", "path": "reference/api/pandas.api.types.is_numeric_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a numeric dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a numeric dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_object_dtype", "path": "reference/api/pandas.api.types.is_object_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the object dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the object dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_period_dtype", "path": "reference/api/pandas.api.types.is_period_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the Period dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the Period dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_re", "path": "reference/api/pandas.api.types.is_re", "type": "General utility functions", "text": "\nCheck if the object is a regex pattern instance.\n\nWhether obj is a regex pattern.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_re_compilable", "path": "reference/api/pandas.api.types.is_re_compilable", "type": "General utility functions", "text": "\nCheck if the object can be compiled into a regex pattern instance.\n\nWhether obj can be compiled as a regex pattern.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_scalar", "path": "reference/api/pandas.api.types.is_scalar", "type": "General utility functions", "text": "\nReturn True if given object is scalar.\n\nThis includes:\n\nnumpy array scalar (e.g. np.int64)\n\nPython builtin numerics\n\nPython builtin byte arrays and strings\n\nNone\n\ndatetime.datetime\n\ndatetime.timedelta\n\nPeriod\n\ndecimal.Decimal\n\nInterval\n\nDateOffset\n\nFraction\n\nNumber.\n\nReturn True if given object is scalar.\n\nExamples\n\npandas supports PEP 3141 numbers:\n\n"}, {"name": "pandas.api.types.is_signed_integer_dtype", "path": "reference/api/pandas.api.types.is_signed_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of a signed integer dtype.\n\nUnlike in is_any_int_dtype, timedelta64 instances will return False.\n\nThe nullable Integer dtypes (e.g. pandas.Int64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of a signed integer dtype and not an\ninstance of timedelta64.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_sparse", "path": "reference/api/pandas.api.types.is_sparse", "type": "General utility functions", "text": "\nCheck whether an array-like is a 1-D pandas sparse array.\n\nCheck that the one-dimensional array-like is a pandas sparse array. Returns\nTrue if it is a pandas sparse array, not another type of sparse array.\n\nArray-like to check.\n\nWhether or not the array-like is a pandas sparse array.\n\nExamples\n\nReturns True if the parameter is a 1-D pandas sparse array.\n\nReturns False if the parameter is not sparse.\n\nReturns False if the parameter is not a pandas sparse array.\n\nReturns False if the parameter has more than one dimension.\n\n"}, {"name": "pandas.api.types.is_string_dtype", "path": "reference/api/pandas.api.types.is_string_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the string dtype.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the string dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_timedelta64_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_dtype", "type": "General utility functions", "text": "\nCheck whether an array-like or dtype is of the timedelta64 dtype.\n\nThe array-like or dtype to check.\n\nWhether or not the array-like or dtype is of the timedelta64 dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_timedelta64_ns_dtype", "path": "reference/api/pandas.api.types.is_timedelta64_ns_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of the timedelta64[ns] dtype.\n\nThis is a very specific dtype, so generic ones like np.timedelta64 will return\nFalse if passed into this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of the timedelta64[ns] dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.is_unsigned_integer_dtype", "path": "reference/api/pandas.api.types.is_unsigned_integer_dtype", "type": "General utility functions", "text": "\nCheck whether the provided array or dtype is of an unsigned integer dtype.\n\nThe nullable Integer dtypes (e.g. pandas.UInt64Dtype) are also considered as\ninteger by this function.\n\nThe array or dtype to check.\n\nWhether or not the array or dtype is of an unsigned integer dtype.\n\nExamples\n\n"}, {"name": "pandas.api.types.pandas_dtype", "path": "reference/api/pandas.api.types.pandas_dtype", "type": "General utility functions", "text": "\nConvert input into a pandas only dtype object or a numpy dtype object.\n\n"}, {"name": "pandas.api.types.union_categoricals", "path": "reference/api/pandas.api.types.union_categoricals", "type": "General utility functions", "text": "\nCombine list-like of Categorical-like, unioning categories.\n\nAll categories must have the same dtype.\n\nCategorical, CategoricalIndex, or Series with dtype=\u2019category\u2019.\n\nIf true, resulting categories will be lexsorted, otherwise they will be\nordered as they appear in the data.\n\nIf true, the ordered attribute of the Categoricals will be ignored. Results in\nan unordered categorical.\n\nall inputs do not have the same dtype\n\nall inputs do not have the same ordered property\n\nall inputs are ordered and their categories are not identical\n\nsort_categories=True and Categoricals are ordered\n\nEmpty list of categoricals passed\n\nNotes\n\nTo learn more about categories, see link\n\nExamples\n\nIf you want to combine categoricals that do not necessarily have the same\ncategories, union_categoricals will combine a list-like of categoricals. The\nnew categories will be the union of the categories being combined.\n\nBy default, the resulting categories will be ordered as they appear in the\ncategories of the data. If you want the categories to be lexsorted, use\nsort_categories=True argument.\n\nunion_categoricals also works with the case of combining two categoricals of\nthe same categories and order information (e.g. what you could also append\nfor).\n\nRaises TypeError because the categories are ordered and not identical.\n\nNew in version 0.20.0\n\nOrdered categoricals with different categories or orderings can be combined by\nusing the ignore_ordered=True argument.\n\nunion_categoricals also works with a CategoricalIndex, or Series containing\ncategorical data, but note that the resulting array will always be a plain\nCategorical\n\n"}, {"name": "pandas.array", "path": "reference/api/pandas.array", "type": "Pandas arrays", "text": "\nCreate an array.\n\nThe scalars inside data should be instances of the scalar type for dtype. It\u2019s\nexpected that data represents a 1-dimensional array of data.\n\nWhen data is an Index or Series, the underlying array will be extracted from\ndata.\n\nThe dtype to use for the array. This may be a NumPy dtype or an extension type\nregistered with pandas using\n`pandas.api.extensions.register_extension_dtype()`.\n\nIf not specified, there are two possibilities:\n\nWhen data is a `Series`, `Index`, or `ExtensionArray`, the dtype will be taken\nfrom the data.\n\nOtherwise, pandas will attempt to infer the dtype from the data.\n\nNote that when data is a NumPy array, `data.dtype` is not used for inferring\nthe array type. This is because NumPy cannot represent all the types of data\nthat can be held in extension arrays.\n\nCurrently, pandas will infer an extension dtype for sequences of\n\nScalar Type\n\nArray Type\n\n`pandas.Interval`\n\n`pandas.arrays.IntervalArray`\n\n`pandas.Period`\n\n`pandas.arrays.PeriodArray`\n\n`datetime.datetime`\n\n`pandas.arrays.DatetimeArray`\n\n`datetime.timedelta`\n\n`pandas.arrays.TimedeltaArray`\n\n`int`\n\n`pandas.arrays.IntegerArray`\n\n`float`\n\n`pandas.arrays.FloatingArray`\n\n`str`\n\n`pandas.arrays.StringArray` or `pandas.arrays.ArrowStringArray`\n\n`bool`\n\n`pandas.arrays.BooleanArray`\n\nThe ExtensionArray created when the scalar type is `str` is determined by\n`pd.options.mode.string_storage` if the dtype is not explicitly given.\n\nFor all other cases, NumPy\u2019s usual inference rules will be used.\n\nChanged in version 1.0.0: Pandas infers nullable-integer dtype for integer\ndata, string dtype for string data, and nullable-boolean dtype for boolean\ndata.\n\nChanged in version 1.2.0: Pandas now also infers nullable-floating dtype for\nfloat-like input data\n\nWhether to copy the data, even if not necessary. Depending on the type of\ndata, creating the new array may require copying data, even if `copy=False`.\n\nThe newly created array.\n\nWhen data is not 1-dimensional.\n\nSee also\n\nConstruct a NumPy array.\n\nConstruct a pandas Series.\n\nConstruct a pandas Index.\n\nExtensionArray wrapping a NumPy array.\n\nExtract the array stored within a Series.\n\nNotes\n\nOmitting the dtype argument means pandas will attempt to infer the best array\ntype from the values in the data. As new array types are added by pandas and\n3rd party libraries, the \u201cbest\u201d array type may change. We recommend specifying\ndtype to ensure that\n\nthe correct array type for the data is returned\n\nthe returned array type doesn\u2019t change as new extension types are added by\npandas and third-party libraries\n\nAdditionally, if the underlying memory representation of the returned array\nmatters, we recommend specifying the dtype as a concrete object rather than a\nstring alias or allowing it to be inferred. For example, a future version of\npandas or a 3rd-party library may include a dedicated ExtensionArray for\nstring data. In this event, the following would no longer return a\n`arrays.PandasArray` backed by a NumPy array.\n\nThis would instead return the new ExtensionArray dedicated for string data. If\nyou really need the new array to be backed by a NumPy array, specify that in\nthe dtype.\n\nFinally, Pandas has arrays that mostly overlap with NumPy\n\n`arrays.DatetimeArray`\n\n`arrays.TimedeltaArray`\n\nWhen data with a `datetime64[ns]` or `timedelta64[ns]` dtype is passed, pandas\nwill always return a `DatetimeArray` or `TimedeltaArray` rather than a\n`PandasArray`. This is for symmetry with the case of timezone-aware data,\nwhich NumPy does not natively support.\n\nExamples\n\nIf a dtype is not specified, pandas will infer the best dtype from the values.\nSee the description of dtype for the types pandas infers for.\n\nYou can use the string alias for dtype\n\nOr specify the actual dtype\n\nIf pandas does not infer a dedicated extension type a `arrays.PandasArray` is\nreturned.\n\nAs mentioned in the \u201cNotes\u201d section, new extension types may be added in the\nfuture (by pandas or 3rd party libraries), causing the return value to no\nlonger be a `arrays.PandasArray`. Specify the dtype as a NumPy dtype if you\nneed to ensure there\u2019s no future change in behavior.\n\ndata must be 1-dimensional. A ValueError is raised when the input has the\nwrong dimensionality.\n\n"}, {"name": "pandas.arrays.ArrowStringArray", "path": "reference/api/pandas.arrays.arrowstringarray", "type": "Pandas arrays", "text": "\nExtension array for string data in a `pyarrow.ChunkedArray`.\n\nNew in version 1.2.0.\n\nWarning\n\nArrowStringArray is considered experimental. The implementation and parts of\nthe API may change without warning.\n\nThe array of data.\n\nSee also\n\nThe recommended function for creating a ArrowStringArray.\n\nThe string methods are available on Series backed by a ArrowStringArray.\n\nNotes\n\nArrowStringArray returns a BooleanArray for comparison methods.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.BooleanArray", "path": "reference/api/pandas.arrays.booleanarray", "type": "Pandas arrays", "text": "\nArray of boolean (True/False) data with missing values.\n\nThis is a pandas Extension array for boolean data, under the hood represented\nby 2 numpy arrays: a boolean array with the data and a boolean array with the\nmask (True indicating missing).\n\nBooleanArray implements Kleene logic (sometimes called three-value logic) for\nlogical operations. See Kleene logical operations for more.\n\nTo construct an BooleanArray from generic array-like input, use\n`pandas.array()` specifying `dtype=\"boolean\"` (see examples below).\n\nNew in version 1.0.0.\n\nWarning\n\nBooleanArray is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nA 1-d boolean-dtype array with the data.\n\nA 1-d boolean-dtype array indicating missing values (True indicates missing).\n\nWhether to copy the values and mask arrays.\n\nExamples\n\nCreate an BooleanArray with `pandas.array()`:\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.DatetimeArray", "path": "reference/api/pandas.arrays.datetimearray", "type": "Pandas arrays", "text": "\nPandas ExtensionArray for tz-naive or tz-aware datetime data.\n\nWarning\n\nDatetimeArray is currently experimental, and its API may change without\nwarning. In particular, `DatetimeArray.dtype` is expected to change to always\nbe an instance of an `ExtensionDtype` subclass.\n\nThe datetime data.\n\nFor DatetimeArray values (or a Series or Index boxing one), dtype and freq\nwill be extracted from values.\n\nNote that the only NumPy dtype allowed is \u2018datetime64[ns]\u2019.\n\nThe frequency.\n\nWhether to copy the underlying array of values.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.IntegerArray", "path": "reference/api/pandas.arrays.integerarray", "type": "Pandas arrays", "text": "\nArray of integer (optional missing) values.\n\nChanged in version 1.0.0: Now uses `pandas.NA` as the missing value rather\nthan `numpy.nan`.\n\nWarning\n\nIntegerArray is currently experimental, and its API or internal implementation\nmay change without warning.\n\nWe represent an IntegerArray with 2 numpy arrays:\n\ndata: contains a numpy integer array of the appropriate dtype\n\nmask: a boolean array holding a mask on the data, True is missing\n\nTo construct an IntegerArray from generic array-like input, use\n`pandas.array()` with one of the integer dtypes (see examples).\n\nSee Nullable integer data type for more.\n\nA 1-d integer-dtype array.\n\nA 1-d boolean-dtype array indicating missing values.\n\nWhether to copy the values and mask.\n\nExamples\n\nCreate an IntegerArray with `pandas.array()`.\n\nString aliases for the dtypes are also available. They are capitalized.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.IntervalArray", "path": "reference/api/pandas.arrays.intervalarray", "type": "Pandas arrays", "text": "\nPandas array for interval data that are closed on the same side.\n\nNew in version 0.24.0.\n\nArray-like containing Interval objects from which to build the IntervalArray.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nIf None, dtype will be inferred.\n\nCopy the input data.\n\nVerify that the IntervalArray is valid.\n\nSee also\n\nThe base pandas Index type.\n\nA bounded slice-like interval; the elements of an IntervalArray.\n\nFunction to create a fixed frequency IntervalIndex.\n\nBin values into discrete Intervals.\n\nBin values into equal-sized Intervals based on rank or sample quantiles.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nA new `IntervalArray` can be constructed directly from an array-like of\n`Interval` objects:\n\nIt may also be constructed using one of the constructor methods:\n`IntervalArray.from_arrays()`, `IntervalArray.from_breaks()`, and\n`IntervalArray.from_tuples()`.\n\nAttributes\n\n`left`\n\nReturn the left endpoints of each Interval in the IntervalArray as an Index.\n\n`right`\n\nReturn the right endpoints of each Interval in the IntervalArray as an Index.\n\n`closed`\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n`mid`\n\nReturn the midpoint of each Interval in the IntervalArray as an Index.\n\n`length`\n\nReturn an Index with entries denoting the length of each Interval in the\nIntervalArray.\n\n`is_empty`\n\nIndicates if an interval is empty, meaning it contains no points.\n\n`is_non_overlapping_monotonic`\n\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\nMethods\n\n`from_arrays`(left, right[, closed, copy, dtype])\n\nConstruct from two arrays defining the left and right bounds.\n\n`from_tuples`(data[, closed, copy, dtype])\n\nConstruct an IntervalArray from an array-like of tuples.\n\n`from_breaks`(breaks[, closed, copy, dtype])\n\nConstruct an IntervalArray from an array of splits.\n\n`contains`(other)\n\nCheck elementwise if the Intervals contain the value.\n\n`overlaps`(other)\n\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\n`set_closed`(closed)\n\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\n`to_tuples`([na_tuple])\n\nReturn an ndarray of tuples of the form (left, right).\n\n"}, {"name": "pandas.arrays.IntervalArray.closed", "path": "reference/api/pandas.arrays.intervalarray.closed", "type": "Pandas arrays", "text": "\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\n"}, {"name": "pandas.arrays.IntervalArray.contains", "path": "reference/api/pandas.arrays.intervalarray.contains", "type": "Pandas arrays", "text": "\nCheck elementwise if the Intervals contain the value.\n\nReturn a boolean mask whether the value is contained in the Intervals of the\nIntervalArray.\n\nNew in version 0.25.0.\n\nThe value to check whether it is contained in the Intervals.\n\nSee also\n\nCheck whether Interval object contains value.\n\nCheck if an Interval overlaps the values in the IntervalArray.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.from_arrays", "path": "reference/api/pandas.arrays.intervalarray.from_arrays", "type": "Pandas arrays", "text": "\nConstruct from two arrays defining the left and right bounds.\n\nLeft bounds for each interval.\n\nRight bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nWhen a value is missing in only one of left or right. When a value in left is\ngreater than the corresponding value in right.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalArray from an array of splits.\n\nConstruct an IntervalArray from an array-like of tuples.\n\nNotes\n\nEach element of left must be less than or equal to the right element at the\nsame position. If an element is missing, it must be missing in both left and\nright. A TypeError is raised when using an unsupported type for left or right.\nAt the moment, \u2018category\u2019, \u2018object\u2019, and \u2018string\u2019 subtypes are not supported.\n\n"}, {"name": "pandas.arrays.IntervalArray.from_breaks", "path": "reference/api/pandas.arrays.intervalarray.from_breaks", "type": "Pandas arrays", "text": "\nConstruct an IntervalArray from an array of splits.\n\nLeft and right bounds for each interval.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nCopy the data.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct from a left and right array.\n\nConstruct from a sequence of tuples.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.from_tuples", "path": "reference/api/pandas.arrays.intervalarray.from_tuples", "type": "Pandas arrays", "text": "\nConstruct an IntervalArray from an array-like of tuples.\n\nArray of tuples.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nBy-default copy the data, this is compat only and ignored.\n\nIf None, dtype will be inferred.\n\nSee also\n\nFunction to create a fixed frequency IntervalIndex.\n\nConstruct an IntervalArray from a left and right array.\n\nConstruct an IntervalArray from an array of splits.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.is_empty", "path": "reference/api/pandas.arrays.intervalarray.is_empty", "type": "Pandas arrays", "text": "\nIndicates if an interval is empty, meaning it contains no points.\n\nNew in version 0.25.0.\n\nA boolean indicating if a scalar `Interval` is empty, or a boolean `ndarray`\npositionally indicating if an `Interval` in an `IntervalArray` or\n`IntervalIndex` is empty.\n\nExamples\n\nAn `Interval` that contains points is not empty:\n\nAn `Interval` that does not contain any points is empty:\n\nAn `Interval` that contains a single point is not empty:\n\nAn `IntervalArray` or `IntervalIndex` returns a boolean `ndarray` positionally\nindicating if an `Interval` is empty:\n\nMissing values are not considered empty:\n\n"}, {"name": "pandas.arrays.IntervalArray.is_non_overlapping_monotonic", "path": "reference/api/pandas.arrays.intervalarray.is_non_overlapping_monotonic", "type": "Pandas arrays", "text": "\nReturn True if the IntervalArray is non-overlapping (no Intervals share\npoints) and is either monotonic increasing or monotonic decreasing, else\nFalse.\n\n"}, {"name": "pandas.arrays.IntervalArray.left", "path": "reference/api/pandas.arrays.intervalarray.left", "type": "Pandas arrays", "text": "\nReturn the left endpoints of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.length", "path": "reference/api/pandas.arrays.intervalarray.length", "type": "Pandas arrays", "text": "\nReturn an Index with entries denoting the length of each Interval in the\nIntervalArray.\n\n"}, {"name": "pandas.arrays.IntervalArray.mid", "path": "reference/api/pandas.arrays.intervalarray.mid", "type": "Pandas arrays", "text": "\nReturn the midpoint of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.overlaps", "path": "reference/api/pandas.arrays.intervalarray.overlaps", "type": "Pandas arrays", "text": "\nCheck elementwise if an Interval overlaps the values in the IntervalArray.\n\nTwo intervals overlap if they share a common point, including closed\nendpoints. Intervals that only have an open endpoint in common do not overlap.\n\nInterval to check against for an overlap.\n\nBoolean array positionally indicating where an overlap occurs.\n\nSee also\n\nCheck whether two Interval objects overlap.\n\nExamples\n\nIntervals that share closed endpoints overlap:\n\nIntervals that only have an open endpoint in common do not overlap:\n\n"}, {"name": "pandas.arrays.IntervalArray.right", "path": "reference/api/pandas.arrays.intervalarray.right", "type": "Pandas arrays", "text": "\nReturn the right endpoints of each Interval in the IntervalArray as an Index.\n\n"}, {"name": "pandas.arrays.IntervalArray.set_closed", "path": "reference/api/pandas.arrays.intervalarray.set_closed", "type": "Pandas arrays", "text": "\nReturn an IntervalArray identical to the current one, but closed on the\nspecified side.\n\nWhether the intervals are closed on the left-side, right-side, both or\nneither.\n\nExamples\n\n"}, {"name": "pandas.arrays.IntervalArray.to_tuples", "path": "reference/api/pandas.arrays.intervalarray.to_tuples", "type": "Pandas arrays", "text": "\nReturn an ndarray of tuples of the form (left, right).\n\nReturns NA as a tuple if True, `(nan, nan)`, or just as the NA value itself if\nFalse, `nan`.\n\n"}, {"name": "pandas.arrays.PandasArray", "path": "reference/api/pandas.arrays.pandasarray", "type": "Pandas arrays", "text": "\nA pandas ExtensionArray for NumPy data.\n\nThis is mostly for internal compatibility, and is not especially useful on its\nown.\n\nThe NumPy ndarray to wrap. Must be 1-dimensional.\n\nWhether to copy values.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.PeriodArray", "path": "reference/api/pandas.arrays.periodarray", "type": "Input/output", "text": "\nPandas ExtensionArray for storing Period data.\n\nUsers should use `period_array()` to create new instances. Alternatively,\n`array()` can be used to create new instances from a sequence of Period\nscalars.\n\nThe data to store. These should be arrays that can be directly converted to\nordinals without inference or copy (PeriodArray, ndarray[int64]), or a box\naround such an array (Series[period], PeriodIndex).\n\nA PeriodDtype instance from which to extract a freq. If both freq and dtype\nare specified, then the frequencies must match.\n\nThe freq to use for the array. Mostly applicable when values is an ndarray of\nintegers, when freq is required. When values is a PeriodArray (or box around),\nit\u2019s checked that `values.freq` matches freq.\n\nWhether to copy the ordinals before storing.\n\nSee also\n\nRepresents a period of time.\n\nImmutable Index for period data.\n\nCreate a fixed-frequency PeriodArray.\n\nConstruct a pandas array.\n\nNotes\n\nThere are two components to a PeriodArray\n\nordinals : integer ndarray\n\nfreq : pd.tseries.offsets.Offset\n\nThe values are physically stored as a 1-D ndarray of integers. These are\ncalled \u201cordinals\u201d and represent some kind of offset from a base.\n\nThe freq indicates the span covered by each element of the array. All elements\nin the PeriodArray have the same freq.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.SparseArray", "path": "reference/api/pandas.arrays.sparsearray", "type": "Pandas arrays", "text": "\nAn ExtensionArray for storing sparse data.\n\nA dense array of values to store in the SparseArray. This may contain\nfill_value.\n\nDeprecated since version 1.4.0: Use a function like np.full to construct an\narray with the desired repeats of the scalar value instead.\n\nElements in data that are `fill_value` are not stored in the SparseArray. For\nmemory savings, this should be the most common value in data. By default,\nfill_value depends on the dtype of data:\n\ndata.dtype\n\nna_value\n\nfloat\n\n`np.nan`\n\nint\n\n`0`\n\nbool\n\nFalse\n\ndatetime64\n\n`pd.NaT`\n\ntimedelta64\n\n`pd.NaT`\n\nThe fill value is potentially specified in three ways. In order of precedence,\nthese are\n\nThe fill_value argument\n\n`dtype.fill_value` if fill_value is None and dtype is a `SparseDtype`\n\n`data.dtype.fill_value` if fill_value is None and dtype is not a `SparseDtype`\nand data is a `SparseArray`.\n\nCan be \u2018integer\u2019 or \u2018block\u2019, default is \u2018integer\u2019. The type of storage for\nsparse locations.\n\n\u2018block\u2019: Stores a block and block_length for each contiguous span of sparse\nvalues. This is best when sparse data tends to be clumped together, with large\nregions of `fill-value` values between sparse values.\n\n\u2018integer\u2019: uses an integer to store the location of each sparse value.\n\nThe dtype to use for the SparseArray. For numpy dtypes, this determines the\ndtype of `self.sp_values`. For SparseDtype, this determines `self.sp_values`\nand `self.fill_value`.\n\nWhether to explicitly copy the incoming data array.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.StringArray", "path": "reference/api/pandas.arrays.stringarray", "type": "Pandas arrays", "text": "\nExtension array for string data.\n\nNew in version 1.0.0.\n\nWarning\n\nStringArray is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nThe array of data.\n\nWarning\n\nCurrently, this expects an object-dtype ndarray where the elements are Python\nstrings or `pandas.NA`. This may change without warning in the future. Use\n`pandas.array()` with `dtype=\"string\"` for a stable way of creating a\nStringArray from any sequence.\n\nWhether to copy the array of data.\n\nSee also\n\nThe recommended function for creating a StringArray.\n\nThe string methods are available on Series backed by a StringArray.\n\nNotes\n\nStringArray returns a BooleanArray for comparison methods.\n\nExamples\n\nUnlike arrays instantiated with `dtype=\"object\"`, `StringArray` will convert\nthe values to strings.\n\nHowever, instantiating StringArrays directly with non-strings will raise an\nerror.\n\nFor comparison methods, StringArray returns a `pandas.BooleanArray`:\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.arrays.TimedeltaArray", "path": "reference/api/pandas.arrays.timedeltaarray", "type": "Pandas arrays", "text": "\nPandas ExtensionArray for timedelta data.\n\nWarning\n\nTimedeltaArray is currently experimental, and its API may change without\nwarning. In particular, `TimedeltaArray.dtype` is expected to change to be an\ninstance of an `ExtensionDtype` subclass.\n\nThe timedelta data.\n\nCurrently, only `numpy.dtype(\"timedelta64[ns]\")` is accepted.\n\nWhether to copy the underlying array of data.\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.bdate_range", "path": "reference/api/pandas.bdate_range", "type": "General functions", "text": "\nReturn a fixed frequency DatetimeIndex, with business day as the default\nfrequency.\n\nLeft bound for generating dates.\n\nRight bound for generating dates.\n\nNumber of periods to generate.\n\nFrequency strings can have multiples, e.g. \u20185H\u2019.\n\nTime zone name for returning localized DatetimeIndex, for example\nAsia/Beijing.\n\nNormalize start/end dates to midnight before generating date range.\n\nName of the resulting DatetimeIndex.\n\nWeekmask of valid business days, passed to `numpy.busdaycalendar`, only used\nwhen custom frequency strings are passed. The default value None is equivalent\nto \u2018Mon Tue Wed Thu Fri\u2019.\n\nDates to exclude from the set of valid business days, passed to\n`numpy.busdaycalendar`, only used when custom frequency strings are passed.\n\nMake the interval closed with respect to the given frequency to the \u2018left\u2019,\n\u2018right\u2019, or both sides (None).\n\nDeprecated since version 1.4.0: Argument closed has been deprecated to\nstandardize boundary inputs. Use inclusive instead, to set each bound as\nclosed or open.\n\nInclude boundaries; Whether to set each bound as closed or open.\n\nNew in version 1.4.0.\n\nFor compatibility. Has no effect on the result.\n\nNotes\n\nOf the four parameters: `start`, `end`, `periods`, and `freq`, exactly three\nmust be specified. Specifying `freq` is a requirement for `bdate_range`. Use\n`date_range` if specifying `freq` is not desired.\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nNote how the two weekend days are skipped in the result.\n\n"}, {"name": "pandas.BooleanDtype", "path": "reference/api/pandas.booleandtype", "type": "Pandas arrays", "text": "\nExtension dtype for boolean data.\n\nNew in version 1.0.0.\n\nWarning\n\nBooleanDtype is considered experimental. The implementation and parts of the\nAPI may change without warning.\n\nExamples\n\nAttributes\n\nNone\n\nMethods\n\nNone\n\n"}, {"name": "pandas.Categorical", "path": "reference/api/pandas.categorical", "type": "Pandas arrays", "text": "\nRepresent a categorical variable in classic R / S-plus fashion.\n\nCategoricals can only take on only a limited, and usually fixed, number of\npossible values (categories). In contrast to statistical categorical\nvariables, a Categorical might have an order, but numerical operations\n(additions, divisions, \u2026) are not possible.\n\nAll values of the Categorical are either in categories or np.nan. Assigning\nvalues outside of categories will raise a ValueError. Order is defined by the\norder of the categories, not lexical order of the values.\n\nThe values of the categorical. If categories are given, values not in\ncategories will be replaced with NaN.\n\nThe unique categories for this categorical. If not given, the categories are\nassumed to be the unique values of values (sorted, if possible, otherwise in\nthe order in which they appear).\n\nWhether or not this categorical is treated as a ordered categorical. If True,\nthe resulting categorical will be ordered. An ordered categorical respects,\nwhen sorted, the order of its categories attribute (which in turn is the\ncategories argument, if provided).\n\nAn instance of `CategoricalDtype` to use for this categorical.\n\nIf the categories do not validate.\n\nIf an explicit `ordered=True` is given but no categories and the values are\nnot sortable.\n\nSee also\n\nType for categorical data.\n\nAn Index with an underlying `Categorical`.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\nMissing values are not included as a category.\n\nHowever, their presence is indicated in the codes attribute by code -1.\n\nOrdered Categoricals can be sorted according to the custom order of the\ncategories and can have a min and max value.\n\nAttributes\n\n`categories`\n\nThe categories of this categorical.\n\n`codes`\n\nThe category codes of this categorical.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\n`dtype`\n\nThe `CategoricalDtype` for this instance.\n\nMethods\n\n`from_codes`(codes[, categories, ordered, dtype])\n\nMake a Categorical type from codes and categories or dtype.\n\n`__array__`([dtype])\n\nThe numpy array interface.\n\n"}, {"name": "pandas.Categorical.__array__", "path": "reference/api/pandas.categorical.__array__", "type": "Pandas arrays", "text": "\nThe numpy array interface.\n\nA numpy array of either the specified dtype or, if dtype==None (default), the\nsame dtype as categorical.categories.dtype.\n\n"}, {"name": "pandas.Categorical.categories", "path": "reference/api/pandas.categorical.categories", "type": "Pandas arrays", "text": "\nThe categories of this categorical.\n\nSetting assigns new values to each category (effectively a rename of each\nindividual category).\n\nThe assigned value has to be a list-like object. All items must be unique and\nthe number of items in the new categories must be the same as the number of\nitems in the old categories.\n\nAssigning to categories is a inplace operation!\n\nIf the new categories do not validate as categories or if the number of new\ncategories is unequal the number of old categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.Categorical.codes", "path": "reference/api/pandas.categorical.codes", "type": "Pandas arrays", "text": "\nThe category codes of this categorical.\n\nCodes are an array of integers which are the positions of the actual values in\nthe categories array.\n\nThere is no setter, use the other categorical methods and the normal item\nsetter to change values in the categorical.\n\nA non-writable view of the codes array.\n\n"}, {"name": "pandas.Categorical.dtype", "path": "reference/api/pandas.categorical.dtype", "type": "Pandas arrays", "text": "\nThe `CategoricalDtype` for this instance.\n\n"}, {"name": "pandas.Categorical.from_codes", "path": "reference/api/pandas.categorical.from_codes", "type": "Pandas arrays", "text": "\nMake a Categorical type from codes and categories or dtype.\n\nThis constructor is useful if you already have codes and categories/dtype and\nso do not need the (computation intensive) factorization step, which is\nusually done on the constructor.\n\nIf your data does not follow this convention, please use the normal\nconstructor.\n\nAn integer array, where each integer points to a category in categories or\ndtype.categories, or else is -1 for NaN.\n\nThe categories for the categorical. Items need to be unique. If the categories\nare not given here, then they must be provided in dtype.\n\nWhether or not this categorical is treated as an ordered categorical. If not\ngiven here or in dtype, the resulting categorical will be unordered.\n\nIf `CategoricalDtype`, cannot be used together with categories or ordered.\n\nExamples\n\n"}, {"name": "pandas.Categorical.ordered", "path": "reference/api/pandas.categorical.ordered", "type": "Pandas arrays", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalDtype", "path": "reference/api/pandas.categoricaldtype", "type": "Pandas arrays", "text": "\nType for categorical data with the categories and orderedness.\n\nMust be unique, and must not contain any nulls. The categories are stored in\nan Index, and if an index is provided the dtype of that index will be used.\n\nWhether or not this categorical is treated as a ordered categorical. None can\nbe used to maintain the ordered value of existing categoricals when used in\noperations that combine categoricals, e.g. astype, and will resolve to False\nif there is no existing ordered to maintain.\n\nSee also\n\nRepresent a categorical variable in classic R / S-plus fashion.\n\nNotes\n\nThis class is useful for specifying the type of a `Categorical` independent of\nthe values. See CategoricalDtype for more.\n\nExamples\n\nAn empty CategoricalDtype with a specific dtype can be created by providing an\nempty index. As follows,\n\nAttributes\n\n`categories`\n\nAn `Index` containing the unique categories allowed.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\nMethods\n\nNone\n\n"}, {"name": "pandas.CategoricalDtype.categories", "path": "reference/api/pandas.categoricaldtype.categories", "type": "Pandas arrays", "text": "\nAn `Index` containing the unique categories allowed.\n\n"}, {"name": "pandas.CategoricalDtype.ordered", "path": "reference/api/pandas.categoricaldtype.ordered", "type": "Pandas arrays", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalIndex", "path": "reference/api/pandas.categoricalindex", "type": "Index Objects", "text": "\nIndex based on an underlying `Categorical`.\n\nCategoricalIndex, like Categorical, can only take on a limited, and usually\nfixed, number of possible values (categories). Also, like Categorical, it\nmight have an order, but numerical operations (additions, divisions, \u2026) are\nnot possible.\n\nThe values of the categorical. If categories are given, values not in\ncategories will be replaced with NaN.\n\nThe categories for the categorical. Items need to be unique. If the categories\nare not given here (and also not in dtype), they will be inferred from the\ndata.\n\nWhether or not this categorical is treated as an ordered categorical. If not\ngiven here or in dtype, the resulting categorical will be unordered.\n\nIf `CategoricalDtype`, cannot be used together with categories or ordered.\n\nMake a copy of input ndarray.\n\nName to be stored in the index.\n\nIf the categories do not validate.\n\nIf an explicit `ordered=True` is given but no categories and the values are\nnot sortable.\n\nSee also\n\nThe base pandas Index type.\n\nA categorical array.\n\nType for categorical data.\n\nNotes\n\nSee the user guide for more.\n\nExamples\n\n`CategoricalIndex` can also be instantiated from a `Categorical`:\n\nOrdered `CategoricalIndex` can have a min and max value.\n\nAttributes\n\n`codes`\n\nThe category codes of this categorical.\n\n`categories`\n\nThe categories of this categorical.\n\n`ordered`\n\nWhether the categories have an ordered relationship.\n\nMethods\n\n`rename_categories`(*args, **kwargs)\n\nRename categories.\n\n`reorder_categories`(*args, **kwargs)\n\nReorder categories as specified in new_categories.\n\n`add_categories`(*args, **kwargs)\n\nAdd new categories.\n\n`remove_categories`(*args, **kwargs)\n\nRemove the specified categories.\n\n`remove_unused_categories`(*args, **kwargs)\n\nRemove categories which are not used.\n\n`set_categories`(*args, **kwargs)\n\nSet the categories to the specified new_categories.\n\n`as_ordered`(*args, **kwargs)\n\nSet the Categorical to be ordered.\n\n`as_unordered`(*args, **kwargs)\n\nSet the Categorical to be unordered.\n\n`map`(mapper)\n\nMap values using input an input mapping or function.\n\n"}, {"name": "pandas.CategoricalIndex.add_categories", "path": "reference/api/pandas.categoricalindex.add_categories", "type": "Index Objects", "text": "\nAdd new categories.\n\nnew_categories will be included at the last/highest place in the categories\nand will be unused directly after this call.\n\nThe new categories to be included.\n\nWhether or not to add the categories inplace or return a copy of this\ncategorical with added categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with new categories added or None if `inplace=True`.\n\nIf the new categories include old categories or do not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.as_ordered", "path": "reference/api/pandas.categoricalindex.as_ordered", "type": "Index Objects", "text": "\nSet the Categorical to be ordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to True.\n\nOrdered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.CategoricalIndex.as_unordered", "path": "reference/api/pandas.categoricalindex.as_unordered", "type": "Index Objects", "text": "\nSet the Categorical to be unordered.\n\nWhether or not to set the ordered attribute in-place or return a copy of this\ncategorical with ordered set to False.\n\nUnordered Categorical or None if `inplace=True`.\n\n"}, {"name": "pandas.CategoricalIndex.categories", "path": "reference/api/pandas.categoricalindex.categories", "type": "Index Objects", "text": "\nThe categories of this categorical.\n\nSetting assigns new values to each category (effectively a rename of each\nindividual category).\n\nThe assigned value has to be a list-like object. All items must be unique and\nthe number of items in the new categories must be the same as the number of\nitems in the old categories.\n\nAssigning to categories is a inplace operation!\n\nIf the new categories do not validate as categories or if the number of new\ncategories is unequal the number of old categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.CategoricalIndex.codes", "path": "reference/api/pandas.categoricalindex.codes", "type": "Index Objects", "text": "\nThe category codes of this categorical.\n\nCodes are an array of integers which are the positions of the actual values in\nthe categories array.\n\nThere is no setter, use the other categorical methods and the normal item\nsetter to change values in the categorical.\n\nA non-writable view of the codes array.\n\n"}, {"name": "pandas.CategoricalIndex.equals", "path": "reference/api/pandas.categoricalindex.equals", "type": "Index Objects", "text": "\nDetermine if two CategoricalIndex objects contain the same elements.\n\nIf two CategoricalIndex objects have equal elements True, otherwise False.\n\n"}, {"name": "pandas.CategoricalIndex.map", "path": "reference/api/pandas.categoricalindex.map", "type": "Index Objects", "text": "\nMap values using input an input mapping or function.\n\nMaps the values (their categories, not the codes) of the index to new\ncategories. If the mapping correspondence is one-to-one the result is a\n`CategoricalIndex` which has the same order property as the original,\notherwise an `Index` is returned.\n\nIf a dict or `Series` is used any unmapped category is mapped to NaN. Note\nthat if this happens an `Index` will be returned.\n\nMapping correspondence.\n\nMapped index.\n\nSee also\n\nApply a mapping correspondence on an `Index`.\n\nApply a mapping correspondence on a `Series`.\n\nApply more complex functions on a `Series`.\n\nExamples\n\nIf the mapping is one-to-one the ordering of the categories is preserved:\n\nIf the mapping is not one-to-one an `Index` is returned:\n\nIf a dict is used, all unmapped categories are mapped to NaN and the result is\nan `Index`:\n\n"}, {"name": "pandas.CategoricalIndex.ordered", "path": "reference/api/pandas.categoricalindex.ordered", "type": "Index Objects", "text": "\nWhether the categories have an ordered relationship.\n\n"}, {"name": "pandas.CategoricalIndex.remove_categories", "path": "reference/api/pandas.categoricalindex.remove_categories", "type": "Index Objects", "text": "\nRemove the specified categories.\n\nremovals must be included in the old categories. Values which were in the\nremoved categories will be set to NaN\n\nThe categories which should be removed.\n\nWhether or not to remove the categories inplace or return a copy of this\ncategorical with removed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the removals are not contained in the categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.remove_unused_categories", "path": "reference/api/pandas.categoricalindex.remove_unused_categories", "type": "Index Objects", "text": "\nRemove categories which are not used.\n\nWhether or not to drop unused categories inplace or return a copy of this\ncategorical with unused categories dropped.\n\nDeprecated since version 1.2.0.\n\nCategorical with unused categories dropped or None if `inplace=True`.\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nSet the categories to the specified ones.\n\nExamples\n\n"}, {"name": "pandas.CategoricalIndex.rename_categories", "path": "reference/api/pandas.categoricalindex.rename_categories", "type": "Index Objects", "text": "\nRename categories.\n\nNew categories which will replace old categories.\n\nlist-like: all items must be unique and the number of items in the new\ncategories must match the existing number of categories.\n\ndict-like: specifies a mapping from old categories to new. Categories not\ncontained in the mapping are passed through and extra categories in the\nmapping are ignored.\n\ncallable : a callable that is called on all items in the old categories and\nwhose return values comprise the new categories.\n\nWhether or not to rename the categories inplace or return a copy of this\ncategorical with renamed categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf new categories are list-like and do not have the same number of items than\nthe current categories or do not validate as categories\n\nSee also\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\nExamples\n\nFor dict-like `new_categories`, extra keys are ignored and categories not in\nthe dictionary are passed through\n\nYou may also provide a callable to create the new categories\n\n"}, {"name": "pandas.CategoricalIndex.reorder_categories", "path": "reference/api/pandas.categoricalindex.reorder_categories", "type": "Index Objects", "text": "\nReorder categories as specified in new_categories.\n\nnew_categories need to include all old categories and no new category items.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not to reorder the categories inplace or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nCategorical with removed categories or None if `inplace=True`.\n\nIf the new categories do not contain all old category items or any new ones\n\nSee also\n\nRename categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\nSet the categories to the specified ones.\n\n"}, {"name": "pandas.CategoricalIndex.set_categories", "path": "reference/api/pandas.categoricalindex.set_categories", "type": "Index Objects", "text": "\nSet the categories to the specified new_categories.\n\nnew_categories can include new categories (which will result in unused\ncategories) or remove old categories (which results in values set to NaN). If\nrename==True, the categories will simple be renamed (less or more items than\nin old categories will result in values set to NaN or in unused categories\nrespectively).\n\nThis method can be used to perform more than one action of adding, removing,\nand reordering simultaneously and is therefore faster than performing the\nindividual steps via the more specialised methods.\n\nOn the other hand this methods does not do checks (e.g., whether the old\ncategories are included in the new categories on a reorder), which can result\nin surprising changes, for example when using special string dtypes, which\ndoes not considers a S1 string equal to a single char python string.\n\nThe categories in new order.\n\nWhether or not the categorical is treated as a ordered categorical. If not\ngiven, do not change the ordered information.\n\nWhether or not the new_categories should be considered as a rename of the old\ncategories or as reordered categories.\n\nWhether or not to reorder the categories in-place or return a copy of this\ncategorical with reordered categories.\n\nDeprecated since version 1.3.0.\n\nIf new_categories does not validate as categories\n\nSee also\n\nRename categories.\n\nReorder categories.\n\nAdd new categories.\n\nRemove the specified categories.\n\nRemove categories which are not used.\n\n"}, {"name": "pandas.concat", "path": "reference/api/pandas.concat", "type": "General functions", "text": "\nConcatenate pandas objects along a particular axis with optional set logic\nalong the other axes.\n\nCan also add a layer of hierarchical indexing on the concatenation axis, which\nmay be useful if the labels are the same (or overlapping) on the passed axis\nnumber.\n\nIf a mapping is passed, the sorted keys will be used as the keys argument,\nunless it is passed, in which case the values will be selected (see below).\nAny None objects will be dropped silently unless they are all None in which\ncase a ValueError will be raised.\n\nThe axis to concatenate along.\n\nHow to handle indexes on other axis (or axes).\n\nIf True, do not use the index values along the concatenation axis. The\nresulting axis will be labeled 0, \u2026, n - 1. This is useful if you are\nconcatenating objects where the concatenation axis does not have meaningful\nindexing information. Note the index values on the other axes are still\nrespected in the join.\n\nIf multiple levels passed, should contain tuples. Construct hierarchical index\nusing the passed keys as the outermost level.\n\nSpecific levels (unique values) to use for constructing a MultiIndex.\nOtherwise they will be inferred from the keys.\n\nNames for the levels in the resulting hierarchical index.\n\nCheck whether the new concatenated axis contains duplicates. This can be very\nexpensive relative to the actual data concatenation.\n\nSort non-concatenation axis if it is not already aligned when join is \u2018outer\u2019.\nThis has no effect when `join='inner'`, which already preserves the order of\nthe non-concatenation axis.\n\nChanged in version 1.0.0: Changed to not sort by default.\n\nIf False, do not copy data unnecessarily.\n\nWhen concatenating all `Series` along the index (axis=0), a `Series` is\nreturned. When `objs` contains at least one `DataFrame`, a `DataFrame` is\nreturned. When concatenating along the columns (axis=1), a `DataFrame` is\nreturned.\n\nSee also\n\nConcatenate Series.\n\nConcatenate DataFrames.\n\nJoin DataFrames using indexes.\n\nMerge DataFrames by indexes or columns.\n\nNotes\n\nThe keys, levels, and names arguments are all optional.\n\nA walkthrough of how this method fits in with other tools for combining pandas\nobjects can be found here.\n\nExamples\n\nCombine two `Series`.\n\nClear the existing index and reset it in the result by setting the\n`ignore_index` option to `True`.\n\nAdd a hierarchical index at the outermost level of the data with the `keys`\noption.\n\nLabel the index keys you create with the `names` option.\n\nCombine two `DataFrame` objects with identical columns.\n\nCombine `DataFrame` objects with overlapping columns and return everything.\nColumns outside the intersection will be filled with `NaN` values.\n\nCombine `DataFrame` objects with overlapping columns and return only those\nthat are shared by passing `inner` to the `join` keyword argument.\n\nCombine `DataFrame` objects horizontally along the x axis by passing in\n`axis=1`.\n\nPrevent the result from including duplicate index values with the\n`verify_integrity` option.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.dataframegroupby.aggregate", "type": "GroupBy", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nCan also accept a Numba JIT function with `engine='numba'` specified. Only\npassing a single function is supported with this engine.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function func group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nTransforms the Series on each group based on the given function.\n\nNotes\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nThe aggregation is for each column.\n\nMultiple aggregations\n\nSelect a column for aggregation\n\nDifferent aggregations per column\n\nTo control the output names with different aggregations per column, pandas\nsupports \u201cnamed aggregation\u201d\n\nThe keywords are the output column names\n\nThe values are tuples whose first element is the column to select and the\nsecond element is the aggregation to apply to that column. Pandas provides the\n`pandas.NamedAgg` namedtuple with the fields `['column', 'aggfunc']` to make\nit clearer what the arguments are. As usual, the aggregation can be a callable\nor a string alias.\n\nSee Named aggregation for more.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe aggregating function.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.all", "path": "reference/api/pandas.core.groupby.dataframegroupby.all", "type": "GroupBy", "text": "\nReturn True if all values in the group are truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if all elements\nare True within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.any", "path": "reference/api/pandas.core.groupby.dataframegroupby.any", "type": "GroupBy", "text": "\nReturn True if any value in the group is truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if any element is\nTrue within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.backfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.backfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.bfill", "path": "reference/api/pandas.core.groupby.dataframegroupby.bfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.boxplot", "path": "reference/api/pandas.core.groupby.dataframegroupby.boxplot", "type": "GroupBy", "text": "\nMake box plots from DataFrameGroupBy data.\n\n`False` \\- no subplots will be used\n\n`True` \\- create a subplot for each group.\n\nCan be any valid input to groupby.\n\nThe layout of the plot: (rows, columns).\n\nWhether x-axes will be shared among subplots.\n\nWhether y-axes will be shared among subplots.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nAll other plotting keyword arguments to be passed to matplotlib\u2019s boxplot\nfunction.\n\nExamples\n\nYou can create boxplots for grouped data and show them as separate subplots:\n\nThe `subplots=False` option shows the boxplots in a single figure.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.corr", "path": "reference/api/pandas.core.groupby.dataframegroupby.corr", "type": "GroupBy", "text": "\nCompute pairwise correlation of columns, excluding NA/null values.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float. Note that the returned matrix from corr will have 1\nalong the diagonals and will be symmetric regardless of the callable\u2019s\nbehavior.\n\nMinimum number of observations required per pair of columns to have a valid\nresult. Currently only available for Pearson and Spearman correlation.\n\nCorrelation matrix.\n\nSee also\n\nCompute pairwise correlation with another DataFrame or Series.\n\nCompute the correlation between two Series.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.corrwith", "path": "reference/api/pandas.core.groupby.dataframegroupby.corrwith", "type": "GroupBy", "text": "\nCompute pairwise correlation.\n\nPairwise correlation is computed between rows or columns of DataFrame with\nrows or columns of Series or DataFrame. DataFrames are first aligned along\nboth axes before computing the correlations.\n\nObject with which to compute correlations.\n\nThe axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-\nwise.\n\nDrop missing indices from result.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float.\n\nPairwise correlations.\n\nSee also\n\nCompute pairwise correlation of columns.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.count", "path": "reference/api/pandas.core.groupby.dataframegroupby.count", "type": "GroupBy", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cov", "path": "reference/api/pandas.core.groupby.dataframegroupby.cov", "type": "GroupBy", "text": "\nCompute pairwise covariance of columns, excluding NA/null values.\n\nCompute the pairwise covariance among the series of a DataFrame. The returned\ndata frame is the covariance matrix of the columns of the DataFrame.\n\nBoth NA and null values are automatically excluded from the calculation. (See\nthe note below about bias from missing values.) A threshold can be set for the\nminimum number of observations for each value created. Comparisons with\nobservations below this threshold will be returned as `NaN`.\n\nThis method is generally used for the analysis of time series data to\nunderstand the relationship between different measures across time.\n\nMinimum number of observations required per pair of columns to have a valid\nresult.\n\nDelta degrees of freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nNew in version 1.1.0.\n\nThe covariance matrix of the series of the DataFrame.\n\nSee also\n\nCompute covariance with another Series.\n\nExponential weighted sample covariance.\n\nExpanding sample covariance.\n\nRolling sample covariance.\n\nNotes\n\nReturns the covariance matrix of the DataFrame\u2019s time series. The covariance\nis normalized by N-ddof.\n\nFor DataFrames that have Series that are missing data (assuming that data is\nmissing at random) the returned covariance matrix will be an unbiased estimate\nof the variance and covariance between the member Series.\n\nHowever, for many applications this estimate may not be acceptable because the\nestimate covariance matrix is not guaranteed to be positive semi-definite.\nThis could lead to estimate correlations having absolute values which are\ngreater than one, and/or a non-invertible covariance matrix. See Estimation of\ncovariance matrices for more details.\n\nExamples\n\nMinimum number of periods\n\nThis method also supports an optional `min_periods` keyword that specifies the\nrequired minimum number of non-NA observations for each column pair in order\nto have a valid result:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumcount", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumcount", "type": "GroupBy", "text": "\nNumber each item in each group from 0 to the length of that group - 1.\n\nEssentially this is equivalent to\n\nIf False, number in reverse, from length of group - 1 to 0.\n\nSequence number of each element within each group.\n\nSee also\n\nNumber the groups themselves.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummax", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummax", "type": "GroupBy", "text": "\nCumulative max for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cummin", "path": "reference/api/pandas.core.groupby.dataframegroupby.cummin", "type": "GroupBy", "text": "\nCumulative min for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumprod", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumprod", "type": "GroupBy", "text": "\nCumulative product for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.cumsum", "path": "reference/api/pandas.core.groupby.dataframegroupby.cumsum", "type": "GroupBy", "text": "\nCumulative sum for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.describe", "path": "reference/api/pandas.core.groupby.dataframegroupby.describe", "type": "GroupBy", "text": "\nGenerate descriptive statistics.\n\nDescriptive statistics include those that summarize the central tendency,\ndispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.\n\nAnalyzes both numeric and object series, as well as `DataFrame` column sets of\nmixed data types. The output will vary depending on what is provided. Refer to\nthe notes below for more detail.\n\nThe percentiles to include in the output. All should fall between 0 and 1. The\ndefault is `[.25, .5, .75]`, which returns the 25th, 50th, and 75th\npercentiles.\n\nA white list of data types to include in the result. Ignored for `Series`.\nHere are the options:\n\n\u2018all\u2019 : All columns of the input will be included in the output.\n\nA list-like of dtypes : Limits the results to the provided data types. To\nlimit the result to numeric types submit `numpy.number`. To limit it instead\nto object columns submit the `numpy.object` data type. Strings can also be\nused in the style of `select_dtypes` (e.g. `df.describe(include=['O'])`). To\nselect pandas categorical columns, use `'category'`\n\nNone (default) : The result will include all numeric columns.\n\nA black list of data types to omit from the result. Ignored for `Series`. Here\nare the options:\n\nA list-like of dtypes : Excludes the provided data types from the result. To\nexclude numeric types submit `numpy.number`. To exclude object columns submit\nthe data type `numpy.object`. Strings can also be used in the style of\n`select_dtypes` (e.g. `df.describe(exclude=['O'])`). To exclude pandas\ncategorical columns, use `'category'`\n\nNone (default) : The result will exclude nothing.\n\nWhether to treat datetime dtypes as numeric. This affects statistics\ncalculated for the column. For DataFrame input, this also controls whether\ndatetime columns are included by default.\n\nNew in version 1.1.0.\n\nSummary statistics of the Series or Dataframe provided.\n\nSee also\n\nCount number of non-NA/null observations.\n\nMaximum of the values in the object.\n\nMinimum of the values in the object.\n\nMean of the values.\n\nStandard deviation of the observations.\n\nSubset of a DataFrame including/excluding columns based on their dtype.\n\nNotes\n\nFor numeric data, the result\u2019s index will include `count`, `mean`, `std`,\n`min`, `max` as well as lower, `50` and upper percentiles. By default the\nlower percentile is `25` and the upper percentile is `75`. The `50` percentile\nis the same as the median.\n\nFor object data (e.g. strings or timestamps), the result\u2019s index will include\n`count`, `unique`, `top`, and `freq`. The `top` is the most common value. The\n`freq` is the most common value\u2019s frequency. Timestamps also include the\n`first` and `last` items.\n\nIf multiple object values have the highest count, then the `count` and `top`\nresults will be arbitrarily chosen from among those with the highest count.\n\nFor mixed data types provided via a `DataFrame`, the default is to return only\nan analysis of numeric columns. If the dataframe consists only of object and\ncategorical data without any numeric columns, the default is to return an\nanalysis of both the object and categorical columns. If `include='all'` is\nprovided as an option, the result will include a union of attributes of each\ntype.\n\nThe include and exclude parameters can be used to limit which columns in a\n`DataFrame` are analyzed for the output. The parameters are ignored when\nanalyzing a `Series`.\n\nExamples\n\nDescribing a numeric `Series`.\n\nDescribing a categorical `Series`.\n\nDescribing a timestamp `Series`.\n\nDescribing a `DataFrame`. By default only numeric fields are returned.\n\nDescribing all columns of a `DataFrame` regardless of data type.\n\nDescribing a column from a `DataFrame` by accessing it as an attribute.\n\nIncluding only numeric columns in a `DataFrame` description.\n\nIncluding only string columns in a `DataFrame` description.\n\nIncluding only categorical columns from a `DataFrame` description.\n\nExcluding numeric columns from a `DataFrame` description.\n\nExcluding object columns from a `DataFrame` description.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.diff", "path": "reference/api/pandas.core.groupby.dataframegroupby.diff", "type": "GroupBy", "text": "\nFirst discrete difference of element.\n\nCalculates the difference of a Dataframe element compared with another element\nin the Dataframe (default is element in previous row).\n\nPeriods to shift for calculating difference, accepts negative values.\n\nTake difference over rows (0) or columns (1).\n\nFirst differences of the Series.\n\nSee also\n\nPercent change over given number of periods.\n\nShift index by desired number of periods with an optional time freq.\n\nFirst discrete difference of object.\n\nNotes\n\nFor boolean dtypes, this uses `operator.xor()` rather than `operator.sub()`.\nThe result is calculated according to current dtype in Dataframe, however\ndtype of the result is always float64.\n\nExamples\n\nDifference with previous row\n\nDifference with previous column\n\nDifference with 3rd previous row\n\nDifference with following row\n\nOverflow in input dtype\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.ffill", "path": "reference/api/pandas.core.groupby.dataframegroupby.ffill", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.fillna", "path": "reference/api/pandas.core.groupby.dataframegroupby.fillna", "type": "GroupBy", "text": "\nFill NA/NaN values using the specified method.\n\nValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of\nvalues specifying which value to use for each index (for a Series) or column\n(for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\nThis value cannot be a list.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use next valid\nobservation to fill gap.\n\nAxis along which to fill missing values.\n\nIf True, fill in-place. Note: this will modify any other views on this object\n(e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nObject with missing values filled or None if `inplace=True`.\n\nSee also\n\nFill NaN values using interpolation.\n\nConform object to new index.\n\nConvert TimeSeries to specified frequency.\n\nExamples\n\nReplace all NaN elements with 0s.\n\nWe can also propagate non-null values forward or backward.\n\nReplace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3\nrespectively.\n\nOnly replace the first NaN element.\n\nWhen filling using a DataFrame, replacement happens along the same column\nnames and same indices\n\nNote that column D is not affected since it is not present in df2.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.filter", "path": "reference/api/pandas.core.groupby.dataframegroupby.filter", "type": "GroupBy", "text": "\nReturn a copy of a DataFrame excluding filtered elements.\n\nElements from groups are filtered if they do not satisfy the boolean criterion\nspecified by func.\n\nFunction to apply to each subframe. Should return True or False.\n\nIf False, groups that evaluate False are filled with NaNs.\n\nNotes\n\nEach subframe is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.hist", "path": "reference/api/pandas.core.groupby.dataframegroupby.hist", "type": "GroupBy", "text": "\nMake a histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ncalls `matplotlib.pyplot.hist()`, on each series in the DataFrame, resulting\nin one histogram per column.\n\nThe pandas object holding the data.\n\nIf passed, will be used to limit data to a subset of columns.\n\nIf passed, then used to form histograms for separate groups.\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels. For example, a value of 90 displays the x labels\nrotated 90 degrees clockwise.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels. For example, a value of 90 displays the y labels\nrotated 90 degrees clockwise.\n\nThe axes to plot the histogram on.\n\nIn case subplots=True, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in. Note\nthat passing in both an ax and sharex=True will alter all x axis labels for\nall subplots in a figure.\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.\n\nThe size in inches of the figure to create. Uses the value in\nmatplotlib.rcParams by default.\n\nTuple of (rows, columns) for the layout of the histograms.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.hist()`.\n\nSee also\n\nPlot a histogram using matplotlib.\n\nExamples\n\nThis example draws a histogram based on the length and width of some animals,\ndisplayed in three bins\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmax", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmax", "type": "GroupBy", "text": "\nReturn index of first occurrence of maximum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of maxima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the maximum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmax`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the maximum value in each column.\n\nTo return the index for the maximum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.idxmin", "path": "reference/api/pandas.core.groupby.dataframegroupby.idxmin", "type": "GroupBy", "text": "\nReturn index of first occurrence of minimum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of minima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the minimum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmin`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the minimum value in each column.\n\nTo return the index for the minimum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.mad", "path": "reference/api/pandas.core.groupby.dataframegroupby.mad", "type": "GroupBy", "text": "\nReturn the mean absolute deviation of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.nunique", "path": "reference/api/pandas.core.groupby.dataframegroupby.nunique", "type": "GroupBy", "text": "\nReturn DataFrame with counts of unique elements in each position.\n\nDon\u2019t include NaN in the counts.\n\nExamples\n\nCheck for rows with the same id but conflicting values:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.pad", "path": "reference/api/pandas.core.groupby.dataframegroupby.pad", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.pct_change", "path": "reference/api/pandas.core.groupby.dataframegroupby.pct_change", "type": "GroupBy", "text": "\nCalculate pct_change of each value to previous entry in group.\n\nPercentage changes within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.plot", "path": "reference/api/pandas.core.groupby.dataframegroupby.plot", "type": "GroupBy", "text": "\nClass implementing the .plot attribute for groupby objects.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.quantile", "path": "reference/api/pandas.core.groupby.dataframegroupby.quantile", "type": "GroupBy", "text": "\nReturn group values at the given quantile, a la numpy.percentile.\n\nValue(s) between 0 and 1 providing the quantile(s) to compute.\n\nMethod to use when the desired quantile falls between two points.\n\nReturn type determined by caller of GroupBy object.\n\nSee also\n\nSimilar method for Series.\n\nSimilar method for DataFrame.\n\nNumPy method to compute qth percentile.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.rank", "path": "reference/api/pandas.core.groupby.dataframegroupby.rank", "type": "GroupBy", "text": "\nProvide the rank of values within each group.\n\naverage: average rank of group.\n\nmin: lowest rank in group.\n\nmax: highest rank in group.\n\nfirst: ranks assigned in order they appear in the array.\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFalse for ranks by high (1) to low (N).\n\nkeep: leave NA values where they are.\n\ntop: smallest rank if ascending.\n\nbottom: smallest rank if descending.\n\nCompute percentage rank of data within each group.\n\nThe axis of the object over which to compute the rank.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.resample", "path": "reference/api/pandas.core.groupby.dataframegroupby.resample", "type": "GroupBy", "text": "\nProvide resampling when using a TimeGrouper.\n\nGiven a grouper, the function resamples it according to a string \u201cstring\u201d ->\n\u201cfrequency\u201d.\n\nSee the frequency aliases documentation for more details.\n\nThe offset string or object representing target grouper conversion.\n\nPossible arguments are how, fill_method, limit, kind and on, and other\narguments of TimeGrouper.\n\nReturn a new grouper with our resampler appended.\n\nSee also\n\nSpecify a frequency to resample with when grouping by a key.\n\nFrequency conversion and resampling of time series.\n\nExamples\n\nDownsample the DataFrame into 3 minute bins and sum the values of the\ntimestamps falling into a bin.\n\nUpsample the series into 30 second bins.\n\nResample by month. Values are assigned to the month of the period.\n\nDownsample the series into 3 minute bins as above, but close the right side of\nthe bin interval.\n\nDownsample the series into 3 minute bins and close the right side of the bin\ninterval, but label each bin using the right edge instead of the left.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.sample", "path": "reference/api/pandas.core.groupby.dataframegroupby.sample", "type": "GroupBy", "text": "\nReturn a random sample of items from each group.\n\nYou can use random_state for reproducibility.\n\nNew in version 1.1.0.\n\nNumber of items to return for each group. Cannot be used with frac and must be\nno larger than the smallest group unless replace is True. Default is one if\nfrac is None.\n\nFraction of items to return. Cannot be used with n.\n\nAllow or disallow sampling of the same row more than once.\n\nDefault None results in equal probability weighting. If passed a list-like\nthen values must have the same length as the underlying DataFrame or Series\nobject and will be used as sampling probabilities after normalization within\neach group. Values must be non-negative with at least one positive element\nwithin each group.\n\nIf int, array-like, or BitGenerator, seed for random number generator. If\nnp.random.RandomState or np.random.Generator, use as given.\n\nChanged in version 1.4.0: np.random.Generator objects now accepted\n\nA new object of same type as caller containing items randomly sampled within\neach group from the caller object.\n\nSee also\n\nGenerate random samples from a DataFrame object.\n\nGenerate a random sample from a given 1-D numpy array.\n\nExamples\n\nSelect one row at random for each distinct value in column a. The random_state\nargument can be used to guarantee reproducibility:\n\nSet frac to sample fixed proportions rather than counts:\n\nControl sample probabilities within groups by setting weights:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.shift", "path": "reference/api/pandas.core.groupby.dataframegroupby.shift", "type": "GroupBy", "text": "\nShift each group by periods observations.\n\nIf freq is passed, the index will be increased using the periods and the freq.\n\nNumber of periods to shift.\n\nFrequency string.\n\nShift direction.\n\nThe scalar value to use for newly introduced missing values.\n\nObject shifted within each group.\n\nSee also\n\nShift values of Index.\n\nShift the time index, using the index\u2019s frequency if available.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.size", "path": "reference/api/pandas.core.groupby.dataframegroupby.size", "type": "GroupBy", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.skew", "path": "reference/api/pandas.core.groupby.dataframegroupby.skew", "type": "GroupBy", "text": "\nReturn unbiased skew over requested axis.\n\nNormalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.take", "path": "reference/api/pandas.core.groupby.dataframegroupby.take", "type": "GroupBy", "text": "\nReturn the elements in the given positional indices along an axis.\n\nThis means that we are not indexing according to actual values in the index\nattribute of the object. We are indexing according to the actual position of\nthe element in the object.\n\nAn array of ints indicating which positions to take.\n\nThe axis on which to select elements. `0` means that we are selecting rows,\n`1` means that we are selecting columns.\n\nBefore pandas 1.0, `is_copy=False` can be specified to ensure that the return\nvalue is an actual copy. Starting with pandas 1.0, `take` always returns a\ncopy, and the keyword is therefore deprecated.\n\nDeprecated since version 1.0.0.\n\nFor compatibility with `numpy.take()`. Has no effect on the output.\n\nAn array-like containing the elements taken from the object.\n\nSee also\n\nSelect a subset of a DataFrame by labels.\n\nSelect a subset of a DataFrame by positions.\n\nTake elements from an array along an axis.\n\nExamples\n\nTake elements at positions 0 and 3 along the axis 0 (default).\n\nNote how the actual indices selected (0 and 1) do not correspond to our\nselected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd\nrows, not rows whose indices equal 0 and 3.\n\nTake elements at indices 1 and 2 along the axis 1 (column selection).\n\nWe may take elements using negative integers for positive indices, starting\nfrom the end of the object, just like with Python lists.\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.transform", "path": "reference/api/pandas.core.groupby.dataframegroupby.transform", "type": "GroupBy", "text": "\nCall function producing a like-indexed DataFrame on each group and return a\nDataFrame having the same indexes as the original object filled with the\ntransformed values.\n\nFunction to apply to each group.\n\nCan also accept a Numba JIT function with `engine='numba'` specified.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or the global setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function `func` group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\nNotes\n\nEach group is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nThe current implementation imposes three requirements on f:\n\nf must return a value that either has the same shape as the input subframe or\ncan be broadcast to the shape of the input subframe. For example, if f returns\na scalar it will be broadcast to have the same shape as the input subframe.\n\nif this is a DataFrame, f must support application column-by-column in the\nsubframe. If f also supports application to the entire subframe, then a fast\npath is used starting from the second chunk.\n\nf must not mutate groups. Mutation is not supported and may produce unexpected\nresults. See Mutating with User Defined Function (UDF) methods for more\ndetails.\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nBroadcast result of the transformation\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, for example:\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.tshift", "path": "reference/api/pandas.core.groupby.dataframegroupby.tshift", "type": "GroupBy", "text": "\nShift the time index, using the index\u2019s frequency if available.\n\nDeprecated since version 1.1.0: Use shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nIncrement to use from the tseries module or time rule expressed as a string\n(e.g. \u2018EOM\u2019).\n\nCorresponds to the axis that contains the Index.\n\nNotes\n\nIf freq is not specified then tries to use the freq or inferred_freq\nattributes of the index. If neither of those attributes exist, a ValueError is\nthrown\n\n"}, {"name": "pandas.core.groupby.DataFrameGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.dataframegroupby.value_counts", "type": "GroupBy", "text": "\nReturn a Series or DataFrame containing counts of unique rows.\n\nNew in version 1.4.0.\n\nColumns to use when counting unique combinations.\n\nReturn proportions rather than frequencies.\n\nSort by frequencies.\n\nSort in ascending order.\n\nDon\u2019t include counts of rows that contain NA values.\n\nSeries if the groupby as_index is True, otherwise DataFrame.\n\nSee also\n\nEquivalent method on Series.\n\nEquivalent method on DataFrame.\n\nEquivalent method on SeriesGroupBy.\n\nNotes\n\nIf the groupby as_index is True then the returned Series will have a\nMultiIndex with one level per input column.\n\nIf the groupby as_index is False then the returned DataFrame will have an\nadditional column with the value_counts. The column is labelled \u2018count\u2019 or\n\u2018proportion\u2019, depending on the `normalize` parameter.\n\nBy default, rows that contain any NA values are omitted from the result.\n\nBy default, the result will be in descending order so that the first element\nof each group is the most frequently-occurring row.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.__iter__", "path": "reference/api/pandas.core.groupby.groupby.__iter__", "type": "GroupBy", "text": "\nGroupby iterator.\n\n"}, {"name": "pandas.core.groupby.GroupBy.agg", "path": "reference/api/pandas.core.groupby.groupby.agg", "type": "GroupBy", "text": "\n\n"}, {"name": "pandas.core.groupby.GroupBy.all", "path": "reference/api/pandas.core.groupby.groupby.all", "type": "GroupBy", "text": "\nReturn True if all values in the group are truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if all elements\nare True within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.any", "path": "reference/api/pandas.core.groupby.groupby.any", "type": "GroupBy", "text": "\nReturn True if any value in the group is truthful, else False.\n\nFlag to ignore nan values during truth testing.\n\nDataFrame or Series of boolean values, where a value is True if any element is\nTrue within its respective group, False otherwise.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.apply", "path": "reference/api/pandas.core.groupby.groupby.apply", "type": "GroupBy", "text": "\nApply function `func` group-wise and combine the results together.\n\nThe function passed to `apply` must take a dataframe as its first argument and\nreturn a DataFrame, Series or scalar. `apply` will then take care of combining\nthe results back together into a single dataframe or series. `apply` is\ntherefore a highly flexible grouping method.\n\nWhile `apply` is a very flexible method, its downside is that using it can be\nquite a bit slower than using more specific methods like `agg` or `transform`.\nPandas offers a wide range of method that will be much faster than using\n`apply` for their specific purposes, so try to use them before reaching for\n`apply`.\n\nA callable that takes a dataframe as its first argument, and returns a\ndataframe, a series or a scalar. In addition the callable may take positional\nand keyword arguments.\n\nOptional positional and keyword arguments to pass to `func`.\n\nSee also\n\nApply function to the full GroupBy object instead of to each group.\n\nApply aggregate function to the GroupBy object.\n\nApply function column-by-column to the GroupBy object.\n\nApply a function to a Series.\n\nApply a function to each row or column of a DataFrame.\n\nNotes\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nNotice that `g` has two groups, `a` and `b`. Calling apply in various ways, we\ncan get different grouping results:\n\nExample 1: below the function passed to apply takes a DataFrame as its\nargument and returns a DataFrame. apply combines the result for each group\ntogether into a new DataFrame:\n\nExample 2: The function passed to apply takes a DataFrame as its argument and\nreturns a Series. apply combines the result for each group together into a new\nDataFrame.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`.\n\nExample 3: The function passed to apply takes a DataFrame as its argument and\nreturns a scalar. apply combines the result for each group together into a\nSeries, including setting the index as appropriate:\n\n"}, {"name": "pandas.core.groupby.GroupBy.backfill", "path": "reference/api/pandas.core.groupby.groupby.backfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.bfill", "path": "reference/api/pandas.core.groupby.groupby.bfill", "type": "GroupBy", "text": "\nBackward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nBackward fill the missing values in the dataset.\n\nBackward fill the missing values in the dataset.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.count", "path": "reference/api/pandas.core.groupby.groupby.count", "type": "GroupBy", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumcount", "path": "reference/api/pandas.core.groupby.groupby.cumcount", "type": "GroupBy", "text": "\nNumber each item in each group from 0 to the length of that group - 1.\n\nEssentially this is equivalent to\n\nIf False, number in reverse, from length of group - 1 to 0.\n\nSequence number of each element within each group.\n\nSee also\n\nNumber the groups themselves.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.cummax", "path": "reference/api/pandas.core.groupby.groupby.cummax", "type": "GroupBy", "text": "\nCumulative max for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cummin", "path": "reference/api/pandas.core.groupby.groupby.cummin", "type": "GroupBy", "text": "\nCumulative min for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumprod", "path": "reference/api/pandas.core.groupby.groupby.cumprod", "type": "GroupBy", "text": "\nCumulative product for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.cumsum", "path": "reference/api/pandas.core.groupby.groupby.cumsum", "type": "GroupBy", "text": "\nCumulative sum for each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ffill", "path": "reference/api/pandas.core.groupby.groupby.ffill", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.first", "path": "reference/api/pandas.core.groupby.groupby.first", "type": "GroupBy", "text": "\nCompute first of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed first of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.get_group", "path": "reference/api/pandas.core.groupby.groupby.get_group", "type": "GroupBy", "text": "\nConstruct DataFrame from group with provided name.\n\nThe name of the group to get as a DataFrame.\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby\nwas called on will be used.\n\n"}, {"name": "pandas.core.groupby.GroupBy.groups", "path": "reference/api/pandas.core.groupby.groupby.groups", "type": "GroupBy", "text": "\nDict {group name -> group labels}.\n\n"}, {"name": "pandas.core.groupby.GroupBy.head", "path": "reference/api/pandas.core.groupby.groupby.head", "type": "GroupBy", "text": "\nReturn first n rows of each group.\n\nSimilar to `.apply(lambda x: x.head(n))`, but it returns a subset of rows from\nthe original DataFrame with original index and order preserved (`as_index`\nflag is ignored).\n\nIf positive: number of entries to include from start of each group. If\nnegative: number of entries to exclude from end of each group.\n\nSubset of original Series or DataFrame as determined by n.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.indices", "path": "reference/api/pandas.core.groupby.groupby.indices", "type": "GroupBy", "text": "\nDict {group name -> group indices}.\n\n"}, {"name": "pandas.core.groupby.GroupBy.last", "path": "reference/api/pandas.core.groupby.groupby.last", "type": "GroupBy", "text": "\nCompute last of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed last of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.max", "path": "reference/api/pandas.core.groupby.groupby.max", "type": "GroupBy", "text": "\nCompute max of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed max of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.mean", "path": "reference/api/pandas.core.groupby.groupby.mean", "type": "GroupBy", "text": "\nCompute mean of groups, excluding missing values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nGroupby one column and return the mean of the remaining columns in each group.\n\nGroupby two columns and return the mean of the remaining column.\n\nGroupby one column and return the mean of only particular column in the group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.median", "path": "reference/api/pandas.core.groupby.groupby.median", "type": "GroupBy", "text": "\nCompute median of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nMedian of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.min", "path": "reference/api/pandas.core.groupby.groupby.min", "type": "GroupBy", "text": "\nCompute min of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed min of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ngroup", "path": "reference/api/pandas.core.groupby.groupby.ngroup", "type": "GroupBy", "text": "\nNumber each group from 0 to the number of groups - 1.\n\nThis is the enumerative complement of cumcount. Note that the numbers given to\nthe groups match the order in which the groups would be seen when iterating\nover the groupby object, not the order they are first observed.\n\nIf False, number in reverse, from number of group - 1 to 0.\n\nUnique numbers for each group.\n\nSee also\n\nNumber the rows in each group.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.nth", "path": "reference/api/pandas.core.groupby.groupby.nth", "type": "GroupBy", "text": "\nTake the nth row from each group if n is an int, otherwise a subset of rows.\n\nCan be either a call or an index. dropna is not available with index notation.\nIndex notation accepts a comma separated list of integers and slices.\n\nIf dropna, will take the nth non-null row, dropna is either \u2018all\u2019 or \u2018any\u2019;\nthis is equivalent to calling dropna(how=dropna) before the groupby.\n\nA single nth value for the row or a list of nth values or slices.\n\nChanged in version 1.4.0: Added slice and lists containiing slices. Added\nindex notation.\n\nApply the specified dropna operation before counting which row is the nth row.\nOnly supported if n is an int.\n\nN-th value within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nIndex notation may also be used\n\nSpecifying dropna allows count ignoring `NaN`\n\nNaNs denote group exhausted when using dropna\n\nSpecifying as_index=False in groupby keeps the original index.\n\n"}, {"name": "pandas.core.groupby.GroupBy.ohlc", "path": "reference/api/pandas.core.groupby.groupby.ohlc", "type": "GroupBy", "text": "\nCompute open, high, low and close values of a group, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nOpen, high, low and close values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pad", "path": "reference/api/pandas.core.groupby.groupby.pad", "type": "GroupBy", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nObject with missing values filled.\n\nSee also\n\nReturns Series with minimum number of char in object.\n\nObject with missing values filled or None if inplace=True.\n\nFill NaN values of a Series.\n\nFill NaN values of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pct_change", "path": "reference/api/pandas.core.groupby.groupby.pct_change", "type": "GroupBy", "text": "\nCalculate pct_change of each value to previous entry in group.\n\nPercentage changes within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.pipe", "path": "reference/api/pandas.core.groupby.groupby.pipe", "type": "GroupBy", "text": "\nApply a function func with arguments to this GroupBy object and return the\nfunction\u2019s result.\n\nUse .pipe when you want to improve readability by chaining together functions\nthat expect Series, DataFrames, GroupBy or Resampler objects. Instead of\nwriting\n\nYou can write\n\nwhich is much more readable.\n\nFunction to apply to this GroupBy object or, alternatively, a (callable,\ndata_keyword) tuple where data_keyword is a string indicating the keyword of\ncallable that expects the GroupBy object.\n\nPositional arguments passed into func.\n\nA dictionary of keyword arguments passed into func.\n\nSee also\n\nApply a function with arguments to a series.\n\nApply a function with arguments to a dataframe.\n\nApply function to each group instead of to the full GroupBy object.\n\nNotes\n\nSee more here\n\nExamples\n\nTo get the difference between each groups maximum and minimum value in one\npass, you can do\n\n"}, {"name": "pandas.core.groupby.GroupBy.prod", "path": "reference/api/pandas.core.groupby.groupby.prod", "type": "GroupBy", "text": "\nCompute prod of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed prod of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.rank", "path": "reference/api/pandas.core.groupby.groupby.rank", "type": "GroupBy", "text": "\nProvide the rank of values within each group.\n\naverage: average rank of group.\n\nmin: lowest rank in group.\n\nmax: highest rank in group.\n\nfirst: ranks assigned in order they appear in the array.\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFalse for ranks by high (1) to low (N).\n\nkeep: leave NA values where they are.\n\ntop: smallest rank if ascending.\n\nbottom: smallest rank if descending.\n\nCompute percentage rank of data within each group.\n\nThe axis of the object over which to compute the rank.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.sem", "path": "reference/api/pandas.core.groupby.groupby.sem", "type": "GroupBy", "text": "\nCompute standard error of the mean of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\nStandard error of the mean of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.size", "path": "reference/api/pandas.core.groupby.groupby.size", "type": "GroupBy", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.std", "path": "reference/api/pandas.core.groupby.groupby.std", "type": "GroupBy", "text": "\nCompute standard deviation of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nStandard deviation of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.GroupBy.sum", "path": "reference/api/pandas.core.groupby.groupby.sum", "type": "GroupBy", "text": "\nCompute sum of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed sum of values within each group.\n\n"}, {"name": "pandas.core.groupby.GroupBy.tail", "path": "reference/api/pandas.core.groupby.groupby.tail", "type": "GroupBy", "text": "\nReturn last n rows of each group.\n\nSimilar to `.apply(lambda x: x.tail(n))`, but it returns a subset of rows from\nthe original DataFrame with original index and order preserved (`as_index`\nflag is ignored).\n\nIf positive: number of entries to include from end of each group. If negative:\nnumber of entries to exclude from start of each group.\n\nSubset of original Series or DataFrame as determined by n.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.groupby.GroupBy.var", "path": "reference/api/pandas.core.groupby.groupby.var", "type": "GroupBy", "text": "\nCompute variance of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nVariance of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.aggregate", "path": "reference/api/pandas.core.groupby.seriesgroupby.aggregate", "type": "Series", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series or when passed to Series.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nCan also accept a Numba JIT function with `engine='numba'` specified. Only\npassing a single function is supported with this engine.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function func group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nTransforms the Series on each group based on the given function.\n\nNotes\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nThe output column names can be controlled by passing the desired column names\nand aggregations as keyword arguments.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe aggregating function.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.hist", "path": "reference/api/pandas.core.groupby.seriesgroupby.hist", "type": "Series", "text": "\nDraw histogram of the input series using matplotlib.\n\nIf passed, then used to form histograms for separate groups.\n\nIf not passed, uses gca().\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels.\n\nFigure size in inches by default.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nTo be passed to the actual plotting function.\n\nA histogram plot.\n\nSee also\n\nPlot a histogram using matplotlib.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_decreasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_decreasing", "type": "Series", "text": "\nReturn boolean if values in the object are monotonic_decreasing.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.is_monotonic_increasing", "path": "reference/api/pandas.core.groupby.seriesgroupby.is_monotonic_increasing", "type": "Series", "text": "\nAlias for is_monotonic.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nlargest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nlargest", "type": "Series", "text": "\nReturn the largest n elements.\n\nReturn this many descending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n largest values in the Series, sorted in decreasing order.\n\nSee also\n\nGet the n smallest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values(ascending=False).head(n)` for small n relative to\nthe size of the `Series` object.\n\nExamples\n\nThe n largest elements where `n=5` by default.\n\nThe n largest elements where `n=3`. Default keep value is \u2018first\u2019 so Malta\nwill be kept.\n\nThe n largest elements where `n=3` and keeping the last duplicates. Brunei\nwill be kept since it is the last with value 434000 based on the index order.\n\nThe n largest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has five elements due to the three duplicates.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nsmallest", "path": "reference/api/pandas.core.groupby.seriesgroupby.nsmallest", "type": "Series", "text": "\nReturn the smallest n elements.\n\nReturn this many ascending sorted values.\n\nWhen there are duplicate values that cannot all fit in a Series of n elements:\n\n`first` : return the first n occurrences in order of appearance.\n\n`last` : return the last n occurrences in reverse order of appearance.\n\n`all` : keep all occurrences. This can result in a Series of size larger than\nn.\n\nThe n smallest values in the Series, sorted in increasing order.\n\nSee also\n\nGet the n largest elements.\n\nSort Series by values.\n\nReturn the first n rows.\n\nNotes\n\nFaster than `.sort_values().head(n)` for small n relative to the size of the\n`Series` object.\n\nExamples\n\nThe n smallest elements where `n=5` by default.\n\nThe n smallest elements where `n=3`. Default keep value is \u2018first\u2019 so Nauru\nand Tuvalu will be kept.\n\nThe n smallest elements where `n=3` and keeping the last duplicates. Anguilla\nand Tuvalu will be kept since they are the last with value 11300 based on the\nindex order.\n\nThe n smallest elements where `n=3` with all duplicates kept. Note that the\nreturned Series has four elements due to the three duplicates.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.nunique", "path": "reference/api/pandas.core.groupby.seriesgroupby.nunique", "type": "Series", "text": "\nReturn number of unique elements in the group.\n\nNumber of unique values within each group.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.transform", "path": "reference/api/pandas.core.groupby.seriesgroupby.transform", "type": "Series", "text": "\nCall function producing a like-indexed Series on each group and return a\nSeries having the same indexes as the original object filled with the\ntransformed values.\n\nFunction to apply to each group.\n\nCan also accept a Numba JIT function with `engine='numba'` specified.\n\nIf the `'numba'` engine is chosen, the function must be a user defined\nfunction with `values` and `index` as the first and second arguments\nrespectively in the function signature. Each group\u2019s index will be passed to\nthe user defined function and optionally available for use.\n\nChanged in version 1.1.0.\n\nPositional arguments to pass to func.\n\n`'cython'` : Runs the function through C-extensions from cython.\n\n`'numba'` : Runs the function through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or the global setting `compute.use_numba`\n\nNew in version 1.1.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to the function\n\nNew in version 1.1.0.\n\nKeyword arguments to be passed into func.\n\nSee also\n\nApply function `func` group-wise and combine the results together.\n\nAggregate using one or more operations over the specified axis.\n\nCall `func` on self producing a Series with the same axis shape as self.\n\nNotes\n\nEach group is endowed the attribute \u2018name\u2019 in case you need to know which\ngroup you are working on.\n\nThe current implementation imposes three requirements on f:\n\nf must return a value that either has the same shape as the input subframe or\ncan be broadcast to the shape of the input subframe. For example, if f returns\na scalar it will be broadcast to have the same shape as the input subframe.\n\nif this is a DataFrame, f must support application column-by-column in the\nsubframe. If f also supports application to the entire subframe, then a fast\npath is used starting from the second chunk.\n\nf must not mutate groups. Mutation is not supported and may produce unexpected\nresults. See Mutating with User Defined Function (UDF) methods for more\ndetails.\n\nWhen using `engine='numba'`, there will be no \u201cfall back\u201d behavior internally.\nThe group data and group index will be passed as numpy arrays to the JITed\nuser defined function, and no alternative execution attempts will be tried.\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, see the examples below.\n\nExamples\n\nBroadcast result of the transformation\n\nChanged in version 1.3.0: The resulting dtype will reflect the return value of\nthe passed `func`, for example:\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.unique", "path": "reference/api/pandas.core.groupby.seriesgroupby.unique", "type": "Series", "text": "\nReturn unique values of Series object.\n\nUniques are returned in order of appearance. Hash table-based unique,\ntherefore does NOT sort.\n\nThe unique values returned as a NumPy array. See Notes.\n\nSee also\n\nTop-level unique method for any 1-d array-like object.\n\nReturn Index with unique values from an Index object.\n\nNotes\n\nReturns the unique values as a NumPy array. In case of an extension-array\nbacked Series, a new `ExtensionArray` of that type with just the unique values\nis returned. This includes\n\nCategorical\n\nPeriod\n\nDatetime with Timezone\n\nInterval\n\nSparse\n\nIntegerNA\n\nSee Examples section.\n\nExamples\n\nAn Categorical will return categories in the order of appearance and with the\nsame dtype.\n\n"}, {"name": "pandas.core.groupby.SeriesGroupBy.value_counts", "path": "reference/api/pandas.core.groupby.seriesgroupby.value_counts", "type": "Series", "text": "\n\n"}, {"name": "pandas.core.resample.Resampler.__iter__", "path": "reference/api/pandas.core.resample.resampler.__iter__", "type": "Resampling", "text": "\nGroupby iterator.\n\n"}, {"name": "pandas.core.resample.Resampler.aggregate", "path": "reference/api/pandas.core.resample.resampler.aggregate", "type": "Resampling", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nAggregate using callable, string, dict, or list of string/callables.\n\nTransforms the Series on each group based on the given function.\n\nAggregate using one or more operations over the specified axis.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.apply", "path": "reference/api/pandas.core.resample.resampler.apply", "type": "Resampling", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nAggregate using callable, string, dict, or list of string/callables.\n\nTransforms the Series on each group based on the given function.\n\nAggregate using one or more operations over the specified axis.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.asfreq", "path": "reference/api/pandas.core.resample.resampler.asfreq", "type": "Resampling", "text": "\nReturn the values at the new freq, essentially a reindex.\n\nValue to use for missing values, applied during upsampling (note this does not\nfill NaNs that already were present).\n\nValues at the specified freq.\n\nSee also\n\nConvert TimeSeries to specified frequency.\n\nConvert TimeSeries to specified frequency.\n\n"}, {"name": "pandas.core.resample.Resampler.backfill", "path": "reference/api/pandas.core.resample.resampler.backfill", "type": "Resampling", "text": "\nBackward fill the new missing values in the resampled data.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency). The\nbackward fill will replace NaN values that appeared in the resampled data with\nthe next value in the original sequence. Missing values that existed in the\noriginal data will not be modified.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with backward filled NaN values.\n\nSee also\n\nAlias of backfill.\n\nFill NaN values using the specified method, which can be \u2018backfill\u2019.\n\nFill NaN values with nearest neighbor starting from center.\n\nForward fill NaN values.\n\nFill NaN values in the Series using the specified method, which can be\n\u2018backfill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018backfill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nResampling a DataFrame that has missing values:\n\n"}, {"name": "pandas.core.resample.Resampler.bfill", "path": "reference/api/pandas.core.resample.resampler.bfill", "type": "Resampling", "text": "\nBackward fill the new missing values in the resampled data.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency). The\nbackward fill will replace NaN values that appeared in the resampled data with\nthe next value in the original sequence. Missing values that existed in the\noriginal data will not be modified.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with backward filled NaN values.\n\nSee also\n\nAlias of backfill.\n\nFill NaN values using the specified method, which can be \u2018backfill\u2019.\n\nFill NaN values with nearest neighbor starting from center.\n\nForward fill NaN values.\n\nFill NaN values in the Series using the specified method, which can be\n\u2018backfill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018backfill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nResampling a DataFrame that has missing values:\n\n"}, {"name": "pandas.core.resample.Resampler.count", "path": "reference/api/pandas.core.resample.resampler.count", "type": "Resampling", "text": "\nCompute count of group, excluding missing values.\n\nCount of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.ffill", "path": "reference/api/pandas.core.resample.resampler.ffill", "type": "Resampling", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nSee also\n\nFill NA/NaN values using the specified method.\n\nFill NA/NaN values using the specified method.\n\n"}, {"name": "pandas.core.resample.Resampler.fillna", "path": "reference/api/pandas.core.resample.resampler.fillna", "type": "Resampling", "text": "\nFill missing values introduced by upsampling.\n\nIn statistics, imputation is the process of replacing missing data with\nsubstituted values [1]. When resampling data, missing values may appear (e.g.,\nwhen the resampling frequency is higher than the original frequency).\n\nMissing values that existed in the original data will not be modified.\n\nMethod to use for filling holes in resampled data\n\n\u2018pad\u2019 or \u2018ffill\u2019: use previous valid observation to fill gap (forward fill).\n\n\u2018backfill\u2019 or \u2018bfill\u2019: use next valid observation to fill gap.\n\n\u2018nearest\u2019: use nearest valid observation to fill gap.\n\nLimit of how many consecutive missing values to fill.\n\nAn upsampled Series or DataFrame with missing values filled.\n\nSee also\n\nBackward fill NaN values in the resampled data.\n\nForward fill NaN values in the resampled data.\n\nFill NaN values in the resampled data with nearest neighbor starting from\ncenter.\n\nFill NaN values using interpolation.\n\nFill NaN values in the Series using the specified method, which can be \u2018bfill\u2019\nand \u2018ffill\u2019.\n\nFill NaN values in the DataFrame using the specified method, which can be\n\u2018bfill\u2019 and \u2018ffill\u2019.\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Imputation_(statistics)\n\nExamples\n\nResampling a Series:\n\nWithout filling the missing values you get:\n\nMissing values present before the upsampling are not affected.\n\nDataFrame resampling is done column-wise. All the same options are available.\n\n"}, {"name": "pandas.core.resample.Resampler.first", "path": "reference/api/pandas.core.resample.resampler.first", "type": "Resampling", "text": "\nCompute first of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed first of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.get_group", "path": "reference/api/pandas.core.resample.resampler.get_group", "type": "Resampling", "text": "\nConstruct DataFrame from group with provided name.\n\nThe name of the group to get as a DataFrame.\n\nThe DataFrame to take the DataFrame out of. If it is None, the object groupby\nwas called on will be used.\n\n"}, {"name": "pandas.core.resample.Resampler.groups", "path": "reference/api/pandas.core.resample.resampler.groups", "type": "Resampling", "text": "\nDict {group name -> group labels}.\n\n"}, {"name": "pandas.core.resample.Resampler.indices", "path": "reference/api/pandas.core.resample.resampler.indices", "type": "Resampling", "text": "\nDict {group name -> group indices}.\n\n"}, {"name": "pandas.core.resample.Resampler.interpolate", "path": "reference/api/pandas.core.resample.resampler.interpolate", "type": "Resampling", "text": "\nInterpolate values according to different methods.\n\nFill NaN values using an interpolation method.\n\nPlease note that only `method='linear'` is supported for DataFrame/Series with\na MultiIndex.\n\nInterpolation technique to use. One of:\n\n\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the\nonly method supported on MultiIndexes.\n\n\u2018time\u2019: Works on daily and higher resolution data to interpolate given length\nof interval.\n\n\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n\u2018pad\u2019: Fill in NaNs using existing values.\n\n\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019,\n\u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the\nnumerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you\nalso specify an order (int), e.g. `df.interpolate(method='polynomial',\norder=5)`.\n\n\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019:\nWrappers around the SciPy interpolation methods of similar names. See Notes.\n\n\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which\nreplaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n\nAxis to interpolate along.\n\nMaximum number of consecutive NaNs to fill. Must be greater than 0.\n\nUpdate the data in place if possible.\n\nConsecutive NaNs will be filled in this direction.\n\nIf \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019\n\nelse the default is \u2018forward\u2019\n\nChanged in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or\n\u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if\nlimit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.\n\nIf limit is specified, consecutive NaNs will be filled with this restriction.\n\n`None`: No fill restriction.\n\n\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).\n\n\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).\n\nDowncast dtypes if possible.\n\nKeyword arguments to pass on to the interpolating function.\n\nReturns the same object type as the caller, interpolated at some or all `NaN`\nvalues or None if `inplace=True`.\n\nSee also\n\nFill missing values using different methods.\n\nPiecewise cubic polynomials (Akima interpolator).\n\nPiecewise polynomial in the Bernstein basis.\n\nInterpolate a 1-D function.\n\nInterpolate polynomial (Krogh interpolator).\n\nPCHIP 1-d monotonic cubic interpolation.\n\nCubic spline data interpolator.\n\nNotes\n\nThe \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are\nwrappers around the respective SciPy implementations of similar names. These\nuse the actual numerical values of the index. For more information on their\nbehavior, see the SciPy documentation and SciPy tutorial.\n\nExamples\n\nFilling in `NaN` in a `Series` via linear interpolation.\n\nFilling in `NaN` in a Series by padding, but filling at most two consecutive\n`NaN` at a time.\n\nFilling in `NaN` in a Series via polynomial interpolation or splines: Both\n\u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an `order`\n(int).\n\nFill the DataFrame forward (that is, going down) along each column using\nlinear interpolation.\n\nNote how the last entry in column \u2018a\u2019 is interpolated differently, because\nthere is no entry after it to use for interpolation. Note how the first entry\nin column \u2018b\u2019 remains `NaN`, because there is no entry before it to use for\ninterpolation.\n\nUsing polynomial interpolation.\n\n"}, {"name": "pandas.core.resample.Resampler.last", "path": "reference/api/pandas.core.resample.resampler.last", "type": "Resampling", "text": "\nCompute last of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed last of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.max", "path": "reference/api/pandas.core.resample.resampler.max", "type": "Resampling", "text": "\nCompute max of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed max of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.mean", "path": "reference/api/pandas.core.resample.resampler.mean", "type": "Resampling", "text": "\nCompute mean of groups, excluding missing values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{{'nopython': True, 'nogil':\nFalse, 'parallel': False}}`\n\nNew in version 1.4.0.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\nExamples\n\nGroupby one column and return the mean of the remaining columns in each group.\n\nGroupby two columns and return the mean of the remaining column.\n\nGroupby one column and return the mean of only particular column in the group.\n\n"}, {"name": "pandas.core.resample.Resampler.median", "path": "reference/api/pandas.core.resample.resampler.median", "type": "Resampling", "text": "\nCompute median of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nMedian of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.min", "path": "reference/api/pandas.core.resample.resampler.min", "type": "Resampling", "text": "\nCompute min of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed min of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.nearest", "path": "reference/api/pandas.core.resample.resampler.nearest", "type": "Resampling", "text": "\nResample by using the nearest value.\n\nWhen resampling data, missing values may appear (e.g., when the resampling\nfrequency is higher than the original frequency). The nearest method will\nreplace `NaN` values that appeared in the resampled data with the value from\nthe nearest member of the sequence, based on the index value. Missing values\nthat existed in the original data will not be modified. If limit is given,\nfill only this many values in each direction for each of the original values.\n\nLimit of how many values to fill.\n\nAn upsampled Series or DataFrame with `NaN` values filled with their nearest\nvalue.\n\nSee also\n\nBackward fill the new missing values in the resampled data.\n\nForward fill `NaN` values.\n\nExamples\n\nLimit the number of upsampled values imputed by the nearest:\n\n"}, {"name": "pandas.core.resample.Resampler.nunique", "path": "reference/api/pandas.core.resample.resampler.nunique", "type": "Resampling", "text": "\nReturn number of unique elements in the group.\n\nNumber of unique values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.ohlc", "path": "reference/api/pandas.core.resample.resampler.ohlc", "type": "Resampling", "text": "\nCompute open, high, low and close values of a group, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex\n\nOpen, high, low and close values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.pad", "path": "reference/api/pandas.core.resample.resampler.pad", "type": "Resampling", "text": "\nForward fill the values.\n\nLimit of how many values to fill.\n\nSee also\n\nFill NA/NaN values using the specified method.\n\nFill NA/NaN values using the specified method.\n\n"}, {"name": "pandas.core.resample.Resampler.pipe", "path": "reference/api/pandas.core.resample.resampler.pipe", "type": "Resampling", "text": "\nApply a function func with arguments to this Resampler object and return the\nfunction\u2019s result.\n\nUse .pipe when you want to improve readability by chaining together functions\nthat expect Series, DataFrames, GroupBy or Resampler objects. Instead of\nwriting\n\nYou can write\n\nwhich is much more readable.\n\nFunction to apply to this Resampler object or, alternatively, a (callable,\ndata_keyword) tuple where data_keyword is a string indicating the keyword of\ncallable that expects the Resampler object.\n\nPositional arguments passed into func.\n\nA dictionary of keyword arguments passed into func.\n\nSee also\n\nApply a function with arguments to a series.\n\nApply a function with arguments to a dataframe.\n\nApply function to each group instead of to the full Resampler object.\n\nNotes\n\nSee more here\n\nExamples\n\nTo get the difference between each 2-day period\u2019s maximum and minimum value in\none pass, you can do\n\n"}, {"name": "pandas.core.resample.Resampler.prod", "path": "reference/api/pandas.core.resample.resampler.prod", "type": "Resampling", "text": "\nCompute prod of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed prod of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.quantile", "path": "reference/api/pandas.core.resample.resampler.quantile", "type": "Resampling", "text": "\nReturn value at the given quantile.\n\nQuantile of values within each group.\n\nSee also\n\nReturn a series, where the index is q and the values are the quantiles.\n\nReturn a DataFrame, where the columns are the columns of self, and the values\nare the quantiles.\n\nReturn a DataFrame, where the coulmns are groupby columns, and the values are\nits quantiles.\n\n"}, {"name": "pandas.core.resample.Resampler.sem", "path": "reference/api/pandas.core.resample.resampler.sem", "type": "Resampling", "text": "\nCompute standard error of the mean of groups, excluding missing values.\n\nFor multiple groupings, the result index will be a MultiIndex.\n\nDegrees of freedom.\n\nStandard error of the mean of values within each group.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.size", "path": "reference/api/pandas.core.resample.resampler.size", "type": "Resampling", "text": "\nCompute group sizes.\n\nNumber of rows in each group as a Series if as_index is True or a DataFrame if\nas_index is False.\n\nSee also\n\nApply a function groupby to a Series.\n\nApply a function groupby to each row or column of a DataFrame.\n\n"}, {"name": "pandas.core.resample.Resampler.std", "path": "reference/api/pandas.core.resample.resampler.std", "type": "Resampling", "text": "\nCompute standard deviation of groups, excluding missing values.\n\nDegrees of freedom.\n\nStandard deviation of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.sum", "path": "reference/api/pandas.core.resample.resampler.sum", "type": "Resampling", "text": "\nCompute sum of group values.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nComputed sum of values within each group.\n\n"}, {"name": "pandas.core.resample.Resampler.transform", "path": "reference/api/pandas.core.resample.resampler.transform", "type": "Resampling", "text": "\nCall function producing a like-indexed Series on each group and return a\nSeries with the transformed values.\n\nTo apply to each group. Should return a Series with the same index.\n\nExamples\n\n"}, {"name": "pandas.core.resample.Resampler.var", "path": "reference/api/pandas.core.resample.resampler.var", "type": "Resampling", "text": "\nCompute variance of groups, excluding missing values.\n\nDegrees of freedom.\n\nVariance of values within each group.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.corr", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.corr", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sample correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndex DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.cov", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.cov", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndex DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.mean", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.mean", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.std", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.std", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) standard deviation.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.sum", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.sum", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.ewm.ExponentialMovingWindow.var", "path": "reference/api/pandas.core.window.ewm.exponentialmovingwindow.var", "type": "Window", "text": "\nCalculate the ewm (exponential weighted moment) variance.\n\nUse a standard estimation bias correction.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling ewm with Series data.\n\nCalling ewm with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.aggregate", "path": "reference/api/pandas.core.window.expanding.expanding.aggregate", "type": "Window", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series/Dataframe or when passed to Series/Dataframe.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nSimilar DataFrame method.\n\nSimilar Series method.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.apply", "path": "reference/api/pandas.core.window.expanding.expanding.apply", "type": "Window", "text": "\nCalculate the expanding custom aggregation function.\n\nMust produce a single value from an ndarray input if `raw=True` or a single\nvalue from a Series if `raw=False`. Can also accept a Numba JIT function with\n`engine='numba'` specified.\n\nChanged in version 1.0.0.\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\n`'cython'` : Runs rolling apply through C-extensions from cython.\n\n`'numba'` : Runs rolling apply through JIT compiled code from numba. Only\navailable when `raw` is set to `True`.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.0.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to both the `func` and the\n`apply` rolling aggregation.\n\nNew in version 1.0.0.\n\nPositional arguments to be passed into func.\n\nKeyword arguments to be passed into func.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating apply for Series.\n\nAggregating apply for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.corr", "path": "reference/api/pandas.core.window.expanding.expanding.corr", "type": "Window", "text": "\nCalculate the expanding correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nSimilar method to calculate covariance.\n\nNumPy Pearson\u2019s correlation calculation.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\nNotes\n\nThis function uses Pearson\u2019s definition of correlation\n(https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\nWhen other is not specified, the output will be self correlation (e.g. all\n1\u2019s), except for `DataFrame` inputs with pairwise set to True.\n\nFunction will return `NaN` for correlations of equal valued sequences; this is\nthe result of a 0/0 division error.\n\nWhen pairwise is set to False, only matching columns between self and other\nwill be used.\n\nWhen pairwise is set to True, the output will be a MultiIndex DataFrame with\nthe original index on the first level, and the other DataFrame columns on the\nsecond level.\n\nIn the case of missing elements, only complete pairwise observations will be\nused.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.count", "path": "reference/api/pandas.core.window.expanding.expanding.count", "type": "Window", "text": "\nCalculate the expanding count of non NaN observations.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating count for Series.\n\nAggregating count for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.cov", "path": "reference/api/pandas.core.window.expanding.expanding.cov", "type": "Window", "text": "\nCalculate the expanding sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.kurt", "path": "reference/api/pandas.core.window.expanding.expanding.kurt", "type": "Window", "text": "\nCalculate the expanding Fisher\u2019s definition of kurtosis without bias.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nReference SciPy method.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating kurt for Series.\n\nAggregating kurt for DataFrame.\n\nNotes\n\nA minimum of four periods is required for the calculation.\n\nExamples\n\nThe example below will show a rolling calculation with a window size of four\nmatching the equivalent function call using scipy.stats.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.max", "path": "reference/api/pandas.core.window.expanding.expanding.max", "type": "Window", "text": "\nCalculate the expanding maximum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating max for Series.\n\nAggregating max for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.mean", "path": "reference/api/pandas.core.window.expanding.expanding.mean", "type": "Window", "text": "\nCalculate the expanding mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.median", "path": "reference/api/pandas.core.window.expanding.expanding.median", "type": "Window", "text": "\nCalculate the expanding median.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating median for Series.\n\nAggregating median for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.min", "path": "reference/api/pandas.core.window.expanding.expanding.min", "type": "Window", "text": "\nCalculate the expanding minimum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating min for Series.\n\nAggregating min for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.quantile", "path": "reference/api/pandas.core.window.expanding.expanding.quantile", "type": "Window", "text": "\nCalculate the expanding quantile.\n\nQuantile to compute. 0 <= quantile <= 1.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating quantile for Series.\n\nAggregating quantile for DataFrame.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.rank", "path": "reference/api/pandas.core.window.expanding.expanding.rank", "type": "Window", "text": "\nCalculate the expanding rank.\n\nNew in version 1.4.0.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating rank for Series.\n\nAggregating rank for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.sem", "path": "reference/api/pandas.core.window.expanding.expanding.sem", "type": "Window", "text": "\nCalculate the expanding standard error of mean.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating sem for Series.\n\nAggregating sem for DataFrame.\n\nNotes\n\nA minimum of one period is required for the calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.skew", "path": "reference/api/pandas.core.window.expanding.expanding.skew", "type": "Window", "text": "\nCalculate the expanding unbiased skewness.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nThird moment of a probability density.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating skew for Series.\n\nAggregating skew for DataFrame.\n\nNotes\n\nA minimum of three periods is required for the rolling calculation.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.std", "path": "reference/api/pandas.core.window.expanding.expanding.std", "type": "Window", "text": "\nCalculate the expanding standard deviation.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.std()` is different than the default\n`ddof` of 0 in `numpy.std()`.\n\nA minimum of one period is required for the rolling calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.expanding.Expanding.sum", "path": "reference/api/pandas.core.window.expanding.expanding.sum", "type": "Window", "text": "\nCalculate the expanding sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.expanding.Expanding.var", "path": "reference/api/pandas.core.window.expanding.expanding.var", "type": "Window", "text": "\nCalculate the expanding variance.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling expanding with Series data.\n\nCalling expanding with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.var()` is different than the default\n`ddof` of 0 in `numpy.var()`.\n\nA minimum of one period is required for the rolling calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.aggregate", "path": "reference/api/pandas.core.window.rolling.rolling.aggregate", "type": "Window", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a Series/Dataframe or when passed to Series/Dataframe.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nCalling object with Series data.\n\nCalling object with DataFrame data.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.apply", "path": "reference/api/pandas.core.window.rolling.rolling.apply", "type": "Window", "text": "\nCalculate the rolling custom aggregation function.\n\nMust produce a single value from an ndarray input if `raw=True` or a single\nvalue from a Series if `raw=False`. Can also accept a Numba JIT function with\n`engine='numba'` specified.\n\nChanged in version 1.0.0.\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\n`'cython'` : Runs rolling apply through C-extensions from cython.\n\n`'numba'` : Runs rolling apply through JIT compiled code from numba. Only\navailable when `raw` is set to `True`.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.0.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}` and will be applied to both the `func` and the\n`apply` rolling aggregation.\n\nNew in version 1.0.0.\n\nPositional arguments to be passed into func.\n\nKeyword arguments to be passed into func.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating apply for Series.\n\nAggregating apply for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.corr", "path": "reference/api/pandas.core.window.rolling.rolling.corr", "type": "Window", "text": "\nCalculate the rolling correlation.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nSimilar method to calculate covariance.\n\nNumPy Pearson\u2019s correlation calculation.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating corr for Series.\n\nAggregating corr for DataFrame.\n\nNotes\n\nThis function uses Pearson\u2019s definition of correlation\n(https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n\nWhen other is not specified, the output will be self correlation (e.g. all\n1\u2019s), except for `DataFrame` inputs with pairwise set to True.\n\nFunction will return `NaN` for correlations of equal valued sequences; this is\nthe result of a 0/0 division error.\n\nWhen pairwise is set to False, only matching columns between self and other\nwill be used.\n\nWhen pairwise is set to True, the output will be a MultiIndex DataFrame with\nthe original index on the first level, and the other DataFrame columns on the\nsecond level.\n\nIn the case of missing elements, only complete pairwise observations will be\nused.\n\nExamples\n\nThe below example shows a rolling calculation with a window size of four\nmatching the equivalent function call using `numpy.corrcoef()`.\n\nThe below example shows a similar rolling calculation on a DataFrame using the\npairwise option.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.count", "path": "reference/api/pandas.core.window.rolling.rolling.count", "type": "Window", "text": "\nCalculate the rolling count of non NaN observations.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating count for Series.\n\nAggregating count for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.cov", "path": "reference/api/pandas.core.window.rolling.rolling.cov", "type": "Window", "text": "\nCalculate the rolling sample covariance.\n\nIf not supplied then will default to self and produce pairwise output.\n\nIf False then only matching columns between self and other will be used and\nthe output will be a DataFrame. If True then all pairwise combinations will be\ncalculated and the output will be a MultiIndexed DataFrame in the case of\nDataFrame inputs. In the case of missing elements, only complete pairwise\nobservations will be used.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating cov for Series.\n\nAggregating cov for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.kurt", "path": "reference/api/pandas.core.window.rolling.rolling.kurt", "type": "Window", "text": "\nCalculate the rolling Fisher\u2019s definition of kurtosis without bias.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nReference SciPy method.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating kurt for Series.\n\nAggregating kurt for DataFrame.\n\nNotes\n\nA minimum of four periods is required for the calculation.\n\nExamples\n\nThe example below will show a rolling calculation with a window size of four\nmatching the equivalent function call using scipy.stats.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.max", "path": "reference/api/pandas.core.window.rolling.rolling.max", "type": "Window", "text": "\nCalculate the rolling maximum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating max for Series.\n\nAggregating max for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.mean", "path": "reference/api/pandas.core.window.rolling.rolling.mean", "type": "Window", "text": "\nCalculate the rolling mean.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nThe below examples will show rolling mean calculations with window sizes of\ntwo and three, respectively.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.median", "path": "reference/api/pandas.core.window.rolling.rolling.median", "type": "Window", "text": "\nCalculate the rolling median.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating median for Series.\n\nAggregating median for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nCompute the rolling median of a series with a window size of 3.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.min", "path": "reference/api/pandas.core.window.rolling.rolling.min", "type": "Window", "text": "\nCalculate the rolling minimum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating min for Series.\n\nAggregating min for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nPerforming a rolling minimum with a window size of 3.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.quantile", "path": "reference/api/pandas.core.window.rolling.rolling.quantile", "type": "Window", "text": "\nCalculate the rolling quantile.\n\nQuantile to compute. 0 <= quantile <= 1.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating quantile for Series.\n\nAggregating quantile for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.rank", "path": "reference/api/pandas.core.window.rolling.rolling.rank", "type": "Window", "text": "\nCalculate the rolling rank.\n\nNew in version 1.4.0.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating rank for Series.\n\nAggregating rank for DataFrame.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.sem", "path": "reference/api/pandas.core.window.rolling.rolling.sem", "type": "Window", "text": "\nCalculate the rolling standard error of mean.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sem for Series.\n\nAggregating sem for DataFrame.\n\nNotes\n\nA minimum of one period is required for the calculation.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.skew", "path": "reference/api/pandas.core.window.rolling.rolling.skew", "type": "Window", "text": "\nCalculate the rolling unbiased skewness.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nThird moment of a probability density.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating skew for Series.\n\nAggregating skew for DataFrame.\n\nNotes\n\nA minimum of three periods is required for the rolling calculation.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.std", "path": "reference/api/pandas.core.window.rolling.rolling.std", "type": "Window", "text": "\nCalculate the rolling standard deviation.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.std()` is different than the default\n`ddof` of 0 in `numpy.std()`.\n\nA minimum of one period is required for the rolling calculation.\n\nThe implementation is susceptible to floating point imprecision as shown in\nthe example below.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Rolling.sum", "path": "reference/api/pandas.core.window.rolling.rolling.sum", "type": "Window", "text": "\nCalculate the rolling sum.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.3.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.3.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\nNotes\n\nSee Numba engine and Numba (JIT compilation) for extended documentation and\nperformance considerations for the Numba engine.\n\nExamples\n\nFor DataFrame, each sum is computed column-wise.\n\n"}, {"name": "pandas.core.window.rolling.Rolling.var", "path": "reference/api/pandas.core.window.rolling.rolling.var", "type": "Window", "text": "\nCalculate the rolling variance.\n\nDelta Degrees of Freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nFor NumPy compatibility and will not have an effect on the result.\n\n`'cython'` : Runs the operation through C-extensions from cython.\n\n`'numba'` : Runs the operation through JIT compiled code from numba.\n\n`None` : Defaults to `'cython'` or globally setting `compute.use_numba`\n\nNew in version 1.4.0.\n\nFor `'cython'` engine, there are no accepted `engine_kwargs`\n\nFor `'numba'` engine, the engine can accept `nopython`, `nogil` and `parallel`\ndictionary keys. The values must either be `True` or `False`. The default\n`engine_kwargs` for the `'numba'` engine is `{'nopython': True, 'nogil':\nFalse, 'parallel': False}`\n\nNew in version 1.4.0.\n\nFor NumPy compatibility and will not have an effect on the result.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nEquivalent method for NumPy array.\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\nNotes\n\nThe default `ddof` of 1 used in `Series.var()` is different than the default\n`ddof` of 0 in `numpy.var()`.\n\nA minimum of one period is required for the rolling calculation.\n\nThe implementation is susceptible to floating point imprecision as shown in\nthe example below.\n\nExamples\n\n"}, {"name": "pandas.core.window.rolling.Window.mean", "path": "reference/api/pandas.core.window.rolling.window.mean", "type": "Window", "text": "\nCalculate the rolling weighted window mean.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating mean for Series.\n\nAggregating mean for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.std", "path": "reference/api/pandas.core.window.rolling.window.std", "type": "Window", "text": "\nCalculate the rolling weighted window standard deviation.\n\nNew in version 1.0.0.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating std for Series.\n\nAggregating std for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.sum", "path": "reference/api/pandas.core.window.rolling.window.sum", "type": "Window", "text": "\nCalculate the rolling weighted window sum.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating sum for Series.\n\nAggregating sum for DataFrame.\n\n"}, {"name": "pandas.core.window.rolling.Window.var", "path": "reference/api/pandas.core.window.rolling.window.var", "type": "Window", "text": "\nCalculate the rolling weighted window variance.\n\nNew in version 1.0.0.\n\nKeyword arguments to configure the `SciPy` weighted window type.\n\nReturn type is the same as the original object with `np.float64` dtype.\n\nSee also\n\nCalling rolling with Series data.\n\nCalling rolling with DataFrames.\n\nAggregating var for Series.\n\nAggregating var for DataFrame.\n\n"}, {"name": "pandas.crosstab", "path": "reference/api/pandas.crosstab", "type": "General functions", "text": "\nCompute a simple cross tabulation of two (or more) factors. By default\ncomputes a frequency table of the factors unless an array of values and an\naggregation function are passed.\n\nValues to group by in the rows.\n\nValues to group by in the columns.\n\nArray of values to aggregate according to the factors. Requires aggfunc be\nspecified.\n\nIf passed, must match number of row arrays passed.\n\nIf passed, must match number of column arrays passed.\n\nIf specified, requires values be specified as well.\n\nAdd row/column margins (subtotals).\n\nName of the row/column that will contain the totals when margins is True.\n\nDo not include columns whose entries are all NaN.\n\nNormalize by dividing all values by the sum of values.\n\nIf passed \u2018all\u2019 or True, will normalize over all values.\n\nIf passed \u2018index\u2019 will normalize over each row.\n\nIf passed \u2018columns\u2019 will normalize over each column.\n\nIf margins is True, will also normalize margin values.\n\nCross tabulation of the data.\n\nSee also\n\nReshape data based on column values.\n\nCreate a pivot table as a DataFrame.\n\nNotes\n\nAny Series passed will have their name attributes used unless row or column\nnames for the cross-tabulation are specified.\n\nAny input passed containing Categorical data will have all of its categories\nincluded in the cross-tabulation, even if the actual data does not contain any\ninstances of a particular category.\n\nIn the event that there aren\u2019t overlapping indexes an empty DataFrame will be\nreturned.\n\nExamples\n\nHere \u2018c\u2019 and \u2018f\u2019 are not represented in the data and will not be shown in the\noutput because dropna is True by default. Set dropna=False to preserve\ncategories with no data.\n\n"}, {"name": "pandas.cut", "path": "reference/api/pandas.cut", "type": "General functions", "text": "\nBin values into discrete intervals.\n\nUse cut when you need to segment and sort data values into bins. This function\nis also useful for going from a continuous variable to a categorical variable.\nFor example, cut could convert ages to groups of age ranges. Supports binning\ninto an equal number of bins, or a pre-specified array of bins.\n\nThe input array to be binned. Must be 1-dimensional.\n\nThe criteria to bin by.\n\nint : Defines the number of equal-width bins in the range of x. The range of x\nis extended by .1% on each side to include the minimum and maximum values of\nx.\n\nsequence of scalars : Defines the bin edges allowing for non-uniform width. No\nextension of the range of x is done.\n\nIntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for\nbins must be non-overlapping.\n\nIndicates whether bins includes the rightmost edge or not. If `right == True`\n(the default), then the bins `[1, 2, 3, 4]` indicate (1,2], (2,3], (3,4]. This\nargument is ignored when bins is an IntervalIndex.\n\nSpecifies the labels for the returned bins. Must be the same length as the\nresulting bins. If False, returns only integer indicators of the bins. This\naffects the type of the output container (see below). This argument is ignored\nwhen bins is an IntervalIndex. If True, raises an error. When ordered=False,\nlabels must be provided.\n\nWhether to return the bins or not. Useful when bins is provided as a scalar.\n\nThe precision at which to store and display the bins labels.\n\nWhether the first interval should be left-inclusive or not.\n\nIf bin edges are not unique, raise ValueError or drop non-uniques.\n\nWhether the labels are ordered or not. Applies to returned types Categorical\nand Series (with Categorical dtype). If True, the resulting categorical will\nbe ordered. If False, the resulting categorical will be unordered (labels must\nbe provided).\n\nNew in version 1.1.0.\n\nAn array-like object representing the respective bin for each value of x. The\ntype depends on the value of labels.\n\nNone (default) : returns a Series for Series x or a Categorical for all other\ninputs. The values stored within are Interval dtype.\n\nsequence of scalars : returns a Series for Series x or a Categorical for all\nother inputs. The values stored within are whatever the type in the sequence\nis.\n\nFalse : returns an ndarray of integers.\n\nThe computed or specified bins. Only returned when retbins=True. For scalar or\nsequence bins, this is an ndarray with the computed bins. If set\nduplicates=drop, bins will drop non-unique bin. For an IntervalIndex bins,\nthis is equal to bins.\n\nSee also\n\nDiscretize variable into equal-sized buckets based on rank or based on sample\nquantiles.\n\nArray type for storing data that come from a fixed set of values.\n\nOne-dimensional array with axis labels (including time series).\n\nImmutable Index implementing an ordered, sliceable set.\n\nNotes\n\nAny NA values will be NA in the result. Out of bounds values will be NA in the\nresulting Series or Categorical object.\n\nExamples\n\nDiscretize into three equal-sized bins.\n\nDiscovers the same bins, but assign them specific labels. Notice that the\nreturned Categorical\u2019s categories are labels and is ordered.\n\n`ordered=False` will result in unordered categories when labels are passed.\nThis parameter can be used to allow non-unique labels:\n\n`labels=False` implies you just want the bins back.\n\nPassing a Series as an input returns a Series with categorical dtype:\n\nPassing a Series as an input returns a Series with mapping value. It is used\nto map numerically to intervals based on bins.\n\nUse drop optional when bins is not unique\n\nPassing an IntervalIndex for bins results in those categories exactly. Notice\nthat values not covered by the IntervalIndex are set to NaN. 0 is to the left\nof the first bin (which is closed on the right), and 1.5 falls between two\nbins.\n\n"}, {"name": "pandas.DataFrame", "path": "reference/api/pandas.dataframe", "type": "DataFrame", "text": "\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns). Arithmetic\noperations align on both row and column labels. Can be thought of as a dict-\nlike container for Series objects. The primary pandas data structure.\n\nDict can contain Series, arrays, constants, dataclass or list-like objects. If\ndata is a dict, column order follows insertion-order. If a dict contains\nSeries which have an index defined, it is aligned by its index.\n\nChanged in version 0.25.0: If data is a list of dicts, column order follows\ninsertion-order.\n\nIndex to use for resulting frame. Will default to RangeIndex if no indexing\ninformation part of input data and no index provided.\n\nColumn labels to use for resulting frame when data does not have them,\ndefaulting to RangeIndex(0, 1, 2, \u2026, n). If data contains column labels, will\nperform column selection instead.\n\nData type to force. Only a single dtype is allowed. If None, infer.\n\nCopy data from inputs. For dict data, the default of None behaves like\n`copy=True`. For DataFrame or 2d ndarray input, the default of None behaves\nlike `copy=False`.\n\nChanged in version 1.3.0.\n\nSee also\n\nConstructor from tuples, also record arrays.\n\nFrom dicts of Series, arrays, or dicts.\n\nRead a comma-separated values (csv) file into DataFrame.\n\nRead general delimited file into DataFrame.\n\nRead text from clipboard into DataFrame.\n\nExamples\n\nConstructing DataFrame from a dictionary.\n\nNotice that the inferred dtype is int64.\n\nTo enforce a single dtype:\n\nConstructing DataFrame from a dictionary including Series:\n\nConstructing DataFrame from numpy ndarray:\n\nConstructing DataFrame from a numpy ndarray that has labeled columns:\n\nConstructing DataFrame from dataclass:\n\nAttributes\n\n`at`\n\nAccess a single value for a row/column label pair.\n\n`attrs`\n\nDictionary of global attributes of this dataset.\n\n`axes`\n\nReturn a list representing the axes of the DataFrame.\n\n`columns`\n\nThe column labels of the DataFrame.\n\n`dtypes`\n\nReturn the dtypes in the DataFrame.\n\n`empty`\n\nIndicator whether Series/DataFrame is empty.\n\n`flags`\n\nGet the properties associated with this pandas object.\n\n`iat`\n\nAccess a single value for a row/column pair by integer position.\n\n`iloc`\n\nPurely integer-location based indexing for selection by position.\n\n`index`\n\nThe index (row labels) of the DataFrame.\n\n`loc`\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`ndim`\n\nReturn an int representing the number of axes / array dimensions.\n\n`shape`\n\nReturn a tuple representing the dimensionality of the DataFrame.\n\n`size`\n\nReturn an int representing the number of elements in this object.\n\n`style`\n\nReturns a Styler object.\n\n`values`\n\nReturn a Numpy representation of the DataFrame.\n\nT\n\nMethods\n\n`abs`()\n\nReturn a Series/DataFrame with absolute numeric value of each element.\n\n`add`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator add).\n\n`add_prefix`(prefix)\n\nPrefix labels with string prefix.\n\n`add_suffix`(suffix)\n\nSuffix labels with string suffix.\n\n`agg`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`aggregate`([func, axis])\n\nAggregate using one or more operations over the specified axis.\n\n`align`(other[, join, axis, level, copy, ...])\n\nAlign two objects on their axes with the specified join method.\n\n`all`([axis, bool_only, skipna, level])\n\nReturn whether all elements are True, potentially over an axis.\n\n`any`([axis, bool_only, skipna, level])\n\nReturn whether any element is True, potentially over an axis.\n\n`append`(other[, ignore_index, ...])\n\nAppend rows of other to the end of caller, returning a new object.\n\n`apply`(func[, axis, raw, result_type, args])\n\nApply a function along an axis of the DataFrame.\n\n`applymap`(func[, na_action])\n\nApply a function to a Dataframe elementwise.\n\n`asfreq`(freq[, method, how, normalize, ...])\n\nConvert time series to specified frequency.\n\n`asof`(where[, subset])\n\nReturn the last row(s) without any NaNs before where.\n\n`assign`(**kwargs)\n\nAssign new columns to a DataFrame.\n\n`astype`(dtype[, copy, errors])\n\nCast a pandas object to a specified dtype `dtype`.\n\n`at_time`(time[, asof, axis])\n\nSelect values at particular time of day (e.g., 9:30AM).\n\n`backfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`between_time`(start_time, end_time[, ...])\n\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\n`bfill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\n`bool`()\n\nReturn the bool of a single element Series or DataFrame.\n\n`boxplot`([column, by, ax, fontsize, rot, ...])\n\nMake a box plot from DataFrame columns.\n\n`clip`([lower, upper, axis, inplace])\n\nTrim values at input threshold(s).\n\n`combine`(other, func[, fill_value, overwrite])\n\nPerform column-wise combine with another DataFrame.\n\n`combine_first`(other)\n\nUpdate null elements with value in the same location in other.\n\n`compare`(other[, align_axis, keep_shape, ...])\n\nCompare to another DataFrame and show the differences.\n\n`convert_dtypes`([infer_objects, ...])\n\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\n`copy`([deep])\n\nMake a copy of this object's indices and data.\n\n`corr`([method, min_periods])\n\nCompute pairwise correlation of columns, excluding NA/null values.\n\n`corrwith`(other[, axis, drop, method])\n\nCompute pairwise correlation.\n\n`count`([axis, level, numeric_only])\n\nCount non-NA cells for each column or row.\n\n`cov`([min_periods, ddof])\n\nCompute pairwise covariance of columns, excluding NA/null values.\n\n`cummax`([axis, skipna])\n\nReturn cumulative maximum over a DataFrame or Series axis.\n\n`cummin`([axis, skipna])\n\nReturn cumulative minimum over a DataFrame or Series axis.\n\n`cumprod`([axis, skipna])\n\nReturn cumulative product over a DataFrame or Series axis.\n\n`cumsum`([axis, skipna])\n\nReturn cumulative sum over a DataFrame or Series axis.\n\n`describe`([percentiles, include, exclude, ...])\n\nGenerate descriptive statistics.\n\n`diff`([periods, axis])\n\nFirst discrete difference of element.\n\n`div`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`divide`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`dot`(other)\n\nCompute the matrix multiplication between the DataFrame and other.\n\n`drop`([labels, axis, index, columns, level, ...])\n\nDrop specified labels from rows or columns.\n\n`drop_duplicates`([subset, keep, inplace, ...])\n\nReturn DataFrame with duplicate rows removed.\n\n`droplevel`(level[, axis])\n\nReturn Series/DataFrame with requested index / column level(s) removed.\n\n`dropna`([axis, how, thresh, subset, inplace])\n\nRemove missing values.\n\n`duplicated`([subset, keep])\n\nReturn boolean Series denoting duplicate rows.\n\n`eq`(other[, axis, level])\n\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\n`equals`(other)\n\nTest whether two objects contain the same elements.\n\n`eval`(expr[, inplace])\n\nEvaluate a string describing operations on DataFrame columns.\n\n`ewm`([com, span, halflife, alpha, ...])\n\nProvide exponentially weighted (EW) calculations.\n\n`expanding`([min_periods, center, axis, method])\n\nProvide expanding window calculations.\n\n`explode`(column[, ignore_index])\n\nTransform each element of a list-like to a row, replicating index values.\n\n`ffill`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`fillna`([value, method, axis, inplace, ...])\n\nFill NA/NaN values using the specified method.\n\n`filter`([items, like, regex, axis])\n\nSubset the dataframe rows or columns according to the specified index labels.\n\n`first`(offset)\n\nSelect initial periods of time series data based on a date offset.\n\n`first_valid_index`()\n\nReturn index for first non-NA value or None, if no NA value is found.\n\n`floordiv`(other[, axis, level, fill_value])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\n`from_dict`(data[, orient, dtype, columns])\n\nConstruct DataFrame from dict of array-like or dicts.\n\n`from_records`(data[, index, exclude, ...])\n\nConvert structured or record ndarray to DataFrame.\n\n`ge`(other[, axis, level])\n\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\n`get`(key[, default])\n\nGet item from object for given key (ex: DataFrame column).\n\n`groupby`([by, axis, level, as_index, sort, ...])\n\nGroup DataFrame using a mapper or by a Series of columns.\n\n`gt`(other[, axis, level])\n\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\n`head`([n])\n\nReturn the first n rows.\n\n`hist`([column, by, grid, xlabelsize, xrot, ...])\n\nMake a histogram of the DataFrame's columns.\n\n`idxmax`([axis, skipna])\n\nReturn index of first occurrence of maximum over requested axis.\n\n`idxmin`([axis, skipna])\n\nReturn index of first occurrence of minimum over requested axis.\n\n`infer_objects`()\n\nAttempt to infer better dtypes for object columns.\n\n`info`([verbose, buf, max_cols, memory_usage, ...])\n\nPrint a concise summary of a DataFrame.\n\n`insert`(loc, column, value[, allow_duplicates])\n\nInsert column into DataFrame at specified location.\n\n`interpolate`([method, axis, limit, inplace, ...])\n\nFill NaN values using an interpolation method.\n\n`isin`(values)\n\nWhether each element in the DataFrame is contained in values.\n\n`isna`()\n\nDetect missing values.\n\n`isnull`()\n\nDataFrame.isnull is an alias for DataFrame.isna.\n\n`items`()\n\nIterate over (column name, Series) pairs.\n\n`iteritems`()\n\nIterate over (column name, Series) pairs.\n\n`iterrows`()\n\nIterate over DataFrame rows as (index, Series) pairs.\n\n`itertuples`([index, name])\n\nIterate over DataFrame rows as namedtuples.\n\n`join`(other[, on, how, lsuffix, rsuffix, sort])\n\nJoin columns of another DataFrame.\n\n`keys`()\n\nGet the 'info axis' (see Indexing for more).\n\n`kurt`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`kurtosis`([axis, skipna, level, numeric_only])\n\nReturn unbiased kurtosis over requested axis.\n\n`last`(offset)\n\nSelect final periods of time series data based on a date offset.\n\n`last_valid_index`()\n\nReturn index for last non-NA value or None, if no NA value is found.\n\n`le`(other[, axis, level])\n\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\n`lookup`(row_labels, col_labels)\n\n(DEPRECATED) Label-based \"fancy indexing\" function for DataFrame.\n\n`lt`(other[, axis, level])\n\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\n`mad`([axis, skipna, level])\n\nReturn the mean absolute deviation of the values over the requested axis.\n\n`mask`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is True.\n\n`max`([axis, skipna, level, numeric_only])\n\nReturn the maximum of the values over the requested axis.\n\n`mean`([axis, skipna, level, numeric_only])\n\nReturn the mean of the values over the requested axis.\n\n`median`([axis, skipna, level, numeric_only])\n\nReturn the median of the values over the requested axis.\n\n`melt`([id_vars, value_vars, var_name, ...])\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\n`memory_usage`([index, deep])\n\nReturn the memory usage of each column in bytes.\n\n`merge`(right[, how, on, left_on, right_on, ...])\n\nMerge DataFrame or named Series objects with a database-style join.\n\n`min`([axis, skipna, level, numeric_only])\n\nReturn the minimum of the values over the requested axis.\n\n`mod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\n`mode`([axis, numeric_only, dropna])\n\nGet the mode(s) of each element along the selected axis.\n\n`mul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`multiply`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\n`ne`(other[, axis, level])\n\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\n`nlargest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in descending order.\n\n`notna`()\n\nDetect existing (non-missing) values.\n\n`notnull`()\n\nDataFrame.notnull is an alias for DataFrame.notna.\n\n`nsmallest`(n, columns[, keep])\n\nReturn the first n rows ordered by columns in ascending order.\n\n`nunique`([axis, dropna])\n\nCount number of distinct elements in specified axis.\n\n`pad`([axis, inplace, limit, downcast])\n\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\n`pct_change`([periods, fill_method, limit, freq])\n\nPercentage change between the current and a prior element.\n\n`pipe`(func, *args, **kwargs)\n\nApply chainable functions that expect Series or DataFrames.\n\n`pivot`([index, columns, values])\n\nReturn reshaped DataFrame organized by given index / column values.\n\n`pivot_table`([values, index, columns, ...])\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\n`plot`\n\nalias of `pandas.plotting._core.PlotAccessor`\n\n`pop`(item)\n\nReturn item and drop from frame.\n\n`pow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\n`prod`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`product`([axis, skipna, level, numeric_only, ...])\n\nReturn the product of the values over the requested axis.\n\n`quantile`([q, axis, numeric_only, interpolation])\n\nReturn values at the given quantile over requested axis.\n\n`query`(expr[, inplace])\n\nQuery the columns of a DataFrame with a boolean expression.\n\n`radd`(other[, axis, level, fill_value])\n\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\n`rank`([axis, method, numeric_only, ...])\n\nCompute numerical data ranks (1 through n) along axis.\n\n`rdiv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`reindex`([labels, index, columns, axis, ...])\n\nConform Series/DataFrame to new index with optional filling logic.\n\n`reindex_like`(other[, method, copy, limit, ...])\n\nReturn an object with matching indices as other object.\n\n`rename`([mapper, index, columns, axis, copy, ...])\n\nAlter axes labels.\n\n`rename_axis`([mapper, index, columns, axis, ...])\n\nSet the name of the axis for the index or columns.\n\n`reorder_levels`(order[, axis])\n\nRearrange index levels using input order.\n\n`replace`([to_replace, value, inplace, limit, ...])\n\nReplace values given in to_replace with value.\n\n`resample`(rule[, axis, closed, label, ...])\n\nResample time-series data.\n\n`reset_index`([level, drop, inplace, ...])\n\nReset the index, or a level of it.\n\n`rfloordiv`(other[, axis, level, fill_value])\n\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\n`rmod`(other[, axis, level, fill_value])\n\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\n`rmul`(other[, axis, level, fill_value])\n\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\n`rolling`(window[, min_periods, center, ...])\n\nProvide rolling window calculations.\n\n`round`([decimals])\n\nRound a DataFrame to a variable number of decimal places.\n\n`rpow`(other[, axis, level, fill_value])\n\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\n`rsub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\n`rtruediv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\n`sample`([n, frac, replace, weights, ...])\n\nReturn a random sample of items from an axis of object.\n\n`select_dtypes`([include, exclude])\n\nReturn a subset of the DataFrame's columns based on the column dtypes.\n\n`sem`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased standard error of the mean over requested axis.\n\n`set_axis`(labels[, axis, inplace])\n\nAssign desired index to given axis.\n\n`set_flags`(*[, copy, allows_duplicate_labels])\n\nReturn a new object with updated flags.\n\n`set_index`(keys[, drop, append, inplace, ...])\n\nSet the DataFrame index using existing columns.\n\n`shift`([periods, freq, axis, fill_value])\n\nShift index by desired number of periods with an optional time freq.\n\n`skew`([axis, skipna, level, numeric_only])\n\nReturn unbiased skew over requested axis.\n\n`slice_shift`([periods, axis])\n\n(DEPRECATED) Equivalent to shift without copying data.\n\n`sort_index`([axis, level, ascending, ...])\n\nSort object by labels (along an axis).\n\n`sort_values`(by[, axis, ascending, inplace, ...])\n\nSort by the values along either axis.\n\n`sparse`\n\nalias of `pandas.core.arrays.sparse.accessor.SparseFrameAccessor`\n\n`squeeze`([axis])\n\nSqueeze 1 dimensional axis objects into scalars.\n\n`stack`([level, dropna])\n\nStack the prescribed level(s) from columns to index.\n\n`std`([axis, skipna, level, ddof, numeric_only])\n\nReturn sample standard deviation over requested axis.\n\n`sub`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`subtract`(other[, axis, level, fill_value])\n\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\n`sum`([axis, skipna, level, numeric_only, ...])\n\nReturn the sum of the values over the requested axis.\n\n`swapaxes`(axis1, axis2[, copy])\n\nInterchange axes and swap values axes appropriately.\n\n`swaplevel`([i, j, axis])\n\nSwap levels i and j in a `MultiIndex`.\n\n`tail`([n])\n\nReturn the last n rows.\n\n`take`(indices[, axis, is_copy])\n\nReturn the elements in the given positional indices along an axis.\n\n`to_clipboard`([excel, sep])\n\nCopy object to the system clipboard.\n\n`to_csv`([path_or_buf, sep, na_rep, ...])\n\nWrite object to a comma-separated values (csv) file.\n\n`to_dict`([orient, into])\n\nConvert the DataFrame to a dictionary.\n\n`to_excel`(excel_writer[, sheet_name, na_rep, ...])\n\nWrite object to an Excel sheet.\n\n`to_feather`(path, **kwargs)\n\nWrite a DataFrame to the binary Feather format.\n\n`to_gbq`(destination_table[, project_id, ...])\n\nWrite a DataFrame to a Google BigQuery table.\n\n`to_hdf`(path_or_buf, key[, mode, complevel, ...])\n\nWrite the contained data to an HDF5 file using HDFStore.\n\n`to_html`([buf, columns, col_space, header, ...])\n\nRender a DataFrame as an HTML table.\n\n`to_json`([path_or_buf, orient, date_format, ...])\n\nConvert the object to a JSON string.\n\n`to_latex`([buf, columns, col_space, header, ...])\n\nRender object to a LaTeX tabular, longtable, or nested table.\n\n`to_markdown`([buf, mode, index, storage_options])\n\nPrint DataFrame in Markdown-friendly format.\n\n`to_numpy`([dtype, copy, na_value])\n\nConvert the DataFrame to a NumPy array.\n\n`to_parquet`([path, engine, compression, ...])\n\nWrite a DataFrame to the binary parquet format.\n\n`to_period`([freq, axis, copy])\n\nConvert DataFrame from DatetimeIndex to PeriodIndex.\n\n`to_pickle`(path[, compression, protocol, ...])\n\nPickle (serialize) object to file.\n\n`to_records`([index, column_dtypes, index_dtypes])\n\nConvert DataFrame to a NumPy record array.\n\n`to_sql`(name, con[, schema, if_exists, ...])\n\nWrite records stored in a DataFrame to a SQL database.\n\n`to_stata`(path[, convert_dates, write_index, ...])\n\nExport DataFrame object to Stata dta format.\n\n`to_string`([buf, columns, col_space, header, ...])\n\nRender a DataFrame to a console-friendly tabular output.\n\n`to_timestamp`([freq, how, axis, copy])\n\nCast to DatetimeIndex of timestamps, at beginning of period.\n\n`to_xarray`()\n\nReturn an xarray object from the pandas object.\n\n`to_xml`([path_or_buffer, index, root_name, ...])\n\nRender a DataFrame to an XML document.\n\n`transform`(func[, axis])\n\nCall `func` on self producing a DataFrame with the same axis shape as self.\n\n`transpose`(*args[, copy])\n\nTranspose index and columns.\n\n`truediv`(other[, axis, level, fill_value])\n\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\n`truncate`([before, after, axis, copy])\n\nTruncate a Series or DataFrame before and after some index value.\n\n`tshift`([periods, freq, axis])\n\n(DEPRECATED) Shift the time index, using the index's frequency if available.\n\n`tz_convert`(tz[, axis, level, copy])\n\nConvert tz-aware axis to target time zone.\n\n`tz_localize`(tz[, axis, level, copy, ...])\n\nLocalize tz-naive index of a Series or DataFrame to target time zone.\n\n`unstack`([level, fill_value])\n\nPivot a level of the (necessarily hierarchical) index labels.\n\n`update`(other[, join, overwrite, ...])\n\nModify in place using non-NA values from another DataFrame.\n\n`value_counts`([subset, normalize, sort, ...])\n\nReturn a Series containing counts of unique rows in the DataFrame.\n\n`var`([axis, skipna, level, ddof, numeric_only])\n\nReturn unbiased variance over requested axis.\n\n`where`(cond[, other, inplace, axis, level, ...])\n\nReplace values where the condition is False.\n\n`xs`(key[, axis, level, drop_level])\n\nReturn cross-section from the Series/DataFrame.\n\n"}, {"name": "pandas.DataFrame.__iter__", "path": "reference/api/pandas.dataframe.__iter__", "type": "DataFrame", "text": "\nIterate over info axis.\n\nInfo axis as iterator.\n\n"}, {"name": "pandas.DataFrame.abs", "path": "reference/api/pandas.dataframe.abs", "type": "DataFrame", "text": "\nReturn a Series/DataFrame with absolute numeric value of each element.\n\nThis function only applies to elements that are all numeric.\n\nSeries/DataFrame containing the absolute value of each element.\n\nSee also\n\nCalculate the absolute value element-wise.\n\nNotes\n\nFor `complex` inputs, `1.2 + 1j`, the absolute value is \\\\(\\sqrt{ a^2 + b^2\n}\\\\).\n\nExamples\n\nAbsolute numeric values in a Series.\n\nAbsolute numeric values in a Series with complex numbers.\n\nAbsolute numeric values in a Series with a Timedelta element.\n\nSelect rows with data closest to certain value using argsort (from\nStackOverflow).\n\n"}, {"name": "pandas.DataFrame.add", "path": "reference/api/pandas.dataframe.add", "type": "DataFrame", "text": "\nGet Addition of dataframe and other, element-wise (binary operator add).\n\nEquivalent to `dataframe + other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, radd.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.add_prefix", "path": "reference/api/pandas.dataframe.add_prefix", "type": "DataFrame", "text": "\nPrefix labels with string prefix.\n\nFor Series, the row labels are prefixed. For DataFrame, the column labels are\nprefixed.\n\nThe string to add before each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nSuffix row labels with string suffix.\n\nSuffix column labels with string suffix.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.add_suffix", "path": "reference/api/pandas.dataframe.add_suffix", "type": "DataFrame", "text": "\nSuffix labels with string suffix.\n\nFor Series, the row labels are suffixed. For DataFrame, the column labels are\nsuffixed.\n\nThe string to add after each label.\n\nNew Series or DataFrame with updated labels.\n\nSee also\n\nPrefix row labels with string prefix.\n\nPrefix column labels with string prefix.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.agg", "path": "reference/api/pandas.dataframe.agg", "type": "DataFrame", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nIf 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply\nfunction to each row.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nPerform any type of operations.\n\nPerform transformation type operations.\n\nPerform operations over groups.\n\nPerform operations over resampled bins.\n\nPerform operations over rolling window.\n\nPerform operations over expanding window.\n\nPerform operation over exponential weighted window.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\nAggregate these functions over the rows.\n\nDifferent aggregations per column.\n\nAggregate different functions over the columns and rename the index of the\nresulting DataFrame.\n\nAggregate over the columns.\n\n"}, {"name": "pandas.DataFrame.aggregate", "path": "reference/api/pandas.dataframe.aggregate", "type": "DataFrame", "text": "\nAggregate using one or more operations over the specified axis.\n\nFunction to use for aggregating the data. If a function, must either work when\npassed a DataFrame or when passed to DataFrame.apply.\n\nAccepted combinations are:\n\nfunction\n\nstring function name\n\nlist of functions and/or function names, e.g. `[np.sum, 'mean']`\n\ndict of axis labels -> functions, function names or list of such.\n\nIf 0 or \u2018index\u2019: apply function to each column. If 1 or \u2018columns\u2019: apply\nfunction to each row.\n\nPositional arguments to pass to func.\n\nKeyword arguments to pass to func.\n\nThe return can be:\n\nscalar : when Series.agg is called with single function\n\nSeries : when DataFrame.agg is called with a single function\n\nDataFrame : when DataFrame.agg is called with several functions\n\nReturn scalar, Series or DataFrame.\n\nSee also\n\nPerform any type of operations.\n\nPerform transformation type operations.\n\nPerform operations over groups.\n\nPerform operations over resampled bins.\n\nPerform operations over rolling window.\n\nPerform operations over expanding window.\n\nPerform operation over exponential weighted window.\n\nNotes\n\nagg is an alias for aggregate. Use the alias.\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nA passed user-defined-function will be passed a Series for evaluation.\n\nExamples\n\nAggregate these functions over the rows.\n\nDifferent aggregations per column.\n\nAggregate different functions over the columns and rename the index of the\nresulting DataFrame.\n\nAggregate over the columns.\n\n"}, {"name": "pandas.DataFrame.align", "path": "reference/api/pandas.dataframe.align", "type": "DataFrame", "text": "\nAlign two objects on their axes with the specified join method.\n\nJoin method is specified for each axis Index.\n\nAlign on index (0), columns (1), or both (None).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nAlways returns new objects. If copy=False and no reindexing is required then\noriginal objects are returned.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMethod to use for filling holes in reindexed Series:\n\npad / ffill: propagate last valid observation forward to next valid.\n\nbackfill / bfill: use NEXT valid observation to fill gap.\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nFilling axis, method and limit.\n\nBroadcast values along this axis, if aligning two objects of different\ndimensions.\n\nAligned objects.\n\nExamples\n\nAlign on columns:\n\nWe can also align on the index:\n\nFinally, the default axis=None will align on both index and columns:\n\n"}, {"name": "pandas.DataFrame.all", "path": "reference/api/pandas.dataframe.all", "type": "DataFrame", "text": "\nReturn whether all elements are True, potentially over an axis.\n\nReturns True unless there at least one element within a series or along a\nDataframe axis that is False or equivalent (e.g. zero or empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be True, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, DataFrame is returned; otherwise, Series is\nreturned.\n\nSee also\n\nReturn True if all elements are True.\n\nReturn True if one (or more) elements are True.\n\nExamples\n\nSeries\n\nDataFrames\n\nCreate a dataframe from a dictionary.\n\nDefault behaviour checks if column-wise values all return True.\n\nSpecify `axis='columns'` to check if row-wise values all return True.\n\nOr `axis=None` for whether every value is True.\n\n"}, {"name": "pandas.DataFrame.any", "path": "reference/api/pandas.dataframe.any", "type": "DataFrame", "text": "\nReturn whether any element is True, potentially over an axis.\n\nReturns False unless there is at least one element within a series or along a\nDataframe axis that is True or equivalent (e.g. non-zero or non-empty).\n\nIndicate which axis or axes should be reduced.\n\n0 / \u2018index\u2019 : reduce the index, return a Series whose index is the original\ncolumn labels.\n\n1 / \u2018columns\u2019 : reduce the columns, return a Series whose index is the\noriginal index.\n\nNone : reduce all axes, return a scalar.\n\nInclude only boolean columns. If None, will attempt to use everything, then\nuse only boolean data. Not implemented for Series.\n\nExclude NA/null values. If the entire row/column is NA and skipna is True,\nthen the result will be False, as for an empty row/column. If skipna is False,\nthen NA are treated as True, because these are not equal to zero.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nIf level is specified, then, DataFrame is returned; otherwise, Series is\nreturned.\n\nSee also\n\nNumpy version of this method.\n\nReturn whether any element is True.\n\nReturn whether all elements are True.\n\nReturn whether any element is True over requested axis.\n\nReturn whether all elements are True over requested axis.\n\nExamples\n\nSeries\n\nFor Series input, the output is a scalar indicating whether any element is\nTrue.\n\nDataFrame\n\nWhether each column contains at least one True element (the default).\n\nAggregating over the columns.\n\nAggregating over the entire DataFrame with `axis=None`.\n\nany for an empty DataFrame is an empty Series.\n\n"}, {"name": "pandas.DataFrame.append", "path": "reference/api/pandas.dataframe.append", "type": "DataFrame", "text": "\nAppend rows of other to the end of caller, returning a new object.\n\nColumns in other that are not in the caller are added as new columns.\n\nThe data to append.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nIf True, raise ValueError on creating index with duplicates.\n\nSort columns if the columns of self and other are not aligned.\n\nChanged in version 1.0.0: Changed to not sort by default.\n\nA new DataFrame consisting of the rows of caller and the rows of other.\n\nSee also\n\nGeneral function to concatenate DataFrame or Series objects.\n\nNotes\n\nIf a list of dict/series is passed and the keys are all contained in the\nDataFrame\u2019s index, the order of the columns in the resulting DataFrame will be\nunchanged.\n\nIteratively appending rows to a DataFrame can be more computationally\nintensive than a single concatenate. A better solution is to append those rows\nto a list and then concatenate the list with the original DataFrame all at\nonce.\n\nExamples\n\nWith ignore_index set to True:\n\nThe following, while not recommended methods for generating DataFrames, show\ntwo ways to generate a DataFrame from multiple data sources.\n\nLess efficient:\n\nMore efficient:\n\n"}, {"name": "pandas.DataFrame.apply", "path": "reference/api/pandas.dataframe.apply", "type": "DataFrame", "text": "\nApply a function along an axis of the DataFrame.\n\nObjects passed to the function are Series objects whose index is either the\nDataFrame\u2019s index (`axis=0`) or the DataFrame\u2019s columns (`axis=1`). By default\n(`result_type=None`), the final return type is inferred from the return type\nof the applied function. Otherwise, it depends on the result_type argument.\n\nFunction to apply to each column or row.\n\nAxis along which the function is applied:\n\n0 or \u2018index\u2019: apply function to each column.\n\n1 or \u2018columns\u2019: apply function to each row.\n\nDetermines if row or column is passed as a Series or ndarray object:\n\n`False` : passes each row or column as a Series to the function.\n\n`True` : the passed function will receive ndarray objects instead. If you are\njust applying a NumPy reduction function this will achieve much better\nperformance.\n\nThese only act when `axis=1` (columns):\n\n\u2018expand\u2019 : list-like results will be turned into columns.\n\n\u2018reduce\u2019 : returns a Series if possible rather than expanding list-like\nresults. This is the opposite of \u2018expand\u2019.\n\n\u2018broadcast\u2019 : results will be broadcast to the original shape of the\nDataFrame, the original index and columns will be retained.\n\nThe default behaviour (None) depends on the return value of the applied\nfunction: list-like results will be returned as a Series of those. However if\nthe apply function returns a Series these are expanded to columns.\n\nPositional arguments to pass to func in addition to the array/series.\n\nAdditional keyword arguments to pass as keywords arguments to func.\n\nResult of applying `func` along the given axis of the DataFrame.\n\nSee also\n\nFor elementwise operations.\n\nOnly perform aggregating type operations.\n\nOnly perform transforming type operations.\n\nNotes\n\nFunctions that mutate the passed object can produce unexpected behavior or\nerrors and are not supported. See Mutating with User Defined Function (UDF)\nmethods for more details.\n\nExamples\n\nUsing a numpy universal function (in this case the same as `np.sqrt(df)`):\n\nUsing a reducing function on either axis\n\nReturning a list-like will result in a Series\n\nPassing `result_type='expand'` will expand list-like results to columns of a\nDataframe\n\nReturning a Series inside the function is similar to passing\n`result_type='expand'`. The resulting column names will be the Series index.\n\nPassing `result_type='broadcast'` will ensure the same shape result, whether\nlist-like or scalar is returned by the function, and broadcast it along the\naxis. The resulting column names will be the originals.\n\n"}, {"name": "pandas.DataFrame.applymap", "path": "reference/api/pandas.dataframe.applymap", "type": "DataFrame", "text": "\nApply a function to a Dataframe elementwise.\n\nThis method applies a function that accepts and returns a scalar to every\nelement of a DataFrame.\n\nPython function, returns a single value from a single value.\n\nIf \u2018ignore\u2019, propagate NaN values, without passing them to func.\n\nNew in version 1.2.\n\nAdditional keyword arguments to pass as keywords arguments to func.\n\nNew in version 1.3.0.\n\nTransformed DataFrame.\n\nSee also\n\nApply a function along input axis of DataFrame.\n\nExamples\n\nLike Series.map, NA values can be ignored:\n\nNote that a vectorized version of func often exists, which will be much\nfaster. You could square each number elementwise.\n\nBut it\u2019s better to avoid applymap in that case.\n\n"}, {"name": "pandas.DataFrame.asfreq", "path": "reference/api/pandas.dataframe.asfreq", "type": "DataFrame", "text": "\nConvert time series to specified frequency.\n\nReturns the original data conformed to a new index with the specified\nfrequency.\n\nIf the index of this DataFrame is a `PeriodIndex`, the new index is the result\nof transforming the original index with `PeriodIndex.asfreq` (so the original\nindex will map one-to-one to the new index).\n\nOtherwise, the new index will be equivalent to `pd.date_range(start, end,\nfreq=freq)` where `start` and `end` are, respectively, the first and last\nentries in the original index (see `pandas.date_range()`). The values\ncorresponding to any timesteps in the new index which were not present in the\noriginal index will be null (`NaN`), unless a method for filling such unknowns\nis provided (see the `method` parameter below).\n\nThe `resample()` method is more appropriate if an operation on each group of\ntimesteps (such as an aggregate) is necessary to represent the data at the new\nfrequency.\n\nFrequency DateOffset or string.\n\nMethod to use for filling holes in reindexed Series (note this does not fill\nNaNs that already were present):\n\n\u2018pad\u2019 / \u2018ffill\u2019: propagate last valid observation forward to next valid\n\n\u2018backfill\u2019 / \u2018bfill\u2019: use NEXT valid observation to fill.\n\nFor PeriodIndex only (see PeriodIndex.asfreq).\n\nWhether to reset output index to midnight.\n\nValue to use for missing values, applied during upsampling (note this does not\nfill NaNs that already were present).\n\nDataFrame object reindexed to the specified frequency.\n\nSee also\n\nConform DataFrame to new index with optional filling logic.\n\nNotes\n\nTo learn more about the frequency strings, please see this link.\n\nExamples\n\nStart by creating a series with 4 one minute timestamps.\n\nUpsample the series into 30 second bins.\n\nUpsample again, providing a `fill value`.\n\nUpsample again, providing a `method`.\n\n"}, {"name": "pandas.DataFrame.asof", "path": "reference/api/pandas.dataframe.asof", "type": "DataFrame", "text": "\nReturn the last row(s) without any NaNs before where.\n\nThe last row (for each element in where, if list) without any NaN is taken. In\ncase of a `DataFrame`, the last row without NaN considering only the subset of\ncolumns (if not None)\n\nIf there is no good value, NaN is returned for a Series or a Series of NaN\nvalues for a DataFrame\n\nDate(s) before which the last row(s) are returned.\n\nFor DataFrame, if not None, only use these columns to check for NaNs.\n\nThe return can be:\n\nscalar : when self is a Series and where is a scalar\n\nSeries: when self is a Series and where is an array-like, or when self is a\nDataFrame and where is a scalar\n\nDataFrame : when self is a DataFrame and where is an array-like\n\nReturn scalar, Series, or DataFrame.\n\nSee also\n\nPerform an asof merge. Similar to left join.\n\nNotes\n\nDates are assumed to be sorted. Raises if this is not the case.\n\nExamples\n\nA Series and a scalar where.\n\nFor a sequence where, a Series is returned. The first value is NaN, because\nthe first element of where is before the first index value.\n\nMissing values are not considered. The following is `2.0`, not NaN, even\nthough NaN is at the index location for `30`.\n\nTake all columns into consideration\n\nTake a single column into consideration\n\n"}, {"name": "pandas.DataFrame.assign", "path": "reference/api/pandas.dataframe.assign", "type": "DataFrame", "text": "\nAssign new columns to a DataFrame.\n\nReturns a new object with all original columns in addition to new ones.\nExisting columns that are re-assigned will be overwritten.\n\nThe column names are keywords. If the values are callable, they are computed\non the DataFrame and assigned to the new columns. The callable must not change\ninput DataFrame (though pandas doesn\u2019t check it). If the values are not\ncallable, (e.g. a Series, scalar, or array), they are simply assigned.\n\nA new DataFrame with the new columns in addition to all the existing columns.\n\nNotes\n\nAssigning multiple columns within the same `assign` is possible. Later items\nin \u2018**kwargs\u2019 may refer to newly created or modified columns in \u2018df\u2019; items\nare computed and assigned into \u2018df\u2019 in order.\n\nExamples\n\nWhere the value is a callable, evaluated on df:\n\nAlternatively, the same behavior can be achieved by directly referencing an\nexisting Series or sequence:\n\nYou can create multiple columns within the same assign where one of the\ncolumns depends on another one defined within the same assign:\n\n"}, {"name": "pandas.DataFrame.astype", "path": "reference/api/pandas.dataframe.astype", "type": "DataFrame", "text": "\nCast a pandas object to a specified dtype `dtype`.\n\nUse a numpy.dtype or Python type to cast entire pandas object to the same\ntype. Alternatively, use {col: dtype, \u2026}, where col is a column label and\ndtype is a numpy.dtype or Python type to cast one or more of the DataFrame\u2019s\ncolumns to column-specific types.\n\nReturn a copy when `copy=True` (be very careful setting `copy=False` as\nchanges to values then may propagate to other pandas objects).\n\nControl raising of exceptions on invalid data for provided dtype.\n\n`raise` : allow exceptions to be raised\n\n`ignore` : suppress exceptions. On error return original object.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nCast a numpy array to a specified type.\n\nNotes\n\nDeprecated since version 1.3.0: Using `astype` to convert from timezone-naive\ndtype to timezone-aware dtype is deprecated and will raise in a future\nversion. Use `Series.dt.tz_localize()` instead.\n\nExamples\n\nCreate a DataFrame:\n\nCast all columns to int32:\n\nCast col1 to int32 using a dictionary:\n\nCreate a series:\n\nConvert to categorical type:\n\nConvert to ordered categorical type with custom ordering:\n\nNote that using `copy=False` and changing data on a new pandas object may\npropagate changes:\n\nCreate a series of dates:\n\n"}, {"name": "pandas.DataFrame.at", "path": "reference/api/pandas.dataframe.at", "type": "DataFrame", "text": "\nAccess a single value for a row/column label pair.\n\nSimilar to `loc`, in that both provide label-based lookups. Use `at` if you\nonly need to get or set a single value in a DataFrame or Series.\n\nIf \u2018label\u2019 does not exist in DataFrame.\n\nSee also\n\nAccess a single value for a row/column pair by integer position.\n\nAccess a group of rows and columns by label(s).\n\nAccess a single value using a label.\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a Series\n\n"}, {"name": "pandas.DataFrame.at_time", "path": "reference/api/pandas.dataframe.at_time", "type": "DataFrame", "text": "\nSelect values at particular time of day (e.g., 9:30AM).\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values between particular times of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values at particular time of the day.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.attrs", "path": "reference/api/pandas.dataframe.attrs", "type": "DataFrame", "text": "\nDictionary of global attributes of this dataset.\n\nWarning\n\nattrs is experimental and may change without warning.\n\nSee also\n\nGlobal flags applying to this object.\n\n"}, {"name": "pandas.DataFrame.axes", "path": "reference/api/pandas.dataframe.axes", "type": "DataFrame", "text": "\nReturn a list representing the axes of the DataFrame.\n\nIt has the row axis labels and column axis labels as the only members. They\nare returned in that order.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.backfill", "path": "reference/api/pandas.dataframe.backfill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.between_time", "path": "reference/api/pandas.dataframe.between_time", "type": "DataFrame", "text": "\nSelect values between particular times of the day (e.g., 9:00-9:30 AM).\n\nBy setting `start_time` to be later than `end_time`, you can get the times\nthat are not between the two times.\n\nInitial time as a time filter limit.\n\nEnd time as a time filter limit.\n\nWhether the start time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nWhether the end time needs to be included in the result.\n\nDeprecated since version 1.4.0: Arguments include_start and include_end have\nbeen deprecated to standardize boundary inputs. Use inclusive instead, to set\neach bound as closed or open.\n\nInclude boundaries; whether to set each bound as closed or open.\n\nDetermine range time on index or columns value.\n\nData from the original object filtered to the specified dates range.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect values at a particular time of the day.\n\nSelect initial periods of time series based on a date offset.\n\nSelect final periods of time series based on a date offset.\n\nGet just the index locations for values between particular times of the day.\n\nExamples\n\nYou get the times that are not between two times by setting `start_time` later\nthan `end_time`:\n\n"}, {"name": "pandas.DataFrame.bfill", "path": "reference/api/pandas.dataframe.bfill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='bfill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.bool", "path": "reference/api/pandas.dataframe.bool", "type": "DataFrame", "text": "\nReturn the bool of a single element Series or DataFrame.\n\nThis must be a boolean scalar value, either True or False. It will raise a\nValueError if the Series or DataFrame does not have exactly 1 element, or that\nelement is not boolean (integer values 0 and 1 will also raise an exception).\n\nThe value in the Series or DataFrame.\n\nSee also\n\nChange the data type of a Series, including to boolean.\n\nChange the data type of a DataFrame, including to boolean.\n\nNumPy boolean data type, used by pandas for boolean values.\n\nExamples\n\nThe method will only work for single element objects with a boolean value:\n\n"}, {"name": "pandas.DataFrame.boxplot", "path": "reference/api/pandas.dataframe.boxplot", "type": "DataFrame", "text": "\nMake a box plot from DataFrame columns.\n\nMake a box-and-whisker plot from DataFrame columns, optionally grouped by some\nother columns. A box plot is a method for graphically depicting groups of\nnumerical data through their quartiles. The box extends from the Q1 to Q3\nquartile values of the data, with a line at the median (Q2). The whiskers\nextend from the edges of box to show the range of the data. By default, they\nextend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box,\nending at the farthest data point within that interval. Outliers are plotted\nas separate dots.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nColumn name or list of names, or vector. Can be any valid input to\n`pandas.DataFrame.groupby()`.\n\nColumn in the DataFrame to `pandas.DataFrame.groupby()`. One box-plot will be\ndone per value of columns in by.\n\nThe matplotlib axes to be used by boxplot.\n\nTick label font size in points or as a string (e.g., large).\n\nThe rotation angle of labels (in degrees) with respect to the screen\ncoordinate system.\n\nSetting this to True will show the grid.\n\nThe size of the figure to create in matplotlib.\n\nFor example, (3, 5) will display the subplots using 3 columns and 5 rows,\nstarting from the top-left.\n\nThe kind of object to return. The default is `axes`.\n\n\u2018axes\u2019 returns the matplotlib axes the boxplot is drawn on.\n\n\u2018dict\u2019 returns a dictionary whose values are the matplotlib Lines of the\nboxplot.\n\n\u2018both\u2019 returns a namedtuple with the axes and dict.\n\nwhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned.\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.boxplot()`.\n\nSee Notes.\n\nSee also\n\nMake a histogram.\n\nMatplotlib equivalent plot.\n\nNotes\n\nThe return type depends on the return_type parameter:\n\n\u2018axes\u2019 : object of class matplotlib.axes.Axes\n\n\u2018dict\u2019 : dict of matplotlib.lines.Line2D objects\n\n\u2018both\u2019 : a namedtuple with structure (ax, lines)\n\nFor data grouped with `by`, return a Series of the above or a numpy array:\n\n`Series`\n\n`array` (for `return_type = None`)\n\nUse `return_type='dict'` when you want to tweak the appearance of the lines\nafter plotting. In this case a dict containing the Lines making up the boxes,\ncaps, fliers, medians, and whiskers is returned.\n\nExamples\n\nBoxplots can be created for every column in the dataframe by `df.boxplot()` or\nindicating the columns to be used:\n\nBoxplots of variables distributions grouped by the values of a third variable\ncan be created using the option `by`. For instance:\n\nA list of strings (i.e. `['X', 'Y']`) can be passed to boxplot in order to\ngroup the data by combination of the variables in the x-axis:\n\nThe layout of boxplot can be adjusted giving a tuple to `layout`:\n\nAdditional formatting can be done to the boxplot, like suppressing the grid\n(`grid=False`), rotating the labels in the x-axis (i.e. `rot=45`) or changing\nthe fontsize (i.e. `fontsize=15`):\n\nThe parameter `return_type` can be used to select the type of element returned\nby boxplot. When `return_type='axes'` is selected, the matplotlib axes on\nwhich the boxplot is drawn are returned:\n\nWhen grouping with `by`, a Series mapping columns to `return_type` is\nreturned:\n\nIf `return_type` is None, a NumPy array of axes with the same shape as\n`layout` is returned:\n\n"}, {"name": "pandas.DataFrame.clip", "path": "reference/api/pandas.dataframe.clip", "type": "DataFrame", "text": "\nTrim values at input threshold(s).\n\nAssigns values outside boundary to boundary values. Thresholds can be singular\nvalues or array like, and in the latter case the clipping is performed\nelement-wise in the specified axis.\n\nMinimum threshold value. All values below this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nMaximum threshold value. All values above this threshold will be set to it. A\nmissing threshold (e.g NA) will not clip the value.\n\nAlign object with lower and upper along the given axis.\n\nWhether to perform the operation in place on the data.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nSame type as calling object with the values outside the clip boundaries\nreplaced or None if `inplace=True`.\n\nSee also\n\nTrim values at input threshold in series.\n\nTrim values at input threshold in dataframe.\n\nClip (limit) the values in an array.\n\nExamples\n\nClips per column using lower and upper thresholds:\n\nClips using specific lower and upper thresholds per column element:\n\nClips using specific lower threshold per column element, with missing values:\n\n"}, {"name": "pandas.DataFrame.columns", "path": "reference/api/pandas.dataframe.columns", "type": "DataFrame", "text": "\nThe column labels of the DataFrame.\n\n"}, {"name": "pandas.DataFrame.combine", "path": "reference/api/pandas.dataframe.combine", "type": "DataFrame", "text": "\nPerform column-wise combine with another DataFrame.\n\nCombines a DataFrame with other DataFrame using func to element-wise combine\ncolumns. The row and column indexes of the resulting DataFrame will be the\nunion of the two.\n\nThe DataFrame to merge column-wise.\n\nFunction that takes two series as inputs and return a Series or a scalar. Used\nto merge the two dataframes column by columns.\n\nThe value to fill NaNs with prior to passing any column to the merge func.\n\nIf True, columns in self that do not exist in other will be overwritten with\nNaNs.\n\nCombination of the provided DataFrames.\n\nSee also\n\nCombine two DataFrame objects and default to non-null values in frame calling\nthe method.\n\nExamples\n\nCombine using a simple function that chooses the smaller column.\n\nExample using a true element-wise combine function.\n\nUsing fill_value fills Nones prior to passing the column to the merge\nfunction.\n\nHowever, if the same element in both dataframes is None, that None is\npreserved\n\nExample that demonstrates the use of overwrite and behavior when the axis\ndiffer between the dataframes.\n\nDemonstrating the preference of the passed in dataframe.\n\n"}, {"name": "pandas.DataFrame.combine_first", "path": "reference/api/pandas.dataframe.combine_first", "type": "DataFrame", "text": "\nUpdate null elements with value in the same location in other.\n\nCombine two DataFrame objects by filling null values in one DataFrame with\nnon-null values from other DataFrame. The row and column indexes of the\nresulting DataFrame will be the union of the two.\n\nProvided DataFrame to use to fill null values.\n\nThe result of combining the provided DataFrame with the other object.\n\nSee also\n\nPerform series-wise operation on two DataFrames using a given function.\n\nExamples\n\nNull values still persist if the location of that null value does not exist in\nother\n\n"}, {"name": "pandas.DataFrame.compare", "path": "reference/api/pandas.dataframe.compare", "type": "DataFrame", "text": "\nCompare to another DataFrame and show the differences.\n\nNew in version 1.1.0.\n\nObject to compare with.\n\nDetermine which axis to align the comparison on.\n\nwith rows drawn alternately from self and other.\n\nwith columns drawn alternately from self and other.\n\nIf true, all rows and columns are kept. Otherwise, only the ones with\ndifferent values are kept.\n\nIf true, the result keeps values that are equal. Otherwise, equal values are\nshown as NaNs.\n\nDataFrame that shows the differences stacked side by side.\n\nThe resulting index will be a MultiIndex with \u2018self\u2019 and \u2018other\u2019 stacked\nalternately at the inner level.\n\nWhen the two DataFrames don\u2019t have identical labels or shape.\n\nSee also\n\nCompare with another Series and show differences.\n\nTest whether two objects contain the same elements.\n\nNotes\n\nMatching NaNs will not appear as a difference.\n\nCan only compare identically-labeled (i.e. same shape, identical row and\ncolumn labels) DataFrames\n\nExamples\n\nAlign the differences on columns\n\nStack the differences on rows\n\nKeep the equal values\n\nKeep all original rows and columns\n\nKeep all original rows and columns and also all original values\n\n"}, {"name": "pandas.DataFrame.convert_dtypes", "path": "reference/api/pandas.dataframe.convert_dtypes", "type": "General utility functions", "text": "\nConvert columns to best possible dtypes using dtypes supporting `pd.NA`.\n\nNew in version 1.0.0.\n\nWhether object dtypes should be converted to the best possible types.\n\nWhether object dtypes should be converted to `StringDtype()`.\n\nWhether, if possible, conversion can be done to integer extension types.\n\nWhether object dtypes should be converted to `BooleanDtypes()`.\n\nWhether, if possible, conversion can be done to floating extension types. If\nconvert_integer is also True, preference will be give to integer dtypes if the\nfloats can be faithfully casted to integers.\n\nNew in version 1.2.0.\n\nCopy of input object with new dtype.\n\nSee also\n\nInfer dtypes of objects.\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to a numeric type.\n\nNotes\n\nBy default, `convert_dtypes` will attempt to convert a Series (or each Series\nin a DataFrame) to dtypes that support `pd.NA`. By using the options\n`convert_string`, `convert_integer`, `convert_boolean` and `convert_boolean`,\nit is possible to turn off individual conversions to `StringDtype`, the\ninteger extension types, `BooleanDtype` or floating extension types,\nrespectively.\n\nFor object-dtyped columns, if `infer_objects` is `True`, use the inference\nrules as during normal Series/DataFrame construction. Then, if possible,\nconvert to `StringDtype`, `BooleanDtype` or an appropriate integer or floating\nextension type, otherwise leave as `object`.\n\nIf the dtype is integer, convert to an appropriate integer extension type.\n\nIf the dtype is numeric, and consists of all integers, convert to an\nappropriate integer extension type. Otherwise, convert to an appropriate\nfloating extension type.\n\nChanged in version 1.2: Starting with pandas 1.2, this method also converts\nfloat columns to the nullable floating extension type.\n\nIn the future, as new dtypes are added that support `pd.NA`, the results of\nthis method will change to support those new dtypes.\n\nExamples\n\nStart with a DataFrame with default dtypes.\n\nConvert the DataFrame to use best possible dtypes.\n\nStart with a Series of strings and missing data represented by `np.nan`.\n\nObtain a Series with dtype `StringDtype`.\n\n"}, {"name": "pandas.DataFrame.copy", "path": "reference/api/pandas.dataframe.copy", "type": "DataFrame", "text": "\nMake a copy of this object\u2019s indices and data.\n\nWhen `deep=True` (default), a new object will be created with a copy of the\ncalling object\u2019s data and indices. Modifications to the data or indices of the\ncopy will not be reflected in the original object (see notes below).\n\nWhen `deep=False`, a new object will be created without copying the calling\nobject\u2019s data or index (only references to the data and index are copied). Any\nchanges to the data of the original will be reflected in the shallow copy (and\nvice versa).\n\nMake a deep copy, including a copy of the data and the indices. With\n`deep=False` neither the indices nor the data are copied.\n\nObject type matches caller.\n\nNotes\n\nWhen `deep=True`, data is copied but actual Python objects will not be copied\nrecursively, only the reference to the object. This is in contrast to\ncopy.deepcopy in the Standard Library, which recursively copies object data\n(see examples below).\n\nWhile `Index` objects are copied when `deep=True`, the underlying numpy array\nis not copied for performance reasons. Since `Index` is immutable, the\nunderlying data can be safely shared and a copy is not needed.\n\nExamples\n\nShallow copy versus default (deep) copy:\n\nShallow copy shares data and index with original.\n\nDeep copy has own copy of data and index.\n\nUpdates to the data shared by shallow copy and original is reflected in both;\ndeep copy remains unchanged.\n\nNote that when copying an object containing Python objects, a deep copy will\ncopy the data, but will not do so recursively. Updating a nested data object\nwill be reflected in the deep copy.\n\n"}, {"name": "pandas.DataFrame.corr", "path": "reference/api/pandas.dataframe.corr", "type": "DataFrame", "text": "\nCompute pairwise correlation of columns, excluding NA/null values.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float. Note that the returned matrix from corr will have 1\nalong the diagonals and will be symmetric regardless of the callable\u2019s\nbehavior.\n\nMinimum number of observations required per pair of columns to have a valid\nresult. Currently only available for Pearson and Spearman correlation.\n\nCorrelation matrix.\n\nSee also\n\nCompute pairwise correlation with another DataFrame or Series.\n\nCompute the correlation between two Series.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.corrwith", "path": "reference/api/pandas.dataframe.corrwith", "type": "DataFrame", "text": "\nCompute pairwise correlation.\n\nPairwise correlation is computed between rows or columns of DataFrame with\nrows or columns of Series or DataFrame. DataFrames are first aligned along\nboth axes before computing the correlations.\n\nObject with which to compute correlations.\n\nThe axis to use. 0 or \u2018index\u2019 to compute column-wise, 1 or \u2018columns\u2019 for row-\nwise.\n\nDrop missing indices from result.\n\nMethod of correlation:\n\npearson : standard correlation coefficient\n\nkendall : Kendall Tau correlation coefficient\n\nspearman : Spearman rank correlation\n\nand returning a float.\n\nPairwise correlations.\n\nSee also\n\nCompute pairwise correlation of columns.\n\n"}, {"name": "pandas.DataFrame.count", "path": "reference/api/pandas.dataframe.count", "type": "DataFrame", "text": "\nCount non-NA cells for each column or row.\n\nThe values None, NaN, NaT, and optionally numpy.inf (depending on\npandas.options.mode.use_inf_as_na) are considered NA.\n\nIf 0 or \u2018index\u2019 counts are generated for each column. If 1 or \u2018columns\u2019 counts\nare generated for each row.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a DataFrame. A str specifies the level name.\n\nInclude only float, int or boolean data.\n\nFor each column/row the number of non-NA/null entries. If level is specified\nreturns a DataFrame.\n\nSee also\n\nNumber of non-NA elements in a Series.\n\nCount unique combinations of columns.\n\nNumber of DataFrame rows and columns (including NA elements).\n\nBoolean same-sized DataFrame showing places of NA elements.\n\nExamples\n\nConstructing DataFrame from a dictionary:\n\nNotice the uncounted NA values:\n\nCounts for each row:\n\n"}, {"name": "pandas.DataFrame.cov", "path": "reference/api/pandas.dataframe.cov", "type": "DataFrame", "text": "\nCompute pairwise covariance of columns, excluding NA/null values.\n\nCompute the pairwise covariance among the series of a DataFrame. The returned\ndata frame is the covariance matrix of the columns of the DataFrame.\n\nBoth NA and null values are automatically excluded from the calculation. (See\nthe note below about bias from missing values.) A threshold can be set for the\nminimum number of observations for each value created. Comparisons with\nobservations below this threshold will be returned as `NaN`.\n\nThis method is generally used for the analysis of time series data to\nunderstand the relationship between different measures across time.\n\nMinimum number of observations required per pair of columns to have a valid\nresult.\n\nDelta degrees of freedom. The divisor used in calculations is `N - ddof`,\nwhere `N` represents the number of elements.\n\nNew in version 1.1.0.\n\nThe covariance matrix of the series of the DataFrame.\n\nSee also\n\nCompute covariance with another Series.\n\nExponential weighted sample covariance.\n\nExpanding sample covariance.\n\nRolling sample covariance.\n\nNotes\n\nReturns the covariance matrix of the DataFrame\u2019s time series. The covariance\nis normalized by N-ddof.\n\nFor DataFrames that have Series that are missing data (assuming that data is\nmissing at random) the returned covariance matrix will be an unbiased estimate\nof the variance and covariance between the member Series.\n\nHowever, for many applications this estimate may not be acceptable because the\nestimate covariance matrix is not guaranteed to be positive semi-definite.\nThis could lead to estimate correlations having absolute values which are\ngreater than one, and/or a non-invertible covariance matrix. See Estimation of\ncovariance matrices for more details.\n\nExamples\n\nMinimum number of periods\n\nThis method also supports an optional `min_periods` keyword that specifies the\nrequired minimum number of non-NA observations for each column pair in order\nto have a valid result:\n\n"}, {"name": "pandas.DataFrame.cummax", "path": "reference/api/pandas.dataframe.cummax", "type": "DataFrame", "text": "\nReturn cumulative maximum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nmaximum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative maximum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the maximum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the maximum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the maximum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cummin", "path": "reference/api/pandas.dataframe.cummin", "type": "DataFrame", "text": "\nReturn cumulative minimum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nminimum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative minimum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the minimum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the minimum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the minimum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cumprod", "path": "reference/api/pandas.dataframe.cumprod", "type": "DataFrame", "text": "\nReturn cumulative product over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative\nproduct.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative product of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the product over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the product in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the product in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.cumsum", "path": "reference/api/pandas.dataframe.cumsum", "type": "DataFrame", "text": "\nReturn cumulative sum over a DataFrame or Series axis.\n\nReturns a DataFrame or Series of the same size containing the cumulative sum.\n\nThe index or the name of the axis. 0 is equivalent to None or \u2018index\u2019.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith NumPy.\n\nReturn cumulative sum of Series or DataFrame.\n\nSee also\n\nSimilar functionality but ignores `NaN` values.\n\nReturn the sum over DataFrame axis.\n\nReturn cumulative maximum over DataFrame axis.\n\nReturn cumulative minimum over DataFrame axis.\n\nReturn cumulative sum over DataFrame axis.\n\nReturn cumulative product over DataFrame axis.\n\nExamples\n\nSeries\n\nBy default, NA values are ignored.\n\nTo include NA values in the operation, use `skipna=False`\n\nDataFrame\n\nBy default, iterates over rows and finds the sum in each column. This is\nequivalent to `axis=None` or `axis='index'`.\n\nTo iterate over columns and find the sum in each row, use `axis=1`\n\n"}, {"name": "pandas.DataFrame.describe", "path": "reference/api/pandas.dataframe.describe", "type": "DataFrame", "text": "\nGenerate descriptive statistics.\n\nDescriptive statistics include those that summarize the central tendency,\ndispersion and shape of a dataset\u2019s distribution, excluding `NaN` values.\n\nAnalyzes both numeric and object series, as well as `DataFrame` column sets of\nmixed data types. The output will vary depending on what is provided. Refer to\nthe notes below for more detail.\n\nThe percentiles to include in the output. All should fall between 0 and 1. The\ndefault is `[.25, .5, .75]`, which returns the 25th, 50th, and 75th\npercentiles.\n\nA white list of data types to include in the result. Ignored for `Series`.\nHere are the options:\n\n\u2018all\u2019 : All columns of the input will be included in the output.\n\nA list-like of dtypes : Limits the results to the provided data types. To\nlimit the result to numeric types submit `numpy.number`. To limit it instead\nto object columns submit the `numpy.object` data type. Strings can also be\nused in the style of `select_dtypes` (e.g. `df.describe(include=['O'])`). To\nselect pandas categorical columns, use `'category'`\n\nNone (default) : The result will include all numeric columns.\n\nA black list of data types to omit from the result. Ignored for `Series`. Here\nare the options:\n\nA list-like of dtypes : Excludes the provided data types from the result. To\nexclude numeric types submit `numpy.number`. To exclude object columns submit\nthe data type `numpy.object`. Strings can also be used in the style of\n`select_dtypes` (e.g. `df.describe(exclude=['O'])`). To exclude pandas\ncategorical columns, use `'category'`\n\nNone (default) : The result will exclude nothing.\n\nWhether to treat datetime dtypes as numeric. This affects statistics\ncalculated for the column. For DataFrame input, this also controls whether\ndatetime columns are included by default.\n\nNew in version 1.1.0.\n\nSummary statistics of the Series or Dataframe provided.\n\nSee also\n\nCount number of non-NA/null observations.\n\nMaximum of the values in the object.\n\nMinimum of the values in the object.\n\nMean of the values.\n\nStandard deviation of the observations.\n\nSubset of a DataFrame including/excluding columns based on their dtype.\n\nNotes\n\nFor numeric data, the result\u2019s index will include `count`, `mean`, `std`,\n`min`, `max` as well as lower, `50` and upper percentiles. By default the\nlower percentile is `25` and the upper percentile is `75`. The `50` percentile\nis the same as the median.\n\nFor object data (e.g. strings or timestamps), the result\u2019s index will include\n`count`, `unique`, `top`, and `freq`. The `top` is the most common value. The\n`freq` is the most common value\u2019s frequency. Timestamps also include the\n`first` and `last` items.\n\nIf multiple object values have the highest count, then the `count` and `top`\nresults will be arbitrarily chosen from among those with the highest count.\n\nFor mixed data types provided via a `DataFrame`, the default is to return only\nan analysis of numeric columns. If the dataframe consists only of object and\ncategorical data without any numeric columns, the default is to return an\nanalysis of both the object and categorical columns. If `include='all'` is\nprovided as an option, the result will include a union of attributes of each\ntype.\n\nThe include and exclude parameters can be used to limit which columns in a\n`DataFrame` are analyzed for the output. The parameters are ignored when\nanalyzing a `Series`.\n\nExamples\n\nDescribing a numeric `Series`.\n\nDescribing a categorical `Series`.\n\nDescribing a timestamp `Series`.\n\nDescribing a `DataFrame`. By default only numeric fields are returned.\n\nDescribing all columns of a `DataFrame` regardless of data type.\n\nDescribing a column from a `DataFrame` by accessing it as an attribute.\n\nIncluding only numeric columns in a `DataFrame` description.\n\nIncluding only string columns in a `DataFrame` description.\n\nIncluding only categorical columns from a `DataFrame` description.\n\nExcluding numeric columns from a `DataFrame` description.\n\nExcluding object columns from a `DataFrame` description.\n\n"}, {"name": "pandas.DataFrame.diff", "path": "reference/api/pandas.dataframe.diff", "type": "DataFrame", "text": "\nFirst discrete difference of element.\n\nCalculates the difference of a Dataframe element compared with another element\nin the Dataframe (default is element in previous row).\n\nPeriods to shift for calculating difference, accepts negative values.\n\nTake difference over rows (0) or columns (1).\n\nFirst differences of the Series.\n\nSee also\n\nPercent change over given number of periods.\n\nShift index by desired number of periods with an optional time freq.\n\nFirst discrete difference of object.\n\nNotes\n\nFor boolean dtypes, this uses `operator.xor()` rather than `operator.sub()`.\nThe result is calculated according to current dtype in Dataframe, however\ndtype of the result is always float64.\n\nExamples\n\nDifference with previous row\n\nDifference with previous column\n\nDifference with 3rd previous row\n\nDifference with following row\n\nOverflow in input dtype\n\n"}, {"name": "pandas.DataFrame.div", "path": "reference/api/pandas.dataframe.div", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `dataframe / other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rtruediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.divide", "path": "reference/api/pandas.dataframe.divide", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\ntruediv).\n\nEquivalent to `dataframe / other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rtruediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.dot", "path": "reference/api/pandas.dataframe.dot", "type": "DataFrame", "text": "\nCompute the matrix multiplication between the DataFrame and other.\n\nThis method computes the matrix product between the DataFrame and the values\nof an other Series, DataFrame or a numpy array.\n\nIt can also be called using `self @ other` in Python >= 3.5.\n\nThe other object to compute the matrix product with.\n\nIf other is a Series, return the matrix product between self and other as a\nSeries. If other is a DataFrame or a numpy.array, return the matrix product of\nself and other in a DataFrame of a np.array.\n\nSee also\n\nSimilar method for Series.\n\nNotes\n\nThe dimensions of DataFrame and other must be compatible in order to compute\nthe matrix multiplication. In addition, the column names of DataFrame and the\nindex of other must contain the same values, as they will be aligned prior to\nthe multiplication.\n\nThe dot method for Series computes the inner product, instead of the matrix\nproduct here.\n\nExamples\n\nHere we multiply a DataFrame with a Series.\n\nHere we multiply a DataFrame with another DataFrame.\n\nNote that the dot method give the same result as @\n\nThe dot method works also if other is an np.array.\n\nNote how shuffling of the objects does not change the result.\n\n"}, {"name": "pandas.DataFrame.drop", "path": "reference/api/pandas.dataframe.drop", "type": "DataFrame", "text": "\nDrop specified labels from rows or columns.\n\nRemove rows or columns by specifying label names and corresponding axis, or by\nspecifying directly index or column names. When using a multi-index, labels on\ndifferent levels can be removed by specifying the level. See the user guide\n<advanced.shown_levels> for more information about the now unused levels.\n\nIndex or column labels to drop. A tuple will be used as a single label and not\ntreated as a list-like.\n\nWhether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or\n\u2018columns\u2019).\n\nAlternative to specifying axis (`labels, axis=0` is equivalent to\n`index=labels`).\n\nAlternative to specifying axis (`labels, axis=1` is equivalent to\n`columns=labels`).\n\nFor MultiIndex, level from which the labels will be removed.\n\nIf False, return a copy. Otherwise, do operation inplace and return None.\n\nIf \u2018ignore\u2019, suppress error and only existing labels are dropped.\n\nDataFrame without the removed index or column labels or None if\n`inplace=True`.\n\nIf any of the labels is not found in the selected axis.\n\nSee also\n\nLabel-location based indexer for selection by label.\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are\nmissing.\n\nReturn DataFrame with duplicate rows removed, optionally only considering\ncertain columns.\n\nReturn Series with specified index labels removed.\n\nExamples\n\nDrop columns\n\nDrop a row by index\n\nDrop columns and/or rows of MultiIndex DataFrame\n\nDrop a specific index combination from the MultiIndex DataFrame, i.e., drop\nthe combination `'falcon'` and `'weight'`, which deletes only the\ncorresponding row\n\n"}, {"name": "pandas.DataFrame.drop_duplicates", "path": "reference/api/pandas.dataframe.drop_duplicates", "type": "DataFrame", "text": "\nReturn DataFrame with duplicate rows removed.\n\nConsidering certain columns is optional. Indexes, including time indexes are\nignored.\n\nOnly consider certain columns for identifying duplicates, by default use all\nof the columns.\n\nDetermines which duplicates (if any) to keep. - `first` : Drop duplicates\nexcept for the first occurrence. - `last` : Drop duplicates except for the\nlast occurrence. - False : Drop all duplicates.\n\nWhether to drop duplicates in place or to return a copy.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nDataFrame with duplicates removed or None if `inplace=True`.\n\nSee also\n\nCount unique combinations of columns.\n\nExamples\n\nConsider dataset containing ramen rating.\n\nBy default, it removes duplicate rows based on all columns.\n\nTo remove duplicates on specific column(s), use `subset`.\n\nTo remove duplicates and keep last occurrences, use `keep`.\n\n"}, {"name": "pandas.DataFrame.droplevel", "path": "reference/api/pandas.dataframe.droplevel", "type": "DataFrame", "text": "\nReturn Series/DataFrame with requested index / column level(s) removed.\n\nIf a string is given, must be the name of a level If list-like, elements must\nbe names or positional indexes of levels.\n\nAxis along which the level(s) is removed:\n\n0 or \u2018index\u2019: remove level(s) in column.\n\n1 or \u2018columns\u2019: remove level(s) in row.\n\nSeries/DataFrame with requested index / column level(s) removed.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.dropna", "path": "reference/api/pandas.dataframe.dropna", "type": "DataFrame", "text": "\nRemove missing values.\n\nSee the User Guide for more on which values are considered missing, and how to\nwork with missing data.\n\nDetermine if rows or columns which contain missing values are removed.\n\n0, or \u2018index\u2019 : Drop rows which contain missing values.\n\n1, or \u2018columns\u2019 : Drop columns which contain missing value.\n\nChanged in version 1.0.0: Pass tuple or list to drop on multiple axes. Only a\nsingle axis is allowed.\n\nDetermine if row or column is removed from DataFrame, when we have at least\none NA or all NA.\n\n\u2018any\u2019 : If any NA values are present, drop that row or column.\n\n\u2018all\u2019 : If all values are NA, drop that row or column.\n\nRequire that many non-NA values.\n\nLabels along other axis to consider, e.g. if you are dropping rows these would\nbe a list of columns to include.\n\nIf True, do operation inplace and return None.\n\nDataFrame with NA entries dropped from it or None if `inplace=True`.\n\nSee also\n\nIndicate missing values.\n\nIndicate existing (non-missing) values.\n\nReplace missing values.\n\nDrop missing values.\n\nDrop missing indices.\n\nExamples\n\nDrop the rows where at least one element is missing.\n\nDrop the columns where at least one element is missing.\n\nDrop the rows where all elements are missing.\n\nKeep only the rows with at least 2 non-NA values.\n\nDefine in which columns to look for missing values.\n\nKeep the DataFrame with valid entries in the same variable.\n\n"}, {"name": "pandas.DataFrame.dtypes", "path": "reference/api/pandas.dataframe.dtypes", "type": "General utility functions", "text": "\nReturn the dtypes in the DataFrame.\n\nThis returns a Series with the data type of each column. The result\u2019s index is\nthe original DataFrame\u2019s columns. Columns with mixed types are stored with the\n`object` dtype. See the User Guide for more.\n\nThe data type of each column.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.duplicated", "path": "reference/api/pandas.dataframe.duplicated", "type": "DataFrame", "text": "\nReturn boolean Series denoting duplicate rows.\n\nConsidering certain columns is optional.\n\nOnly consider certain columns for identifying duplicates, by default use all\nof the columns.\n\nDetermines which duplicates (if any) to mark.\n\n`first` : Mark duplicates as `True` except for the first occurrence.\n\n`last` : Mark duplicates as `True` except for the last occurrence.\n\nFalse : Mark all duplicates as `True`.\n\nBoolean series for each duplicated rows.\n\nSee also\n\nEquivalent method on index.\n\nEquivalent method on Series.\n\nRemove duplicate values from Series.\n\nRemove duplicate values from DataFrame.\n\nExamples\n\nConsider dataset containing ramen rating.\n\nBy default, for each set of duplicated values, the first occurrence is set on\nFalse and all others on True.\n\nBy using \u2018last\u2019, the last occurrence of each set of duplicated values is set\non False and all others on True.\n\nBy setting `keep` on False, all duplicates are True.\n\nTo find duplicates on specific column(s), use `subset`.\n\n"}, {"name": "pandas.DataFrame.empty", "path": "reference/api/pandas.dataframe.empty", "type": "DataFrame", "text": "\nIndicator whether Series/DataFrame is empty.\n\nTrue if Series/DataFrame is entirely empty (no items), meaning any of the axes\nare of length 0.\n\nIf Series/DataFrame is empty, return True, if not return False.\n\nSee also\n\nReturn series without null values.\n\nReturn DataFrame with labels on given axis omitted where (all or any) data are\nmissing.\n\nNotes\n\nIf Series/DataFrame contains only NaNs, it is still not considered empty. See\nthe example below.\n\nExamples\n\nAn example of an actual empty DataFrame. Notice the index is empty:\n\nIf we only have NaNs in our DataFrame, it is not considered empty! We will\nneed to drop the NaNs to make the DataFrame empty:\n\n"}, {"name": "pandas.DataFrame.eq", "path": "reference/api/pandas.dataframe.eq", "type": "DataFrame", "text": "\nGet Equal to of dataframe and other, element-wise (binary operator eq).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.equals", "path": "reference/api/pandas.dataframe.equals", "type": "DataFrame", "text": "\nTest whether two objects contain the same elements.\n\nThis function allows two Series or DataFrames to be compared against each\nother to see if they have the same shape and elements. NaNs in the same\nlocation are considered equal.\n\nThe row/column index do not need to have the same type, as long as the values\nare considered equal. Corresponding columns must be of the same dtype.\n\nThe other Series or DataFrame to be compared with the first.\n\nTrue if all elements are the same in both objects, False otherwise.\n\nSee also\n\nCompare two Series objects of the same length and return a Series where each\nelement is True if the element in each Series is equal, False otherwise.\n\nCompare two DataFrame objects of the same shape and return a DataFrame where\neach element is True if the respective element in each DataFrame is equal,\nFalse otherwise.\n\nRaises an AssertionError if left and right are not equal. Provides an easy\ninterface to ignore inequality in dtypes, indexes and precision among others.\n\nLike assert_series_equal, but targets DataFrames.\n\nReturn True if two arrays have the same shape and elements, False otherwise.\n\nExamples\n\nDataFrames df and exactly_equal have the same types and values for their\nelements and column labels, which will return True.\n\nDataFrames df and different_column_type have the same element types and\nvalues, but have different types for the column labels, which will still\nreturn True.\n\nDataFrames df and different_data_type have different types for the same values\nfor their elements, and will return False even though their column labels are\nthe same values and types.\n\n"}, {"name": "pandas.DataFrame.eval", "path": "reference/api/pandas.dataframe.eval", "type": "DataFrame", "text": "\nEvaluate a string describing operations on DataFrame columns.\n\nOperates on columns only, not specific rows or elements. This allows eval to\nrun arbitrary code, which can make you vulnerable to code injection if you\npass user input to this function.\n\nThe expression string to evaluate.\n\nIf the expression contains an assignment, whether to perform the operation\ninplace and mutate the existing DataFrame. Otherwise, a new DataFrame is\nreturned.\n\nSee the documentation for `eval()` for complete details on the keyword\narguments accepted by `query()`.\n\nThe result of the evaluation or None if `inplace=True`.\n\nSee also\n\nEvaluates a boolean expression to query the columns of a frame.\n\nCan evaluate an expression or function to create new values for a column.\n\nEvaluate a Python expression as a string using various backends.\n\nNotes\n\nFor more details see the API documentation for `eval()`. For detailed examples\nsee enhancing performance with eval.\n\nExamples\n\nAssignment is allowed though by default the original DataFrame is not\nmodified.\n\nUse `inplace=True` to modify the original DataFrame.\n\nMultiple columns can be assigned to using multi-line expressions:\n\n"}, {"name": "pandas.DataFrame.ewm", "path": "reference/api/pandas.dataframe.ewm", "type": "DataFrame", "text": "\nProvide exponentially weighted (EW) calculations.\n\nExactly one parameter: `com`, `span`, `halflife`, or `alpha` must be provided.\n\nSpecify decay in terms of center of mass\n\n\\\\(\\alpha = 1 / (1 + com)\\\\), for \\\\(com \\geq 0\\\\).\n\nSpecify decay in terms of span\n\n\\\\(\\alpha = 2 / (span + 1)\\\\), for \\\\(span \\geq 1\\\\).\n\nSpecify decay in terms of half-life\n\n\\\\(\\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right)\\\\), for \\\\(halflife >\n0\\\\).\n\nIf `times` is specified, the time unit (str or timedelta) over which an\nobservation decays to half its value. Only applicable to `mean()`, and\nhalflife value will not apply to the other functions.\n\nNew in version 1.1.0.\n\nSpecify smoothing factor \\\\(\\alpha\\\\) directly\n\n\\\\(0 < \\alpha \\leq 1\\\\).\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nDivide by decaying adjustment factor in beginning periods to account for\nimbalance in relative weightings (viewing EWMA as a moving average).\n\nWhen `adjust=True` (default), the EW function is calculated using weights\n\\\\(w_i = (1 - \\alpha)^i\\\\). For example, the EW moving average of the series\n[\\\\(x_0, x_1, ..., x_t\\\\)] would be:\n\nWhen `adjust=False`, the exponentially weighted function is calculated\nrecursively:\n\nIgnore missing values when calculating weights.\n\nWhen `ignore_na=False` (default), weights are based on absolute positions. For\nexample, the weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final\nweighted average of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\((1-\\alpha)^2\\\\) and\n\\\\(1\\\\) if `adjust=True`, and \\\\((1-\\alpha)^2\\\\) and \\\\(\\alpha\\\\) if\n`adjust=False`.\n\nWhen `ignore_na=True`, weights are based on relative positions. For example,\nthe weights of \\\\(x_0\\\\) and \\\\(x_2\\\\) used in calculating the final weighted\naverage of [\\\\(x_0\\\\), None, \\\\(x_2\\\\)] are \\\\(1-\\alpha\\\\) and \\\\(1\\\\) if\n`adjust=True`, and \\\\(1-\\alpha\\\\) and \\\\(\\alpha\\\\) if `adjust=False`.\n\nIf `0` or `'index'`, calculate across the rows.\n\nIf `1` or `'columns'`, calculate across the columns.\n\nNew in version 1.1.0.\n\nOnly applicable to `mean()`.\n\nTimes corresponding to the observations. Must be monotonically increasing and\n`datetime64[ns]` dtype.\n\nIf 1-D array like, a sequence with the same shape as the observations.\n\nDeprecated since version 1.4.0: If str, the name of the column in the\nDataFrame representing the times.\n\nNew in version 1.4.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nOnly applicable to `mean()`\n\nSee also\n\nProvides rolling window calculations.\n\nProvides expanding transformations.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nadjust\n\nignore_na\n\ntimes\n\nExponentially weighted mean with weights calculated with a timedelta\n`halflife` relative to `times`.\n\n"}, {"name": "pandas.DataFrame.expanding", "path": "reference/api/pandas.dataframe.expanding", "type": "DataFrame", "text": "\nProvide expanding window calculations.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nDeprecated since version 1.1.0.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nNew in version 1.3.0.\n\nSee also\n\nProvides rolling window calculations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nmin_periods\n\nExpanding sum with 1 vs 3 observations needed to calculate a value.\n\n"}, {"name": "pandas.DataFrame.explode", "path": "reference/api/pandas.dataframe.explode", "type": "DataFrame", "text": "\nTransform each element of a list-like to a row, replicating index values.\n\nNew in version 0.25.0.\n\nColumn(s) to explode. For multiple columns, specify a non-empty list with each\nelement be str or tuple, and all specified columns their list-like data on\nsame row of the frame must have matching length.\n\nNew in version 1.3.0: Multi-column explode\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.1.0.\n\nExploded lists to rows of the subset columns; index will be duplicated for\nthese rows.\n\nIf columns of the frame are not unique.\n\nIf specified columns to explode is empty list.\n\nIf specified columns to explode have not matching count of elements rowwise in\nthe frame.\n\nSee also\n\nPivot a level of the (necessarily hierarchical) index labels.\n\nUnpivot a DataFrame from wide format to long format.\n\nExplode a DataFrame from list-like columns to long format.\n\nNotes\n\nThis routine will explode list-likes including lists, tuples, sets, Series,\nand np.ndarray. The result dtype of the subset rows will be object. Scalars\nwill be returned unchanged, and empty list-likes will result in a np.nan for\nthat row. In addition, the ordering of rows in the output will be non-\ndeterministic when exploding sets.\n\nExamples\n\nSingle-column explode.\n\nMulti-column explode.\n\n"}, {"name": "pandas.DataFrame.ffill", "path": "reference/api/pandas.dataframe.ffill", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.fillna", "path": "reference/api/pandas.dataframe.fillna", "type": "DataFrame", "text": "\nFill NA/NaN values using the specified method.\n\nValue to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of\nvalues specifying which value to use for each index (for a Series) or column\n(for a DataFrame). Values not in the dict/Series/DataFrame will not be filled.\nThis value cannot be a list.\n\nMethod to use for filling holes in reindexed Series pad / ffill: propagate\nlast valid observation forward to next valid backfill / bfill: use next valid\nobservation to fill gap.\n\nAxis along which to fill missing values.\n\nIf True, fill in-place. Note: this will modify any other views on this object\n(e.g., a no-copy slice for a column in a DataFrame).\n\nIf method is specified, this is the maximum number of consecutive NaN values\nto forward/backward fill. In other words, if there is a gap with more than\nthis number of consecutive NaNs, it will only be partially filled. If method\nis not specified, this is the maximum number of entries along the entire axis\nwhere NaNs will be filled. Must be greater than 0 if not None.\n\nA dict of item->dtype of what to downcast if possible, or the string \u2018infer\u2019\nwhich will try to downcast to an appropriate equal type (e.g. float64 to int64\nif possible).\n\nObject with missing values filled or None if `inplace=True`.\n\nSee also\n\nFill NaN values using interpolation.\n\nConform object to new index.\n\nConvert TimeSeries to specified frequency.\n\nExamples\n\nReplace all NaN elements with 0s.\n\nWe can also propagate non-null values forward or backward.\n\nReplace all NaN elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3\nrespectively.\n\nOnly replace the first NaN element.\n\nWhen filling using a DataFrame, replacement happens along the same column\nnames and same indices\n\nNote that column D is not affected since it is not present in df2.\n\n"}, {"name": "pandas.DataFrame.filter", "path": "reference/api/pandas.dataframe.filter", "type": "DataFrame", "text": "\nSubset the dataframe rows or columns according to the specified index labels.\n\nNote that this routine does not filter a dataframe on its contents. The filter\nis applied to the labels of the index.\n\nKeep labels from axis which are in items.\n\nKeep labels from axis for which \u201clike in label == True\u201d.\n\nKeep labels from axis for which re.search(regex, label) == True.\n\nThe axis to filter on, expressed either as an index (int) or axis name (str).\nBy default this is the info axis, \u2018index\u2019 for Series, \u2018columns\u2019 for DataFrame.\n\nSee also\n\nAccess a group of rows and columns by label(s) or a boolean array.\n\nNotes\n\nThe `items`, `like`, and `regex` parameters are enforced to be mutually\nexclusive.\n\n`axis` defaults to the info axis that is used when indexing with `[]`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.first", "path": "reference/api/pandas.dataframe.first", "type": "DataFrame", "text": "\nSelect initial periods of time series data based on a date offset.\n\nWhen having a DataFrame with dates as index, this function can select the\nfirst few rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20181M\u2019 will\ndisplay all the rows having their index within the first month.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect final periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the first 3 days:\n\nNotice the data for 3 first calendar days were returned, not the first 3 days\nobserved in the dataset, and therefore data for 2018-04-13 was not returned.\n\n"}, {"name": "pandas.DataFrame.first_valid_index", "path": "reference/api/pandas.dataframe.first_valid_index", "type": "DataFrame", "text": "\nReturn index for first non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.DataFrame.flags", "path": "reference/api/pandas.dataframe.flags", "type": "DataFrame", "text": "\nGet the properties associated with this pandas object.\n\nThe available flags are\n\n`Flags.allows_duplicate_labels`\n\nSee also\n\nFlags that apply to pandas objects.\n\nGlobal metadata applying to this dataset.\n\nNotes\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\nFlags can be get or set using `.`\n\nOr by slicing with a key\n\n"}, {"name": "pandas.DataFrame.floordiv", "path": "reference/api/pandas.dataframe.floordiv", "type": "DataFrame", "text": "\nGet Integer division of dataframe and other, element-wise (binary operator\nfloordiv).\n\nEquivalent to `dataframe // other`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version,\nrfloordiv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.from_dict", "path": "reference/api/pandas.dataframe.from_dict", "type": "DataFrame", "text": "\nConstruct DataFrame from dict of array-like or dicts.\n\nCreates DataFrame object from dictionary by columns or by index allowing dtype\nspecification.\n\nOf the form {field : array-like} or {field : dict}.\n\nThe \u201corientation\u201d of the data. If the keys of the passed dict should be the\ncolumns of the resulting DataFrame, pass \u2018columns\u2019 (default). Otherwise if the\nkeys should be rows, pass \u2018index\u2019. If \u2018tight\u2019, assume a dict with keys\n[\u2018index\u2019, \u2018columns\u2019, \u2018data\u2019, \u2018index_names\u2019, \u2018column_names\u2019].\n\nNew in version 1.4.0: \u2018tight\u2019 as an allowed value for the `orient` argument\n\nData type to force, otherwise infer.\n\nColumn labels to use when `orient='index'`. Raises a ValueError if used with\n`orient='columns'` or `orient='tight'`.\n\nSee also\n\nDataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame.\n\nDataFrame object creation using constructor.\n\nConvert the DataFrame to a dictionary.\n\nExamples\n\nBy default the keys of the dict become the DataFrame columns:\n\nSpecify `orient='index'` to create the DataFrame using dictionary keys as\nrows:\n\nWhen using the \u2018index\u2019 orientation, the column names can be specified\nmanually:\n\nSpecify `orient='tight'` to create the DataFrame using a \u2018tight\u2019 format:\n\n"}, {"name": "pandas.DataFrame.from_records", "path": "reference/api/pandas.dataframe.from_records", "type": "DataFrame", "text": "\nConvert structured or record ndarray to DataFrame.\n\nCreates a DataFrame object from a structured ndarray, sequence of tuples or\ndicts, or DataFrame.\n\nStructured input data.\n\nField of array to use as the index, alternately a specific set of input labels\nto use.\n\nColumns or fields to exclude.\n\nColumn names to use. If the passed data do not have names associated with\nthem, this argument provides names for the columns. Otherwise this argument\nindicates the order of the columns in the result (any names not found in the\ndata will become all-NA columns).\n\nAttempt to convert values of non-string, non-numeric objects (like\ndecimal.Decimal) to floating point, useful for SQL result sets.\n\nNumber of rows to read if data is an iterator.\n\nSee also\n\nDataFrame from dict of array-like or dicts.\n\nDataFrame object creation using constructor.\n\nExamples\n\nData can be provided as a structured ndarray:\n\nData can be provided as a list of dicts:\n\nData can be provided as a list of tuples with corresponding columns:\n\n"}, {"name": "pandas.DataFrame.ge", "path": "reference/api/pandas.dataframe.ge", "type": "DataFrame", "text": "\nGet Greater than or equal to of dataframe and other, element-wise (binary\noperator ge).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.get", "path": "reference/api/pandas.dataframe.get", "type": "DataFrame", "text": "\nGet item from object for given key (ex: DataFrame column).\n\nReturns default value if not found.\n\nExamples\n\nIf the key isn\u2019t found, the default value will be used.\n\n"}, {"name": "pandas.DataFrame.groupby", "path": "reference/api/pandas.dataframe.groupby", "type": "GroupBy", "text": "\nGroup DataFrame using a mapper or by a Series of columns.\n\nA groupby operation involves some combination of splitting the object,\napplying a function, and combining the results. This can be used to group\nlarge amounts of data and compute operations on these groups.\n\nUsed to determine the groups for the groupby. If `by` is a function, it\u2019s\ncalled on each value of the object\u2019s index. If a dict or Series is passed, the\nSeries or dict VALUES will be used to determine the groups (the Series\u2019 values\nare first aligned; see `.align()` method). If a list or ndarray of length\nequal to the selected axis is passed (see the groupby user guide), the values\nare used as-is to determine the groups. A label or list of labels may be\npassed to group by the columns in `self`. Notice that a tuple is interpreted\nas a (single) key.\n\nSplit along rows (0) or columns (1).\n\nIf the axis is a MultiIndex (hierarchical), group by a particular level or\nlevels.\n\nFor aggregated output, return object with group labels as the index. Only\nrelevant for DataFrame input. as_index=False is effectively \u201cSQL-style\u201d\ngrouped output.\n\nSort group keys. Get better performance by turning this off. Note this does\nnot influence the order of observations within each group. Groupby preserves\nthe order of rows within each group.\n\nWhen calling apply, add group keys to index to identify pieces.\n\nReduce the dimensionality of the return type if possible, otherwise return a\nconsistent type.\n\nDeprecated since version 1.1.0.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nIf True, and if group keys contain NA values, NA values together with\nrow/column will be dropped. If False, NA values will also be treated as the\nkey in groups.\n\nNew in version 1.1.0.\n\nReturns a groupby object that contains information about the groups.\n\nSee also\n\nConvenience method for frequency conversion and resampling of time series.\n\nNotes\n\nSee the user guide for more detailed usage and examples, including splitting\nan object into groups, iterating through groups, selecting a group,\naggregation, and more.\n\nExamples\n\nHierarchical Indexes\n\nWe can groupby different levels of a hierarchical index using the level\nparameter:\n\nWe can also choose to include NA in group keys or not by setting dropna\nparameter, the default setting is True.\n\n"}, {"name": "pandas.DataFrame.gt", "path": "reference/api/pandas.dataframe.gt", "type": "DataFrame", "text": "\nGet Greater than of dataframe and other, element-wise (binary operator gt).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.head", "path": "reference/api/pandas.dataframe.head", "type": "DataFrame", "text": "\nReturn the first n rows.\n\nThis function returns the first n rows for the object based on position. It is\nuseful for quickly testing if your object has the right type of data in it.\n\nFor negative values of n, this function returns all rows except the last n\nrows, equivalent to `df[:-n]`.\n\nNumber of rows to select.\n\nThe first n rows of the caller object.\n\nSee also\n\nReturns the last n rows.\n\nExamples\n\nViewing the first 5 lines\n\nViewing the first n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.DataFrame.hist", "path": "reference/api/pandas.dataframe.hist", "type": "DataFrame", "text": "\nMake a histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ncalls `matplotlib.pyplot.hist()`, on each series in the DataFrame, resulting\nin one histogram per column.\n\nThe pandas object holding the data.\n\nIf passed, will be used to limit data to a subset of columns.\n\nIf passed, then used to form histograms for separate groups.\n\nWhether to show axis grid lines.\n\nIf specified changes the x-axis label size.\n\nRotation of x axis labels. For example, a value of 90 displays the x labels\nrotated 90 degrees clockwise.\n\nIf specified changes the y-axis label size.\n\nRotation of y axis labels. For example, a value of 90 displays the y labels\nrotated 90 degrees clockwise.\n\nThe axes to plot the histogram on.\n\nIn case subplots=True, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in. Note\nthat passing in both an ax and sharex=True will alter all x axis labels for\nall subplots in a figure.\n\nIn case subplots=True, share y axis and set some y axis labels to invisible.\n\nThe size in inches of the figure to create. Uses the value in\nmatplotlib.rcParams by default.\n\nTuple of (rows, columns) for the layout of the histograms.\n\nNumber of histogram bins to be used. If an integer is given, bins + 1 bin\nedges are calculated and returned. If bins is a sequence, gives bin edges,\nincluding left edge of first bin and right edge of last bin. In this case,\nbins is returned unmodified.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nWhether to show the legend.\n\nNew in version 1.1.0.\n\nAll other plotting keyword arguments to be passed to\n`matplotlib.pyplot.hist()`.\n\nSee also\n\nPlot a histogram using matplotlib.\n\nExamples\n\nThis example draws a histogram based on the length and width of some animals,\ndisplayed in three bins\n\n"}, {"name": "pandas.DataFrame.iat", "path": "reference/api/pandas.dataframe.iat", "type": "DataFrame", "text": "\nAccess a single value for a row/column pair by integer position.\n\nSimilar to `iloc`, in that both provide integer-based lookups. Use `iat` if\nyou only need to get or set a single value in a DataFrame or Series.\n\nWhen integer position is out of bounds.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess a group of rows and columns by label(s).\n\nAccess a group of rows and columns by integer position(s).\n\nExamples\n\nGet value at specified row/column pair\n\nSet value at specified row/column pair\n\nGet value within a series\n\n"}, {"name": "pandas.DataFrame.idxmax", "path": "reference/api/pandas.dataframe.idxmax", "type": "DataFrame", "text": "\nReturn index of first occurrence of maximum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of maxima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the maximum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmax`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the maximum value in each column.\n\nTo return the index for the maximum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.DataFrame.idxmin", "path": "reference/api/pandas.dataframe.idxmin", "type": "DataFrame", "text": "\nReturn index of first occurrence of minimum over requested axis.\n\nNA/null values are excluded.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIndexes of minima along the specified axis.\n\nIf the row/column is empty\n\nSee also\n\nReturn index of the minimum element.\n\nNotes\n\nThis method is the DataFrame version of `ndarray.argmin`.\n\nExamples\n\nConsider a dataset containing food consumption in Argentina.\n\nBy default, it returns the index for the minimum value in each column.\n\nTo return the index for the minimum value in each row, use `axis=\"columns\"`.\n\n"}, {"name": "pandas.DataFrame.iloc", "path": "reference/api/pandas.dataframe.iloc", "type": "DataFrame", "text": "\nPurely integer-location based indexing for selection by position.\n\n`.iloc[]` is primarily integer position based (from `0` to `length-1` of the\naxis), but may also be used with a boolean array.\n\nAllowed inputs are:\n\nAn integer, e.g. `5`.\n\nA list or array of integers, e.g. `[4, 3, 0]`.\n\nA slice object with ints, e.g. `1:7`.\n\nA boolean array.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above). This is useful in\nmethod chains, when you don\u2019t have a reference to the calling object, but\nwould like to base your selection on some value.\n\n`.iloc` will raise `IndexError` if a requested indexer is out-of-bounds,\nexcept slice indexers which allow out-of-bounds indexing (this conforms with\npython/numpy slice semantics).\n\nSee more at Selection by Position.\n\nSee also\n\nFast integer location scalar accessor.\n\nPurely label-location based indexer for selection by label.\n\nPurely integer-location based indexing for selection by position.\n\nExamples\n\nIndexing just the rows\n\nWith a scalar integer.\n\nWith a list of integers.\n\nWith a slice object.\n\nWith a boolean mask the same length as the index.\n\nWith a callable, useful in method chains. The x passed to the `lambda` is the\nDataFrame being sliced. This selects the rows whose index label even.\n\nIndexing both axes\n\nYou can mix the indexer types for the index and columns. Use `:` to select the\nentire axis.\n\nWith scalar integers.\n\nWith lists of integers.\n\nWith slice objects.\n\nWith a boolean array whose length matches the columns.\n\nWith a callable function that expects the Series or DataFrame.\n\n"}, {"name": "pandas.DataFrame.index", "path": "reference/api/pandas.dataframe.index", "type": "DataFrame", "text": "\nThe index (row labels) of the DataFrame.\n\n"}, {"name": "pandas.DataFrame.infer_objects", "path": "reference/api/pandas.dataframe.infer_objects", "type": "DataFrame", "text": "\nAttempt to infer better dtypes for object columns.\n\nAttempts soft conversion of object-dtyped columns, leaving non-object and\nunconvertible columns unchanged. The inference rules are the same as during\nnormal Series/DataFrame construction.\n\nSee also\n\nConvert argument to datetime.\n\nConvert argument to timedelta.\n\nConvert argument to numeric type.\n\nConvert argument to best possible dtype.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.info", "path": "reference/api/pandas.dataframe.info", "type": "DataFrame", "text": "\nPrint a concise summary of a DataFrame.\n\nThis method prints information about a DataFrame including the index dtype and\ncolumns, non-null values and memory usage.\n\nDataFrame to print information about.\n\nWhether to print the full summary. By default, the setting in\n`pandas.options.display.max_info_columns` is followed.\n\nWhere to send the output. By default, the output is printed to sys.stdout.\nPass a writable buffer if you need to further process the output. max_cols :\nint, optional When to switch from the verbose to the truncated output. If the\nDataFrame has more than max_cols columns, the truncated output is used. By\ndefault, the setting in `pandas.options.display.max_info_columns` is used.\n\nSpecifies whether total memory usage of the DataFrame elements (including the\nindex) should be displayed. By default, this follows the\n`pandas.options.display.memory_usage` setting.\n\nTrue always show memory usage. False never shows memory usage. A value of\n\u2018deep\u2019 is equivalent to \u201cTrue with deep introspection\u201d. Memory usage is shown\nin human-readable units (base-2 representation). Without deep introspection a\nmemory estimation is made based in column dtype and number of rows assuming\nvalues consume the same memory amount for corresponding dtypes. With deep\nmemory introspection, a real memory usage calculation is performed at the cost\nof computational resources.\n\nWhether to show the non-null counts. By default, this is shown only if the\nDataFrame is smaller than `pandas.options.display.max_info_rows` and\n`pandas.options.display.max_info_columns`. A value of True always shows the\ncounts, and False never shows the counts.\n\nDeprecated since version 1.2.0: Use show_counts instead.\n\nThis method prints a summary of a DataFrame and returns None.\n\nSee also\n\nGenerate descriptive statistics of DataFrame columns.\n\nMemory usage of DataFrame columns.\n\nExamples\n\nPrints information of all columns:\n\nPrints a summary of columns count and its dtypes but not per column\ninformation:\n\nPipe output of DataFrame.info to buffer instead of sys.stdout, get buffer\ncontent and writes to a text file:\n\nThe memory_usage parameter allows deep introspection mode, specially useful\nfor big DataFrames and fine-tune memory optimization:\n\n"}, {"name": "pandas.DataFrame.insert", "path": "reference/api/pandas.dataframe.insert", "type": "DataFrame", "text": "\nInsert column into DataFrame at specified location.\n\nRaises a ValueError if column is already contained in the DataFrame, unless\nallow_duplicates is set to True.\n\nInsertion index. Must verify 0 <= loc <= len(columns).\n\nLabel of the inserted column.\n\nSee also\n\nInsert new item by index.\n\nExamples\n\nNotice that pandas uses index alignment in case of value from type Series:\n\n"}, {"name": "pandas.DataFrame.interpolate", "path": "reference/api/pandas.dataframe.interpolate", "type": "DataFrame", "text": "\nFill NaN values using an interpolation method.\n\nPlease note that only `method='linear'` is supported for DataFrame/Series with\na MultiIndex.\n\nInterpolation technique to use. One of:\n\n\u2018linear\u2019: Ignore the index and treat the values as equally spaced. This is the\nonly method supported on MultiIndexes.\n\n\u2018time\u2019: Works on daily and higher resolution data to interpolate given length\nof interval.\n\n\u2018index\u2019, \u2018values\u2019: use the actual numerical values of the index.\n\n\u2018pad\u2019: Fill in NaNs using existing values.\n\n\u2018nearest\u2019, \u2018zero\u2019, \u2018slinear\u2019, \u2018quadratic\u2019, \u2018cubic\u2019, \u2018spline\u2019, \u2018barycentric\u2019,\n\u2018polynomial\u2019: Passed to scipy.interpolate.interp1d. These methods use the\nnumerical values of the index. Both \u2018polynomial\u2019 and \u2018spline\u2019 require that you\nalso specify an order (int), e.g. `df.interpolate(method='polynomial',\norder=5)`.\n\n\u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019, \u2018akima\u2019, \u2018cubicspline\u2019:\nWrappers around the SciPy interpolation methods of similar names. See Notes.\n\n\u2018from_derivatives\u2019: Refers to scipy.interpolate.BPoly.from_derivatives which\nreplaces \u2018piecewise_polynomial\u2019 interpolation method in scipy 0.18.\n\nAxis to interpolate along.\n\nMaximum number of consecutive NaNs to fill. Must be greater than 0.\n\nUpdate the data in place if possible.\n\nConsecutive NaNs will be filled in this direction.\n\nIf \u2018method\u2019 is \u2018pad\u2019 or \u2018ffill\u2019, \u2018limit_direction\u2019 must be \u2018forward\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, \u2018limit_direction\u2019 must be \u2018backwards\u2019.\n\nIf \u2018method\u2019 is \u2018backfill\u2019 or \u2018bfill\u2019, the default is \u2018backward\u2019\n\nelse the default is \u2018forward\u2019\n\nChanged in version 1.1.0: raises ValueError if limit_direction is \u2018forward\u2019 or\n\u2018both\u2019 and method is \u2018backfill\u2019 or \u2018bfill\u2019. raises ValueError if\nlimit_direction is \u2018backward\u2019 or \u2018both\u2019 and method is \u2018pad\u2019 or \u2018ffill\u2019.\n\nIf limit is specified, consecutive NaNs will be filled with this restriction.\n\n`None`: No fill restriction.\n\n\u2018inside\u2019: Only fill NaNs surrounded by valid values (interpolate).\n\n\u2018outside\u2019: Only fill NaNs outside valid values (extrapolate).\n\nDowncast dtypes if possible.\n\nKeyword arguments to pass on to the interpolating function.\n\nReturns the same object type as the caller, interpolated at some or all `NaN`\nvalues or None if `inplace=True`.\n\nSee also\n\nFill missing values using different methods.\n\nPiecewise cubic polynomials (Akima interpolator).\n\nPiecewise polynomial in the Bernstein basis.\n\nInterpolate a 1-D function.\n\nInterpolate polynomial (Krogh interpolator).\n\nPCHIP 1-d monotonic cubic interpolation.\n\nCubic spline data interpolator.\n\nNotes\n\nThe \u2018krogh\u2019, \u2018piecewise_polynomial\u2019, \u2018spline\u2019, \u2018pchip\u2019 and \u2018akima\u2019 methods are\nwrappers around the respective SciPy implementations of similar names. These\nuse the actual numerical values of the index. For more information on their\nbehavior, see the SciPy documentation and SciPy tutorial.\n\nExamples\n\nFilling in `NaN` in a `Series` via linear interpolation.\n\nFilling in `NaN` in a Series by padding, but filling at most two consecutive\n`NaN` at a time.\n\nFilling in `NaN` in a Series via polynomial interpolation or splines: Both\n\u2018polynomial\u2019 and \u2018spline\u2019 methods require that you also specify an `order`\n(int).\n\nFill the DataFrame forward (that is, going down) along each column using\nlinear interpolation.\n\nNote how the last entry in column \u2018a\u2019 is interpolated differently, because\nthere is no entry after it to use for interpolation. Note how the first entry\nin column \u2018b\u2019 remains `NaN`, because there is no entry before it to use for\ninterpolation.\n\nUsing polynomial interpolation.\n\n"}, {"name": "pandas.DataFrame.isin", "path": "reference/api/pandas.dataframe.isin", "type": "DataFrame", "text": "\nWhether each element in the DataFrame is contained in values.\n\nThe result will only be true at a location if all the labels match. If values\nis a Series, that\u2019s the index. If values is a dict, the keys must be the\ncolumn names, which must match. If values is a DataFrame, then both the index\nand column labels must match.\n\nDataFrame of booleans showing whether each element in the DataFrame is\ncontained in values.\n\nSee also\n\nEquality test for DataFrame.\n\nEquivalent method on Series.\n\nTest if pattern or regex is contained within a string of a Series or Index.\n\nExamples\n\nWhen `values` is a list check whether every value in the DataFrame is present\nin the list (which animals have 0 or 2 legs or wings)\n\nTo check if `values` is not in the DataFrame, use the `~` operator:\n\nWhen `values` is a dict, we can pass values to check for each column\nseparately:\n\nWhen `values` is a Series or DataFrame the index and column must match. Note\nthat \u2018falcon\u2019 does not match based on the number of legs in other.\n\n"}, {"name": "pandas.DataFrame.isna", "path": "reference/api/pandas.dataframe.isna", "type": "DataFrame", "text": "\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.DataFrame.isnull", "path": "reference/api/pandas.dataframe.isnull", "type": "DataFrame", "text": "\nDataFrame.isnull is an alias for DataFrame.isna.\n\nDetect missing values.\n\nReturn a boolean same-sized object indicating if the values are NA. NA values,\nsuch as None or `numpy.NaN`, gets mapped to True values. Everything else gets\nmapped to False values. Characters such as empty strings `''` or `numpy.inf`\nare not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`).\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is an NA value.\n\nSee also\n\nAlias of isna.\n\nBoolean inverse of isna.\n\nOmit axes labels with missing values.\n\nTop-level isna.\n\nExamples\n\nShow which entries in a DataFrame are NA.\n\nShow which entries in a Series are NA.\n\n"}, {"name": "pandas.DataFrame.items", "path": "reference/api/pandas.dataframe.items", "type": "DataFrame", "text": "\nIterate over (column name, Series) pairs.\n\nIterates over the DataFrame columns, returning a tuple with the column name\nand the content as a Series.\n\nThe column names for the DataFrame being iterated over.\n\nThe column entries belonging to each label, as a Series.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over DataFrame rows as namedtuples of the values.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.iteritems", "path": "reference/api/pandas.dataframe.iteritems", "type": "DataFrame", "text": "\nIterate over (column name, Series) pairs.\n\nIterates over the DataFrame columns, returning a tuple with the column name\nand the content as a Series.\n\nThe column names for the DataFrame being iterated over.\n\nThe column entries belonging to each label, as a Series.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over DataFrame rows as namedtuples of the values.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.iterrows", "path": "reference/api/pandas.dataframe.iterrows", "type": "DataFrame", "text": "\nIterate over DataFrame rows as (index, Series) pairs.\n\nThe index of the row. A tuple for a MultiIndex.\n\nThe data of the row as a Series.\n\nSee also\n\nIterate over DataFrame rows as namedtuples of the values.\n\nIterate over (column name, Series) pairs.\n\nNotes\n\nBecause `iterrows` returns a Series for each row, it does not preserve dtypes\nacross the rows (dtypes are preserved across columns for DataFrames). For\nexample,\n\nTo preserve dtypes while iterating over the rows, it is better to use\n`itertuples()` which returns namedtuples of the values and which is generally\nfaster than `iterrows`.\n\nYou should never modify something you are iterating over. This is not\nguaranteed to work in all cases. Depending on the data types, the iterator\nreturns a copy and not a view, and writing to it will have no effect.\n\n"}, {"name": "pandas.DataFrame.itertuples", "path": "reference/api/pandas.dataframe.itertuples", "type": "DataFrame", "text": "\nIterate over DataFrame rows as namedtuples.\n\nIf True, return the index as the first element of the tuple.\n\nThe name of the returned namedtuples or None to return regular tuples.\n\nAn object to iterate over namedtuples for each row in the DataFrame with the\nfirst field possibly being the index and following fields being the column\nvalues.\n\nSee also\n\nIterate over DataFrame rows as (index, Series) pairs.\n\nIterate over (column name, Series) pairs.\n\nNotes\n\nThe column names will be renamed to positional names if they are invalid\nPython identifiers, repeated, or start with an underscore. On python versions\n< 3.7 regular tuples are returned for DataFrames with a large number of\ncolumns (>254).\n\nExamples\n\nBy setting the index parameter to False we can remove the index as the first\nelement of the tuple:\n\nWith the name parameter set we set a custom name for the yielded namedtuples:\n\n"}, {"name": "pandas.DataFrame.join", "path": "reference/api/pandas.dataframe.join", "type": "DataFrame", "text": "\nJoin columns of another DataFrame.\n\nJoin columns with other DataFrame either on index or on a key column.\nEfficiently join multiple DataFrame objects by index at once by passing a\nlist.\n\nIndex should be similar to one of the columns in this one. If a Series is\npassed, its name attribute must be set, and that will be used as the column\nname in the resulting joined DataFrame.\n\nColumn or index level name(s) in the caller to join on the index in other,\notherwise joins index-on-index. If multiple values given, the other DataFrame\nmust have a MultiIndex. Can pass an array as the join key if it is not already\ncontained in the calling DataFrame. Like an Excel VLOOKUP operation.\n\nHow to handle the operation of the two objects.\n\nleft: use calling frame\u2019s index (or column if on is specified)\n\nright: use other\u2019s index.\n\nouter: form union of calling frame\u2019s index (or column if on is specified) with\nother\u2019s index, and sort it. lexicographically.\n\ninner: form intersection of calling frame\u2019s index (or column if on is\nspecified) with other\u2019s index, preserving the order of the calling\u2019s one.\n\ncross: creates the cartesian product from both frames, preserves the order of\nthe left keys.\n\nNew in version 1.2.0.\n\nSuffix to use from left frame\u2019s overlapping columns.\n\nSuffix to use from right frame\u2019s overlapping columns.\n\nOrder result DataFrame lexicographically by the join key. If False, the order\nof the join key depends on the join type (how keyword).\n\nA dataframe containing columns from both the caller and other.\n\nSee also\n\nFor column(s)-on-column(s) operations.\n\nNotes\n\nParameters on, lsuffix, and rsuffix are not supported when passing a list of\nDataFrame objects.\n\nSupport for specifying index levels as the on parameter was added in version\n0.23.0.\n\nExamples\n\nJoin DataFrames using their indexes.\n\nIf we want to join using the key columns, we need to set key to be the index\nin both df and other. The joined DataFrame will have key as its index.\n\nAnother option to join using the key columns is to use the on parameter.\nDataFrame.join always uses other\u2019s index but we can use any column in df. This\nmethod preserves the original DataFrame\u2019s index in the result.\n\nUsing non-unique key values shows how they are matched.\n\n"}, {"name": "pandas.DataFrame.keys", "path": "reference/api/pandas.dataframe.keys", "type": "DataFrame", "text": "\nGet the \u2018info axis\u2019 (see Indexing for more).\n\nThis is index for Series, columns for DataFrame.\n\nInfo axis.\n\n"}, {"name": "pandas.DataFrame.kurt", "path": "reference/api/pandas.dataframe.kurt", "type": "DataFrame", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.kurtosis", "path": "reference/api/pandas.dataframe.kurtosis", "type": "DataFrame", "text": "\nReturn unbiased kurtosis over requested axis.\n\nKurtosis obtained using Fisher\u2019s definition of kurtosis (kurtosis of normal ==\n0.0). Normalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.last", "path": "reference/api/pandas.dataframe.last", "type": "DataFrame", "text": "\nSelect final periods of time series data based on a date offset.\n\nFor a DataFrame with a sorted DatetimeIndex, this function selects the last\nfew rows based on a date offset.\n\nThe offset length of the data that will be selected. For instance, \u20183D\u2019 will\ndisplay all the rows having their index within the last 3 days.\n\nA subset of the caller.\n\nIf the index is not a `DatetimeIndex`\n\nSee also\n\nSelect initial periods of time series based on a date offset.\n\nSelect values at a particular time of the day.\n\nSelect values between particular times of the day.\n\nExamples\n\nGet the rows for the last 3 days:\n\nNotice the data for 3 last calendar days were returned, not the last 3\nobserved days in the dataset, and therefore data for 2018-04-11 was not\nreturned.\n\n"}, {"name": "pandas.DataFrame.last_valid_index", "path": "reference/api/pandas.dataframe.last_valid_index", "type": "DataFrame", "text": "\nReturn index for last non-NA value or None, if no NA value is found.\n\nNotes\n\nIf all elements are non-NA/null, returns None. Also returns None for empty\nSeries/DataFrame.\n\n"}, {"name": "pandas.DataFrame.le", "path": "reference/api/pandas.dataframe.le", "type": "DataFrame", "text": "\nGet Less than or equal to of dataframe and other, element-wise (binary\noperator le).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.loc", "path": "reference/api/pandas.dataframe.loc", "type": "DataFrame", "text": "\nAccess a group of rows and columns by label(s) or a boolean array.\n\n`.loc[]` is primarily label based, but may also be used with a boolean array.\n\nAllowed inputs are:\n\nA single label, e.g. `5` or `'a'`, (note that `5` is interpreted as a label of\nthe index, and never as an integer position along the index).\n\nA list or array of labels, e.g. `['a', 'b', 'c']`.\n\nA slice object with labels, e.g. `'a':'f'`.\n\nWarning\n\nNote that contrary to usual python slices, both the start and the stop are\nincluded\n\nA boolean array of the same length as the axis being sliced, e.g. `[True,\nFalse, True]`.\n\nAn alignable boolean Series. The index of the key will be aligned before\nmasking.\n\nAn alignable Index. The Index of the returned selection will be the input.\n\nA `callable` function with one argument (the calling Series or DataFrame) and\nthat returns valid output for indexing (one of the above)\n\nSee more at Selection by Label.\n\nIf any items are not found.\n\nIf an indexed key is passed and its index is unalignable to the frame index.\n\nSee also\n\nAccess a single value for a row/column label pair.\n\nAccess group of rows and columns by integer position(s).\n\nReturns a cross-section (row(s) or column(s)) from the Series/DataFrame.\n\nAccess group of values using labels.\n\nExamples\n\nGetting values\n\nSingle label. Note this returns the row as a Series.\n\nList of labels. Note using `[[]]` returns a DataFrame.\n\nSingle label for row and column\n\nSlice with labels for row and single label for column. As mentioned above,\nnote that both the start and stop of the slice are included.\n\nBoolean list with the same length as the row axis\n\nAlignable boolean Series:\n\nIndex (same behavior as `df.reindex`)\n\nConditional that returns a boolean Series\n\nConditional that returns a boolean Series with column labels specified\n\nCallable that returns a boolean Series\n\nSetting values\n\nSet value for all items matching the list of labels\n\nSet value for an entire row\n\nSet value for an entire column\n\nSet value for rows matching callable condition\n\nGetting values on a DataFrame with an index that has integer labels\n\nAnother example using integers for the index\n\nSlice with integer labels for rows. As mentioned above, note that both the\nstart and stop of the slice are included.\n\nGetting values with a MultiIndex\n\nA number of examples using a DataFrame with a MultiIndex\n\nSingle label. Note this returns a DataFrame with a single index.\n\nSingle index tuple. Note this returns a Series.\n\nSingle label for row and column. Similar to passing in a tuple, this returns a\nSeries.\n\nSingle tuple. Note using `[[]]` returns a DataFrame.\n\nSingle tuple for the index with a single label for the column\n\nSlice from index tuple to single label\n\nSlice from index tuple to index tuple\n\n"}, {"name": "pandas.DataFrame.lookup", "path": "reference/api/pandas.dataframe.lookup", "type": "DataFrame", "text": "\nLabel-based \u201cfancy indexing\u201d function for DataFrame. Given equal-length arrays\nof row and column labels, return an array of the values corresponding to each\n(row, col) pair.\n\nDeprecated since version 1.2.0: DataFrame.lookup is deprecated, use\nDataFrame.melt and DataFrame.loc instead. For further details see Looking up\nvalues by index/column labels.\n\nThe row labels to use for lookup.\n\nThe column labels to use for lookup.\n\nThe found values.\n\n"}, {"name": "pandas.DataFrame.lt", "path": "reference/api/pandas.dataframe.lt", "type": "DataFrame", "text": "\nGet Less than of dataframe and other, element-wise (binary operator lt).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.mad", "path": "reference/api/pandas.dataframe.mad", "type": "DataFrame", "text": "\nReturn the mean absolute deviation of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\n"}, {"name": "pandas.DataFrame.mask", "path": "reference/api/pandas.dataframe.mask", "type": "DataFrame", "text": "\nReplace values where the condition is True.\n\nWhere cond is False, keep the original value. Where True, replace with\ncorresponding value from other. If cond is callable, it is computed on the\nSeries/DataFrame and should return boolean Series/DataFrame or array. The\ncallable must not change input Series/DataFrame (though pandas doesn\u2019t check\nit).\n\nEntries where cond is True are replaced with corresponding value from other.\nIf other is callable, it is computed on the Series/DataFrame and should return\nscalar or Series/DataFrame. The callable must not change input\nSeries/DataFrame (though pandas doesn\u2019t check it).\n\nWhether to perform the operation in place on the data.\n\nAlignment axis if needed.\n\nAlignment level if needed.\n\nNote that currently this parameter won\u2019t affect the results and will always\ncoerce to a suitable dtype.\n\n\u2018raise\u2019 : allow exceptions to be raised.\n\n\u2018ignore\u2019 : suppress exceptions. On error return original object.\n\nTry to cast the result back to the input type (if possible).\n\nDeprecated since version 1.3.0: Manually cast back if necessary.\n\nSee also\n\nReturn an object of same shape as self.\n\nNotes\n\nThe mask method is an application of the if-then idiom. For each element in\nthe calling DataFrame, if `cond` is `False` the element is used; otherwise the\ncorresponding element from the DataFrame `other` is used.\n\nThe signature for `DataFrame.where()` differs from `numpy.where()`. Roughly\n`df1.where(m, df2)` is equivalent to `np.where(m, df1, df2)`.\n\nFor further details and examples see the `mask` documentation in indexing.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.max", "path": "reference/api/pandas.dataframe.max", "type": "DataFrame", "text": "\nReturn the maximum of the values over the requested axis.\n\nIf you want the index of the maximum, use `idxmax`. This is the equivalent of\nthe `numpy.ndarray` method `argmax`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.mean", "path": "reference/api/pandas.dataframe.mean", "type": "DataFrame", "text": "\nReturn the mean of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.median", "path": "reference/api/pandas.dataframe.median", "type": "DataFrame", "text": "\nReturn the median of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.melt", "path": "reference/api/pandas.dataframe.melt", "type": "DataFrame", "text": "\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nThis function is useful to massage a DataFrame into a format where one or more\ncolumns are identifier variables (id_vars), while all other columns,\nconsidered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis,\nleaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.\n\nColumn(s) to use as identifier variables.\n\nColumn(s) to unpivot. If not specified, uses all columns that are not set as\nid_vars.\n\nName to use for the \u2018variable\u2019 column. If None it uses `frame.columns.name` or\n\u2018variable\u2019.\n\nName to use for the \u2018value\u2019 column.\n\nIf columns are a MultiIndex then use this level to melt.\n\nIf True, original index is ignored. If False, the original index is retained.\nIndex labels will be repeated as necessary.\n\nNew in version 1.1.0.\n\nUnpivoted DataFrame.\n\nSee also\n\nIdentical method.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nReturn reshaped DataFrame organized by given index / column values.\n\nExplode a DataFrame from list-like columns to long format.\n\nExamples\n\nThe names of \u2018variable\u2019 and \u2018value\u2019 columns can be customized:\n\nOriginal index values can be kept around:\n\nIf you have multi-index columns:\n\n"}, {"name": "pandas.DataFrame.memory_usage", "path": "reference/api/pandas.dataframe.memory_usage", "type": "DataFrame", "text": "\nReturn the memory usage of each column in bytes.\n\nThe memory usage can optionally include the contribution of the index and\nelements of object dtype.\n\nThis value is displayed in DataFrame.info by default. This can be suppressed\nby setting `pandas.options.display.memory_usage` to False.\n\nSpecifies whether to include the memory usage of the DataFrame\u2019s index in\nreturned Series. If `index=True`, the memory usage of the index is the first\nitem in the output.\n\nIf True, introspect the data deeply by interrogating object dtypes for system-\nlevel memory consumption, and include it in the returned values.\n\nA Series whose index is the original column names and whose values is the\nmemory usage of each column in bytes.\n\nSee also\n\nTotal bytes consumed by the elements of an ndarray.\n\nBytes consumed by a Series.\n\nMemory-efficient array for string values with many repeated values.\n\nConcise summary of a DataFrame.\n\nExamples\n\nThe memory footprint of object dtype columns is ignored by default:\n\nUse a Categorical for efficient storage of an object-dtype column with many\nrepeated values.\n\n"}, {"name": "pandas.DataFrame.merge", "path": "reference/api/pandas.dataframe.merge", "type": "DataFrame", "text": "\nMerge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on columns, the\nDataFrame indexes will be ignored. Otherwise if joining indexes on indexes or\nindexes on a column or columns, the index will be passed on. When performing a\ncross merge, no column specifications to merge on are allowed.\n\nWarning\n\nIf both key columns contain rows where the key is a null value, those rows\nwill be matched against each other. This is different from usual SQL join\nbehaviour and can lead to unexpected results.\n\nObject to merge with.\n\nType of merge to be performed.\n\nleft: use only keys from left frame, similar to a SQL left outer join;\npreserve key order.\n\nright: use only keys from right frame, similar to a SQL right outer join;\npreserve key order.\n\nouter: use union of keys from both frames, similar to a SQL full outer join;\nsort keys lexicographically.\n\ninner: use intersection of keys from both frames, similar to a SQL inner join;\npreserve the order of the left keys.\n\ncross: creates the cartesian product from both frames, preserves the order of\nthe left keys.\n\nNew in version 1.2.0.\n\nColumn or index level names to join on. These must be found in both\nDataFrames. If on is None and not merging on indexes then this defaults to the\nintersection of the columns in both DataFrames.\n\nColumn or index level names to join on in the left DataFrame. Can also be an\narray or list of arrays of the length of the left DataFrame. These arrays are\ntreated as if they are columns.\n\nColumn or index level names to join on in the right DataFrame. Can also be an\narray or list of arrays of the length of the right DataFrame. These arrays are\ntreated as if they are columns.\n\nUse the index from the left DataFrame as the join key(s). If it is a\nMultiIndex, the number of keys in the other DataFrame (either the index or a\nnumber of columns) must match the number of levels.\n\nUse the index from the right DataFrame as the join key. Same caveats as\nleft_index.\n\nSort the join keys lexicographically in the result DataFrame. If False, the\norder of the join keys depends on the join type (how keyword).\n\nA length-2 sequence where each element is optionally a string indicating the\nsuffix to add to overlapping column names in left and right respectively. Pass\na value of None instead of a string to indicate that the column name from left\nor right should be left as-is, with no suffix. At least one of the values must\nnot be None.\n\nIf False, avoid copy if possible.\n\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with\ninformation on the source of each row. The column can be given a different\nname by providing a string argument. The column will have a Categorical type\nwith the value of \u201cleft_only\u201d for observations whose merge key only appears in\nthe left DataFrame, \u201cright_only\u201d for observations whose merge key only appears\nin the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in\nboth DataFrames.\n\nIf specified, checks if merge is of specified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right\ndatasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nA DataFrame of the two merged objects.\n\nSee also\n\nMerge with optional filling/interpolation.\n\nMerge on nearest keys.\n\nSimilar method using indices.\n\nNotes\n\nSupport for specifying index levels as the on, left_on, and right_on\nparameters was added in version 0.23.0 Support for merging named Series\nobjects was added in version 0.24.0\n\nExamples\n\nMerge df1 and df2 on the lkey and rkey columns. The value columns have the\ndefault suffixes, _x and _y, appended.\n\nMerge DataFrames df1 and df2 with specified left and right suffixes appended\nto any overlapping columns.\n\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have\nany overlapping columns.\n\n"}, {"name": "pandas.DataFrame.min", "path": "reference/api/pandas.dataframe.min", "type": "DataFrame", "text": "\nReturn the minimum of the values over the requested axis.\n\nIf you want the index of the minimum, use `idxmin`. This is the equivalent of\nthe `numpy.ndarray` method `argmin`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.mod", "path": "reference/api/pandas.dataframe.mod", "type": "DataFrame", "text": "\nGet Modulo of dataframe and other, element-wise (binary operator mod).\n\nEquivalent to `dataframe % other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmod.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.mode", "path": "reference/api/pandas.dataframe.mode", "type": "DataFrame", "text": "\nGet the mode(s) of each element along the selected axis.\n\nThe mode of a set of values is the value that appears most often. It can be\nmultiple values.\n\nThe axis to iterate over while searching for the mode:\n\n0 or \u2018index\u2019 : get mode of each column\n\n1 or \u2018columns\u2019 : get mode of each row.\n\nIf True, only apply to numeric columns.\n\nDon\u2019t consider counts of NaN/NaT.\n\nThe modes of each column or row.\n\nSee also\n\nReturn the highest frequency value in a Series.\n\nReturn the counts of values in a Series.\n\nExamples\n\nBy default, missing values are not considered, and the mode of wings are both\n0 and 2. Because the resulting DataFrame has two rows, the second row of\n`species` and `legs` contains `NaN`.\n\nSetting `dropna=False` `NaN` values are considered and they can be the mode\n(like for wings).\n\nSetting `numeric_only=True`, only the mode of numeric columns is computed, and\ncolumns of other types are ignored.\n\nTo compute the mode over columns and not rows, use the axis parameter:\n\n"}, {"name": "pandas.DataFrame.mul", "path": "reference/api/pandas.dataframe.mul", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\nEquivalent to `dataframe * other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.multiply", "path": "reference/api/pandas.dataframe.multiply", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator mul).\n\nEquivalent to `dataframe * other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rmul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.ndim", "path": "reference/api/pandas.dataframe.ndim", "type": "DataFrame", "text": "\nReturn an int representing the number of axes / array dimensions.\n\nReturn 1 if Series. Otherwise return 2 if DataFrame.\n\nSee also\n\nNumber of array dimensions.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.ne", "path": "reference/api/pandas.dataframe.ne", "type": "DataFrame", "text": "\nGet Not equal to of dataframe and other, element-wise (binary operator ne).\n\nAmong flexible wrappers (eq, ne, le, lt, ge, gt) to comparison operators.\n\nEquivalent to ==, !=, <=, <, >=, > with support to choose axis (rows or\ncolumns) and level for comparison.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nResult of the comparison.\n\nSee also\n\nCompare DataFrames for equality elementwise.\n\nCompare DataFrames for inequality elementwise.\n\nCompare DataFrames for less than inequality or equality elementwise.\n\nCompare DataFrames for strictly less than inequality elementwise.\n\nCompare DataFrames for greater than inequality or equality elementwise.\n\nCompare DataFrames for strictly greater than inequality elementwise.\n\nNotes\n\nMismatched indices will be unioned together. NaN values are considered\ndifferent (i.e. NaN != NaN).\n\nExamples\n\nComparison with a scalar, using either the operator or method:\n\nWhen other is a `Series`, the columns of a DataFrame are aligned with the\nindex of other and broadcast:\n\nUse the method to control the broadcast axis:\n\nWhen comparing to an arbitrary sequence, the number of columns must match the\nnumber elements in other:\n\nUse the method to control the axis:\n\nCompare to a DataFrame of different shape.\n\nCompare to a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.nlargest", "path": "reference/api/pandas.dataframe.nlargest", "type": "DataFrame", "text": "\nReturn the first n rows ordered by columns in descending order.\n\nReturn the first n rows with the largest values in columns, in descending\norder. The columns that are not specified are returned as well, but not used\nfor ordering.\n\nThis method is equivalent to `df.sort_values(columns,\nascending=False).head(n)`, but more performant.\n\nNumber of rows to return.\n\nColumn label(s) to order by.\n\nWhere there are duplicate values:\n\n`first` : prioritize the first occurrence(s)\n\n`last` : prioritize the last occurrence(s)\n\n`all` : do not drop any duplicates, even it means selecting more than n items.\n\nThe first n rows ordered by the given columns in descending order.\n\nSee also\n\nReturn the first n rows ordered by columns in ascending order.\n\nSort DataFrame by the values.\n\nReturn the first n rows without re-ordering.\n\nNotes\n\nThis function cannot be used with all column types. For example, when\nspecifying columns with object or category dtypes, `TypeError` is raised.\n\nExamples\n\nIn the following example, we will use `nlargest` to select the three rows\nhaving the largest values in column \u201cpopulation\u201d.\n\nWhen using `keep='last'`, ties are resolved in reverse order:\n\nWhen using `keep='all'`, all duplicate items are maintained:\n\nTo order by the largest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can\nspecify multiple columns like in the next example.\n\n"}, {"name": "pandas.DataFrame.notna", "path": "reference/api/pandas.dataframe.notna", "type": "DataFrame", "text": "\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.DataFrame.notnull", "path": "reference/api/pandas.dataframe.notnull", "type": "DataFrame", "text": "\nDataFrame.notnull is an alias for DataFrame.notna.\n\nDetect existing (non-missing) values.\n\nReturn a boolean same-sized object indicating if the values are not NA. Non-\nmissing values get mapped to True. Characters such as empty strings `''` or\n`numpy.inf` are not considered NA values (unless you set\n`pandas.options.mode.use_inf_as_na = True`). NA values, such as None or\n`numpy.NaN`, get mapped to False values.\n\nMask of bool values for each element in DataFrame that indicates whether an\nelement is not an NA value.\n\nSee also\n\nAlias of notna.\n\nBoolean inverse of notna.\n\nOmit axes labels with missing values.\n\nTop-level notna.\n\nExamples\n\nShow which entries in a DataFrame are not NA.\n\nShow which entries in a Series are not NA.\n\n"}, {"name": "pandas.DataFrame.nsmallest", "path": "reference/api/pandas.dataframe.nsmallest", "type": "DataFrame", "text": "\nReturn the first n rows ordered by columns in ascending order.\n\nReturn the first n rows with the smallest values in columns, in ascending\norder. The columns that are not specified are returned as well, but not used\nfor ordering.\n\nThis method is equivalent to `df.sort_values(columns,\nascending=True).head(n)`, but more performant.\n\nNumber of items to retrieve.\n\nColumn name or names to order by.\n\nWhere there are duplicate values:\n\n`first` : take the first occurrence.\n\n`last` : take the last occurrence.\n\n`all` : do not drop any duplicates, even it means selecting more than n items.\n\nSee also\n\nReturn the first n rows ordered by columns in descending order.\n\nSort DataFrame by the values.\n\nReturn the first n rows without re-ordering.\n\nExamples\n\nIn the following example, we will use `nsmallest` to select the three rows\nhaving the smallest values in column \u201cpopulation\u201d.\n\nWhen using `keep='last'`, ties are resolved in reverse order:\n\nWhen using `keep='all'`, all duplicate items are maintained:\n\nTo order by the smallest values in column \u201cpopulation\u201d and then \u201cGDP\u201d, we can\nspecify multiple columns like in the next example.\n\n"}, {"name": "pandas.DataFrame.nunique", "path": "reference/api/pandas.dataframe.nunique", "type": "DataFrame", "text": "\nCount number of distinct elements in specified axis.\n\nReturn Series with number of distinct elements. Can ignore NaN values.\n\nThe axis to use. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nDon\u2019t include NaN in the counts.\n\nSee also\n\nMethod nunique for Series.\n\nCount non-NA cells for each column or row.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.pad", "path": "reference/api/pandas.dataframe.pad", "type": "DataFrame", "text": "\nSynonym for `DataFrame.fillna()` with `method='ffill'`.\n\nObject with missing values filled or None if `inplace=True`.\n\n"}, {"name": "pandas.DataFrame.pct_change", "path": "reference/api/pandas.dataframe.pct_change", "type": "DataFrame", "text": "\nPercentage change between the current and a prior element.\n\nComputes the percentage change from the immediately previous row by default.\nThis is useful in comparing the percentage of change in a time series of\nelements.\n\nPeriods to shift for forming percent change.\n\nHow to handle NAs before computing percent changes.\n\nThe number of consecutive NAs to fill before stopping.\n\nIncrement to use from time series API (e.g. \u2018M\u2019 or BDay()).\n\nAdditional keyword arguments are passed into DataFrame.shift or Series.shift.\n\nThe same type as the calling object.\n\nSee also\n\nCompute the difference of two elements in a Series.\n\nCompute the difference of two elements in a DataFrame.\n\nShift the index by some number of periods.\n\nShift the index by some number of periods.\n\nExamples\n\nSeries\n\nSee the percentage change in a Series where filling NAs with last valid\nobservation forward to next valid.\n\nDataFrame\n\nPercentage change in French franc, Deutsche Mark, and Italian lira from\n1980-01-01 to 1980-03-01.\n\nPercentage of change in GOOG and APPL stock volume. Shows computing the\npercentage change between columns.\n\n"}, {"name": "pandas.DataFrame.pipe", "path": "reference/api/pandas.dataframe.pipe", "type": "DataFrame", "text": "\nApply chainable functions that expect Series or DataFrames.\n\nFunction to apply to the Series/DataFrame. `args`, and `kwargs` are passed\ninto `func`. Alternatively a `(callable, data_keyword)` tuple where\n`data_keyword` is a string indicating the keyword of `callable` that expects\nthe Series/DataFrame.\n\nPositional arguments passed into `func`.\n\nA dictionary of keyword arguments passed into `func`.\n\nSee also\n\nApply a function along input axis of DataFrame.\n\nApply a function elementwise on a whole DataFrame.\n\nApply a mapping correspondence on a `Series`.\n\nNotes\n\nUse `.pipe` when chaining together functions that expect Series, DataFrames or\nGroupBy objects. Instead of writing\n\nYou can write\n\nIf you have a function that takes the data as (say) the second argument, pass\na tuple indicating which keyword expects the data. For example, suppose `f`\ntakes its data as `arg2`:\n\n"}, {"name": "pandas.DataFrame.pivot", "path": "reference/api/pandas.dataframe.pivot", "type": "DataFrame", "text": "\nReturn reshaped DataFrame organized by given index / column values.\n\nReshape data (produce a \u201cpivot\u201d table) based on column values. Uses unique\nvalues from specified index / columns to form axes of the resulting DataFrame.\nThis function does not support data aggregation, multiple values will result\nin a MultiIndex in the columns. See the User Guide for more on reshaping.\n\nColumn to use to make new frame\u2019s index. If None, uses existing index.\n\nChanged in version 1.1.0: Also accept list of index names.\n\nColumn to use to make new frame\u2019s columns.\n\nChanged in version 1.1.0: Also accept list of columns names.\n\nColumn(s) to use for populating new frame\u2019s values. If not specified, all\nremaining columns will be used and the result will have hierarchically indexed\ncolumns.\n\nReturns reshaped DataFrame.\n\nWhen there are any index, columns combinations with multiple values.\nDataFrame.pivot_table when you need to aggregate.\n\nSee also\n\nGeneralization of pivot that can handle duplicate values for one index/column\npair.\n\nPivot based on the index values instead of a column.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nNotes\n\nFor finer-tuned control, see hierarchical indexing documentation along with\nthe related stack/unstack methods.\n\nExamples\n\nYou could also assign a list of column names or a list of index names.\n\nA ValueError is raised if there are any duplicates.\n\nNotice that the first two rows are the same for our index and columns\narguments.\n\n"}, {"name": "pandas.DataFrame.pivot_table", "path": "reference/api/pandas.dataframe.pivot_table", "type": "DataFrame", "text": "\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nThe levels in the pivot table will be stored in MultiIndex objects\n(hierarchical indexes) on the index and columns of the result DataFrame.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable index. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf an array is passed, it must be the same length as the data. The list can\ncontain any of the other types (except list). Keys to group by on the pivot\ntable column. If an array is passed, it is being used as the same manner as\ncolumn values.\n\nIf list of functions passed, the resulting pivot table will have hierarchical\ncolumns whose top level are the function names (inferred from the function\nobjects themselves) If dict is passed, the key is column to aggregate and\nvalue is function or list of functions.\n\nValue to replace missing values with (in the resulting pivot table, after\naggregation).\n\nAdd all row / columns (e.g. for subtotal / grand totals).\n\nDo not include columns whose entries are all NaN.\n\nName of the row / column that will contain the totals when margins is True.\n\nThis only applies if any of the groupers are Categoricals. If True: only show\nobserved values for categorical groupers. If False: show all values for\ncategorical groupers.\n\nChanged in version 0.25.0.\n\nSpecifies if the result should be sorted.\n\nNew in version 1.3.0.\n\nAn Excel style pivot table.\n\nSee also\n\nPivot without aggregation that can handle non-numeric data.\n\nUnpivot a DataFrame from wide to long format, optionally leaving identifiers\nset.\n\nWide panel to long format. Less flexible but more user-friendly than melt.\n\nExamples\n\nThis first example aggregates values by taking the sum.\n\nWe can also fill missing values using the fill_value parameter.\n\nThe next example aggregates by taking the mean across multiple columns.\n\nWe can also calculate multiple types of aggregations for any given value\ncolumn.\n\n"}, {"name": "pandas.DataFrame.plot", "path": "reference/api/pandas.dataframe.plot", "type": "DataFrame", "text": "\nMake plots of Series or DataFrame.\n\nUses the backend specified by the option `plotting.backend`. By default,\nmatplotlib is used.\n\nThe object for which the method is called.\n\nOnly used if data is a DataFrame.\n\nAllows plotting of one column versus another. Only used if data is a\nDataFrame.\n\nThe kind of plot to produce:\n\n\u2018line\u2019 : line plot (default)\n\n\u2018bar\u2019 : vertical bar plot\n\n\u2018barh\u2019 : horizontal bar plot\n\n\u2018hist\u2019 : histogram\n\n\u2018box\u2019 : boxplot\n\n\u2018kde\u2019 : Kernel Density Estimation plot\n\n\u2018density\u2019 : same as \u2018kde\u2019\n\n\u2018area\u2019 : area plot\n\n\u2018pie\u2019 : pie plot\n\n\u2018scatter\u2019 : scatter plot (DataFrame only)\n\n\u2018hexbin\u2019 : hexbin plot (DataFrame only)\n\nAn axes of the current figure.\n\nMake separate subplots for each column.\n\nIn case `subplots=True`, share x axis and set some x axis labels to invisible;\ndefaults to True if ax is None otherwise False if an ax is passed in; Be\naware, that passing in both an ax and `sharex=True` will alter all x axis\nlabels for all axis in a figure.\n\nIn case `subplots=True`, share y axis and set some y axis labels to invisible.\n\n(rows, columns) for the layout of subplots.\n\nSize of a figure object.\n\nUse index as ticks for x axis.\n\nTitle to use for the plot. If a string is passed, print the string at the top\nof the figure. If a list is passed and subplots is True, print each item in\nthe list above the corresponding subplot.\n\nAxis grid lines.\n\nPlace legend on axis subplots.\n\nThe matplotlib line style per column.\n\nUse log scaling or symlog scaling on x axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on y axis. .. versionchanged:: 0.25.0\n\nUse log scaling or symlog scaling on both x and y axes. .. versionchanged::\n0.25.0\n\nValues to use for the xticks.\n\nValues to use for the yticks.\n\nSet the x limits of the current axes.\n\nSet the y limits of the current axes.\n\nName to use for the xlabel on x-axis. Default uses index name as xlabel, or\nthe x-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nName to use for the ylabel on y-axis. Default will show no ylabel, or the\ny-column name for planar plots.\n\nNew in version 1.1.0.\n\nChanged in version 1.2.0: Now applicable to planar plots (scatter, hexbin).\n\nRotation for ticks (xticks for vertical, yticks for horizontal plots).\n\nFont size for xticks and yticks.\n\nColormap to select colors from. If string, load colormap with that name from\nmatplotlib.\n\nIf True, plot colorbar (only relevant for \u2018scatter\u2019 and \u2018hexbin\u2019 plots).\n\nSpecify relative alignments for bar plot layout. From 0 (left/bottom-end) to 1\n(right/top-end). Default is 0.5 (center).\n\nIf True, draw a table using the data in the DataFrame and the data will be\ntransposed to meet matplotlib\u2019s default layout. If a Series or DataFrame is\npassed, use passed data to draw a table.\n\nSee Plotting with Error Bars for detail.\n\nEquivalent to yerr.\n\nIf True, create stacked plot.\n\nSort column names to determine plot ordering.\n\nWhether to plot on the secondary y-axis if a list/tuple, which columns to plot\non secondary y-axis.\n\nWhen using a secondary_y axis, automatically mark the column labels with\n\u201c(right)\u201d in the legend.\n\nIf True, boolean values can be plotted.\n\nBackend to use instead of the backend specified in the option\n`plotting.backend`. For instance, \u2018matplotlib\u2019. Alternatively, to specify the\n`plotting.backend` for the whole session, set `pd.options.plotting.backend`.\n\nNew in version 1.0.0.\n\nOptions to pass to matplotlib plotting method.\n\nIf the backend is not the default matplotlib one, the return value will be the\nobject returned by the backend.\n\nNotes\n\nSee matplotlib documentation online for more on this subject\n\nIf kind = \u2018bar\u2019 or \u2018barh\u2019, you can specify relative alignments for bar plot\nlayout by position keyword. From 0 (left/bottom-end) to 1 (right/top-end).\nDefault is 0.5 (center)\n\n"}, {"name": "pandas.DataFrame.plot.area", "path": "reference/api/pandas.dataframe.plot.area", "type": "DataFrame", "text": "\nDraw a stacked area plot.\n\nAn area plot displays quantitative data visually. This function wraps the\nmatplotlib area function.\n\nCoordinates for the X axis. By default uses the index.\n\nColumn to plot. By default uses all columns.\n\nArea plots are stacked by default. Set to False to create a unstacked plot.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nArea plot, or array of area plots if subplots is True.\n\nSee also\n\nMake plots of DataFrame using matplotlib / pylab.\n\nExamples\n\nDraw an area plot based on basic business metrics:\n\nArea plots are stacked by default. To produce an unstacked plot, pass\n`stacked=False`:\n\nDraw an area plot for a single column:\n\nDraw with a different x:\n\n"}, {"name": "pandas.DataFrame.plot.bar", "path": "reference/api/pandas.dataframe.plot.bar", "type": "DataFrame", "text": "\nVertical bar plot.\n\nA bar plot is a plot that presents categorical data with rectangular bars with\nlengths proportional to the values that they represent. A bar plot shows\ncomparisons among discrete categories. One axis of the plot shows the specific\ncategories being compared, and the other axis represents a measured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nHorizontal bar plot.\n\nMake plots of a DataFrame.\n\nMake a bar plot with matplotlib.\n\nExamples\n\nBasic plot.\n\nPlot a whole dataframe to a bar plot. Each column is assigned a distinct\ncolor, and each row is nested in a group along the horizontal axis.\n\nPlot stacked bar charts for the DataFrame\n\nInstead of nesting, the figure can be split by column with `subplots=True`. In\nthis case, a `numpy.ndarray` of `matplotlib.axes.Axes` are returned.\n\nIf you don\u2019t like the default colours, you can specify how you\u2019d like each\ncolumn to be colored.\n\nPlot a single column.\n\nPlot only selected categories for the DataFrame.\n\n"}, {"name": "pandas.DataFrame.plot.barh", "path": "reference/api/pandas.dataframe.plot.barh", "type": "DataFrame", "text": "\nMake a horizontal bar plot.\n\nA horizontal bar plot is a plot that presents quantitative data with\nrectangular bars with lengths proportional to the values that they represent.\nA bar plot shows comparisons among discrete categories. One axis of the plot\nshows the specific categories being compared, and the other axis represents a\nmeasured value.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s bar will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color bars for column a in green and\nbars for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nVertical bar plot.\n\nMake plots of DataFrame using matplotlib.\n\nPlot a vertical bar plot using matplotlib.\n\nExamples\n\nBasic example\n\nPlot a whole DataFrame to a horizontal bar plot\n\nPlot stacked barh charts for the DataFrame\n\nWe can specify colors for each column\n\nPlot a column of the DataFrame to a horizontal bar plot\n\nPlot DataFrame versus the desired column\n\n"}, {"name": "pandas.DataFrame.plot.box", "path": "reference/api/pandas.dataframe.plot.box", "type": "DataFrame", "text": "\nMake a box plot of the DataFrame columns.\n\nA box plot is a method for graphically depicting groups of numerical data\nthrough their quartiles. The box extends from the Q1 to Q3 quartile values of\nthe data, with a line at the median (Q2). The whiskers extend from the edges\nof box to show the range of the data. The position of the whiskers is set by\ndefault to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points\nare those past the end of the whiskers.\n\nFor further details see Wikipedia\u2019s entry for boxplot.\n\nA consideration when using this chart is that the box and the whiskers can\noverlap, which is very common when plotting small sets of data.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nAdditional keywords are documented in `DataFrame.plot()`.\n\nSee also\n\nAnother method to draw a box plot.\n\nDraw a box plot from a Series object.\n\nDraw a box plot in matplotlib.\n\nExamples\n\nDraw a box plot from a DataFrame with four columns of randomly generated data.\n\nYou can also generate groupings if you specify the by parameter (which can\ntake a column name, or a list or tuple of column names):\n\nChanged in version 1.4.0.\n\n"}, {"name": "pandas.DataFrame.plot.density", "path": "reference/api/pandas.dataframe.plot.density", "type": "DataFrame", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.DataFrame.plot.hexbin", "path": "reference/api/pandas.dataframe.plot.hexbin", "type": "DataFrame", "text": "\nGenerate a hexagonal binning plot.\n\nGenerate a hexagonal binning plot of x versus y. If C is None (the default),\nthis is a histogram of the number of occurrences of the observations at\n`(x[i], y[i])`.\n\nIf C is specified, specifies values at given coordinates `(x[i], y[i])`. These\nvalues are accumulated for each hexagonal bin and then reduced according to\nreduce_C_function, having as default the NumPy\u2019s mean function\n(`numpy.mean()`). (If C is specified, it must also be a 1-D sequence of the\nsame length as x and y, or a column label.)\n\nThe column label or position for x points.\n\nThe column label or position for y points.\n\nThe column label or position for the value of (x, y) point.\n\nFunction of one argument that reduces all the values in a bin to a single\nnumber (e.g. np.mean, np.max, np.sum, np.std).\n\nThe number of hexagons in the x-direction. The corresponding number of\nhexagons in the y-direction is chosen in a way that the hexagons are\napproximately regular. Alternatively, gridsize can be a tuple with two\nelements specifying the number of hexagons in the x-direction and the\ny-direction.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nThe matplotlib `Axes` on which the hexbin is plotted.\n\nSee also\n\nMake plots of a DataFrame.\n\nHexagonal binning plot using matplotlib, the matplotlib function that is used\nunder the hood.\n\nExamples\n\nThe following examples are generated with random data from a normal\ndistribution.\n\nThe next example uses C and np.sum as reduce_C_function. Note that\n\u2018observations\u2019 values ranges from 1 to 5 but the result plot shows values up\nto more than 25. This is because of the reduce_C_function.\n\n"}, {"name": "pandas.DataFrame.plot.hist", "path": "reference/api/pandas.dataframe.plot.hist", "type": "DataFrame", "text": "\nDraw one histogram of the DataFrame\u2019s columns.\n\nA histogram is a representation of the distribution of data. This function\ngroups the values of all given Series in the DataFrame into bins and draws all\nbins in one `matplotlib.axes.Axes`. This is useful when the DataFrame\u2019s Series\nare in a similar scale.\n\nColumn in the DataFrame to group by.\n\nChanged in version 1.4.0: Previously, by is silently ignore and makes no\ngroupings\n\nNumber of histogram bins to be used.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nReturn a histogram plot.\n\nSee also\n\nDraw histograms per DataFrame\u2019s Series.\n\nDraw a histogram with Series\u2019 data.\n\nExamples\n\nWhen we roll a die 6000 times, we expect to get each value around 1000 times.\nBut when we roll two dice and sum the result, the distribution is going to be\nquite different. A histogram illustrates those distributions.\n\nA grouped histogram can be generated by providing the parameter by (which can\nbe a column name, or a list of column names):\n\n"}, {"name": "pandas.DataFrame.plot.kde", "path": "reference/api/pandas.dataframe.plot.kde", "type": "DataFrame", "text": "\nGenerate Kernel Density Estimate plot using Gaussian kernels.\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to\nestimate the probability density function (PDF) of a random variable. This\nfunction uses Gaussian kernels and includes automatic bandwidth determination.\n\nThe method used to calculate the estimator bandwidth. This can be \u2018scott\u2019,\n\u2018silverman\u2019, a scalar constant or a callable. If None (default), \u2018scott\u2019 is\nused. See `scipy.stats.gaussian_kde` for more information.\n\nEvaluation points for the estimated PDF. If None (default), 1000 equally\nspaced points are used. If ind is a NumPy array, the KDE is evaluated at the\npoints passed. If ind is an integer, ind number of equally spaced points are\nused.\n\nAdditional keyword arguments are documented in `pandas.%(this-\ndatatype)s.plot()`.\n\nSee also\n\nRepresentation of a kernel-density estimate using Gaussian kernels. This is\nthe function used internally to estimate the PDF.\n\nExamples\n\nGiven a Series of points randomly sampled from an unknown distribution,\nestimate its PDF using KDE with automatic bandwidth determination and plot the\nresults, evaluating them at 1000 equally spaced points (default):\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\nFor DataFrame, it works in the same way:\n\nA scalar bandwidth can be specified. Using a small bandwidth value can lead to\nover-fitting, while using a large bandwidth value may result in under-fitting:\n\nFinally, the ind parameter determines the evaluation points for the plot of\nthe estimated PDF:\n\n"}, {"name": "pandas.DataFrame.plot.line", "path": "reference/api/pandas.dataframe.plot.line", "type": "DataFrame", "text": "\nPlot Series or DataFrame as lines.\n\nThis function is useful to plot lines using DataFrame\u2019s values as coordinates.\n\nAllows plotting of one column versus another. If not specified, the index of\nthe DataFrame is used.\n\nAllows plotting of one column versus another. If not specified, all numerical\ncolumns are used.\n\nThe color for each of the DataFrame\u2019s columns. Possible values are:\n\nfor instance \u2018red\u2019 or \u2018#a98d19\u2019.\n\ncode, which will be used for each column recursively. For instance\n[\u2018green\u2019,\u2019yellow\u2019] each column\u2019s line will be filled in green or yellow,\nalternatively. If there is only a single column to be plotted, then only the\nfirst color from the color list will be used.\n\ncolored accordingly. For example, if your columns are called a and b, then\npassing {\u2018a\u2019: \u2018green\u2019, \u2018b\u2019: \u2018red\u2019} will color lines for column a in green and\nlines for column b in red.\n\nNew in version 1.1.0.\n\nAdditional keyword arguments are documented in `DataFrame.plot()`.\n\nAn ndarray is returned with one `matplotlib.axes.Axes` per column when\n`subplots=True`.\n\nSee also\n\nPlot y versus x as lines and/or markers.\n\nExamples\n\nThe following example shows the populations for some animals over the years.\n\nAn example with subplots, so an array of axes is returned.\n\nLet\u2019s repeat the same example, but specifying colors for each column (in this\ncase, for each animal).\n\nThe following example shows the relationship between both populations.\n\n"}, {"name": "pandas.DataFrame.plot.pie", "path": "reference/api/pandas.dataframe.plot.pie", "type": "DataFrame", "text": "\nGenerate a pie plot.\n\nA pie plot is a proportional representation of the numerical data in a column.\nThis function wraps `matplotlib.pyplot.pie()` for the specified column. If no\ncolumn reference is passed and `subplots=True` a pie plot is drawn for each\nnumerical column independently.\n\nLabel or position of the column to plot. If not provided, `subplots=True`\nargument must be passed.\n\nKeyword arguments to pass on to `DataFrame.plot()`.\n\nA NumPy array is returned when subplots is True.\n\nSee also\n\nGenerate a pie plot for a Series.\n\nMake plots of a DataFrame.\n\nExamples\n\nIn the example below we have a DataFrame with the information about planet\u2019s\nmass and radius. We pass the \u2018mass\u2019 column to the pie function to get a pie\nplot.\n\n"}, {"name": "pandas.DataFrame.plot.scatter", "path": "reference/api/pandas.dataframe.plot.scatter", "type": "DataFrame", "text": "\nCreate a scatter plot with varying marker point size and color.\n\nThe coordinates of each point are defined by two dataframe columns and filled\ncircles are used to represent each point. This kind of plot is useful to see\ncomplex correlations between two variables. Points could be for instance\nnatural 2D coordinates like longitude and latitude in a map or, in general,\nany pair of metrics that can be plotted against each other.\n\nThe column name or column position to be used as horizontal coordinates for\neach point.\n\nThe column name or column position to be used as vertical coordinates for each\npoint.\n\nThe size of each point. Possible values are:\n\nA string with the name of the column to be used for marker\u2019s size.\n\nA single scalar so all points have the same size.\n\nA sequence of scalars, which will be used for each point\u2019s size recursively.\nFor instance, when passing [2,14] all points size will be either 2 or 14,\nalternatively.\n\nChanged in version 1.1.0.\n\nThe color of each point. Possible values are:\n\nA single color string referred to by name, RGB or RGBA code, for instance\n\u2018red\u2019 or \u2018#a98d19\u2019.\n\nA sequence of color strings referred to by name, RGB or RGBA code, which will\nbe used for each point\u2019s color recursively. For instance [\u2018green\u2019,\u2019yellow\u2019]\nall points will be filled in green or yellow, alternatively.\n\nA column name or position whose values will be used to color the marker points\naccording to a colormap.\n\nKeyword arguments to pass on to `DataFrame.plot()`.\n\nSee also\n\nScatter plot using multiple input data formats.\n\nExamples\n\nLet\u2019s see how to draw a scatter plot using coordinates from the values in a\nDataFrame\u2019s columns.\n\nAnd now with the color determined by a column as well.\n\n"}, {"name": "pandas.DataFrame.pop", "path": "reference/api/pandas.dataframe.pop", "type": "DataFrame", "text": "\nReturn item and drop from frame. Raise KeyError if not found.\n\nLabel of column to be popped.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.pow", "path": "reference/api/pandas.dataframe.pow", "type": "DataFrame", "text": "\nGet Exponential power of dataframe and other, element-wise (binary operator\npow).\n\nEquivalent to `dataframe ** other`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version, rpow.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.prod", "path": "reference/api/pandas.dataframe.prod", "type": "DataFrame", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.product", "path": "reference/api/pandas.dataframe.product", "type": "DataFrame", "text": "\nReturn the product of the values over the requested axis.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the product of an empty or all-NA Series is `1`\n\nThis can be controlled with the `min_count` parameter\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.quantile", "path": "reference/api/pandas.dataframe.quantile", "type": "DataFrame", "text": "\nReturn values at the given quantile over requested axis.\n\nValue between 0 <= q <= 1, the quantile(s) to compute.\n\nEquals 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for column-wise.\n\nIf False, the quantile of datetime and timedelta data will be computed as\nwell.\n\nThis optional parameter specifies the interpolation method to use, when the\ndesired quantile lies between two data points i and j:\n\nlinear: i + (j - i) * fraction, where fraction is the fractional part of the\nindex surrounded by i and j.\n\nlower: i.\n\nhigher: j.\n\nnearest: i or j whichever is nearest.\n\nmidpoint: (i \\+ j) / 2.\n\nindex is `q`, the columns are the columns of self, and the values are the\nquantiles.\n\nindex is the columns of self and the values are the quantiles.\n\nSee also\n\nRolling quantile.\n\nNumpy function to compute the percentile.\n\nExamples\n\nSpecifying numeric_only=False will also compute the quantile of datetime and\ntimedelta data.\n\n"}, {"name": "pandas.DataFrame.query", "path": "reference/api/pandas.dataframe.query", "type": "DataFrame", "text": "\nQuery the columns of a DataFrame with a boolean expression.\n\nThe query string to evaluate.\n\nYou can refer to variables in the environment by prefixing them with an \u2018@\u2019\ncharacter like `@a + b`.\n\nYou can refer to column names that are not valid Python variable names by\nsurrounding them in backticks. Thus, column names containing spaces or\npunctuations (besides underscores) or starting with digits must be surrounded\nby backticks. (For example, a column named \u201cArea (cm^2)\u201d would be referenced\nas ``Area (cm^2)``). Column names which are Python keywords (like \u201clist\u201d,\n\u201cfor\u201d, \u201cimport\u201d, etc) cannot be used.\n\nFor example, if one of your columns is called `a a` and you want to sum it\nwith `b`, your query should be ``a a` + b`.\n\nNew in version 0.25.0: Backtick quoting introduced.\n\nNew in version 1.0.0: Expanding functionality of backtick quoting for more\nthan only spaces.\n\nWhether the query should modify the data in place or return a modified copy.\n\nSee the documentation for `eval()` for complete details on the keyword\narguments accepted by `DataFrame.query()`.\n\nDataFrame resulting from the provided query expression or None if\n`inplace=True`.\n\nSee also\n\nEvaluate a string describing operations on DataFrame columns.\n\nEvaluate a string describing operations on DataFrame columns.\n\nNotes\n\nThe result of the evaluation of this expression is first passed to\n`DataFrame.loc` and if that fails because of a multidimensional key (e.g., a\nDataFrame) then the result will be passed to `DataFrame.__getitem__()`.\n\nThis method uses the top-level `eval()` function to evaluate the passed query.\n\nThe `query()` method uses a slightly modified Python syntax by default. For\nexample, the `&` and `|` (bitwise) operators have the precedence of their\nboolean cousins, `and` and `or`. This is syntactically valid Python, however\nthe semantics are different.\n\nYou can change the semantics of the expression by passing the keyword argument\n`parser='python'`. This enforces the same semantics as evaluation in Python\nspace. Likewise, you can pass `engine='python'` to evaluate an expression\nusing Python itself as a backend. This is not recommended as it is inefficient\ncompared to using `numexpr` as the engine.\n\nThe `DataFrame.index` and `DataFrame.columns` attributes of the `DataFrame`\ninstance are placed in the query namespace by default, which allows you to\ntreat both the index and columns of the frame as a column in the frame. The\nidentifier `index` is used for the frame index; you can also use the name of\nthe index to identify it in a query. Please note that Python keywords may not\nbe used as identifiers.\n\nFor further details and examples see the `query` documentation in indexing.\n\nBacktick quoted variables\n\nBacktick quoted variables are parsed as literal Python code and are converted\ninternally to a Python valid identifier. This can lead to the following\nproblems.\n\nDuring parsing a number of disallowed characters inside the backtick quoted\nstring are replaced by strings that are allowed as a Python identifier. These\ncharacters include all operators in Python, the space character, the question\nmark, the exclamation mark, the dollar sign, and the euro sign. For other\ncharacters that fall outside the ASCII range (U+0001..U+007F) and those that\nare not further specified in PEP 3131, the query parser will raise an error.\nThis excludes whitespace different than the space character, but also the\nhashtag (as it is used for comments) and the backtick itself (backtick can\nalso not be escaped).\n\nIn a special case, quotes that make a pair around a backtick can confuse the\nparser. For example, ``it's` > `that's`` will raise an error, as it forms a\nquoted string (`'s > `that'`) with a backtick inside.\n\nSee also the Python documentation about lexical analysis\n(https://docs.python.org/3/reference/lexical_analysis.html) in combination\nwith the source code in `pandas.core.computation.parsing`.\n\nExamples\n\nThe previous expression is equivalent to\n\nFor columns with spaces in their name, you can use backtick quoting.\n\nThe previous expression is equivalent to\n\n"}, {"name": "pandas.DataFrame.radd", "path": "reference/api/pandas.dataframe.radd", "type": "DataFrame", "text": "\nGet Addition of dataframe and other, element-wise (binary operator radd).\n\nEquivalent to `other + dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, add.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rank", "path": "reference/api/pandas.dataframe.rank", "type": "DataFrame", "text": "\nCompute numerical data ranks (1 through n) along axis.\n\nBy default, equal values are assigned a rank that is the average of the ranks\nof those values.\n\nIndex to direct ranking.\n\nHow to rank the group of records that have the same value (i.e. ties):\n\naverage: average rank of the group\n\nmin: lowest rank in the group\n\nmax: highest rank in the group\n\nfirst: ranks assigned in order they appear in the array\n\ndense: like \u2018min\u2019, but rank always increases by 1 between groups.\n\nFor DataFrame objects, rank only numeric columns if set to True.\n\nHow to rank NaN values:\n\nkeep: assign NaN rank to NaN values\n\ntop: assign lowest rank to NaN values\n\nbottom: assign highest rank to NaN values\n\nWhether or not the elements should be ranked in ascending order.\n\nWhether or not to display the returned rankings in percentile form.\n\nReturn a Series or DataFrame with data ranks as values.\n\nSee also\n\nRank of values within each group.\n\nExamples\n\nThe following example shows how the method behaves with the above parameters:\n\ndefault_rank: this is the default behaviour obtained without using any\nparameter.\n\nmax_rank: setting `method = 'max'` the records that have the same values are\nranked using the highest rank (e.g.: since \u2018cat\u2019 and \u2018dog\u2019 are both in the 2nd\nand 3rd position, rank 3 is assigned.)\n\nNA_bottom: choosing `na_option = 'bottom'`, if there are records with NaN\nvalues they are placed at the bottom of the ranking.\n\npct_rank: when setting `pct = True`, the ranking is expressed as percentile\nrank.\n\n"}, {"name": "pandas.DataFrame.rdiv", "path": "reference/api/pandas.dataframe.rdiv", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, truediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.reindex", "path": "reference/api/pandas.dataframe.reindex", "type": "DataFrame", "text": "\nConform Series/DataFrame to new index with optional filling logic.\n\nPlaces NA/NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\n`copy=False`.\n\nNew labels / index to conform to, should be specified using keywords.\nPreferably an Index object to avoid duplicating data.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: Propagate last valid observation forward to next valid.\n\nbackfill / bfill: Use next valid observation to fill gap.\n\nnearest: Use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nValue to use for missing values. Defaults to NaN, but can be any \u201ccompatible\u201d\nvalue.\n\nMaximum number of consecutive elements to forward or backward fill.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations most satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to same indices as other DataFrame.\n\nExamples\n\n`DataFrame.reindex` supports two calling conventions\n\n`(index=index_labels, columns=column_labels, ...)`\n\n`(labels, axis={'index', 'columns'}, ...)`\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nCreate a dataframe with some fictional data.\n\nCreate a new index and reindex the dataframe. By default values in the new\nindex that do not have corresponding records in the dataframe are assigned\n`NaN`.\n\nWe can fill in the missing values by passing a value to the keyword\n`fill_value`. Because the index is not monotonically increasing or decreasing,\nwe cannot use arguments to the keyword `method` to fill the `NaN` values.\n\nWe can also reindex the columns.\n\nOr we can use \u201caxis-style\u201d keyword arguments\n\nTo further illustrate the filling functionality in `reindex`, we will create a\ndataframe with a monotonically increasing index (for example, a sequence of\ndates).\n\nSuppose we decide to expand the dataframe to cover a wider date range.\n\nThe index entries that did not have a value in the original data frame (for\nexample, \u20182009-12-29\u2019) are by default filled with `NaN`. If desired, we can\nfill in the missing values using one of several options.\n\nFor example, to back-propagate the last valid value to fill the `NaN` values,\npass `bfill` as an argument to the `method` keyword.\n\nPlease note that the `NaN` value present in the original dataframe (at index\nvalue 2010-01-03) will not be filled by any of the value propagation schemes.\nThis is because filling while reindexing does not look at dataframe values,\nbut only compares the original and desired indexes. If you do want to fill in\nthe `NaN` values present in the original dataframe, use the `fillna()` method.\n\nSee the user guide for more.\n\n"}, {"name": "pandas.DataFrame.reindex_like", "path": "reference/api/pandas.dataframe.reindex_like", "type": "DataFrame", "text": "\nReturn an object with matching indices as other object.\n\nConform the object to the same index on all axes. Optional filling logic,\nplacing NaN in locations having no value in the previous index. A new object\nis produced unless the new index is equivalent to the current one and\ncopy=False.\n\nIts row and column indices are used to define the new indices of this object.\n\nMethod to use for filling holes in reindexed DataFrame. Please note: this is\nonly applicable to DataFrames/Series with a monotonically\nincreasing/decreasing index.\n\nNone (default): don\u2019t fill gaps\n\npad / ffill: propagate last valid observation forward to next valid\n\nbackfill / bfill: use next valid observation to fill gap\n\nnearest: use nearest valid observations to fill gap.\n\nReturn a new object, even if the passed indexes are the same.\n\nMaximum number of consecutive labels to fill for inexact matches.\n\nMaximum distance between original and new labels for inexact matches. The\nvalues of the index at the matching locations must satisfy the equation\n`abs(index[indexer] - target) <= tolerance`.\n\nTolerance may be a scalar value, which applies the same tolerance to all\nvalues, or list-like, which applies variable tolerance per element. List-like\nincludes list, tuple, array, Series, and must be the same size as the index\nand its dtype must exactly match the index\u2019s type.\n\nSame type as caller, but with changed indices on each axis.\n\nSee also\n\nSet row labels.\n\nRemove row labels or move them to new columns.\n\nChange to new indices or expand indices.\n\nNotes\n\nSame as calling `.reindex(index=other.index, columns=other.columns,...)`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.rename", "path": "reference/api/pandas.dataframe.rename", "type": "DataFrame", "text": "\nAlter axes labels.\n\nFunction / dict values must be unique (1-to-1). Labels not contained in a dict\n/ Series will be left as-is. Extra labels listed don\u2019t throw an error.\n\nSee the user guide for more.\n\nDict-like or function transformations to apply to that axis\u2019 values. Use\neither `mapper` and `axis` to specify the axis to target with `mapper`, or\n`index` and `columns`.\n\nAlternative to specifying axis (`mapper, axis=0` is equivalent to\n`index=mapper`).\n\nAlternative to specifying axis (`mapper, axis=1` is equivalent to\n`columns=mapper`).\n\nAxis to target with `mapper`. Can be either the axis name (\u2018index\u2019, \u2018columns\u2019)\nor number (0, 1). The default is \u2018index\u2019.\n\nAlso copy underlying data.\n\nWhether to return a new DataFrame. If True then value of copy is ignored.\n\nIn case of a MultiIndex, only rename labels in the specified level.\n\nIf \u2018raise\u2019, raise a KeyError when a dict-like mapper, index, or columns\ncontains labels that are not present in the Index being transformed. If\n\u2018ignore\u2019, existing keys will be renamed and extra keys will be ignored.\n\nDataFrame with the renamed axis labels or None if `inplace=True`.\n\nIf any of the labels is not found in the selected axis and \u201cerrors=\u2019raise\u2019\u201d.\n\nSee also\n\nSet the name of the axis.\n\nExamples\n\n`DataFrame.rename` supports two calling conventions\n\n`(index=index_mapper, columns=columns_mapper, ...)`\n\n`(mapper, axis={'index', 'columns'}, ...)`\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nRename columns using a mapping:\n\nRename index using a mapping:\n\nCast index labels to a different type:\n\nUsing axis-style parameters:\n\n"}, {"name": "pandas.DataFrame.rename_axis", "path": "reference/api/pandas.dataframe.rename_axis", "type": "DataFrame", "text": "\nSet the name of the axis for the index or columns.\n\nValue to set the axis name attribute.\n\nA scalar, list-like, dict-like or functions transformations to apply to that\naxis\u2019 values. Note that the `columns` parameter is not allowed if the object\nis a Series. This parameter only apply for DataFrame type objects.\n\nUse either `mapper` and `axis` to specify the axis to target with `mapper`, or\n`index` and/or `columns`.\n\nThe axis to rename.\n\nAlso copy underlying data.\n\nModifies the object directly, instead of creating a new Series or DataFrame.\n\nThe same type as the caller or None if `inplace=True`.\n\nSee also\n\nAlter Series index labels or name.\n\nAlter DataFrame index labels or name.\n\nSet new names on index.\n\nNotes\n\n`DataFrame.rename_axis` supports two calling conventions\n\n`(index=index_mapper, columns=columns_mapper, ...)`\n\n`(mapper, axis={'index', 'columns'}, ...)`\n\nThe first calling convention will only modify the names of the index and/or\nthe names of the Index object that is the columns. In this case, the parameter\n`copy` is ignored.\n\nThe second calling convention will modify the names of the corresponding index\nif mapper is a list or a scalar. However, if mapper is dict-like or a\nfunction, it will use the deprecated behavior of modifying the axis labels.\n\nWe highly recommend using keyword arguments to clarify your intent.\n\nExamples\n\nSeries\n\nDataFrame\n\nMultiIndex\n\n"}, {"name": "pandas.DataFrame.reorder_levels", "path": "reference/api/pandas.dataframe.reorder_levels", "type": "DataFrame", "text": "\nRearrange index levels using input order. May not drop or duplicate levels.\n\nList representing new level order. Reference level by number (position) or by\nkey (label).\n\nWhere to reorder levels.\n\nExamples\n\nLet\u2019s reorder the levels of the index:\n\n"}, {"name": "pandas.DataFrame.replace", "path": "reference/api/pandas.dataframe.replace", "type": "DataFrame", "text": "\nReplace values given in to_replace with value.\n\nValues of the DataFrame are replaced with other values dynamically.\n\nThis differs from updating with `.loc` or `.iloc`, which require you to\nspecify a location to update with some value.\n\nHow to find the values that will be replaced.\n\nnumeric, str or regex:\n\nnumeric: numeric values equal to to_replace will be replaced with value\n\nstr: string exactly matching to_replace will be replaced with value\n\nregex: regexs matching to_replace will be replaced with value\n\nlist of str, regex, or numeric:\n\nFirst, if to_replace and value are both lists, they must be the same length.\n\nSecond, if `regex=True` then all of the strings in both lists will be\ninterpreted as regexs otherwise they will match directly. This doesn\u2019t matter\nmuch for value since there are only a few possible substitution regexes you\ncan use.\n\nstr, regex and numeric rules apply as above.\n\ndict:\n\nDicts can be used to specify different replacement values for different\nexisting values. For example, `{'a': 'b', 'y': 'z'}` replaces the value \u2018a\u2019\nwith \u2018b\u2019 and \u2018y\u2019 with \u2018z\u2019. To use a dict in this way the value parameter\nshould be None.\n\nFor a DataFrame a dict can specify that different values should be replaced in\ndifferent columns. For example, `{'a': 1, 'b': 'z'}` looks for the value 1 in\ncolumn \u2018a\u2019 and the value \u2018z\u2019 in column \u2018b\u2019 and replaces these values with\nwhatever is specified in value. The value parameter should not be `None` in\nthis case. You can treat this as a special case of passing two lists except\nthat you are specifying the column to search in.\n\nFor a DataFrame nested dictionaries, e.g., `{'a': {'b': np.nan}}`, are read as\nfollows: look in column \u2018a\u2019 for the value \u2018b\u2019 and replace it with NaN. The\nvalue parameter should be `None` to use a nested dict in this way. You can\nnest regular expressions as well. Note that column names (the top-level\ndictionary keys in a nested dictionary) cannot be regular expressions.\n\nNone:\n\nThis means that the regex argument must be a string, compiled regular\nexpression, or list, dict, ndarray or Series of such elements. If value is\nalso `None` then this must be a nested dictionary or Series.\n\nSee the examples section for examples of each of these.\n\nValue to replace any values matching to_replace with. For a DataFrame a dict\nof values can be used to specify which value to use for each column (columns\nnot in the dict will not be filled). Regular expressions, strings and lists or\ndicts of such objects are also allowed.\n\nIf True, performs operation inplace and returns None.\n\nMaximum size gap to forward or backward fill.\n\nWhether to interpret to_replace and/or value as regular expressions. If this\nis `True` then to_replace must be a string. Alternatively, this could be a\nregular expression or a list, dict, or array of regular expressions in which\ncase to_replace must be `None`.\n\nThe method to use when for replacement, when to_replace is a scalar, list or\ntuple and value is `None`.\n\nChanged in version 0.23.0: Added to DataFrame.\n\nObject after replacement.\n\nIf regex is not a `bool` and to_replace is not `None`.\n\nIf to_replace is not a scalar, array-like, `dict`, or `None`\n\nIf to_replace is a `dict` and value is not a `list`, `dict`, `ndarray`, or\n`Series`\n\nIf to_replace is `None` and regex is not compilable into a regular expression\nor is a list, dict, ndarray, or Series.\n\nWhen replacing multiple `bool` or `datetime64` objects and the arguments to\nto_replace does not match the type of the value being replaced\n\nIf a `list` or an `ndarray` is passed to to_replace and value but they are not\nthe same length.\n\nSee also\n\nFill NA values.\n\nReplace values based on boolean condition.\n\nSimple string replacement.\n\nNotes\n\nRegex substitution is performed under the hood with `re.sub`. The rules for\nsubstitution for `re.sub` are the same.\n\nRegular expressions will only substitute on strings, meaning you cannot\nprovide, for example, a regular expression matching floating point numbers and\nexpect the columns in your frame that have a numeric dtype to be matched.\nHowever, if those floating point numbers are strings, then you can do this.\n\nThis method has a lot of options. You are encouraged to experiment and play\nwith this method to gain intuition about how it works.\n\nWhen dict is used as the to_replace value, it is like key(s) in the dict are\nthe to_replace part and value(s) in the dict are the value parameter.\n\nExamples\n\nScalar `to_replace` and `value`\n\nList-like `to_replace`\n\ndict-like `to_replace`\n\nRegular expression `to_replace`\n\nCompare the behavior of `s.replace({'a': None})` and `s.replace('a', None)` to\nunderstand the peculiarities of the to_replace parameter:\n\nWhen one uses a dict as the to_replace value, it is like the value(s) in the\ndict are equal to the value parameter. `s.replace({'a': None})` is equivalent\nto `s.replace(to_replace={'a': None}, value=None, method=None)`:\n\nWhen `value` is not explicitly passed and to_replace is a scalar, list or\ntuple, replace uses the method parameter (default \u2018pad\u2019) to do the\nreplacement. So this is why the \u2018a\u2019 values are being replaced by 10 in rows 1\nand 2 and \u2018b\u2019 in row 4 in this case.\n\nOn the other hand, if `None` is explicitly passed for `value`, it will be\nrespected:\n\nChanged in version 1.4.0: Previously the explicit `None` was silently ignored.\n\n"}, {"name": "pandas.DataFrame.resample", "path": "reference/api/pandas.dataframe.resample", "type": "DataFrame", "text": "\nResample time-series data.\n\nConvenience method for frequency conversion and resampling of time series. The\nobject must have a datetime-like index (DatetimeIndex, PeriodIndex, or\nTimedeltaIndex), or the caller must pass the label of a datetime-like\nseries/index to the `on`/`level` keyword parameter.\n\nThe offset string or object representing target conversion.\n\nWhich axis to use for up- or down-sampling. For Series this will default to 0,\ni.e. along the rows. Must be DatetimeIndex, TimedeltaIndex or PeriodIndex.\n\nWhich side of bin interval is closed. The default is \u2018left\u2019 for all frequency\noffsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which all have a\ndefault of \u2018right\u2019.\n\nWhich bin edge label to label bucket with. The default is \u2018left\u2019 for all\nfrequency offsets except for \u2018M\u2019, \u2018A\u2019, \u2018Q\u2019, \u2018BM\u2019, \u2018BA\u2019, \u2018BQ\u2019, and \u2018W\u2019 which\nall have a default of \u2018right\u2019.\n\nFor PeriodIndex only, controls whether to use the start or end of rule.\n\nPass \u2018timestamp\u2019 to convert the resulting index to a DateTimeIndex or \u2018period\u2019\nto convert it to a PeriodIndex. By default the input representation is\nretained.\n\nAdjust the resampled time labels.\n\nDeprecated since version 1.1.0: You should add the loffset to the df.index\nafter the resample. See below.\n\nFor frequencies that evenly subdivide 1 day, the \u201corigin\u201d of the aggregated\nintervals. For example, for \u20185min\u2019 frequency, base could range from 0 through\n4. Defaults to 0.\n\nDeprecated since version 1.1.0: The new arguments that you should use are\n\u2018offset\u2019 or \u2018origin\u2019.\n\nFor a DataFrame, column to use instead of index for resampling. Column must be\ndatetime-like.\n\nFor a MultiIndex, level (name or number) to use for resampling. level must be\ndatetime-like.\n\nThe timestamp on which to adjust the grouping. The timezone of origin must\nmatch the timezone of the index. If string, must be one of the following:\n\n\u2018epoch\u2019: origin is 1970-01-01\n\n\u2018start\u2019: origin is the first value of the timeseries\n\n\u2018start_day\u2019: origin is the first day at midnight of the timeseries\n\nNew in version 1.1.0.\n\n\u2018end\u2019: origin is the last value of the timeseries\n\n\u2018end_day\u2019: origin is the ceiling midnight of the last day\n\nNew in version 1.3.0.\n\nAn offset timedelta added to the origin.\n\nNew in version 1.1.0.\n\n`Resampler` object.\n\nSee also\n\nResample a Series.\n\nResample a DataFrame.\n\nGroup DataFrame by mapping, function, label, or list of labels.\n\nReindex a DataFrame with the given frequency without grouping.\n\nNotes\n\nSee the user guide for more.\n\nTo learn more about the offset strings, please see this link.\n\nExamples\n\nStart by creating a series with 9 one minute timestamps.\n\nDownsample the series into 3 minute bins and sum the values of the timestamps\nfalling into a bin.\n\nDownsample the series into 3 minute bins as above, but label each bin using\nthe right edge instead of the left. Please note that the value in the bucket\nused as the label is not included in the bucket, which it labels. For example,\nin the original series the bucket `2000-01-01 00:03:00` contains the value 3,\nbut the summed value in the resampled bucket with the label `2000-01-01\n00:03:00` does not include 3 (if it did, the summed value would be 6, not 3).\nTo include this value close the right side of the bin interval as illustrated\nin the example below this one.\n\nDownsample the series into 3 minute bins as above, but close the right side of\nthe bin interval.\n\nUpsample the series into 30 second bins.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`pad` method.\n\nUpsample the series into 30 second bins and fill the `NaN` values using the\n`bfill` method.\n\nPass a custom function via `apply`\n\nFor a Series with a PeriodIndex, the keyword convention can be used to control\nwhether to use the start or end of rule.\n\nResample a year by quarter using \u2018start\u2019 convention. Values are assigned to\nthe first quarter of the period.\n\nResample quarters by month using \u2018end\u2019 convention. Values are assigned to the\nlast month of the period.\n\nFor DataFrame objects, the keyword on can be used to specify the column\ninstead of the index for resampling.\n\nFor a DataFrame with MultiIndex, the keyword level can be used to specify on\nwhich level the resampling needs to take place.\n\nIf you want to adjust the start of the bins based on a fixed timestamp:\n\nIf you want to adjust the start of the bins with an offset Timedelta, the two\nfollowing lines are equivalent:\n\nIf you want to take the largest Timestamp as the end of the bins:\n\nIn contrast with the start_day, you can use end_day to take the ceiling\nmidnight of the largest Timestamp as the end of the bins and drop the bins not\ncontaining data:\n\nTo replace the use of the deprecated base argument, you can now use offset, in\nthis example it is equivalent to have base=2:\n\nTo replace the use of the deprecated loffset argument:\n\n"}, {"name": "pandas.DataFrame.reset_index", "path": "reference/api/pandas.dataframe.reset_index", "type": "DataFrame", "text": "\nReset the index, or a level of it.\n\nReset the index of the DataFrame, and use the default one instead. If the\nDataFrame has a MultiIndex, this method can remove one or more levels.\n\nOnly remove the given levels from the index. Removes all levels by default.\n\nDo not try to insert index into dataframe columns. This resets the index to\nthe default integer index.\n\nModify the DataFrame in place (do not create a new object).\n\nIf the columns have multiple levels, determines which level the labels are\ninserted into. By default it is inserted into the first level.\n\nIf the columns have multiple levels, determines how the other levels are\nnamed. If None then the index name is repeated.\n\nDataFrame with the new index or None if `inplace=True`.\n\nSee also\n\nOpposite of reset_index.\n\nChange to new indices or expand indices.\n\nChange to same indices as other DataFrame.\n\nExamples\n\nWhen we reset the index, the old index is added as a column, and a new\nsequential index is used:\n\nWe can use the drop parameter to avoid the old index being added as a column:\n\nYou can also use reset_index with MultiIndex.\n\nIf the index has multiple levels, we can reset a subset of them:\n\nIf we are not dropping the index, by default, it is placed in the top level.\nWe can place it in another level:\n\nWhen the index is inserted under another level, we can specify under which one\nwith the parameter col_fill:\n\nIf we specify a nonexistent level for col_fill, it is created:\n\n"}, {"name": "pandas.DataFrame.rfloordiv", "path": "reference/api/pandas.dataframe.rfloordiv", "type": "DataFrame", "text": "\nGet Integer division of dataframe and other, element-wise (binary operator\nrfloordiv).\n\nEquivalent to `other // dataframe`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version,\nfloordiv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rmod", "path": "reference/api/pandas.dataframe.rmod", "type": "DataFrame", "text": "\nGet Modulo of dataframe and other, element-wise (binary operator rmod).\n\nEquivalent to `other % dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, mod.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rmul", "path": "reference/api/pandas.dataframe.rmul", "type": "DataFrame", "text": "\nGet Multiplication of dataframe and other, element-wise (binary operator\nrmul).\n\nEquivalent to `other * dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, mul.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rolling", "path": "reference/api/pandas.dataframe.rolling", "type": "DataFrame", "text": "\nProvide rolling window calculations.\n\nSize of the moving window.\n\nIf an integer, the fixed number of observations used for each window.\n\nIf an offset, the time period of each window. Each window will be a variable\nsized based on the observations included in the time-period. This is only\nvalid for datetimelike indexes. To learn more about the offsets & frequency\nstrings, please see this link.\n\nIf a BaseIndexer subclass, the window boundaries based on the defined\n`get_window_bounds` method. Additional rolling keyword arguments, namely\n`min_periods`, `center`, and `closed` will be passed to `get_window_bounds`.\n\nMinimum number of observations in window required to have a value; otherwise,\nresult is `np.nan`.\n\nFor a window that is specified by an offset, `min_periods` will default to 1.\n\nFor a window that is specified by an integer, `min_periods` will default to\nthe size of the window.\n\nIf False, set the window labels as the right edge of the window index.\n\nIf True, set the window labels as the center of the window index.\n\nIf `None`, all points are evenly weighted.\n\nIf a string, it must be a valid scipy.signal window function.\n\nCertain Scipy window types require additional parameters to be passed in the\naggregation function. The additional parameters must match the keywords\nspecified in the Scipy window type method signature.\n\nFor a DataFrame, a column label or Index level on which to calculate the\nrolling window, rather than the DataFrame\u2019s index.\n\nProvided integer column is ignored and excluded from result since an integer\nindex is not used to calculate the rolling window.\n\nIf `0` or `'index'`, roll across the rows.\n\nIf `1` or `'columns'`, roll across the columns.\n\nIf `'right'`, the first point in the window is excluded from calculations.\n\nIf `'left'`, the last point in the window is excluded from calculations.\n\nIf `'both'`, the no points in the window are excluded from calculations.\n\nIf `'neither'`, the first and last points in the window are excluded from\ncalculations.\n\nDefault `None` (`'right'`).\n\nChanged in version 1.2.0: The closed parameter with fixed windows is now\nsupported.\n\nNew in version 1.3.0.\n\nExecute the rolling operation per single column or row (`'single'`) or over\nthe entire object (`'table'`).\n\nThis argument is only implemented when specifying `engine='numba'` in the\nmethod call.\n\nSee also\n\nProvides expanding transformations.\n\nProvides exponential weighted functions.\n\nNotes\n\nSee Windowing Operations for further usage details and examples.\n\nExamples\n\nwindow\n\nRolling sum with a window length of 2 observations.\n\nRolling sum with a window span of 2 seconds.\n\nRolling sum with forward looking windows with 2 observations.\n\nmin_periods\n\nRolling sum with a window length of 2 observations, but only needs a minimum\nof 1 observation to calculate a value.\n\ncenter\n\nRolling sum with the result assigned to the center of the window index.\n\nwin_type\n\nRolling sum with a window length of 2, using the Scipy `'gaussian'` window\ntype. `std` is required in the aggregation function.\n\n"}, {"name": "pandas.DataFrame.round", "path": "reference/api/pandas.dataframe.round", "type": "DataFrame", "text": "\nRound a DataFrame to a variable number of decimal places.\n\nNumber of decimal places to round each column to. If an int is given, round\neach column to the same number of places. Otherwise dict and Series round to\nvariable numbers of places. Column names should be in the keys if decimals is\na dict-like, or in the index if decimals is a Series. Any columns not included\nin decimals will be left as is. Elements of decimals which are not columns of\nthe input will be ignored.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nAdditional keywords have no effect but might be accepted for compatibility\nwith numpy.\n\nA DataFrame with the affected columns rounded to the specified number of\ndecimal places.\n\nSee also\n\nRound a numpy array to the given number of decimals.\n\nRound a Series to the given number of decimals.\n\nExamples\n\nBy providing an integer each column is rounded to the same number of decimal\nplaces\n\nWith a dict, the number of places for specific columns can be specified with\nthe column names as key and the number of decimal places as value\n\nUsing a Series, the number of places for specific columns can be specified\nwith the column names as index and the number of decimal places as value\n\n"}, {"name": "pandas.DataFrame.rpow", "path": "reference/api/pandas.dataframe.rpow", "type": "DataFrame", "text": "\nGet Exponential power of dataframe and other, element-wise (binary operator\nrpow).\n\nEquivalent to `other ** dataframe`, but with support to substitute a\nfill_value for missing data in one of the inputs. With reverse version, pow.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rsub", "path": "reference/api/pandas.dataframe.rsub", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator rsub).\n\nEquivalent to `other - dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, sub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.rtruediv", "path": "reference/api/pandas.dataframe.rtruediv", "type": "DataFrame", "text": "\nGet Floating division of dataframe and other, element-wise (binary operator\nrtruediv).\n\nEquivalent to `other / dataframe`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, truediv.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.sample", "path": "reference/api/pandas.dataframe.sample", "type": "DataFrame", "text": "\nReturn a random sample of items from an axis of object.\n\nYou can use random_state for reproducibility.\n\nNumber of items from axis to return. Cannot be used with frac. Default = 1 if\nfrac = None.\n\nFraction of axis items to return. Cannot be used with n.\n\nAllow or disallow sampling of the same row more than once.\n\nDefault \u2018None\u2019 results in equal probability weighting. If passed a Series,\nwill align with target object on index. Index values in weights not found in\nsampled object will be ignored and index values in sampled object not in\nweights will be assigned weights of zero. If called on a DataFrame, will\naccept the name of a column when axis = 0. Unless weights are a Series,\nweights must be same length as axis being sampled. If weights do not sum to 1,\nthey will be normalized to sum to 1. Missing values in the weights column will\nbe treated as zero. Infinite values not allowed.\n\nIf int, array-like, or BitGenerator, seed for random number generator. If\nnp.random.RandomState or np.random.Generator, use as given.\n\nChanged in version 1.1.0: array-like and BitGenerator object now passed to\nnp.random.RandomState() as seed\n\nChanged in version 1.4.0: np.random.Generator objects now accepted\n\nAxis to sample. Accepts axis number or name. Default is stat axis for given\ndata type (0 for Series and DataFrames).\n\nIf True, the resulting index will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.3.0.\n\nA new object of same type as caller containing n items randomly sampled from\nthe caller object.\n\nSee also\n\nGenerates random samples from each group of a DataFrame object.\n\nGenerates random samples from each group of a Series object.\n\nGenerates a random sample from a given 1-D numpy array.\n\nNotes\n\nIf frac > 1, replacement should be set to True.\n\nExamples\n\nExtract 3 random elements from the `Series` `df['num_legs']`: Note that we use\nrandom_state to ensure the reproducibility of the examples.\n\nA random 50% sample of the `DataFrame` with replacement:\n\nAn upsample sample of the `DataFrame` with replacement: Note that replace\nparameter has to be True for frac parameter > 1.\n\nUsing a DataFrame column as weights. Rows with larger value in the\nnum_specimen_seen column are more likely to be sampled.\n\n"}, {"name": "pandas.DataFrame.select_dtypes", "path": "reference/api/pandas.dataframe.select_dtypes", "type": "General utility functions", "text": "\nReturn a subset of the DataFrame\u2019s columns based on the column dtypes.\n\nA selection of dtypes or strings to be included/excluded. At least one of\nthese parameters must be supplied.\n\nThe subset of the frame including the dtypes in `include` and excluding the\ndtypes in `exclude`.\n\nIf both of `include` and `exclude` are empty\n\nIf `include` and `exclude` have overlapping elements\n\nIf any kind of string dtype is passed in.\n\nSee also\n\nReturn Series with the data type of each column.\n\nNotes\n\nTo select all numeric types, use `np.number` or `'number'`\n\nTo select strings you must use the `object` dtype, but note that this will\nreturn all object dtype columns\n\nSee the numpy dtype hierarchy\n\nTo select datetimes, use `np.datetime64`, `'datetime'` or `'datetime64'`\n\nTo select timedeltas, use `np.timedelta64`, `'timedelta'` or `'timedelta64'`\n\nTo select Pandas categorical dtypes, use `'category'`\n\nTo select Pandas datetimetz dtypes, use `'datetimetz'` (new in 0.20.0) or\n`'datetime64[ns, tz]'`\n\nExamples\n\n"}, {"name": "pandas.DataFrame.sem", "path": "reference/api/pandas.dataframe.sem", "type": "DataFrame", "text": "\nReturn unbiased standard error of the mean over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\n"}, {"name": "pandas.DataFrame.set_axis", "path": "reference/api/pandas.dataframe.set_axis", "type": "DataFrame", "text": "\nAssign desired index to given axis.\n\nIndexes for column or row labels can be changed by assigning a list-like or\nIndex.\n\nThe values for the new index.\n\nThe axis to update. The value 0 identifies the rows, and 1 identifies the\ncolumns.\n\nWhether to return a new DataFrame instance.\n\nAn object of type DataFrame or None if `inplace=True`.\n\nSee also\n\nAlter the name of the index or columns.\n\nExamples\n\nChange the row labels.\n\nChange the column labels.\n\nNow, update the labels inplace.\n\n"}, {"name": "pandas.DataFrame.set_flags", "path": "reference/api/pandas.dataframe.set_flags", "type": "DataFrame", "text": "\nReturn a new object with updated flags.\n\nWhether the returned object allows duplicate labels.\n\nThe same type as the caller.\n\nSee also\n\nGlobal metadata applying to this dataset.\n\nGlobal flags applying to this object.\n\nNotes\n\nThis method returns a new object that\u2019s a view on the same data as the input.\nMutating the input or the output values will be reflected in the other.\n\nThis method is intended to be used in method chains.\n\n\u201cFlags\u201d differ from \u201cmetadata\u201d. Flags reflect properties of the pandas object\n(the Series or DataFrame). Metadata refer to properties of the dataset, and\nshould be stored in `DataFrame.attrs`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.set_index", "path": "reference/api/pandas.dataframe.set_index", "type": "DataFrame", "text": "\nSet the DataFrame index using existing columns.\n\nSet the DataFrame index (row labels) using one or more existing columns or\narrays (of the correct length). The index can replace the existing index or\nexpand on it.\n\nThis parameter can be either a single column key, a single array of the same\nlength as the calling DataFrame, or a list containing an arbitrary combination\nof column keys and arrays. Here, \u201carray\u201d encompasses `Series`, `Index`,\n`np.ndarray`, and instances of `Iterator`.\n\nDelete columns to be used as the new index.\n\nWhether to append columns to existing index.\n\nIf True, modifies the DataFrame in place (do not create a new object).\n\nCheck the new index for duplicates. Otherwise defer the check until necessary.\nSetting to False will improve the performance of this method.\n\nChanged row labels or None if `inplace=True`.\n\nSee also\n\nOpposite of set_index.\n\nChange to new indices or expand indices.\n\nChange to same indices as other DataFrame.\n\nExamples\n\nSet the index to become the \u2018month\u2019 column:\n\nCreate a MultiIndex using columns \u2018year\u2019 and \u2018month\u2019:\n\nCreate a MultiIndex using an Index and a column:\n\nCreate a MultiIndex using two Series:\n\n"}, {"name": "pandas.DataFrame.shape", "path": "reference/api/pandas.dataframe.shape", "type": "DataFrame", "text": "\nReturn a tuple representing the dimensionality of the DataFrame.\n\nSee also\n\nTuple of array dimensions.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.shift", "path": "reference/api/pandas.dataframe.shift", "type": "DataFrame", "text": "\nShift index by desired number of periods with an optional time freq.\n\nWhen freq is not passed, shift the index without realigning the data. If freq\nis passed (in this case, the index must be date or datetime, or it will raise\na NotImplementedError), the index will be increased using the periods and the\nfreq. freq can be inferred when specified as \u201cinfer\u201d as long as either freq or\ninferred_freq attribute is set in the index.\n\nNumber of periods to shift. Can be positive or negative.\n\nOffset to use from the tseries module or time rule (e.g. \u2018EOM\u2019). If freq is\nspecified then the index values are shifted but the data is not realigned.\nThat is, use freq if you would like to extend the index when shifting and\npreserve the original data. If freq is specified as \u201cinfer\u201d then it will be\ninferred from the freq or inferred_freq attributes of the index. If neither of\nthose attributes exist, a ValueError is thrown.\n\nShift direction.\n\nThe scalar value to use for newly introduced missing values. the default\ndepends on the dtype of self. For numeric data, `np.nan` is used. For\ndatetime, timedelta, or period data, etc. `NaT` is used. For extension dtypes,\n`self.dtype.na_value` is used.\n\nChanged in version 1.1.0.\n\nCopy of input object, shifted.\n\nSee also\n\nShift values of Index.\n\nShift values of DatetimeIndex.\n\nShift values of PeriodIndex.\n\nShift the time index, using the index\u2019s frequency if available.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.size", "path": "reference/api/pandas.dataframe.size", "type": "DataFrame", "text": "\nReturn an int representing the number of elements in this object.\n\nReturn the number of rows if Series. Otherwise return the number of rows times\nnumber of columns if DataFrame.\n\nSee also\n\nNumber of elements in the array.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.skew", "path": "reference/api/pandas.dataframe.skew", "type": "DataFrame", "text": "\nReturn unbiased skew over requested axis.\n\nNormalized by N-1.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nAdditional keyword arguments to be passed to the function.\n\n"}, {"name": "pandas.DataFrame.slice_shift", "path": "reference/api/pandas.dataframe.slice_shift", "type": "DataFrame", "text": "\nEquivalent to shift without copying data. The shifted data will not include\nthe dropped periods and the shifted axis will be smaller than the original.\n\nDeprecated since version 1.2.0: slice_shift is deprecated, use\nDataFrame/Series.shift instead.\n\nNumber of periods to move, can be positive or negative.\n\nNotes\n\nWhile the slice_shift is faster than shift, you may pay for it later during\nalignment.\n\n"}, {"name": "pandas.DataFrame.sort_index", "path": "reference/api/pandas.dataframe.sort_index", "type": "DataFrame", "text": "\nSort object by labels (along an axis).\n\nReturns a new DataFrame sorted by label if inplace argument is `False`,\notherwise updates the original DataFrame and returns None.\n\nThe axis along which to sort. The value 0 identifies the rows, and 1\nidentifies the columns.\n\nIf not None, sort on values in specified index level(s).\n\nSort ascending vs. descending. When the index is a MultiIndex the sort\ndirection can be controlled for each level individually.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\nmergesort and stable are the only stable algorithms. For DataFrames, this\noption is only applied when sorting on a single column or label.\n\nPuts NaNs at the beginning if first; last puts NaNs at the end. Not\nimplemented for MultiIndex.\n\nIf True and sorting by level and index is multilevel, sort by other levels too\n(in order) after sorting by specified level.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nIf not None, apply the key function to the index values before sorting. This\nis similar to the key argument in the builtin `sorted()` function, with the\nnotable difference that this key function should be vectorized. It should\nexpect an `Index` and return an `Index` of the same shape. For MultiIndex\ninputs, the key is applied per level.\n\nNew in version 1.1.0.\n\nThe original DataFrame sorted by the labels or None if `inplace=True`.\n\nSee also\n\nSort Series by the index.\n\nSort DataFrame by the value.\n\nSort Series by the value.\n\nExamples\n\nBy default, it sorts in ascending order, to sort in descending order, use\n`ascending=False`\n\nA key function can be specified which is applied to the index before sorting.\nFor a `MultiIndex` this is applied to each level separately.\n\n"}, {"name": "pandas.DataFrame.sort_values", "path": "reference/api/pandas.dataframe.sort_values", "type": "DataFrame", "text": "\nSort by the values along either axis.\n\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index\nlabels.\n\nAxis to be sorted.\n\nSort ascending vs. descending. Specify list for multiple sort orders. If this\nis a list of bools, must match the length of the by.\n\nIf True, perform operation in-place.\n\nChoice of sorting algorithm. See also `numpy.sort()` for more information.\nmergesort and stable are the only stable algorithms. For DataFrames, this\noption is only applied when sorting on a single column or label.\n\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nNew in version 1.0.0.\n\nApply the key function to the values before sorting. This is similar to the\nkey argument in the builtin `sorted()` function, with the notable difference\nthat this key function should be vectorized. It should expect a `Series` and\nreturn a Series with the same shape as the input. It will be applied to each\ncolumn in by independently.\n\nNew in version 1.1.0.\n\nDataFrame with sorted values or None if `inplace=True`.\n\nSee also\n\nSort a DataFrame by the index.\n\nSimilar method for a Series.\n\nExamples\n\nSort by col1\n\nSort by multiple columns\n\nSort Descending\n\nPutting NAs first\n\nSorting with a key function\n\nNatural sort with the key argument, using the natsort\n<https://github.com/SethMMorton/natsort> package.\n\n"}, {"name": "pandas.DataFrame.sparse", "path": "reference/api/pandas.dataframe.sparse", "type": "DataFrame", "text": "\nDataFrame accessor for sparse data.\n\nNew in version 0.25.0.\n\n"}, {"name": "pandas.DataFrame.sparse.density", "path": "reference/api/pandas.dataframe.sparse.density", "type": "DataFrame", "text": "\nRatio of non-sparse points to total (dense) data points.\n\n"}, {"name": "pandas.DataFrame.sparse.from_spmatrix", "path": "reference/api/pandas.dataframe.sparse.from_spmatrix", "type": "DataFrame", "text": "\nCreate a new DataFrame from a scipy sparse matrix.\n\nNew in version 0.25.0.\n\nMust be convertible to csc format.\n\nRow and column labels to use for the resulting DataFrame. Defaults to a\nRangeIndex.\n\nEach column of the DataFrame is stored as a `arrays.SparseArray`.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.sparse.to_coo", "path": "reference/api/pandas.dataframe.sparse.to_coo", "type": "DataFrame", "text": "\nReturn the contents of the frame as a sparse SciPy COO matrix.\n\nNew in version 0.25.0.\n\nIf the caller is heterogeneous and contains booleans or objects, the result\nwill be of dtype=object. See Notes.\n\nNotes\n\nThe dtype will be the lowest-common-denominator type (implicit upcasting);\nthat is to say if the dtypes (even of numeric types) are mixed, the one that\naccommodates all will be chosen.\n\ne.g. If the dtypes are float16 and float32, dtype will be upcast to float32.\nBy numpy.find_common_type convention, mixing int64 and and uint64 will result\nin a float64 dtype.\n\n"}, {"name": "pandas.DataFrame.sparse.to_dense", "path": "reference/api/pandas.dataframe.sparse.to_dense", "type": "DataFrame", "text": "\nConvert a DataFrame with sparse values to dense.\n\nNew in version 0.25.0.\n\nA DataFrame with the same values stored as dense arrays.\n\nExamples\n\n"}, {"name": "pandas.DataFrame.squeeze", "path": "reference/api/pandas.dataframe.squeeze", "type": "DataFrame", "text": "\nSqueeze 1 dimensional axis objects into scalars.\n\nSeries or DataFrames with a single element are squeezed to a scalar.\nDataFrames with a single column or a single row are squeezed to a Series.\nOtherwise the object is unchanged.\n\nThis method is most useful when you don\u2019t know if your object is a Series or\nDataFrame, but you do know it has just a single column. In that case you can\nsafely call squeeze to ensure you have a Series.\n\nA specific axis to squeeze. By default, all length-1 axes are squeezed.\n\nThe projection after squeezing axis or all the axes.\n\nSee also\n\nInteger-location based indexing for selecting scalars.\n\nInteger-location based indexing for selecting Series.\n\nInverse of DataFrame.squeeze for a single-column DataFrame.\n\nExamples\n\nSlicing might produce a Series with a single value:\n\nSqueezing objects with more than one value in every axis does nothing:\n\nSqueezing is even more effective when used with DataFrames.\n\nSlicing a single column will produce a DataFrame with the columns having only\none value:\n\nSo the columns can be squeezed down, resulting in a Series:\n\nSlicing a single row from a single column will produce a single scalar\nDataFrame:\n\nSqueezing the rows produces a single scalar Series:\n\nSqueezing all axes will project directly into a scalar:\n\n"}, {"name": "pandas.DataFrame.stack", "path": "reference/api/pandas.dataframe.stack", "type": "DataFrame", "text": "\nStack the prescribed level(s) from columns to index.\n\nReturn a reshaped DataFrame or Series having a multi-level index with one or\nmore new inner-most levels compared to the current DataFrame. The new inner-\nmost levels are created by pivoting the columns of the current dataframe:\n\nif the columns have a single level, the output is a Series;\n\nif the columns have multiple levels, the new index level(s) is (are) taken\nfrom the prescribed level(s) and the output is a DataFrame.\n\nLevel(s) to stack from the column axis onto the index axis, defined as one\nindex or label, or a list of indices or labels.\n\nWhether to drop rows in the resulting Frame/Series with missing values.\nStacking a column level onto the index axis can create combinations of index\nand column values that are missing from the original dataframe. See Examples\nsection.\n\nStacked dataframe or series.\n\nSee also\n\nUnstack prescribed level(s) from index axis onto column axis.\n\nReshape dataframe from long format to wide format.\n\nCreate a spreadsheet-style pivot table as a DataFrame.\n\nNotes\n\nThe function is named by analogy with a collection of books being reorganized\nfrom being side by side on a horizontal position (the columns of the\ndataframe) to being stacked vertically on top of each other (in the index of\nthe dataframe).\n\nExamples\n\nSingle level columns\n\nStacking a dataframe with a single level column axis returns a Series:\n\nMulti level columns: simple case\n\nStacking a dataframe with a multi-level column axis:\n\nMissing values\n\nIt is common to have missing values when stacking a dataframe with multi-level\ncolumns, as the stacked dataframe typically has more values than the original\ndataframe. Missing values are filled with NaNs:\n\nPrescribing the level(s) to be stacked\n\nThe first parameter controls which level or levels are stacked:\n\nDropping missing values\n\nNote that rows where all values are missing are dropped by default but this\nbehaviour can be controlled via the dropna keyword parameter:\n\n"}, {"name": "pandas.DataFrame.std", "path": "reference/api/pandas.dataframe.std", "type": "DataFrame", "text": "\nReturn sample standard deviation over requested axis.\n\nNormalized by N-1 by default. This can be changed using the ddof argument.\n\nExclude NA/null values. If an entire row/column is NA, the result will be NA.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nDelta Degrees of Freedom. The divisor used in calculations is N - ddof, where\nN represents the number of elements.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nNotes\n\nTo have the same behaviour as numpy.std, use ddof=0 (instead of the default\nddof=1)\n\nExamples\n\nThe standard deviation of the columns can be found as follows:\n\nAlternatively, ddof=0 can be set to normalize by N instead of N-1:\n\n"}, {"name": "pandas.DataFrame.style", "path": "reference/api/pandas.dataframe.style", "type": "Style", "text": "\nReturns a Styler object.\n\nContains methods for building a styled HTML representation of the DataFrame.\n\nSee also\n\nHelps style a DataFrame or Series according to the data with HTML and CSS.\n\n"}, {"name": "pandas.DataFrame.sub", "path": "reference/api/pandas.dataframe.sub", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\nEquivalent to `dataframe - other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rsub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.subtract", "path": "reference/api/pandas.dataframe.subtract", "type": "DataFrame", "text": "\nGet Subtraction of dataframe and other, element-wise (binary operator sub).\n\nEquivalent to `dataframe - other`, but with support to substitute a fill_value\nfor missing data in one of the inputs. With reverse version, rsub.\n\nAmong flexible wrappers (add, sub, mul, div, mod, pow) to arithmetic\noperators: +, -, *, /, //, %, **.\n\nAny single or multiple element data structure, or list-like object.\n\nWhether to compare by the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\nFor Series input, axis to match Series index on.\n\nBroadcast across a level, matching Index values on the passed MultiIndex\nlevel.\n\nFill existing missing (NaN) values, and any new element needed for successful\nDataFrame alignment, with this value before computation. If data in both\ncorresponding DataFrame locations is missing the result will be missing.\n\nResult of the arithmetic operation.\n\nSee also\n\nAdd DataFrames.\n\nSubtract DataFrames.\n\nMultiply DataFrames.\n\nDivide DataFrames (float division).\n\nDivide DataFrames (float division).\n\nDivide DataFrames (integer division).\n\nCalculate modulo (remainder after division).\n\nCalculate exponential power.\n\nNotes\n\nMismatched indices will be unioned together.\n\nExamples\n\nAdd a scalar with operator version which return the same results.\n\nDivide by constant with reverse version.\n\nSubtract a list and Series by axis with operator version.\n\nMultiply a DataFrame of different shape with operator version.\n\nDivide by a MultiIndex by level.\n\n"}, {"name": "pandas.DataFrame.sum", "path": "reference/api/pandas.dataframe.sum", "type": "DataFrame", "text": "\nReturn the sum of the values over the requested axis.\n\nThis is equivalent to the method `numpy.sum`.\n\nAxis for the function to be applied on.\n\nExclude NA/null values when computing the result.\n\nIf the axis is a MultiIndex (hierarchical), count along a particular level,\ncollapsing into a Series.\n\nInclude only float, int, boolean columns. If None, will attempt to use\neverything, then use only numeric data. Not implemented for Series.\n\nThe required number of valid values to perform the operation. If fewer than\n`min_count` non-NA values are present the result will be NA.\n\nAdditional keyword arguments to be passed to the function.\n\nSee also\n\nReturn the sum.\n\nReturn the minimum.\n\nReturn the maximum.\n\nReturn the index of the minimum.\n\nReturn the index of the maximum.\n\nReturn the sum over the requested axis.\n\nReturn the minimum over the requested axis.\n\nReturn the maximum over the requested axis.\n\nReturn the index of the minimum over the requested axis.\n\nReturn the index of the maximum over the requested axis.\n\nExamples\n\nBy default, the sum of an empty or all-NA Series is `0`.\n\nThis can be controlled with the `min_count` parameter. For example, if you\u2019d\nlike the sum of an empty series to be NaN, pass `min_count=1`.\n\nThanks to the `skipna` parameter, `min_count` handles all-NA and empty series\nidentically.\n\n"}, {"name": "pandas.DataFrame.swapaxes", "path": "reference/api/pandas.dataframe.swapaxes", "type": "DataFrame", "text": "\nInterchange axes and swap values axes appropriately.\n\n"}, {"name": "pandas.DataFrame.swaplevel", "path": "reference/api/pandas.dataframe.swaplevel", "type": "DataFrame", "text": "\nSwap levels i and j in a `MultiIndex`.\n\nDefault is to swap the two innermost levels of the index.\n\nLevels of the indices to be swapped. Can pass level name as string.\n\nThe axis to swap levels on. 0 or \u2018index\u2019 for row-wise, 1 or \u2018columns\u2019 for\ncolumn-wise.\n\nDataFrame with levels swapped in MultiIndex.\n\nExamples\n\nIn the following example, we will swap the levels of the indices. Here, we\nwill swap the levels column-wise, but levels can be swapped row-wise in a\nsimilar manner. Note that column-wise is the default behaviour. By not\nsupplying any arguments for i and j, we swap the last and second to last\nindices.\n\nBy supplying one argument, we can choose which index to swap the last index\nwith. We can for example swap the first index with the last one as follows.\n\nWe can also define explicitly which indices we want to swap by supplying\nvalues for both i and j. Here, we for example swap the first and second\nindices.\n\n"}, {"name": "pandas.DataFrame.T", "path": "reference/api/pandas.dataframe.t", "type": "DataFrame", "text": "\n\n"}, {"name": "pandas.DataFrame.tail", "path": "reference/api/pandas.dataframe.tail", "type": "DataFrame", "text": "\nReturn the last n rows.\n\nThis function returns last n rows from the object based on position. It is\nuseful for quickly verifying data, for example, after sorting or appending\nrows.\n\nFor negative values of n, this function returns all rows except the first n\nrows, equivalent to `df[n:]`.\n\nNumber of rows to select.\n\nThe last n rows of the caller object.\n\nSee also\n\nThe first n rows of the caller object.\n\nExamples\n\nViewing the last 5 lines\n\nViewing the last n lines (three in this case)\n\nFor negative values of n\n\n"}, {"name": "pandas.DataFrame.take", "path": "reference/api/pandas.dataframe.take", "type": "DataFrame", "text": "\nReturn the elements in the given positional indices along an axis.\n\nThis means that we are not indexing according to actual values in the index\nattribute of the object. We are indexing according to the actual position of\nthe element in the object.\n\nAn array of ints indicating which positions to take.\n\nThe axis on which to select elements. `0` means that we are selecting rows,\n`1` means that we are selecting columns.\n\nBefore pandas 1.0, `is_copy=False` can be specified to ensure that the return\nvalue is an actual copy. Starting with pandas 1.0, `take` always returns a\ncopy, and the keyword is therefore deprecated.\n\nDeprecated since version 1.0.0.\n\nFor compatibility with `numpy.take()`. Has no effect on the output.\n\nAn array-like containing the elements taken from the object.\n\nSee also\n\nSelect a subset of a DataFrame by labels.\n\nSelect a subset of a DataFrame by positions.\n\nTake elements from an array along an axis.\n\nExamples\n\nTake elements at positions 0 and 3 along the axis 0 (default).\n\nNote how the actual indices selected (0 and 1) do not correspond to our\nselected indices 0 and 3. That\u2019s because we are selecting the 0th and 3rd\nrows, not rows whose indices equal 0 and 3.\n\nTake elements at indices 1 and 2 along the axis 1 (column selection).\n\nWe may take elements using negative integers for positive indices, starting\nfrom the end of the object, just like with Python lists.\n\n"}, {"name": "pandas.DataFrame.to_clipboard", "path": "reference/api/pandas.dataframe.to_clipboard", "type": "DataFrame", "text": "\nCopy object to the system clipboard.\n\nWrite a text representation of object to the system clipboard. This can be\npasted into Excel, for example.\n\nProduce output in a csv format for easy pasting into excel.\n\nTrue, use the provided separator for csv pasting.\n\nFalse, write a string representation of the object to the clipboard.\n\nField delimiter.\n\nThese parameters will be passed to DataFrame.to_csv.\n\nSee also\n\nWrite a DataFrame to a comma-separated values (csv) file.\n\nRead text from clipboard and pass to read_csv.\n\nNotes\n\nRequirements for your platform.\n\nLinux : xclip, or xsel (with PyQt4 modules)\n\nWindows : none\n\nmacOS : none\n\nExamples\n\nCopy the contents of a DataFrame to the clipboard.\n\nWe can omit the index by passing the keyword index and setting it to false.\n\n"}, {"name": "pandas.DataFrame.to_csv", "path": "reference/api/pandas.dataframe.to_csv", "type": "DataFrame", "text": "\nWrite object to a comma-separated values (csv) file.\n\nString, path object (implementing os.PathLike[str]), or file-like object\nimplementing a write() function. If None, the result is returned as a string.\nIf a non-binary file object is passed, it should be opened with newline=\u2019\u2019,\ndisabling universal newlines. If a binary file object is passed, mode might\nneed to contain a \u2018b\u2019.\n\nChanged in version 1.2.0: Support for binary file objects was introduced.\n\nString of length 1. Field delimiter for the output file.\n\nMissing data representation.\n\nFormat string for floating point numbers.\n\nColumns to write.\n\nWrite out the column names. If a list of strings is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If None is given, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe object uses MultiIndex. If False do not print fields for index names. Use\nindex_label=False for easier importing in R.\n\nPython write mode, default \u2018w\u2019.\n\nA string representing the encoding to use in the output file, defaults to\n\u2018utf-8\u2019. encoding is not supported if path_or_buf is a non-binary file object.\n\nFor on-the-fly compression of the output data. If \u2018infer\u2019 and \u2018%s\u2019 path-like,\nthen detect compression from the following extensions: \u2018.gz\u2019, \u2018.bz2\u2019, \u2018.zip\u2019,\n\u2018.xz\u2019, or \u2018.zst\u2019 (otherwise no compression). Set to `None` for no compression.\nCan also be a dict with key `'method'` set to one of {`'zip'`, `'gzip'`,\n`'bz2'`, `'zstd'`} and other key-value pairs are forwarded to\n`zipfile.ZipFile`, `gzip.GzipFile`, `bz2.BZ2File`, or\n`zstandard.ZstdDecompressor`, respectively. As an example, the following could\nbe passed for faster compression and to create a reproducible gzip archive:\n`compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1}`.\n\nChanged in version 1.0.0: May now be a dict with key \u2018method\u2019 as compression\nmode and other entries as additional compression options if compression mode\nis \u2018zip\u2019.\n\nChanged in version 1.1.0: Passing compression options as keys in dict is\nsupported for compression modes \u2018gzip\u2019, \u2018bz2\u2019, \u2018zstd\u2019, and \u2018zip\u2019.\n\nChanged in version 1.2.0: Compression is supported for binary file objects.\n\nChanged in version 1.2.0: Previous versions forwarded dict entries for \u2018gzip\u2019\nto gzip.open instead of gzip.GzipFile which prevented setting mtime.\n\nDefaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are\nconverted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-\nnumeric.\n\nString of length 1. Character used to quote fields.\n\nThe newline character or character sequence to use in the output file.\nDefaults to os.linesep, which depends on the OS in which this method is called\n(\u2019\\n\u2019 for linux, \u2018\\r\\n\u2019 for Windows, i.e.).\n\nRows to write at a time.\n\nFormat string for datetime objects.\n\nControl quoting of quotechar inside a field.\n\nString of length 1. Character used to escape sep and quotechar when\nappropriate.\n\nCharacter recognized as decimal separator. E.g. use \u2018,\u2019 for European data.\n\nSpecifies how encoding and decoding errors are to be handled. See the errors\nargument for `open()` for a full list of options.\n\nNew in version 1.1.0.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nIf path_or_buf is None, returns the resulting csv format as a string.\nOtherwise returns None.\n\nSee also\n\nLoad a CSV file into a DataFrame.\n\nWrite DataFrame to an Excel file.\n\nExamples\n\nCreate \u2018out.zip\u2019 containing \u2018out.csv\u2019\n\nTo write a csv file to a new folder or nested folder you will first need to\ncreate it using either Pathlib or os:\n\n"}, {"name": "pandas.DataFrame.to_dict", "path": "reference/api/pandas.dataframe.to_dict", "type": "DataFrame", "text": "\nConvert the DataFrame to a dictionary.\n\nThe type of the key-value pairs can be customized with the parameters (see\nbelow).\n\nDetermines the type of the values of the dictionary.\n\n\u2018dict\u2019 (default) : dict like {column -> {index -> value}}\n\n\u2018list\u2019 : dict like {column -> [values]}\n\n\u2018series\u2019 : dict like {column -> Series(values)}\n\n\u2018split\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values]}\n\n\u2018tight\u2019 : dict like {\u2018index\u2019 -> [index], \u2018columns\u2019 -> [columns], \u2018data\u2019 ->\n[values], \u2018index_names\u2019 -> [index.names], \u2018column_names\u2019 -> [column.names]}\n\n\u2018records\u2019 : list like [{column -> value}, \u2026 , {column -> value}]\n\n\u2018index\u2019 : dict like {index -> {column -> value}}\n\nAbbreviations are allowed. s indicates series and sp indicates split.\n\nNew in version 1.4.0: \u2018tight\u2019 as an allowed value for the `orient` argument\n\nThe collections.abc.Mapping subclass used for all Mappings in the return\nvalue. Can be the actual class or an empty instance of the mapping type you\nwant. If you want a collections.defaultdict, you must pass it initialized.\n\nReturn a collections.abc.Mapping object representing the DataFrame. The\nresulting transformation depends on the orient parameter.\n\nSee also\n\nCreate a DataFrame from a dictionary.\n\nConvert a DataFrame to JSON format.\n\nExamples\n\nYou can specify the return orientation.\n\nYou can also specify the mapping type.\n\nIf you want a defaultdict, you need to initialize it:\n\n"}, {"name": "pandas.DataFrame.to_excel", "path": "reference/api/pandas.dataframe.to_excel", "type": "DataFrame", "text": "\nWrite object to an Excel sheet.\n\nTo write a single object to an Excel .xlsx file it is only necessary to\nspecify a target file name. To write to multiple sheets it is necessary to\ncreate an ExcelWriter object with a target file name, and specify a sheet in\nthe file to write to.\n\nMultiple sheets may be written to by specifying unique sheet_name. With all\ndata written to the file it is necessary to save the changes. Note that\ncreating an ExcelWriter object with a file name that already exists will\nresult in the contents of the existing file being erased.\n\nFile path or existing ExcelWriter.\n\nName of sheet which will contain DataFrame.\n\nMissing data representation.\n\nFormat string for floating point numbers. For example `float_format=\"%.2f\"`\nwill format 0.1234 to 0.12.\n\nColumns to write.\n\nWrite out the column names. If a list of string is given it is assumed to be\naliases for the column names.\n\nWrite row names (index).\n\nColumn label for index column(s) if desired. If not specified, and header and\nindex are True, then the index names are used. A sequence should be given if\nthe DataFrame uses MultiIndex.\n\nUpper left cell row to dump data frame.\n\nUpper left cell column to dump data frame.\n\nWrite engine to use, \u2018openpyxl\u2019 or \u2018xlsxwriter\u2019. You can also set this via the\noptions `io.excel.xlsx.writer`, `io.excel.xls.writer`, and\n`io.excel.xlsm.writer`.\n\nDeprecated since version 1.2.0: As the xlwt package is no longer maintained,\nthe `xlwt` engine will be removed in a future version of pandas.\n\nWrite MultiIndex and Hierarchical Rows as merged cells.\n\nEncoding of the resulting excel file. Only necessary for xlwt, other writers\nsupport unicode natively.\n\nRepresentation for infinity (there is no native representation for infinity in\nExcel).\n\nDisplay more information in the error logs.\n\nSpecifies the one-based bottommost row and rightmost column that is to be\nfrozen.\n\nExtra options that make sense for a particular storage connection, e.g. host,\nport, username, password, etc. For HTTP(S) URLs the key-value pairs are\nforwarded to `urllib` as header options. For other URLs (e.g. starting with\n\u201cs3://\u201d, and \u201cgcs://\u201d) the key-value pairs are forwarded to `fsspec`. Please\nsee `fsspec` and `urllib` for more details.\n\nNew in version 1.2.0.\n\nSee also\n\nWrite DataFrame to a comma-separated values (csv) file.\n\nClass for writing DataFrame objects into excel sheets.\n\nRead an Excel file into a pandas DataFrame.\n\nRead a comma-separated values (csv) file into Da