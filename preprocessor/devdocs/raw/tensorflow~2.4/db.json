{"index":"<h1 class=\"devsite-page-title\">Module: tf</h1>   <p><devsite-mathjax config=\"TeX-AMS-MML_SVG\"></devsite-mathjax> </p>   <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/__init__.py\">  View source on GitHub </a> </td> </table> <h2 id=\"tensorflow\" data-text=\"TensorFlow\">TensorFlow</h2> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">pip install tensorflow\n</pre> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"audio\"><code translate=\"no\" dir=\"ltr\">audio</code></a> module: Public API for tf.audio namespace.</p> <p><a href=\"autodiff\"><code translate=\"no\" dir=\"ltr\">autodiff</code></a> module: Public API for tf.autodiff namespace.</p> <p><a href=\"autograph\"><code translate=\"no\" dir=\"ltr\">autograph</code></a> module: Conversion of plain Python into TensorFlow graph code.</p> <p><a href=\"bitwise\"><code translate=\"no\" dir=\"ltr\">bitwise</code></a> module: Operations for manipulating the binary representations of integers.</p> <p><a href=\"compat\"><code translate=\"no\" dir=\"ltr\">compat</code></a> module: Compatibility functions.</p> <p><a href=\"config\"><code translate=\"no\" dir=\"ltr\">config</code></a> module: Public API for tf.config namespace.</p> <p><a href=\"data\"><code translate=\"no\" dir=\"ltr\">data</code></a> module: <a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a> API for input pipelines.</p> <p><a href=\"debugging\"><code translate=\"no\" dir=\"ltr\">debugging</code></a> module: Public API for tf.debugging namespace.</p> <p><a href=\"distribute\"><code translate=\"no\" dir=\"ltr\">distribute</code></a> module: Library for running a computation across multiple devices.</p> <p><a href=\"dtypes\"><code translate=\"no\" dir=\"ltr\">dtypes</code></a> module: Public API for tf.dtypes namespace.</p> <p><a href=\"errors\"><code translate=\"no\" dir=\"ltr\">errors</code></a> module: Exception types for TensorFlow errors.</p> <p><a href=\"estimator\"><code translate=\"no\" dir=\"ltr\">estimator</code></a> module: Estimator: High level tools for working with models.</p> <p><a href=\"experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.experimental namespace.</p> <p><a href=\"feature_column\"><code translate=\"no\" dir=\"ltr\">feature_column</code></a> module: Public API for tf.feature_column namespace.</p> <p><a href=\"graph_util\"><code translate=\"no\" dir=\"ltr\">graph_util</code></a> module: Helpers to manipulate a tensor graph in python.</p> <p><a href=\"image\"><code translate=\"no\" dir=\"ltr\">image</code></a> module: Image ops.</p> <p><a href=\"keras/initializers\"><code translate=\"no\" dir=\"ltr\">initializers</code></a> module: Keras initializer serialization / deserialization.</p> <p><a href=\"io\"><code translate=\"no\" dir=\"ltr\">io</code></a> module: Public API for tf.io namespace.</p> <p><a href=\"keras\"><code translate=\"no\" dir=\"ltr\">keras</code></a> module: Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p><a href=\"linalg\"><code translate=\"no\" dir=\"ltr\">linalg</code></a> module: Operations for linear algebra.</p> <p><a href=\"lite\"><code translate=\"no\" dir=\"ltr\">lite</code></a> module: Public API for tf.lite namespace.</p> <p><a href=\"lookup\"><code translate=\"no\" dir=\"ltr\">lookup</code></a> module: Public API for tf.lookup namespace.</p> <p><a href=\"keras/losses\"><code translate=\"no\" dir=\"ltr\">losses</code></a> module: Built-in loss functions.</p> <p><a href=\"math\"><code translate=\"no\" dir=\"ltr\">math</code></a> module: Math Operations.</p> <p><a href=\"keras/metrics\"><code translate=\"no\" dir=\"ltr\">metrics</code></a> module: Built-in metrics.</p> <p><a href=\"mixed_precision\"><code translate=\"no\" dir=\"ltr\">mixed_precision</code></a> module: Public API for tf.mixed_precision namespace.</p> <p><a href=\"mlir\"><code translate=\"no\" dir=\"ltr\">mlir</code></a> module: Public API for tf.mlir namespace.</p> <p><a href=\"nest\"><code translate=\"no\" dir=\"ltr\">nest</code></a> module: Public API for tf.nest namespace.</p> <p><a href=\"nn\"><code translate=\"no\" dir=\"ltr\">nn</code></a> module: Wrappers for primitive Neural Net (NN) Operations.</p> <p><a href=\"keras/optimizers\"><code translate=\"no\" dir=\"ltr\">optimizers</code></a> module: Built-in optimizer classes.</p> <p><a href=\"profiler\"><code translate=\"no\" dir=\"ltr\">profiler</code></a> module: Public API for tf.profiler namespace.</p> <p><a href=\"quantization\"><code translate=\"no\" dir=\"ltr\">quantization</code></a> module: Public API for tf.quantization namespace.</p> <p><a href=\"queue\"><code translate=\"no\" dir=\"ltr\">queue</code></a> module: Public API for tf.queue namespace.</p> <p><a href=\"ragged\"><code translate=\"no\" dir=\"ltr\">ragged</code></a> module: Ragged Tensors.</p> <p><a href=\"random\"><code translate=\"no\" dir=\"ltr\">random</code></a> module: Public API for tf.random namespace.</p> <p><a href=\"raw_ops\"><code translate=\"no\" dir=\"ltr\">raw_ops</code></a> module: Public API for tf.raw_ops namespace.</p> <p><a href=\"saved_model\"><code translate=\"no\" dir=\"ltr\">saved_model</code></a> module: Public API for tf.saved_model namespace.</p> <p><a href=\"sets\"><code translate=\"no\" dir=\"ltr\">sets</code></a> module: Tensorflow set operations.</p> <p><a href=\"signal\"><code translate=\"no\" dir=\"ltr\">signal</code></a> module: Signal processing operations.</p> <p><a href=\"sparse\"><code translate=\"no\" dir=\"ltr\">sparse</code></a> module: Sparse Tensor Representation.</p> <p><a href=\"strings\"><code translate=\"no\" dir=\"ltr\">strings</code></a> module: Operations for working with string Tensors.</p> <p><a href=\"summary\"><code translate=\"no\" dir=\"ltr\">summary</code></a> module: Operations for writing summary data, for use in analysis and visualization.</p> <p><a href=\"sysconfig\"><code translate=\"no\" dir=\"ltr\">sysconfig</code></a> module: System configuration library.</p> <p><a href=\"test\"><code translate=\"no\" dir=\"ltr\">test</code></a> module: Testing.</p> <p><a href=\"tpu\"><code translate=\"no\" dir=\"ltr\">tpu</code></a> module: Ops related to Tensor Processing Units.</p> <p><a href=\"train\"><code translate=\"no\" dir=\"ltr\">train</code></a> module: Support for training models.</p> <p><a href=\"types\"><code translate=\"no\" dir=\"ltr\">types</code></a> module: Public TensorFlow type definitions.</p> <p><a href=\"version\"><code translate=\"no\" dir=\"ltr\">version</code></a> module: Public API for tf.version namespace.</p> <p><a href=\"xla\"><code translate=\"no\" dir=\"ltr\">xla</code></a> module: Public API for tf.xla namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"aggregationmethod\"><code translate=\"no\" dir=\"ltr\">class AggregationMethod</code></a>: A class listing aggregation methods used to combine gradients.</p> <p><a href=\"criticalsection\"><code translate=\"no\" dir=\"ltr\">class CriticalSection</code></a>: Critical section.</p> <p><a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">class DType</code></a>: Represents the type of the elements in a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"devicespec\"><code translate=\"no\" dir=\"ltr\">class DeviceSpec</code></a>: Represents a (possibly partial) specification for a TensorFlow device.</p> <p><a href=\"gradienttape\"><code translate=\"no\" dir=\"ltr\">class GradientTape</code></a>: Record operations for automatic differentiation.</p> <p><a href=\"graph\"><code translate=\"no\" dir=\"ltr\">class Graph</code></a>: A TensorFlow computation, represented as a dataflow graph.</p> <p><a href=\"indexedslices\"><code translate=\"no\" dir=\"ltr\">class IndexedSlices</code></a>: A sparse representation of a set of tensor slices at given indices.</p> <p><a href=\"indexedslicesspec\"><code translate=\"no\" dir=\"ltr\">class IndexedSlicesSpec</code></a>: Type specification for a <a href=\"indexedslices\"><code translate=\"no\" dir=\"ltr\">tf.IndexedSlices</code></a>.</p> <p><a href=\"module\"><code translate=\"no\" dir=\"ltr\">class Module</code></a>: Base neural network module class.</p> <p><a href=\"operation\"><code translate=\"no\" dir=\"ltr\">class Operation</code></a>: Represents a graph node that performs computation on tensors.</p> <p><a href=\"optionalspec\"><code translate=\"no\" dir=\"ltr\">class OptionalSpec</code></a>: Type specification for <a href=\"experimental/optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a>.</p> <p><a href=\"raggedtensor\"><code translate=\"no\" dir=\"ltr\">class RaggedTensor</code></a>: Represents a ragged tensor.</p> <p><a href=\"raggedtensorspec\"><code translate=\"no\" dir=\"ltr\">class RaggedTensorSpec</code></a>: Type specification for a <a href=\"raggedtensor\"><code translate=\"no\" dir=\"ltr\">tf.RaggedTensor</code></a>.</p> <p><a href=\"registergradient\"><code translate=\"no\" dir=\"ltr\">class RegisterGradient</code></a>: A decorator for registering the gradient function for an op type.</p> <p><a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">class SparseTensor</code></a>: Represents a sparse tensor.</p> <p><a href=\"sparsetensorspec\"><code translate=\"no\" dir=\"ltr\">class SparseTensorSpec</code></a>: Type specification for a <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>.</p> <p><a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">class Tensor</code></a>: A tensor is a multidimensional array of elements represented by a</p> <p><a href=\"tensorarray\"><code translate=\"no\" dir=\"ltr\">class TensorArray</code></a>: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p><a href=\"tensorarrayspec\"><code translate=\"no\" dir=\"ltr\">class TensorArraySpec</code></a>: Type specification for a <a href=\"tensorarray\"><code translate=\"no\" dir=\"ltr\">tf.TensorArray</code></a>.</p> <p><a href=\"tensorshape\"><code translate=\"no\" dir=\"ltr\">class TensorShape</code></a>: Represents the shape of a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"tensorspec\"><code translate=\"no\" dir=\"ltr\">class TensorSpec</code></a>: Describes a tf.Tensor.</p> <p><a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">class TypeSpec</code></a>: Specifies a TensorFlow value type.</p> <p><a href=\"unconnectedgradients\"><code translate=\"no\" dir=\"ltr\">class UnconnectedGradients</code></a>: Controls how gradient computation behaves when y does not depend on x.</p> <p><a href=\"variable\"><code translate=\"no\" dir=\"ltr\">class Variable</code></a>: See the <a href=\"https://tensorflow.org/guide/variable\">variable guide</a>.</p> <p><a href=\"variableaggregation\"><code translate=\"no\" dir=\"ltr\">class VariableAggregation</code></a>: Indicates how a distributed variable will be aggregated.</p> <p><a href=\"variablesynchronization\"><code translate=\"no\" dir=\"ltr\">class VariableSynchronization</code></a>: Indicates when a distributed variable will be synced.</p> <p><a href=\"constant_initializer\"><code translate=\"no\" dir=\"ltr\">class constant_initializer</code></a>: Initializer that generates tensors with constant values.</p> <p><a href=\"name_scope\"><code translate=\"no\" dir=\"ltr\">class name_scope</code></a>: A context manager for use when defining a Python op.</p> <p><a href=\"ones_initializer\"><code translate=\"no\" dir=\"ltr\">class ones_initializer</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href=\"random_normal_initializer\"><code translate=\"no\" dir=\"ltr\">class random_normal_initializer</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href=\"random_uniform_initializer\"><code translate=\"no\" dir=\"ltr\">class random_uniform_initializer</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href=\"zeros_initializer\"><code translate=\"no\" dir=\"ltr\">class zeros_initializer</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"debugging/assert\"><code translate=\"no\" dir=\"ltr\">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href=\"math/abs\"><code translate=\"no\" dir=\"ltr\">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href=\"math/acos\"><code translate=\"no\" dir=\"ltr\">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href=\"math/acosh\"><code translate=\"no\" dir=\"ltr\">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href=\"math/add_n\"><code translate=\"no\" dir=\"ltr\">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href=\"math/argmax\"><code translate=\"no\" dir=\"ltr\">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor.</p> <p><a href=\"math/argmin\"><code translate=\"no\" dir=\"ltr\">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor.</p> <p><a href=\"argsort\"><code translate=\"no\" dir=\"ltr\">argsort(...)</code></a>: Returns the indices of a tensor that give its sorted order along an axis.</p> <p><a href=\"dtypes/as_dtype\"><code translate=\"no\" dir=\"ltr\">as_dtype(...)</code></a>: Converts the given <code translate=\"no\" dir=\"ltr\">type_value</code> to a <code translate=\"no\" dir=\"ltr\">DType</code>.</p> <p><a href=\"strings/as_string\"><code translate=\"no\" dir=\"ltr\">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href=\"math/asin\"><code translate=\"no\" dir=\"ltr\">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href=\"math/asinh\"><code translate=\"no\" dir=\"ltr\">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href=\"debugging/assert_equal\"><code translate=\"no\" dir=\"ltr\">assert_equal(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x == y</code> holds element-wise.</p> <p><a href=\"debugging/assert_greater\"><code translate=\"no\" dir=\"ltr\">assert_greater(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &gt; y</code> holds element-wise.</p> <p><a href=\"debugging/assert_less\"><code translate=\"no\" dir=\"ltr\">assert_less(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &lt; y</code> holds element-wise.</p> <p><a href=\"debugging/assert_rank\"><code translate=\"no\" dir=\"ltr\">assert_rank(...)</code></a>: Assert that <code translate=\"no\" dir=\"ltr\">x</code> has rank equal to <code translate=\"no\" dir=\"ltr\">rank</code>.</p> <p><a href=\"math/atan\"><code translate=\"no\" dir=\"ltr\">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href=\"math/atan2\"><code translate=\"no\" dir=\"ltr\">atan2(...)</code></a>: Computes arctangent of <code translate=\"no\" dir=\"ltr\">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href=\"math/atanh\"><code translate=\"no\" dir=\"ltr\">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href=\"batch_to_space\"><code translate=\"no\" dir=\"ltr\">batch_to_space(...)</code></a>: BatchToSpace for N-D tensors of type T.</p> <p><a href=\"bitcast\"><code translate=\"no\" dir=\"ltr\">bitcast(...)</code></a>: Bitcasts a tensor from one type to another without copying data.</p> <p><a href=\"boolean_mask\"><code translate=\"no\" dir=\"ltr\">boolean_mask(...)</code></a>: Apply boolean mask to tensor.</p> <p><a href=\"broadcast_dynamic_shape\"><code translate=\"no\" dir=\"ltr\">broadcast_dynamic_shape(...)</code></a>: Computes the shape of a broadcast given symbolic shapes.</p> <p><a href=\"broadcast_static_shape\"><code translate=\"no\" dir=\"ltr\">broadcast_static_shape(...)</code></a>: Computes the shape of a broadcast given known shapes.</p> <p><a href=\"broadcast_to\"><code translate=\"no\" dir=\"ltr\">broadcast_to(...)</code></a>: Broadcast an array for a compatible shape.</p> <p><a href=\"case\"><code translate=\"no\" dir=\"ltr\">case(...)</code></a>: Create a case operation.</p> <p><a href=\"cast\"><code translate=\"no\" dir=\"ltr\">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href=\"clip_by_global_norm\"><code translate=\"no\" dir=\"ltr\">clip_by_global_norm(...)</code></a>: Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p><a href=\"clip_by_norm\"><code translate=\"no\" dir=\"ltr\">clip_by_norm(...)</code></a>: Clips tensor values to a maximum L2-norm.</p> <p><a href=\"clip_by_value\"><code translate=\"no\" dir=\"ltr\">clip_by_value(...)</code></a>: Clips tensor values to a specified min and max.</p> <p><a href=\"dtypes/complex\"><code translate=\"no\" dir=\"ltr\">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href=\"concat\"><code translate=\"no\" dir=\"ltr\">concat(...)</code></a>: Concatenates tensors along one dimension.</p> <p><a href=\"cond\"><code translate=\"no\" dir=\"ltr\">cond(...)</code></a>: Return <code translate=\"no\" dir=\"ltr\">true_fn()</code> if the predicate <code translate=\"no\" dir=\"ltr\">pred</code> is true else <code translate=\"no\" dir=\"ltr\">false_fn()</code>.</p> <p><a href=\"constant\"><code translate=\"no\" dir=\"ltr\">constant(...)</code></a>: Creates a constant tensor from a tensor-like object.</p> <p><a href=\"control_dependencies\"><code translate=\"no\" dir=\"ltr\">control_dependencies(...)</code></a>: Wrapper for <a href=\"graph#control_dependencies\"><code translate=\"no\" dir=\"ltr\">Graph.control_dependencies()</code></a> using the default graph.</p> <p><a href=\"convert_to_tensor\"><code translate=\"no\" dir=\"ltr\">convert_to_tensor(...)</code></a>: Converts the given <code translate=\"no\" dir=\"ltr\">value</code> to a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"math/cos\"><code translate=\"no\" dir=\"ltr\">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href=\"math/cosh\"><code translate=\"no\" dir=\"ltr\">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href=\"math/cumsum\"><code translate=\"no\" dir=\"ltr\">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate=\"no\" dir=\"ltr\">x</code> along <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"custom_gradient\"><code translate=\"no\" dir=\"ltr\">custom_gradient(...)</code></a>: Decorator to define a function with a custom gradient.</p> <p><a href=\"device\"><code translate=\"no\" dir=\"ltr\">device(...)</code></a>: Specifies the device for ops created/executed in this context.</p> <p><a href=\"math/divide\"><code translate=\"no\" dir=\"ltr\">divide(...)</code></a>: Computes Python style division of <code translate=\"no\" dir=\"ltr\">x</code> by <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"dynamic_partition\"><code translate=\"no\" dir=\"ltr\">dynamic_partition(...)</code></a>: Partitions <code translate=\"no\" dir=\"ltr\">data</code> into <code translate=\"no\" dir=\"ltr\">num_partitions</code> tensors using indices from <code translate=\"no\" dir=\"ltr\">partitions</code>.</p> <p><a href=\"dynamic_stitch\"><code translate=\"no\" dir=\"ltr\">dynamic_stitch(...)</code></a>: Interleave the values from the <code translate=\"no\" dir=\"ltr\">data</code> tensors into a single tensor.</p> <p><a href=\"edit_distance\"><code translate=\"no\" dir=\"ltr\">edit_distance(...)</code></a>: Computes the Levenshtein distance between sequences.</p> <p><a href=\"linalg/eig\"><code translate=\"no\" dir=\"ltr\">eig(...)</code></a>: Computes the eigen decomposition of a batch of matrices.</p> <p><a href=\"linalg/eigvals\"><code translate=\"no\" dir=\"ltr\">eigvals(...)</code></a>: Computes the eigenvalues of one or more matrices.</p> <p><a href=\"einsum\"><code translate=\"no\" dir=\"ltr\">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href=\"ensure_shape\"><code translate=\"no\" dir=\"ltr\">ensure_shape(...)</code></a>: Updates the shape of a tensor and checks at runtime that the shape holds.</p> <p><a href=\"math/equal\"><code translate=\"no\" dir=\"ltr\">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href=\"executing_eagerly\"><code translate=\"no\" dir=\"ltr\">executing_eagerly(...)</code></a>: Checks whether the current thread has eager execution enabled.</p> <p><a href=\"math/exp\"><code translate=\"no\" dir=\"ltr\">exp(...)</code></a>: Computes exponential of x element-wise. \\(y = e^x\\).</p> <p><a href=\"expand_dims\"><code translate=\"no\" dir=\"ltr\">expand_dims(...)</code></a>: Returns a tensor with a length 1 axis inserted at index <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"extract_volume_patches\"><code translate=\"no\" dir=\"ltr\">extract_volume_patches(...)</code></a>: Extract <code translate=\"no\" dir=\"ltr\">patches</code> from <code translate=\"no\" dir=\"ltr\">input</code> and put them in the <code translate=\"no\" dir=\"ltr\">\"depth\"</code> output dimension. 3D extension of <code translate=\"no\" dir=\"ltr\">extract_image_patches</code>.</p> <p><a href=\"eye\"><code translate=\"no\" dir=\"ltr\">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href=\"fill\"><code translate=\"no\" dir=\"ltr\">fill(...)</code></a>: Creates a tensor filled with a scalar value.</p> <p><a href=\"fingerprint\"><code translate=\"no\" dir=\"ltr\">fingerprint(...)</code></a>: Generates fingerprint values.</p> <p><a href=\"math/floor\"><code translate=\"no\" dir=\"ltr\">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href=\"foldl\"><code translate=\"no\" dir=\"ltr\">foldl(...)</code></a>: foldl on the list of tensors unpacked from <code translate=\"no\" dir=\"ltr\">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href=\"foldr\"><code translate=\"no\" dir=\"ltr\">foldr(...)</code></a>: foldr on the list of tensors unpacked from <code translate=\"no\" dir=\"ltr\">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href=\"function\"><code translate=\"no\" dir=\"ltr\">function(...)</code></a>: Compiles a function into a callable TensorFlow graph.</p> <p><a href=\"gather\"><code translate=\"no\" dir=\"ltr\">gather(...)</code></a>: Gather slices from params axis <code translate=\"no\" dir=\"ltr\">axis</code> according to indices.</p> <p><a href=\"gather_nd\"><code translate=\"no\" dir=\"ltr\">gather_nd(...)</code></a>: Gather slices from <code translate=\"no\" dir=\"ltr\">params</code> into a Tensor with shape specified by <code translate=\"no\" dir=\"ltr\">indices</code>.</p> <p><a href=\"get_logger\"><code translate=\"no\" dir=\"ltr\">get_logger(...)</code></a>: Return TF logger instance.</p> <p><a href=\"get_static_value\"><code translate=\"no\" dir=\"ltr\">get_static_value(...)</code></a>: Returns the constant value of the given tensor, if efficiently calculable.</p> <p><a href=\"grad_pass_through\"><code translate=\"no\" dir=\"ltr\">grad_pass_through(...)</code></a>: Creates a grad-pass-through op with the forward behavior provided in f.</p> <p><a href=\"gradients\"><code translate=\"no\" dir=\"ltr\">gradients(...)</code></a>: Constructs symbolic derivatives of sum of <code translate=\"no\" dir=\"ltr\">ys</code> w.r.t. x in <code translate=\"no\" dir=\"ltr\">xs</code>.</p> <p><a href=\"math/greater\"><code translate=\"no\" dir=\"ltr\">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href=\"math/greater_equal\"><code translate=\"no\" dir=\"ltr\">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href=\"group\"><code translate=\"no\" dir=\"ltr\">group(...)</code></a>: Create an op that groups multiple operations.</p> <p><a href=\"guarantee_const\"><code translate=\"no\" dir=\"ltr\">guarantee_const(...)</code></a>: Gives a guarantee to the TF runtime that the input tensor is a constant.</p> <p><a href=\"hessians\"><code translate=\"no\" dir=\"ltr\">hessians(...)</code></a>: Constructs the Hessian of sum of <code translate=\"no\" dir=\"ltr\">ys</code> with respect to <code translate=\"no\" dir=\"ltr\">x</code> in <code translate=\"no\" dir=\"ltr\">xs</code>.</p> <p><a href=\"histogram_fixed_width\"><code translate=\"no\" dir=\"ltr\">histogram_fixed_width(...)</code></a>: Return histogram of values.</p> <p><a href=\"histogram_fixed_width_bins\"><code translate=\"no\" dir=\"ltr\">histogram_fixed_width_bins(...)</code></a>: Bins the given values for use in a histogram.</p> <p><a href=\"identity\"><code translate=\"no\" dir=\"ltr\">identity(...)</code></a>: Return a Tensor with the same shape and contents as input.</p> <p><a href=\"identity_n\"><code translate=\"no\" dir=\"ltr\">identity_n(...)</code></a>: Returns a list of tensors with the same shapes and contents as the input</p> <p><a href=\"graph_util/import_graph_def\"><code translate=\"no\" dir=\"ltr\">import_graph_def(...)</code></a>: Imports the graph from <code translate=\"no\" dir=\"ltr\">graph_def</code> into the current default <code translate=\"no\" dir=\"ltr\">Graph</code>. (deprecated arguments)</p> <p><a href=\"init_scope\"><code translate=\"no\" dir=\"ltr\">init_scope(...)</code></a>: A context manager that lifts ops out of control-flow scopes and function-building graphs.</p> <p><a href=\"inside_function\"><code translate=\"no\" dir=\"ltr\">inside_function(...)</code></a>: Indicates whether the caller code is executing inside a <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>.</p> <p><a href=\"is_tensor\"><code translate=\"no\" dir=\"ltr\">is_tensor(...)</code></a>: Checks whether <code translate=\"no\" dir=\"ltr\">x</code> is a TF-native type that can be passed to many TF ops.</p> <p><a href=\"math/less\"><code translate=\"no\" dir=\"ltr\">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href=\"math/less_equal\"><code translate=\"no\" dir=\"ltr\">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href=\"linspace\"><code translate=\"no\" dir=\"ltr\">linspace(...)</code></a>: Generates evenly-spaced values in an interval along a given axis.</p> <p><a href=\"load_library\"><code translate=\"no\" dir=\"ltr\">load_library(...)</code></a>: Loads a TensorFlow plugin.</p> <p><a href=\"load_op_library\"><code translate=\"no\" dir=\"ltr\">load_op_library(...)</code></a>: Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p><a href=\"math/logical_and\"><code translate=\"no\" dir=\"ltr\">logical_and(...)</code></a>: Logical AND function.</p> <p><a href=\"math/logical_not\"><code translate=\"no\" dir=\"ltr\">logical_not(...)</code></a>: Returns the truth value of <code translate=\"no\" dir=\"ltr\">NOT x</code> element-wise.</p> <p><a href=\"math/logical_or\"><code translate=\"no\" dir=\"ltr\">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href=\"make_ndarray\"><code translate=\"no\" dir=\"ltr\">make_ndarray(...)</code></a>: Create a numpy ndarray from a tensor.</p> <p><a href=\"make_tensor_proto\"><code translate=\"no\" dir=\"ltr\">make_tensor_proto(...)</code></a>: Create a TensorProto.</p> <p><a href=\"map_fn\"><code translate=\"no\" dir=\"ltr\">map_fn(...)</code></a>: Transforms <code translate=\"no\" dir=\"ltr\">elems</code> by applying <code translate=\"no\" dir=\"ltr\">fn</code> to each element unstacked on axis 0. (deprecated arguments)</p> <p><a href=\"linalg/matmul\"><code translate=\"no\" dir=\"ltr\">matmul(...)</code></a>: Multiplies matrix <code translate=\"no\" dir=\"ltr\">a</code> by matrix <code translate=\"no\" dir=\"ltr\">b</code>, producing <code translate=\"no\" dir=\"ltr\">a</code> * <code translate=\"no\" dir=\"ltr\">b</code>.</p> <p><a href=\"linalg/sqrtm\"><code translate=\"no\" dir=\"ltr\">matrix_square_root(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href=\"math/maximum\"><code translate=\"no\" dir=\"ltr\">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href=\"meshgrid\"><code translate=\"no\" dir=\"ltr\">meshgrid(...)</code></a>: Broadcasts parameters for evaluation on an N-D grid.</p> <p><a href=\"math/minimum\"><code translate=\"no\" dir=\"ltr\">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href=\"math/multiply\"><code translate=\"no\" dir=\"ltr\">multiply(...)</code></a>: Returns an element-wise x * y.</p> <p><a href=\"math/negative\"><code translate=\"no\" dir=\"ltr\">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href=\"no_gradient\"><code translate=\"no\" dir=\"ltr\">no_gradient(...)</code></a>: Specifies that ops of type <code translate=\"no\" dir=\"ltr\">op_type</code> is not differentiable.</p> <p><a href=\"no_op\"><code translate=\"no\" dir=\"ltr\">no_op(...)</code></a>: Does nothing. Only useful as a placeholder for control edges.</p> <p><a href=\"nondifferentiable_batch_function\"><code translate=\"no\" dir=\"ltr\">nondifferentiable_batch_function(...)</code></a>: Batches the computation done by the decorated function.</p> <p><a href=\"norm\"><code translate=\"no\" dir=\"ltr\">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors.</p> <p><a href=\"math/not_equal\"><code translate=\"no\" dir=\"ltr\">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href=\"numpy_function\"><code translate=\"no\" dir=\"ltr\">numpy_function(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href=\"one_hot\"><code translate=\"no\" dir=\"ltr\">one_hot(...)</code></a>: Returns a one-hot tensor.</p> <p><a href=\"ones\"><code translate=\"no\" dir=\"ltr\">ones(...)</code></a>: Creates a tensor with all elements set to one (1).</p> <p><a href=\"ones_like\"><code translate=\"no\" dir=\"ltr\">ones_like(...)</code></a>: Creates a tensor of all ones that has the same shape as the input.</p> <p><a href=\"pad\"><code translate=\"no\" dir=\"ltr\">pad(...)</code></a>: Pads a tensor.</p> <p><a href=\"parallel_stack\"><code translate=\"no\" dir=\"ltr\">parallel_stack(...)</code></a>: Stacks a list of rank-<code translate=\"no\" dir=\"ltr\">R</code> tensors into one rank-<code translate=\"no\" dir=\"ltr\">(R+1)</code> tensor in parallel.</p> <p><a href=\"math/pow\"><code translate=\"no\" dir=\"ltr\">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href=\"print\"><code translate=\"no\" dir=\"ltr\">print(...)</code></a>: Print the specified inputs.</p> <p><a href=\"py_function\"><code translate=\"no\" dir=\"ltr\">py_function(...)</code></a>: Wraps a python function into a TensorFlow op that executes it eagerly.</p> <p><a href=\"quantize_and_dequantize_v4\"><code translate=\"no\" dir=\"ltr\">quantize_and_dequantize_v4(...)</code></a>: Returns the gradient of <code translate=\"no\" dir=\"ltr\">QuantizeAndDequantizeV4</code>.</p> <p><a href=\"range\"><code translate=\"no\" dir=\"ltr\">range(...)</code></a>: Creates a sequence of numbers.</p> <p><a href=\"rank\"><code translate=\"no\" dir=\"ltr\">rank(...)</code></a>: Returns the rank of a tensor.</p> <p><a href=\"realdiv\"><code translate=\"no\" dir=\"ltr\">realdiv(...)</code></a>: Returns x / y element-wise for real types.</p> <p><a href=\"recompute_grad\"><code translate=\"no\" dir=\"ltr\">recompute_grad(...)</code></a>: An eager-compatible version of recompute_grad.</p> <p><a href=\"math/reduce_all\"><code translate=\"no\" dir=\"ltr\">reduce_all(...)</code></a>: Computes the \"logical and\" of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_any\"><code translate=\"no\" dir=\"ltr\">reduce_any(...)</code></a>: Computes the \"logical or\" of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_logsumexp\"><code translate=\"no\" dir=\"ltr\">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))).</p> <p><a href=\"math/reduce_max\"><code translate=\"no\" dir=\"ltr\">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_mean\"><code translate=\"no\" dir=\"ltr\">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_min\"><code translate=\"no\" dir=\"ltr\">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_prod\"><code translate=\"no\" dir=\"ltr\">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_sum\"><code translate=\"no\" dir=\"ltr\">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor.</p> <p><a href=\"register_tensor_conversion_function\"><code translate=\"no\" dir=\"ltr\">register_tensor_conversion_function(...)</code></a>: Registers a function for converting objects of <code translate=\"no\" dir=\"ltr\">base_type</code> to <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"repeat\"><code translate=\"no\" dir=\"ltr\">repeat(...)</code></a>: Repeat elements of <code translate=\"no\" dir=\"ltr\">input</code>.</p> <p><a href=\"required_space_to_batch_paddings\"><code translate=\"no\" dir=\"ltr\">required_space_to_batch_paddings(...)</code></a>: Calculate padding required to make block_shape divide input_shape.</p> <p><a href=\"reshape\"><code translate=\"no\" dir=\"ltr\">reshape(...)</code></a>: Reshapes a tensor.</p> <p><a href=\"reverse\"><code translate=\"no\" dir=\"ltr\">reverse(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href=\"reverse_sequence\"><code translate=\"no\" dir=\"ltr\">reverse_sequence(...)</code></a>: Reverses variable length slices.</p> <p><a href=\"roll\"><code translate=\"no\" dir=\"ltr\">roll(...)</code></a>: Rolls the elements of a tensor along an axis.</p> <p><a href=\"math/round\"><code translate=\"no\" dir=\"ltr\">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href=\"dtypes/saturate_cast\"><code translate=\"no\" dir=\"ltr\">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate=\"no\" dir=\"ltr\">value</code> to <code translate=\"no\" dir=\"ltr\">dtype</code>.</p> <p><a href=\"math/scalar_mul\"><code translate=\"no\" dir=\"ltr\">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> object.</p> <p><a href=\"scan\"><code translate=\"no\" dir=\"ltr\">scan(...)</code></a>: scan on the list of tensors unpacked from <code translate=\"no\" dir=\"ltr\">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href=\"scatter_nd\"><code translate=\"no\" dir=\"ltr\">scatter_nd(...)</code></a>: Scatter <code translate=\"no\" dir=\"ltr\">updates</code> into a new tensor according to <code translate=\"no\" dir=\"ltr\">indices</code>.</p> <p><a href=\"searchsorted\"><code translate=\"no\" dir=\"ltr\">searchsorted(...)</code></a>: Searches input tensor for values on the innermost dimension.</p> <p><a href=\"sequence_mask\"><code translate=\"no\" dir=\"ltr\">sequence_mask(...)</code></a>: Returns a mask tensor representing the first N positions of each cell.</p> <p><a href=\"shape\"><code translate=\"no\" dir=\"ltr\">shape(...)</code></a>: Returns a tensor containing the shape of the input tensor.</p> <p><a href=\"shape_n\"><code translate=\"no\" dir=\"ltr\">shape_n(...)</code></a>: Returns shape of tensors.</p> <p><a href=\"math/sigmoid\"><code translate=\"no\" dir=\"ltr\">sigmoid(...)</code></a>: Computes sigmoid of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/sign\"><code translate=\"no\" dir=\"ltr\">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href=\"math/sin\"><code translate=\"no\" dir=\"ltr\">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href=\"math/sinh\"><code translate=\"no\" dir=\"ltr\">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href=\"size\"><code translate=\"no\" dir=\"ltr\">size(...)</code></a>: Returns the size of a tensor.</p> <p><a href=\"slice\"><code translate=\"no\" dir=\"ltr\">slice(...)</code></a>: Extracts a slice from a tensor.</p> <p><a href=\"sort\"><code translate=\"no\" dir=\"ltr\">sort(...)</code></a>: Sorts a tensor.</p> <p><a href=\"space_to_batch\"><code translate=\"no\" dir=\"ltr\">space_to_batch(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href=\"space_to_batch_nd\"><code translate=\"no\" dir=\"ltr\">space_to_batch_nd(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href=\"split\"><code translate=\"no\" dir=\"ltr\">split(...)</code></a>: Splits a tensor <code translate=\"no\" dir=\"ltr\">value</code> into a list of sub tensors.</p> <p><a href=\"math/sqrt\"><code translate=\"no\" dir=\"ltr\">sqrt(...)</code></a>: Computes element-wise square root of the input tensor.</p> <p><a href=\"math/square\"><code translate=\"no\" dir=\"ltr\">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href=\"squeeze\"><code translate=\"no\" dir=\"ltr\">squeeze(...)</code></a>: Removes dimensions of size 1 from the shape of a tensor.</p> <p><a href=\"stack\"><code translate=\"no\" dir=\"ltr\">stack(...)</code></a>: Stacks a list of rank-<code translate=\"no\" dir=\"ltr\">R</code> tensors into one rank-<code translate=\"no\" dir=\"ltr\">(R+1)</code> tensor.</p> <p><a href=\"stop_gradient\"><code translate=\"no\" dir=\"ltr\">stop_gradient(...)</code></a>: Stops gradient computation.</p> <p><a href=\"strided_slice\"><code translate=\"no\" dir=\"ltr\">strided_slice(...)</code></a>: Extracts a strided slice of a tensor (generalized Python array indexing).</p> <p><a href=\"math/subtract\"><code translate=\"no\" dir=\"ltr\">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href=\"switch_case\"><code translate=\"no\" dir=\"ltr\">switch_case(...)</code></a>: Create a switch/case operation, i.e. an integer-indexed conditional.</p> <p><a href=\"math/tan\"><code translate=\"no\" dir=\"ltr\">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href=\"math/tanh\"><code translate=\"no\" dir=\"ltr\">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"tensor_scatter_nd_add\"><code translate=\"no\" dir=\"ltr\">tensor_scatter_nd_add(...)</code></a>: Adds sparse <code translate=\"no\" dir=\"ltr\">updates</code> to an existing tensor according to <code translate=\"no\" dir=\"ltr\">indices</code>.</p> <p><a href=\"tensor_scatter_nd_max\"><code translate=\"no\" dir=\"ltr\">tensor_scatter_nd_max(...)</code></a></p> <p><a href=\"tensor_scatter_nd_min\"><code translate=\"no\" dir=\"ltr\">tensor_scatter_nd_min(...)</code></a></p> <p><a href=\"tensor_scatter_nd_sub\"><code translate=\"no\" dir=\"ltr\">tensor_scatter_nd_sub(...)</code></a>: Subtracts sparse <code translate=\"no\" dir=\"ltr\">updates</code> from an existing tensor according to <code translate=\"no\" dir=\"ltr\">indices</code>.</p> <p><a href=\"tensor_scatter_nd_update\"><code translate=\"no\" dir=\"ltr\">tensor_scatter_nd_update(...)</code></a>: \"Scatter <code translate=\"no\" dir=\"ltr\">updates</code> into an existing tensor according to <code translate=\"no\" dir=\"ltr\">indices</code>.</p> <p><a href=\"tensordot\"><code translate=\"no\" dir=\"ltr\">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href=\"tile\"><code translate=\"no\" dir=\"ltr\">tile(...)</code></a>: Constructs a tensor by tiling a given tensor.</p> <p><a href=\"timestamp\"><code translate=\"no\" dir=\"ltr\">timestamp(...)</code></a>: Provides the time since epoch in seconds.</p> <p><a href=\"transpose\"><code translate=\"no\" dir=\"ltr\">transpose(...)</code></a>: Transposes <code translate=\"no\" dir=\"ltr\">a</code>, where <code translate=\"no\" dir=\"ltr\">a</code> is a Tensor.</p> <p><a href=\"math/truediv\"><code translate=\"no\" dir=\"ltr\">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href=\"truncatediv\"><code translate=\"no\" dir=\"ltr\">truncatediv(...)</code></a>: Returns x / y element-wise for integer types.</p> <p><a href=\"truncatemod\"><code translate=\"no\" dir=\"ltr\">truncatemod(...)</code></a>: Returns element-wise remainder of division. This emulates C semantics in that</p> <p><a href=\"tuple\"><code translate=\"no\" dir=\"ltr\">tuple(...)</code></a>: Group tensors together.</p> <p><a href=\"type_spec_from_value\"><code translate=\"no\" dir=\"ltr\">type_spec_from_value(...)</code></a>: Returns a <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">tf.TypeSpec</code></a> that represents the given <code translate=\"no\" dir=\"ltr\">value</code>.</p> <p><a href=\"unique\"><code translate=\"no\" dir=\"ltr\">unique(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href=\"unique_with_counts\"><code translate=\"no\" dir=\"ltr\">unique_with_counts(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href=\"unravel_index\"><code translate=\"no\" dir=\"ltr\">unravel_index(...)</code></a>: Converts an array of flat indices into a tuple of coordinate arrays.</p> <p><a href=\"unstack\"><code translate=\"no\" dir=\"ltr\">unstack(...)</code></a>: Unpacks the given dimension of a rank-<code translate=\"no\" dir=\"ltr\">R</code> tensor into rank-<code translate=\"no\" dir=\"ltr\">(R-1)</code> tensors.</p> <p><a href=\"variable_creator_scope\"><code translate=\"no\" dir=\"ltr\">variable_creator_scope(...)</code></a>: Scope which defines a variable creation function to be used by variable().</p> <p><a href=\"vectorized_map\"><code translate=\"no\" dir=\"ltr\">vectorized_map(...)</code></a>: Parallel map on the list of tensors unpacked from <code translate=\"no\" dir=\"ltr\">elems</code> on dimension 0.</p> <p><a href=\"where\"><code translate=\"no\" dir=\"ltr\">where(...)</code></a>: Return the elements where <code translate=\"no\" dir=\"ltr\">condition</code> is <code translate=\"no\" dir=\"ltr\">True</code> (multiplexing <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>).</p> <p><a href=\"while_loop\"><code translate=\"no\" dir=\"ltr\">while_loop(...)</code></a>: Repeat <code translate=\"no\" dir=\"ltr\">body</code> while the condition <code translate=\"no\" dir=\"ltr\">cond</code> is true. (deprecated argument values)</p> <p><a href=\"zeros\"><code translate=\"no\" dir=\"ltr\">zeros(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href=\"zeros_like\"><code translate=\"no\" dir=\"ltr\">zeros_like(...)</code></a>: Creates a tensor with all elements set to zero.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> <strong>version</strong> </td> <td> <code translate=\"no\" dir=\"ltr\">'2.4.0'</code> </td> </tr>\n<tr> <td> bfloat16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> bool </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> complex128 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> complex64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> double </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> half </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> newaxis </td> <td> <code translate=\"no\" dir=\"ltr\">None</code> </td> </tr>\n<tr> <td> qint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> qint32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> qint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> quint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> quint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> resource </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> string </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> variant </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf</a>\n  </p>\n</div>\n","audio":"<h1 class=\"devsite-page-title\">Module: tf.audio</h1>       <p>Public API for tf.audio namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"audio/decode_wav\"><code translate=\"no\" dir=\"ltr\">decode_wav(...)</code></a>: Decode a 16-bit PCM WAV file to a float tensor.</p> <p><a href=\"audio/encode_wav\"><code translate=\"no\" dir=\"ltr\">encode_wav(...)</code></a>: Encode audio data using the WAV file format.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/audio\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/audio</a>\n  </p>\n</div>\n","data":"<h1 class=\"devsite-page-title\">Module: tf.data</h1>       <p><a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a> API for input pipelines.</p> <p>See <a href=\"https://tensorflow.org/guide/data\">Importing Data</a> for an overview.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"data/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Experimental API for building input pipelines.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">class Dataset</code></a>: Represents a potentially large set of elements.</p> <p><a href=\"data/datasetspec\"><code translate=\"no\" dir=\"ltr\">class DatasetSpec</code></a>: Type specification for <a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a>.</p> <p><a href=\"data/fixedlengthrecorddataset\"><code translate=\"no\" dir=\"ltr\">class FixedLengthRecordDataset</code></a>: A <code translate=\"no\" dir=\"ltr\">Dataset</code> of fixed-length records from one or more binary files.</p> <p><a href=\"data/iterator\"><code translate=\"no\" dir=\"ltr\">class Iterator</code></a>: Represents an iterator of a <a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a>.</p> <p><a href=\"data/iteratorspec\"><code translate=\"no\" dir=\"ltr\">class IteratorSpec</code></a>: Type specification for <a href=\"data/iterator\"><code translate=\"no\" dir=\"ltr\">tf.data.Iterator</code></a>.</p> <p><a href=\"data/options\"><code translate=\"no\" dir=\"ltr\">class Options</code></a>: Represents options for <a href=\"data/dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a>.</p> <p><a href=\"data/tfrecorddataset\"><code translate=\"no\" dir=\"ltr\">class TFRecordDataset</code></a>: A <code translate=\"no\" dir=\"ltr\">Dataset</code> comprising records from one or more TFRecord files.</p> <p><a href=\"data/textlinedataset\"><code translate=\"no\" dir=\"ltr\">class TextLineDataset</code></a>: A <code translate=\"no\" dir=\"ltr\">Dataset</code> comprising lines from one or more text files.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> AUTOTUNE </td> <td> <code translate=\"no\" dir=\"ltr\">-1</code> </td> </tr>\n<tr> <td> INFINITE_CARDINALITY </td> <td> <code translate=\"no\" dir=\"ltr\">-1</code> </td> </tr>\n<tr> <td> UNKNOWN_CARDINALITY </td> <td> <code translate=\"no\" dir=\"ltr\">-2</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data</a>\n  </p>\n</div>\n","compat":"<h1 class=\"devsite-page-title\">Module: tf.compat</h1>       <p>Compatibility functions.</p> <p>The <a href=\"compat\"><code translate=\"no\" dir=\"ltr\">tf.compat</code></a> module contains two sets of compatibility functions.</p> <h2 id=\"tensorflow_1x_and_2x_apis\" data-text=\"Tensorflow 1.x and 2.x APIs\">Tensorflow 1.x and 2.x APIs</h2> <p>The <a href=\"compat/v1\"><code translate=\"no\" dir=\"ltr\">compat.v1</code></a> and <code translate=\"no\" dir=\"ltr\">compat.v2</code> submodules provide a complete copy of both the <a href=\"compat/v1\"><code translate=\"no\" dir=\"ltr\">v1</code></a> and <code translate=\"no\" dir=\"ltr\">v2</code> APIs for backwards and forwards compatibility across TensorFlow versions 1.x and 2.x. See the <a href=\"https://www.tensorflow.org/guide/migrate\">migration guide</a> for details.</p> <h2 id=\"utilities_for_writing_compatible_code\" data-text=\"Utilities for writing compatible code\">Utilities for writing compatible code</h2> <p>Aside from the <a href=\"compat/v1\"><code translate=\"no\" dir=\"ltr\">compat.v1</code></a> and <code translate=\"no\" dir=\"ltr\">compat.v2</code> submodules, <a href=\"compat\"><code translate=\"no\" dir=\"ltr\">tf.compat</code></a> also contains a set of helper functions for writing code that works in both:</p> <ul> <li>TensorFlow 1.x and 2.x</li> <li>Python 2 and 3</li> </ul> <h2 id=\"type_collections\" data-text=\"Type collections\">Type collections</h2> <p>The compatibility module also provides the following aliases for common sets of python types:</p> <ul> <li><code translate=\"no\" dir=\"ltr\">bytes_or_text_types</code></li> <li><code translate=\"no\" dir=\"ltr\">complex_types</code></li> <li><code translate=\"no\" dir=\"ltr\">integral_types</code></li> <li><code translate=\"no\" dir=\"ltr\">real_types</code></li> </ul> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"compat/v1\"><code translate=\"no\" dir=\"ltr\">v1</code></a> module: Bring in all of the public TensorFlow interface into this module.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"compat/as_bytes\"><code translate=\"no\" dir=\"ltr\">as_bytes(...)</code></a>: Converts <code translate=\"no\" dir=\"ltr\">bytearray</code>, <code translate=\"no\" dir=\"ltr\">bytes</code>, or unicode python input types to <code translate=\"no\" dir=\"ltr\">bytes</code>.</p> <p><a href=\"compat/as_str\"><code translate=\"no\" dir=\"ltr\">as_str(...)</code></a></p> <p><a href=\"compat/as_str_any\"><code translate=\"no\" dir=\"ltr\">as_str_any(...)</code></a>: Converts input to <code translate=\"no\" dir=\"ltr\">str</code> type.</p> <p><a href=\"compat/as_text\"><code translate=\"no\" dir=\"ltr\">as_text(...)</code></a>: Converts any string-like python input types to unicode.</p> <p><a href=\"compat/dimension_at_index\"><code translate=\"no\" dir=\"ltr\">dimension_at_index(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href=\"compat/dimension_value\"><code translate=\"no\" dir=\"ltr\">dimension_value(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href=\"compat/forward_compatibility_horizon\"><code translate=\"no\" dir=\"ltr\">forward_compatibility_horizon(...)</code></a>: Context manager for testing forward compatibility of generated graphs.</p> <p><a href=\"compat/forward_compatible\"><code translate=\"no\" dir=\"ltr\">forward_compatible(...)</code></a>: Return true if the forward compatibility window has expired.</p> <p><a href=\"compat/path_to_str\"><code translate=\"no\" dir=\"ltr\">path_to_str(...)</code></a>: Converts input which is a <code translate=\"no\" dir=\"ltr\">PathLike</code> object to <code translate=\"no\" dir=\"ltr\">str</code> type.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> bytes_or_text_types </td> <td> \n</td> </tr>\n<tr> <td> complex_types </td> <td> \n</td> </tr>\n<tr> <td> integral_types </td> <td> \n</td> </tr>\n<tr> <td> real_types </td> <td> \n</td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat</a>\n  </p>\n</div>\n","keras":"<h1 class=\"devsite-page-title\">Module: tf.keras</h1>       <p>Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p>Detailed documentation and user guides are available at <a href=\"https://www.tensorflow.org/guide/keras\">tensorflow.org</a>.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"keras/activations\"><code translate=\"no\" dir=\"ltr\">activations</code></a> module: Built-in activation functions.</p> <p><a href=\"keras/applications\"><code translate=\"no\" dir=\"ltr\">applications</code></a> module: Keras Applications are canned architectures with pre-trained weights.</p> <p><a href=\"keras/backend\"><code translate=\"no\" dir=\"ltr\">backend</code></a> module: Keras backend API.</p> <p><a href=\"keras/callbacks\"><code translate=\"no\" dir=\"ltr\">callbacks</code></a> module: Callbacks: utilities called at certain points during model training.</p> <p><a href=\"keras/constraints\"><code translate=\"no\" dir=\"ltr\">constraints</code></a> module: Constraints: functions that impose constraints on weight values.</p> <p><a href=\"keras/datasets\"><code translate=\"no\" dir=\"ltr\">datasets</code></a> module: Public API for tf.keras.datasets namespace.</p> <p><a href=\"keras/estimator\"><code translate=\"no\" dir=\"ltr\">estimator</code></a> module: Keras estimator API.</p> <p><a href=\"keras/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.keras.experimental namespace.</p> <p><a href=\"keras/initializers\"><code translate=\"no\" dir=\"ltr\">initializers</code></a> module: Keras initializer serialization / deserialization.</p> <p><a href=\"keras/layers\"><code translate=\"no\" dir=\"ltr\">layers</code></a> module: Keras layers API.</p> <p><a href=\"keras/losses\"><code translate=\"no\" dir=\"ltr\">losses</code></a> module: Built-in loss functions.</p> <p><a href=\"keras/metrics\"><code translate=\"no\" dir=\"ltr\">metrics</code></a> module: Built-in metrics.</p> <p><a href=\"keras/mixed_precision\"><code translate=\"no\" dir=\"ltr\">mixed_precision</code></a> module: Keras mixed precision API.</p> <p><a href=\"keras/models\"><code translate=\"no\" dir=\"ltr\">models</code></a> module: Code for model cloning, plus model-related API entries.</p> <p><a href=\"keras/optimizers\"><code translate=\"no\" dir=\"ltr\">optimizers</code></a> module: Built-in optimizer classes.</p> <p><a href=\"keras/preprocessing\"><code translate=\"no\" dir=\"ltr\">preprocessing</code></a> module: Keras data preprocessing utils.</p> <p><a href=\"keras/regularizers\"><code translate=\"no\" dir=\"ltr\">regularizers</code></a> module: Built-in regularizers.</p> <p><a href=\"keras/utils\"><code translate=\"no\" dir=\"ltr\">utils</code></a> module: Public API for tf.keras.utils namespace.</p> <p><a href=\"keras/wrappers\"><code translate=\"no\" dir=\"ltr\">wrappers</code></a> module: Public API for tf.keras.wrappers namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"keras/model\"><code translate=\"no\" dir=\"ltr\">class Model</code></a>: <code translate=\"no\" dir=\"ltr\">Model</code> groups layers into an object with training and inference features.</p> <p><a href=\"keras/sequential\"><code translate=\"no\" dir=\"ltr\">class Sequential</code></a>: <code translate=\"no\" dir=\"ltr\">Sequential</code> groups a linear stack of layers into a <a href=\"keras/model\"><code translate=\"no\" dir=\"ltr\">tf.keras.Model</code></a>.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"keras/input\"><code translate=\"no\" dir=\"ltr\">Input(...)</code></a>: <code translate=\"no\" dir=\"ltr\">Input()</code> is used to instantiate a Keras tensor.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> <strong>version</strong> </td> <td> <code translate=\"no\" dir=\"ltr\">'2.4.0'</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras</a>\n  </p>\n</div>\n","config":"<h1 class=\"devsite-page-title\">Module: tf.config</h1>       <p>Public API for tf.config namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"config/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.config.experimental namespace.</p> <p><a href=\"config/optimizer\"><code translate=\"no\" dir=\"ltr\">optimizer</code></a> module: Public API for tf.config.optimizer namespace.</p> <p><a href=\"config/threading\"><code translate=\"no\" dir=\"ltr\">threading</code></a> module: Public API for tf.config.threading namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"config/logicaldevice\"><code translate=\"no\" dir=\"ltr\">class LogicalDevice</code></a>: Abstraction for a logical device initialized by the runtime.</p> <p><a href=\"config/logicaldeviceconfiguration\"><code translate=\"no\" dir=\"ltr\">class LogicalDeviceConfiguration</code></a>: Configuration class for a logical devices.</p> <p><a href=\"config/physicaldevice\"><code translate=\"no\" dir=\"ltr\">class PhysicalDevice</code></a>: Abstraction for a locally visible physical device.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"config/experimental_connect_to_cluster\"><code translate=\"no\" dir=\"ltr\">experimental_connect_to_cluster(...)</code></a>: Connects to the given cluster.</p> <p><a href=\"config/experimental_connect_to_host\"><code translate=\"no\" dir=\"ltr\">experimental_connect_to_host(...)</code></a>: Connects to a single machine to enable remote execution on it.</p> <p><a href=\"config/experimental_functions_run_eagerly\"><code translate=\"no\" dir=\"ltr\">experimental_functions_run_eagerly(...)</code></a>: Returns the value of the <code translate=\"no\" dir=\"ltr\">experimental_run_functions_eagerly</code> setting. (deprecated)</p> <p><a href=\"config/experimental_run_functions_eagerly\"><code translate=\"no\" dir=\"ltr\">experimental_run_functions_eagerly(...)</code></a>: Enables / disables eager execution of <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>s. (deprecated)</p> <p><a href=\"config/functions_run_eagerly\"><code translate=\"no\" dir=\"ltr\">functions_run_eagerly(...)</code></a>: Returns the value of the <code translate=\"no\" dir=\"ltr\">run_functions_eagerly</code> setting.</p> <p><a href=\"config/get_logical_device_configuration\"><code translate=\"no\" dir=\"ltr\">get_logical_device_configuration(...)</code></a>: Get the virtual device configuration for a <a href=\"config/physicaldevice\"><code translate=\"no\" dir=\"ltr\">tf.config.PhysicalDevice</code></a>.</p> <p><a href=\"config/get_soft_device_placement\"><code translate=\"no\" dir=\"ltr\">get_soft_device_placement(...)</code></a>: Get if soft device placement is enabled.</p> <p><a href=\"config/get_visible_devices\"><code translate=\"no\" dir=\"ltr\">get_visible_devices(...)</code></a>: Get the list of visible physical devices.</p> <p><a href=\"config/list_logical_devices\"><code translate=\"no\" dir=\"ltr\">list_logical_devices(...)</code></a>: Return a list of logical devices created by runtime.</p> <p><a href=\"config/list_physical_devices\"><code translate=\"no\" dir=\"ltr\">list_physical_devices(...)</code></a>: Return a list of physical devices visible to the host runtime.</p> <p><a href=\"config/run_functions_eagerly\"><code translate=\"no\" dir=\"ltr\">run_functions_eagerly(...)</code></a>: Enables / disables eager execution of <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>s.</p> <p><a href=\"config/set_logical_device_configuration\"><code translate=\"no\" dir=\"ltr\">set_logical_device_configuration(...)</code></a>: Set the logical device configuration for a <a href=\"config/physicaldevice\"><code translate=\"no\" dir=\"ltr\">tf.config.PhysicalDevice</code></a>.</p> <p><a href=\"config/set_soft_device_placement\"><code translate=\"no\" dir=\"ltr\">set_soft_device_placement(...)</code></a>: Set if soft device placement is enabled.</p> <p><a href=\"config/set_visible_devices\"><code translate=\"no\" dir=\"ltr\">set_visible_devices(...)</code></a>: Set the list of visible devices.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/config\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/config</a>\n  </p>\n</div>\n","data/dataset":"<h1 class=\"devsite-page-title\">tf.data.Dataset</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L110-L2275\">  View source on GitHub </a> </td> </table> <p>Represents a potentially large set of elements.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.data.Dataset(\n    variant_tensor\n)\n</pre>  <p>The <a href=\"dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a> API supports writing descriptive and efficient input pipelines. <code translate=\"no\" dir=\"ltr\">Dataset</code> usage follows a common pattern:</p> <ol> <li>Create a source dataset from your input data.</li> <li>Apply dataset transformations to preprocess the data.</li> <li>Iterate over the dataset and process the elements.</li> </ol> <p>Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.</p> <h4 id=\"source_datasets\" data-text=\"Source Datasets:\">Source Datasets:</h4> <p>The simplest way to create a dataset is to create it from a python <code translate=\"no\" dir=\"ltr\">list</code>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n</pre> <p>To process lines from files, use <a href=\"textlinedataset\"><code translate=\"no\" dir=\"ltr\">tf.data.TextLineDataset</code></a>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n</pre> <p>To process records written in the <code translate=\"no\" dir=\"ltr\">TFRecord</code> format, use <code translate=\"no\" dir=\"ltr\">TFRecordDataset</code>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\n</pre> <p>To create a dataset of all files matching a pattern, use <a href=\"dataset#list_files\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.list_files</code></a>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.list_files(\"/path/*.txt\")  # doctest: +SKIP\n</pre> <p>See <a href=\"fixedlengthrecorddataset\"><code translate=\"no\" dir=\"ltr\">tf.data.FixedLengthRecordDataset</code></a> and <a href=\"dataset#from_generator\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.from_generator</code></a> for more ways to create datasets.</p> <h4 id=\"transformations\" data-text=\"Transformations:\">Transformations:</h4> <p>Once you have a dataset, you can apply transformations to prepare the data for your model:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.map(lambda x: x*2)\nlist(dataset.as_numpy_iterator())\n[2, 4, 6]\n</pre> <h4 id=\"common_terms\" data-text=\"Common Terms:\">Common Terms:</h4> <p><strong>Element</strong>: A single output from calling <code translate=\"no\" dir=\"ltr\">next()</code> on a dataset iterator. Elements may be nested structures containing multiple components. For example, the element <code translate=\"no\" dir=\"ltr\">(1, (3, \"apple\"))</code> has one tuple nested in another tuple. The components are <code translate=\"no\" dir=\"ltr\">1</code>, <code translate=\"no\" dir=\"ltr\">3</code>, and <code translate=\"no\" dir=\"ltr\">\"apple\"</code>.</p> <p><strong>Component</strong>: The leaf in the nested structure of an element.</p> <h4 id=\"supported_types\" data-text=\"Supported types:\">Supported types:</h4> <p>Elements can be nested structures of tuples, named tuples, and dictionaries. Note that Python lists are <em>not</em> treated as nested structures of components. Instead, lists are converted to tensors and treated as components. For example, the element <code translate=\"no\" dir=\"ltr\">(1, [1, 2, 3])</code> has only two components; the tensor <code translate=\"no\" dir=\"ltr\">1</code> and the tensor <code translate=\"no\" dir=\"ltr\">[1, 2, 3]</code>. Element components can be of any type representable by <a href=\"../typespec\"><code translate=\"no\" dir=\"ltr\">tf.TypeSpec</code></a>, including <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, <a href=\"dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a>, <a href=\"../sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>, <a href=\"../raggedtensor\"><code translate=\"no\" dir=\"ltr\">tf.RaggedTensor</code></a>, and <a href=\"../tensorarray\"><code translate=\"no\" dir=\"ltr\">tf.TensorArray</code></a>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = 1 # Integer element\nb = 2.0 # Float element\nc = (1, 2) # Tuple element with 2 components\nd = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\nPoint = collections.namedtuple(\"Point\", [\"x\", \"y\"]) # doctest: +SKIP\ne = Point(1, 2) # Named tuple # doctest: +SKIP\nf = tf.data.Dataset.range(10) # Dataset element\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">variant_tensor</code> </td> <td> A DT_VARIANT tensor that represents the dataset. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_spec</code> </td> <td> The type specification of an element of this dataset. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n</pre> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"apply\" data-text=\"apply\"><code translate=\"no\" dir=\"ltr\">apply</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1974-L2002\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\napply(\n    transformation_func\n)\n</pre> <p>Applies a transformation function to this dataset.</p> <p><code translate=\"no\" dir=\"ltr\">apply</code> enables chaining of custom <code translate=\"no\" dir=\"ltr\">Dataset</code> transformations, which are represented as functions that take one <code translate=\"no\" dir=\"ltr\">Dataset</code> argument and return a transformed <code translate=\"no\" dir=\"ltr\">Dataset</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(100)\ndef dataset_fn(ds):\n  return ds.filter(lambda x: x &lt; 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">transformation_func</code> </td> <td> A function that takes one <code translate=\"no\" dir=\"ltr\">Dataset</code> argument and returns a <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Dataset</code> returned by applying <code translate=\"no\" dir=\"ltr\">transformation_func</code> to this dataset. </td> </tr> </table> <h3 id=\"as_numpy_iterator\" data-text=\"as_numpy_iterator\"><code translate=\"no\" dir=\"ltr\">as_numpy_iterator</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L479-L535\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nas_numpy_iterator()\n</pre> <p>Returns an iterator which converts all elements of the dataset to numpy.</p> <p>Use <code translate=\"no\" dir=\"ltr\">as_numpy_iterator</code> to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using <code translate=\"no\" dir=\"ltr\">as_numpy_iterator</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n</pre> <p>This method requires that you are running in eager mode and the dataset's element_spec contains only <code translate=\"no\" dir=\"ltr\">TensorSpec</code> components.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n1\n2\n3\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n</pre> <p><code translate=\"no\" dir=\"ltr\">as_numpy_iterator()</code> will preserve the nested structure of dataset elements.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> An iterable over the elements of the dataset, with their tensors converted to numpy arrays. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> if an element contains a non-<code translate=\"no\" dir=\"ltr\">Tensor</code> value. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> if eager execution is not enabled. </td> </tr> </table> <h3 id=\"batch\" data-text=\"batch\"><code translate=\"no\" dir=\"ltr\">batch</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1508-L1539\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nbatch(\n    batch_size, drop_remainder=False\n)\n</pre> <p>Combines consecutive elements of this dataset into batches.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n</pre> <p>The components of the resulting element will have an additional outer dimension, which will be <code translate=\"no\" dir=\"ltr\">batch_size</code> (or <code translate=\"no\" dir=\"ltr\">N % batch_size</code> for the last element if <code translate=\"no\" dir=\"ltr\">batch_size</code> does not divide the number of input elements <code translate=\"no\" dir=\"ltr\">N</code> evenly and <code translate=\"no\" dir=\"ltr\">drop_remainder</code> is <code translate=\"no\" dir=\"ltr\">False</code>). If your program depends on the batches having the same outer dimension, you should set the <code translate=\"no\" dir=\"ltr\">drop_remainder</code> argument to <code translate=\"no\" dir=\"ltr\">True</code> to prevent the smaller batch from being produced.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">batch_size</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of consecutive elements of this dataset to combine in a single batch. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">drop_remainder</code> </td> <td> (Optional.) A <a href=\"../../tf#bool\"><code translate=\"no\" dir=\"ltr\">tf.bool</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing whether the last batch should be dropped in the case it has fewer than <code translate=\"no\" dir=\"ltr\">batch_size</code> elements; the default behavior is not to drop the smaller batch. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"cache\" data-text=\"cache\"><code translate=\"no\" dir=\"ltr\">cache</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1352-L1400\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ncache(\n    filename=''\n)\n</pre> <p>Caches the elements in this dataset.</p> <p>The first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> For the cache to be finalized, the input dataset must be iterated through in its entirety. Otherwise, subsequent iterations will not use cached data.</span>\n</blockquote> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n</pre> <p>When caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to <code translate=\"no\" dir=\"ltr\">.cache()</code> will have no effect until the cache file is removed or the filename is changed.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")  # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file! # doctest: +SKIP\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[0, 1, 2, 3, 4]\n</pre> <blockquote class=\"note\">\n<strong>Note:</strong><span> <code translate=\"no\" dir=\"ltr\">cache</code> will produce exactly the same elements during each iteration through the dataset. If you wish to randomize the iteration order, make sure to call <code translate=\"no\" dir=\"ltr\">shuffle</code> <em>after</em> calling <code translate=\"no\" dir=\"ltr\">cache</code>.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">filename</code> </td> <td> A <a href=\"../../tf#string\"><code translate=\"no\" dir=\"ltr\">tf.string</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"cardinality\" data-text=\"cardinality\"><code translate=\"no\" dir=\"ltr\">cardinality</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L2249-L2275\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ncardinality()\n</pre> <p>Returns the cardinality of the dataset, if known.</p> <p><code translate=\"no\" dir=\"ltr\">cardinality</code> may return <a href=\"../data#INFINITE_CARDINALITY\"><code translate=\"no\" dir=\"ltr\">tf.data.INFINITE_CARDINALITY</code></a> if the dataset contains an infinite number of elements or <a href=\"../data#UNKNOWN_CARDINALITY\"><code translate=\"no\" dir=\"ltr\">tf.data.UNKNOWN_CARDINALITY</code></a> if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A scalar <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> <code translate=\"no\" dir=\"ltr\">Tensor</code> representing the cardinality of the dataset. If the cardinality is infinite or unknown, <code translate=\"no\" dir=\"ltr\">cardinality</code> returns the named constants <a href=\"../data#INFINITE_CARDINALITY\"><code translate=\"no\" dir=\"ltr\">tf.data.INFINITE_CARDINALITY</code></a> and <a href=\"../data#UNKNOWN_CARDINALITY\"><code translate=\"no\" dir=\"ltr\">tf.data.UNKNOWN_CARDINALITY</code></a> respectively. </td> </tr> \n</table> <h3 id=\"concatenate\" data-text=\"concatenate\"><code translate=\"no\" dir=\"ltr\">concatenate</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1112-L1139\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nconcatenate(\n    dataset\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> by concatenating the given dataset with this dataset.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.data.Dataset.range(1, 4)  # ==&gt; [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==&gt; [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have the same\n# nested structures and output types.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n&lt;dtype: 'int64'&gt; and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n&lt;dtype: 'int64'&gt; and &lt;dtype: 'string'&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dataset</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> to be concatenated. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"enumerate\" data-text=\"enumerate\"><code translate=\"no\" dir=\"ltr\">enumerate</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1259-L1290\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nenumerate(\n    start=0\n)\n</pre> <p>Enumerates the elements of this dataset.</p> <p>It is similar to python's <code translate=\"no\" dir=\"ltr\">enumerate</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# The nested structure of the input dataset determines the structure of\n# elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">start</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the start value for enumeration. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"filter\" data-text=\"filter\"><code translate=\"no\" dir=\"ltr\">filter</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1951-L1972\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nfilter(\n    predicate\n)\n</pre> <p>Filters this dataset according to <code translate=\"no\" dir=\"ltr\">predicate</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x &lt; 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndef filter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">predicate</code> </td> <td> A function mapping a dataset element to a boolean. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Dataset</code> containing the elements of this dataset for which <code translate=\"no\" dir=\"ltr\">predicate</code> is <code translate=\"no\" dir=\"ltr\">True</code>. </td> </tr> </table> <h3 id=\"flat_map\" data-text=\"flat_map\"><code translate=\"no\" dir=\"ltr\">flat_map</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1814-L1837\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nflat_map(\n    map_func\n)\n</pre> <p>Maps <code translate=\"no\" dir=\"ltr\">map_func</code> across this dataset and flattens the result.</p> <p>Use <code translate=\"no\" dir=\"ltr\">flat_map</code> if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices(\n               [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(lambda x: Dataset.from_tensor_slices(x))\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n</pre> <p><a href=\"dataset#interleave\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.interleave()</code></a> is a generalization of <code translate=\"no\" dir=\"ltr\">flat_map</code>, since <code translate=\"no\" dir=\"ltr\">flat_map</code> produces the same output as <a href=\"dataset#interleave\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.interleave(cycle_length=1)</code></a></p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">map_func</code> </td> <td> A function mapping a dataset element to a dataset. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"from_generator\" data-text=\"from_generator\"><code translate=\"no\" dir=\"ltr\">from_generator</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L729-L1028\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_generator(\n    generator, output_types=None, output_shapes=None, args=None,\n    output_signature=None\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> whose elements are generated by <code translate=\"no\" dir=\"ltr\">generator</code>. (deprecated arguments)</p> <aside class=\"warning\"><strong>Warning:</strong><span> SOME ARGUMENTS ARE DEPRECATED: <code translate=\"no\" dir=\"ltr\">(output_shapes, output_types)</code>. They will be removed in a future version. Instructions for updating: Use output_signature instead</span></aside> <p>The <code translate=\"no\" dir=\"ltr\">generator</code> argument must be a callable object that returns an object that supports the <code translate=\"no\" dir=\"ltr\">iter()</code> protocol (e.g. a generator function).</p> <p>The elements generated by <code translate=\"no\" dir=\"ltr\">generator</code> must be compatible with either the given <code translate=\"no\" dir=\"ltr\">output_signature</code> argument or with the given <code translate=\"no\" dir=\"ltr\">output_types</code> and (optionally) <code translate=\"no\" dir=\"ltr\">output_shapes</code> arguments, whichiver was specified.</p> <p>The recommended way to call <code translate=\"no\" dir=\"ltr\">from_generator</code> is to use the <code translate=\"no\" dir=\"ltr\">output_signature</code> argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by <a href=\"../typespec\"><code translate=\"no\" dir=\"ltr\">tf.TypeSpec</code></a> objects from <code translate=\"no\" dir=\"ltr\">output_signature</code> argument:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndef gen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\n\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\n\nlist(dataset.take(1))\n[(&lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;,\n&lt;tf.RaggedTensor [[1, 2], [3]]&gt;)]\n</pre> <p>There is also a deprecated way to call <code translate=\"no\" dir=\"ltr\">from_generator</code> by either with <code translate=\"no\" dir=\"ltr\">output_types</code> argument alone or together with <code translate=\"no\" dir=\"ltr\">output_shapes</code> argument. In this case the output of the function will be assumed to consist of <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects with with the types defined by <code translate=\"no\" dir=\"ltr\">output_types</code> and with the shapes which are either unknown or defined by <code translate=\"no\" dir=\"ltr\">output_shapes</code>.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> The current implementation of <a href=\"dataset#from_generator\"><code translate=\"no\" dir=\"ltr\">Dataset.from_generator()</code></a> uses <a href=\"../numpy_function\"><code translate=\"no\" dir=\"ltr\">tf.numpy_function</code></a> and inherits the same constraints. In particular, it requires the dataset and iterator related operations to be placed on a device in the same process as the Python program that called <a href=\"dataset#from_generator\"><code translate=\"no\" dir=\"ltr\">Dataset.from_generator()</code></a>. The body of <code translate=\"no\" dir=\"ltr\">generator</code> will not be serialized in a <code translate=\"no\" dir=\"ltr\">GraphDef</code>, and you should not use this method if you need to serialize your model and restore it in a different environment.</span>\n</blockquote>\n<blockquote class=\"note\">\n<strong>Note:</strong><span> If <code translate=\"no\" dir=\"ltr\">generator</code> depends on mutable global variables or other external state, be aware that the runtime may invoke <code translate=\"no\" dir=\"ltr\">generator</code> multiple times (in order to support repeating the <code translate=\"no\" dir=\"ltr\">Dataset</code>) and at any time between the call to <a href=\"dataset#from_generator\"><code translate=\"no\" dir=\"ltr\">Dataset.from_generator()</code></a> and the production of the first element from the generator. Mutating global variables or external state can cause undefined behavior, and we recommend that you explicitly cache any external state in <code translate=\"no\" dir=\"ltr\">generator</code> before calling <a href=\"dataset#from_generator\"><code translate=\"no\" dir=\"ltr\">Dataset.from_generator()</code></a>.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">generator</code> </td> <td> A callable object that returns an object that supports the <code translate=\"no\" dir=\"ltr\">iter()</code> protocol. If <code translate=\"no\" dir=\"ltr\">args</code> is not specified, <code translate=\"no\" dir=\"ltr\">generator</code> must take no arguments; otherwise it must take as many arguments as there are values in <code translate=\"no\" dir=\"ltr\">args</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">output_types</code> </td> <td> (Optional.) A nested structure of <a href=\"../dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> objects corresponding to each component of an element yielded by <code translate=\"no\" dir=\"ltr\">generator</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">output_shapes</code> </td> <td> (Optional.) A nested structure of <a href=\"../tensorshape\"><code translate=\"no\" dir=\"ltr\">tf.TensorShape</code></a> objects corresponding to each component of an element yielded by <code translate=\"no\" dir=\"ltr\">generator</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">args</code> </td> <td> (Optional.) A tuple of <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects that will be evaluated and passed to <code translate=\"no\" dir=\"ltr\">generator</code> as NumPy-array arguments. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">output_signature</code> </td> <td> (Optional.) A nested structure of <a href=\"../typespec\"><code translate=\"no\" dir=\"ltr\">tf.TypeSpec</code></a> objects corresponding to each component of an element yielded by <code translate=\"no\" dir=\"ltr\">generator</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"from_tensor_slices\" data-text=\"from_tensor_slices\"><code translate=\"no\" dir=\"ltr\">from_tensor_slices</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L615-L691\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_tensor_slices(\n    tensors\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> whose elements are slices of the given tensors.</p> <p>The given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==&gt; 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==&gt; 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n</pre> <p>Note that if <code translate=\"no\" dir=\"ltr\">tensors</code> contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more <a href=\"../constant\"><code translate=\"no\" dir=\"ltr\">tf.constant</code></a> operations. For large datasets (&gt; 1 GB), this can waste memory and run into byte limits of graph serialization. If <code translate=\"no\" dir=\"ltr\">tensors</code> contains one or more large NumPy arrays, consider the alternative described in <a href=\"https://tensorflow.org/guide/data#consuming_numpy_arrays\">this guide</a>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">tensors</code> </td> <td> A dataset element, with each component having the same size in the first dimension. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"from_tensors\" data-text=\"from_tensors\"><code translate=\"no\" dir=\"ltr\">from_tensors</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L578-L613\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_tensors(\n    tensors\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> with a single element, comprising the given tensors.</p> <p><code translate=\"no\" dir=\"ltr\">from_tensors</code> produces a dataset containing only a single element. To slice the input tensor into multiple elements, use <code translate=\"no\" dir=\"ltr\">from_tensor_slices</code> instead.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n</pre> <p>Note that if <code translate=\"no\" dir=\"ltr\">tensors</code> contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more <a href=\"../constant\"><code translate=\"no\" dir=\"ltr\">tf.constant</code></a> operations. For large datasets (&gt; 1 GB), this can waste memory and run into byte limits of graph serialization. If <code translate=\"no\" dir=\"ltr\">tensors</code> contains one or more large NumPy arrays, consider the alternative described in <a href=\"https://tensorflow.org/guide/data#consuming_numpy_arrays\">this guide</a>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">tensors</code> </td> <td> A dataset element. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"interleave\" data-text=\"interleave\"><code translate=\"no\" dir=\"ltr\">interleave</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1839-L1949\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ninterleave(\n    map_func, cycle_length=None, block_length=None, num_parallel_calls=None,\n    deterministic=None\n)\n</pre> <p>Maps <code translate=\"no\" dir=\"ltr\">map_func</code> across this dataset, and interleaves the results.</p> <p>For example, you can use <a href=\"dataset#interleave\"><code translate=\"no\" dir=\"ltr\">Dataset.interleave()</code></a> to process many input files concurrently:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndef parse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n</pre> <p>The <code translate=\"no\" dir=\"ltr\">cycle_length</code> and <code translate=\"no\" dir=\"ltr\">block_length</code> arguments control the order in which elements are produced. <code translate=\"no\" dir=\"ltr\">cycle_length</code> controls the number of input elements that are processed concurrently. If you set <code translate=\"no\" dir=\"ltr\">cycle_length</code> to 1, this transformation will handle one input element at a time, and will produce identical results to <a href=\"dataset#flat_map\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.flat_map</code></a>. In general, this transformation will apply <code translate=\"no\" dir=\"ltr\">map_func</code> to <code translate=\"no\" dir=\"ltr\">cycle_length</code> input elements, open iterators on the returned <code translate=\"no\" dir=\"ltr\">Dataset</code> objects, and cycle through them producing <code translate=\"no\" dir=\"ltr\">block_length</code> consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.</p> <h4 id=\"for_example\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = Dataset.range(1, 6)  # ==&gt; [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n</pre> <blockquote class=\"note\">\n<strong>Note:</strong><span> The order of elements yielded by this transformation is deterministic, as long as <code translate=\"no\" dir=\"ltr\">map_func</code> is a pure function and <code translate=\"no\" dir=\"ltr\">deterministic=True</code>. If <code translate=\"no\" dir=\"ltr\">map_func</code> contains any stateful operations, the order in which that state is accessed is undefined.</span>\n</blockquote> <p>Performance can often be improved by setting <code translate=\"no\" dir=\"ltr\">num_parallel_calls</code> so that <code translate=\"no\" dir=\"ltr\">interleave</code> will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set <code translate=\"no\" dir=\"ltr\">deterministic=False</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">map_func</code> </td> <td> A function mapping a dataset element to a dataset. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">cycle_length</code> </td> <td> (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If <code translate=\"no\" dir=\"ltr\">num_parallel_calls</code> is set to <a href=\"../data#AUTOTUNE\"><code translate=\"no\" dir=\"ltr\">tf.data.AUTOTUNE</code></a>, the <code translate=\"no\" dir=\"ltr\">cycle_length</code> argument identifies the maximum degree of parallelism. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">block_length</code> </td> <td> (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">num_parallel_calls</code> </td> <td> (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value <a href=\"../data#AUTOTUNE\"><code translate=\"no\" dir=\"ltr\">tf.data.AUTOTUNE</code></a> is used, then the number of parallel calls is set dynamically based on available CPU. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">deterministic</code> </td> <td> (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If <code translate=\"no\" dir=\"ltr\">deterministic</code> is <code translate=\"no\" dir=\"ltr\">None</code>, the <a href=\"options#experimental_deterministic\"><code translate=\"no\" dir=\"ltr\">tf.data.Options.experimental_deterministic</code></a> dataset option (<code translate=\"no\" dir=\"ltr\">True</code> by default) is used to decide whether to produce elements deterministically. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"list_files\" data-text=\"list_files\"><code translate=\"no\" dir=\"ltr\">list_files</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1169-L1236\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None\n)\n</pre> <p>A dataset of all files matching one or more glob patterns.</p> <p>The <code translate=\"no\" dir=\"ltr\">file_pattern</code> argument should be a small number of glob patterns. If your filenames have already been globbed, use <a href=\"dataset#from_tensor_slices\"><code translate=\"no\" dir=\"ltr\">Dataset.from_tensor_slices(filenames)</code></a> instead, as re-globbing every filename with <code translate=\"no\" dir=\"ltr\">list_files</code> may result in poor performance with remote storage systems.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> The default behavior of this method is to return filenames in a non-deterministic random shuffled order. Pass a <code translate=\"no\" dir=\"ltr\">seed</code> or <code translate=\"no\" dir=\"ltr\">shuffle=False</code> to get results in a deterministic order.</span>\n</blockquote> <h4 id=\"example\" data-text=\"Example:\">Example:</h4> <p>If we had the following files on our filesystem:</p> <ul> <li>/path/to/dir/a.txt</li> <li>/path/to/dir/b.py</li> <li>/path/to/dir/c.py</li> </ul> <p>If we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:</p> <ul> <li>/path/to/dir/b.py</li> <li>/path/to/dir/c.py</li> </ul>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">file_pattern</code> </td> <td> A string, a list of strings, or a <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shuffle</code> </td> <td> (Optional.) If <code translate=\"no\" dir=\"ltr\">True</code>, the file names will be shuffled randomly. Defaults to <code translate=\"no\" dir=\"ltr\">True</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">seed</code> </td> <td> (Optional.) A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the random seed that will be used to create the distribution. See <a href=\"../random/set_seed\"><code translate=\"no\" dir=\"ltr\">tf.random.set_seed</code></a> for behavior. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code> of strings corresponding to file names. </td> </tr> </table> <h3 id=\"map\" data-text=\"map\"><code translate=\"no\" dir=\"ltr\">map</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1667-L1812\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmap(\n    map_func, num_parallel_calls=None, deterministic=None\n)\n</pre> <p>Maps <code translate=\"no\" dir=\"ltr\">map_func</code> across the elements of this dataset.</p> <p>This transformation applies <code translate=\"no\" dir=\"ltr\">map_func</code> to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. <code translate=\"no\" dir=\"ltr\">map_func</code> can be used to change both the values and the structure of a dataset's elements. For example, adding 1 to each element, or projecting a subset of element components.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = Dataset.range(1, 6)  # ==&gt; [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n</pre> <p>The input signature of <code translate=\"no\" dir=\"ltr\">map_func</code> is determined by the structure of each element in this dataset.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n</pre> <p>The value or values returned by <code translate=\"no\" dir=\"ltr\">map_func</code> determine the structure of each element in the returned dataset.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndef g(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndef h(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndef i(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n</pre> <p><code translate=\"no\" dir=\"ltr\">map_func</code> can accept as arguments and return any type of dataset element.</p> <p>Note that irrespective of the context in which <code translate=\"no\" dir=\"ltr\">map_func</code> is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:</p> <p>1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.</p> <p>2) Use <a href=\"../py_function\"><code translate=\"no\" dir=\"ltr\">tf.py_function</code></a>, which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndef upper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n</pre> <p>3) Use <a href=\"../numpy_function\"><code translate=\"no\" dir=\"ltr\">tf.numpy_function</code></a>, which also allows you to write arbitrary Python code. Note that <a href=\"../py_function\"><code translate=\"no\" dir=\"ltr\">tf.py_function</code></a> accepts <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> whereas <a href=\"../numpy_function\"><code translate=\"no\" dir=\"ltr\">tf.numpy_function</code></a> accepts numpy arrays and returns only numpy arrays. For example:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndef upper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n</pre> <p>Note that the use of <a href=\"../numpy_function\"><code translate=\"no\" dir=\"ltr\">tf.numpy_function</code></a> and <a href=\"../py_function\"><code translate=\"no\" dir=\"ltr\">tf.py_function</code></a> in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).</p> <p>Performance can often be improved by setting <code translate=\"no\" dir=\"ltr\">num_parallel_calls</code> so that <code translate=\"no\" dir=\"ltr\">map</code> will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set <code translate=\"no\" dir=\"ltr\">deterministic=False</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = Dataset.range(1, 6)  # ==&gt; [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">map_func</code> </td> <td> A function mapping a dataset element to another dataset element. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">num_parallel_calls</code> </td> <td> (Optional.) A <a href=\"../../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value <a href=\"../data#AUTOTUNE\"><code translate=\"no\" dir=\"ltr\">tf.data.AUTOTUNE</code></a> is used, then the number of parallel calls is set dynamically based on available CPU. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">deterministic</code> </td> <td> (Optional.) A boolean controlling whether determinism should be traded for performance by allowing elements to be produced out of order. If <code translate=\"no\" dir=\"ltr\">deterministic</code> is <code translate=\"no\" dir=\"ltr\">None</code>, the <a href=\"options#experimental_deterministic\"><code translate=\"no\" dir=\"ltr\">tf.data.Options.experimental_deterministic</code></a> dataset option (<code translate=\"no\" dir=\"ltr\">True</code> by default) is used to decide whether to produce elements deterministically. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"options\" data-text=\"options\"><code translate=\"no\" dir=\"ltr\">options</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L352-L358\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptions()\n</pre> <p>Returns the options for this dataset and its inputs.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"options\"><code translate=\"no\" dir=\"ltr\">tf.data.Options</code></a> object representing the dataset options. </td> </tr> \n</table> <h3 id=\"padded_batch\" data-text=\"padded_batch\"><code translate=\"no\" dir=\"ltr\">padded_batch</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1541-L1665\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\npadded_batch(\n    batch_size, padded_shapes=None, padding_values=None, drop_remainder=False\n)\n</pre> <p>Combines consecutive elements of this dataset into padded batches.</p> <p>This transformation combines multiple consecutive elements of the input dataset into a single element.</p> <p>Like <a href=\"dataset#batch\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.batch</code></a>, the components of the resulting element will have an additional outer dimension, which will be <code translate=\"no\" dir=\"ltr\">batch_size</code> (or <code translate=\"no\" dir=\"ltr\">N % batch_size</code> for the last element if <code translate=\"no\" dir=\"ltr\">batch_size</code> does not divide the number of input elements <code translate=\"no\" dir=\"ltr\">N</code> evenly and <code translate=\"no\" dir=\"ltr\">drop_remainder</code> is <code translate=\"no\" dir=\"ltr\">False</code>). If your program depends on the batches having the same outer dimension, you should set the <code translate=\"no\" dir=\"ltr\">drop_remainder</code> argument to <code translate=\"no\" dir=\"ltr\">True</code> to prevent the smaller batch from being produced.</p> <p>Unlike <a href=\"dataset#batch\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.batch</code></a>, the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in <code translate=\"no\" dir=\"ltr\">padded_shapes</code>. The <code translate=\"no\" dir=\"ltr\">padded_shapes</code> argument determines the resulting shape for each dimension of each component in an output element:</p> <ul> <li>If the dimension is a constant, the component will be padded out to that length in that dimension.</li> <li>If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.</li> </ul> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n</pre> <p>See also <a href=\"experimental/dense_to_sparse_batch\"><code translate=\"no\" dir=\"ltr\">tf.data.experimental.dense_to_sparse_batch</code></a>, which combines elements that may have different shapes into a <a href=\"../sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">batch_size</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of consecutive elements of this dataset to combine in a single batch. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">padded_shapes</code> </td> <td> (Optional.) A nested structure of <a href=\"../tensorshape\"><code translate=\"no\" dir=\"ltr\">tf.TensorShape</code></a> or <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. <code translate=\"no\" dir=\"ltr\">padded_shapes</code> must be set if any component has an unknown rank. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">padding_values</code> </td> <td> (Optional.) A nested structure of scalar-shaped <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the padding values to use for the respective components. None represents that the nested structure should be padded with default values. Defaults are <code translate=\"no\" dir=\"ltr\">0</code> for numeric types and the empty string for string types. The <code translate=\"no\" dir=\"ltr\">padding_values</code> should have the same structure as the input dataset. If <code translate=\"no\" dir=\"ltr\">padding_values</code> is a single element and the input dataset has multiple components, then the same <code translate=\"no\" dir=\"ltr\">padding_values</code> will be used to pad every component of the dataset. If <code translate=\"no\" dir=\"ltr\">padding_values</code> is a scalar, then its value will be broadcasted to match the shape of each component. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">drop_remainder</code> </td> <td> (Optional.) A <a href=\"../../tf#bool\"><code translate=\"no\" dir=\"ltr\">tf.bool</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing whether the last batch should be dropped in the case it has fewer than <code translate=\"no\" dir=\"ltr\">batch_size</code> elements; the default behavior is not to drop the smaller batch. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If a component has an unknown rank, and the <code translate=\"no\" dir=\"ltr\">padded_shapes</code> argument is not set. </td> </tr> </table> <h3 id=\"prefetch\" data-text=\"prefetch\"><code translate=\"no\" dir=\"ltr\">prefetch</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1141-L1167\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprefetch(\n    buffer_size\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> that prefetches elements from this dataset.</p> <p>Most dataset input pipelines should end with a call to <code translate=\"no\" dir=\"ltr\">prefetch</code>. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Like other <code translate=\"no\" dir=\"ltr\">Dataset</code> methods, prefetch operates on the elements of the input dataset. It has no concept of examples vs. batches. <code translate=\"no\" dir=\"ltr\">examples.prefetch(2)</code> will prefetch two elements (2 examples), while <code translate=\"no\" dir=\"ltr\">examples.batch(20).prefetch(2)</code> will prefetch 2 elements (2 batches, of 20 examples each).</span>\n</blockquote> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">buffer_size</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the maximum number of elements that will be buffered when prefetching. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"range\" data-text=\"range\"><code translate=\"no\" dir=\"ltr\">range</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1030-L1065\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nrange(\n    *args, **kwargs\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> of a step-separated range of values.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">*args</code> </td> <td> follows the same semantics as python's xrange. len(args) == 1 -&gt; start = 0, stop = args[0], step = 1. len(args) == 2 -&gt; start = args[0], stop = args[1], step = 1. len(args) == 3 -&gt; start = args[0], stop = args[1], step = args[2]. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">**kwargs</code> </td> <td> <ul> <li>output_type: Its expected dtype. (Optional, default: <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>). </li>\n</ul>\n</td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">RangeDataset</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if len(args) == 0. </td> </tr> </table> <h3 id=\"reduce\" data-text=\"reduce\"><code translate=\"no\" dir=\"ltr\">reduce</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L2085-L2196\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nreduce(\n    initial_state, reduce_func\n)\n</pre> <p>Reduces the input dataset to a single element.</p> <p>The transformation calls <code translate=\"no\" dir=\"ltr\">reduce_func</code> successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The <code translate=\"no\" dir=\"ltr\">initial_state</code> argument is used for the initial state and the final state is returned as the result.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n5\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">initial_state</code> </td> <td> An element representing the initial state of the transformation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">reduce_func</code> </td> <td> A function that maps <code translate=\"no\" dir=\"ltr\">(old_state, input_element)</code> to <code translate=\"no\" dir=\"ltr\">new_state</code>. It must take two arguments and return a new element The structure of <code translate=\"no\" dir=\"ltr\">new_state</code> must match the structure of <code translate=\"no\" dir=\"ltr\">initial_state</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A dataset element corresponding to the final state of the transformation. </td> </tr> \n</table> <h3 id=\"repeat\" data-text=\"repeat\"><code translate=\"no\" dir=\"ltr\">repeat</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1238-L1257\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrepeat(\n    count=None\n)\n</pre> <p>Repeats this dataset so each original value is seen <code translate=\"no\" dir=\"ltr\">count</code> times.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n</pre> <blockquote class=\"note\">\n<strong>Note:</strong><span> If this dataset is a function of global state (e.g. a random number generator), then different repetitions may produce different elements.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">count</code> </td> <td> (Optional.) A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of times the dataset should be repeated. The default behavior (if <code translate=\"no\" dir=\"ltr\">count</code> is <code translate=\"no\" dir=\"ltr\">None</code> or <code translate=\"no\" dir=\"ltr\">-1</code>) is for the dataset be repeated indefinitely. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"shard\" data-text=\"shard\"><code translate=\"no\" dir=\"ltr\">shard</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1440-L1506\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nshard(\n    num_shards, index\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> that includes only 1/<code translate=\"no\" dir=\"ltr\">num_shards</code> of this dataset.</p> <p><code translate=\"no\" dir=\"ltr\">shard</code> is deterministic. The Dataset produced by <code translate=\"no\" dir=\"ltr\">A.shard(n, i)</code> will contain all elements of A whose index mod n = i.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n</pre> <p>This dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.</p> <p>When reading a single input file, you can shard elements as follows:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">d = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n</pre> <h4 id=\"important_caveats\" data-text=\"Important caveats:\">Important caveats:</h4> <ul> <li>Be sure to shard before you use any randomizing operator (such as shuffle).</li> <li>Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:</li> </ul> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">d = Dataset.list_files(pattern)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">num_shards</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of shards operating in parallel. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">index</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the worker index. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">InvalidArgumentError</code> </td> <td> if <code translate=\"no\" dir=\"ltr\">num_shards</code> or <code translate=\"no\" dir=\"ltr\">index</code> are illegal values. <blockquote class=\"note\">\n<strong>Note:</strong><span> error checking is done on a best-effort basis, and errors aren't guaranteed to be caught upon dataset creation. (e.g. providing in a placeholder tensor bypasses the early checking, and will instead result in an error during a session.run call.) </span>\n</blockquote>\n</td> </tr> </table> <h3 id=\"shuffle\" data-text=\"shuffle\"><code translate=\"no\" dir=\"ltr\">shuffle</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1292-L1350\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None\n)\n</pre> <p>Randomly shuffles the elements of this dataset.</p> <p>This dataset fills a buffer with <code translate=\"no\" dir=\"ltr\">buffer_size</code> elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.</p> <p>For instance, if your dataset contains 10,000 elements but <code translate=\"no\" dir=\"ltr\">buffer_size</code> is set to 1,000, then <code translate=\"no\" dir=\"ltr\">shuffle</code> will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.</p> <p><code translate=\"no\" dir=\"ltr\">reshuffle_each_iteration</code> controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the <code translate=\"no\" dir=\"ltr\">repeat</code> transformation:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 2, 0]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)  # doctest: +SKIP\n[1, 0, 2, 1, 0, 2]\n</pre> <p>In TF 2.0, <a href=\"dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a> objects are Python iterables which makes it possible to also create epochs through Python iteration:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 2, 0]\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\nlist(dataset.as_numpy_iterator())  # doctest: +SKIP\n[1, 0, 2]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">buffer_size</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of elements from this dataset from which the new dataset will sample. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">seed</code> </td> <td> (Optional.) A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the random seed that will be used to create the distribution. See <a href=\"../random/set_seed\"><code translate=\"no\" dir=\"ltr\">tf.random.set_seed</code></a> for behavior. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">reshuffle_each_iteration</code> </td> <td> (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to <code translate=\"no\" dir=\"ltr\">True</code>.) </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"skip\" data-text=\"skip\"><code translate=\"no\" dir=\"ltr\">skip</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1421-L1438\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nskip(\n    count\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> that skips <code translate=\"no\" dir=\"ltr\">count</code> elements from this dataset.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">count</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of elements of this dataset that should be skipped to form the new dataset. If <code translate=\"no\" dir=\"ltr\">count</code> is greater than the size of this dataset, the new dataset will contain no elements. If <code translate=\"no\" dir=\"ltr\">count</code> is -1, skips the entire dataset. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"take\" data-text=\"take\"><code translate=\"no\" dir=\"ltr\">take</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1402-L1419\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntake(\n    count\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> with at most <code translate=\"no\" dir=\"ltr\">count</code> elements from this dataset.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">count</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of elements of this dataset that should be taken to form the new dataset. If <code translate=\"no\" dir=\"ltr\">count</code> is -1, or if <code translate=\"no\" dir=\"ltr\">count</code> is greater than the size of this dataset, the new dataset will contain all elements of this dataset. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"unbatch\" data-text=\"unbatch\"><code translate=\"no\" dir=\"ltr\">unbatch</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L2198-L2220\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nunbatch()\n</pre> <p>Splits elements of a dataset into multiple elements.</p> <p>For example, if elements of the dataset are shaped <code translate=\"no\" dir=\"ltr\">[B, a0, a1, ...]</code>, where <code translate=\"no\" dir=\"ltr\">B</code> may vary for each input element, then for each element in the dataset, the unbatched dataset will contain <code translate=\"no\" dir=\"ltr\">B</code> consecutive elements of shape <code translate=\"no\" dir=\"ltr\">[a0, a1, ...]</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n</pre> <blockquote class=\"note\">\n<strong>Note:</strong><span> <code translate=\"no\" dir=\"ltr\">unbatch</code> requires a data copy to slice up the batched tensor into smaller, unbatched tensors. When optimizing performance, try to avoid unnecessary usage of <code translate=\"no\" dir=\"ltr\">unbatch</code>.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> \n</table> <h3 id=\"window\" data-text=\"window\"><code translate=\"no\" dir=\"ltr\">window</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L2004-L2083\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwindow(\n    size, shift=None, stride=1, drop_remainder=False\n)\n</pre> <p>Combines (nests of) input elements into a dataset of (nests of) windows.</p> <p>A \"window\" is a finite dataset of flat elements of size <code translate=\"no\" dir=\"ltr\">size</code> (or possibly fewer if there are not enough input elements to fill the window and <code translate=\"no\" dir=\"ltr\">drop_remainder</code> evaluates to <code translate=\"no\" dir=\"ltr\">False</code>).</p> <p>The <code translate=\"no\" dir=\"ltr\">shift</code> argument determines the number of input elements by which the window moves on each iteration. If windows and elements are both numbered starting at 0, the first element in window <code translate=\"no\" dir=\"ltr\">k</code> will be element <code translate=\"no\" dir=\"ltr\">k * shift</code> of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.</p> <p>The <code translate=\"no\" dir=\"ltr\">stride</code> argument determines the stride of the input elements, and the <code translate=\"no\" dir=\"ltr\">shift</code> argument determines the shift of the window.</p> <h4 id=\"for_example_2\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.range(7).window(2)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1]\n[2, 3]\n[4, 5]\n[6]\ndataset = tf.data.Dataset.range(7).window(3, 2, 1, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[2, 3, 4]\n[4, 5, 6]\ndataset = tf.data.Dataset.range(7).window(3, 1, 2, True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n</pre> <p>Note that when the <code translate=\"no\" dir=\"ltr\">window</code> transformation is applied to a dataset of nested elements, it produces a dataset of nested windows.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nnested = ([1, 2, 3, 4], [5, 6, 7, 8])\ndataset = tf.data.Dataset.from_tensor_slices(nested).window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print(tuple(to_numpy(component) for component in window))\n([1, 2], [5, 6])\n([3, 4], [7, 8])\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3, 4]})\ndataset = dataset.window(2)\nfor window in dataset:\n  def to_numpy(ds):\n    return list(ds.as_numpy_iterator())\n  print({'a': to_numpy(window['a'])})\n{'a': [1, 2]}\n{'a': [3, 4]}\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">size</code> </td> <td> A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of elements of the input dataset to combine into a window. Must be positive. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shift</code> </td> <td> (Optional.) A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the number of input elements by which the window moves in each iteration. Defaults to <code translate=\"no\" dir=\"ltr\">size</code>. Must be positive. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">stride</code> </td> <td> (Optional.) A <a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\". </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">drop_remainder</code> </td> <td> (Optional.) A <a href=\"../../tf#bool\"><code translate=\"no\" dir=\"ltr\">tf.bool</code></a> scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>, representing whether the last windows should be dropped if their size is smaller than <code translate=\"no\" dir=\"ltr\">size</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code> of (nests of) windows -- a finite datasets of flat elements created from the (nests of) input elements. </td> </tr> </table> <h3 id=\"with_options\" data-text=\"with_options\"><code translate=\"no\" dir=\"ltr\">with_options</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L2222-L2247\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwith_options(\n    options\n)\n</pre> <p>Returns a new <a href=\"dataset\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset</code></a> with the given options set.</p> <p>The options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.experimental_deterministic = False\nds = ds.with_options(options)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">options</code> </td> <td> A <a href=\"options\"><code translate=\"no\" dir=\"ltr\">tf.data.Options</code></a> that identifies the options the use. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code> with the given options. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> when an option is set more than once to a non-default value </td> </tr> </table> <h3 id=\"zip\" data-text=\"zip\"><code translate=\"no\" dir=\"ltr\">zip</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L1067-L1110\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nzip(\n    datasets\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">Dataset</code> by zipping together the given datasets.</p> <p>This method has similar semantics to the built-in <code translate=\"no\" dir=\"ltr\">zip()</code> function in Python, with the main difference being that the <code translate=\"no\" dir=\"ltr\">datasets</code> argument can be an arbitrary nested structure of <code translate=\"no\" dir=\"ltr\">Dataset</code> objects.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# The nested structure of the `datasets` argument determines the\n# structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==&gt; [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==&gt; [ 4, 5, 6 ]\nds = tf.data.Dataset.zip((a, b))\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip((b, a))\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==&gt; [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip((a, b, c))\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==&gt; [ 13, 14 ]\nds = tf.data.Dataset.zip((a, d))\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">datasets</code> </td> <td> A nested structure of datasets. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">Dataset</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Dataset</code>. </td> </tr> </table> <h3 id=\"__bool__\" data-text=\"__bool__\"><code translate=\"no\" dir=\"ltr\">__bool__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L427-L428\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__bool__()\n</pre> <h3 id=\"__iter__\" data-text=\"__iter__\"><code translate=\"no\" dir=\"ltr\">__iter__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L409-L425\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__iter__()\n</pre> <p>Creates an iterator for elements of this dataset.</p> <p>The returned iterator implements the Python Iterator protocol.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> An <a href=\"iterator\"><code translate=\"no\" dir=\"ltr\">tf.data.Iterator</code></a> for the elements of this dataset. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If not inside of tf.function and not executing eagerly. </td> </tr> </table> <h3 id=\"__len__\" data-text=\"__len__\"><code translate=\"no\" dir=\"ltr\">__len__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L432-L455\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__len__()\n</pre> <p>Returns the length of the dataset if it is known and finite.</p> <p>This method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use <a href=\"dataset#cardinality\"><code translate=\"no\" dir=\"ltr\">tf.data.Dataset.cardinality</code></a> instead.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> An integer representing the length of the dataset. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If the dataset length is unknown or infinite, or if eager execution is not enabled. </td> </tr> </table> <h3 id=\"__nonzero__\" data-text=\"__nonzero__\"><code translate=\"no\" dir=\"ltr\">__nonzero__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/dataset_ops.py#L427-L428\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__nonzero__()\n</pre>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/Dataset\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/Dataset</a>\n  </p>\n</div>\n","dtypes":"<h1 class=\"devsite-page-title\">Module: tf.dtypes</h1>       <p>Public API for tf.dtypes namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">class DType</code></a>: Represents the type of the elements in a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"dtypes/as_dtype\"><code translate=\"no\" dir=\"ltr\">as_dtype(...)</code></a>: Converts the given <code translate=\"no\" dir=\"ltr\">type_value</code> to a <code translate=\"no\" dir=\"ltr\">DType</code>.</p> <p><a href=\"cast\"><code translate=\"no\" dir=\"ltr\">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href=\"dtypes/complex\"><code translate=\"no\" dir=\"ltr\">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href=\"dtypes/saturate_cast\"><code translate=\"no\" dir=\"ltr\">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate=\"no\" dir=\"ltr\">value</code> to <code translate=\"no\" dir=\"ltr\">dtype</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> QUANTIZED_DTYPES </td> <td> \n</td> </tr>\n<tr> <td> bfloat16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> bool </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> complex128 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> complex64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> double </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> float64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> half </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> int8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> qint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> qint32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> qint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> quint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> quint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> resource </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> string </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint16 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint32 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint64 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> uint8 </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr>\n<tr> <td> variant </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/dtypes\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/dtypes</a>\n  </p>\n</div>\n","errors":"<h1 class=\"devsite-page-title\">Module: tf.errors</h1>       <p>Exception types for TensorFlow errors.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"errors/abortederror\"><code translate=\"no\" dir=\"ltr\">class AbortedError</code></a>: The operation was aborted, typically due to a concurrent action.</p> <p><a href=\"errors/alreadyexistserror\"><code translate=\"no\" dir=\"ltr\">class AlreadyExistsError</code></a>: Raised when an entity that we attempted to create already exists.</p> <p><a href=\"errors/cancellederror\"><code translate=\"no\" dir=\"ltr\">class CancelledError</code></a>: Raised when an operation or step is cancelled.</p> <p><a href=\"errors/datalosserror\"><code translate=\"no\" dir=\"ltr\">class DataLossError</code></a>: Raised when unrecoverable data loss or corruption is encountered.</p> <p><a href=\"errors/deadlineexceedederror\"><code translate=\"no\" dir=\"ltr\">class DeadlineExceededError</code></a>: Raised when a deadline expires before an operation could complete.</p> <p><a href=\"errors/failedpreconditionerror\"><code translate=\"no\" dir=\"ltr\">class FailedPreconditionError</code></a>: Operation was rejected because the system is not in a state to execute it.</p> <p><a href=\"errors/internalerror\"><code translate=\"no\" dir=\"ltr\">class InternalError</code></a>: Raised when the system experiences an internal error.</p> <p><a href=\"errors/invalidargumenterror\"><code translate=\"no\" dir=\"ltr\">class InvalidArgumentError</code></a>: Raised when an operation receives an invalid argument.</p> <p><a href=\"errors/notfounderror\"><code translate=\"no\" dir=\"ltr\">class NotFoundError</code></a>: Raised when a requested entity (e.g., a file or directory) was not found.</p> <p><a href=\"errors/operror\"><code translate=\"no\" dir=\"ltr\">class OpError</code></a>: A generic error that is raised when TensorFlow execution fails.</p> <p><a href=\"errors/operatornotallowedingrapherror\"><code translate=\"no\" dir=\"ltr\">class OperatorNotAllowedInGraphError</code></a>: An error is raised for unsupported operator in Graph execution.</p> <p><a href=\"errors/outofrangeerror\"><code translate=\"no\" dir=\"ltr\">class OutOfRangeError</code></a>: Raised when an operation iterates past the valid input range.</p> <p><a href=\"errors/permissiondeniederror\"><code translate=\"no\" dir=\"ltr\">class PermissionDeniedError</code></a>: Raised when the caller does not have permission to run an operation.</p> <p><a href=\"errors/resourceexhaustederror\"><code translate=\"no\" dir=\"ltr\">class ResourceExhaustedError</code></a>: Some resource has been exhausted.</p> <p><a href=\"errors/unauthenticatederror\"><code translate=\"no\" dir=\"ltr\">class UnauthenticatedError</code></a>: The request does not have valid authentication credentials.</p> <p><a href=\"errors/unavailableerror\"><code translate=\"no\" dir=\"ltr\">class UnavailableError</code></a>: Raised when the runtime is currently unavailable.</p> <p><a href=\"errors/unimplementederror\"><code translate=\"no\" dir=\"ltr\">class UnimplementedError</code></a>: Raised when an operation has not been implemented.</p> <p><a href=\"errors/unknownerror\"><code translate=\"no\" dir=\"ltr\">class UnknownError</code></a>: Unknown error.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> ABORTED </td> <td> <code translate=\"no\" dir=\"ltr\">10</code> </td> </tr>\n<tr> <td> ALREADY_EXISTS </td> <td> <code translate=\"no\" dir=\"ltr\">6</code> </td> </tr>\n<tr> <td> CANCELLED </td> <td> <code translate=\"no\" dir=\"ltr\">1</code> </td> </tr>\n<tr> <td> DATA_LOSS </td> <td> <code translate=\"no\" dir=\"ltr\">15</code> </td> </tr>\n<tr> <td> DEADLINE_EXCEEDED </td> <td> <code translate=\"no\" dir=\"ltr\">4</code> </td> </tr>\n<tr> <td> FAILED_PRECONDITION </td> <td> <code translate=\"no\" dir=\"ltr\">9</code> </td> </tr>\n<tr> <td> INTERNAL </td> <td> <code translate=\"no\" dir=\"ltr\">13</code> </td> </tr>\n<tr> <td> INVALID_ARGUMENT </td> <td> <code translate=\"no\" dir=\"ltr\">3</code> </td> </tr>\n<tr> <td> NOT_FOUND </td> <td> <code translate=\"no\" dir=\"ltr\">5</code> </td> </tr>\n<tr> <td> OK </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> OUT_OF_RANGE </td> <td> <code translate=\"no\" dir=\"ltr\">11</code> </td> </tr>\n<tr> <td> PERMISSION_DENIED </td> <td> <code translate=\"no\" dir=\"ltr\">7</code> </td> </tr>\n<tr> <td> RESOURCE_EXHAUSTED </td> <td> <code translate=\"no\" dir=\"ltr\">8</code> </td> </tr>\n<tr> <td> UNAUTHENTICATED </td> <td> <code translate=\"no\" dir=\"ltr\">16</code> </td> </tr>\n<tr> <td> UNAVAILABLE </td> <td> <code translate=\"no\" dir=\"ltr\">14</code> </td> </tr>\n<tr> <td> UNIMPLEMENTED </td> <td> <code translate=\"no\" dir=\"ltr\">12</code> </td> </tr>\n<tr> <td> UNKNOWN </td> <td> <code translate=\"no\" dir=\"ltr\">2</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/errors\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/errors</a>\n  </p>\n</div>\n","experimental":"<h1 class=\"devsite-page-title\">Module: tf.experimental</h1>       <p>Public API for tf.experimental namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"experimental/dlpack\"><code translate=\"no\" dir=\"ltr\">dlpack</code></a> module: Public API for tf.experimental.dlpack namespace.</p> <p><a href=\"experimental/numpy\"><code translate=\"no\" dir=\"ltr\">numpy</code></a> module: # tf.experimental.numpy: NumPy API on TensorFlow.</p> <p><a href=\"experimental/tensorrt\"><code translate=\"no\" dir=\"ltr\">tensorrt</code></a> module: Public API for tf.experimental.tensorrt namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"experimental/optional\"><code translate=\"no\" dir=\"ltr\">class Optional</code></a>: Represents a value that may or may not be present.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"experimental/async_clear_error\"><code translate=\"no\" dir=\"ltr\">async_clear_error(...)</code></a>: Clear pending operations and error statuses in async execution.</p> <p><a href=\"experimental/async_scope\"><code translate=\"no\" dir=\"ltr\">async_scope(...)</code></a>: Context manager for grouping async operations.</p> <p><a href=\"experimental/function_executor_type\"><code translate=\"no\" dir=\"ltr\">function_executor_type(...)</code></a>: Context manager for setting the executor of eager defined functions.</p> <p><a href=\"experimental/register_filesystem_plugin\"><code translate=\"no\" dir=\"ltr\">register_filesystem_plugin(...)</code></a>: Loads a TensorFlow FileSystem plugin.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental</a>\n  </p>\n</div>\n","graph_util":"<h1 class=\"devsite-page-title\">Module: tf.graph_util</h1>       <p>Helpers to manipulate a tensor graph in python.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"graph_util/import_graph_def\"><code translate=\"no\" dir=\"ltr\">import_graph_def(...)</code></a>: Imports the graph from <code translate=\"no\" dir=\"ltr\">graph_def</code> into the current default <code translate=\"no\" dir=\"ltr\">Graph</code>. (deprecated arguments)</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/graph_util\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/graph_util</a>\n  </p>\n</div>\n","io":"<h1 class=\"devsite-page-title\">Module: tf.io</h1>       <p>Public API for tf.io namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"io/gfile\"><code translate=\"no\" dir=\"ltr\">gfile</code></a> module: Public API for tf.io.gfile namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"io/fixedlenfeature\"><code translate=\"no\" dir=\"ltr\">class FixedLenFeature</code></a>: Configuration for parsing a fixed-length input feature.</p> <p><a href=\"io/fixedlensequencefeature\"><code translate=\"no\" dir=\"ltr\">class FixedLenSequenceFeature</code></a>: Configuration for parsing a variable-length input feature into a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"io/raggedfeature\"><code translate=\"no\" dir=\"ltr\">class RaggedFeature</code></a>: Configuration for passing a RaggedTensor input feature.</p> <p><a href=\"io/sparsefeature\"><code translate=\"no\" dir=\"ltr\">class SparseFeature</code></a>: Configuration for parsing a sparse input feature from an <code translate=\"no\" dir=\"ltr\">Example</code>.</p> <p><a href=\"io/tfrecordoptions\"><code translate=\"no\" dir=\"ltr\">class TFRecordOptions</code></a>: Options used for manipulating TFRecord files.</p> <p><a href=\"io/tfrecordwriter\"><code translate=\"no\" dir=\"ltr\">class TFRecordWriter</code></a>: A class to write records to a TFRecords file.</p> <p><a href=\"io/varlenfeature\"><code translate=\"no\" dir=\"ltr\">class VarLenFeature</code></a>: Configuration for parsing a variable-length input feature.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"io/decode_and_crop_jpeg\"><code translate=\"no\" dir=\"ltr\">decode_and_crop_jpeg(...)</code></a>: Decode and Crop a JPEG-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_base64\"><code translate=\"no\" dir=\"ltr\">decode_base64(...)</code></a>: Decode web-safe base64-encoded strings.</p> <p><a href=\"io/decode_bmp\"><code translate=\"no\" dir=\"ltr\">decode_bmp(...)</code></a>: Decode the first frame of a BMP-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_compressed\"><code translate=\"no\" dir=\"ltr\">decode_compressed(...)</code></a>: Decompress strings.</p> <p><a href=\"io/decode_csv\"><code translate=\"no\" dir=\"ltr\">decode_csv(...)</code></a>: Convert CSV records to tensors. Each column maps to one tensor.</p> <p><a href=\"io/decode_gif\"><code translate=\"no\" dir=\"ltr\">decode_gif(...)</code></a>: Decode the frame(s) of a GIF-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_image\"><code translate=\"no\" dir=\"ltr\">decode_image(...)</code></a>: Function for <code translate=\"no\" dir=\"ltr\">decode_bmp</code>, <code translate=\"no\" dir=\"ltr\">decode_gif</code>, <code translate=\"no\" dir=\"ltr\">decode_jpeg</code>, and <code translate=\"no\" dir=\"ltr\">decode_png</code>.</p> <p><a href=\"io/decode_jpeg\"><code translate=\"no\" dir=\"ltr\">decode_jpeg(...)</code></a>: Decode a JPEG-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_json_example\"><code translate=\"no\" dir=\"ltr\">decode_json_example(...)</code></a>: Convert JSON-encoded Example records to binary protocol buffer strings.</p> <p><a href=\"io/decode_png\"><code translate=\"no\" dir=\"ltr\">decode_png(...)</code></a>: Decode a PNG-encoded image to a uint8 or uint16 tensor.</p> <p><a href=\"io/decode_proto\"><code translate=\"no\" dir=\"ltr\">decode_proto(...)</code></a>: The op extracts fields from a serialized protocol buffers message into tensors.</p> <p><a href=\"io/decode_raw\"><code translate=\"no\" dir=\"ltr\">decode_raw(...)</code></a>: Convert raw byte strings into tensors.</p> <p><a href=\"io/deserialize_many_sparse\"><code translate=\"no\" dir=\"ltr\">deserialize_many_sparse(...)</code></a>: Deserialize and concatenate <code translate=\"no\" dir=\"ltr\">SparseTensors</code> from a serialized minibatch.</p> <p><a href=\"io/encode_base64\"><code translate=\"no\" dir=\"ltr\">encode_base64(...)</code></a>: Encode strings into web-safe base64 format.</p> <p><a href=\"io/encode_jpeg\"><code translate=\"no\" dir=\"ltr\">encode_jpeg(...)</code></a>: JPEG-encode an image.</p> <p><a href=\"io/encode_png\"><code translate=\"no\" dir=\"ltr\">encode_png(...)</code></a>: PNG-encode an image.</p> <p><a href=\"io/encode_proto\"><code translate=\"no\" dir=\"ltr\">encode_proto(...)</code></a>: The op serializes protobuf messages provided in the input tensors.</p> <p><a href=\"io/extract_jpeg_shape\"><code translate=\"no\" dir=\"ltr\">extract_jpeg_shape(...)</code></a>: Extract the shape information of a JPEG-encoded image.</p> <p><a href=\"io/is_jpeg\"><code translate=\"no\" dir=\"ltr\">is_jpeg(...)</code></a>: Convenience function to check if the 'contents' encodes a JPEG image.</p> <p><a href=\"io/match_filenames_once\"><code translate=\"no\" dir=\"ltr\">match_filenames_once(...)</code></a>: Save the list of files matching pattern, so it is only computed once.</p> <p><a href=\"io/matching_files\"><code translate=\"no\" dir=\"ltr\">matching_files(...)</code></a>: Returns the set of files matching one or more glob patterns.</p> <p><a href=\"io/parse_example\"><code translate=\"no\" dir=\"ltr\">parse_example(...)</code></a>: Parses <code translate=\"no\" dir=\"ltr\">Example</code> protos into a <code translate=\"no\" dir=\"ltr\">dict</code> of tensors.</p> <p><a href=\"io/parse_sequence_example\"><code translate=\"no\" dir=\"ltr\">parse_sequence_example(...)</code></a>: Parses a batch of <code translate=\"no\" dir=\"ltr\">SequenceExample</code> protos.</p> <p><a href=\"io/parse_single_example\"><code translate=\"no\" dir=\"ltr\">parse_single_example(...)</code></a>: Parses a single <code translate=\"no\" dir=\"ltr\">Example</code> proto.</p> <p><a href=\"io/parse_single_sequence_example\"><code translate=\"no\" dir=\"ltr\">parse_single_sequence_example(...)</code></a>: Parses a single <code translate=\"no\" dir=\"ltr\">SequenceExample</code> proto.</p> <p><a href=\"io/parse_tensor\"><code translate=\"no\" dir=\"ltr\">parse_tensor(...)</code></a>: Transforms a serialized tensorflow.TensorProto proto into a Tensor.</p> <p><a href=\"io/read_file\"><code translate=\"no\" dir=\"ltr\">read_file(...)</code></a>: Reads and outputs the entire contents of the input filename.</p> <p><a href=\"io/serialize_many_sparse\"><code translate=\"no\" dir=\"ltr\">serialize_many_sparse(...)</code></a>: Serialize <code translate=\"no\" dir=\"ltr\">N</code>-minibatch <code translate=\"no\" dir=\"ltr\">SparseTensor</code> into an <code translate=\"no\" dir=\"ltr\">[N, 3]</code> <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p><a href=\"io/serialize_sparse\"><code translate=\"no\" dir=\"ltr\">serialize_sparse(...)</code></a>: Serialize a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> into a 3-vector (1-D <code translate=\"no\" dir=\"ltr\">Tensor</code>) object.</p> <p><a href=\"io/serialize_tensor\"><code translate=\"no\" dir=\"ltr\">serialize_tensor(...)</code></a>: Transforms a Tensor into a serialized TensorProto proto.</p> <p><a href=\"io/write_file\"><code translate=\"no\" dir=\"ltr\">write_file(...)</code></a>: Writes contents to the file at input filename. Creates file and recursively</p> <p><a href=\"io/write_graph\"><code translate=\"no\" dir=\"ltr\">write_graph(...)</code></a>: Writes a graph proto to a file.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/io\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/io</a>\n  </p>\n</div>\n","autodiff":"<h1 class=\"devsite-page-title\">Module: tf.autodiff</h1>       <p>Public API for tf.autodiff namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"autodiff/forwardaccumulator\"><code translate=\"no\" dir=\"ltr\">class ForwardAccumulator</code></a>: Computes Jacobian-vector products (\"JVP\"s) using forward-mode autodiff.</p> <p><a href=\"gradienttape\"><code translate=\"no\" dir=\"ltr\">class GradientTape</code></a>: Record operations for automatic differentiation.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/autodiff\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/autodiff</a>\n  </p>\n</div>\n","autograph":"<h1 class=\"devsite-page-title\">Module: tf.autograph</h1>       <p>Conversion of plain Python into TensorFlow graph code.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> In TensorFlow 2.0, AutoGraph is automatically applied when using <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>. This module contains lower-level APIs for advanced use.</span>\n</blockquote> <p>For more information, see the <a href=\"https://www.tensorflow.org/guide/autograph\">AutoGraph guide</a>.</p> <p>By equivalent graph code we mean code that generates a TensorFlow graph when run. The generated graph has the same effects as the original code when executed (for example with <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a> or <a href=\"compat/v1/session#run\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Session.run</code></a>). In other words, using AutoGraph can be thought of as running Python in TensorFlow.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"autograph/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.autograph.experimental namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"autograph/set_verbosity\"><code translate=\"no\" dir=\"ltr\">set_verbosity(...)</code></a>: Sets the AutoGraph verbosity level.</p> <p><a href=\"autograph/to_code\"><code translate=\"no\" dir=\"ltr\">to_code(...)</code></a>: Returns the source code generated by AutoGraph, as a string.</p> <p><a href=\"autograph/to_graph\"><code translate=\"no\" dir=\"ltr\">to_graph(...)</code></a>: Converts a Python entity into a TensorFlow graph.</p> <p><a href=\"autograph/trace\"><code translate=\"no\" dir=\"ltr\">trace(...)</code></a>: Traces argument information at compilation time.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/autograph\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/autograph</a>\n  </p>\n</div>\n","estimator":"<h1 class=\"devsite-page-title\">Module: tf.estimator</h1>       <p>Estimator: High level tools for working with models.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"estimator/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.estimator.experimental namespace.</p> <p><a href=\"estimator/export\"><code translate=\"no\" dir=\"ltr\">export</code></a> module: All public utility methods for exporting Estimator to SavedModel.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"estimator/baselineclassifier\"><code translate=\"no\" dir=\"ltr\">class BaselineClassifier</code></a>: A classifier that can establish a simple baseline.</p> <p><a href=\"estimator/baselineestimator\"><code translate=\"no\" dir=\"ltr\">class BaselineEstimator</code></a>: An estimator that can establish a simple baseline.</p> <p><a href=\"estimator/baselineregressor\"><code translate=\"no\" dir=\"ltr\">class BaselineRegressor</code></a>: A regressor that can establish a simple baseline.</p> <p><a href=\"estimator/bestexporter\"><code translate=\"no\" dir=\"ltr\">class BestExporter</code></a>: This class exports the serving graph and checkpoints of the best models.</p> <p><a href=\"estimator/binaryclasshead\"><code translate=\"no\" dir=\"ltr\">class BinaryClassHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for single label binary classification.</p> <p><a href=\"estimator/boostedtreesclassifier\"><code translate=\"no\" dir=\"ltr\">class BoostedTreesClassifier</code></a>: A Classifier for Tensorflow Boosted Trees models.</p> <p><a href=\"estimator/boostedtreesestimator\"><code translate=\"no\" dir=\"ltr\">class BoostedTreesEstimator</code></a>: An Estimator for Tensorflow Boosted Trees models.</p> <p><a href=\"estimator/boostedtreesregressor\"><code translate=\"no\" dir=\"ltr\">class BoostedTreesRegressor</code></a>: A Regressor for Tensorflow Boosted Trees models.</p> <p><a href=\"estimator/checkpointsaverhook\"><code translate=\"no\" dir=\"ltr\">class CheckpointSaverHook</code></a>: Saves checkpoints every N steps or seconds.</p> <p><a href=\"estimator/checkpointsaverlistener\"><code translate=\"no\" dir=\"ltr\">class CheckpointSaverListener</code></a>: Interface for listeners that take action before or after checkpoint save.</p> <p><a href=\"estimator/dnnclassifier\"><code translate=\"no\" dir=\"ltr\">class DNNClassifier</code></a>: A classifier for TensorFlow DNN models.</p> <p><a href=\"estimator/dnnestimator\"><code translate=\"no\" dir=\"ltr\">class DNNEstimator</code></a>: An estimator for TensorFlow DNN models with user-specified head.</p> <p><a href=\"estimator/dnnlinearcombinedclassifier\"><code translate=\"no\" dir=\"ltr\">class DNNLinearCombinedClassifier</code></a>: An estimator for TensorFlow Linear and DNN joined classification models.</p> <p><a href=\"estimator/dnnlinearcombinedestimator\"><code translate=\"no\" dir=\"ltr\">class DNNLinearCombinedEstimator</code></a>: An estimator for TensorFlow Linear and DNN joined models with custom head.</p> <p><a href=\"estimator/dnnlinearcombinedregressor\"><code translate=\"no\" dir=\"ltr\">class DNNLinearCombinedRegressor</code></a>: An estimator for TensorFlow Linear and DNN joined models for regression.</p> <p><a href=\"estimator/dnnregressor\"><code translate=\"no\" dir=\"ltr\">class DNNRegressor</code></a>: A regressor for TensorFlow DNN models.</p> <p><a href=\"estimator/estimator\"><code translate=\"no\" dir=\"ltr\">class Estimator</code></a>: Estimator class to train and evaluate TensorFlow models.</p> <p><a href=\"estimator/estimatorspec\"><code translate=\"no\" dir=\"ltr\">class EstimatorSpec</code></a>: Ops and objects returned from a <code translate=\"no\" dir=\"ltr\">model_fn</code> and passed to an <code translate=\"no\" dir=\"ltr\">Estimator</code>.</p> <p><a href=\"estimator/evalspec\"><code translate=\"no\" dir=\"ltr\">class EvalSpec</code></a>: Configuration for the \"eval\" part for the <code translate=\"no\" dir=\"ltr\">train_and_evaluate</code> call.</p> <p><a href=\"estimator/exporter\"><code translate=\"no\" dir=\"ltr\">class Exporter</code></a>: A class representing a type of model export.</p> <p><a href=\"estimator/feedfnhook\"><code translate=\"no\" dir=\"ltr\">class FeedFnHook</code></a>: Runs <code translate=\"no\" dir=\"ltr\">feed_fn</code> and sets the <code translate=\"no\" dir=\"ltr\">feed_dict</code> accordingly.</p> <p><a href=\"estimator/finalexporter\"><code translate=\"no\" dir=\"ltr\">class FinalExporter</code></a>: This class exports the serving graph and checkpoints at the end.</p> <p><a href=\"estimator/finalopshook\"><code translate=\"no\" dir=\"ltr\">class FinalOpsHook</code></a>: A hook which evaluates <code translate=\"no\" dir=\"ltr\">Tensors</code> at the end of a session.</p> <p><a href=\"estimator/globalstepwaiterhook\"><code translate=\"no\" dir=\"ltr\">class GlobalStepWaiterHook</code></a>: Delays execution until global step reaches <code translate=\"no\" dir=\"ltr\">wait_until_step</code>.</p> <p><a href=\"estimator/head\"><code translate=\"no\" dir=\"ltr\">class Head</code></a>: Interface for the head/top of a model.</p> <p><a href=\"estimator/latestexporter\"><code translate=\"no\" dir=\"ltr\">class LatestExporter</code></a>: This class regularly exports the serving graph and checkpoints.</p> <p><a href=\"estimator/linearclassifier\"><code translate=\"no\" dir=\"ltr\">class LinearClassifier</code></a>: Linear classifier model.</p> <p><a href=\"estimator/linearestimator\"><code translate=\"no\" dir=\"ltr\">class LinearEstimator</code></a>: An estimator for TensorFlow linear models with user-specified head.</p> <p><a href=\"estimator/linearregressor\"><code translate=\"no\" dir=\"ltr\">class LinearRegressor</code></a>: An estimator for TensorFlow Linear regression problems.</p> <p><a href=\"estimator/loggingtensorhook\"><code translate=\"no\" dir=\"ltr\">class LoggingTensorHook</code></a>: Prints the given tensors every N local steps, every N seconds, or at end.</p> <p><a href=\"estimator/logisticregressionhead\"><code translate=\"no\" dir=\"ltr\">class LogisticRegressionHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for logistic regression.</p> <p><a href=\"estimator/modekeys\"><code translate=\"no\" dir=\"ltr\">class ModeKeys</code></a>: Standard names for Estimator model modes.</p> <p><a href=\"estimator/multiclasshead\"><code translate=\"no\" dir=\"ltr\">class MultiClassHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for multi class classification.</p> <p><a href=\"estimator/multihead\"><code translate=\"no\" dir=\"ltr\">class MultiHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for multi-objective learning.</p> <p><a href=\"estimator/multilabelhead\"><code translate=\"no\" dir=\"ltr\">class MultiLabelHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for multi-label classification.</p> <p><a href=\"estimator/nanlossduringtrainingerror\"><code translate=\"no\" dir=\"ltr\">class NanLossDuringTrainingError</code></a>: Unspecified run-time error.</p> <p><a href=\"estimator/nantensorhook\"><code translate=\"no\" dir=\"ltr\">class NanTensorHook</code></a>: Monitors the loss tensor and stops training if loss is NaN.</p> <p><a href=\"estimator/poissonregressionhead\"><code translate=\"no\" dir=\"ltr\">class PoissonRegressionHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for poisson regression using <a href=\"nn/log_poisson_loss\"><code translate=\"no\" dir=\"ltr\">tf.nn.log_poisson_loss</code></a>.</p> <p><a href=\"estimator/profilerhook\"><code translate=\"no\" dir=\"ltr\">class ProfilerHook</code></a>: Captures CPU/GPU profiling information every N steps or seconds.</p> <p><a href=\"estimator/regressionhead\"><code translate=\"no\" dir=\"ltr\">class RegressionHead</code></a>: Creates a <code translate=\"no\" dir=\"ltr\">Head</code> for regression using the <code translate=\"no\" dir=\"ltr\">mean_squared_error</code> loss.</p> <p><a href=\"estimator/runconfig\"><code translate=\"no\" dir=\"ltr\">class RunConfig</code></a>: This class specifies the configurations for an <code translate=\"no\" dir=\"ltr\">Estimator</code> run.</p> <p><a href=\"estimator/secondorsteptimer\"><code translate=\"no\" dir=\"ltr\">class SecondOrStepTimer</code></a>: Timer that triggers at most once every N seconds or once every N steps.</p> <p><a href=\"estimator/sessionrunargs\"><code translate=\"no\" dir=\"ltr\">class SessionRunArgs</code></a>: Represents arguments to be added to a <code translate=\"no\" dir=\"ltr\">Session.run()</code> call.</p> <p><a href=\"estimator/sessionruncontext\"><code translate=\"no\" dir=\"ltr\">class SessionRunContext</code></a>: Provides information about the <code translate=\"no\" dir=\"ltr\">session.run()</code> call being made.</p> <p><a href=\"estimator/sessionrunhook\"><code translate=\"no\" dir=\"ltr\">class SessionRunHook</code></a>: Hook to extend calls to MonitoredSession.run().</p> <p><a href=\"estimator/sessionrunvalues\"><code translate=\"no\" dir=\"ltr\">class SessionRunValues</code></a>: Contains the results of <code translate=\"no\" dir=\"ltr\">Session.run()</code>.</p> <p><a href=\"estimator/stepcounterhook\"><code translate=\"no\" dir=\"ltr\">class StepCounterHook</code></a>: Hook that counts steps per second.</p> <p><a href=\"estimator/stopatstephook\"><code translate=\"no\" dir=\"ltr\">class StopAtStepHook</code></a>: Hook that requests stop at a specified step.</p> <p><a href=\"estimator/summarysaverhook\"><code translate=\"no\" dir=\"ltr\">class SummarySaverHook</code></a>: Saves summaries every N steps.</p> <p><a href=\"estimator/trainspec\"><code translate=\"no\" dir=\"ltr\">class TrainSpec</code></a>: Configuration for the \"train\" part for the <code translate=\"no\" dir=\"ltr\">train_and_evaluate</code> call.</p> <p><a href=\"estimator/vocabinfo\"><code translate=\"no\" dir=\"ltr\">class VocabInfo</code></a>: Vocabulary information for warm-starting.</p> <p><a href=\"estimator/warmstartsettings\"><code translate=\"no\" dir=\"ltr\">class WarmStartSettings</code></a>: Settings for warm-starting in <code translate=\"no\" dir=\"ltr\">tf.estimator.Estimators</code>.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"estimator/add_metrics\"><code translate=\"no\" dir=\"ltr\">add_metrics(...)</code></a>: Creates a new <a href=\"estimator/estimator\"><code translate=\"no\" dir=\"ltr\">tf.estimator.Estimator</code></a> which has given metrics.</p> <p><a href=\"estimator/classifier_parse_example_spec\"><code translate=\"no\" dir=\"ltr\">classifier_parse_example_spec(...)</code></a>: Generates parsing spec for tf.parse_example to be used with classifiers.</p> <p><a href=\"estimator/regressor_parse_example_spec\"><code translate=\"no\" dir=\"ltr\">regressor_parse_example_spec(...)</code></a>: Generates parsing spec for tf.parse_example to be used with regressors.</p> <p><a href=\"estimator/train_and_evaluate\"><code translate=\"no\" dir=\"ltr\">train_and_evaluate(...)</code></a>: Train and evaluate the <code translate=\"no\" dir=\"ltr\">estimator</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator</a>\n  </p>\n</div>\n","image":"<h1 class=\"devsite-page-title\">Module: tf.image</h1>       <p>Image ops.</p> <p>The <a href=\"image\"><code translate=\"no\" dir=\"ltr\">tf.image</code></a> module contains various functions for image processing and decoding-encoding Ops.</p> <p>Many of the encoding/decoding functions are also available in the core <a href=\"io\"><code translate=\"no\" dir=\"ltr\">tf.io</code></a> module.</p> <h2 id=\"image_processing\" data-text=\"Image processing\">Image processing</h2> <h3 id=\"resizing\" data-text=\"Resizing\">Resizing</h3> <p>The resizing Ops accept input images as tensors of several types. They always output resized images as float32 tensors.</p> <p>The convenience function <a href=\"image/resize\"><code translate=\"no\" dir=\"ltr\">tf.image.resize</code></a> supports both 4-D and 3-D tensors as input and output. 4-D tensors are for batches of images, 3-D tensors for individual images.</p> <p>Resized images will be distorted if their original aspect ratio is not the same as size. To avoid distortions see tf.image.resize_with_pad.</p> <ul> <li><a href=\"image/resize\"><code translate=\"no\" dir=\"ltr\">tf.image.resize</code></a></li> <li><a href=\"image/resize_with_pad\"><code translate=\"no\" dir=\"ltr\">tf.image.resize_with_pad</code></a></li> <li><a href=\"image/resize_with_crop_or_pad\"><code translate=\"no\" dir=\"ltr\">tf.image.resize_with_crop_or_pad</code></a></li> </ul> <p>The Class <a href=\"image/resizemethod\"><code translate=\"no\" dir=\"ltr\">tf.image.ResizeMethod</code></a> provides various resize methods like <code translate=\"no\" dir=\"ltr\">bilinear</code>, <code translate=\"no\" dir=\"ltr\">nearest_neighbor</code>.</p> <h3 id=\"converting_between_colorspaces\" data-text=\"Converting Between Colorspaces\">Converting Between Colorspaces</h3> <p>Image ops work either on individual images or on batches of images, depending on the shape of their input Tensor.</p> <p>If 3-D, the shape is <code translate=\"no\" dir=\"ltr\">[height, width, channels]</code>, and the Tensor represents one image. If 4-D, the shape is <code translate=\"no\" dir=\"ltr\">[batch_size, height, width, channels]</code>, and the Tensor represents <code translate=\"no\" dir=\"ltr\">batch_size</code> images.</p> <p>Currently, <code translate=\"no\" dir=\"ltr\">channels</code> can usefully be 1, 2, 3, or 4. Single-channel images are grayscale, images with 3 channels are encoded as either RGB or HSV. Images with 2 or 4 channels include an alpha channel, which has to be stripped from the image before passing the image to most image processing functions (and can be re-attached later).</p> <p>Internally, images are either stored in as one <code translate=\"no\" dir=\"ltr\">float32</code> per channel per pixel (implicitly, values are assumed to lie in <code translate=\"no\" dir=\"ltr\">[0,1)</code>) or one <code translate=\"no\" dir=\"ltr\">uint8</code> per channel per pixel (values are assumed to lie in <code translate=\"no\" dir=\"ltr\">[0,255]</code>).</p> <p>TensorFlow can convert between images in RGB or HSV or YIQ.</p> <ul> <li>\n<a href=\"image/rgb_to_grayscale\"><code translate=\"no\" dir=\"ltr\">tf.image.rgb_to_grayscale</code></a>, <a href=\"image/grayscale_to_rgb\"><code translate=\"no\" dir=\"ltr\">tf.image.grayscale_to_rgb</code></a>\n</li> <li>\n<a href=\"image/rgb_to_hsv\"><code translate=\"no\" dir=\"ltr\">tf.image.rgb_to_hsv</code></a>, <a href=\"image/hsv_to_rgb\"><code translate=\"no\" dir=\"ltr\">tf.image.hsv_to_rgb</code></a>\n</li> <li>\n<a href=\"image/rgb_to_yiq\"><code translate=\"no\" dir=\"ltr\">tf.image.rgb_to_yiq</code></a>, <a href=\"image/yiq_to_rgb\"><code translate=\"no\" dir=\"ltr\">tf.image.yiq_to_rgb</code></a>\n</li> <li>\n<a href=\"image/rgb_to_yuv\"><code translate=\"no\" dir=\"ltr\">tf.image.rgb_to_yuv</code></a>, <a href=\"image/yuv_to_rgb\"><code translate=\"no\" dir=\"ltr\">tf.image.yuv_to_rgb</code></a>\n</li> <li><a href=\"image/image_gradients\"><code translate=\"no\" dir=\"ltr\">tf.image.image_gradients</code></a></li> <li><a href=\"image/convert_image_dtype\"><code translate=\"no\" dir=\"ltr\">tf.image.convert_image_dtype</code></a></li> </ul> <h3 id=\"image_adjustments\" data-text=\"Image Adjustments\">Image Adjustments</h3> <p>TensorFlow provides functions to adjust images in various ways: brightness, contrast, hue, and saturation. Each adjustment can be done with predefined parameters or with random parameters picked from predefined intervals. Random adjustments are often useful to expand a training set and reduce overfitting.</p> <p>If several adjustments are chained it is advisable to minimize the number of redundant conversions by first converting the images to the most natural data type and representation.</p> <ul> <li><a href=\"image/adjust_brightness\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_brightness</code></a></li> <li><a href=\"image/adjust_contrast\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_contrast</code></a></li> <li><a href=\"image/adjust_gamma\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_gamma</code></a></li> <li><a href=\"image/adjust_hue\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_hue</code></a></li> <li><a href=\"image/adjust_jpeg_quality\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_jpeg_quality</code></a></li> <li><a href=\"image/adjust_saturation\"><code translate=\"no\" dir=\"ltr\">tf.image.adjust_saturation</code></a></li> <li><a href=\"image/random_brightness\"><code translate=\"no\" dir=\"ltr\">tf.image.random_brightness</code></a></li> <li><a href=\"image/random_contrast\"><code translate=\"no\" dir=\"ltr\">tf.image.random_contrast</code></a></li> <li><a href=\"image/random_hue\"><code translate=\"no\" dir=\"ltr\">tf.image.random_hue</code></a></li> <li><a href=\"image/random_saturation\"><code translate=\"no\" dir=\"ltr\">tf.image.random_saturation</code></a></li> <li><a href=\"image/per_image_standardization\"><code translate=\"no\" dir=\"ltr\">tf.image.per_image_standardization</code></a></li> </ul> <h3 id=\"working_with_bounding_boxes\" data-text=\"Working with Bounding Boxes\">Working with Bounding Boxes</h3> <ul> <li><a href=\"image/draw_bounding_boxes\"><code translate=\"no\" dir=\"ltr\">tf.image.draw_bounding_boxes</code></a></li> <li><a href=\"image/combined_non_max_suppression\"><code translate=\"no\" dir=\"ltr\">tf.image.combined_non_max_suppression</code></a></li> <li><a href=\"image/generate_bounding_box_proposals\"><code translate=\"no\" dir=\"ltr\">tf.image.generate_bounding_box_proposals</code></a></li> <li><a href=\"image/non_max_suppression\"><code translate=\"no\" dir=\"ltr\">tf.image.non_max_suppression</code></a></li> <li><a href=\"image/non_max_suppression_overlaps\"><code translate=\"no\" dir=\"ltr\">tf.image.non_max_suppression_overlaps</code></a></li> <li><a href=\"image/non_max_suppression_padded\"><code translate=\"no\" dir=\"ltr\">tf.image.non_max_suppression_padded</code></a></li> <li><a href=\"image/non_max_suppression_with_scores\"><code translate=\"no\" dir=\"ltr\">tf.image.non_max_suppression_with_scores</code></a></li> <li><a href=\"image/pad_to_bounding_box\"><code translate=\"no\" dir=\"ltr\">tf.image.pad_to_bounding_box</code></a></li> <li><a href=\"image/sample_distorted_bounding_box\"><code translate=\"no\" dir=\"ltr\">tf.image.sample_distorted_bounding_box</code></a></li> </ul> <h3 id=\"cropping\" data-text=\"Cropping\">Cropping</h3> <ul> <li><a href=\"image/central_crop\"><code translate=\"no\" dir=\"ltr\">tf.image.central_crop</code></a></li> <li><a href=\"image/crop_and_resize\"><code translate=\"no\" dir=\"ltr\">tf.image.crop_and_resize</code></a></li> <li><a href=\"image/crop_to_bounding_box\"><code translate=\"no\" dir=\"ltr\">tf.image.crop_to_bounding_box</code></a></li> <li><a href=\"io/decode_and_crop_jpeg\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_and_crop_jpeg</code></a></li> <li><a href=\"image/extract_glimpse\"><code translate=\"no\" dir=\"ltr\">tf.image.extract_glimpse</code></a></li> <li><a href=\"image/random_crop\"><code translate=\"no\" dir=\"ltr\">tf.image.random_crop</code></a></li> <li><a href=\"image/resize_with_crop_or_pad\"><code translate=\"no\" dir=\"ltr\">tf.image.resize_with_crop_or_pad</code></a></li> </ul> <h3 id=\"flipping_rotating_and_transposing\" data-text=\"Flipping, Rotating and Transposing\">Flipping, Rotating and Transposing</h3> <ul> <li><a href=\"image/flip_left_right\"><code translate=\"no\" dir=\"ltr\">tf.image.flip_left_right</code></a></li> <li><a href=\"image/flip_up_down\"><code translate=\"no\" dir=\"ltr\">tf.image.flip_up_down</code></a></li> <li><a href=\"image/random_flip_left_right\"><code translate=\"no\" dir=\"ltr\">tf.image.random_flip_left_right</code></a></li> <li><a href=\"image/random_flip_up_down\"><code translate=\"no\" dir=\"ltr\">tf.image.random_flip_up_down</code></a></li> <li><a href=\"image/rot90\"><code translate=\"no\" dir=\"ltr\">tf.image.rot90</code></a></li> <li><a href=\"image/transpose\"><code translate=\"no\" dir=\"ltr\">tf.image.transpose</code></a></li> </ul> <h2 id=\"image_decoding_and_encoding\" data-text=\"Image decoding and encoding\">Image decoding and encoding</h2> <p>TensorFlow provides Ops to decode and encode JPEG and PNG formats. Encoded images are represented by scalar string Tensors, decoded images by 3-D uint8 tensors of shape <code translate=\"no\" dir=\"ltr\">[height, width, channels]</code>. (PNG also supports uint16.)</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <code translate=\"no\" dir=\"ltr\">decode_gif</code> returns a 4-D array <code translate=\"no\" dir=\"ltr\">[num_frames, height, width, 3]</code></span>\n</blockquote> <p>The encode and decode Ops apply to one image at a time. Their input and output are all of variable size. If you need fixed size images, pass the output of the decode Ops to one of the cropping and resizing Ops.</p> <ul> <li><a href=\"io/decode_bmp\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_bmp</code></a></li> <li><a href=\"io/decode_gif\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_gif</code></a></li> <li><a href=\"io/decode_image\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_image</code></a></li> <li><a href=\"io/decode_jpeg\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_jpeg</code></a></li> <li><a href=\"io/decode_and_crop_jpeg\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_and_crop_jpeg</code></a></li> <li><a href=\"io/decode_png\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_png</code></a></li> <li><a href=\"io/encode_jpeg\"><code translate=\"no\" dir=\"ltr\">tf.io.encode_jpeg</code></a></li> <li><a href=\"io/encode_png\"><code translate=\"no\" dir=\"ltr\">tf.io.encode_png</code></a></li> </ul> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"image/resizemethod\"><code translate=\"no\" dir=\"ltr\">class ResizeMethod</code></a>: See <a href=\"image/resize\"><code translate=\"no\" dir=\"ltr\">tf.image.resize</code></a> for details.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"image/adjust_brightness\"><code translate=\"no\" dir=\"ltr\">adjust_brightness(...)</code></a>: Adjust the brightness of RGB or Grayscale images.</p> <p><a href=\"image/adjust_contrast\"><code translate=\"no\" dir=\"ltr\">adjust_contrast(...)</code></a>: Adjust contrast of RGB or grayscale images.</p> <p><a href=\"image/adjust_gamma\"><code translate=\"no\" dir=\"ltr\">adjust_gamma(...)</code></a>: Performs <a href=\"http://en.wikipedia.org/wiki/Gamma_correction\">Gamma Correction</a>.</p> <p><a href=\"image/adjust_hue\"><code translate=\"no\" dir=\"ltr\">adjust_hue(...)</code></a>: Adjust hue of RGB images.</p> <p><a href=\"image/adjust_jpeg_quality\"><code translate=\"no\" dir=\"ltr\">adjust_jpeg_quality(...)</code></a>: Adjust jpeg encoding quality of an image.</p> <p><a href=\"image/adjust_saturation\"><code translate=\"no\" dir=\"ltr\">adjust_saturation(...)</code></a>: Adjust saturation of RGB images.</p> <p><a href=\"image/central_crop\"><code translate=\"no\" dir=\"ltr\">central_crop(...)</code></a>: Crop the central region of the image(s).</p> <p><a href=\"image/combined_non_max_suppression\"><code translate=\"no\" dir=\"ltr\">combined_non_max_suppression(...)</code></a>: Greedily selects a subset of bounding boxes in descending order of score.</p> <p><a href=\"image/convert_image_dtype\"><code translate=\"no\" dir=\"ltr\">convert_image_dtype(...)</code></a>: Convert <code translate=\"no\" dir=\"ltr\">image</code> to <code translate=\"no\" dir=\"ltr\">dtype</code>, scaling its values if needed.</p> <p><a href=\"image/crop_and_resize\"><code translate=\"no\" dir=\"ltr\">crop_and_resize(...)</code></a>: Extracts crops from the input image tensor and resizes them.</p> <p><a href=\"image/crop_to_bounding_box\"><code translate=\"no\" dir=\"ltr\">crop_to_bounding_box(...)</code></a>: Crops an image to a specified bounding box.</p> <p><a href=\"io/decode_and_crop_jpeg\"><code translate=\"no\" dir=\"ltr\">decode_and_crop_jpeg(...)</code></a>: Decode and Crop a JPEG-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_bmp\"><code translate=\"no\" dir=\"ltr\">decode_bmp(...)</code></a>: Decode the first frame of a BMP-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_gif\"><code translate=\"no\" dir=\"ltr\">decode_gif(...)</code></a>: Decode the frame(s) of a GIF-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_image\"><code translate=\"no\" dir=\"ltr\">decode_image(...)</code></a>: Function for <code translate=\"no\" dir=\"ltr\">decode_bmp</code>, <code translate=\"no\" dir=\"ltr\">decode_gif</code>, <code translate=\"no\" dir=\"ltr\">decode_jpeg</code>, and <code translate=\"no\" dir=\"ltr\">decode_png</code>.</p> <p><a href=\"io/decode_jpeg\"><code translate=\"no\" dir=\"ltr\">decode_jpeg(...)</code></a>: Decode a JPEG-encoded image to a uint8 tensor.</p> <p><a href=\"io/decode_png\"><code translate=\"no\" dir=\"ltr\">decode_png(...)</code></a>: Decode a PNG-encoded image to a uint8 or uint16 tensor.</p> <p><a href=\"image/draw_bounding_boxes\"><code translate=\"no\" dir=\"ltr\">draw_bounding_boxes(...)</code></a>: Draw bounding boxes on a batch of images.</p> <p><a href=\"io/encode_jpeg\"><code translate=\"no\" dir=\"ltr\">encode_jpeg(...)</code></a>: JPEG-encode an image.</p> <p><a href=\"io/encode_png\"><code translate=\"no\" dir=\"ltr\">encode_png(...)</code></a>: PNG-encode an image.</p> <p><a href=\"image/extract_glimpse\"><code translate=\"no\" dir=\"ltr\">extract_glimpse(...)</code></a>: Extracts a glimpse from the input tensor.</p> <p><a href=\"io/extract_jpeg_shape\"><code translate=\"no\" dir=\"ltr\">extract_jpeg_shape(...)</code></a>: Extract the shape information of a JPEG-encoded image.</p> <p><a href=\"image/extract_patches\"><code translate=\"no\" dir=\"ltr\">extract_patches(...)</code></a>: Extract <code translate=\"no\" dir=\"ltr\">patches</code> from <code translate=\"no\" dir=\"ltr\">images</code>.</p> <p><a href=\"image/flip_left_right\"><code translate=\"no\" dir=\"ltr\">flip_left_right(...)</code></a>: Flip an image horizontally (left to right).</p> <p><a href=\"image/flip_up_down\"><code translate=\"no\" dir=\"ltr\">flip_up_down(...)</code></a>: Flip an image vertically (upside down).</p> <p><a href=\"image/generate_bounding_box_proposals\"><code translate=\"no\" dir=\"ltr\">generate_bounding_box_proposals(...)</code></a>: Generate bounding box proposals from encoded bounding boxes.</p> <p><a href=\"image/grayscale_to_rgb\"><code translate=\"no\" dir=\"ltr\">grayscale_to_rgb(...)</code></a>: Converts one or more images from Grayscale to RGB.</p> <p><a href=\"image/hsv_to_rgb\"><code translate=\"no\" dir=\"ltr\">hsv_to_rgb(...)</code></a>: Convert one or more images from HSV to RGB.</p> <p><a href=\"image/image_gradients\"><code translate=\"no\" dir=\"ltr\">image_gradients(...)</code></a>: Returns image gradients (dy, dx) for each color channel.</p> <p><a href=\"io/is_jpeg\"><code translate=\"no\" dir=\"ltr\">is_jpeg(...)</code></a>: Convenience function to check if the 'contents' encodes a JPEG image.</p> <p><a href=\"image/non_max_suppression\"><code translate=\"no\" dir=\"ltr\">non_max_suppression(...)</code></a>: Greedily selects a subset of bounding boxes in descending order of score.</p> <p><a href=\"image/non_max_suppression_overlaps\"><code translate=\"no\" dir=\"ltr\">non_max_suppression_overlaps(...)</code></a>: Greedily selects a subset of bounding boxes in descending order of score.</p> <p><a href=\"image/non_max_suppression_padded\"><code translate=\"no\" dir=\"ltr\">non_max_suppression_padded(...)</code></a>: Greedily selects a subset of bounding boxes in descending order of score.</p> <p><a href=\"image/non_max_suppression_with_scores\"><code translate=\"no\" dir=\"ltr\">non_max_suppression_with_scores(...)</code></a>: Greedily selects a subset of bounding boxes in descending order of score.</p> <p><a href=\"image/pad_to_bounding_box\"><code translate=\"no\" dir=\"ltr\">pad_to_bounding_box(...)</code></a>: Pad <code translate=\"no\" dir=\"ltr\">image</code> with zeros to the specified <code translate=\"no\" dir=\"ltr\">height</code> and <code translate=\"no\" dir=\"ltr\">width</code>.</p> <p><a href=\"image/per_image_standardization\"><code translate=\"no\" dir=\"ltr\">per_image_standardization(...)</code></a>: Linearly scales each image in <code translate=\"no\" dir=\"ltr\">image</code> to have mean 0 and variance 1.</p> <p><a href=\"image/psnr\"><code translate=\"no\" dir=\"ltr\">psnr(...)</code></a>: Returns the Peak Signal-to-Noise Ratio between a and b.</p> <p><a href=\"image/random_brightness\"><code translate=\"no\" dir=\"ltr\">random_brightness(...)</code></a>: Adjust the brightness of images by a random factor.</p> <p><a href=\"image/random_contrast\"><code translate=\"no\" dir=\"ltr\">random_contrast(...)</code></a>: Adjust the contrast of an image or images by a random factor.</p> <p><a href=\"image/random_crop\"><code translate=\"no\" dir=\"ltr\">random_crop(...)</code></a>: Randomly crops a tensor to a given size.</p> <p><a href=\"image/random_flip_left_right\"><code translate=\"no\" dir=\"ltr\">random_flip_left_right(...)</code></a>: Randomly flip an image horizontally (left to right).</p> <p><a href=\"image/random_flip_up_down\"><code translate=\"no\" dir=\"ltr\">random_flip_up_down(...)</code></a>: Randomly flips an image vertically (upside down).</p> <p><a href=\"image/random_hue\"><code translate=\"no\" dir=\"ltr\">random_hue(...)</code></a>: Adjust the hue of RGB images by a random factor.</p> <p><a href=\"image/random_jpeg_quality\"><code translate=\"no\" dir=\"ltr\">random_jpeg_quality(...)</code></a>: Randomly changes jpeg encoding quality for inducing jpeg noise.</p> <p><a href=\"image/random_saturation\"><code translate=\"no\" dir=\"ltr\">random_saturation(...)</code></a>: Adjust the saturation of RGB images by a random factor.</p> <p><a href=\"image/resize\"><code translate=\"no\" dir=\"ltr\">resize(...)</code></a>: Resize <code translate=\"no\" dir=\"ltr\">images</code> to <code translate=\"no\" dir=\"ltr\">size</code> using the specified <code translate=\"no\" dir=\"ltr\">method</code>.</p> <p><a href=\"image/resize_with_crop_or_pad\"><code translate=\"no\" dir=\"ltr\">resize_with_crop_or_pad(...)</code></a>: Crops and/or pads an image to a target width and height.</p> <p><a href=\"image/resize_with_pad\"><code translate=\"no\" dir=\"ltr\">resize_with_pad(...)</code></a>: Resizes and pads an image to a target width and height.</p> <p><a href=\"image/rgb_to_grayscale\"><code translate=\"no\" dir=\"ltr\">rgb_to_grayscale(...)</code></a>: Converts one or more images from RGB to Grayscale.</p> <p><a href=\"image/rgb_to_hsv\"><code translate=\"no\" dir=\"ltr\">rgb_to_hsv(...)</code></a>: Converts one or more images from RGB to HSV.</p> <p><a href=\"image/rgb_to_yiq\"><code translate=\"no\" dir=\"ltr\">rgb_to_yiq(...)</code></a>: Converts one or more images from RGB to YIQ.</p> <p><a href=\"image/rgb_to_yuv\"><code translate=\"no\" dir=\"ltr\">rgb_to_yuv(...)</code></a>: Converts one or more images from RGB to YUV.</p> <p><a href=\"image/rot90\"><code translate=\"no\" dir=\"ltr\">rot90(...)</code></a>: Rotate image(s) counter-clockwise by 90 degrees.</p> <p><a href=\"image/sample_distorted_bounding_box\"><code translate=\"no\" dir=\"ltr\">sample_distorted_bounding_box(...)</code></a>: Generate a single randomly distorted bounding box for an image.</p> <p><a href=\"image/sobel_edges\"><code translate=\"no\" dir=\"ltr\">sobel_edges(...)</code></a>: Returns a tensor holding Sobel edge maps.</p> <p><a href=\"image/ssim\"><code translate=\"no\" dir=\"ltr\">ssim(...)</code></a>: Computes SSIM index between img1 and img2.</p> <p><a href=\"image/ssim_multiscale\"><code translate=\"no\" dir=\"ltr\">ssim_multiscale(...)</code></a>: Computes the MS-SSIM between img1 and img2.</p> <p><a href=\"image/stateless_random_brightness\"><code translate=\"no\" dir=\"ltr\">stateless_random_brightness(...)</code></a>: Adjust the brightness of images by a random factor deterministically.</p> <p><a href=\"image/stateless_random_contrast\"><code translate=\"no\" dir=\"ltr\">stateless_random_contrast(...)</code></a>: Adjust the contrast of images by a random factor deterministically.</p> <p><a href=\"image/stateless_random_crop\"><code translate=\"no\" dir=\"ltr\">stateless_random_crop(...)</code></a>: Randomly crops a tensor to a given size in a deterministic manner.</p> <p><a href=\"image/stateless_random_flip_left_right\"><code translate=\"no\" dir=\"ltr\">stateless_random_flip_left_right(...)</code></a>: Randomly flip an image horizontally (left to right) deterministically.</p> <p><a href=\"image/stateless_random_flip_up_down\"><code translate=\"no\" dir=\"ltr\">stateless_random_flip_up_down(...)</code></a>: Randomly flip an image vertically (upside down) deterministically.</p> <p><a href=\"image/stateless_random_hue\"><code translate=\"no\" dir=\"ltr\">stateless_random_hue(...)</code></a>: Adjust the hue of RGB images by a random factor deterministically.</p> <p><a href=\"image/stateless_random_jpeg_quality\"><code translate=\"no\" dir=\"ltr\">stateless_random_jpeg_quality(...)</code></a>: Deterministically radomize jpeg encoding quality for inducing jpeg noise.</p> <p><a href=\"image/stateless_random_saturation\"><code translate=\"no\" dir=\"ltr\">stateless_random_saturation(...)</code></a>: Adjust the saturation of RGB images by a random factor deterministically.</p> <p><a href=\"image/stateless_sample_distorted_bounding_box\"><code translate=\"no\" dir=\"ltr\">stateless_sample_distorted_bounding_box(...)</code></a>: Generate a randomly distorted bounding box for an image deterministically.</p> <p><a href=\"image/total_variation\"><code translate=\"no\" dir=\"ltr\">total_variation(...)</code></a>: Calculate and return the total variation for one or more images.</p> <p><a href=\"image/transpose\"><code translate=\"no\" dir=\"ltr\">transpose(...)</code></a>: Transpose image(s) by swapping the height and width dimension.</p> <p><a href=\"image/yiq_to_rgb\"><code translate=\"no\" dir=\"ltr\">yiq_to_rgb(...)</code></a>: Converts one or more images from YIQ to RGB.</p> <p><a href=\"image/yuv_to_rgb\"><code translate=\"no\" dir=\"ltr\">yuv_to_rgb(...)</code></a>: Converts one or more images from YUV to RGB.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/image\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/image</a>\n  </p>\n</div>\n","keras/initializers":"<h1 class=\"devsite-page-title\">Module: tf.keras.initializers</h1>       <p>Keras initializer serialization / deserialization.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/initializers\"><code translate=\"no\" dir=\"ltr\">tf.initializers</code></a></p> </section> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"initializers/constant\"><code translate=\"no\" dir=\"ltr\">class Constant</code></a>: Initializer that generates tensors with constant values.</p> <p><a href=\"initializers/glorotnormal\"><code translate=\"no\" dir=\"ltr\">class GlorotNormal</code></a>: The Glorot normal initializer, also called Xavier normal initializer.</p> <p><a href=\"initializers/glorotuniform\"><code translate=\"no\" dir=\"ltr\">class GlorotUniform</code></a>: The Glorot uniform initializer, also called Xavier uniform initializer.</p> <p><a href=\"initializers/henormal\"><code translate=\"no\" dir=\"ltr\">class HeNormal</code></a>: He normal initializer.</p> <p><a href=\"initializers/heuniform\"><code translate=\"no\" dir=\"ltr\">class HeUniform</code></a>: He uniform variance scaling initializer.</p> <p><a href=\"initializers/identity\"><code translate=\"no\" dir=\"ltr\">class Identity</code></a>: Initializer that generates the identity matrix.</p> <p><a href=\"initializers/initializer\"><code translate=\"no\" dir=\"ltr\">class Initializer</code></a>: Initializer base class: all Keras initializers inherit from this class.</p> <p><a href=\"initializers/lecunnormal\"><code translate=\"no\" dir=\"ltr\">class LecunNormal</code></a>: Lecun normal initializer.</p> <p><a href=\"initializers/lecununiform\"><code translate=\"no\" dir=\"ltr\">class LecunUniform</code></a>: Lecun uniform initializer.</p> <p><a href=\"initializers/ones\"><code translate=\"no\" dir=\"ltr\">class Ones</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href=\"initializers/orthogonal\"><code translate=\"no\" dir=\"ltr\">class Orthogonal</code></a>: Initializer that generates an orthogonal matrix.</p> <p><a href=\"initializers/randomnormal\"><code translate=\"no\" dir=\"ltr\">class RandomNormal</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href=\"initializers/randomuniform\"><code translate=\"no\" dir=\"ltr\">class RandomUniform</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href=\"initializers/truncatednormal\"><code translate=\"no\" dir=\"ltr\">class TruncatedNormal</code></a>: Initializer that generates a truncated normal distribution.</p> <p><a href=\"initializers/variancescaling\"><code translate=\"no\" dir=\"ltr\">class VarianceScaling</code></a>: Initializer capable of adapting its scale to the shape of weights tensors.</p> <p><a href=\"initializers/zeros\"><code translate=\"no\" dir=\"ltr\">class Zeros</code></a>: Initializer that generates tensors initialized to 0.</p> <p><a href=\"initializers/constant\"><code translate=\"no\" dir=\"ltr\">class constant</code></a>: Initializer that generates tensors with constant values.</p> <p><a href=\"initializers/glorotnormal\"><code translate=\"no\" dir=\"ltr\">class glorot_normal</code></a>: The Glorot normal initializer, also called Xavier normal initializer.</p> <p><a href=\"initializers/glorotuniform\"><code translate=\"no\" dir=\"ltr\">class glorot_uniform</code></a>: The Glorot uniform initializer, also called Xavier uniform initializer.</p> <p><a href=\"initializers/henormal\"><code translate=\"no\" dir=\"ltr\">class he_normal</code></a>: He normal initializer.</p> <p><a href=\"initializers/heuniform\"><code translate=\"no\" dir=\"ltr\">class he_uniform</code></a>: He uniform variance scaling initializer.</p> <p><a href=\"initializers/identity\"><code translate=\"no\" dir=\"ltr\">class identity</code></a>: Initializer that generates the identity matrix.</p> <p><a href=\"initializers/lecunnormal\"><code translate=\"no\" dir=\"ltr\">class lecun_normal</code></a>: Lecun normal initializer.</p> <p><a href=\"initializers/lecununiform\"><code translate=\"no\" dir=\"ltr\">class lecun_uniform</code></a>: Lecun uniform initializer.</p> <p><a href=\"initializers/ones\"><code translate=\"no\" dir=\"ltr\">class ones</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href=\"initializers/orthogonal\"><code translate=\"no\" dir=\"ltr\">class orthogonal</code></a>: Initializer that generates an orthogonal matrix.</p> <p><a href=\"initializers/randomnormal\"><code translate=\"no\" dir=\"ltr\">class random_normal</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href=\"initializers/randomuniform\"><code translate=\"no\" dir=\"ltr\">class random_uniform</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href=\"initializers/truncatednormal\"><code translate=\"no\" dir=\"ltr\">class truncated_normal</code></a>: Initializer that generates a truncated normal distribution.</p> <p><a href=\"initializers/variancescaling\"><code translate=\"no\" dir=\"ltr\">class variance_scaling</code></a>: Initializer capable of adapting its scale to the shape of weights tensors.</p> <p><a href=\"initializers/zeros\"><code translate=\"no\" dir=\"ltr\">class zeros</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"initializers/deserialize\"><code translate=\"no\" dir=\"ltr\">deserialize(...)</code></a>: Return an <code translate=\"no\" dir=\"ltr\">Initializer</code> object from its config.</p> <p><a href=\"initializers/get\"><code translate=\"no\" dir=\"ltr\">get(...)</code></a></p> <p><a href=\"initializers/serialize\"><code translate=\"no\" dir=\"ltr\">serialize(...)</code></a></p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/initializers\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/initializers</a>\n  </p>\n</div>\n","linalg":"<h1 class=\"devsite-page-title\">Module: tf.linalg</h1>       <p>Operations for linear algebra.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"linalg/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.linalg.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"linalg/linearoperator\"><code translate=\"no\" dir=\"ltr\">class LinearOperator</code></a>: Base class defining a [batch of] linear operator[s].</p> <p><a href=\"linalg/linearoperatoradjoint\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorAdjoint</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> representing the adjoint of another operator.</p> <p><a href=\"linalg/linearoperatorblockdiag\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorBlockDiag</code></a>: Combines one or more <code translate=\"no\" dir=\"ltr\">LinearOperators</code> in to a Block Diagonal matrix.</p> <p><a href=\"linalg/linearoperatorblocklowertriangular\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorBlockLowerTriangular</code></a>: Combines <code translate=\"no\" dir=\"ltr\">LinearOperators</code> into a blockwise lower-triangular matrix.</p> <p><a href=\"linalg/linearoperatorcirculant\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorCirculant</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a circulant matrix.</p> <p><a href=\"linalg/linearoperatorcirculant2d\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorCirculant2D</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a block circulant matrix.</p> <p><a href=\"linalg/linearoperatorcirculant3d\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorCirculant3D</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a nested block circulant matrix.</p> <p><a href=\"linalg/linearoperatorcomposition\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorComposition</code></a>: Composes one or more <code translate=\"no\" dir=\"ltr\">LinearOperators</code>.</p> <p><a href=\"linalg/linearoperatordiag\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorDiag</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] square diagonal matrix.</p> <p><a href=\"linalg/linearoperatorfullmatrix\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorFullMatrix</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> that wraps a [batch] matrix.</p> <p><a href=\"linalg/linearoperatorhouseholder\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorHouseholder</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] of Householder transformations.</p> <p><a href=\"linalg/linearoperatoridentity\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorIdentity</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] square identity matrix.</p> <p><a href=\"linalg/linearoperatorinversion\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorInversion</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> representing the inverse of another operator.</p> <p><a href=\"linalg/linearoperatorkronecker\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorKronecker</code></a>: Kronecker product between two <code translate=\"no\" dir=\"ltr\">LinearOperators</code>.</p> <p><a href=\"linalg/linearoperatorlowrankupdate\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorLowRankUpdate</code></a>: Perturb a <code translate=\"no\" dir=\"ltr\">LinearOperator</code> with a rank <code translate=\"no\" dir=\"ltr\">K</code> update.</p> <p><a href=\"linalg/linearoperatorlowertriangular\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorLowerTriangular</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] square lower triangular matrix.</p> <p><a href=\"linalg/linearoperatorpermutation\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorPermutation</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] of permutation matrices.</p> <p><a href=\"linalg/linearoperatorscaledidentity\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorScaledIdentity</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a scaled [batch] identity matrix <code translate=\"no\" dir=\"ltr\">A = c I</code>.</p> <p><a href=\"linalg/linearoperatortoeplitz\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorToeplitz</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] of toeplitz matrices.</p> <p><a href=\"linalg/linearoperatortridiag\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorTridiag</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] square tridiagonal matrix.</p> <p><a href=\"linalg/linearoperatorzeros\"><code translate=\"no\" dir=\"ltr\">class LinearOperatorZeros</code></a>: <code translate=\"no\" dir=\"ltr\">LinearOperator</code> acting like a [batch] zero matrix.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"linalg/adjoint\"><code translate=\"no\" dir=\"ltr\">adjoint(...)</code></a>: Transposes the last two dimensions of and conjugates tensor <code translate=\"no\" dir=\"ltr\">matrix</code>.</p> <p><a href=\"linalg/band_part\"><code translate=\"no\" dir=\"ltr\">band_part(...)</code></a>: Copy a tensor setting everything outside a central band in each innermost matrix to zero.</p> <p><a href=\"linalg/banded_triangular_solve\"><code translate=\"no\" dir=\"ltr\">banded_triangular_solve(...)</code></a>: Solve triangular systems of equations with a banded solver.</p> <p><a href=\"linalg/cholesky\"><code translate=\"no\" dir=\"ltr\">cholesky(...)</code></a>: Computes the Cholesky decomposition of one or more square matrices.</p> <p><a href=\"linalg/cholesky_solve\"><code translate=\"no\" dir=\"ltr\">cholesky_solve(...)</code></a>: Solves systems of linear eqns <code translate=\"no\" dir=\"ltr\">A X = RHS</code>, given Cholesky factorizations.</p> <p><a href=\"linalg/cross\"><code translate=\"no\" dir=\"ltr\">cross(...)</code></a>: Compute the pairwise cross product.</p> <p><a href=\"linalg/det\"><code translate=\"no\" dir=\"ltr\">det(...)</code></a>: Computes the determinant of one or more square matrices.</p> <p><a href=\"linalg/diag\"><code translate=\"no\" dir=\"ltr\">diag(...)</code></a>: Returns a batched diagonal tensor with given batched diagonal values.</p> <p><a href=\"linalg/diag_part\"><code translate=\"no\" dir=\"ltr\">diag_part(...)</code></a>: Returns the batched diagonal part of a batched tensor.</p> <p><a href=\"linalg/eig\"><code translate=\"no\" dir=\"ltr\">eig(...)</code></a>: Computes the eigen decomposition of a batch of matrices.</p> <p><a href=\"linalg/eigh\"><code translate=\"no\" dir=\"ltr\">eigh(...)</code></a>: Computes the eigen decomposition of a batch of self-adjoint matrices.</p> <p><a href=\"linalg/eigvals\"><code translate=\"no\" dir=\"ltr\">eigvals(...)</code></a>: Computes the eigenvalues of one or more matrices.</p> <p><a href=\"linalg/eigvalsh\"><code translate=\"no\" dir=\"ltr\">eigvalsh(...)</code></a>: Computes the eigenvalues of one or more self-adjoint matrices.</p> <p><a href=\"einsum\"><code translate=\"no\" dir=\"ltr\">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href=\"linalg/expm\"><code translate=\"no\" dir=\"ltr\">expm(...)</code></a>: Computes the matrix exponential of one or more square matrices.</p> <p><a href=\"eye\"><code translate=\"no\" dir=\"ltr\">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href=\"linalg/global_norm\"><code translate=\"no\" dir=\"ltr\">global_norm(...)</code></a>: Computes the global norm of multiple tensors.</p> <p><a href=\"linalg/inv\"><code translate=\"no\" dir=\"ltr\">inv(...)</code></a>: Computes the inverse of one or more square invertible matrices or their adjoints (conjugate transposes).</p> <p><a href=\"math/l2_normalize\"><code translate=\"no\" dir=\"ltr\">l2_normalize(...)</code></a>: Normalizes along dimension <code translate=\"no\" dir=\"ltr\">axis</code> using an L2 norm.</p> <p><a href=\"linalg/logdet\"><code translate=\"no\" dir=\"ltr\">logdet(...)</code></a>: Computes log of the determinant of a hermitian positive definite matrix.</p> <p><a href=\"linalg/logm\"><code translate=\"no\" dir=\"ltr\">logm(...)</code></a>: Computes the matrix logarithm of one or more square matrices:</p> <p><a href=\"linalg/lstsq\"><code translate=\"no\" dir=\"ltr\">lstsq(...)</code></a>: Solves one or more linear least-squares problems.</p> <p><a href=\"linalg/lu\"><code translate=\"no\" dir=\"ltr\">lu(...)</code></a>: Computes the LU decomposition of one or more square matrices.</p> <p><a href=\"linalg/lu_matrix_inverse\"><code translate=\"no\" dir=\"ltr\">lu_matrix_inverse(...)</code></a>: Computes the inverse given the LU decomposition(s) of one or more matrices.</p> <p><a href=\"linalg/lu_reconstruct\"><code translate=\"no\" dir=\"ltr\">lu_reconstruct(...)</code></a>: The reconstruct one or more matrices from their LU decomposition(s).</p> <p><a href=\"linalg/lu_solve\"><code translate=\"no\" dir=\"ltr\">lu_solve(...)</code></a>: Solves systems of linear eqns <code translate=\"no\" dir=\"ltr\">A X = RHS</code>, given LU factorizations.</p> <p><a href=\"linalg/matmul\"><code translate=\"no\" dir=\"ltr\">matmul(...)</code></a>: Multiplies matrix <code translate=\"no\" dir=\"ltr\">a</code> by matrix <code translate=\"no\" dir=\"ltr\">b</code>, producing <code translate=\"no\" dir=\"ltr\">a</code> * <code translate=\"no\" dir=\"ltr\">b</code>.</p> <p><a href=\"linalg/matrix_rank\"><code translate=\"no\" dir=\"ltr\">matrix_rank(...)</code></a>: Compute the matrix rank of one or more matrices.</p> <p><a href=\"linalg/matrix_transpose\"><code translate=\"no\" dir=\"ltr\">matrix_transpose(...)</code></a>: Transposes last two dimensions of tensor <code translate=\"no\" dir=\"ltr\">a</code>.</p> <p><a href=\"linalg/matvec\"><code translate=\"no\" dir=\"ltr\">matvec(...)</code></a>: Multiplies matrix <code translate=\"no\" dir=\"ltr\">a</code> by vector <code translate=\"no\" dir=\"ltr\">b</code>, producing <code translate=\"no\" dir=\"ltr\">a</code> * <code translate=\"no\" dir=\"ltr\">b</code>.</p> <p><a href=\"norm\"><code translate=\"no\" dir=\"ltr\">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors.</p> <p><a href=\"linalg/normalize\"><code translate=\"no\" dir=\"ltr\">normalize(...)</code></a>: Normalizes <code translate=\"no\" dir=\"ltr\">tensor</code> along dimension <code translate=\"no\" dir=\"ltr\">axis</code> using specified norm.</p> <p><a href=\"linalg/pinv\"><code translate=\"no\" dir=\"ltr\">pinv(...)</code></a>: Compute the Moore-Penrose pseudo-inverse of one or more matrices.</p> <p><a href=\"linalg/qr\"><code translate=\"no\" dir=\"ltr\">qr(...)</code></a>: Computes the QR decompositions of one or more matrices.</p> <p><a href=\"linalg/set_diag\"><code translate=\"no\" dir=\"ltr\">set_diag(...)</code></a>: Returns a batched matrix tensor with new batched diagonal values.</p> <p><a href=\"linalg/slogdet\"><code translate=\"no\" dir=\"ltr\">slogdet(...)</code></a>: Computes the sign and the log of the absolute value of the determinant of</p> <p><a href=\"linalg/solve\"><code translate=\"no\" dir=\"ltr\">solve(...)</code></a>: Solves systems of linear equations.</p> <p><a href=\"linalg/sqrtm\"><code translate=\"no\" dir=\"ltr\">sqrtm(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href=\"linalg/svd\"><code translate=\"no\" dir=\"ltr\">svd(...)</code></a>: Computes the singular value decompositions of one or more matrices.</p> <p><a href=\"linalg/tensor_diag\"><code translate=\"no\" dir=\"ltr\">tensor_diag(...)</code></a>: Returns a diagonal tensor with a given diagonal values.</p> <p><a href=\"linalg/tensor_diag_part\"><code translate=\"no\" dir=\"ltr\">tensor_diag_part(...)</code></a>: Returns the diagonal part of the tensor.</p> <p><a href=\"tensordot\"><code translate=\"no\" dir=\"ltr\">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href=\"linalg/trace\"><code translate=\"no\" dir=\"ltr\">trace(...)</code></a>: Compute the trace of a tensor <code translate=\"no\" dir=\"ltr\">x</code>.</p> <p><a href=\"linalg/triangular_solve\"><code translate=\"no\" dir=\"ltr\">triangular_solve(...)</code></a>: Solve systems of linear equations with upper or lower triangular matrices.</p> <p><a href=\"linalg/tridiagonal_matmul\"><code translate=\"no\" dir=\"ltr\">tridiagonal_matmul(...)</code></a>: Multiplies tridiagonal matrix by matrix.</p> <p><a href=\"linalg/tridiagonal_solve\"><code translate=\"no\" dir=\"ltr\">tridiagonal_solve(...)</code></a>: Solves tridiagonal systems of equations.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/linalg\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/linalg</a>\n  </p>\n</div>\n","lite":"<h1 class=\"devsite-page-title\">Module: tf.lite</h1>       <p>Public API for tf.lite namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"lite/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.lite.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"lite/interpreter\"><code translate=\"no\" dir=\"ltr\">class Interpreter</code></a>: Interpreter interface for TensorFlow Lite Models.</p> <p><a href=\"lite/opsset\"><code translate=\"no\" dir=\"ltr\">class OpsSet</code></a>: Enum class defining the sets of ops available to generate TFLite models.</p> <p><a href=\"lite/optimize\"><code translate=\"no\" dir=\"ltr\">class Optimize</code></a>: Enum defining the optimizations to apply when generating tflite graphs.</p> <p><a href=\"lite/representativedataset\"><code translate=\"no\" dir=\"ltr\">class RepresentativeDataset</code></a>: Representative dataset to evaluate optimizations.</p> <p><a href=\"lite/tfliteconverter\"><code translate=\"no\" dir=\"ltr\">class TFLiteConverter</code></a>: Converts a TensorFlow model into TensorFlow Lite model.</p> <p><a href=\"lite/targetspec\"><code translate=\"no\" dir=\"ltr\">class TargetSpec</code></a>: Specification of target device.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/lite\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/lite</a>\n  </p>\n</div>\n","bitwise":"<h1 class=\"devsite-page-title\">Module: tf.bitwise</h1>       <p>Operations for manipulating the binary representations of integers.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"bitwise/bitwise_and\"><code translate=\"no\" dir=\"ltr\">bitwise_and(...)</code></a>: Elementwise computes the bitwise AND of <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"bitwise/bitwise_or\"><code translate=\"no\" dir=\"ltr\">bitwise_or(...)</code></a>: Elementwise computes the bitwise OR of <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"bitwise/bitwise_xor\"><code translate=\"no\" dir=\"ltr\">bitwise_xor(...)</code></a>: Elementwise computes the bitwise XOR of <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"bitwise/invert\"><code translate=\"no\" dir=\"ltr\">invert(...)</code></a>: Invert (flip) each bit of supported types; for example, type <code translate=\"no\" dir=\"ltr\">uint8</code> value 01010101 becomes 10101010.</p> <p><a href=\"bitwise/left_shift\"><code translate=\"no\" dir=\"ltr\">left_shift(...)</code></a>: Elementwise computes the bitwise left-shift of <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"bitwise/right_shift\"><code translate=\"no\" dir=\"ltr\">right_shift(...)</code></a>: Elementwise computes the bitwise right-shift of <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/bitwise\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/bitwise</a>\n  </p>\n</div>\n","debugging":"<h1 class=\"devsite-page-title\">Module: tf.debugging</h1>       <p>Public API for tf.debugging namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"debugging/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.debugging.experimental namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"debugging/assert\"><code translate=\"no\" dir=\"ltr\">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href=\"debugging/assert_all_finite\"><code translate=\"no\" dir=\"ltr\">assert_all_finite(...)</code></a>: Assert that the tensor does not contain any NaN's or Inf's.</p> <p><a href=\"debugging/assert_equal\"><code translate=\"no\" dir=\"ltr\">assert_equal(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x == y</code> holds element-wise.</p> <p><a href=\"debugging/assert_greater\"><code translate=\"no\" dir=\"ltr\">assert_greater(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &gt; y</code> holds element-wise.</p> <p><a href=\"debugging/assert_greater_equal\"><code translate=\"no\" dir=\"ltr\">assert_greater_equal(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &gt;= y</code> holds element-wise.</p> <p><a href=\"debugging/assert_integer\"><code translate=\"no\" dir=\"ltr\">assert_integer(...)</code></a>: Assert that <code translate=\"no\" dir=\"ltr\">x</code> is of integer dtype.</p> <p><a href=\"debugging/assert_less\"><code translate=\"no\" dir=\"ltr\">assert_less(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &lt; y</code> holds element-wise.</p> <p><a href=\"debugging/assert_less_equal\"><code translate=\"no\" dir=\"ltr\">assert_less_equal(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &lt;= y</code> holds element-wise.</p> <p><a href=\"debugging/assert_near\"><code translate=\"no\" dir=\"ltr\">assert_near(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> are close element-wise.</p> <p><a href=\"debugging/assert_negative\"><code translate=\"no\" dir=\"ltr\">assert_negative(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &lt; 0</code> holds element-wise.</p> <p><a href=\"debugging/assert_non_negative\"><code translate=\"no\" dir=\"ltr\">assert_non_negative(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &gt;= 0</code> holds element-wise.</p> <p><a href=\"debugging/assert_non_positive\"><code translate=\"no\" dir=\"ltr\">assert_non_positive(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &lt;= 0</code> holds element-wise.</p> <p><a href=\"debugging/assert_none_equal\"><code translate=\"no\" dir=\"ltr\">assert_none_equal(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x != y</code> holds for all elements.</p> <p><a href=\"debugging/assert_positive\"><code translate=\"no\" dir=\"ltr\">assert_positive(...)</code></a>: Assert the condition <code translate=\"no\" dir=\"ltr\">x &gt; 0</code> holds element-wise.</p> <p><a href=\"debugging/assert_proper_iterable\"><code translate=\"no\" dir=\"ltr\">assert_proper_iterable(...)</code></a>: Static assert that values is a \"proper\" iterable.</p> <p><a href=\"debugging/assert_rank\"><code translate=\"no\" dir=\"ltr\">assert_rank(...)</code></a>: Assert that <code translate=\"no\" dir=\"ltr\">x</code> has rank equal to <code translate=\"no\" dir=\"ltr\">rank</code>.</p> <p><a href=\"debugging/assert_rank_at_least\"><code translate=\"no\" dir=\"ltr\">assert_rank_at_least(...)</code></a>: Assert that <code translate=\"no\" dir=\"ltr\">x</code> has rank of at least <code translate=\"no\" dir=\"ltr\">rank</code>.</p> <p><a href=\"debugging/assert_rank_in\"><code translate=\"no\" dir=\"ltr\">assert_rank_in(...)</code></a>: Assert that <code translate=\"no\" dir=\"ltr\">x</code> has a rank in <code translate=\"no\" dir=\"ltr\">ranks</code>.</p> <p><a href=\"debugging/assert_same_float_dtype\"><code translate=\"no\" dir=\"ltr\">assert_same_float_dtype(...)</code></a>: Validate and return float type based on <code translate=\"no\" dir=\"ltr\">tensors</code> and <code translate=\"no\" dir=\"ltr\">dtype</code>.</p> <p><a href=\"debugging/assert_scalar\"><code translate=\"no\" dir=\"ltr\">assert_scalar(...)</code></a>: Asserts that the given <code translate=\"no\" dir=\"ltr\">tensor</code> is a scalar.</p> <p><a href=\"debugging/assert_shapes\"><code translate=\"no\" dir=\"ltr\">assert_shapes(...)</code></a>: Assert tensor shapes and dimension size relationships between tensors.</p> <p><a href=\"debugging/assert_type\"><code translate=\"no\" dir=\"ltr\">assert_type(...)</code></a>: Asserts that the given <code translate=\"no\" dir=\"ltr\">Tensor</code> is of the specified type.</p> <p><a href=\"debugging/check_numerics\"><code translate=\"no\" dir=\"ltr\">check_numerics(...)</code></a>: Checks a tensor for NaN and Inf values.</p> <p><a href=\"debugging/disable_check_numerics\"><code translate=\"no\" dir=\"ltr\">disable_check_numerics(...)</code></a>: Disable the eager/graph unified numerics checking mechanism.</p> <p><a href=\"debugging/enable_check_numerics\"><code translate=\"no\" dir=\"ltr\">enable_check_numerics(...)</code></a>: Enable tensor numerics checking in an eager/graph unified fashion.</p> <p><a href=\"debugging/get_log_device_placement\"><code translate=\"no\" dir=\"ltr\">get_log_device_placement(...)</code></a>: Get if device placements are logged.</p> <p><a href=\"debugging/is_numeric_tensor\"><code translate=\"no\" dir=\"ltr\">is_numeric_tensor(...)</code></a>: Returns <code translate=\"no\" dir=\"ltr\">True</code> if the elements of <code translate=\"no\" dir=\"ltr\">tensor</code> are numbers.</p> <p><a href=\"debugging/set_log_device_placement\"><code translate=\"no\" dir=\"ltr\">set_log_device_placement(...)</code></a>: Set if device placements should be logged.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/debugging\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/debugging</a>\n  </p>\n</div>\n","distribute":"<h1 class=\"devsite-page-title\">Module: tf.distribute</h1>       <p>Library for running a computation across multiple devices.</p> <p>The intent of this library is that you can write an algorithm in a stylized way and it will be usable with a variety of different <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> implementations. Each descendant will implement a different strategy for distributing the algorithm across multiple devices/machines. Furthermore, these changes can be hidden inside the specific layers and other library classes that need special treatment to run in a distributed setting, so that most users' model definition code can run unchanged. The <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> API works the same way with eager and graph execution.</p> <p><em>Guides</em></p> <ul> <li><a href=\"https://www.tensorflow.org/guide/distributed_training\">TensorFlow v2.x</a></li> <li><a href=\"https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/distribute_strategy.ipynb\">TensorFlow v1.x</a></li> </ul> <p><em>Tutorials</em></p> <ul> <li>\n<p><a href=\"https://www.tensorflow.org/tutorials/distribute/\">Distributed Training Tutorials</a></p> <p>The tutorials cover how to use <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> to do distributed training with native Keras APIs, custom training loops, and Esitmator APIs. They also cover how to save/load model when using <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a>.</p>\n</li> </ul> <p><em>Glossary</em></p> <ul> <li>\n<em>Data parallelism</em> is where we run multiple copies of the model on different slices of the input data. This is in contrast to <em>model parallelism</em> where we divide up a single copy of a model across multiple devices. Note: we only support data parallelism for now, but hope to add support for model parallelism in the future.</li> <li>A <em>device</em> is a CPU or accelerator (e.g. GPUs, TPUs) on some machine that TensorFlow can run operations on (see e.g. <a href=\"device\"><code translate=\"no\" dir=\"ltr\">tf.device</code></a>). You may have multiple devices on a single machine, or be connected to devices on multiple machines. Devices used to run computations are called <em>worker devices</em>. Devices used to store variables are <em>parameter devices</em>. For some strategies, such as <a href=\"distribute/mirroredstrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.MirroredStrategy</code></a>, the worker and parameter devices will be the same (see mirrored variables below). For others they will be different. For example, <a href=\"distribute/experimental/centralstoragestrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.experimental.CentralStorageStrategy</code></a> puts the variables on a single device (which may be a worker device or may be the CPU), and <a href=\"distribute/experimental/parameterserverstrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.experimental.ParameterServerStrategy</code></a> puts the variables on separate machines called <em>parameter servers</em> (see below).</li> <li>A <em>replica</em> is one copy of the model, running on one slice of the input data. Right now each replica is executed on its own worker device, but once we add support for model parallelism a replica may span multiple worker devices.</li> <li>A <em>host</em> is the CPU device on a machine with worker devices, typically used for running input pipelines.</li> <li>A <em>worker</em> is defined to be the physical machine(s) containing the physical devices (e.g. GPUs, TPUs) on which the replicated computation is executed. A worker may contain one or more replicas, but contains at least one replica. Typically one worker will correspond to one machine, but in the case of very large models with model parallelism, one worker may span multiple machines. We typically run one input pipeline per worker, feeding all the replicas on that worker.</li> <li>\n<em>Synchronous</em>, or more commonly <em>sync</em>, training is where the updates from each replica are aggregated together before updating the model variables. This is in contrast to <em>asynchronous</em>, or <em>async</em> training, where each replica updates the model variables independently. You may also have replicas partitioned into groups which are in sync within each group but async between groups.</li> <li><p><em>Parameter servers</em>: These are machines that hold a single copy of parameters/variables, used by some strategies (right now just <a href=\"distribute/experimental/parameterserverstrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.experimental.ParameterServerStrategy</code></a>). All replicas that want to operate on a variable retrieve it at the beginning of a step and send an update to be applied at the end of the step. These can in priniciple support either sync or async training, but right now we only have support for async training with parameter servers. Compare to <a href=\"distribute/experimental/centralstoragestrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.experimental.CentralStorageStrategy</code></a>, which puts all variables on a single device on the same machine (and does sync training), and <a href=\"distribute/mirroredstrategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.MirroredStrategy</code></a>, which mirrors variables to multiple devices (see below).</p></li> <li>\n<p><em>Replica context</em> vs. <em>Cross-replica context</em> vs <em>Update context</em></p> <p>A <em>replica context</em> applies when you execute the computation function that was called with <code translate=\"no\" dir=\"ltr\">strategy.run</code>. Conceptually, you're in replica context when executing the computation function that is being replicated.</p> <p>An <em>update context</em> is entered in a <a href=\"distribute/strategyextended#update\"><code translate=\"no\" dir=\"ltr\">tf.distribute.StrategyExtended.update</code></a> call.</p> <p>An <em>cross-replica context</em> is entered when you enter a <code translate=\"no\" dir=\"ltr\">strategy.scope</code>. This is useful for calling <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> methods which operate across the replicas (like <code translate=\"no\" dir=\"ltr\">reduce_to()</code>). By default you start in a <em>replica context</em> (the \"default single <em>replica context</em>\") and then some methods can switch you back and forth.</p>\n</li> <li>\n<p><em>Distributed value</em>: Distributed value is represented by the base class <a href=\"distribute/distributedvalues\"><code translate=\"no\" dir=\"ltr\">tf.distribute.DistributedValues</code></a>. <a href=\"distribute/distributedvalues\"><code translate=\"no\" dir=\"ltr\">tf.distribute.DistributedValues</code></a> is useful to represent values on multiple devices, and it contains a map from replica id to values. Two representative kinds of <a href=\"distribute/distributedvalues\"><code translate=\"no\" dir=\"ltr\">tf.distribute.DistributedValues</code></a> are \"PerReplica\" and \"Mirrored\" values.</p> <p>\"PerReplica\" values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by <a href=\"distribute/strategy#experimental_distribute_dataset\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy.experimental_distribute_dataset</code></a> and <a href=\"distribute/strategy#distribute_datasets_from_function\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy.distribute_datasets_from_function</code></a>. They are also the typical result returned by <a href=\"distribute/strategy#run\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy.run</code></a>.</p> <p>\"Mirrored\" values are like \"PerReplica\" values, except we know that the value on all replicas are the same. We can safely read a \"Mirrored\" value in a cross-replica context by using the value on any replica.</p>\n</li> <li><p><em>Unwrapping</em> and <em>merging</em>: Consider calling a function <code translate=\"no\" dir=\"ltr\">fn</code> on multiple replicas, like <code translate=\"no\" dir=\"ltr\">strategy.run(fn, args=[w])</code> with an argument <code translate=\"no\" dir=\"ltr\">w</code> that is a <a href=\"distribute/distributedvalues\"><code translate=\"no\" dir=\"ltr\">tf.distribute.DistributedValues</code></a>. This means <code translate=\"no\" dir=\"ltr\">w</code> will have a map taking replica id <code translate=\"no\" dir=\"ltr\">0</code> to <code translate=\"no\" dir=\"ltr\">w0</code>, replica id <code translate=\"no\" dir=\"ltr\">1</code> to <code translate=\"no\" dir=\"ltr\">w1</code>, etc. <code translate=\"no\" dir=\"ltr\">strategy.run()</code> unwraps <code translate=\"no\" dir=\"ltr\">w</code> before calling <code translate=\"no\" dir=\"ltr\">fn</code>, so it calls <code translate=\"no\" dir=\"ltr\">fn(w0)</code> on device <code translate=\"no\" dir=\"ltr\">d0</code>, <code translate=\"no\" dir=\"ltr\">fn(w1)</code> on device <code translate=\"no\" dir=\"ltr\">d1</code>, etc. It then merges the return values from <code translate=\"no\" dir=\"ltr\">fn()</code>, which leads to one common object if the returned values are the same object from every replica, or a <code translate=\"no\" dir=\"ltr\">DistributedValues</code> object otherwise.</p></li> <li><p><em>Reductions</em> and <em>all-reduce</em>: A <em>reduction</em> is a method of aggregating multiple values into one value, like \"sum\" or \"mean\". If a strategy is doing sync training, we will perform a reduction on the gradients to a parameter from all replicas before applying the update. <em>All-reduce</em> is an algorithm for performing a reduction on values from multiple devices and making the result available on all of those devices.</p></li> <li><p><em>Mirrored variables</em>: These are variables that are created on multiple devices, where we keep the variables in sync by applying the same updates to every copy. Mirrored variables are created with <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable(...synchronization=tf.VariableSynchronization.ON_WRITE...)</code></a>. Normally they are only used in synchronous training.</p></li> <li>\n<p><em>SyncOnRead variables</em></p> <p><em>SyncOnRead variables</em> are created by <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable(...synchronization=tf.VariableSynchronization.ON_READ...)</code></a>, and they are created on multiple devices. In replica context, each component variable on the local replica can perform reads and writes without synchronization with each other. When the <em>SyncOnRead variable</em> is read in cross-replica context, the values from component variables are aggregated and returned.</p> <p><em>SyncOnRead variables</em> bring a lot of custom configuration difficulty to the underlying logic, so we do not encourage users to instantiate and use <em>SyncOnRead variable</em> on their own. We have mainly used <em>SyncOnRead variables</em> for use cases such as batch norm and metrics. For performance reasons, we often don't need to keep these statistics in sync every step and they can be accumulated on each replica independently. The only time we want to sync them is reporting or checkpointing, which typically happens in cross-replica context. <em>SyncOnRead variables</em> are also often used by advanced users who want to control when variable values are aggregated. For example, users sometimes want to maintain gradients independently on each replica for a couple of steps without aggregation.</p>\n</li> <li>\n<p><em>Distribute-aware layers</em></p> <p>Layers are generally called in a replica context, except when defining a Keras functional model. <a href=\"distribute/in_cross_replica_context\"><code translate=\"no\" dir=\"ltr\">tf.distribute.in_cross_replica_context</code></a> will let you determine which case you are in. If in a replica context, the <a href=\"distribute/get_replica_context\"><code translate=\"no\" dir=\"ltr\">tf.distribute.get_replica_context</code></a> function will return the default replica context outside a strategy scope, <code translate=\"no\" dir=\"ltr\">None</code> within a strategy scope, and a <a href=\"distribute/replicacontext\"><code translate=\"no\" dir=\"ltr\">tf.distribute.ReplicaContext</code></a> object inside a strategy scope and within a <a href=\"distribute/strategy#run\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy.run</code></a> function. The <code translate=\"no\" dir=\"ltr\">ReplicaContext</code> object has an <code translate=\"no\" dir=\"ltr\">all_reduce</code> method for aggregating across all replicas.</p>\n</li> </ul> <p>Note that we provide a default version of <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> that is used when no other strategy is in scope, that provides the same API with reasonable default behavior.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"distribute/cluster_resolver\"><code translate=\"no\" dir=\"ltr\">cluster_resolver</code></a> module: Library imports for ClusterResolvers.</p> <p><a href=\"distribute/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.distribute.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"distribute/crossdeviceops\"><code translate=\"no\" dir=\"ltr\">class CrossDeviceOps</code></a>: Base class for cross-device reduction and broadcasting algorithms.</p> <p><a href=\"distribute/distributeddataset\"><code translate=\"no\" dir=\"ltr\">class DistributedDataset</code></a>: Represents a dataset distributed among devices and machines.</p> <p><a href=\"distribute/distributediterator\"><code translate=\"no\" dir=\"ltr\">class DistributedIterator</code></a>: An iterator over <a href=\"distribute/distributeddataset\"><code translate=\"no\" dir=\"ltr\">tf.distribute.DistributedDataset</code></a>.</p> <p><a href=\"distribute/distributedvalues\"><code translate=\"no\" dir=\"ltr\">class DistributedValues</code></a>: Base class for representing distributed values.</p> <p><a href=\"distribute/hierarchicalcopyallreduce\"><code translate=\"no\" dir=\"ltr\">class HierarchicalCopyAllReduce</code></a>: Hierarchical copy all-reduce implementation of CrossDeviceOps.</p> <p><a href=\"distribute/inputcontext\"><code translate=\"no\" dir=\"ltr\">class InputContext</code></a>: A class wrapping information needed by an input function.</p> <p><a href=\"distribute/inputoptions\"><code translate=\"no\" dir=\"ltr\">class InputOptions</code></a>: Run options for <code translate=\"no\" dir=\"ltr\">experimental_distribute_dataset(s_from_function)</code>.</p> <p><a href=\"distribute/inputreplicationmode\"><code translate=\"no\" dir=\"ltr\">class InputReplicationMode</code></a>: Replication mode for input function.</p> <p><a href=\"distribute/mirroredstrategy\"><code translate=\"no\" dir=\"ltr\">class MirroredStrategy</code></a>: Synchronous training across multiple replicas on one machine.</p> <p><a href=\"distribute/multiworkermirroredstrategy\"><code translate=\"no\" dir=\"ltr\">class MultiWorkerMirroredStrategy</code></a>: A distribution strategy for synchronous training on multiple workers.</p> <p><a href=\"distribute/ncclallreduce\"><code translate=\"no\" dir=\"ltr\">class NcclAllReduce</code></a>: NCCL all-reduce implementation of CrossDeviceOps.</p> <p><a href=\"distribute/onedevicestrategy\"><code translate=\"no\" dir=\"ltr\">class OneDeviceStrategy</code></a>: A distribution strategy for running on a single device.</p> <p><a href=\"distribute/reduceop\"><code translate=\"no\" dir=\"ltr\">class ReduceOp</code></a>: Indicates how a set of values should be reduced.</p> <p><a href=\"distribute/reductiontoonedevice\"><code translate=\"no\" dir=\"ltr\">class ReductionToOneDevice</code></a>: A CrossDeviceOps implementation that copies values to one device to reduce.</p> <p><a href=\"distribute/replicacontext\"><code translate=\"no\" dir=\"ltr\">class ReplicaContext</code></a>: A class with a collection of APIs that can be called in a replica context.</p> <p><a href=\"distribute/runoptions\"><code translate=\"no\" dir=\"ltr\">class RunOptions</code></a>: Run options for <code translate=\"no\" dir=\"ltr\">strategy.run</code>.</p> <p><a href=\"distribute/server\"><code translate=\"no\" dir=\"ltr\">class Server</code></a>: An in-process TensorFlow server, for use in distributed training.</p> <p><a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">class Strategy</code></a>: A state &amp; compute distribution policy on a list of devices.</p> <p><a href=\"distribute/strategyextended\"><code translate=\"no\" dir=\"ltr\">class StrategyExtended</code></a>: Additional APIs for algorithms that need to be distribution-aware.</p> <p><a href=\"distribute/tpustrategy\"><code translate=\"no\" dir=\"ltr\">class TPUStrategy</code></a>: Synchronous training on TPUs and TPU Pods.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"distribute/experimental_set_strategy\"><code translate=\"no\" dir=\"ltr\">experimental_set_strategy(...)</code></a>: Set a <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> as current without <code translate=\"no\" dir=\"ltr\">with strategy.scope()</code>.</p> <p><a href=\"distribute/get_replica_context\"><code translate=\"no\" dir=\"ltr\">get_replica_context(...)</code></a>: Returns the current <a href=\"distribute/replicacontext\"><code translate=\"no\" dir=\"ltr\">tf.distribute.ReplicaContext</code></a> or <code translate=\"no\" dir=\"ltr\">None</code>.</p> <p><a href=\"distribute/get_strategy\"><code translate=\"no\" dir=\"ltr\">get_strategy(...)</code></a>: Returns the current <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a> object.</p> <p><a href=\"distribute/has_strategy\"><code translate=\"no\" dir=\"ltr\">has_strategy(...)</code></a>: Return if there is a current non-default <a href=\"distribute/strategy\"><code translate=\"no\" dir=\"ltr\">tf.distribute.Strategy</code></a>.</p> <p><a href=\"distribute/in_cross_replica_context\"><code translate=\"no\" dir=\"ltr\">in_cross_replica_context(...)</code></a>: Returns <code translate=\"no\" dir=\"ltr\">True</code> if in a cross-replica context.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute</a>\n  </p>\n</div>\n","feature_column":"<h1 class=\"devsite-page-title\">Module: tf.feature_column</h1>       <p>Public API for tf.feature_column namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"feature_column/bucketized_column\"><code translate=\"no\" dir=\"ltr\">bucketized_column(...)</code></a>: Represents discretized dense input bucketed by <code translate=\"no\" dir=\"ltr\">boundaries</code>.</p> <p><a href=\"feature_column/categorical_column_with_hash_bucket\"><code translate=\"no\" dir=\"ltr\">categorical_column_with_hash_bucket(...)</code></a>: Represents sparse feature where ids are set by hashing.</p> <p><a href=\"feature_column/categorical_column_with_identity\"><code translate=\"no\" dir=\"ltr\">categorical_column_with_identity(...)</code></a>: A <code translate=\"no\" dir=\"ltr\">CategoricalColumn</code> that returns identity values.</p> <p><a href=\"feature_column/categorical_column_with_vocabulary_file\"><code translate=\"no\" dir=\"ltr\">categorical_column_with_vocabulary_file(...)</code></a>: A <code translate=\"no\" dir=\"ltr\">CategoricalColumn</code> with a vocabulary file.</p> <p><a href=\"feature_column/categorical_column_with_vocabulary_list\"><code translate=\"no\" dir=\"ltr\">categorical_column_with_vocabulary_list(...)</code></a>: A <code translate=\"no\" dir=\"ltr\">CategoricalColumn</code> with in-memory vocabulary.</p> <p><a href=\"feature_column/crossed_column\"><code translate=\"no\" dir=\"ltr\">crossed_column(...)</code></a>: Returns a column for performing crosses of categorical features.</p> <p><a href=\"feature_column/embedding_column\"><code translate=\"no\" dir=\"ltr\">embedding_column(...)</code></a>: <code translate=\"no\" dir=\"ltr\">DenseColumn</code> that converts from sparse, categorical input.</p> <p><a href=\"feature_column/indicator_column\"><code translate=\"no\" dir=\"ltr\">indicator_column(...)</code></a>: Represents multi-hot representation of given categorical column.</p> <p><a href=\"feature_column/make_parse_example_spec\"><code translate=\"no\" dir=\"ltr\">make_parse_example_spec(...)</code></a>: Creates parsing spec dictionary from input feature_columns.</p> <p><a href=\"feature_column/numeric_column\"><code translate=\"no\" dir=\"ltr\">numeric_column(...)</code></a>: Represents real valued or numerical features.</p> <p><a href=\"feature_column/sequence_categorical_column_with_hash_bucket\"><code translate=\"no\" dir=\"ltr\">sequence_categorical_column_with_hash_bucket(...)</code></a>: A sequence of categorical terms where ids are set by hashing.</p> <p><a href=\"feature_column/sequence_categorical_column_with_identity\"><code translate=\"no\" dir=\"ltr\">sequence_categorical_column_with_identity(...)</code></a>: Returns a feature column that represents sequences of integers.</p> <p><a href=\"feature_column/sequence_categorical_column_with_vocabulary_file\"><code translate=\"no\" dir=\"ltr\">sequence_categorical_column_with_vocabulary_file(...)</code></a>: A sequence of categorical terms where ids use a vocabulary file.</p> <p><a href=\"feature_column/sequence_categorical_column_with_vocabulary_list\"><code translate=\"no\" dir=\"ltr\">sequence_categorical_column_with_vocabulary_list(...)</code></a>: A sequence of categorical terms where ids use an in-memory list.</p> <p><a href=\"feature_column/sequence_numeric_column\"><code translate=\"no\" dir=\"ltr\">sequence_numeric_column(...)</code></a>: Returns a feature column that represents sequences of numeric data.</p> <p><a href=\"feature_column/shared_embeddings\"><code translate=\"no\" dir=\"ltr\">shared_embeddings(...)</code></a>: List of dense columns that convert from sparse, categorical input.</p> <p><a href=\"feature_column/weighted_categorical_column\"><code translate=\"no\" dir=\"ltr\">weighted_categorical_column(...)</code></a>: Applies weight values to a <code translate=\"no\" dir=\"ltr\">CategoricalColumn</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/feature_column\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/feature_column</a>\n  </p>\n</div>\n","keras/losses":"<h1 class=\"devsite-page-title\">Module: tf.keras.losses</h1>       <p>Built-in loss functions.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/losses\"><code translate=\"no\" dir=\"ltr\">tf.losses</code></a></p> </section> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"losses/binarycrossentropy\"><code translate=\"no\" dir=\"ltr\">class BinaryCrossentropy</code></a>: Computes the cross-entropy loss between true labels and predicted labels.</p> <p><a href=\"losses/categoricalcrossentropy\"><code translate=\"no\" dir=\"ltr\">class CategoricalCrossentropy</code></a>: Computes the crossentropy loss between the labels and predictions.</p> <p><a href=\"losses/categoricalhinge\"><code translate=\"no\" dir=\"ltr\">class CategoricalHinge</code></a>: Computes the categorical hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/cosinesimilarity\"><code translate=\"no\" dir=\"ltr\">class CosineSimilarity</code></a>: Computes the cosine similarity between labels and predictions.</p> <p><a href=\"losses/hinge\"><code translate=\"no\" dir=\"ltr\">class Hinge</code></a>: Computes the hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/huber\"><code translate=\"no\" dir=\"ltr\">class Huber</code></a>: Computes the Huber loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kldivergence\"><code translate=\"no\" dir=\"ltr\">class KLDivergence</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/logcosh\"><code translate=\"no\" dir=\"ltr\">class LogCosh</code></a>: Computes the logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"losses/loss\"><code translate=\"no\" dir=\"ltr\">class Loss</code></a>: Loss base class.</p> <p><a href=\"losses/meanabsoluteerror\"><code translate=\"no\" dir=\"ltr\">class MeanAbsoluteError</code></a>: Computes the mean of absolute difference between labels and predictions.</p> <p><a href=\"losses/meanabsolutepercentageerror\"><code translate=\"no\" dir=\"ltr\">class MeanAbsolutePercentageError</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/meansquarederror\"><code translate=\"no\" dir=\"ltr\">class MeanSquaredError</code></a>: Computes the mean of squares of errors between labels and predictions.</p> <p><a href=\"losses/meansquaredlogarithmicerror\"><code translate=\"no\" dir=\"ltr\">class MeanSquaredLogarithmicError</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/poisson\"><code translate=\"no\" dir=\"ltr\">class Poisson</code></a>: Computes the Poisson loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/reduction\"><code translate=\"no\" dir=\"ltr\">class Reduction</code></a>: Types of loss reduction.</p> <p><a href=\"losses/sparsecategoricalcrossentropy\"><code translate=\"no\" dir=\"ltr\">class SparseCategoricalCrossentropy</code></a>: Computes the crossentropy loss between the labels and predictions.</p> <p><a href=\"losses/squaredhinge\"><code translate=\"no\" dir=\"ltr\">class SquaredHinge</code></a>: Computes the squared hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">KLD(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">MAE(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">MAPE(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">MSE(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">MSLE(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/binary_crossentropy\"><code translate=\"no\" dir=\"ltr\">binary_crossentropy(...)</code></a>: Computes the binary crossentropy loss.</p> <p><a href=\"losses/categorical_crossentropy\"><code translate=\"no\" dir=\"ltr\">categorical_crossentropy(...)</code></a>: Computes the categorical crossentropy loss.</p> <p><a href=\"losses/categorical_hinge\"><code translate=\"no\" dir=\"ltr\">categorical_hinge(...)</code></a>: Computes the categorical hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/cosine_similarity\"><code translate=\"no\" dir=\"ltr\">cosine_similarity(...)</code></a>: Computes the cosine similarity between labels and predictions.</p> <p><a href=\"losses/deserialize\"><code translate=\"no\" dir=\"ltr\">deserialize(...)</code></a>: Deserializes a serialized loss class/function instance.</p> <p><a href=\"losses/get\"><code translate=\"no\" dir=\"ltr\">get(...)</code></a>: Retrieves a Keras loss as a <code translate=\"no\" dir=\"ltr\">function</code>/<code translate=\"no\" dir=\"ltr\">Loss</code> class instance.</p> <p><a href=\"losses/hinge\"><code translate=\"no\" dir=\"ltr\">hinge(...)</code></a>: Computes the hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/huber\"><code translate=\"no\" dir=\"ltr\">huber(...)</code></a>: Computes Huber loss value.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kl_divergence(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kld(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kullback_leibler_divergence(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/log_cosh\"><code translate=\"no\" dir=\"ltr\">log_cosh(...)</code></a>: Logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"losses/log_cosh\"><code translate=\"no\" dir=\"ltr\">logcosh(...)</code></a>: Logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">mae(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">mape(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">mean_absolute_error(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">mean_absolute_percentage_error(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">mean_squared_error(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">mean_squared_logarithmic_error(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">mse(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">msle(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/poisson\"><code translate=\"no\" dir=\"ltr\">poisson(...)</code></a>: Computes the Poisson loss between y_true and y_pred.</p> <p><a href=\"losses/serialize\"><code translate=\"no\" dir=\"ltr\">serialize(...)</code></a>: Serializes loss function or <code translate=\"no\" dir=\"ltr\">Loss</code> instance.</p> <p><a href=\"losses/sparse_categorical_crossentropy\"><code translate=\"no\" dir=\"ltr\">sparse_categorical_crossentropy(...)</code></a>: Computes the sparse categorical crossentropy loss.</p> <p><a href=\"losses/squared_hinge\"><code translate=\"no\" dir=\"ltr\">squared_hinge(...)</code></a>: Computes the squared hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/losses\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/losses</a>\n  </p>\n</div>\n","lookup":"<h1 class=\"devsite-page-title\">Module: tf.lookup</h1>       <p>Public API for tf.lookup namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"lookup/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.lookup.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"lookup/keyvaluetensorinitializer\"><code translate=\"no\" dir=\"ltr\">class KeyValueTensorInitializer</code></a>: Table initializers given <code translate=\"no\" dir=\"ltr\">keys</code> and <code translate=\"no\" dir=\"ltr\">values</code> tensors.</p> <p><a href=\"lookup/statichashtable\"><code translate=\"no\" dir=\"ltr\">class StaticHashTable</code></a>: A generic hash table that is immutable once initialized.</p> <p><a href=\"lookup/staticvocabularytable\"><code translate=\"no\" dir=\"ltr\">class StaticVocabularyTable</code></a>: String to Id table that assigns out-of-vocabulary keys to hash buckets.</p> <p><a href=\"lookup/textfileindex\"><code translate=\"no\" dir=\"ltr\">class TextFileIndex</code></a>: The key and value content to get from each line.</p> <p><a href=\"lookup/textfileinitializer\"><code translate=\"no\" dir=\"ltr\">class TextFileInitializer</code></a>: Table initializers from a text file.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/lookup\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/lookup</a>\n  </p>\n</div>\n","mixed_precision":"<h1 class=\"devsite-page-title\">Module: tf.mixed_precision</h1>       <p>Public API for tf.mixed_precision namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"mixed_precision/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.mixed_precision.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mixed_precision\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mixed_precision</a>\n  </p>\n</div>\n","nest":"<h1 class=\"devsite-page-title\">Module: tf.nest</h1>       <p>Public API for tf.nest namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"nest/assert_same_structure\"><code translate=\"no\" dir=\"ltr\">assert_same_structure(...)</code></a>: Asserts that two structures are nested in the same way.</p> <p><a href=\"nest/flatten\"><code translate=\"no\" dir=\"ltr\">flatten(...)</code></a>: Returns a flat list from a given nested structure.</p> <p><a href=\"nest/is_nested\"><code translate=\"no\" dir=\"ltr\">is_nested(...)</code></a>: Returns true if its input is a collections.abc.Sequence (except strings).</p> <p><a href=\"nest/map_structure\"><code translate=\"no\" dir=\"ltr\">map_structure(...)</code></a>: Applies <code translate=\"no\" dir=\"ltr\">func</code> to each entry in <code translate=\"no\" dir=\"ltr\">structure</code> and returns a new structure.</p> <p><a href=\"nest/pack_sequence_as\"><code translate=\"no\" dir=\"ltr\">pack_sequence_as(...)</code></a>: Returns a given flattened sequence packed into a given structure.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nest\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nest</a>\n  </p>\n</div>\n","keras/optimizers":"<h1 class=\"devsite-page-title\">Module: tf.keras.optimizers</h1>       <p>Built-in optimizer classes.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\"><code translate=\"no\" dir=\"ltr\">tf.optimizers</code></a></p> </section> <p>For more examples see the base class <a href=\"optimizers/optimizer\"><code translate=\"no\" dir=\"ltr\">tf.keras.optimizers.Optimizer</code></a>.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"optimizers/schedules\"><code translate=\"no\" dir=\"ltr\">schedules</code></a> module: Public API for tf.keras.optimizers.schedules namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"optimizers/adadelta\"><code translate=\"no\" dir=\"ltr\">class Adadelta</code></a>: Optimizer that implements the Adadelta algorithm.</p> <p><a href=\"optimizers/adagrad\"><code translate=\"no\" dir=\"ltr\">class Adagrad</code></a>: Optimizer that implements the Adagrad algorithm.</p> <p><a href=\"optimizers/adam\"><code translate=\"no\" dir=\"ltr\">class Adam</code></a>: Optimizer that implements the Adam algorithm.</p> <p><a href=\"optimizers/adamax\"><code translate=\"no\" dir=\"ltr\">class Adamax</code></a>: Optimizer that implements the Adamax algorithm.</p> <p><a href=\"optimizers/ftrl\"><code translate=\"no\" dir=\"ltr\">class Ftrl</code></a>: Optimizer that implements the FTRL algorithm.</p> <p><a href=\"optimizers/nadam\"><code translate=\"no\" dir=\"ltr\">class Nadam</code></a>: Optimizer that implements the NAdam algorithm.</p> <p><a href=\"optimizers/optimizer\"><code translate=\"no\" dir=\"ltr\">class Optimizer</code></a>: Base class for Keras optimizers.</p> <p><a href=\"optimizers/rmsprop\"><code translate=\"no\" dir=\"ltr\">class RMSprop</code></a>: Optimizer that implements the RMSprop algorithm.</p> <p><a href=\"optimizers/sgd\"><code translate=\"no\" dir=\"ltr\">class SGD</code></a>: Gradient descent (with momentum) optimizer.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"optimizers/deserialize\"><code translate=\"no\" dir=\"ltr\">deserialize(...)</code></a>: Inverse of the <code translate=\"no\" dir=\"ltr\">serialize</code> function.</p> <p><a href=\"optimizers/get\"><code translate=\"no\" dir=\"ltr\">get(...)</code></a>: Retrieves a Keras Optimizer instance.</p> <p><a href=\"optimizers/serialize\"><code translate=\"no\" dir=\"ltr\">serialize(...)</code></a></p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/optimizers\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/optimizers</a>\n  </p>\n</div>\n","nn":"<h1 class=\"devsite-page-title\">Module: tf.nn</h1>       <p>Wrappers for primitive Neural Net (NN) Operations.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"nn/rnncelldevicewrapper\"><code translate=\"no\" dir=\"ltr\">class RNNCellDeviceWrapper</code></a>: Operator that ensures an RNNCell runs on a particular device.</p> <p><a href=\"nn/rnncelldropoutwrapper\"><code translate=\"no\" dir=\"ltr\">class RNNCellDropoutWrapper</code></a>: Operator adding dropout to inputs and outputs of the given cell.</p> <p><a href=\"nn/rnncellresidualwrapper\"><code translate=\"no\" dir=\"ltr\">class RNNCellResidualWrapper</code></a>: RNNCell wrapper that ensures cell inputs are added to the outputs.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"random/all_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">all_candidate_sampler(...)</code></a>: Generate the set of all classes.</p> <p><a href=\"nn/atrous_conv2d\"><code translate=\"no\" dir=\"ltr\">atrous_conv2d(...)</code></a>: Atrous convolution (a.k.a. convolution with holes or dilated convolution).</p> <p><a href=\"nn/atrous_conv2d_transpose\"><code translate=\"no\" dir=\"ltr\">atrous_conv2d_transpose(...)</code></a>: The transpose of <code translate=\"no\" dir=\"ltr\">atrous_conv2d</code>.</p> <p><a href=\"nn/avg_pool\"><code translate=\"no\" dir=\"ltr\">avg_pool(...)</code></a>: Performs the avg pooling on the input.</p> <p><a href=\"nn/avg_pool1d\"><code translate=\"no\" dir=\"ltr\">avg_pool1d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href=\"nn/avg_pool2d\"><code translate=\"no\" dir=\"ltr\">avg_pool2d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href=\"nn/avg_pool3d\"><code translate=\"no\" dir=\"ltr\">avg_pool3d(...)</code></a>: Performs the average pooling on the input.</p> <p><a href=\"nn/batch_norm_with_global_normalization\"><code translate=\"no\" dir=\"ltr\">batch_norm_with_global_normalization(...)</code></a>: Batch normalization.</p> <p><a href=\"nn/batch_normalization\"><code translate=\"no\" dir=\"ltr\">batch_normalization(...)</code></a>: Batch normalization.</p> <p><a href=\"nn/bias_add\"><code translate=\"no\" dir=\"ltr\">bias_add(...)</code></a>: Adds <code translate=\"no\" dir=\"ltr\">bias</code> to <code translate=\"no\" dir=\"ltr\">value</code>.</p> <p><a href=\"nn/collapse_repeated\"><code translate=\"no\" dir=\"ltr\">collapse_repeated(...)</code></a>: Merge repeated labels into single labels.</p> <p><a href=\"nn/compute_accidental_hits\"><code translate=\"no\" dir=\"ltr\">compute_accidental_hits(...)</code></a>: Compute the position ids in <code translate=\"no\" dir=\"ltr\">sampled_candidates</code> matching <code translate=\"no\" dir=\"ltr\">true_classes</code>.</p> <p><a href=\"nn/compute_average_loss\"><code translate=\"no\" dir=\"ltr\">compute_average_loss(...)</code></a>: Scales per-example losses with sample_weights and computes their average.</p> <p><a href=\"nn/conv1d\"><code translate=\"no\" dir=\"ltr\">conv1d(...)</code></a>: Computes a 1-D convolution given 3-D input and filter tensors.</p> <p><a href=\"nn/conv1d_transpose\"><code translate=\"no\" dir=\"ltr\">conv1d_transpose(...)</code></a>: The transpose of <code translate=\"no\" dir=\"ltr\">conv1d</code>.</p> <p><a href=\"nn/conv2d\"><code translate=\"no\" dir=\"ltr\">conv2d(...)</code></a>: Computes a 2-D convolution given <code translate=\"no\" dir=\"ltr\">input</code> and 4-D <code translate=\"no\" dir=\"ltr\">filters</code> tensors.</p> <p><a href=\"nn/conv2d_transpose\"><code translate=\"no\" dir=\"ltr\">conv2d_transpose(...)</code></a>: The transpose of <code translate=\"no\" dir=\"ltr\">conv2d</code>.</p> <p><a href=\"nn/conv3d\"><code translate=\"no\" dir=\"ltr\">conv3d(...)</code></a>: Computes a 3-D convolution given 5-D <code translate=\"no\" dir=\"ltr\">input</code> and <code translate=\"no\" dir=\"ltr\">filters</code> tensors.</p> <p><a href=\"nn/conv3d_transpose\"><code translate=\"no\" dir=\"ltr\">conv3d_transpose(...)</code></a>: The transpose of <code translate=\"no\" dir=\"ltr\">conv3d</code>.</p> <p><a href=\"nn/conv_transpose\"><code translate=\"no\" dir=\"ltr\">conv_transpose(...)</code></a>: The transpose of <code translate=\"no\" dir=\"ltr\">convolution</code>.</p> <p><a href=\"nn/convolution\"><code translate=\"no\" dir=\"ltr\">convolution(...)</code></a>: Computes sums of N-D convolutions (actually cross-correlation).</p> <p><a href=\"nn/crelu\"><code translate=\"no\" dir=\"ltr\">crelu(...)</code></a>: Computes Concatenated ReLU.</p> <p><a href=\"nn/ctc_beam_search_decoder\"><code translate=\"no\" dir=\"ltr\">ctc_beam_search_decoder(...)</code></a>: Performs beam search decoding on the logits given in input.</p> <p><a href=\"nn/ctc_greedy_decoder\"><code translate=\"no\" dir=\"ltr\">ctc_greedy_decoder(...)</code></a>: Performs greedy decoding on the logits given in input (best path).</p> <p><a href=\"nn/ctc_loss\"><code translate=\"no\" dir=\"ltr\">ctc_loss(...)</code></a>: Computes CTC (Connectionist Temporal Classification) loss.</p> <p><a href=\"nn/ctc_unique_labels\"><code translate=\"no\" dir=\"ltr\">ctc_unique_labels(...)</code></a>: Get unique labels and indices for batched labels for <a href=\"nn/ctc_loss\"><code translate=\"no\" dir=\"ltr\">tf.nn.ctc_loss</code></a>.</p> <p><a href=\"nn/depth_to_space\"><code translate=\"no\" dir=\"ltr\">depth_to_space(...)</code></a>: DepthToSpace for tensors of type T.</p> <p><a href=\"nn/depthwise_conv2d\"><code translate=\"no\" dir=\"ltr\">depthwise_conv2d(...)</code></a>: Depthwise 2-D convolution.</p> <p><a href=\"nn/depthwise_conv2d_backprop_filter\"><code translate=\"no\" dir=\"ltr\">depthwise_conv2d_backprop_filter(...)</code></a>: Computes the gradients of depthwise convolution with respect to the filter.</p> <p><a href=\"nn/depthwise_conv2d_backprop_input\"><code translate=\"no\" dir=\"ltr\">depthwise_conv2d_backprop_input(...)</code></a>: Computes the gradients of depthwise convolution with respect to the input.</p> <p><a href=\"nn/dilation2d\"><code translate=\"no\" dir=\"ltr\">dilation2d(...)</code></a>: Computes the grayscale dilation of 4-D <code translate=\"no\" dir=\"ltr\">input</code> and 3-D <code translate=\"no\" dir=\"ltr\">filters</code> tensors.</p> <p><a href=\"nn/dropout\"><code translate=\"no\" dir=\"ltr\">dropout(...)</code></a>: Computes dropout: randomly sets elements to zero to prevent overfitting.</p> <p><a href=\"nn/elu\"><code translate=\"no\" dir=\"ltr\">elu(...)</code></a>: Computes exponential linear: <code translate=\"no\" dir=\"ltr\">exp(features) - 1</code> if &lt; 0, <code translate=\"no\" dir=\"ltr\">features</code> otherwise.</p> <p><a href=\"nn/embedding_lookup\"><code translate=\"no\" dir=\"ltr\">embedding_lookup(...)</code></a>: Looks up embeddings for the given <code translate=\"no\" dir=\"ltr\">ids</code> from a list of tensors.</p> <p><a href=\"nn/embedding_lookup_sparse\"><code translate=\"no\" dir=\"ltr\">embedding_lookup_sparse(...)</code></a>: Looks up embeddings for the given ids and weights from a list of tensors.</p> <p><a href=\"nn/erosion2d\"><code translate=\"no\" dir=\"ltr\">erosion2d(...)</code></a>: Computes the grayscale erosion of 4-D <code translate=\"no\" dir=\"ltr\">value</code> and 3-D <code translate=\"no\" dir=\"ltr\">filters</code> tensors.</p> <p><a href=\"random/fixed_unigram_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">fixed_unigram_candidate_sampler(...)</code></a>: Samples a set of classes using the provided (fixed) base distribution.</p> <p><a href=\"nn/fractional_avg_pool\"><code translate=\"no\" dir=\"ltr\">fractional_avg_pool(...)</code></a>: Performs fractional average pooling on the input.</p> <p><a href=\"nn/fractional_max_pool\"><code translate=\"no\" dir=\"ltr\">fractional_max_pool(...)</code></a>: Performs fractional max pooling on the input.</p> <p><a href=\"nn/gelu\"><code translate=\"no\" dir=\"ltr\">gelu(...)</code></a>: Compute the Gaussian Error Linear Unit (GELU) activation function.</p> <p><a href=\"math/in_top_k\"><code translate=\"no\" dir=\"ltr\">in_top_k(...)</code></a>: Says whether the targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p> <p><a href=\"nn/isotonic_regression\"><code translate=\"no\" dir=\"ltr\">isotonic_regression(...)</code></a>: Solves isotonic regression problems along the given axis.</p> <p><a href=\"nn/l2_loss\"><code translate=\"no\" dir=\"ltr\">l2_loss(...)</code></a>: L2 Loss.</p> <p><a href=\"math/l2_normalize\"><code translate=\"no\" dir=\"ltr\">l2_normalize(...)</code></a>: Normalizes along dimension <code translate=\"no\" dir=\"ltr\">axis</code> using an L2 norm.</p> <p><a href=\"nn/leaky_relu\"><code translate=\"no\" dir=\"ltr\">leaky_relu(...)</code></a>: Compute the Leaky ReLU activation function.</p> <p><a href=\"random/learned_unigram_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">learned_unigram_candidate_sampler(...)</code></a>: Samples a set of classes from a distribution learned during training.</p> <p><a href=\"nn/local_response_normalization\"><code translate=\"no\" dir=\"ltr\">local_response_normalization(...)</code></a>: Local Response Normalization.</p> <p><a href=\"nn/log_poisson_loss\"><code translate=\"no\" dir=\"ltr\">log_poisson_loss(...)</code></a>: Computes log Poisson loss given <code translate=\"no\" dir=\"ltr\">log_input</code>.</p> <p><a href=\"nn/log_softmax\"><code translate=\"no\" dir=\"ltr\">log_softmax(...)</code></a>: Computes log softmax activations.</p> <p><a href=\"nn/local_response_normalization\"><code translate=\"no\" dir=\"ltr\">lrn(...)</code></a>: Local Response Normalization.</p> <p><a href=\"nn/max_pool\"><code translate=\"no\" dir=\"ltr\">max_pool(...)</code></a>: Performs the max pooling on the input.</p> <p><a href=\"nn/max_pool1d\"><code translate=\"no\" dir=\"ltr\">max_pool1d(...)</code></a>: Performs the max pooling on the input.</p> <p><a href=\"nn/max_pool2d\"><code translate=\"no\" dir=\"ltr\">max_pool2d(...)</code></a>: Performs the max pooling on the input.</p> <p><a href=\"nn/max_pool3d\"><code translate=\"no\" dir=\"ltr\">max_pool3d(...)</code></a>: Performs the max pooling on the input.</p> <p><a href=\"nn/max_pool_with_argmax\"><code translate=\"no\" dir=\"ltr\">max_pool_with_argmax(...)</code></a>: Performs max pooling on the input and outputs both max values and indices.</p> <p><a href=\"nn/moments\"><code translate=\"no\" dir=\"ltr\">moments(...)</code></a>: Calculates the mean and variance of <code translate=\"no\" dir=\"ltr\">x</code>.</p> <p><a href=\"nn/nce_loss\"><code translate=\"no\" dir=\"ltr\">nce_loss(...)</code></a>: Computes and returns the noise-contrastive estimation training loss.</p> <p><a href=\"nn/normalize_moments\"><code translate=\"no\" dir=\"ltr\">normalize_moments(...)</code></a>: Calculate the mean and variance of based on the sufficient statistics.</p> <p><a href=\"nn/pool\"><code translate=\"no\" dir=\"ltr\">pool(...)</code></a>: Performs an N-D pooling operation.</p> <p><a href=\"nn/relu\"><code translate=\"no\" dir=\"ltr\">relu(...)</code></a>: Computes rectified linear: <code translate=\"no\" dir=\"ltr\">max(features, 0)</code>.</p> <p><a href=\"nn/relu6\"><code translate=\"no\" dir=\"ltr\">relu6(...)</code></a>: Computes Rectified Linear 6: <code translate=\"no\" dir=\"ltr\">min(max(features, 0), 6)</code>.</p> <p><a href=\"nn/safe_embedding_lookup_sparse\"><code translate=\"no\" dir=\"ltr\">safe_embedding_lookup_sparse(...)</code></a>: Lookup embedding results, accounting for invalid IDs and empty features.</p> <p><a href=\"nn/sampled_softmax_loss\"><code translate=\"no\" dir=\"ltr\">sampled_softmax_loss(...)</code></a>: Computes and returns the sampled softmax training loss.</p> <p><a href=\"nn/scale_regularization_loss\"><code translate=\"no\" dir=\"ltr\">scale_regularization_loss(...)</code></a>: Scales the sum of the given regularization losses by number of replicas.</p> <p><a href=\"nn/selu\"><code translate=\"no\" dir=\"ltr\">selu(...)</code></a>: Computes scaled exponential linear: <code translate=\"no\" dir=\"ltr\">scale * alpha * (exp(features) - 1)</code></p> <p><a href=\"nn/separable_conv2d\"><code translate=\"no\" dir=\"ltr\">separable_conv2d(...)</code></a>: 2-D convolution with separable filters.</p> <p><a href=\"math/sigmoid\"><code translate=\"no\" dir=\"ltr\">sigmoid(...)</code></a>: Computes sigmoid of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"nn/sigmoid_cross_entropy_with_logits\"><code translate=\"no\" dir=\"ltr\">sigmoid_cross_entropy_with_logits(...)</code></a>: Computes sigmoid cross entropy given <code translate=\"no\" dir=\"ltr\">logits</code>.</p> <p><a href=\"nn/silu\"><code translate=\"no\" dir=\"ltr\">silu(...)</code></a>: Computes the SiLU or Swish activation function: <code translate=\"no\" dir=\"ltr\">x * sigmoid(x)</code>.</p> <p><a href=\"nn/softmax\"><code translate=\"no\" dir=\"ltr\">softmax(...)</code></a>: Computes softmax activations.</p> <p><a href=\"nn/softmax_cross_entropy_with_logits\"><code translate=\"no\" dir=\"ltr\">softmax_cross_entropy_with_logits(...)</code></a>: Computes softmax cross entropy between <code translate=\"no\" dir=\"ltr\">logits</code> and <code translate=\"no\" dir=\"ltr\">labels</code>.</p> <p><a href=\"math/softplus\"><code translate=\"no\" dir=\"ltr\">softplus(...)</code></a>: Computes softplus: <code translate=\"no\" dir=\"ltr\">log(exp(features) + 1)</code>.</p> <p><a href=\"nn/softsign\"><code translate=\"no\" dir=\"ltr\">softsign(...)</code></a>: Computes softsign: <code translate=\"no\" dir=\"ltr\">features / (abs(features) + 1)</code>.</p> <p><a href=\"space_to_batch\"><code translate=\"no\" dir=\"ltr\">space_to_batch(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href=\"nn/space_to_depth\"><code translate=\"no\" dir=\"ltr\">space_to_depth(...)</code></a>: SpaceToDepth for tensors of type T.</p> <p><a href=\"nn/sparse_softmax_cross_entropy_with_logits\"><code translate=\"no\" dir=\"ltr\">sparse_softmax_cross_entropy_with_logits(...)</code></a>: Computes sparse softmax cross entropy between <code translate=\"no\" dir=\"ltr\">logits</code> and <code translate=\"no\" dir=\"ltr\">labels</code>.</p> <p><a href=\"nn/sufficient_statistics\"><code translate=\"no\" dir=\"ltr\">sufficient_statistics(...)</code></a>: Calculate the sufficient statistics for the mean and variance of <code translate=\"no\" dir=\"ltr\">x</code>.</p> <p><a href=\"nn/silu\"><code translate=\"no\" dir=\"ltr\">swish(...)</code></a>: Computes the SiLU or Swish activation function: <code translate=\"no\" dir=\"ltr\">x * sigmoid(x)</code>.</p> <p><a href=\"math/tanh\"><code translate=\"no\" dir=\"ltr\">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/top_k\"><code translate=\"no\" dir=\"ltr\">top_k(...)</code></a>: Finds values and indices of the <code translate=\"no\" dir=\"ltr\">k</code> largest entries for the last dimension.</p> <p><a href=\"nn/weighted_cross_entropy_with_logits\"><code translate=\"no\" dir=\"ltr\">weighted_cross_entropy_with_logits(...)</code></a>: Computes a weighted cross entropy.</p> <p><a href=\"nn/weighted_moments\"><code translate=\"no\" dir=\"ltr\">weighted_moments(...)</code></a>: Returns the frequency-weighted mean and variance of <code translate=\"no\" dir=\"ltr\">x</code>.</p> <p><a href=\"nn/with_space_to_batch\"><code translate=\"no\" dir=\"ltr\">with_space_to_batch(...)</code></a>: Performs <code translate=\"no\" dir=\"ltr\">op</code> on the space-to-batch representation of <code translate=\"no\" dir=\"ltr\">input</code>.</p> <p><a href=\"math/zero_fraction\"><code translate=\"no\" dir=\"ltr\">zero_fraction(...)</code></a>: Returns the fraction of zeros in <code translate=\"no\" dir=\"ltr\">value</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn</a>\n  </p>\n</div>\n","math":"<h1 class=\"devsite-page-title\">Module: tf.math</h1>   <p><devsite-mathjax config=\"TeX-AMS-MML_SVG\"></devsite-mathjax> </p>    <p>Math Operations.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Functions taking <code translate=\"no\" dir=\"ltr\">Tensor</code> arguments can also take anything accepted by <a href=\"convert_to_tensor\"><code translate=\"no\" dir=\"ltr\">tf.convert_to_tensor</code></a>.</span>\n</blockquote>\n<blockquote class=\"note\">\n<strong>Note:</strong><span> Elementwise binary operations in TensorFlow follow <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">numpy-style broadcasting</a>.</span>\n</blockquote> <p>TensorFlow provides a variety of math functions including:</p> <ul> <li>Basic arithmetic operators and trigonometric functions.</li> <li>Special math functions (like: <a href=\"math/igamma\"><code translate=\"no\" dir=\"ltr\">tf.math.igamma</code></a> and <a href=\"math/zeta\"><code translate=\"no\" dir=\"ltr\">tf.math.zeta</code></a>)</li> <li>Complex number functions (like: <a href=\"math/imag\"><code translate=\"no\" dir=\"ltr\">tf.math.imag</code></a> and <a href=\"math/angle\"><code translate=\"no\" dir=\"ltr\">tf.math.angle</code></a>)</li> <li>Reductions and scans (like: <a href=\"math/reduce_mean\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_mean</code></a> and <a href=\"math/cumsum\"><code translate=\"no\" dir=\"ltr\">tf.math.cumsum</code></a>)</li> <li>Segment functions (like: <a href=\"math/segment_sum\"><code translate=\"no\" dir=\"ltr\">tf.math.segment_sum</code></a>)</li> </ul> <p>See: <a href=\"linalg\"><code translate=\"no\" dir=\"ltr\">tf.linalg</code></a> for matrix and tensor functions.</p>  <h2 id=\"about_segmentation\" data-text=\"About Segmentation\">About Segmentation</h2> <p>TensorFlow provides several operations that you can use to perform common math computations on tensor segments. Here a segmentation is a partitioning of a tensor along the first dimension, i.e. it defines a mapping from the first dimension onto <code translate=\"no\" dir=\"ltr\">segment_ids</code>. The <code translate=\"no\" dir=\"ltr\">segment_ids</code> tensor should be the size of the first dimension, <code translate=\"no\" dir=\"ltr\">d0</code>, with consecutive IDs in the range <code translate=\"no\" dir=\"ltr\">0</code> to <code translate=\"no\" dir=\"ltr\">k</code>, where <code translate=\"no\" dir=\"ltr\">k&lt;d0</code>. In particular, a segmentation of a matrix tensor is a mapping of rows to segments.</p> <h4 id=\"for_example\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\ntf.math.segment_sum(c, tf.constant([0, 0, 1]))\n#  ==&gt;  [[0 0 0 0]\n#        [5 6 7 8]]\n</pre> <p>The standard <code translate=\"no\" dir=\"ltr\">segment_*</code> functions assert that the segment indices are sorted. If you have unsorted indices use the equivalent <code translate=\"no\" dir=\"ltr\">unsorted_segment_</code> function. These functions take an additional argument <code translate=\"no\" dir=\"ltr\">num_segments</code> so that the output tensor can be efficiently allocated.</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])\ntf.math.unsorted_segment_sum(c, tf.constant([0, 1, 0]), num_segments=2)\n# ==&gt; [[ 6,  8, 10, 12],\n#       [-1, -2, -3, -4]]\n</pre> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"math/special\"><code translate=\"no\" dir=\"ltr\">special</code></a> module: Public API for tf.math.special namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"math/abs\"><code translate=\"no\" dir=\"ltr\">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href=\"math/accumulate_n\"><code translate=\"no\" dir=\"ltr\">accumulate_n(...)</code></a>: Returns the element-wise sum of a list of tensors.</p> <p><a href=\"math/acos\"><code translate=\"no\" dir=\"ltr\">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href=\"math/acosh\"><code translate=\"no\" dir=\"ltr\">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href=\"math/add_n\"><code translate=\"no\" dir=\"ltr\">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href=\"math/angle\"><code translate=\"no\" dir=\"ltr\">angle(...)</code></a>: Returns the element-wise argument of a complex (or real) tensor.</p> <p><a href=\"math/argmax\"><code translate=\"no\" dir=\"ltr\">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor.</p> <p><a href=\"math/argmin\"><code translate=\"no\" dir=\"ltr\">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor.</p> <p><a href=\"math/asin\"><code translate=\"no\" dir=\"ltr\">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href=\"math/asinh\"><code translate=\"no\" dir=\"ltr\">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href=\"math/atan\"><code translate=\"no\" dir=\"ltr\">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href=\"math/atan2\"><code translate=\"no\" dir=\"ltr\">atan2(...)</code></a>: Computes arctangent of <code translate=\"no\" dir=\"ltr\">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href=\"math/atanh\"><code translate=\"no\" dir=\"ltr\">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href=\"math/bessel_i0\"><code translate=\"no\" dir=\"ltr\">bessel_i0(...)</code></a>: Computes the Bessel i0 function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/bessel_i0e\"><code translate=\"no\" dir=\"ltr\">bessel_i0e(...)</code></a>: Computes the Bessel i0e function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/bessel_i1\"><code translate=\"no\" dir=\"ltr\">bessel_i1(...)</code></a>: Computes the Bessel i1 function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/bessel_i1e\"><code translate=\"no\" dir=\"ltr\">bessel_i1e(...)</code></a>: Computes the Bessel i1e function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/betainc\"><code translate=\"no\" dir=\"ltr\">betainc(...)</code></a>: Compute the regularized incomplete beta integral \\(I_x(a, b)\\).</p> <p><a href=\"math/bincount\"><code translate=\"no\" dir=\"ltr\">bincount(...)</code></a>: Counts the number of occurrences of each value in an integer array.</p> <p><a href=\"math/ceil\"><code translate=\"no\" dir=\"ltr\">ceil(...)</code></a>: Return the ceiling of the input, element-wise.</p> <p><a href=\"math/confusion_matrix\"><code translate=\"no\" dir=\"ltr\">confusion_matrix(...)</code></a>: Computes the confusion matrix from predictions and labels.</p> <p><a href=\"math/conj\"><code translate=\"no\" dir=\"ltr\">conj(...)</code></a>: Returns the complex conjugate of a complex number.</p> <p><a href=\"math/cos\"><code translate=\"no\" dir=\"ltr\">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href=\"math/cosh\"><code translate=\"no\" dir=\"ltr\">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href=\"math/count_nonzero\"><code translate=\"no\" dir=\"ltr\">count_nonzero(...)</code></a>: Computes number of nonzero elements across dimensions of a tensor.</p> <p><a href=\"math/cumprod\"><code translate=\"no\" dir=\"ltr\">cumprod(...)</code></a>: Compute the cumulative product of the tensor <code translate=\"no\" dir=\"ltr\">x</code> along <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"math/cumsum\"><code translate=\"no\" dir=\"ltr\">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate=\"no\" dir=\"ltr\">x</code> along <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"math/cumulative_logsumexp\"><code translate=\"no\" dir=\"ltr\">cumulative_logsumexp(...)</code></a>: Compute the cumulative log-sum-exp of the tensor <code translate=\"no\" dir=\"ltr\">x</code> along <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"math/digamma\"><code translate=\"no\" dir=\"ltr\">digamma(...)</code></a>: Computes Psi, the derivative of Lgamma (the log of the absolute value of</p> <p><a href=\"math/divide\"><code translate=\"no\" dir=\"ltr\">divide(...)</code></a>: Computes Python style division of <code translate=\"no\" dir=\"ltr\">x</code> by <code translate=\"no\" dir=\"ltr\">y</code>.</p> <p><a href=\"math/divide_no_nan\"><code translate=\"no\" dir=\"ltr\">divide_no_nan(...)</code></a>: Computes a safe divide which returns 0 if the y is zero.</p> <p><a href=\"math/equal\"><code translate=\"no\" dir=\"ltr\">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href=\"math/erf\"><code translate=\"no\" dir=\"ltr\">erf(...)</code></a>: Computes the Gauss error function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/erfc\"><code translate=\"no\" dir=\"ltr\">erfc(...)</code></a>: Computes the complementary error function of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/erfcinv\"><code translate=\"no\" dir=\"ltr\">erfcinv(...)</code></a>: Computes the inverse of complementary error function.</p> <p><a href=\"math/erfinv\"><code translate=\"no\" dir=\"ltr\">erfinv(...)</code></a>: Compute inverse error function.</p> <p><a href=\"math/exp\"><code translate=\"no\" dir=\"ltr\">exp(...)</code></a>: Computes exponential of x element-wise. \\(y = e^x\\).</p> <p><a href=\"math/expm1\"><code translate=\"no\" dir=\"ltr\">expm1(...)</code></a>: Computes <code translate=\"no\" dir=\"ltr\">exp(x) - 1</code> element-wise.</p> <p><a href=\"math/floor\"><code translate=\"no\" dir=\"ltr\">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href=\"math/floordiv\"><code translate=\"no\" dir=\"ltr\">floordiv(...)</code></a>: Divides <code translate=\"no\" dir=\"ltr\">x / y</code> elementwise, rounding toward the most negative integer.</p> <p><a href=\"math/floormod\"><code translate=\"no\" dir=\"ltr\">floormod(...)</code></a>: Returns element-wise remainder of division. When <code translate=\"no\" dir=\"ltr\">x &lt; 0</code> xor <code translate=\"no\" dir=\"ltr\">y &lt; 0</code> is</p> <p><a href=\"math/greater\"><code translate=\"no\" dir=\"ltr\">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href=\"math/greater_equal\"><code translate=\"no\" dir=\"ltr\">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href=\"math/igamma\"><code translate=\"no\" dir=\"ltr\">igamma(...)</code></a>: Compute the lower regularized incomplete Gamma function <code translate=\"no\" dir=\"ltr\">P(a, x)</code>.</p> <p><a href=\"math/igammac\"><code translate=\"no\" dir=\"ltr\">igammac(...)</code></a>: Compute the upper regularized incomplete Gamma function <code translate=\"no\" dir=\"ltr\">Q(a, x)</code>.</p> <p><a href=\"math/imag\"><code translate=\"no\" dir=\"ltr\">imag(...)</code></a>: Returns the imaginary part of a complex (or real) tensor.</p> <p><a href=\"math/in_top_k\"><code translate=\"no\" dir=\"ltr\">in_top_k(...)</code></a>: Says whether the targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p> <p><a href=\"math/invert_permutation\"><code translate=\"no\" dir=\"ltr\">invert_permutation(...)</code></a>: Computes the inverse permutation of a tensor.</p> <p><a href=\"math/is_finite\"><code translate=\"no\" dir=\"ltr\">is_finite(...)</code></a>: Returns which elements of x are finite.</p> <p><a href=\"math/is_inf\"><code translate=\"no\" dir=\"ltr\">is_inf(...)</code></a>: Returns which elements of x are Inf.</p> <p><a href=\"math/is_nan\"><code translate=\"no\" dir=\"ltr\">is_nan(...)</code></a>: Returns which elements of x are NaN.</p> <p><a href=\"math/is_non_decreasing\"><code translate=\"no\" dir=\"ltr\">is_non_decreasing(...)</code></a>: Returns <code translate=\"no\" dir=\"ltr\">True</code> if <code translate=\"no\" dir=\"ltr\">x</code> is non-decreasing.</p> <p><a href=\"math/is_strictly_increasing\"><code translate=\"no\" dir=\"ltr\">is_strictly_increasing(...)</code></a>: Returns <code translate=\"no\" dir=\"ltr\">True</code> if <code translate=\"no\" dir=\"ltr\">x</code> is strictly increasing.</p> <p><a href=\"math/l2_normalize\"><code translate=\"no\" dir=\"ltr\">l2_normalize(...)</code></a>: Normalizes along dimension <code translate=\"no\" dir=\"ltr\">axis</code> using an L2 norm.</p> <p><a href=\"math/lbeta\"><code translate=\"no\" dir=\"ltr\">lbeta(...)</code></a>: Computes \\(ln(|Beta(x)|)\\), reducing along the last dimension.</p> <p><a href=\"math/less\"><code translate=\"no\" dir=\"ltr\">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href=\"math/less_equal\"><code translate=\"no\" dir=\"ltr\">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href=\"math/lgamma\"><code translate=\"no\" dir=\"ltr\">lgamma(...)</code></a>: Computes the log of the absolute value of <code translate=\"no\" dir=\"ltr\">Gamma(x)</code> element-wise.</p> <p><a href=\"math/log\"><code translate=\"no\" dir=\"ltr\">log(...)</code></a>: Computes natural logarithm of x element-wise.</p> <p><a href=\"math/log1p\"><code translate=\"no\" dir=\"ltr\">log1p(...)</code></a>: Computes natural logarithm of (1 + x) element-wise.</p> <p><a href=\"math/log_sigmoid\"><code translate=\"no\" dir=\"ltr\">log_sigmoid(...)</code></a>: Computes log sigmoid of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"nn/log_softmax\"><code translate=\"no\" dir=\"ltr\">log_softmax(...)</code></a>: Computes log softmax activations.</p> <p><a href=\"math/logical_and\"><code translate=\"no\" dir=\"ltr\">logical_and(...)</code></a>: Logical AND function.</p> <p><a href=\"math/logical_not\"><code translate=\"no\" dir=\"ltr\">logical_not(...)</code></a>: Returns the truth value of <code translate=\"no\" dir=\"ltr\">NOT x</code> element-wise.</p> <p><a href=\"math/logical_or\"><code translate=\"no\" dir=\"ltr\">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href=\"math/logical_xor\"><code translate=\"no\" dir=\"ltr\">logical_xor(...)</code></a>: Logical XOR function.</p> <p><a href=\"math/maximum\"><code translate=\"no\" dir=\"ltr\">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href=\"math/minimum\"><code translate=\"no\" dir=\"ltr\">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href=\"math/floormod\"><code translate=\"no\" dir=\"ltr\">mod(...)</code></a>: Returns element-wise remainder of division. When <code translate=\"no\" dir=\"ltr\">x &lt; 0</code> xor <code translate=\"no\" dir=\"ltr\">y &lt; 0</code> is</p> <p><a href=\"math/multiply\"><code translate=\"no\" dir=\"ltr\">multiply(...)</code></a>: Returns an element-wise x * y.</p> <p><a href=\"math/multiply_no_nan\"><code translate=\"no\" dir=\"ltr\">multiply_no_nan(...)</code></a>: Computes the product of x and y and returns 0 if the y is zero, even if x is NaN or infinite.</p> <p><a href=\"math/ndtri\"><code translate=\"no\" dir=\"ltr\">ndtri(...)</code></a>: Compute quantile of Standard Normal.</p> <p><a href=\"math/negative\"><code translate=\"no\" dir=\"ltr\">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href=\"math/nextafter\"><code translate=\"no\" dir=\"ltr\">nextafter(...)</code></a>: Returns the next representable value of <code translate=\"no\" dir=\"ltr\">x1</code> in the direction of <code translate=\"no\" dir=\"ltr\">x2</code>, element-wise.</p> <p><a href=\"math/not_equal\"><code translate=\"no\" dir=\"ltr\">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href=\"math/polygamma\"><code translate=\"no\" dir=\"ltr\">polygamma(...)</code></a>: Compute the polygamma function \\(\\psi^{(n)}(x)\\).</p> <p><a href=\"math/polyval\"><code translate=\"no\" dir=\"ltr\">polyval(...)</code></a>: Computes the elementwise value of a polynomial.</p> <p><a href=\"math/pow\"><code translate=\"no\" dir=\"ltr\">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href=\"math/real\"><code translate=\"no\" dir=\"ltr\">real(...)</code></a>: Returns the real part of a complex (or real) tensor.</p> <p><a href=\"math/reciprocal\"><code translate=\"no\" dir=\"ltr\">reciprocal(...)</code></a>: Computes the reciprocal of x element-wise.</p> <p><a href=\"math/reciprocal_no_nan\"><code translate=\"no\" dir=\"ltr\">reciprocal_no_nan(...)</code></a>: Performs a safe reciprocal operation, element wise.</p> <p><a href=\"math/reduce_all\"><code translate=\"no\" dir=\"ltr\">reduce_all(...)</code></a>: Computes the \"logical and\" of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_any\"><code translate=\"no\" dir=\"ltr\">reduce_any(...)</code></a>: Computes the \"logical or\" of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_euclidean_norm\"><code translate=\"no\" dir=\"ltr\">reduce_euclidean_norm(...)</code></a>: Computes the Euclidean norm of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_logsumexp\"><code translate=\"no\" dir=\"ltr\">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))).</p> <p><a href=\"math/reduce_max\"><code translate=\"no\" dir=\"ltr\">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_mean\"><code translate=\"no\" dir=\"ltr\">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_min\"><code translate=\"no\" dir=\"ltr\">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_prod\"><code translate=\"no\" dir=\"ltr\">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_std\"><code translate=\"no\" dir=\"ltr\">reduce_std(...)</code></a>: Computes the standard deviation of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_sum\"><code translate=\"no\" dir=\"ltr\">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor.</p> <p><a href=\"math/reduce_variance\"><code translate=\"no\" dir=\"ltr\">reduce_variance(...)</code></a>: Computes the variance of elements across dimensions of a tensor.</p> <p><a href=\"math/rint\"><code translate=\"no\" dir=\"ltr\">rint(...)</code></a>: Returns element-wise integer closest to x.</p> <p><a href=\"math/round\"><code translate=\"no\" dir=\"ltr\">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href=\"math/rsqrt\"><code translate=\"no\" dir=\"ltr\">rsqrt(...)</code></a>: Computes reciprocal of square root of x element-wise.</p> <p><a href=\"math/scalar_mul\"><code translate=\"no\" dir=\"ltr\">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> object.</p> <p><a href=\"math/segment_max\"><code translate=\"no\" dir=\"ltr\">segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href=\"math/segment_mean\"><code translate=\"no\" dir=\"ltr\">segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href=\"math/segment_min\"><code translate=\"no\" dir=\"ltr\">segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href=\"math/segment_prod\"><code translate=\"no\" dir=\"ltr\">segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href=\"math/segment_sum\"><code translate=\"no\" dir=\"ltr\">segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href=\"math/sigmoid\"><code translate=\"no\" dir=\"ltr\">sigmoid(...)</code></a>: Computes sigmoid of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/sign\"><code translate=\"no\" dir=\"ltr\">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href=\"math/sin\"><code translate=\"no\" dir=\"ltr\">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href=\"math/sinh\"><code translate=\"no\" dir=\"ltr\">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href=\"math/sobol_sample\"><code translate=\"no\" dir=\"ltr\">sobol_sample(...)</code></a>: Generates points from the Sobol sequence.</p> <p><a href=\"nn/softmax\"><code translate=\"no\" dir=\"ltr\">softmax(...)</code></a>: Computes softmax activations.</p> <p><a href=\"math/softplus\"><code translate=\"no\" dir=\"ltr\">softplus(...)</code></a>: Computes softplus: <code translate=\"no\" dir=\"ltr\">log(exp(features) + 1)</code>.</p> <p><a href=\"nn/softsign\"><code translate=\"no\" dir=\"ltr\">softsign(...)</code></a>: Computes softsign: <code translate=\"no\" dir=\"ltr\">features / (abs(features) + 1)</code>.</p> <p><a href=\"math/sqrt\"><code translate=\"no\" dir=\"ltr\">sqrt(...)</code></a>: Computes element-wise square root of the input tensor.</p> <p><a href=\"math/square\"><code translate=\"no\" dir=\"ltr\">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href=\"math/squared_difference\"><code translate=\"no\" dir=\"ltr\">squared_difference(...)</code></a>: Returns conj(x - y)(x - y) element-wise.</p> <p><a href=\"math/subtract\"><code translate=\"no\" dir=\"ltr\">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href=\"math/tan\"><code translate=\"no\" dir=\"ltr\">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href=\"math/tanh\"><code translate=\"no\" dir=\"ltr\">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate=\"no\" dir=\"ltr\">x</code> element-wise.</p> <p><a href=\"math/top_k\"><code translate=\"no\" dir=\"ltr\">top_k(...)</code></a>: Finds values and indices of the <code translate=\"no\" dir=\"ltr\">k</code> largest entries for the last dimension.</p> <p><a href=\"math/truediv\"><code translate=\"no\" dir=\"ltr\">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href=\"math/unsorted_segment_max\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href=\"math/unsorted_segment_mean\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href=\"math/unsorted_segment_min\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href=\"math/unsorted_segment_prod\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href=\"math/unsorted_segment_sqrt_n\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_sqrt_n(...)</code></a>: Computes the sum along segments of a tensor divided by the sqrt(N).</p> <p><a href=\"math/unsorted_segment_sum\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href=\"math/xdivy\"><code translate=\"no\" dir=\"ltr\">xdivy(...)</code></a>: Returns 0 if x == 0, and x / y otherwise, elementwise.</p> <p><a href=\"math/xlog1py\"><code translate=\"no\" dir=\"ltr\">xlog1py(...)</code></a>: Compute x * log1p(y).</p> <p><a href=\"math/xlogy\"><code translate=\"no\" dir=\"ltr\">xlogy(...)</code></a>: Returns 0 if x == 0, and x * log(y) otherwise, elementwise.</p> <p><a href=\"math/zero_fraction\"><code translate=\"no\" dir=\"ltr\">zero_fraction(...)</code></a>: Returns the fraction of zeros in <code translate=\"no\" dir=\"ltr\">value</code>.</p> <p><a href=\"math/zeta\"><code translate=\"no\" dir=\"ltr\">zeta(...)</code></a>: Compute the Hurwitz zeta function \\(\\zeta(x, q)\\).</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/math\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/math</a>\n  </p>\n</div>\n","keras/metrics":"<h1 class=\"devsite-page-title\">Module: tf.keras.metrics</h1>       <p>Built-in metrics.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/metrics\"><code translate=\"no\" dir=\"ltr\">tf.metrics</code></a></p> </section> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"metrics/auc\"><code translate=\"no\" dir=\"ltr\">class AUC</code></a>: Computes the approximate AUC (Area under the curve) via a Riemann sum.</p> <p><a href=\"metrics/accuracy\"><code translate=\"no\" dir=\"ltr\">class Accuracy</code></a>: Calculates how often predictions equal labels.</p> <p><a href=\"metrics/binaryaccuracy\"><code translate=\"no\" dir=\"ltr\">class BinaryAccuracy</code></a>: Calculates how often predictions match binary labels.</p> <p><a href=\"metrics/binarycrossentropy\"><code translate=\"no\" dir=\"ltr\">class BinaryCrossentropy</code></a>: Computes the crossentropy metric between the labels and predictions.</p> <p><a href=\"metrics/categoricalaccuracy\"><code translate=\"no\" dir=\"ltr\">class CategoricalAccuracy</code></a>: Calculates how often predictions matches one-hot labels.</p> <p><a href=\"metrics/categoricalcrossentropy\"><code translate=\"no\" dir=\"ltr\">class CategoricalCrossentropy</code></a>: Computes the crossentropy metric between the labels and predictions.</p> <p><a href=\"metrics/categoricalhinge\"><code translate=\"no\" dir=\"ltr\">class CategoricalHinge</code></a>: Computes the categorical hinge metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/cosinesimilarity\"><code translate=\"no\" dir=\"ltr\">class CosineSimilarity</code></a>: Computes the cosine similarity between the labels and predictions.</p> <p><a href=\"metrics/falsenegatives\"><code translate=\"no\" dir=\"ltr\">class FalseNegatives</code></a>: Calculates the number of false negatives.</p> <p><a href=\"metrics/falsepositives\"><code translate=\"no\" dir=\"ltr\">class FalsePositives</code></a>: Calculates the number of false positives.</p> <p><a href=\"metrics/hinge\"><code translate=\"no\" dir=\"ltr\">class Hinge</code></a>: Computes the hinge metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/kldivergence\"><code translate=\"no\" dir=\"ltr\">class KLDivergence</code></a>: Computes Kullback-Leibler divergence metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/logcosherror\"><code translate=\"no\" dir=\"ltr\">class LogCoshError</code></a>: Computes the logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"metrics/mean\"><code translate=\"no\" dir=\"ltr\">class Mean</code></a>: Computes the (weighted) mean of the given values.</p> <p><a href=\"metrics/meanabsoluteerror\"><code translate=\"no\" dir=\"ltr\">class MeanAbsoluteError</code></a>: Computes the mean absolute error between the labels and predictions.</p> <p><a href=\"metrics/meanabsolutepercentageerror\"><code translate=\"no\" dir=\"ltr\">class MeanAbsolutePercentageError</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/meaniou\"><code translate=\"no\" dir=\"ltr\">class MeanIoU</code></a>: Computes the mean Intersection-Over-Union metric.</p> <p><a href=\"metrics/meanrelativeerror\"><code translate=\"no\" dir=\"ltr\">class MeanRelativeError</code></a>: Computes the mean relative error by normalizing with the given values.</p> <p><a href=\"metrics/meansquarederror\"><code translate=\"no\" dir=\"ltr\">class MeanSquaredError</code></a>: Computes the mean squared error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/meansquaredlogarithmicerror\"><code translate=\"no\" dir=\"ltr\">class MeanSquaredLogarithmicError</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/meantensor\"><code translate=\"no\" dir=\"ltr\">class MeanTensor</code></a>: Computes the element-wise (weighted) mean of the given tensors.</p> <p><a href=\"metrics/metric\"><code translate=\"no\" dir=\"ltr\">class Metric</code></a>: Encapsulates metric logic and state.</p> <p><a href=\"metrics/poisson\"><code translate=\"no\" dir=\"ltr\">class Poisson</code></a>: Computes the Poisson metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/precision\"><code translate=\"no\" dir=\"ltr\">class Precision</code></a>: Computes the precision of the predictions with respect to the labels.</p> <p><a href=\"metrics/precisionatrecall\"><code translate=\"no\" dir=\"ltr\">class PrecisionAtRecall</code></a>: Computes best precision where recall is &gt;= specified value.</p> <p><a href=\"metrics/recall\"><code translate=\"no\" dir=\"ltr\">class Recall</code></a>: Computes the recall of the predictions with respect to the labels.</p> <p><a href=\"metrics/recallatprecision\"><code translate=\"no\" dir=\"ltr\">class RecallAtPrecision</code></a>: Computes best recall where precision is &gt;= specified value.</p> <p><a href=\"metrics/rootmeansquarederror\"><code translate=\"no\" dir=\"ltr\">class RootMeanSquaredError</code></a>: Computes root mean squared error metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/sensitivityatspecificity\"><code translate=\"no\" dir=\"ltr\">class SensitivityAtSpecificity</code></a>: Computes best sensitivity where specificity is &gt;= specified value.</p> <p><a href=\"metrics/sparsecategoricalaccuracy\"><code translate=\"no\" dir=\"ltr\">class SparseCategoricalAccuracy</code></a>: Calculates how often predictions matches integer labels.</p> <p><a href=\"metrics/sparsecategoricalcrossentropy\"><code translate=\"no\" dir=\"ltr\">class SparseCategoricalCrossentropy</code></a>: Computes the crossentropy metric between the labels and predictions.</p> <p><a href=\"metrics/sparsetopkcategoricalaccuracy\"><code translate=\"no\" dir=\"ltr\">class SparseTopKCategoricalAccuracy</code></a>: Computes how often integer targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p> <p><a href=\"metrics/specificityatsensitivity\"><code translate=\"no\" dir=\"ltr\">class SpecificityAtSensitivity</code></a>: Computes best specificity where sensitivity is &gt;= specified value.</p> <p><a href=\"metrics/squaredhinge\"><code translate=\"no\" dir=\"ltr\">class SquaredHinge</code></a>: Computes the squared hinge metric between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/sum\"><code translate=\"no\" dir=\"ltr\">class Sum</code></a>: Computes the (weighted) sum of the given values.</p> <p><a href=\"metrics/topkcategoricalaccuracy\"><code translate=\"no\" dir=\"ltr\">class TopKCategoricalAccuracy</code></a>: Computes how often targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p> <p><a href=\"metrics/truenegatives\"><code translate=\"no\" dir=\"ltr\">class TrueNegatives</code></a>: Calculates the number of true negatives.</p> <p><a href=\"metrics/truepositives\"><code translate=\"no\" dir=\"ltr\">class TruePositives</code></a>: Calculates the number of true positives.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">KLD(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">MAE(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">MAPE(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">MSE(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">MSLE(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/binary_accuracy\"><code translate=\"no\" dir=\"ltr\">binary_accuracy(...)</code></a>: Calculates how often predictions matches binary labels.</p> <p><a href=\"losses/binary_crossentropy\"><code translate=\"no\" dir=\"ltr\">binary_crossentropy(...)</code></a>: Computes the binary crossentropy loss.</p> <p><a href=\"metrics/categorical_accuracy\"><code translate=\"no\" dir=\"ltr\">categorical_accuracy(...)</code></a>: Calculates how often predictions matches one-hot labels.</p> <p><a href=\"losses/categorical_crossentropy\"><code translate=\"no\" dir=\"ltr\">categorical_crossentropy(...)</code></a>: Computes the categorical crossentropy loss.</p> <p><a href=\"metrics/deserialize\"><code translate=\"no\" dir=\"ltr\">deserialize(...)</code></a>: Deserializes a serialized metric class/function instance.</p> <p><a href=\"metrics/get\"><code translate=\"no\" dir=\"ltr\">get(...)</code></a>: Retrieves a Keras metric as a <code translate=\"no\" dir=\"ltr\">function</code>/<code translate=\"no\" dir=\"ltr\">Metric</code> class instance.</p> <p><a href=\"losses/hinge\"><code translate=\"no\" dir=\"ltr\">hinge(...)</code></a>: Computes the hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kl_divergence(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kld(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/kld\"><code translate=\"no\" dir=\"ltr\">kullback_leibler_divergence(...)</code></a>: Computes Kullback-Leibler divergence loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/log_cosh\"><code translate=\"no\" dir=\"ltr\">log_cosh(...)</code></a>: Logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"losses/log_cosh\"><code translate=\"no\" dir=\"ltr\">logcosh(...)</code></a>: Logarithm of the hyperbolic cosine of the prediction error.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">mae(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">mape(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mae\"><code translate=\"no\" dir=\"ltr\">mean_absolute_error(...)</code></a>: Computes the mean absolute error between labels and predictions.</p> <p><a href=\"losses/mape\"><code translate=\"no\" dir=\"ltr\">mean_absolute_percentage_error(...)</code></a>: Computes the mean absolute percentage error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">mean_squared_error(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">mean_squared_logarithmic_error(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/mse\"><code translate=\"no\" dir=\"ltr\">mse(...)</code></a>: Computes the mean squared error between labels and predictions.</p> <p><a href=\"losses/msle\"><code translate=\"no\" dir=\"ltr\">msle(...)</code></a>: Computes the mean squared logarithmic error between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"losses/poisson\"><code translate=\"no\" dir=\"ltr\">poisson(...)</code></a>: Computes the Poisson loss between y_true and y_pred.</p> <p><a href=\"metrics/serialize\"><code translate=\"no\" dir=\"ltr\">serialize(...)</code></a>: Serializes metric function or <code translate=\"no\" dir=\"ltr\">Metric</code> instance.</p> <p><a href=\"metrics/sparse_categorical_accuracy\"><code translate=\"no\" dir=\"ltr\">sparse_categorical_accuracy(...)</code></a>: Calculates how often predictions matches integer labels.</p> <p><a href=\"losses/sparse_categorical_crossentropy\"><code translate=\"no\" dir=\"ltr\">sparse_categorical_crossentropy(...)</code></a>: Computes the sparse categorical crossentropy loss.</p> <p><a href=\"metrics/sparse_top_k_categorical_accuracy\"><code translate=\"no\" dir=\"ltr\">sparse_top_k_categorical_accuracy(...)</code></a>: Computes how often integer targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p> <p><a href=\"losses/squared_hinge\"><code translate=\"no\" dir=\"ltr\">squared_hinge(...)</code></a>: Computes the squared hinge loss between <code translate=\"no\" dir=\"ltr\">y_true</code> and <code translate=\"no\" dir=\"ltr\">y_pred</code>.</p> <p><a href=\"metrics/top_k_categorical_accuracy\"><code translate=\"no\" dir=\"ltr\">top_k_categorical_accuracy(...)</code></a>: Computes how often targets are in the top <code translate=\"no\" dir=\"ltr\">K</code> predictions.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/metrics\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/metrics</a>\n  </p>\n</div>\n","mlir":"<h1 class=\"devsite-page-title\">Module: tf.mlir</h1>       <p>Public API for tf.mlir namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"mlir/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.mlir.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mlir\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mlir</a>\n  </p>\n</div>\n","profiler":"<h1 class=\"devsite-page-title\">Module: tf.profiler</h1>       <p>Public API for tf.profiler namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"profiler/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.profiler.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/profiler\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/profiler</a>\n  </p>\n</div>\n","quantization":"<h1 class=\"devsite-page-title\">Module: tf.quantization</h1>       <p>Public API for tf.quantization namespace.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"quantization/dequantize\"><code translate=\"no\" dir=\"ltr\">dequantize(...)</code></a>: Dequantize the 'input' tensor into a float or bfloat16 Tensor.</p> <p><a href=\"quantization/fake_quant_with_min_max_args\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_args(...)</code></a>: Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.</p> <p><a href=\"quantization/fake_quant_with_min_max_args_gradient\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_args_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxArgs operation.</p> <p><a href=\"quantization/fake_quant_with_min_max_vars\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_vars(...)</code></a>: Fake-quantize the 'inputs' tensor of type float via global float scalars</p> <p><a href=\"quantization/fake_quant_with_min_max_vars_gradient\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_vars_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVars operation.</p> <p><a href=\"quantization/fake_quant_with_min_max_vars_per_channel\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_vars_per_channel(...)</code></a>: Fake-quantize the 'inputs' tensor of type float via per-channel floats</p> <p><a href=\"quantization/fake_quant_with_min_max_vars_per_channel_gradient\"><code translate=\"no\" dir=\"ltr\">fake_quant_with_min_max_vars_per_channel_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.</p> <p><a href=\"quantization/quantize\"><code translate=\"no\" dir=\"ltr\">quantize(...)</code></a>: Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.</p> <p><a href=\"quantization/quantize_and_dequantize\"><code translate=\"no\" dir=\"ltr\">quantize_and_dequantize(...)</code></a>: Quantizes then dequantizes a tensor. (deprecated)</p> <p><a href=\"quantization/quantize_and_dequantize_v2\"><code translate=\"no\" dir=\"ltr\">quantize_and_dequantize_v2(...)</code></a>: Quantizes then dequantizes a tensor.</p> <p><a href=\"quantization/quantized_concat\"><code translate=\"no\" dir=\"ltr\">quantized_concat(...)</code></a>: Concatenates quantized tensors along one dimension.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/quantization\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/quantization</a>\n  </p>\n</div>\n","ragged":"<h1 class=\"devsite-page-title\">Module: tf.ragged</h1>       <p>Ragged Tensors.</p> <p>This package defines ops for manipulating ragged tensors (<a href=\"raggedtensor\"><code translate=\"no\" dir=\"ltr\">tf.RaggedTensor</code></a>), which are tensors with non-uniform shapes. In particular, each <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> has one or more <em>ragged dimensions</em>, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of <code translate=\"no\" dir=\"ltr\">rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code> is ragged, since the column slices (<code translate=\"no\" dir=\"ltr\">rt[0, :]</code>, ..., <code translate=\"no\" dir=\"ltr\">rt[4, :]</code>) have different lengths. For a more detailed description of ragged tensors, see the <a href=\"raggedtensor\"><code translate=\"no\" dir=\"ltr\">tf.RaggedTensor</code></a> class documentation and the <a href=\"https://www.tensorflow.org/guide/ragged_tensors\">Ragged Tensor Guide</a>.</p> <h3 id=\"additional_ops_that_support_raggedtensor\" data-text=\"Additional ops that support RaggedTensor\">Additional ops that support <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>\n</h3> <p>Arguments that accept <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s are marked in <strong>bold</strong>.</p> <ul> <li>\n<code translate=\"no\" dir=\"ltr\">tf.batch_gather</code>(<strong>params</strong>, <strong>indices</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/bitwise_and\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.bitwise_and</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/bitwise_or\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.bitwise_or</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/bitwise_xor\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.bitwise_xor</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/invert\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.invert</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/left_shift\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.left_shift</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"bitwise/right_shift\"><code translate=\"no\" dir=\"ltr\">tf.bitwise.right_shift</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"cast\"><code translate=\"no\" dir=\"ltr\">tf.cast</code></a>(<strong>x</strong>, dtype, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"clip_by_value\"><code translate=\"no\" dir=\"ltr\">tf.clip_by_value</code></a>(<strong>t</strong>, clip_value_min, clip_value_max, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"concat\"><code translate=\"no\" dir=\"ltr\">tf.concat</code></a>(<strong>values</strong>, axis, name=<code translate=\"no\" dir=\"ltr\">'concat'</code>)</li> <li>\n<a href=\"debugging/check_numerics\"><code translate=\"no\" dir=\"ltr\">tf.debugging.check_numerics</code></a>(<strong>tensor</strong>, message, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"dtypes/complex\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.complex</code></a>(<strong>real</strong>, <strong>imag</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"dtypes/saturate_cast\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.saturate_cast</code></a>(<strong>value</strong>, dtype, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"dynamic_partition\"><code translate=\"no\" dir=\"ltr\">tf.dynamic_partition</code></a>(<strong>data</strong>, <strong>partitions</strong>, num_partitions, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"expand_dims\"><code translate=\"no\" dir=\"ltr\">tf.expand_dims</code></a>(<strong>input</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, dim=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"gather_nd\"><code translate=\"no\" dir=\"ltr\">tf.gather_nd</code></a>(<strong>params</strong>, <strong>indices</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>, batch_dims=<code translate=\"no\" dir=\"ltr\">0</code>)</li> <li>\n<a href=\"gather\"><code translate=\"no\" dir=\"ltr\">tf.gather</code></a>(<strong>params</strong>, <strong>indices</strong>, validate_indices=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, batch_dims=<code translate=\"no\" dir=\"ltr\">0</code>)</li> <li>\n<a href=\"identity\"><code translate=\"no\" dir=\"ltr\">tf.identity</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"io/decode_base64\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_base64</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"io/decode_compressed\"><code translate=\"no\" dir=\"ltr\">tf.io.decode_compressed</code></a>(<strong>bytes</strong>, compression_type=<code translate=\"no\" dir=\"ltr\">''</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"io/encode_base64\"><code translate=\"no\" dir=\"ltr\">tf.io.encode_base64</code></a>(<strong>input</strong>, pad=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/abs\"><code translate=\"no\" dir=\"ltr\">tf.math.abs</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/acos\"><code translate=\"no\" dir=\"ltr\">tf.math.acos</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/acosh\"><code translate=\"no\" dir=\"ltr\">tf.math.acosh</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/add_n\"><code translate=\"no\" dir=\"ltr\">tf.math.add_n</code></a>(<strong>inputs</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">tf.math.add</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/angle\"><code translate=\"no\" dir=\"ltr\">tf.math.angle</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/asin\"><code translate=\"no\" dir=\"ltr\">tf.math.asin</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/asinh\"><code translate=\"no\" dir=\"ltr\">tf.math.asinh</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/atan2\"><code translate=\"no\" dir=\"ltr\">tf.math.atan2</code></a>(<strong>y</strong>, <strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/atan\"><code translate=\"no\" dir=\"ltr\">tf.math.atan</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/atanh\"><code translate=\"no\" dir=\"ltr\">tf.math.atanh</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/ceil\"><code translate=\"no\" dir=\"ltr\">tf.math.ceil</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/conj\"><code translate=\"no\" dir=\"ltr\">tf.math.conj</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/cos\"><code translate=\"no\" dir=\"ltr\">tf.math.cos</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/cosh\"><code translate=\"no\" dir=\"ltr\">tf.math.cosh</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/digamma\"><code translate=\"no\" dir=\"ltr\">tf.math.digamma</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/divide_no_nan\"><code translate=\"no\" dir=\"ltr\">tf.math.divide_no_nan</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/divide\"><code translate=\"no\" dir=\"ltr\">tf.math.divide</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/equal\"><code translate=\"no\" dir=\"ltr\">tf.math.equal</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/erf\"><code translate=\"no\" dir=\"ltr\">tf.math.erf</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/erfc\"><code translate=\"no\" dir=\"ltr\">tf.math.erfc</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/erfinv\"><code translate=\"no\" dir=\"ltr\">tf.math.erfinv</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/exp\"><code translate=\"no\" dir=\"ltr\">tf.math.exp</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/expm1\"><code translate=\"no\" dir=\"ltr\">tf.math.expm1</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/floor\"><code translate=\"no\" dir=\"ltr\">tf.math.floor</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/floordiv\"><code translate=\"no\" dir=\"ltr\">tf.math.floordiv</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/floormod\"><code translate=\"no\" dir=\"ltr\">tf.math.floormod</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/greater_equal\"><code translate=\"no\" dir=\"ltr\">tf.math.greater_equal</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/greater\"><code translate=\"no\" dir=\"ltr\">tf.math.greater</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/imag\"><code translate=\"no\" dir=\"ltr\">tf.math.imag</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/is_finite\"><code translate=\"no\" dir=\"ltr\">tf.math.is_finite</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/is_inf\"><code translate=\"no\" dir=\"ltr\">tf.math.is_inf</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/is_nan\"><code translate=\"no\" dir=\"ltr\">tf.math.is_nan</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/less_equal\"><code translate=\"no\" dir=\"ltr\">tf.math.less_equal</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/less\"><code translate=\"no\" dir=\"ltr\">tf.math.less</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/lgamma\"><code translate=\"no\" dir=\"ltr\">tf.math.lgamma</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/log1p\"><code translate=\"no\" dir=\"ltr\">tf.math.log1p</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/log_sigmoid\"><code translate=\"no\" dir=\"ltr\">tf.math.log_sigmoid</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/log\"><code translate=\"no\" dir=\"ltr\">tf.math.log</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/logical_and\"><code translate=\"no\" dir=\"ltr\">tf.math.logical_and</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/logical_not\"><code translate=\"no\" dir=\"ltr\">tf.math.logical_not</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/logical_or\"><code translate=\"no\" dir=\"ltr\">tf.math.logical_or</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/logical_xor\"><code translate=\"no\" dir=\"ltr\">tf.math.logical_xor</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">'LogicalXor'</code>)</li> <li>\n<a href=\"math/maximum\"><code translate=\"no\" dir=\"ltr\">tf.math.maximum</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/minimum\"><code translate=\"no\" dir=\"ltr\">tf.math.minimum</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/multiply\"><code translate=\"no\" dir=\"ltr\">tf.math.multiply</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/ndtri\"><code translate=\"no\" dir=\"ltr\">tf.math.ndtri</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/negative\"><code translate=\"no\" dir=\"ltr\">tf.math.negative</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/not_equal\"><code translate=\"no\" dir=\"ltr\">tf.math.not_equal</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/pow\"><code translate=\"no\" dir=\"ltr\">tf.math.pow</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/real\"><code translate=\"no\" dir=\"ltr\">tf.math.real</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reciprocal\"><code translate=\"no\" dir=\"ltr\">tf.math.reciprocal</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_all\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_all</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_any\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_any</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_max\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_max</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_mean\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_mean</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_min\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_min</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_prod\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_prod</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/reduce_sum\"><code translate=\"no\" dir=\"ltr\">tf.math.reduce_sum</code></a>(<strong>input_tensor</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/rint\"><code translate=\"no\" dir=\"ltr\">tf.math.rint</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/round\"><code translate=\"no\" dir=\"ltr\">tf.math.round</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/rsqrt\"><code translate=\"no\" dir=\"ltr\">tf.math.rsqrt</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/sign\"><code translate=\"no\" dir=\"ltr\">tf.math.sign</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/sin\"><code translate=\"no\" dir=\"ltr\">tf.math.sin</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/sinh\"><code translate=\"no\" dir=\"ltr\">tf.math.sinh</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/sqrt\"><code translate=\"no\" dir=\"ltr\">tf.math.sqrt</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/square\"><code translate=\"no\" dir=\"ltr\">tf.math.square</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/squared_difference\"><code translate=\"no\" dir=\"ltr\">tf.math.squared_difference</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/subtract\"><code translate=\"no\" dir=\"ltr\">tf.math.subtract</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/tan\"><code translate=\"no\" dir=\"ltr\">tf.math.tan</code></a>(<strong>x</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/truediv\"><code translate=\"no\" dir=\"ltr\">tf.math.truediv</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_max\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_max</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_mean\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_mean</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_min\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_min</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_prod\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_prod</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_sqrt_n\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_sqrt_n</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"math/unsorted_segment_sum\"><code translate=\"no\" dir=\"ltr\">tf.math.unsorted_segment_sum</code></a>(<strong>data</strong>, <strong>segment_ids</strong>, num_segments, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"nn/dropout\"><code translate=\"no\" dir=\"ltr\">tf.nn.dropout</code></a>(<strong>x</strong>, keep_prob=<code translate=\"no\" dir=\"ltr\">None</code>, noise_shape=<code translate=\"no\" dir=\"ltr\">None</code>, seed=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, rate=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"one_hot\"><code translate=\"no\" dir=\"ltr\">tf.one_hot</code></a>(<strong>indices</strong>, depth, on_value=<code translate=\"no\" dir=\"ltr\">None</code>, off_value=<code translate=\"no\" dir=\"ltr\">None</code>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, dtype=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"ones_like\"><code translate=\"no\" dir=\"ltr\">tf.ones_like</code></a>(<strong>tensor</strong>, dtype=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, optimize=<code translate=\"no\" dir=\"ltr\">True</code>)</li> <li>\n<a href=\"print\"><code translate=\"no\" dir=\"ltr\">tf.print</code></a>(*<strong>inputs</strong>, **kwargs)</li> <li>\n<a href=\"rank\"><code translate=\"no\" dir=\"ltr\">tf.rank</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"realdiv\"><code translate=\"no\" dir=\"ltr\">tf.realdiv</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"reverse\"><code translate=\"no\" dir=\"ltr\">tf.reverse</code></a>(<strong>tensor</strong>, axis, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"size\"><code translate=\"no\" dir=\"ltr\">tf.size</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>, out_type=<a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a>)</li> <li>\n<a href=\"squeeze\"><code translate=\"no\" dir=\"ltr\">tf.squeeze</code></a>(<strong>input</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, squeeze_dims=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"stack\"><code translate=\"no\" dir=\"ltr\">tf.stack</code></a>(<strong>values</strong>, axis=<code translate=\"no\" dir=\"ltr\">0</code>, name=<code translate=\"no\" dir=\"ltr\">'stack'</code>)</li> <li>\n<a href=\"strings/as_string\"><code translate=\"no\" dir=\"ltr\">tf.strings.as_string</code></a>(<strong>input</strong>, precision=<code translate=\"no\" dir=\"ltr\">-1</code>, scientific=<code translate=\"no\" dir=\"ltr\">False</code>, shortest=<code translate=\"no\" dir=\"ltr\">False</code>, width=<code translate=\"no\" dir=\"ltr\">-1</code>, fill=<code translate=\"no\" dir=\"ltr\">''</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/format\"><code translate=\"no\" dir=\"ltr\">tf.strings.format</code></a>(template, <strong>inputs</strong>, placeholder=<code translate=\"no\" dir=\"ltr\">'{}'</code>, summarize=<code translate=\"no\" dir=\"ltr\">3</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/join\"><code translate=\"no\" dir=\"ltr\">tf.strings.join</code></a>(<strong>inputs</strong>, separator=<code translate=\"no\" dir=\"ltr\">''</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/length\"><code translate=\"no\" dir=\"ltr\">tf.strings.length</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>, unit=<code translate=\"no\" dir=\"ltr\">'BYTE'</code>)</li> <li>\n<a href=\"strings/reduce_join\"><code translate=\"no\" dir=\"ltr\">tf.strings.reduce_join</code></a>(<strong>inputs</strong>, axis=<code translate=\"no\" dir=\"ltr\">None</code>, keepdims=<code translate=\"no\" dir=\"ltr\">False</code>, separator=<code translate=\"no\" dir=\"ltr\">''</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/regex_full_match\"><code translate=\"no\" dir=\"ltr\">tf.strings.regex_full_match</code></a>(<strong>input</strong>, pattern, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/regex_replace\"><code translate=\"no\" dir=\"ltr\">tf.strings.regex_replace</code></a>(<strong>input</strong>, pattern, rewrite, replace_global=<code translate=\"no\" dir=\"ltr\">True</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/strip\"><code translate=\"no\" dir=\"ltr\">tf.strings.strip</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/substr\"><code translate=\"no\" dir=\"ltr\">tf.strings.substr</code></a>(<strong>input</strong>, pos, len, name=<code translate=\"no\" dir=\"ltr\">None</code>, unit=<code translate=\"no\" dir=\"ltr\">'BYTE'</code>)</li> <li>\n<a href=\"strings/to_hash_bucket_fast\"><code translate=\"no\" dir=\"ltr\">tf.strings.to_hash_bucket_fast</code></a>(<strong>input</strong>, num_buckets, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/to_hash_bucket_strong\"><code translate=\"no\" dir=\"ltr\">tf.strings.to_hash_bucket_strong</code></a>(<strong>input</strong>, num_buckets, key, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/to_hash_bucket\"><code translate=\"no\" dir=\"ltr\">tf.strings.to_hash_bucket</code></a>(<strong>input</strong>, num_buckets, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/to_hash_bucket\"><code translate=\"no\" dir=\"ltr\">tf.strings.to_hash_bucket</code></a>(<strong>input</strong>, num_buckets, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/to_number\"><code translate=\"no\" dir=\"ltr\">tf.strings.to_number</code></a>(<strong>input</strong>, out_type=<a href=\"../tf#float32\"><code translate=\"no\" dir=\"ltr\">tf.float32</code></a>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"strings/unicode_script\"><code translate=\"no\" dir=\"ltr\">tf.strings.unicode_script</code></a>(<strong>input</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"tile\"><code translate=\"no\" dir=\"ltr\">tf.tile</code></a>(<strong>input</strong>, multiples, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"truncatediv\"><code translate=\"no\" dir=\"ltr\">tf.truncatediv</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"truncatemod\"><code translate=\"no\" dir=\"ltr\">tf.truncatemod</code></a>(<strong>x</strong>, <strong>y</strong>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"where\"><code translate=\"no\" dir=\"ltr\">tf.where</code></a>(<strong>condition</strong>, <strong>x</strong>=<code translate=\"no\" dir=\"ltr\">None</code>, <strong>y</strong>=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"where\"><code translate=\"no\" dir=\"ltr\">tf.where</code></a>(<strong>condition</strong>, <strong>x</strong>=<code translate=\"no\" dir=\"ltr\">None</code>, <strong>y</strong>=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>)</li> <li>\n<a href=\"zeros_like\"><code translate=\"no\" dir=\"ltr\">tf.zeros_like</code></a>(<strong>tensor</strong>, dtype=<code translate=\"no\" dir=\"ltr\">None</code>, name=<code translate=\"no\" dir=\"ltr\">None</code>, optimize=<code translate=\"no\" dir=\"ltr\">True</code>)n</li> </ul> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"ragged/boolean_mask\"><code translate=\"no\" dir=\"ltr\">boolean_mask(...)</code></a>: Applies a boolean mask to <code translate=\"no\" dir=\"ltr\">data</code> without flattening the mask dimensions.</p> <p><a href=\"ragged/constant\"><code translate=\"no\" dir=\"ltr\">constant(...)</code></a>: Constructs a constant RaggedTensor from a nested Python list.</p> <p><a href=\"ragged/cross\"><code translate=\"no\" dir=\"ltr\">cross(...)</code></a>: Generates feature cross from a list of tensors.</p> <p><a href=\"ragged/cross_hashed\"><code translate=\"no\" dir=\"ltr\">cross_hashed(...)</code></a>: Generates hashed feature cross from a list of tensors.</p> <p><a href=\"ragged/map_flat_values\"><code translate=\"no\" dir=\"ltr\">map_flat_values(...)</code></a>: Applies <code translate=\"no\" dir=\"ltr\">op</code> to the values of one or more RaggedTensors.</p> <p><a href=\"ragged/range\"><code translate=\"no\" dir=\"ltr\">range(...)</code></a>: Returns a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> containing the specified sequences of numbers.</p> <p><a href=\"ragged/row_splits_to_segment_ids\"><code translate=\"no\" dir=\"ltr\">row_splits_to_segment_ids(...)</code></a>: Generates the segmentation corresponding to a RaggedTensor <code translate=\"no\" dir=\"ltr\">row_splits</code>.</p> <p><a href=\"ragged/segment_ids_to_row_splits\"><code translate=\"no\" dir=\"ltr\">segment_ids_to_row_splits(...)</code></a>: Generates the RaggedTensor <code translate=\"no\" dir=\"ltr\">row_splits</code> corresponding to a segmentation.</p> <p><a href=\"ragged/stack\"><code translate=\"no\" dir=\"ltr\">stack(...)</code></a>: Stacks a list of rank-<code translate=\"no\" dir=\"ltr\">R</code> tensors into one rank-<code translate=\"no\" dir=\"ltr\">(R+1)</code> <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p><a href=\"ragged/stack_dynamic_partitions\"><code translate=\"no\" dir=\"ltr\">stack_dynamic_partitions(...)</code></a>: Stacks dynamic partitions of a Tensor or RaggedTensor.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/ragged\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/ragged</a>\n  </p>\n</div>\n","raw_ops":"<h1 class=\"devsite-page-title\">Module: tf.raw_ops</h1>       <p>Public API for tf.raw_ops namespace.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"https://www.tensorflow.org/api_docs/python/tf/raw_ops\"><code translate=\"no\" dir=\"ltr\">tf.raw_ops</code></a> provides direct/low level access to all TensorFlow ops. See <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20181225-tf-raw-ops.md\">the RFC</a> for details. Unless you are library writer, you likely do not need to use these ops directly.</span>\n</blockquote> <table> <thead> <tr> <th>Op Name</th> <th style=\"text-align: center\">Has Gradient</th> </tr> </thead> <tbody> <tr> <td><a id=\"Abort\" href=\"raw_ops/abort\">Abort</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Abs\" href=\"raw_ops/abs\">Abs</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AccumulateNV2\" href=\"raw_ops/accumulatenv2\">AccumulateNV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AccumulatorApplyGradient\" href=\"raw_ops/accumulatorapplygradient\">AccumulatorApplyGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AccumulatorNumAccumulated\" href=\"raw_ops/accumulatornumaccumulated\">AccumulatorNumAccumulated</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AccumulatorSetGlobalStep\" href=\"raw_ops/accumulatorsetglobalstep\">AccumulatorSetGlobalStep</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AccumulatorTakeGradient\" href=\"raw_ops/accumulatortakegradient\">AccumulatorTakeGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Acos\" href=\"raw_ops/acos\">Acos</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Acosh\" href=\"raw_ops/acosh\">Acosh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Add\" href=\"raw_ops/add\">Add</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AddManySparseToTensorsMap\" href=\"raw_ops/addmanysparsetotensorsmap\">AddManySparseToTensorsMap</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AddN\" href=\"raw_ops/addn\">AddN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AddSparseToTensorsMap\" href=\"raw_ops/addsparsetotensorsmap\">AddSparseToTensorsMap</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AddV2\" href=\"raw_ops/addv2\">AddV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AdjustContrast\" href=\"raw_ops/adjustcontrast\">AdjustContrast</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AdjustContrastv2\" href=\"raw_ops/adjustcontrastv2\">AdjustContrastv2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AdjustHue\" href=\"raw_ops/adjusthue\">AdjustHue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AdjustSaturation\" href=\"raw_ops/adjustsaturation\">AdjustSaturation</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"All\" href=\"raw_ops/all\">All</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AllCandidateSampler\" href=\"raw_ops/allcandidatesampler\">AllCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AllToAll\" href=\"raw_ops/alltoall\">AllToAll</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Angle\" href=\"raw_ops/angle\">Angle</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AnonymousIterator\" href=\"raw_ops/anonymousiterator\">AnonymousIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AnonymousIteratorV2\" href=\"raw_ops/anonymousiteratorv2\">AnonymousIteratorV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AnonymousMemoryCache\" href=\"raw_ops/anonymousmemorycache\">AnonymousMemoryCache</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AnonymousMultiDeviceIterator\" href=\"raw_ops/anonymousmultideviceiterator\">AnonymousMultiDeviceIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AnonymousRandomSeedGenerator\" href=\"raw_ops/anonymousrandomseedgenerator\">AnonymousRandomSeedGenerator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AnonymousSeedGenerator\" href=\"raw_ops/anonymousseedgenerator\">AnonymousSeedGenerator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Any\" href=\"raw_ops/any\">Any</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdaMax\" href=\"raw_ops/applyadamax\">ApplyAdaMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdadelta\" href=\"raw_ops/applyadadelta\">ApplyAdadelta</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdagrad\" href=\"raw_ops/applyadagrad\">ApplyAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdagradDA\" href=\"raw_ops/applyadagradda\">ApplyAdagradDA</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdagradV2\" href=\"raw_ops/applyadagradv2\">ApplyAdagradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAdam\" href=\"raw_ops/applyadam\">ApplyAdam</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyAddSign\" href=\"raw_ops/applyaddsign\">ApplyAddSign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyCenteredRMSProp\" href=\"raw_ops/applycenteredrmsprop\">ApplyCenteredRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyFtrl\" href=\"raw_ops/applyftrl\">ApplyFtrl</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyFtrlV2\" href=\"raw_ops/applyftrlv2\">ApplyFtrlV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyGradientDescent\" href=\"raw_ops/applygradientdescent\">ApplyGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyMomentum\" href=\"raw_ops/applymomentum\">ApplyMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyPowerSign\" href=\"raw_ops/applypowersign\">ApplyPowerSign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyProximalAdagrad\" href=\"raw_ops/applyproximaladagrad\">ApplyProximalAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyProximalGradientDescent\" href=\"raw_ops/applyproximalgradientdescent\">ApplyProximalGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApplyRMSProp\" href=\"raw_ops/applyrmsprop\">ApplyRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ApproximateEqual\" href=\"raw_ops/approximateequal\">ApproximateEqual</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ArgMax\" href=\"raw_ops/argmax\">ArgMax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ArgMin\" href=\"raw_ops/argmin\">ArgMin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AsString\" href=\"raw_ops/asstring\">AsString</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Asin\" href=\"raw_ops/asin\">Asin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Asinh\" href=\"raw_ops/asinh\">Asinh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Assert\" href=\"raw_ops/assert\">Assert</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AssertCardinalityDataset\" href=\"raw_ops/assertcardinalitydataset\">AssertCardinalityDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AssertNextDataset\" href=\"raw_ops/assertnextdataset\">AssertNextDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Assign\" href=\"raw_ops/assign\">Assign</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AssignAdd\" href=\"raw_ops/assignadd\">AssignAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AssignAddVariableOp\" href=\"raw_ops/assignaddvariableop\">AssignAddVariableOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AssignSub\" href=\"raw_ops/assignsub\">AssignSub</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AssignSubVariableOp\" href=\"raw_ops/assignsubvariableop\">AssignSubVariableOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AssignVariableOp\" href=\"raw_ops/assignvariableop\">AssignVariableOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Atan\" href=\"raw_ops/atan\">Atan</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Atan2\" href=\"raw_ops/atan2\">Atan2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Atanh\" href=\"raw_ops/atanh\">Atanh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AudioSpectrogram\" href=\"raw_ops/audiospectrogram\">AudioSpectrogram</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AudioSummary\" href=\"raw_ops/audiosummary\">AudioSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AudioSummaryV2\" href=\"raw_ops/audiosummaryv2\">AudioSummaryV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AutoShardDataset\" href=\"raw_ops/autosharddataset\">AutoShardDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"AvgPool\" href=\"raw_ops/avgpool\">AvgPool</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AvgPool3D\" href=\"raw_ops/avgpool3d\">AvgPool3D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AvgPool3DGrad\" href=\"raw_ops/avgpool3dgrad\">AvgPool3DGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"AvgPoolGrad\" href=\"raw_ops/avgpoolgrad\">AvgPoolGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BandedTriangularSolve\" href=\"raw_ops/bandedtriangularsolve\">BandedTriangularSolve</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Barrier\" href=\"raw_ops/barrier\">Barrier</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BarrierClose\" href=\"raw_ops/barrierclose\">BarrierClose</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BarrierIncompleteSize\" href=\"raw_ops/barrierincompletesize\">BarrierIncompleteSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BarrierInsertMany\" href=\"raw_ops/barrierinsertmany\">BarrierInsertMany</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BarrierReadySize\" href=\"raw_ops/barrierreadysize\">BarrierReadySize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BarrierTakeMany\" href=\"raw_ops/barriertakemany\">BarrierTakeMany</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Batch\" href=\"raw_ops/batch\">Batch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchCholesky\" href=\"raw_ops/batchcholesky\">BatchCholesky</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchCholeskyGrad\" href=\"raw_ops/batchcholeskygrad\">BatchCholeskyGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchDataset\" href=\"raw_ops/batchdataset\">BatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchDatasetV2\" href=\"raw_ops/batchdatasetv2\">BatchDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchFFT\" href=\"raw_ops/batchfft\">BatchFFT</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchFFT2D\" href=\"raw_ops/batchfft2d\">BatchFFT2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchFFT3D\" href=\"raw_ops/batchfft3d\">BatchFFT3D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchFunction\" href=\"raw_ops/batchfunction\">BatchFunction</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchIFFT\" href=\"raw_ops/batchifft\">BatchIFFT</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchIFFT2D\" href=\"raw_ops/batchifft2d\">BatchIFFT2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchIFFT3D\" href=\"raw_ops/batchifft3d\">BatchIFFT3D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatMul\" href=\"raw_ops/batchmatmul\">BatchMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BatchMatMulV2\" href=\"raw_ops/batchmatmulv2\">BatchMatMulV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BatchMatrixBandPart\" href=\"raw_ops/batchmatrixbandpart\">BatchMatrixBandPart</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixDeterminant\" href=\"raw_ops/batchmatrixdeterminant\">BatchMatrixDeterminant</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixDiag\" href=\"raw_ops/batchmatrixdiag\">BatchMatrixDiag</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixDiagPart\" href=\"raw_ops/batchmatrixdiagpart\">BatchMatrixDiagPart</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixInverse\" href=\"raw_ops/batchmatrixinverse\">BatchMatrixInverse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixSetDiag\" href=\"raw_ops/batchmatrixsetdiag\">BatchMatrixSetDiag</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixSolve\" href=\"raw_ops/batchmatrixsolve\">BatchMatrixSolve</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixSolveLs\" href=\"raw_ops/batchmatrixsolvels\">BatchMatrixSolveLs</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchMatrixTriangularSolve\" href=\"raw_ops/batchmatrixtriangularsolve\">BatchMatrixTriangularSolve</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchNormWithGlobalNormalization\" href=\"raw_ops/batchnormwithglobalnormalization\">BatchNormWithGlobalNormalization</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BatchNormWithGlobalNormalizationGrad\" href=\"raw_ops/batchnormwithglobalnormalizationgrad\">BatchNormWithGlobalNormalizationGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchSelfAdjointEig\" href=\"raw_ops/batchselfadjointeig\">BatchSelfAdjointEig</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchSelfAdjointEigV2\" href=\"raw_ops/batchselfadjointeigv2\">BatchSelfAdjointEigV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchSvd\" href=\"raw_ops/batchsvd\">BatchSvd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BatchToSpace\" href=\"raw_ops/batchtospace\">BatchToSpace</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BatchToSpaceND\" href=\"raw_ops/batchtospacend\">BatchToSpaceND</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselI0\" href=\"raw_ops/besseli0\">BesselI0</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselI0e\" href=\"raw_ops/besseli0e\">BesselI0e</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselI1\" href=\"raw_ops/besseli1\">BesselI1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselI1e\" href=\"raw_ops/besseli1e\">BesselI1e</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselJ0\" href=\"raw_ops/besselj0\">BesselJ0</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselJ1\" href=\"raw_ops/besselj1\">BesselJ1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselK0\" href=\"raw_ops/besselk0\">BesselK0</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselK0e\" href=\"raw_ops/besselk0e\">BesselK0e</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselK1\" href=\"raw_ops/besselk1\">BesselK1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselK1e\" href=\"raw_ops/besselk1e\">BesselK1e</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselY0\" href=\"raw_ops/bessely0\">BesselY0</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BesselY1\" href=\"raw_ops/bessely1\">BesselY1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Betainc\" href=\"raw_ops/betainc\">Betainc</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BiasAdd\" href=\"raw_ops/biasadd\">BiasAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BiasAddGrad\" href=\"raw_ops/biasaddgrad\">BiasAddGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BiasAddV1\" href=\"raw_ops/biasaddv1\">BiasAddV1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Bincount\" href=\"raw_ops/bincount\">Bincount</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Bitcast\" href=\"raw_ops/bitcast\">Bitcast</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BitwiseAnd\" href=\"raw_ops/bitwiseand\">BitwiseAnd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BitwiseOr\" href=\"raw_ops/bitwiseor\">BitwiseOr</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BitwiseXor\" href=\"raw_ops/bitwisexor\">BitwiseXor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BlockLSTM\" href=\"raw_ops/blocklstm\">BlockLSTM</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BlockLSTMGrad\" href=\"raw_ops/blocklstmgrad\">BlockLSTMGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BlockLSTMGradV2\" href=\"raw_ops/blocklstmgradv2\">BlockLSTMGradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BlockLSTMV2\" href=\"raw_ops/blocklstmv2\">BlockLSTMV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BoostedTreesAggregateStats\" href=\"raw_ops/boostedtreesaggregatestats\">BoostedTreesAggregateStats</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesBucketize\" href=\"raw_ops/boostedtreesbucketize\">BoostedTreesBucketize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCalculateBestFeatureSplit\" href=\"raw_ops/boostedtreescalculatebestfeaturesplit\">BoostedTreesCalculateBestFeatureSplit</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCalculateBestFeatureSplitV2\" href=\"raw_ops/boostedtreescalculatebestfeaturesplitv2\">BoostedTreesCalculateBestFeatureSplitV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCalculateBestGainsPerFeature\" href=\"raw_ops/boostedtreescalculatebestgainsperfeature\">BoostedTreesCalculateBestGainsPerFeature</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCenterBias\" href=\"raw_ops/boostedtreescenterbias\">BoostedTreesCenterBias</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCreateEnsemble\" href=\"raw_ops/boostedtreescreateensemble\">BoostedTreesCreateEnsemble</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesCreateQuantileStreamResource\" href=\"raw_ops/boostedtreescreatequantilestreamresource\">BoostedTreesCreateQuantileStreamResource</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesDeserializeEnsemble\" href=\"raw_ops/boostedtreesdeserializeensemble\">BoostedTreesDeserializeEnsemble</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesEnsembleResourceHandleOp\" href=\"raw_ops/boostedtreesensembleresourcehandleop\">BoostedTreesEnsembleResourceHandleOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesExampleDebugOutputs\" href=\"raw_ops/boostedtreesexampledebugoutputs\">BoostedTreesExampleDebugOutputs</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesFlushQuantileSummaries\" href=\"raw_ops/boostedtreesflushquantilesummaries\">BoostedTreesFlushQuantileSummaries</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesGetEnsembleStates\" href=\"raw_ops/boostedtreesgetensemblestates\">BoostedTreesGetEnsembleStates</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesMakeQuantileSummaries\" href=\"raw_ops/boostedtreesmakequantilesummaries\">BoostedTreesMakeQuantileSummaries</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesMakeStatsSummary\" href=\"raw_ops/boostedtreesmakestatssummary\">BoostedTreesMakeStatsSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesPredict\" href=\"raw_ops/boostedtreespredict\">BoostedTreesPredict</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesQuantileStreamResourceAddSummaries\" href=\"raw_ops/boostedtreesquantilestreamresourceaddsummaries\">BoostedTreesQuantileStreamResourceAddSummaries</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesQuantileStreamResourceDeserialize\" href=\"raw_ops/boostedtreesquantilestreamresourcedeserialize\">BoostedTreesQuantileStreamResourceDeserialize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesQuantileStreamResourceFlush\" href=\"raw_ops/boostedtreesquantilestreamresourceflush\">BoostedTreesQuantileStreamResourceFlush</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesQuantileStreamResourceGetBucketBoundaries\" href=\"raw_ops/boostedtreesquantilestreamresourcegetbucketboundaries\">BoostedTreesQuantileStreamResourceGetBucketBoundaries</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesQuantileStreamResourceHandleOp\" href=\"raw_ops/boostedtreesquantilestreamresourcehandleop\">BoostedTreesQuantileStreamResourceHandleOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesSerializeEnsemble\" href=\"raw_ops/boostedtreesserializeensemble\">BoostedTreesSerializeEnsemble</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesSparseAggregateStats\" href=\"raw_ops/boostedtreessparseaggregatestats\">BoostedTreesSparseAggregateStats</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesSparseCalculateBestFeatureSplit\" href=\"raw_ops/boostedtreessparsecalculatebestfeaturesplit\">BoostedTreesSparseCalculateBestFeatureSplit</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesTrainingPredict\" href=\"raw_ops/boostedtreestrainingpredict\">BoostedTreesTrainingPredict</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesUpdateEnsemble\" href=\"raw_ops/boostedtreesupdateensemble\">BoostedTreesUpdateEnsemble</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BoostedTreesUpdateEnsembleV2\" href=\"raw_ops/boostedtreesupdateensemblev2\">BoostedTreesUpdateEnsembleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BroadcastArgs\" href=\"raw_ops/broadcastargs\">BroadcastArgs</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BroadcastGradientArgs\" href=\"raw_ops/broadcastgradientargs\">BroadcastGradientArgs</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"BroadcastTo\" href=\"raw_ops/broadcastto\">BroadcastTo</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Bucketize\" href=\"raw_ops/bucketize\">Bucketize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"BytesProducedStatsDataset\" href=\"raw_ops/bytesproducedstatsdataset\">BytesProducedStatsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CSRSparseMatrixComponents\" href=\"raw_ops/csrsparsematrixcomponents\">CSRSparseMatrixComponents</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CSRSparseMatrixToDense\" href=\"raw_ops/csrsparsematrixtodense\">CSRSparseMatrixToDense</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CSRSparseMatrixToSparseTensor\" href=\"raw_ops/csrsparsematrixtosparsetensor\">CSRSparseMatrixToSparseTensor</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CSVDataset\" href=\"raw_ops/csvdataset\">CSVDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CSVDatasetV2\" href=\"raw_ops/csvdatasetv2\">CSVDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CTCBeamSearchDecoder\" href=\"raw_ops/ctcbeamsearchdecoder\">CTCBeamSearchDecoder</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CTCGreedyDecoder\" href=\"raw_ops/ctcgreedydecoder\">CTCGreedyDecoder</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CTCLoss\" href=\"raw_ops/ctcloss\">CTCLoss</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CTCLossV2\" href=\"raw_ops/ctclossv2\">CTCLossV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CacheDataset\" href=\"raw_ops/cachedataset\">CacheDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CacheDatasetV2\" href=\"raw_ops/cachedatasetv2\">CacheDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Case\" href=\"raw_ops/case\">Case</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Cast\" href=\"raw_ops/cast\">Cast</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Ceil\" href=\"raw_ops/ceil\">Ceil</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CheckNumerics\" href=\"raw_ops/checknumerics\">CheckNumerics</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CheckNumericsV2\" href=\"raw_ops/checknumericsv2\">CheckNumericsV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Cholesky\" href=\"raw_ops/cholesky\">Cholesky</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CholeskyGrad\" href=\"raw_ops/choleskygrad\">CholeskyGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ChooseFastestBranchDataset\" href=\"raw_ops/choosefastestbranchdataset\">ChooseFastestBranchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ChooseFastestDataset\" href=\"raw_ops/choosefastestdataset\">ChooseFastestDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ClipByValue\" href=\"raw_ops/clipbyvalue\">ClipByValue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CloseSummaryWriter\" href=\"raw_ops/closesummarywriter\">CloseSummaryWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectiveBcastRecv\" href=\"raw_ops/collectivebcastrecv\">CollectiveBcastRecv</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectiveBcastSend\" href=\"raw_ops/collectivebcastsend\">CollectiveBcastSend</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectiveGather\" href=\"raw_ops/collectivegather\">CollectiveGather</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectiveGatherV2\" href=\"raw_ops/collectivegatherv2\">CollectiveGatherV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectivePermute\" href=\"raw_ops/collectivepermute\">CollectivePermute</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CollectiveReduce\" href=\"raw_ops/collectivereduce\">CollectiveReduce</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CollectiveReduceV2\" href=\"raw_ops/collectivereducev2\">CollectiveReduceV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CombinedNonMaxSuppression\" href=\"raw_ops/combinednonmaxsuppression\">CombinedNonMaxSuppression</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CompareAndBitpack\" href=\"raw_ops/compareandbitpack\">CompareAndBitpack</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Complex\" href=\"raw_ops/complex\">Complex</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ComplexAbs\" href=\"raw_ops/complexabs\">ComplexAbs</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CompressElement\" href=\"raw_ops/compresselement\">CompressElement</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ComputeAccidentalHits\" href=\"raw_ops/computeaccidentalhits\">ComputeAccidentalHits</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ComputeBatchSize\" href=\"raw_ops/computebatchsize\">ComputeBatchSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Concat\" href=\"raw_ops/concat\">Concat</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ConcatOffset\" href=\"raw_ops/concatoffset\">ConcatOffset</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ConcatV2\" href=\"raw_ops/concatv2\">ConcatV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ConcatenateDataset\" href=\"raw_ops/concatenatedataset\">ConcatenateDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ConditionalAccumulator\" href=\"raw_ops/conditionalaccumulator\">ConditionalAccumulator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ConfigureDistributedTPU\" href=\"raw_ops/configuredistributedtpu\">ConfigureDistributedTPU</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ConfigureTPUEmbedding\" href=\"raw_ops/configuretpuembedding\">ConfigureTPUEmbedding</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Conj\" href=\"raw_ops/conj\">Conj</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ConjugateTranspose\" href=\"raw_ops/conjugatetranspose\">ConjugateTranspose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Const\" href=\"raw_ops/const\">Const</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ConsumeMutexLock\" href=\"raw_ops/consumemutexlock\">ConsumeMutexLock</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ControlTrigger\" href=\"raw_ops/controltrigger\">ControlTrigger</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Conv2D\" href=\"raw_ops/conv2d\">Conv2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Conv2DBackpropFilter\" href=\"raw_ops/conv2dbackpropfilter\">Conv2DBackpropFilter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Conv2DBackpropInput\" href=\"raw_ops/conv2dbackpropinput\">Conv2DBackpropInput</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Conv3D\" href=\"raw_ops/conv3d\">Conv3D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Conv3DBackpropFilter\" href=\"raw_ops/conv3dbackpropfilter\">Conv3DBackpropFilter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Conv3DBackpropFilterV2\" href=\"raw_ops/conv3dbackpropfilterv2\">Conv3DBackpropFilterV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Conv3DBackpropInput\" href=\"raw_ops/conv3dbackpropinput\">Conv3DBackpropInput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Conv3DBackpropInputV2\" href=\"raw_ops/conv3dbackpropinputv2\">Conv3DBackpropInputV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Copy\" href=\"raw_ops/copy\">Copy</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CopyHost\" href=\"raw_ops/copyhost\">CopyHost</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Cos\" href=\"raw_ops/cos\">Cos</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Cosh\" href=\"raw_ops/cosh\">Cosh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CountUpTo\" href=\"raw_ops/countupto\">CountUpTo</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CreateSummaryDbWriter\" href=\"raw_ops/createsummarydbwriter\">CreateSummaryDbWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CreateSummaryFileWriter\" href=\"raw_ops/createsummaryfilewriter\">CreateSummaryFileWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CropAndResize\" href=\"raw_ops/cropandresize\">CropAndResize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CropAndResizeGradBoxes\" href=\"raw_ops/cropandresizegradboxes\">CropAndResizeGradBoxes</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CropAndResizeGradImage\" href=\"raw_ops/cropandresizegradimage\">CropAndResizeGradImage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Cross\" href=\"raw_ops/cross\">Cross</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CrossReplicaSum\" href=\"raw_ops/crossreplicasum\">CrossReplicaSum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CudnnRNN\" href=\"raw_ops/cudnnrnn\">CudnnRNN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CudnnRNNBackprop\" href=\"raw_ops/cudnnrnnbackprop\">CudnnRNNBackprop</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNBackpropV2\" href=\"raw_ops/cudnnrnnbackpropv2\">CudnnRNNBackpropV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNBackpropV3\" href=\"raw_ops/cudnnrnnbackpropv3\">CudnnRNNBackpropV3</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNCanonicalToParams\" href=\"raw_ops/cudnnrnncanonicaltoparams\">CudnnRNNCanonicalToParams</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNCanonicalToParamsV2\" href=\"raw_ops/cudnnrnncanonicaltoparamsv2\">CudnnRNNCanonicalToParamsV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNParamsSize\" href=\"raw_ops/cudnnrnnparamssize\">CudnnRNNParamsSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNParamsToCanonical\" href=\"raw_ops/cudnnrnnparamstocanonical\">CudnnRNNParamsToCanonical</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNParamsToCanonicalV2\" href=\"raw_ops/cudnnrnnparamstocanonicalv2\">CudnnRNNParamsToCanonicalV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"CudnnRNNV2\" href=\"raw_ops/cudnnrnnv2\">CudnnRNNV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CudnnRNNV3\" href=\"raw_ops/cudnnrnnv3\">CudnnRNNV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Cumprod\" href=\"raw_ops/cumprod\">Cumprod</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Cumsum\" href=\"raw_ops/cumsum\">Cumsum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"CumulativeLogsumexp\" href=\"raw_ops/cumulativelogsumexp\">CumulativeLogsumexp</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DataFormatDimMap\" href=\"raw_ops/dataformatdimmap\">DataFormatDimMap</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DataFormatVecPermute\" href=\"raw_ops/dataformatvecpermute\">DataFormatVecPermute</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DataServiceDataset\" href=\"raw_ops/dataservicedataset\">DataServiceDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetCardinality\" href=\"raw_ops/datasetcardinality\">DatasetCardinality</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetFromGraph\" href=\"raw_ops/datasetfromgraph\">DatasetFromGraph</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetToGraph\" href=\"raw_ops/datasettograph\">DatasetToGraph</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetToGraphV2\" href=\"raw_ops/datasettographv2\">DatasetToGraphV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetToSingleElement\" href=\"raw_ops/datasettosingleelement\">DatasetToSingleElement</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DatasetToTFRecord\" href=\"raw_ops/datasettotfrecord\">DatasetToTFRecord</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Dawsn\" href=\"raw_ops/dawsn\">Dawsn</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DebugGradientIdentity\" href=\"raw_ops/debuggradientidentity\">DebugGradientIdentity</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DebugGradientRefIdentity\" href=\"raw_ops/debuggradientrefidentity\">DebugGradientRefIdentity</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DebugIdentity\" href=\"raw_ops/debugidentity\">DebugIdentity</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DebugIdentityV2\" href=\"raw_ops/debugidentityv2\">DebugIdentityV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DebugNanCount\" href=\"raw_ops/debugnancount\">DebugNanCount</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DebugNumericSummary\" href=\"raw_ops/debugnumericsummary\">DebugNumericSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DebugNumericSummaryV2\" href=\"raw_ops/debugnumericsummaryv2\">DebugNumericSummaryV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeAndCropJpeg\" href=\"raw_ops/decodeandcropjpeg\">DecodeAndCropJpeg</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeBase64\" href=\"raw_ops/decodebase64\">DecodeBase64</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DecodeBmp\" href=\"raw_ops/decodebmp\">DecodeBmp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeCSV\" href=\"raw_ops/decodecsv\">DecodeCSV</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeCompressed\" href=\"raw_ops/decodecompressed\">DecodeCompressed</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeGif\" href=\"raw_ops/decodegif\">DecodeGif</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeImage\" href=\"raw_ops/decodeimage\">DecodeImage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeJSONExample\" href=\"raw_ops/decodejsonexample\">DecodeJSONExample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeJpeg\" href=\"raw_ops/decodejpeg\">DecodeJpeg</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodePaddedRaw\" href=\"raw_ops/decodepaddedraw\">DecodePaddedRaw</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DecodePng\" href=\"raw_ops/decodepng\">DecodePng</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DecodeProtoV2\" href=\"raw_ops/decodeprotov2\">DecodeProtoV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DecodeRaw\" href=\"raw_ops/decoderaw\">DecodeRaw</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DecodeWav\" href=\"raw_ops/decodewav\">DecodeWav</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeepCopy\" href=\"raw_ops/deepcopy\">DeepCopy</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteIterator\" href=\"raw_ops/deleteiterator\">DeleteIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteMemoryCache\" href=\"raw_ops/deletememorycache\">DeleteMemoryCache</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteMultiDeviceIterator\" href=\"raw_ops/deletemultideviceiterator\">DeleteMultiDeviceIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteRandomSeedGenerator\" href=\"raw_ops/deleterandomseedgenerator\">DeleteRandomSeedGenerator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteSeedGenerator\" href=\"raw_ops/deleteseedgenerator\">DeleteSeedGenerator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeleteSessionTensor\" href=\"raw_ops/deletesessiontensor\">DeleteSessionTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DenseBincount\" href=\"raw_ops/densebincount\">DenseBincount</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DenseCountSparseOutput\" href=\"raw_ops/densecountsparseoutput\">DenseCountSparseOutput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DenseToCSRSparseMatrix\" href=\"raw_ops/densetocsrsparsematrix\">DenseToCSRSparseMatrix</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DenseToDenseSetOperation\" href=\"raw_ops/densetodensesetoperation\">DenseToDenseSetOperation</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DenseToSparseBatchDataset\" href=\"raw_ops/densetosparsebatchdataset\">DenseToSparseBatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DenseToSparseSetOperation\" href=\"raw_ops/densetosparsesetoperation\">DenseToSparseSetOperation</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DepthToSpace\" href=\"raw_ops/depthtospace\">DepthToSpace</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DepthwiseConv2dNative\" href=\"raw_ops/depthwiseconv2dnative\">DepthwiseConv2dNative</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DepthwiseConv2dNativeBackpropFilter\" href=\"raw_ops/depthwiseconv2dnativebackpropfilter\">DepthwiseConv2dNativeBackpropFilter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DepthwiseConv2dNativeBackpropInput\" href=\"raw_ops/depthwiseconv2dnativebackpropinput\">DepthwiseConv2dNativeBackpropInput</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Dequantize\" href=\"raw_ops/dequantize\">Dequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeserializeIterator\" href=\"raw_ops/deserializeiterator\">DeserializeIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeserializeManySparse\" href=\"raw_ops/deserializemanysparse\">DeserializeManySparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeserializeSparse\" href=\"raw_ops/deserializesparse\">DeserializeSparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DestroyResourceOp\" href=\"raw_ops/destroyresourceop\">DestroyResourceOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DestroyTemporaryVariable\" href=\"raw_ops/destroytemporaryvariable\">DestroyTemporaryVariable</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DeviceIndex\" href=\"raw_ops/deviceindex\">DeviceIndex</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Diag\" href=\"raw_ops/diag\">Diag</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DiagPart\" href=\"raw_ops/diagpart\">DiagPart</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Digamma\" href=\"raw_ops/digamma\">Digamma</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Dilation2D\" href=\"raw_ops/dilation2d\">Dilation2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Dilation2DBackpropFilter\" href=\"raw_ops/dilation2dbackpropfilter\">Dilation2DBackpropFilter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Dilation2DBackpropInput\" href=\"raw_ops/dilation2dbackpropinput\">Dilation2DBackpropInput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DirectedInterleaveDataset\" href=\"raw_ops/directedinterleavedataset\">DirectedInterleaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Div\" href=\"raw_ops/div\">Div</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DivNoNan\" href=\"raw_ops/divnonan\">DivNoNan</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DrawBoundingBoxes\" href=\"raw_ops/drawboundingboxes\">DrawBoundingBoxes</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DrawBoundingBoxesV2\" href=\"raw_ops/drawboundingboxesv2\">DrawBoundingBoxesV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DummyIterationCounter\" href=\"raw_ops/dummyiterationcounter\">DummyIterationCounter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DummyMemoryCache\" href=\"raw_ops/dummymemorycache\">DummyMemoryCache</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DummySeedGenerator\" href=\"raw_ops/dummyseedgenerator\">DummySeedGenerator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"DynamicPartition\" href=\"raw_ops/dynamicpartition\">DynamicPartition</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"DynamicStitch\" href=\"raw_ops/dynamicstitch\">DynamicStitch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EagerPyFunc\" href=\"raw_ops/eagerpyfunc\">EagerPyFunc</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EditDistance\" href=\"raw_ops/editdistance\">EditDistance</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Eig\" href=\"raw_ops/eig\">Eig</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Einsum\" href=\"raw_ops/einsum\">Einsum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Elu\" href=\"raw_ops/elu\">Elu</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EluGrad\" href=\"raw_ops/elugrad\">EluGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Empty\" href=\"raw_ops/empty\">Empty</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EmptyTensorList\" href=\"raw_ops/emptytensorlist\">EmptyTensorList</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EncodeBase64\" href=\"raw_ops/encodebase64\">EncodeBase64</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EncodeJpeg\" href=\"raw_ops/encodejpeg\">EncodeJpeg</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EncodeJpegVariableQuality\" href=\"raw_ops/encodejpegvariablequality\">EncodeJpegVariableQuality</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EncodePng\" href=\"raw_ops/encodepng\">EncodePng</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EncodeProto\" href=\"raw_ops/encodeproto\">EncodeProto</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EncodeWav\" href=\"raw_ops/encodewav\">EncodeWav</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EnqueueTPUEmbeddingIntegerBatch\" href=\"raw_ops/enqueuetpuembeddingintegerbatch\">EnqueueTPUEmbeddingIntegerBatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EnqueueTPUEmbeddingRaggedTensorBatch\" href=\"raw_ops/enqueuetpuembeddingraggedtensorbatch\">EnqueueTPUEmbeddingRaggedTensorBatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EnqueueTPUEmbeddingSparseBatch\" href=\"raw_ops/enqueuetpuembeddingsparsebatch\">EnqueueTPUEmbeddingSparseBatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EnqueueTPUEmbeddingSparseTensorBatch\" href=\"raw_ops/enqueuetpuembeddingsparsetensorbatch\">EnqueueTPUEmbeddingSparseTensorBatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"EnsureShape\" href=\"raw_ops/ensureshape\">EnsureShape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Enter\" href=\"raw_ops/enter\">Enter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Equal\" href=\"raw_ops/equal\">Equal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Erf\" href=\"raw_ops/erf\">Erf</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Erfc\" href=\"raw_ops/erfc\">Erfc</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Erfinv\" href=\"raw_ops/erfinv\">Erfinv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"EuclideanNorm\" href=\"raw_ops/euclideannorm\">EuclideanNorm</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Exit\" href=\"raw_ops/exit\">Exit</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Exp\" href=\"raw_ops/exp\">Exp</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ExpandDims\" href=\"raw_ops/expanddims\">ExpandDims</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ExperimentalAssertNextDataset\" href=\"raw_ops/experimentalassertnextdataset\">ExperimentalAssertNextDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalAutoShardDataset\" href=\"raw_ops/experimentalautosharddataset\">ExperimentalAutoShardDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalBytesProducedStatsDataset\" href=\"raw_ops/experimentalbytesproducedstatsdataset\">ExperimentalBytesProducedStatsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalCSVDataset\" href=\"raw_ops/experimentalcsvdataset\">ExperimentalCSVDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalChooseFastestDataset\" href=\"raw_ops/experimentalchoosefastestdataset\">ExperimentalChooseFastestDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalDatasetCardinality\" href=\"raw_ops/experimentaldatasetcardinality\">ExperimentalDatasetCardinality</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalDatasetToTFRecord\" href=\"raw_ops/experimentaldatasettotfrecord\">ExperimentalDatasetToTFRecord</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalDenseToSparseBatchDataset\" href=\"raw_ops/experimentaldensetosparsebatchdataset\">ExperimentalDenseToSparseBatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalDirectedInterleaveDataset\" href=\"raw_ops/experimentaldirectedinterleavedataset\">ExperimentalDirectedInterleaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalGroupByReducerDataset\" href=\"raw_ops/experimentalgroupbyreducerdataset\">ExperimentalGroupByReducerDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalGroupByWindowDataset\" href=\"raw_ops/experimentalgroupbywindowdataset\">ExperimentalGroupByWindowDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalIgnoreErrorsDataset\" href=\"raw_ops/experimentalignoreerrorsdataset\">ExperimentalIgnoreErrorsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalIteratorGetDevice\" href=\"raw_ops/experimentaliteratorgetdevice\">ExperimentalIteratorGetDevice</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalLMDBDataset\" href=\"raw_ops/experimentallmdbdataset\">ExperimentalLMDBDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalLatencyStatsDataset\" href=\"raw_ops/experimentallatencystatsdataset\">ExperimentalLatencyStatsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalMapAndBatchDataset\" href=\"raw_ops/experimentalmapandbatchdataset\">ExperimentalMapAndBatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalMapDataset\" href=\"raw_ops/experimentalmapdataset\">ExperimentalMapDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalMatchingFilesDataset\" href=\"raw_ops/experimentalmatchingfilesdataset\">ExperimentalMatchingFilesDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalMaxIntraOpParallelismDataset\" href=\"raw_ops/experimentalmaxintraopparallelismdataset\">ExperimentalMaxIntraOpParallelismDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalNonSerializableDataset\" href=\"raw_ops/experimentalnonserializabledataset\">ExperimentalNonSerializableDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalParallelInterleaveDataset\" href=\"raw_ops/experimentalparallelinterleavedataset\">ExperimentalParallelInterleaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalParseExampleDataset\" href=\"raw_ops/experimentalparseexampledataset\">ExperimentalParseExampleDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalPrivateThreadPoolDataset\" href=\"raw_ops/experimentalprivatethreadpooldataset\">ExperimentalPrivateThreadPoolDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalRandomDataset\" href=\"raw_ops/experimentalrandomdataset\">ExperimentalRandomDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalRebatchDataset\" href=\"raw_ops/experimentalrebatchdataset\">ExperimentalRebatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalScanDataset\" href=\"raw_ops/experimentalscandataset\">ExperimentalScanDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalSetStatsAggregatorDataset\" href=\"raw_ops/experimentalsetstatsaggregatordataset\">ExperimentalSetStatsAggregatorDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalSleepDataset\" href=\"raw_ops/experimentalsleepdataset\">ExperimentalSleepDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalSlidingWindowDataset\" href=\"raw_ops/experimentalslidingwindowdataset\">ExperimentalSlidingWindowDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalSqlDataset\" href=\"raw_ops/experimentalsqldataset\">ExperimentalSqlDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalStatsAggregatorHandle\" href=\"raw_ops/experimentalstatsaggregatorhandle\">ExperimentalStatsAggregatorHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalStatsAggregatorSummary\" href=\"raw_ops/experimentalstatsaggregatorsummary\">ExperimentalStatsAggregatorSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalTakeWhileDataset\" href=\"raw_ops/experimentaltakewhiledataset\">ExperimentalTakeWhileDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalThreadPoolDataset\" href=\"raw_ops/experimentalthreadpooldataset\">ExperimentalThreadPoolDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalThreadPoolHandle\" href=\"raw_ops/experimentalthreadpoolhandle\">ExperimentalThreadPoolHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalUnbatchDataset\" href=\"raw_ops/experimentalunbatchdataset\">ExperimentalUnbatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExperimentalUniqueDataset\" href=\"raw_ops/experimentaluniquedataset\">ExperimentalUniqueDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Expint\" href=\"raw_ops/expint\">Expint</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Expm1\" href=\"raw_ops/expm1\">Expm1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ExtractGlimpse\" href=\"raw_ops/extractglimpse\">ExtractGlimpse</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ExtractGlimpseV2\" href=\"raw_ops/extractglimpsev2\">ExtractGlimpseV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExtractImagePatches\" href=\"raw_ops/extractimagepatches\">ExtractImagePatches</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ExtractJpegShape\" href=\"raw_ops/extractjpegshape\">ExtractJpegShape</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ExtractVolumePatches\" href=\"raw_ops/extractvolumepatches\">ExtractVolumePatches</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FFT\" href=\"raw_ops/fft\">FFT</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FFT2D\" href=\"raw_ops/fft2d\">FFT2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FFT3D\" href=\"raw_ops/fft3d\">FFT3D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FIFOQueue\" href=\"raw_ops/fifoqueue\">FIFOQueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FIFOQueueV2\" href=\"raw_ops/fifoqueuev2\">FIFOQueueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Fact\" href=\"raw_ops/fact\">Fact</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FakeParam\" href=\"raw_ops/fakeparam\">FakeParam</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxArgs\" href=\"raw_ops/fakequantwithminmaxargs\">FakeQuantWithMinMaxArgs</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxArgsGradient\" href=\"raw_ops/fakequantwithminmaxargsgradient\">FakeQuantWithMinMaxArgsGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxVars\" href=\"raw_ops/fakequantwithminmaxvars\">FakeQuantWithMinMaxVars</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxVarsGradient\" href=\"raw_ops/fakequantwithminmaxvarsgradient\">FakeQuantWithMinMaxVarsGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxVarsPerChannel\" href=\"raw_ops/fakequantwithminmaxvarsperchannel\">FakeQuantWithMinMaxVarsPerChannel</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FakeQuantWithMinMaxVarsPerChannelGradient\" href=\"raw_ops/fakequantwithminmaxvarsperchannelgradient\">FakeQuantWithMinMaxVarsPerChannelGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FakeQueue\" href=\"raw_ops/fakequeue\">FakeQueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Fill\" href=\"raw_ops/fill\">Fill</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FilterByLastComponentDataset\" href=\"raw_ops/filterbylastcomponentdataset\">FilterByLastComponentDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FilterDataset\" href=\"raw_ops/filterdataset\">FilterDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Fingerprint\" href=\"raw_ops/fingerprint\">Fingerprint</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FixedLengthRecordDataset\" href=\"raw_ops/fixedlengthrecorddataset\">FixedLengthRecordDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FixedLengthRecordDatasetV2\" href=\"raw_ops/fixedlengthrecorddatasetv2\">FixedLengthRecordDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FixedLengthRecordReader\" href=\"raw_ops/fixedlengthrecordreader\">FixedLengthRecordReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FixedLengthRecordReaderV2\" href=\"raw_ops/fixedlengthrecordreaderv2\">FixedLengthRecordReaderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FixedUnigramCandidateSampler\" href=\"raw_ops/fixedunigramcandidatesampler\">FixedUnigramCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FlatMapDataset\" href=\"raw_ops/flatmapdataset\">FlatMapDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Floor\" href=\"raw_ops/floor\">Floor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FloorDiv\" href=\"raw_ops/floordiv\">FloorDiv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FloorMod\" href=\"raw_ops/floormod\">FloorMod</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FlushSummaryWriter\" href=\"raw_ops/flushsummarywriter\">FlushSummaryWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"For\" href=\"raw_ops/for\">For</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FractionalAvgPool\" href=\"raw_ops/fractionalavgpool\">FractionalAvgPool</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FractionalAvgPoolGrad\" href=\"raw_ops/fractionalavgpoolgrad\">FractionalAvgPoolGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FractionalMaxPool\" href=\"raw_ops/fractionalmaxpool\">FractionalMaxPool</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FractionalMaxPoolGrad\" href=\"raw_ops/fractionalmaxpoolgrad\">FractionalMaxPoolGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FresnelCos\" href=\"raw_ops/fresnelcos\">FresnelCos</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FresnelSin\" href=\"raw_ops/fresnelsin\">FresnelSin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNorm\" href=\"raw_ops/fusedbatchnorm\">FusedBatchNorm</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNormGrad\" href=\"raw_ops/fusedbatchnormgrad\">FusedBatchNormGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNormGradV2\" href=\"raw_ops/fusedbatchnormgradv2\">FusedBatchNormGradV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNormGradV3\" href=\"raw_ops/fusedbatchnormgradv3\">FusedBatchNormGradV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNormV2\" href=\"raw_ops/fusedbatchnormv2\">FusedBatchNormV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedBatchNormV3\" href=\"raw_ops/fusedbatchnormv3\">FusedBatchNormV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"FusedPadConv2D\" href=\"raw_ops/fusedpadconv2d\">FusedPadConv2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"FusedResizeAndPadConv2D\" href=\"raw_ops/fusedresizeandpadconv2d\">FusedResizeAndPadConv2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"GRUBlockCell\" href=\"raw_ops/grublockcell\">GRUBlockCell</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"GRUBlockCellGrad\" href=\"raw_ops/grublockcellgrad\">GRUBlockCellGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Gather\" href=\"raw_ops/gather\">Gather</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GatherNd\" href=\"raw_ops/gathernd\">GatherNd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GatherV2\" href=\"raw_ops/gatherv2\">GatherV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GenerateBoundingBoxProposals\" href=\"raw_ops/generateboundingboxproposals\">GenerateBoundingBoxProposals</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GenerateVocabRemapping\" href=\"raw_ops/generatevocabremapping\">GenerateVocabRemapping</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GeneratorDataset\" href=\"raw_ops/generatordataset\">GeneratorDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"GetSessionHandle\" href=\"raw_ops/getsessionhandle\">GetSessionHandle</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GetSessionHandleV2\" href=\"raw_ops/getsessionhandlev2\">GetSessionHandleV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GetSessionTensor\" href=\"raw_ops/getsessiontensor\">GetSessionTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Greater\" href=\"raw_ops/greater\">Greater</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GreaterEqual\" href=\"raw_ops/greaterequal\">GreaterEqual</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"GroupByReducerDataset\" href=\"raw_ops/groupbyreducerdataset\">GroupByReducerDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"GroupByWindowDataset\" href=\"raw_ops/groupbywindowdataset\">GroupByWindowDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"GuaranteeConst\" href=\"raw_ops/guaranteeconst\">GuaranteeConst</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"HSVToRGB\" href=\"raw_ops/hsvtorgb\">HSVToRGB</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"HashTable\" href=\"raw_ops/hashtable\">HashTable</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"HashTableV2\" href=\"raw_ops/hashtablev2\">HashTableV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"HistogramFixedWidth\" href=\"raw_ops/histogramfixedwidth\">HistogramFixedWidth</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"HistogramSummary\" href=\"raw_ops/histogramsummary\">HistogramSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IFFT\" href=\"raw_ops/ifft\">IFFT</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IFFT2D\" href=\"raw_ops/ifft2d\">IFFT2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IFFT3D\" href=\"raw_ops/ifft3d\">IFFT3D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IRFFT\" href=\"raw_ops/irfft\">IRFFT</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IRFFT2D\" href=\"raw_ops/irfft2d\">IRFFT2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IRFFT3D\" href=\"raw_ops/irfft3d\">IRFFT3D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Identity\" href=\"raw_ops/identity\">Identity</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IdentityN\" href=\"raw_ops/identityn\">IdentityN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IdentityReader\" href=\"raw_ops/identityreader\">IdentityReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IdentityReaderV2\" href=\"raw_ops/identityreaderv2\">IdentityReaderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"If\" href=\"raw_ops/if\">If</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Igamma\" href=\"raw_ops/igamma\">Igamma</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IgammaGradA\" href=\"raw_ops/igammagrada\">IgammaGradA</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Igammac\" href=\"raw_ops/igammac\">Igammac</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IgnoreErrorsDataset\" href=\"raw_ops/ignoreerrorsdataset\">IgnoreErrorsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Imag\" href=\"raw_ops/imag\">Imag</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ImageProjectiveTransformV2\" href=\"raw_ops/imageprojectivetransformv2\">ImageProjectiveTransformV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ImageProjectiveTransformV3\" href=\"raw_ops/imageprojectivetransformv3\">ImageProjectiveTransformV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ImageSummary\" href=\"raw_ops/imagesummary\">ImageSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ImmutableConst\" href=\"raw_ops/immutableconst\">ImmutableConst</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ImportEvent\" href=\"raw_ops/importevent\">ImportEvent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InTopK\" href=\"raw_ops/intopk\">InTopK</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InTopKV2\" href=\"raw_ops/intopkv2\">InTopKV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InfeedDequeue\" href=\"raw_ops/infeeddequeue\">InfeedDequeue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InfeedDequeueTuple\" href=\"raw_ops/infeeddequeuetuple\">InfeedDequeueTuple</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InfeedEnqueue\" href=\"raw_ops/infeedenqueue\">InfeedEnqueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InfeedEnqueuePrelinearizedBuffer\" href=\"raw_ops/infeedenqueueprelinearizedbuffer\">InfeedEnqueuePrelinearizedBuffer</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InfeedEnqueueTuple\" href=\"raw_ops/infeedenqueuetuple\">InfeedEnqueueTuple</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InitializeTable\" href=\"raw_ops/initializetable\">InitializeTable</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InitializeTableFromDataset\" href=\"raw_ops/initializetablefromdataset\">InitializeTableFromDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InitializeTableFromTextFile\" href=\"raw_ops/initializetablefromtextfile\">InitializeTableFromTextFile</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InitializeTableFromTextFileV2\" href=\"raw_ops/initializetablefromtextfilev2\">InitializeTableFromTextFileV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InitializeTableV2\" href=\"raw_ops/initializetablev2\">InitializeTableV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InplaceAdd\" href=\"raw_ops/inplaceadd\">InplaceAdd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InplaceSub\" href=\"raw_ops/inplacesub\">InplaceSub</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InplaceUpdate\" href=\"raw_ops/inplaceupdate\">InplaceUpdate</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"InterleaveDataset\" href=\"raw_ops/interleavedataset\">InterleaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Inv\" href=\"raw_ops/inv\">Inv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InvGrad\" href=\"raw_ops/invgrad\">InvGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Invert\" href=\"raw_ops/invert\">Invert</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"InvertPermutation\" href=\"raw_ops/invertpermutation\">InvertPermutation</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"IsBoostedTreesEnsembleInitialized\" href=\"raw_ops/isboostedtreesensembleinitialized\">IsBoostedTreesEnsembleInitialized</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsBoostedTreesQuantileStreamResourceInitialized\" href=\"raw_ops/isboostedtreesquantilestreamresourceinitialized\">IsBoostedTreesQuantileStreamResourceInitialized</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsFinite\" href=\"raw_ops/isfinite\">IsFinite</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsInf\" href=\"raw_ops/isinf\">IsInf</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsNan\" href=\"raw_ops/isnan\">IsNan</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsVariableInitialized\" href=\"raw_ops/isvariableinitialized\">IsVariableInitialized</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IsotonicRegression\" href=\"raw_ops/isotonicregression\">IsotonicRegression</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Iterator\" href=\"raw_ops/iterator\">Iterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorFromStringHandle\" href=\"raw_ops/iteratorfromstringhandle\">IteratorFromStringHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorFromStringHandleV2\" href=\"raw_ops/iteratorfromstringhandlev2\">IteratorFromStringHandleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorGetDevice\" href=\"raw_ops/iteratorgetdevice\">IteratorGetDevice</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorGetNext\" href=\"raw_ops/iteratorgetnext\">IteratorGetNext</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorGetNextAsOptional\" href=\"raw_ops/iteratorgetnextasoptional\">IteratorGetNextAsOptional</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorGetNextSync\" href=\"raw_ops/iteratorgetnextsync\">IteratorGetNextSync</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorToStringHandle\" href=\"raw_ops/iteratortostringhandle\">IteratorToStringHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"IteratorV2\" href=\"raw_ops/iteratorv2\">IteratorV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"L2Loss\" href=\"raw_ops/l2loss\">L2Loss</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LMDBDataset\" href=\"raw_ops/lmdbdataset\">LMDBDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LMDBReader\" href=\"raw_ops/lmdbreader\">LMDBReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LRN\" href=\"raw_ops/lrn\">LRN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LRNGrad\" href=\"raw_ops/lrngrad\">LRNGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LSTMBlockCell\" href=\"raw_ops/lstmblockcell\">LSTMBlockCell</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LSTMBlockCellGrad\" href=\"raw_ops/lstmblockcellgrad\">LSTMBlockCellGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LatencyStatsDataset\" href=\"raw_ops/latencystatsdataset\">LatencyStatsDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LeakyRelu\" href=\"raw_ops/leakyrelu\">LeakyRelu</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LeakyReluGrad\" href=\"raw_ops/leakyrelugrad\">LeakyReluGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LearnedUnigramCandidateSampler\" href=\"raw_ops/learnedunigramcandidatesampler\">LearnedUnigramCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LeftShift\" href=\"raw_ops/leftshift\">LeftShift</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LegacyParallelInterleaveDatasetV2\" href=\"raw_ops/legacyparallelinterleavedatasetv2\">LegacyParallelInterleaveDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Less\" href=\"raw_ops/less\">Less</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LessEqual\" href=\"raw_ops/lessequal\">LessEqual</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Lgamma\" href=\"raw_ops/lgamma\">Lgamma</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LinSpace\" href=\"raw_ops/linspace\">LinSpace</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ListDiff\" href=\"raw_ops/listdiff\">ListDiff</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadAndRemapMatrix\" href=\"raw_ops/loadandremapmatrix\">LoadAndRemapMatrix</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LoadDataset\" href=\"raw_ops/loaddataset\">LoadDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingADAMParameters\" href=\"raw_ops/loadtpuembeddingadamparameters\">LoadTPUEmbeddingADAMParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingADAMParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingadamparametersgradaccumdebug\">LoadTPUEmbeddingADAMParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingAdadeltaParameters\" href=\"raw_ops/loadtpuembeddingadadeltaparameters\">LoadTPUEmbeddingAdadeltaParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingAdadeltaParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingadadeltaparametersgradaccumdebug\">LoadTPUEmbeddingAdadeltaParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingAdagradParameters\" href=\"raw_ops/loadtpuembeddingadagradparameters\">LoadTPUEmbeddingAdagradParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingAdagradParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingadagradparametersgradaccumdebug\">LoadTPUEmbeddingAdagradParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingCenteredRMSPropParameters\" href=\"raw_ops/loadtpuembeddingcenteredrmspropparameters\">LoadTPUEmbeddingCenteredRMSPropParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingFTRLParameters\" href=\"raw_ops/loadtpuembeddingftrlparameters\">LoadTPUEmbeddingFTRLParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingFTRLParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingftrlparametersgradaccumdebug\">LoadTPUEmbeddingFTRLParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingMDLAdagradLightParameters\" href=\"raw_ops/loadtpuembeddingmdladagradlightparameters\">LoadTPUEmbeddingMDLAdagradLightParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingMomentumParameters\" href=\"raw_ops/loadtpuembeddingmomentumparameters\">LoadTPUEmbeddingMomentumParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingMomentumParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingmomentumparametersgradaccumdebug\">LoadTPUEmbeddingMomentumParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingProximalAdagradParameters\" href=\"raw_ops/loadtpuembeddingproximaladagradparameters\">LoadTPUEmbeddingProximalAdagradParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingproximaladagradparametersgradaccumdebug\">LoadTPUEmbeddingProximalAdagradParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingProximalYogiParameters\" href=\"raw_ops/loadtpuembeddingproximalyogiparameters\">LoadTPUEmbeddingProximalYogiParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingProximalYogiParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingproximalyogiparametersgradaccumdebug\">LoadTPUEmbeddingProximalYogiParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingRMSPropParameters\" href=\"raw_ops/loadtpuembeddingrmspropparameters\">LoadTPUEmbeddingRMSPropParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingRMSPropParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingrmspropparametersgradaccumdebug\">LoadTPUEmbeddingRMSPropParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingStochasticGradientDescentParameters\" href=\"raw_ops/loadtpuembeddingstochasticgradientdescentparameters\">LoadTPUEmbeddingStochasticGradientDescentParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LoadTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug\" href=\"raw_ops/loadtpuembeddingstochasticgradientdescentparametersgradaccumdebug\">LoadTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Log\" href=\"raw_ops/log\">Log</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Log1p\" href=\"raw_ops/log1p\">Log1p</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LogMatrixDeterminant\" href=\"raw_ops/logmatrixdeterminant\">LogMatrixDeterminant</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LogSoftmax\" href=\"raw_ops/logsoftmax\">LogSoftmax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LogUniformCandidateSampler\" href=\"raw_ops/loguniformcandidatesampler\">LogUniformCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LogicalAnd\" href=\"raw_ops/logicaland\">LogicalAnd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LogicalNot\" href=\"raw_ops/logicalnot\">LogicalNot</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LogicalOr\" href=\"raw_ops/logicalor\">LogicalOr</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableExport\" href=\"raw_ops/lookuptableexport\">LookupTableExport</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LookupTableExportV2\" href=\"raw_ops/lookuptableexportv2\">LookupTableExportV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LookupTableFind\" href=\"raw_ops/lookuptablefind\">LookupTableFind</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableFindV2\" href=\"raw_ops/lookuptablefindv2\">LookupTableFindV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableImport\" href=\"raw_ops/lookuptableimport\">LookupTableImport</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LookupTableImportV2\" href=\"raw_ops/lookuptableimportv2\">LookupTableImportV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LookupTableInsert\" href=\"raw_ops/lookuptableinsert\">LookupTableInsert</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableInsertV2\" href=\"raw_ops/lookuptableinsertv2\">LookupTableInsertV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableRemoveV2\" href=\"raw_ops/lookuptableremovev2\">LookupTableRemoveV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"LookupTableSize\" href=\"raw_ops/lookuptablesize\">LookupTableSize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LookupTableSizeV2\" href=\"raw_ops/lookuptablesizev2\">LookupTableSizeV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LoopCond\" href=\"raw_ops/loopcond\">LoopCond</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"LowerBound\" href=\"raw_ops/lowerbound\">LowerBound</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Lu\" href=\"raw_ops/lu\">Lu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MakeIterator\" href=\"raw_ops/makeiterator\">MakeIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapAndBatchDataset\" href=\"raw_ops/mapandbatchdataset\">MapAndBatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapClear\" href=\"raw_ops/mapclear\">MapClear</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapDataset\" href=\"raw_ops/mapdataset\">MapDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapDefun\" href=\"raw_ops/mapdefun\">MapDefun</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapIncompleteSize\" href=\"raw_ops/mapincompletesize\">MapIncompleteSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapPeek\" href=\"raw_ops/mappeek\">MapPeek</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapSize\" href=\"raw_ops/mapsize\">MapSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapStage\" href=\"raw_ops/mapstage\">MapStage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapUnstage\" href=\"raw_ops/mapunstage\">MapUnstage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MapUnstageNoKey\" href=\"raw_ops/mapunstagenokey\">MapUnstageNoKey</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MatMul\" href=\"raw_ops/matmul\">MatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatchingFiles\" href=\"raw_ops/matchingfiles\">MatchingFiles</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MatchingFilesDataset\" href=\"raw_ops/matchingfilesdataset\">MatchingFilesDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MatrixBandPart\" href=\"raw_ops/matrixbandpart\">MatrixBandPart</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDeterminant\" href=\"raw_ops/matrixdeterminant\">MatrixDeterminant</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiag\" href=\"raw_ops/matrixdiag\">MatrixDiag</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiagPart\" href=\"raw_ops/matrixdiagpart\">MatrixDiagPart</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiagPartV2\" href=\"raw_ops/matrixdiagpartv2\">MatrixDiagPartV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiagPartV3\" href=\"raw_ops/matrixdiagpartv3\">MatrixDiagPartV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiagV2\" href=\"raw_ops/matrixdiagv2\">MatrixDiagV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixDiagV3\" href=\"raw_ops/matrixdiagv3\">MatrixDiagV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixExponential\" href=\"raw_ops/matrixexponential\">MatrixExponential</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MatrixInverse\" href=\"raw_ops/matrixinverse\">MatrixInverse</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixLogarithm\" href=\"raw_ops/matrixlogarithm\">MatrixLogarithm</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MatrixSetDiag\" href=\"raw_ops/matrixsetdiag\">MatrixSetDiag</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixSetDiagV2\" href=\"raw_ops/matrixsetdiagv2\">MatrixSetDiagV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixSetDiagV3\" href=\"raw_ops/matrixsetdiagv3\">MatrixSetDiagV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixSolve\" href=\"raw_ops/matrixsolve\">MatrixSolve</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixSolveLs\" href=\"raw_ops/matrixsolvels\">MatrixSolveLs</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixSquareRoot\" href=\"raw_ops/matrixsquareroot\">MatrixSquareRoot</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MatrixTriangularSolve\" href=\"raw_ops/matrixtriangularsolve\">MatrixTriangularSolve</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Max\" href=\"raw_ops/max\">Max</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxIntraOpParallelismDataset\" href=\"raw_ops/maxintraopparallelismdataset\">MaxIntraOpParallelismDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MaxPool\" href=\"raw_ops/maxpool\">MaxPool</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPool3D\" href=\"raw_ops/maxpool3d\">MaxPool3D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPool3DGrad\" href=\"raw_ops/maxpool3dgrad\">MaxPool3DGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPool3DGradGrad\" href=\"raw_ops/maxpool3dgradgrad\">MaxPool3DGradGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPoolGrad\" href=\"raw_ops/maxpoolgrad\">MaxPoolGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPoolGradGrad\" href=\"raw_ops/maxpoolgradgrad\">MaxPoolGradGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPoolGradGradV2\" href=\"raw_ops/maxpoolgradgradv2\">MaxPoolGradGradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MaxPoolGradGradWithArgmax\" href=\"raw_ops/maxpoolgradgradwithargmax\">MaxPoolGradGradWithArgmax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MaxPoolGradV2\" href=\"raw_ops/maxpoolgradv2\">MaxPoolGradV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPoolGradWithArgmax\" href=\"raw_ops/maxpoolgradwithargmax\">MaxPoolGradWithArgmax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MaxPoolV2\" href=\"raw_ops/maxpoolv2\">MaxPoolV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MaxPoolWithArgmax\" href=\"raw_ops/maxpoolwithargmax\">MaxPoolWithArgmax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Maximum\" href=\"raw_ops/maximum\">Maximum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Mean\" href=\"raw_ops/mean\">Mean</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Merge\" href=\"raw_ops/merge\">Merge</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MergeSummary\" href=\"raw_ops/mergesummary\">MergeSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MergeV2Checkpoints\" href=\"raw_ops/mergev2checkpoints\">MergeV2Checkpoints</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Mfcc\" href=\"raw_ops/mfcc\">Mfcc</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Min\" href=\"raw_ops/min\">Min</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Minimum\" href=\"raw_ops/minimum\">Minimum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MirrorPad\" href=\"raw_ops/mirrorpad\">MirrorPad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MirrorPadGrad\" href=\"raw_ops/mirrorpadgrad\">MirrorPadGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Mod\" href=\"raw_ops/mod\">Mod</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ModelDataset\" href=\"raw_ops/modeldataset\">ModelDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Mul\" href=\"raw_ops/mul\">Mul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MulNoNan\" href=\"raw_ops/mulnonan\">MulNoNan</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MultiDeviceIterator\" href=\"raw_ops/multideviceiterator\">MultiDeviceIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MultiDeviceIteratorFromStringHandle\" href=\"raw_ops/multideviceiteratorfromstringhandle\">MultiDeviceIteratorFromStringHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MultiDeviceIteratorGetNextFromShard\" href=\"raw_ops/multideviceiteratorgetnextfromshard\">MultiDeviceIteratorGetNextFromShard</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MultiDeviceIteratorInit\" href=\"raw_ops/multideviceiteratorinit\">MultiDeviceIteratorInit</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MultiDeviceIteratorToStringHandle\" href=\"raw_ops/multideviceiteratortostringhandle\">MultiDeviceIteratorToStringHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Multinomial\" href=\"raw_ops/multinomial\">Multinomial</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableDenseHashTable\" href=\"raw_ops/mutabledensehashtable\">MutableDenseHashTable</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableDenseHashTableV2\" href=\"raw_ops/mutabledensehashtablev2\">MutableDenseHashTableV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableHashTable\" href=\"raw_ops/mutablehashtable\">MutableHashTable</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableHashTableOfTensors\" href=\"raw_ops/mutablehashtableoftensors\">MutableHashTableOfTensors</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableHashTableOfTensorsV2\" href=\"raw_ops/mutablehashtableoftensorsv2\">MutableHashTableOfTensorsV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutableHashTableV2\" href=\"raw_ops/mutablehashtablev2\">MutableHashTableV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"MutexLock\" href=\"raw_ops/mutexlock\">MutexLock</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"MutexV2\" href=\"raw_ops/mutexv2\">MutexV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NcclAllReduce\" href=\"raw_ops/ncclallreduce\">NcclAllReduce</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NcclBroadcast\" href=\"raw_ops/ncclbroadcast\">NcclBroadcast</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NcclReduce\" href=\"raw_ops/ncclreduce\">NcclReduce</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Ndtri\" href=\"raw_ops/ndtri\">Ndtri</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Neg\" href=\"raw_ops/neg\">Neg</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NextAfter\" href=\"raw_ops/nextafter\">NextAfter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NextIteration\" href=\"raw_ops/nextiteration\">NextIteration</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NoOp\" href=\"raw_ops/noop\">NoOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NonDeterministicInts\" href=\"raw_ops/nondeterministicints\">NonDeterministicInts</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NonMaxSuppression\" href=\"raw_ops/nonmaxsuppression\">NonMaxSuppression</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NonMaxSuppressionV2\" href=\"raw_ops/nonmaxsuppressionv2\">NonMaxSuppressionV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NonMaxSuppressionV3\" href=\"raw_ops/nonmaxsuppressionv3\">NonMaxSuppressionV3</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NonMaxSuppressionV4\" href=\"raw_ops/nonmaxsuppressionv4\">NonMaxSuppressionV4</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NonMaxSuppressionV5\" href=\"raw_ops/nonmaxsuppressionv5\">NonMaxSuppressionV5</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NonMaxSuppressionWithOverlaps\" href=\"raw_ops/nonmaxsuppressionwithoverlaps\">NonMaxSuppressionWithOverlaps</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NonSerializableDataset\" href=\"raw_ops/nonserializabledataset\">NonSerializableDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"NotEqual\" href=\"raw_ops/notequal\">NotEqual</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"NthElement\" href=\"raw_ops/nthelement\">NthElement</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"OneHot\" href=\"raw_ops/onehot\">OneHot</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"OneShotIterator\" href=\"raw_ops/oneshotiterator\">OneShotIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OnesLike\" href=\"raw_ops/oneslike\">OnesLike</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"OptimizeDataset\" href=\"raw_ops/optimizedataset\">OptimizeDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OptimizeDatasetV2\" href=\"raw_ops/optimizedatasetv2\">OptimizeDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OptionalFromValue\" href=\"raw_ops/optionalfromvalue\">OptionalFromValue</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"OptionalGetValue\" href=\"raw_ops/optionalgetvalue\">OptionalGetValue</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"OptionalHasValue\" href=\"raw_ops/optionalhasvalue\">OptionalHasValue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OptionalNone\" href=\"raw_ops/optionalnone\">OptionalNone</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapClear\" href=\"raw_ops/orderedmapclear\">OrderedMapClear</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapIncompleteSize\" href=\"raw_ops/orderedmapincompletesize\">OrderedMapIncompleteSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapPeek\" href=\"raw_ops/orderedmappeek\">OrderedMapPeek</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapSize\" href=\"raw_ops/orderedmapsize\">OrderedMapSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapStage\" href=\"raw_ops/orderedmapstage\">OrderedMapStage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapUnstage\" href=\"raw_ops/orderedmapunstage\">OrderedMapUnstage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OrderedMapUnstageNoKey\" href=\"raw_ops/orderedmapunstagenokey\">OrderedMapUnstageNoKey</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedDequeue\" href=\"raw_ops/outfeeddequeue\">OutfeedDequeue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedDequeueTuple\" href=\"raw_ops/outfeeddequeuetuple\">OutfeedDequeueTuple</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedDequeueTupleV2\" href=\"raw_ops/outfeeddequeuetuplev2\">OutfeedDequeueTupleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedDequeueV2\" href=\"raw_ops/outfeeddequeuev2\">OutfeedDequeueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedEnqueue\" href=\"raw_ops/outfeedenqueue\">OutfeedEnqueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"OutfeedEnqueueTuple\" href=\"raw_ops/outfeedenqueuetuple\">OutfeedEnqueueTuple</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Pack\" href=\"raw_ops/pack\">Pack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Pad\" href=\"raw_ops/pad\">Pad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PadV2\" href=\"raw_ops/padv2\">PadV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PaddedBatchDataset\" href=\"raw_ops/paddedbatchdataset\">PaddedBatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PaddedBatchDatasetV2\" href=\"raw_ops/paddedbatchdatasetv2\">PaddedBatchDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PaddingFIFOQueue\" href=\"raw_ops/paddingfifoqueue\">PaddingFIFOQueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PaddingFIFOQueueV2\" href=\"raw_ops/paddingfifoqueuev2\">PaddingFIFOQueueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelConcat\" href=\"raw_ops/parallelconcat\">ParallelConcat</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelDynamicStitch\" href=\"raw_ops/paralleldynamicstitch\">ParallelDynamicStitch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ParallelInterleaveDataset\" href=\"raw_ops/parallelinterleavedataset\">ParallelInterleaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelInterleaveDatasetV2\" href=\"raw_ops/parallelinterleavedatasetv2\">ParallelInterleaveDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelInterleaveDatasetV3\" href=\"raw_ops/parallelinterleavedatasetv3\">ParallelInterleaveDatasetV3</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelInterleaveDatasetV4\" href=\"raw_ops/parallelinterleavedatasetv4\">ParallelInterleaveDatasetV4</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelMapDataset\" href=\"raw_ops/parallelmapdataset\">ParallelMapDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParallelMapDatasetV2\" href=\"raw_ops/parallelmapdatasetv2\">ParallelMapDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParameterizedTruncatedNormal\" href=\"raw_ops/parameterizedtruncatednormal\">ParameterizedTruncatedNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ParseExample\" href=\"raw_ops/parseexample\">ParseExample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseExampleDataset\" href=\"raw_ops/parseexampledataset\">ParseExampleDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseExampleDatasetV2\" href=\"raw_ops/parseexampledatasetv2\">ParseExampleDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseExampleV2\" href=\"raw_ops/parseexamplev2\">ParseExampleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseSequenceExample\" href=\"raw_ops/parsesequenceexample\">ParseSequenceExample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseSequenceExampleV2\" href=\"raw_ops/parsesequenceexamplev2\">ParseSequenceExampleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseSingleExample\" href=\"raw_ops/parsesingleexample\">ParseSingleExample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseSingleSequenceExample\" href=\"raw_ops/parsesinglesequenceexample\">ParseSingleSequenceExample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ParseTensor\" href=\"raw_ops/parsetensor\">ParseTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PartitionedCall\" href=\"raw_ops/partitionedcall\">PartitionedCall</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Placeholder\" href=\"raw_ops/placeholder\">Placeholder</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PlaceholderV2\" href=\"raw_ops/placeholderv2\">PlaceholderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PlaceholderWithDefault\" href=\"raw_ops/placeholderwithdefault\">PlaceholderWithDefault</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Polygamma\" href=\"raw_ops/polygamma\">Polygamma</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PopulationCount\" href=\"raw_ops/populationcount\">PopulationCount</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Pow\" href=\"raw_ops/pow\">Pow</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PrefetchDataset\" href=\"raw_ops/prefetchdataset\">PrefetchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Prelinearize\" href=\"raw_ops/prelinearize\">Prelinearize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PrelinearizeTuple\" href=\"raw_ops/prelinearizetuple\">PrelinearizeTuple</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PreventGradient\" href=\"raw_ops/preventgradient\">PreventGradient</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Print\" href=\"raw_ops/print\">Print</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PrintV2\" href=\"raw_ops/printv2\">PrintV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PriorityQueue\" href=\"raw_ops/priorityqueue\">PriorityQueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PriorityQueueV2\" href=\"raw_ops/priorityqueuev2\">PriorityQueueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"PrivateThreadPoolDataset\" href=\"raw_ops/privatethreadpooldataset\">PrivateThreadPoolDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Prod\" href=\"raw_ops/prod\">Prod</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PyFunc\" href=\"raw_ops/pyfunc\">PyFunc</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"PyFuncStateless\" href=\"raw_ops/pyfuncstateless\">PyFuncStateless</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Qr\" href=\"raw_ops/qr\">Qr</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeAndDequantize\" href=\"raw_ops/quantizeanddequantize\">QuantizeAndDequantize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeAndDequantizeV2\" href=\"raw_ops/quantizeanddequantizev2\">QuantizeAndDequantizeV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeAndDequantizeV3\" href=\"raw_ops/quantizeanddequantizev3\">QuantizeAndDequantizeV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeAndDequantizeV4\" href=\"raw_ops/quantizeanddequantizev4\">QuantizeAndDequantizeV4</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeAndDequantizeV4Grad\" href=\"raw_ops/quantizeanddequantizev4grad\">QuantizeAndDequantizeV4Grad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QuantizeDownAndShrinkRange\" href=\"raw_ops/quantizedownandshrinkrange\">QuantizeDownAndShrinkRange</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizeV2\" href=\"raw_ops/quantizev2\">QuantizeV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedAdd\" href=\"raw_ops/quantizedadd\">QuantizedAdd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedAvgPool\" href=\"raw_ops/quantizedavgpool\">QuantizedAvgPool</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedBatchNormWithGlobalNormalization\" href=\"raw_ops/quantizedbatchnormwithglobalnormalization\">QuantizedBatchNormWithGlobalNormalization</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedBiasAdd\" href=\"raw_ops/quantizedbiasadd\">QuantizedBiasAdd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConcat\" href=\"raw_ops/quantizedconcat\">QuantizedConcat</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2D\" href=\"raw_ops/quantizedconv2d\">QuantizedConv2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DAndRelu\" href=\"raw_ops/quantizedconv2dandrelu\">QuantizedConv2DAndRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DAndReluAndRequantize\" href=\"raw_ops/quantizedconv2dandreluandrequantize\">QuantizedConv2DAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DAndRequantize\" href=\"raw_ops/quantizedconv2dandrequantize\">QuantizedConv2DAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DPerChannel\" href=\"raw_ops/quantizedconv2dperchannel\">QuantizedConv2DPerChannel</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBias\" href=\"raw_ops/quantizedconv2dwithbias\">QuantizedConv2DWithBias</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasAndRelu\" href=\"raw_ops/quantizedconv2dwithbiasandrelu\">QuantizedConv2DWithBiasAndRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasAndReluAndRequantize\" href=\"raw_ops/quantizedconv2dwithbiasandreluandrequantize\">QuantizedConv2DWithBiasAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasAndRequantize\" href=\"raw_ops/quantizedconv2dwithbiasandrequantize\">QuantizedConv2DWithBiasAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasSignedSumAndReluAndRequantize\" href=\"raw_ops/quantizedconv2dwithbiassignedsumandreluandrequantize\">QuantizedConv2DWithBiasSignedSumAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasSumAndRelu\" href=\"raw_ops/quantizedconv2dwithbiassumandrelu\">QuantizedConv2DWithBiasSumAndRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedConv2DWithBiasSumAndReluAndRequantize\" href=\"raw_ops/quantizedconv2dwithbiassumandreluandrequantize\">QuantizedConv2DWithBiasSumAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedDepthwiseConv2D\" href=\"raw_ops/quantizeddepthwiseconv2d\">QuantizedDepthwiseConv2D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedDepthwiseConv2DWithBias\" href=\"raw_ops/quantizeddepthwiseconv2dwithbias\">QuantizedDepthwiseConv2DWithBias</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedDepthwiseConv2DWithBiasAndRelu\" href=\"raw_ops/quantizeddepthwiseconv2dwithbiasandrelu\">QuantizedDepthwiseConv2DWithBiasAndRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize\" href=\"raw_ops/quantizeddepthwiseconv2dwithbiasandreluandrequantize\">QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedInstanceNorm\" href=\"raw_ops/quantizedinstancenorm\">QuantizedInstanceNorm</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMul\" href=\"raw_ops/quantizedmatmul\">QuantizedMatMul</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMulWithBias\" href=\"raw_ops/quantizedmatmulwithbias\">QuantizedMatMulWithBias</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMulWithBiasAndDequantize\" href=\"raw_ops/quantizedmatmulwithbiasanddequantize\">QuantizedMatMulWithBiasAndDequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMulWithBiasAndRelu\" href=\"raw_ops/quantizedmatmulwithbiasandrelu\">QuantizedMatMulWithBiasAndRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMulWithBiasAndReluAndRequantize\" href=\"raw_ops/quantizedmatmulwithbiasandreluandrequantize\">QuantizedMatMulWithBiasAndReluAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMatMulWithBiasAndRequantize\" href=\"raw_ops/quantizedmatmulwithbiasandrequantize\">QuantizedMatMulWithBiasAndRequantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMaxPool\" href=\"raw_ops/quantizedmaxpool\">QuantizedMaxPool</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedMul\" href=\"raw_ops/quantizedmul\">QuantizedMul</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedRelu\" href=\"raw_ops/quantizedrelu\">QuantizedRelu</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedRelu6\" href=\"raw_ops/quantizedrelu6\">QuantizedRelu6</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedReluX\" href=\"raw_ops/quantizedrelux\">QuantizedReluX</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedReshape\" href=\"raw_ops/quantizedreshape\">QuantizedReshape</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QuantizedResizeBilinear\" href=\"raw_ops/quantizedresizebilinear\">QuantizedResizeBilinear</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueClose\" href=\"raw_ops/queueclose\">QueueClose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueCloseV2\" href=\"raw_ops/queueclosev2\">QueueCloseV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueDequeue\" href=\"raw_ops/queuedequeue\">QueueDequeue</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueDequeueMany\" href=\"raw_ops/queuedequeuemany\">QueueDequeueMany</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueDequeueManyV2\" href=\"raw_ops/queuedequeuemanyv2\">QueueDequeueManyV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueDequeueUpTo\" href=\"raw_ops/queuedequeueupto\">QueueDequeueUpTo</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueDequeueUpToV2\" href=\"raw_ops/queuedequeueuptov2\">QueueDequeueUpToV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueDequeueV2\" href=\"raw_ops/queuedequeuev2\">QueueDequeueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueEnqueue\" href=\"raw_ops/queueenqueue\">QueueEnqueue</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueEnqueueMany\" href=\"raw_ops/queueenqueuemany\">QueueEnqueueMany</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueEnqueueManyV2\" href=\"raw_ops/queueenqueuemanyv2\">QueueEnqueueManyV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueEnqueueV2\" href=\"raw_ops/queueenqueuev2\">QueueEnqueueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueIsClosed\" href=\"raw_ops/queueisclosed\">QueueIsClosed</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueIsClosedV2\" href=\"raw_ops/queueisclosedv2\">QueueIsClosedV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"QueueSize\" href=\"raw_ops/queuesize\">QueueSize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"QueueSizeV2\" href=\"raw_ops/queuesizev2\">QueueSizeV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RFFT\" href=\"raw_ops/rfft\">RFFT</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RFFT2D\" href=\"raw_ops/rfft2d\">RFFT2D</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RFFT3D\" href=\"raw_ops/rfft3d\">RFFT3D</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RGBToHSV\" href=\"raw_ops/rgbtohsv\">RGBToHSV</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedBincount\" href=\"raw_ops/raggedbincount\">RaggedBincount</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RaggedCountSparseOutput\" href=\"raw_ops/raggedcountsparseoutput\">RaggedCountSparseOutput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RaggedCross\" href=\"raw_ops/raggedcross\">RaggedCross</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RaggedGather\" href=\"raw_ops/raggedgather\">RaggedGather</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedRange\" href=\"raw_ops/raggedrange\">RaggedRange</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedTensorFromVariant\" href=\"raw_ops/raggedtensorfromvariant\">RaggedTensorFromVariant</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedTensorToSparse\" href=\"raw_ops/raggedtensortosparse\">RaggedTensorToSparse</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedTensorToTensor\" href=\"raw_ops/raggedtensortotensor\">RaggedTensorToTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedTensorToVariant\" href=\"raw_ops/raggedtensortovariant\">RaggedTensorToVariant</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RaggedTensorToVariantGradient\" href=\"raw_ops/raggedtensortovariantgradient\">RaggedTensorToVariantGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomCrop\" href=\"raw_ops/randomcrop\">RandomCrop</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RandomDataset\" href=\"raw_ops/randomdataset\">RandomDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomGamma\" href=\"raw_ops/randomgamma\">RandomGamma</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RandomGammaGrad\" href=\"raw_ops/randomgammagrad\">RandomGammaGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomPoisson\" href=\"raw_ops/randompoisson\">RandomPoisson</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomPoissonV2\" href=\"raw_ops/randompoissonv2\">RandomPoissonV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomShuffle\" href=\"raw_ops/randomshuffle\">RandomShuffle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomShuffleQueue\" href=\"raw_ops/randomshufflequeue\">RandomShuffleQueue</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomShuffleQueueV2\" href=\"raw_ops/randomshufflequeuev2\">RandomShuffleQueueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RandomStandardNormal\" href=\"raw_ops/randomstandardnormal\">RandomStandardNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RandomUniform\" href=\"raw_ops/randomuniform\">RandomUniform</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RandomUniformInt\" href=\"raw_ops/randomuniformint\">RandomUniformInt</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Range\" href=\"raw_ops/range\">Range</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RangeDataset\" href=\"raw_ops/rangedataset\">RangeDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Rank\" href=\"raw_ops/rank\">Rank</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReadFile\" href=\"raw_ops/readfile\">ReadFile</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReadVariableOp\" href=\"raw_ops/readvariableop\">ReadVariableOp</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderNumRecordsProduced\" href=\"raw_ops/readernumrecordsproduced\">ReaderNumRecordsProduced</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderNumRecordsProducedV2\" href=\"raw_ops/readernumrecordsproducedv2\">ReaderNumRecordsProducedV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderNumWorkUnitsCompleted\" href=\"raw_ops/readernumworkunitscompleted\">ReaderNumWorkUnitsCompleted</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderNumWorkUnitsCompletedV2\" href=\"raw_ops/readernumworkunitscompletedv2\">ReaderNumWorkUnitsCompletedV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderRead\" href=\"raw_ops/readerread\">ReaderRead</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderReadUpTo\" href=\"raw_ops/readerreadupto\">ReaderReadUpTo</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderReadUpToV2\" href=\"raw_ops/readerreaduptov2\">ReaderReadUpToV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderReadV2\" href=\"raw_ops/readerreadv2\">ReaderReadV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderReset\" href=\"raw_ops/readerreset\">ReaderReset</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderResetV2\" href=\"raw_ops/readerresetv2\">ReaderResetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderRestoreState\" href=\"raw_ops/readerrestorestate\">ReaderRestoreState</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderRestoreStateV2\" href=\"raw_ops/readerrestorestatev2\">ReaderRestoreStateV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReaderSerializeState\" href=\"raw_ops/readerserializestate\">ReaderSerializeState</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReaderSerializeStateV2\" href=\"raw_ops/readerserializestatev2\">ReaderSerializeStateV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Real\" href=\"raw_ops/real\">Real</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RealDiv\" href=\"raw_ops/realdiv\">RealDiv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RebatchDataset\" href=\"raw_ops/rebatchdataset\">RebatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RebatchDatasetV2\" href=\"raw_ops/rebatchdatasetv2\">RebatchDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Reciprocal\" href=\"raw_ops/reciprocal\">Reciprocal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReciprocalGrad\" href=\"raw_ops/reciprocalgrad\">ReciprocalGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RecordInput\" href=\"raw_ops/recordinput\">RecordInput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Recv\" href=\"raw_ops/recv\">Recv</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RecvTPUEmbeddingActivations\" href=\"raw_ops/recvtpuembeddingactivations\">RecvTPUEmbeddingActivations</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ReduceDataset\" href=\"raw_ops/reducedataset\">ReduceDataset</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReduceJoin\" href=\"raw_ops/reducejoin\">ReduceJoin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefEnter\" href=\"raw_ops/refenter\">RefEnter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefExit\" href=\"raw_ops/refexit\">RefExit</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefIdentity\" href=\"raw_ops/refidentity\">RefIdentity</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefMerge\" href=\"raw_ops/refmerge\">RefMerge</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefNextIteration\" href=\"raw_ops/refnextiteration\">RefNextIteration</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RefSelect\" href=\"raw_ops/refselect\">RefSelect</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RefSwitch\" href=\"raw_ops/refswitch\">RefSwitch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RegexFullMatch\" href=\"raw_ops/regexfullmatch\">RegexFullMatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RegexReplace\" href=\"raw_ops/regexreplace\">RegexReplace</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RegisterDataset\" href=\"raw_ops/registerdataset\">RegisterDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Relu\" href=\"raw_ops/relu\">Relu</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Relu6\" href=\"raw_ops/relu6\">Relu6</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Relu6Grad\" href=\"raw_ops/relu6grad\">Relu6Grad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReluGrad\" href=\"raw_ops/relugrad\">ReluGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RemoteCall\" href=\"raw_ops/remotecall\">RemoteCall</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RepeatDataset\" href=\"raw_ops/repeatdataset\">RepeatDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RequantizationRange\" href=\"raw_ops/requantizationrange\">RequantizationRange</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RequantizationRangePerChannel\" href=\"raw_ops/requantizationrangeperchannel\">RequantizationRangePerChannel</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Requantize\" href=\"raw_ops/requantize\">Requantize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RequantizePerChannel\" href=\"raw_ops/requantizeperchannel\">RequantizePerChannel</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Reshape\" href=\"raw_ops/reshape\">Reshape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResizeArea\" href=\"raw_ops/resizearea\">ResizeArea</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResizeBicubic\" href=\"raw_ops/resizebicubic\">ResizeBicubic</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResizeBicubicGrad\" href=\"raw_ops/resizebicubicgrad\">ResizeBicubicGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResizeBilinear\" href=\"raw_ops/resizebilinear\">ResizeBilinear</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResizeBilinearGrad\" href=\"raw_ops/resizebilineargrad\">ResizeBilinearGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResizeNearestNeighbor\" href=\"raw_ops/resizenearestneighbor\">ResizeNearestNeighbor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResizeNearestNeighborGrad\" href=\"raw_ops/resizenearestneighborgrad\">ResizeNearestNeighborGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceAccumulatorApplyGradient\" href=\"raw_ops/resourceaccumulatorapplygradient\">ResourceAccumulatorApplyGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceAccumulatorNumAccumulated\" href=\"raw_ops/resourceaccumulatornumaccumulated\">ResourceAccumulatorNumAccumulated</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceAccumulatorSetGlobalStep\" href=\"raw_ops/resourceaccumulatorsetglobalstep\">ResourceAccumulatorSetGlobalStep</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceAccumulatorTakeGradient\" href=\"raw_ops/resourceaccumulatortakegradient\">ResourceAccumulatorTakeGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdaMax\" href=\"raw_ops/resourceapplyadamax\">ResourceApplyAdaMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdadelta\" href=\"raw_ops/resourceapplyadadelta\">ResourceApplyAdadelta</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdagrad\" href=\"raw_ops/resourceapplyadagrad\">ResourceApplyAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdagradDA\" href=\"raw_ops/resourceapplyadagradda\">ResourceApplyAdagradDA</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdagradV2\" href=\"raw_ops/resourceapplyadagradv2\">ResourceApplyAdagradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdam\" href=\"raw_ops/resourceapplyadam\">ResourceApplyAdam</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAdamWithAmsgrad\" href=\"raw_ops/resourceapplyadamwithamsgrad\">ResourceApplyAdamWithAmsgrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyAddSign\" href=\"raw_ops/resourceapplyaddsign\">ResourceApplyAddSign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyCenteredRMSProp\" href=\"raw_ops/resourceapplycenteredrmsprop\">ResourceApplyCenteredRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyFtrl\" href=\"raw_ops/resourceapplyftrl\">ResourceApplyFtrl</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyFtrlV2\" href=\"raw_ops/resourceapplyftrlv2\">ResourceApplyFtrlV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyGradientDescent\" href=\"raw_ops/resourceapplygradientdescent\">ResourceApplyGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyKerasMomentum\" href=\"raw_ops/resourceapplykerasmomentum\">ResourceApplyKerasMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyMomentum\" href=\"raw_ops/resourceapplymomentum\">ResourceApplyMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyPowerSign\" href=\"raw_ops/resourceapplypowersign\">ResourceApplyPowerSign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyProximalAdagrad\" href=\"raw_ops/resourceapplyproximaladagrad\">ResourceApplyProximalAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyProximalGradientDescent\" href=\"raw_ops/resourceapplyproximalgradientdescent\">ResourceApplyProximalGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceApplyRMSProp\" href=\"raw_ops/resourceapplyrmsprop\">ResourceApplyRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceConditionalAccumulator\" href=\"raw_ops/resourceconditionalaccumulator\">ResourceConditionalAccumulator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceCountUpTo\" href=\"raw_ops/resourcecountupto\">ResourceCountUpTo</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceGather\" href=\"raw_ops/resourcegather\">ResourceGather</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResourceGatherNd\" href=\"raw_ops/resourcegathernd\">ResourceGatherNd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ResourceScatterAdd\" href=\"raw_ops/resourcescatteradd\">ResourceScatterAdd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterDiv\" href=\"raw_ops/resourcescatterdiv\">ResourceScatterDiv</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterMax\" href=\"raw_ops/resourcescattermax\">ResourceScatterMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterMin\" href=\"raw_ops/resourcescattermin\">ResourceScatterMin</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterMul\" href=\"raw_ops/resourcescattermul\">ResourceScatterMul</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterNdAdd\" href=\"raw_ops/resourcescatterndadd\">ResourceScatterNdAdd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterNdMax\" href=\"raw_ops/resourcescatterndmax\">ResourceScatterNdMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterNdMin\" href=\"raw_ops/resourcescatterndmin\">ResourceScatterNdMin</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterNdSub\" href=\"raw_ops/resourcescatterndsub\">ResourceScatterNdSub</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterNdUpdate\" href=\"raw_ops/resourcescatterndupdate\">ResourceScatterNdUpdate</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterSub\" href=\"raw_ops/resourcescattersub\">ResourceScatterSub</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceScatterUpdate\" href=\"raw_ops/resourcescatterupdate\">ResourceScatterUpdate</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyAdadelta\" href=\"raw_ops/resourcesparseapplyadadelta\">ResourceSparseApplyAdadelta</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyAdagrad\" href=\"raw_ops/resourcesparseapplyadagrad\">ResourceSparseApplyAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyAdagradDA\" href=\"raw_ops/resourcesparseapplyadagradda\">ResourceSparseApplyAdagradDA</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyAdagradV2\" href=\"raw_ops/resourcesparseapplyadagradv2\">ResourceSparseApplyAdagradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyCenteredRMSProp\" href=\"raw_ops/resourcesparseapplycenteredrmsprop\">ResourceSparseApplyCenteredRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyFtrl\" href=\"raw_ops/resourcesparseapplyftrl\">ResourceSparseApplyFtrl</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyFtrlV2\" href=\"raw_ops/resourcesparseapplyftrlv2\">ResourceSparseApplyFtrlV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyKerasMomentum\" href=\"raw_ops/resourcesparseapplykerasmomentum\">ResourceSparseApplyKerasMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyMomentum\" href=\"raw_ops/resourcesparseapplymomentum\">ResourceSparseApplyMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyProximalAdagrad\" href=\"raw_ops/resourcesparseapplyproximaladagrad\">ResourceSparseApplyProximalAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyProximalGradientDescent\" href=\"raw_ops/resourcesparseapplyproximalgradientdescent\">ResourceSparseApplyProximalGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceSparseApplyRMSProp\" href=\"raw_ops/resourcesparseapplyrmsprop\">ResourceSparseApplyRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ResourceStridedSliceAssign\" href=\"raw_ops/resourcestridedsliceassign\">ResourceStridedSliceAssign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Restore\" href=\"raw_ops/restore\">Restore</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RestoreSlice\" href=\"raw_ops/restoreslice\">RestoreSlice</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RestoreV2\" href=\"raw_ops/restorev2\">RestoreV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingADAMParameters\" href=\"raw_ops/retrievetpuembeddingadamparameters\">RetrieveTPUEmbeddingADAMParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingADAMParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingadamparametersgradaccumdebug\">RetrieveTPUEmbeddingADAMParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingAdadeltaParameters\" href=\"raw_ops/retrievetpuembeddingadadeltaparameters\">RetrieveTPUEmbeddingAdadeltaParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingadadeltaparametersgradaccumdebug\">RetrieveTPUEmbeddingAdadeltaParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingAdagradParameters\" href=\"raw_ops/retrievetpuembeddingadagradparameters\">RetrieveTPUEmbeddingAdagradParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingAdagradParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingadagradparametersgradaccumdebug\">RetrieveTPUEmbeddingAdagradParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingCenteredRMSPropParameters\" href=\"raw_ops/retrievetpuembeddingcenteredrmspropparameters\">RetrieveTPUEmbeddingCenteredRMSPropParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingFTRLParameters\" href=\"raw_ops/retrievetpuembeddingftrlparameters\">RetrieveTPUEmbeddingFTRLParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingFTRLParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingftrlparametersgradaccumdebug\">RetrieveTPUEmbeddingFTRLParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingMDLAdagradLightParameters\" href=\"raw_ops/retrievetpuembeddingmdladagradlightparameters\">RetrieveTPUEmbeddingMDLAdagradLightParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingMomentumParameters\" href=\"raw_ops/retrievetpuembeddingmomentumparameters\">RetrieveTPUEmbeddingMomentumParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingMomentumParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingmomentumparametersgradaccumdebug\">RetrieveTPUEmbeddingMomentumParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingProximalAdagradParameters\" href=\"raw_ops/retrievetpuembeddingproximaladagradparameters\">RetrieveTPUEmbeddingProximalAdagradParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingproximaladagradparametersgradaccumdebug\">RetrieveTPUEmbeddingProximalAdagradParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingProximalYogiParameters\" href=\"raw_ops/retrievetpuembeddingproximalyogiparameters\">RetrieveTPUEmbeddingProximalYogiParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingProximalYogiParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingproximalyogiparametersgradaccumdebug\">RetrieveTPUEmbeddingProximalYogiParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingRMSPropParameters\" href=\"raw_ops/retrievetpuembeddingrmspropparameters\">RetrieveTPUEmbeddingRMSPropParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingrmspropparametersgradaccumdebug\">RetrieveTPUEmbeddingRMSPropParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingStochasticGradientDescentParameters\" href=\"raw_ops/retrievetpuembeddingstochasticgradientdescentparameters\">RetrieveTPUEmbeddingStochasticGradientDescentParameters</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RetrieveTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug\" href=\"raw_ops/retrievetpuembeddingstochasticgradientdescentparametersgradaccumdebug\">RetrieveTPUEmbeddingStochasticGradientDescentParametersGradAccumDebug</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Reverse\" href=\"raw_ops/reverse\">Reverse</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReverseSequence\" href=\"raw_ops/reversesequence\">ReverseSequence</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ReverseV2\" href=\"raw_ops/reversev2\">ReverseV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RightShift\" href=\"raw_ops/rightshift\">RightShift</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Rint\" href=\"raw_ops/rint\">Rint</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RngReadAndSkip\" href=\"raw_ops/rngreadandskip\">RngReadAndSkip</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"RngSkip\" href=\"raw_ops/rngskip\">RngSkip</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Roll\" href=\"raw_ops/roll\">Roll</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Round\" href=\"raw_ops/round\">Round</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Rsqrt\" href=\"raw_ops/rsqrt\">Rsqrt</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"RsqrtGrad\" href=\"raw_ops/rsqrtgrad\">RsqrtGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SampleDistortedBoundingBox\" href=\"raw_ops/sampledistortedboundingbox\">SampleDistortedBoundingBox</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SampleDistortedBoundingBoxV2\" href=\"raw_ops/sampledistortedboundingboxv2\">SampleDistortedBoundingBoxV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SamplingDataset\" href=\"raw_ops/samplingdataset\">SamplingDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Save\" href=\"raw_ops/save\">Save</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SaveDataset\" href=\"raw_ops/savedataset\">SaveDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SaveSlices\" href=\"raw_ops/saveslices\">SaveSlices</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SaveV2\" href=\"raw_ops/savev2\">SaveV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScalarSummary\" href=\"raw_ops/scalarsummary\">ScalarSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScaleAndTranslate\" href=\"raw_ops/scaleandtranslate\">ScaleAndTranslate</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScaleAndTranslateGrad\" href=\"raw_ops/scaleandtranslategrad\">ScaleAndTranslateGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScanDataset\" href=\"raw_ops/scandataset\">ScanDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScatterAdd\" href=\"raw_ops/scatteradd\">ScatterAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterDiv\" href=\"raw_ops/scatterdiv\">ScatterDiv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterMax\" href=\"raw_ops/scattermax\">ScatterMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScatterMin\" href=\"raw_ops/scattermin\">ScatterMin</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScatterMul\" href=\"raw_ops/scattermul\">ScatterMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterNd\" href=\"raw_ops/scatternd\">ScatterNd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterNdAdd\" href=\"raw_ops/scatterndadd\">ScatterNdAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterNdMax\" href=\"raw_ops/scatterndmax\">ScatterNdMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScatterNdMin\" href=\"raw_ops/scatterndmin\">ScatterNdMin</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ScatterNdNonAliasingAdd\" href=\"raw_ops/scatterndnonaliasingadd\">ScatterNdNonAliasingAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterNdSub\" href=\"raw_ops/scatterndsub\">ScatterNdSub</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterNdUpdate\" href=\"raw_ops/scatterndupdate\">ScatterNdUpdate</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterSub\" href=\"raw_ops/scattersub\">ScatterSub</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ScatterUpdate\" href=\"raw_ops/scatterupdate\">ScatterUpdate</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SdcaFprint\" href=\"raw_ops/sdcafprint\">SdcaFprint</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SdcaOptimizer\" href=\"raw_ops/sdcaoptimizer\">SdcaOptimizer</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SdcaOptimizerV2\" href=\"raw_ops/sdcaoptimizerv2\">SdcaOptimizerV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SdcaShrinkL1\" href=\"raw_ops/sdcashrinkl1\">SdcaShrinkL1</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SegmentMax\" href=\"raw_ops/segmentmax\">SegmentMax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SegmentMean\" href=\"raw_ops/segmentmean\">SegmentMean</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SegmentMin\" href=\"raw_ops/segmentmin\">SegmentMin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SegmentProd\" href=\"raw_ops/segmentprod\">SegmentProd</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SegmentSum\" href=\"raw_ops/segmentsum\">SegmentSum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Select\" href=\"raw_ops/select\">Select</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SelectV2\" href=\"raw_ops/selectv2\">SelectV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SelfAdjointEig\" href=\"raw_ops/selfadjointeig\">SelfAdjointEig</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SelfAdjointEigV2\" href=\"raw_ops/selfadjointeigv2\">SelfAdjointEigV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Selu\" href=\"raw_ops/selu\">Selu</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SeluGrad\" href=\"raw_ops/selugrad\">SeluGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Send\" href=\"raw_ops/send\">Send</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SendTPUEmbeddingGradients\" href=\"raw_ops/sendtpuembeddinggradients\">SendTPUEmbeddingGradients</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SerializeIterator\" href=\"raw_ops/serializeiterator\">SerializeIterator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SerializeManySparse\" href=\"raw_ops/serializemanysparse\">SerializeManySparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SerializeSparse\" href=\"raw_ops/serializesparse\">SerializeSparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SerializeTensor\" href=\"raw_ops/serializetensor\">SerializeTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SetSize\" href=\"raw_ops/setsize\">SetSize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SetStatsAggregatorDataset\" href=\"raw_ops/setstatsaggregatordataset\">SetStatsAggregatorDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Shape\" href=\"raw_ops/shape\">Shape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ShapeN\" href=\"raw_ops/shapen\">ShapeN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ShardDataset\" href=\"raw_ops/sharddataset\">ShardDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShardedFilename\" href=\"raw_ops/shardedfilename\">ShardedFilename</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShardedFilespec\" href=\"raw_ops/shardedfilespec\">ShardedFilespec</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShuffleAndRepeatDataset\" href=\"raw_ops/shuffleandrepeatdataset\">ShuffleAndRepeatDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShuffleAndRepeatDatasetV2\" href=\"raw_ops/shuffleandrepeatdatasetv2\">ShuffleAndRepeatDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShuffleDataset\" href=\"raw_ops/shuffledataset\">ShuffleDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShuffleDatasetV2\" href=\"raw_ops/shuffledatasetv2\">ShuffleDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShuffleDatasetV3\" href=\"raw_ops/shuffledatasetv3\">ShuffleDatasetV3</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ShutdownDistributedTPU\" href=\"raw_ops/shutdowndistributedtpu\">ShutdownDistributedTPU</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Sigmoid\" href=\"raw_ops/sigmoid\">Sigmoid</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SigmoidGrad\" href=\"raw_ops/sigmoidgrad\">SigmoidGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Sign\" href=\"raw_ops/sign\">Sign</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Sin\" href=\"raw_ops/sin\">Sin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Sinh\" href=\"raw_ops/sinh\">Sinh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Size\" href=\"raw_ops/size\">Size</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SkipDataset\" href=\"raw_ops/skipdataset\">SkipDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SleepDataset\" href=\"raw_ops/sleepdataset\">SleepDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Slice\" href=\"raw_ops/slice\">Slice</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SlidingWindowDataset\" href=\"raw_ops/slidingwindowdataset\">SlidingWindowDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Snapshot\" href=\"raw_ops/snapshot\">Snapshot</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SnapshotDataset\" href=\"raw_ops/snapshotdataset\">SnapshotDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SnapshotDatasetV2\" href=\"raw_ops/snapshotdatasetv2\">SnapshotDatasetV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SobolSample\" href=\"raw_ops/sobolsample\">SobolSample</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Softmax\" href=\"raw_ops/softmax\">Softmax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SoftmaxCrossEntropyWithLogits\" href=\"raw_ops/softmaxcrossentropywithlogits\">SoftmaxCrossEntropyWithLogits</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Softplus\" href=\"raw_ops/softplus\">Softplus</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SoftplusGrad\" href=\"raw_ops/softplusgrad\">SoftplusGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Softsign\" href=\"raw_ops/softsign\">Softsign</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SoftsignGrad\" href=\"raw_ops/softsigngrad\">SoftsignGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SpaceToBatch\" href=\"raw_ops/spacetobatch\">SpaceToBatch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SpaceToBatchND\" href=\"raw_ops/spacetobatchnd\">SpaceToBatchND</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SpaceToDepth\" href=\"raw_ops/spacetodepth\">SpaceToDepth</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseAccumulatorApplyGradient\" href=\"raw_ops/sparseaccumulatorapplygradient\">SparseAccumulatorApplyGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseAccumulatorTakeGradient\" href=\"raw_ops/sparseaccumulatortakegradient\">SparseAccumulatorTakeGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseAdd\" href=\"raw_ops/sparseadd\">SparseAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseAddGrad\" href=\"raw_ops/sparseaddgrad\">SparseAddGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseApplyAdadelta\" href=\"raw_ops/sparseapplyadadelta\">SparseApplyAdadelta</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyAdagrad\" href=\"raw_ops/sparseapplyadagrad\">SparseApplyAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyAdagradDA\" href=\"raw_ops/sparseapplyadagradda\">SparseApplyAdagradDA</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyAdagradV2\" href=\"raw_ops/sparseapplyadagradv2\">SparseApplyAdagradV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyCenteredRMSProp\" href=\"raw_ops/sparseapplycenteredrmsprop\">SparseApplyCenteredRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyFtrl\" href=\"raw_ops/sparseapplyftrl\">SparseApplyFtrl</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyFtrlV2\" href=\"raw_ops/sparseapplyftrlv2\">SparseApplyFtrlV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyMomentum\" href=\"raw_ops/sparseapplymomentum\">SparseApplyMomentum</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyProximalAdagrad\" href=\"raw_ops/sparseapplyproximaladagrad\">SparseApplyProximalAdagrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyProximalGradientDescent\" href=\"raw_ops/sparseapplyproximalgradientdescent\">SparseApplyProximalGradientDescent</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseApplyRMSProp\" href=\"raw_ops/sparseapplyrmsprop\">SparseApplyRMSProp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseBincount\" href=\"raw_ops/sparsebincount\">SparseBincount</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseConcat\" href=\"raw_ops/sparseconcat\">SparseConcat</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseConditionalAccumulator\" href=\"raw_ops/sparseconditionalaccumulator\">SparseConditionalAccumulator</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseCountSparseOutput\" href=\"raw_ops/sparsecountsparseoutput\">SparseCountSparseOutput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseCross\" href=\"raw_ops/sparsecross\">SparseCross</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseCrossHashed\" href=\"raw_ops/sparsecrosshashed\">SparseCrossHashed</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseCrossV2\" href=\"raw_ops/sparsecrossv2\">SparseCrossV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseDenseCwiseAdd\" href=\"raw_ops/sparsedensecwiseadd\">SparseDenseCwiseAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseDenseCwiseDiv\" href=\"raw_ops/sparsedensecwisediv\">SparseDenseCwiseDiv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseDenseCwiseMul\" href=\"raw_ops/sparsedensecwisemul\">SparseDenseCwiseMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseFillEmptyRows\" href=\"raw_ops/sparsefillemptyrows\">SparseFillEmptyRows</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseFillEmptyRowsGrad\" href=\"raw_ops/sparsefillemptyrowsgrad\">SparseFillEmptyRowsGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseMatMul\" href=\"raw_ops/sparsematmul\">SparseMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixAdd\" href=\"raw_ops/sparsematrixadd\">SparseMatrixAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixMatMul\" href=\"raw_ops/sparsematrixmatmul\">SparseMatrixMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixMul\" href=\"raw_ops/sparsematrixmul\">SparseMatrixMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixNNZ\" href=\"raw_ops/sparsematrixnnz\">SparseMatrixNNZ</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixOrderingAMD\" href=\"raw_ops/sparsematrixorderingamd\">SparseMatrixOrderingAMD</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseMatrixSoftmax\" href=\"raw_ops/sparsematrixsoftmax\">SparseMatrixSoftmax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixSoftmaxGrad\" href=\"raw_ops/sparsematrixsoftmaxgrad\">SparseMatrixSoftmaxGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseMatrixSparseCholesky\" href=\"raw_ops/sparsematrixsparsecholesky\">SparseMatrixSparseCholesky</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseMatrixSparseMatMul\" href=\"raw_ops/sparsematrixsparsematmul\">SparseMatrixSparseMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixTranspose\" href=\"raw_ops/sparsematrixtranspose\">SparseMatrixTranspose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseMatrixZeros\" href=\"raw_ops/sparsematrixzeros\">SparseMatrixZeros</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseReduceMax\" href=\"raw_ops/sparsereducemax\">SparseReduceMax</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseReduceMaxSparse\" href=\"raw_ops/sparsereducemaxsparse\">SparseReduceMaxSparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseReduceSum\" href=\"raw_ops/sparsereducesum\">SparseReduceSum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseReduceSumSparse\" href=\"raw_ops/sparsereducesumsparse\">SparseReduceSumSparse</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseReorder\" href=\"raw_ops/sparsereorder\">SparseReorder</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseReshape\" href=\"raw_ops/sparsereshape\">SparseReshape</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseSegmentMean\" href=\"raw_ops/sparsesegmentmean\">SparseSegmentMean</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSegmentMeanGrad\" href=\"raw_ops/sparsesegmentmeangrad\">SparseSegmentMeanGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseSegmentMeanWithNumSegments\" href=\"raw_ops/sparsesegmentmeanwithnumsegments\">SparseSegmentMeanWithNumSegments</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSegmentSqrtN\" href=\"raw_ops/sparsesegmentsqrtn\">SparseSegmentSqrtN</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSegmentSqrtNGrad\" href=\"raw_ops/sparsesegmentsqrtngrad\">SparseSegmentSqrtNGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseSegmentSqrtNWithNumSegments\" href=\"raw_ops/sparsesegmentsqrtnwithnumsegments\">SparseSegmentSqrtNWithNumSegments</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSegmentSum\" href=\"raw_ops/sparsesegmentsum\">SparseSegmentSum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSegmentSumWithNumSegments\" href=\"raw_ops/sparsesegmentsumwithnumsegments\">SparseSegmentSumWithNumSegments</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSlice\" href=\"raw_ops/sparseslice\">SparseSlice</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSliceGrad\" href=\"raw_ops/sparseslicegrad\">SparseSliceGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseSoftmax\" href=\"raw_ops/sparsesoftmax\">SparseSoftmax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSoftmaxCrossEntropyWithLogits\" href=\"raw_ops/sparsesoftmaxcrossentropywithlogits\">SparseSoftmaxCrossEntropyWithLogits</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSparseMaximum\" href=\"raw_ops/sparsesparsemaximum\">SparseSparseMaximum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSparseMinimum\" href=\"raw_ops/sparsesparseminimum\">SparseSparseMinimum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseSplit\" href=\"raw_ops/sparsesplit\">SparseSplit</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseTensorDenseAdd\" href=\"raw_ops/sparsetensordenseadd\">SparseTensorDenseAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseTensorDenseMatMul\" href=\"raw_ops/sparsetensordensematmul\">SparseTensorDenseMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseTensorSliceDataset\" href=\"raw_ops/sparsetensorslicedataset\">SparseTensorSliceDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseTensorToCSRSparseMatrix\" href=\"raw_ops/sparsetensortocsrsparsematrix\">SparseTensorToCSRSparseMatrix</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"SparseToDense\" href=\"raw_ops/sparsetodense\">SparseToDense</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SparseToSparseSetOperation\" href=\"raw_ops/sparsetosparsesetoperation\">SparseToSparseSetOperation</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Spence\" href=\"raw_ops/spence\">Spence</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Split\" href=\"raw_ops/split\">Split</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SplitV\" href=\"raw_ops/splitv\">SplitV</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SqlDataset\" href=\"raw_ops/sqldataset\">SqlDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Sqrt\" href=\"raw_ops/sqrt\">Sqrt</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SqrtGrad\" href=\"raw_ops/sqrtgrad\">SqrtGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Square\" href=\"raw_ops/square\">Square</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SquaredDifference\" href=\"raw_ops/squareddifference\">SquaredDifference</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Squeeze\" href=\"raw_ops/squeeze\">Squeeze</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Stack\" href=\"raw_ops/stack\">Stack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StackClose\" href=\"raw_ops/stackclose\">StackClose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StackCloseV2\" href=\"raw_ops/stackclosev2\">StackCloseV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StackPop\" href=\"raw_ops/stackpop\">StackPop</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StackPopV2\" href=\"raw_ops/stackpopv2\">StackPopV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StackPush\" href=\"raw_ops/stackpush\">StackPush</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StackPushV2\" href=\"raw_ops/stackpushv2\">StackPushV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StackV2\" href=\"raw_ops/stackv2\">StackV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Stage\" href=\"raw_ops/stage\">Stage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StageClear\" href=\"raw_ops/stageclear\">StageClear</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StagePeek\" href=\"raw_ops/stagepeek\">StagePeek</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StageSize\" href=\"raw_ops/stagesize\">StageSize</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulPartitionedCall\" href=\"raw_ops/statefulpartitionedcall\">StatefulPartitionedCall</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulRandomBinomial\" href=\"raw_ops/statefulrandombinomial\">StatefulRandomBinomial</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulStandardNormal\" href=\"raw_ops/statefulstandardnormal\">StatefulStandardNormal</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulStandardNormalV2\" href=\"raw_ops/statefulstandardnormalv2\">StatefulStandardNormalV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulTruncatedNormal\" href=\"raw_ops/statefultruncatednormal\">StatefulTruncatedNormal</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulUniform\" href=\"raw_ops/statefuluniform\">StatefulUniform</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulUniformFullInt\" href=\"raw_ops/statefuluniformfullint\">StatefulUniformFullInt</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatefulUniformInt\" href=\"raw_ops/statefuluniformint\">StatefulUniformInt</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatelessCase\" href=\"raw_ops/statelesscase\">StatelessCase</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessIf\" href=\"raw_ops/statelessif\">StatelessIf</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessMultinomial\" href=\"raw_ops/statelessmultinomial\">StatelessMultinomial</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessParameterizedTruncatedNormal\" href=\"raw_ops/statelessparameterizedtruncatednormal\">StatelessParameterizedTruncatedNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomBinomial\" href=\"raw_ops/statelessrandombinomial\">StatelessRandomBinomial</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomGammaV2\" href=\"raw_ops/statelessrandomgammav2\">StatelessRandomGammaV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomGetKeyCounterAlg\" href=\"raw_ops/statelessrandomgetkeycounteralg\">StatelessRandomGetKeyCounterAlg</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatelessRandomNormal\" href=\"raw_ops/statelessrandomnormal\">StatelessRandomNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomNormalV2\" href=\"raw_ops/statelessrandomnormalv2\">StatelessRandomNormalV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomPoisson\" href=\"raw_ops/statelessrandompoisson\">StatelessRandomPoisson</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniform\" href=\"raw_ops/statelessrandomuniform\">StatelessRandomUniform</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniformFullInt\" href=\"raw_ops/statelessrandomuniformfullint\">StatelessRandomUniformFullInt</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniformFullIntV2\" href=\"raw_ops/statelessrandomuniformfullintv2\">StatelessRandomUniformFullIntV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniformInt\" href=\"raw_ops/statelessrandomuniformint\">StatelessRandomUniformInt</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniformIntV2\" href=\"raw_ops/statelessrandomuniformintv2\">StatelessRandomUniformIntV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessRandomUniformV2\" href=\"raw_ops/statelessrandomuniformv2\">StatelessRandomUniformV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessSampleDistortedBoundingBox\" href=\"raw_ops/statelesssampledistortedboundingbox\">StatelessSampleDistortedBoundingBox</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatelessTruncatedNormal\" href=\"raw_ops/statelesstruncatednormal\">StatelessTruncatedNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessTruncatedNormalV2\" href=\"raw_ops/statelesstruncatednormalv2\">StatelessTruncatedNormalV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StatelessWhile\" href=\"raw_ops/statelesswhile\">StatelessWhile</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StaticRegexFullMatch\" href=\"raw_ops/staticregexfullmatch\">StaticRegexFullMatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StaticRegexReplace\" href=\"raw_ops/staticregexreplace\">StaticRegexReplace</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatsAggregatorHandle\" href=\"raw_ops/statsaggregatorhandle\">StatsAggregatorHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatsAggregatorHandleV2\" href=\"raw_ops/statsaggregatorhandlev2\">StatsAggregatorHandleV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatsAggregatorSetSummaryWriter\" href=\"raw_ops/statsaggregatorsetsummarywriter\">StatsAggregatorSetSummaryWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StatsAggregatorSummary\" href=\"raw_ops/statsaggregatorsummary\">StatsAggregatorSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StopGradient\" href=\"raw_ops/stopgradient\">StopGradient</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StridedSlice\" href=\"raw_ops/stridedslice\">StridedSlice</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StridedSliceAssign\" href=\"raw_ops/stridedsliceassign\">StridedSliceAssign</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StridedSliceGrad\" href=\"raw_ops/stridedslicegrad\">StridedSliceGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringFormat\" href=\"raw_ops/stringformat\">StringFormat</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringJoin\" href=\"raw_ops/stringjoin\">StringJoin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringLength\" href=\"raw_ops/stringlength\">StringLength</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringLower\" href=\"raw_ops/stringlower\">StringLower</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringNGrams\" href=\"raw_ops/stringngrams\">StringNGrams</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringSplit\" href=\"raw_ops/stringsplit\">StringSplit</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringSplitV2\" href=\"raw_ops/stringsplitv2\">StringSplitV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringStrip\" href=\"raw_ops/stringstrip\">StringStrip</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"StringToHashBucket\" href=\"raw_ops/stringtohashbucket\">StringToHashBucket</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringToHashBucketFast\" href=\"raw_ops/stringtohashbucketfast\">StringToHashBucketFast</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringToHashBucketStrong\" href=\"raw_ops/stringtohashbucketstrong\">StringToHashBucketStrong</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringToNumber\" href=\"raw_ops/stringtonumber\">StringToNumber</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"StringUpper\" href=\"raw_ops/stringupper\">StringUpper</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Sub\" href=\"raw_ops/sub\">Sub</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Substr\" href=\"raw_ops/substr\">Substr</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Sum\" href=\"raw_ops/sum\">Sum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SummaryWriter\" href=\"raw_ops/summarywriter\">SummaryWriter</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Svd\" href=\"raw_ops/svd\">Svd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Switch\" href=\"raw_ops/switch\">Switch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"SymbolicGradient\" href=\"raw_ops/symbolicgradient\">SymbolicGradient</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TFRecordDataset\" href=\"raw_ops/tfrecorddataset\">TFRecordDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TFRecordReader\" href=\"raw_ops/tfrecordreader\">TFRecordReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TFRecordReaderV2\" href=\"raw_ops/tfrecordreaderv2\">TFRecordReaderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TPUCompilationResult\" href=\"raw_ops/tpucompilationresult\">TPUCompilationResult</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TPUEmbeddingActivations\" href=\"raw_ops/tpuembeddingactivations\">TPUEmbeddingActivations</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TPUOrdinalSelector\" href=\"raw_ops/tpuordinalselector\">TPUOrdinalSelector</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TPUPartitionedCall\" href=\"raw_ops/tpupartitionedcall\">TPUPartitionedCall</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TPUReplicateMetadata\" href=\"raw_ops/tpureplicatemetadata\">TPUReplicateMetadata</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TPUReplicatedInput\" href=\"raw_ops/tpureplicatedinput\">TPUReplicatedInput</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TPUReplicatedOutput\" href=\"raw_ops/tpureplicatedoutput\">TPUReplicatedOutput</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TakeDataset\" href=\"raw_ops/takedataset\">TakeDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TakeManySparseFromTensorsMap\" href=\"raw_ops/takemanysparsefromtensorsmap\">TakeManySparseFromTensorsMap</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TakeWhileDataset\" href=\"raw_ops/takewhiledataset\">TakeWhileDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Tan\" href=\"raw_ops/tan\">Tan</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Tanh\" href=\"raw_ops/tanh\">Tanh</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TanhGrad\" href=\"raw_ops/tanhgrad\">TanhGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TemporaryVariable\" href=\"raw_ops/temporaryvariable\">TemporaryVariable</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorArray\" href=\"raw_ops/tensorarray\">TensorArray</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayClose\" href=\"raw_ops/tensorarrayclose\">TensorArrayClose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayCloseV2\" href=\"raw_ops/tensorarrayclosev2\">TensorArrayCloseV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayCloseV3\" href=\"raw_ops/tensorarrayclosev3\">TensorArrayCloseV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayConcat\" href=\"raw_ops/tensorarrayconcat\">TensorArrayConcat</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayConcatV2\" href=\"raw_ops/tensorarrayconcatv2\">TensorArrayConcatV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayConcatV3\" href=\"raw_ops/tensorarrayconcatv3\">TensorArrayConcatV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGather\" href=\"raw_ops/tensorarraygather\">TensorArrayGather</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGatherV2\" href=\"raw_ops/tensorarraygatherv2\">TensorArrayGatherV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGatherV3\" href=\"raw_ops/tensorarraygatherv3\">TensorArrayGatherV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGrad\" href=\"raw_ops/tensorarraygrad\">TensorArrayGrad</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGradV2\" href=\"raw_ops/tensorarraygradv2\">TensorArrayGradV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGradV3\" href=\"raw_ops/tensorarraygradv3\">TensorArrayGradV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayGradWithShape\" href=\"raw_ops/tensorarraygradwithshape\">TensorArrayGradWithShape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayPack\" href=\"raw_ops/tensorarraypack\">TensorArrayPack</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorArrayRead\" href=\"raw_ops/tensorarrayread\">TensorArrayRead</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayReadV2\" href=\"raw_ops/tensorarrayreadv2\">TensorArrayReadV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayReadV3\" href=\"raw_ops/tensorarrayreadv3\">TensorArrayReadV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayScatter\" href=\"raw_ops/tensorarrayscatter\">TensorArrayScatter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayScatterV2\" href=\"raw_ops/tensorarrayscatterv2\">TensorArrayScatterV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayScatterV3\" href=\"raw_ops/tensorarrayscatterv3\">TensorArrayScatterV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySize\" href=\"raw_ops/tensorarraysize\">TensorArraySize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySizeV2\" href=\"raw_ops/tensorarraysizev2\">TensorArraySizeV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySizeV3\" href=\"raw_ops/tensorarraysizev3\">TensorArraySizeV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySplit\" href=\"raw_ops/tensorarraysplit\">TensorArraySplit</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySplitV2\" href=\"raw_ops/tensorarraysplitv2\">TensorArraySplitV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArraySplitV3\" href=\"raw_ops/tensorarraysplitv3\">TensorArraySplitV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayUnpack\" href=\"raw_ops/tensorarrayunpack\">TensorArrayUnpack</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorArrayV2\" href=\"raw_ops/tensorarrayv2\">TensorArrayV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayV3\" href=\"raw_ops/tensorarrayv3\">TensorArrayV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayWrite\" href=\"raw_ops/tensorarraywrite\">TensorArrayWrite</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayWriteV2\" href=\"raw_ops/tensorarraywritev2\">TensorArrayWriteV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorArrayWriteV3\" href=\"raw_ops/tensorarraywritev3\">TensorArrayWriteV3</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorDataset\" href=\"raw_ops/tensordataset\">TensorDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorListConcat\" href=\"raw_ops/tensorlistconcat\">TensorListConcat</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListConcatLists\" href=\"raw_ops/tensorlistconcatlists\">TensorListConcatLists</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListConcatV2\" href=\"raw_ops/tensorlistconcatv2\">TensorListConcatV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListElementShape\" href=\"raw_ops/tensorlistelementshape\">TensorListElementShape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListFromTensor\" href=\"raw_ops/tensorlistfromtensor\">TensorListFromTensor</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListGather\" href=\"raw_ops/tensorlistgather\">TensorListGather</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListGetItem\" href=\"raw_ops/tensorlistgetitem\">TensorListGetItem</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListLength\" href=\"raw_ops/tensorlistlength\">TensorListLength</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListPopBack\" href=\"raw_ops/tensorlistpopback\">TensorListPopBack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListPushBack\" href=\"raw_ops/tensorlistpushback\">TensorListPushBack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListPushBackBatch\" href=\"raw_ops/tensorlistpushbackbatch\">TensorListPushBackBatch</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListReserve\" href=\"raw_ops/tensorlistreserve\">TensorListReserve</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorListResize\" href=\"raw_ops/tensorlistresize\">TensorListResize</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListScatter\" href=\"raw_ops/tensorlistscatter\">TensorListScatter</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListScatterIntoExistingList\" href=\"raw_ops/tensorlistscatterintoexistinglist\">TensorListScatterIntoExistingList</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListScatterV2\" href=\"raw_ops/tensorlistscatterv2\">TensorListScatterV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListSetItem\" href=\"raw_ops/tensorlistsetitem\">TensorListSetItem</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListSplit\" href=\"raw_ops/tensorlistsplit\">TensorListSplit</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorListStack\" href=\"raw_ops/tensorliststack\">TensorListStack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorScatterAdd\" href=\"raw_ops/tensorscatteradd\">TensorScatterAdd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorScatterMax\" href=\"raw_ops/tensorscattermax\">TensorScatterMax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorScatterMin\" href=\"raw_ops/tensorscattermin\">TensorScatterMin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorScatterSub\" href=\"raw_ops/tensorscattersub\">TensorScatterSub</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorScatterUpdate\" href=\"raw_ops/tensorscatterupdate\">TensorScatterUpdate</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorSliceDataset\" href=\"raw_ops/tensorslicedataset\">TensorSliceDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TensorStridedSliceUpdate\" href=\"raw_ops/tensorstridedsliceupdate\">TensorStridedSliceUpdate</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorSummary\" href=\"raw_ops/tensorsummary\">TensorSummary</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TensorSummaryV2\" href=\"raw_ops/tensorsummaryv2\">TensorSummaryV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TextLineDataset\" href=\"raw_ops/textlinedataset\">TextLineDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TextLineReader\" href=\"raw_ops/textlinereader\">TextLineReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TextLineReaderV2\" href=\"raw_ops/textlinereaderv2\">TextLineReaderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ThreadPoolDataset\" href=\"raw_ops/threadpooldataset\">ThreadPoolDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ThreadPoolHandle\" href=\"raw_ops/threadpoolhandle\">ThreadPoolHandle</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"ThreadUnsafeUnigramCandidateSampler\" href=\"raw_ops/threadunsafeunigramcandidatesampler\">ThreadUnsafeUnigramCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Tile\" href=\"raw_ops/tile\">Tile</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TileGrad\" href=\"raw_ops/tilegrad\">TileGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Timestamp\" href=\"raw_ops/timestamp\">Timestamp</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ToBool\" href=\"raw_ops/tobool\">ToBool</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TopK\" href=\"raw_ops/topk\">TopK</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TopKV2\" href=\"raw_ops/topkv2\">TopKV2</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Transpose\" href=\"raw_ops/transpose\">Transpose</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TridiagonalMatMul\" href=\"raw_ops/tridiagonalmatmul\">TridiagonalMatMul</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TridiagonalSolve\" href=\"raw_ops/tridiagonalsolve\">TridiagonalSolve</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TruncateDiv\" href=\"raw_ops/truncatediv\">TruncateDiv</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"TruncateMod\" href=\"raw_ops/truncatemod\">TruncateMod</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"TruncatedNormal\" href=\"raw_ops/truncatednormal\">TruncatedNormal</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Unbatch\" href=\"raw_ops/unbatch\">Unbatch</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnbatchDataset\" href=\"raw_ops/unbatchdataset\">UnbatchDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnbatchGrad\" href=\"raw_ops/unbatchgrad\">UnbatchGrad</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UncompressElement\" href=\"raw_ops/uncompresselement\">UncompressElement</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnicodeDecode\" href=\"raw_ops/unicodedecode\">UnicodeDecode</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnicodeDecodeWithOffsets\" href=\"raw_ops/unicodedecodewithoffsets\">UnicodeDecodeWithOffsets</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnicodeEncode\" href=\"raw_ops/unicodeencode\">UnicodeEncode</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnicodeScript\" href=\"raw_ops/unicodescript\">UnicodeScript</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnicodeTranscode\" href=\"raw_ops/unicodetranscode\">UnicodeTranscode</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UniformCandidateSampler\" href=\"raw_ops/uniformcandidatesampler\">UniformCandidateSampler</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Unique\" href=\"raw_ops/unique\">Unique</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UniqueDataset\" href=\"raw_ops/uniquedataset\">UniqueDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UniqueV2\" href=\"raw_ops/uniquev2\">UniqueV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UniqueWithCounts\" href=\"raw_ops/uniquewithcounts\">UniqueWithCounts</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UniqueWithCountsV2\" href=\"raw_ops/uniquewithcountsv2\">UniqueWithCountsV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Unpack\" href=\"raw_ops/unpack\">Unpack</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"UnravelIndex\" href=\"raw_ops/unravelindex\">UnravelIndex</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnsortedSegmentJoin\" href=\"raw_ops/unsortedsegmentjoin\">UnsortedSegmentJoin</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnsortedSegmentMax\" href=\"raw_ops/unsortedsegmentmax\">UnsortedSegmentMax</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"UnsortedSegmentMin\" href=\"raw_ops/unsortedsegmentmin\">UnsortedSegmentMin</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"UnsortedSegmentProd\" href=\"raw_ops/unsortedsegmentprod\">UnsortedSegmentProd</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"UnsortedSegmentSum\" href=\"raw_ops/unsortedsegmentsum\">UnsortedSegmentSum</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Unstage\" href=\"raw_ops/unstage\">Unstage</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UnwrapDatasetVariant\" href=\"raw_ops/unwrapdatasetvariant\">UnwrapDatasetVariant</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"UpperBound\" href=\"raw_ops/upperbound\">UpperBound</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"VarHandleOp\" href=\"raw_ops/varhandleop\">VarHandleOp</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"VarIsInitializedOp\" href=\"raw_ops/varisinitializedop\">VarIsInitializedOp</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Variable\" href=\"raw_ops/variable\">Variable</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"VariableShape\" href=\"raw_ops/variableshape\">VariableShape</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"VariableV2\" href=\"raw_ops/variablev2\">VariableV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Where\" href=\"raw_ops/where\">Where</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"While\" href=\"raw_ops/while\">While</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"WholeFileReader\" href=\"raw_ops/wholefilereader\">WholeFileReader</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"WholeFileReaderV2\" href=\"raw_ops/wholefilereaderv2\">WholeFileReaderV2</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WindowDataset\" href=\"raw_ops/windowdataset\">WindowDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WorkerHeartbeat\" href=\"raw_ops/workerheartbeat\">WorkerHeartbeat</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WrapDatasetVariant\" href=\"raw_ops/wrapdatasetvariant\">WrapDatasetVariant</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteAudioSummary\" href=\"raw_ops/writeaudiosummary\">WriteAudioSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteFile\" href=\"raw_ops/writefile\">WriteFile</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteGraphSummary\" href=\"raw_ops/writegraphsummary\">WriteGraphSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteHistogramSummary\" href=\"raw_ops/writehistogramsummary\">WriteHistogramSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteImageSummary\" href=\"raw_ops/writeimagesummary\">WriteImageSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteRawProtoSummary\" href=\"raw_ops/writerawprotosummary\">WriteRawProtoSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteScalarSummary\" href=\"raw_ops/writescalarsummary\">WriteScalarSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"WriteSummary\" href=\"raw_ops/writesummary\">WriteSummary</a></td> <td style=\"text-align: center\">❌</td> </tr> <tr> <td><a id=\"Xdivy\" href=\"raw_ops/xdivy\">Xdivy</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Xlog1py\" href=\"raw_ops/xlog1py\">Xlog1py</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Xlogy\" href=\"raw_ops/xlogy\">Xlogy</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ZerosLike\" href=\"raw_ops/zeroslike\">ZerosLike</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"Zeta\" href=\"raw_ops/zeta\">Zeta</a></td> <td style=\"text-align: center\">✔️</td> </tr> <tr> <td><a id=\"ZipDataset\" href=\"raw_ops/zipdataset\">ZipDataset</a></td> <td style=\"text-align: center\">❌</td> </tr> </tbody> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops</a>\n  </p>\n</div>\n","queue":"<h1 class=\"devsite-page-title\">Module: tf.queue</h1>       <p>Public API for tf.queue namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"queue/fifoqueue\"><code translate=\"no\" dir=\"ltr\">class FIFOQueue</code></a>: A queue implementation that dequeues elements in first-in first-out order.</p> <p><a href=\"queue/paddingfifoqueue\"><code translate=\"no\" dir=\"ltr\">class PaddingFIFOQueue</code></a>: A FIFOQueue that supports batching variable-sized tensors by padding.</p> <p><a href=\"queue/priorityqueue\"><code translate=\"no\" dir=\"ltr\">class PriorityQueue</code></a>: A queue implementation that dequeues elements in prioritized order.</p> <p><a href=\"queue/queuebase\"><code translate=\"no\" dir=\"ltr\">class QueueBase</code></a>: Base class for queue implementations.</p> <p><a href=\"queue/randomshufflequeue\"><code translate=\"no\" dir=\"ltr\">class RandomShuffleQueue</code></a>: A queue implementation that dequeues elements in a random order.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/queue\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/queue</a>\n  </p>\n</div>\n","random":"<h1 class=\"devsite-page-title\">Module: tf.random</h1>       <p>Public API for tf.random namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"random/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.random.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"random/algorithm\"><code translate=\"no\" dir=\"ltr\">class Algorithm</code></a>: An enumeration.</p> <p><a href=\"random/generator\"><code translate=\"no\" dir=\"ltr\">class Generator</code></a>: Random-number generator.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"random/all_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">all_candidate_sampler(...)</code></a>: Generate the set of all classes.</p> <p><a href=\"random/categorical\"><code translate=\"no\" dir=\"ltr\">categorical(...)</code></a>: Draws samples from a categorical distribution.</p> <p><a href=\"random/create_rng_state\"><code translate=\"no\" dir=\"ltr\">create_rng_state(...)</code></a>: Creates a RNG state from an integer or a vector.</p> <p><a href=\"random/fixed_unigram_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">fixed_unigram_candidate_sampler(...)</code></a>: Samples a set of classes using the provided (fixed) base distribution.</p> <p><a href=\"random/gamma\"><code translate=\"no\" dir=\"ltr\">gamma(...)</code></a>: Draws <code translate=\"no\" dir=\"ltr\">shape</code> samples from each of the given Gamma distribution(s).</p> <p><a href=\"random/get_global_generator\"><code translate=\"no\" dir=\"ltr\">get_global_generator(...)</code></a>: Retrieves the global generator.</p> <p><a href=\"random/learned_unigram_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">learned_unigram_candidate_sampler(...)</code></a>: Samples a set of classes from a distribution learned during training.</p> <p><a href=\"random/log_uniform_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">log_uniform_candidate_sampler(...)</code></a>: Samples a set of classes using a log-uniform (Zipfian) base distribution.</p> <p><a href=\"random/normal\"><code translate=\"no\" dir=\"ltr\">normal(...)</code></a>: Outputs random values from a normal distribution.</p> <p><a href=\"random/poisson\"><code translate=\"no\" dir=\"ltr\">poisson(...)</code></a>: Draws <code translate=\"no\" dir=\"ltr\">shape</code> samples from each of the given Poisson distribution(s).</p> <p><a href=\"random/set_global_generator\"><code translate=\"no\" dir=\"ltr\">set_global_generator(...)</code></a>: Replaces the global generator with another <code translate=\"no\" dir=\"ltr\">Generator</code> object.</p> <p><a href=\"random/set_seed\"><code translate=\"no\" dir=\"ltr\">set_seed(...)</code></a>: Sets the global random seed.</p> <p><a href=\"random/shuffle\"><code translate=\"no\" dir=\"ltr\">shuffle(...)</code></a>: Randomly shuffles a tensor along its first dimension.</p> <p><a href=\"random/stateless_binomial\"><code translate=\"no\" dir=\"ltr\">stateless_binomial(...)</code></a>: Outputs deterministic pseudorandom values from a binomial distribution.</p> <p><a href=\"random/stateless_categorical\"><code translate=\"no\" dir=\"ltr\">stateless_categorical(...)</code></a>: Draws deterministic pseudorandom samples from a categorical distribution.</p> <p><a href=\"random/stateless_gamma\"><code translate=\"no\" dir=\"ltr\">stateless_gamma(...)</code></a>: Outputs deterministic pseudorandom values from a gamma distribution.</p> <p><a href=\"random/stateless_normal\"><code translate=\"no\" dir=\"ltr\">stateless_normal(...)</code></a>: Outputs deterministic pseudorandom values from a normal distribution.</p> <p><a href=\"random/stateless_parameterized_truncated_normal\"><code translate=\"no\" dir=\"ltr\">stateless_parameterized_truncated_normal(...)</code></a>: Outputs random values from a truncated normal distribution.</p> <p><a href=\"random/stateless_poisson\"><code translate=\"no\" dir=\"ltr\">stateless_poisson(...)</code></a>: Outputs deterministic pseudorandom values from a Poisson distribution.</p> <p><a href=\"random/stateless_truncated_normal\"><code translate=\"no\" dir=\"ltr\">stateless_truncated_normal(...)</code></a>: Outputs deterministic pseudorandom values, truncated normally distributed.</p> <p><a href=\"random/stateless_uniform\"><code translate=\"no\" dir=\"ltr\">stateless_uniform(...)</code></a>: Outputs deterministic pseudorandom values from a uniform distribution.</p> <p><a href=\"random/truncated_normal\"><code translate=\"no\" dir=\"ltr\">truncated_normal(...)</code></a>: Outputs random values from a truncated normal distribution.</p> <p><a href=\"random/uniform\"><code translate=\"no\" dir=\"ltr\">uniform(...)</code></a>: Outputs random values from a uniform distribution.</p> <p><a href=\"random/uniform_candidate_sampler\"><code translate=\"no\" dir=\"ltr\">uniform_candidate_sampler(...)</code></a>: Samples a set of classes using a uniform base distribution.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/random\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/random</a>\n  </p>\n</div>\n","saved_model":"<h1 class=\"devsite-page-title\">Module: tf.saved_model</h1>       <p>Public API for tf.saved_model namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"saved_model/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.saved_model.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"saved_model/asset\"><code translate=\"no\" dir=\"ltr\">class Asset</code></a>: Represents a file asset to hermetically include in a SavedModel.</p> <p><a href=\"saved_model/loadoptions\"><code translate=\"no\" dir=\"ltr\">class LoadOptions</code></a>: Options for loading a SavedModel.</p> <p><a href=\"saved_model/saveoptions\"><code translate=\"no\" dir=\"ltr\">class SaveOptions</code></a>: Options for saving to SavedModel.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"saved_model/contains_saved_model\"><code translate=\"no\" dir=\"ltr\">contains_saved_model(...)</code></a>: Checks whether the provided export directory could contain a SavedModel.</p> <p><a href=\"saved_model/load\"><code translate=\"no\" dir=\"ltr\">load(...)</code></a>: Load a SavedModel from <code translate=\"no\" dir=\"ltr\">export_dir</code>.</p> <p><a href=\"saved_model/save\"><code translate=\"no\" dir=\"ltr\">save(...)</code></a>: Exports the Trackable object <code translate=\"no\" dir=\"ltr\">obj</code> to <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\">SavedModel format</a>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> ASSETS_DIRECTORY </td> <td> <code translate=\"no\" dir=\"ltr\">'assets'</code> </td> </tr>\n<tr> <td> ASSETS_KEY </td> <td> <code translate=\"no\" dir=\"ltr\">'saved_model_assets'</code> </td> </tr>\n<tr> <td> CLASSIFY_INPUTS </td> <td> <code translate=\"no\" dir=\"ltr\">'inputs'</code> </td> </tr>\n<tr> <td> CLASSIFY_METHOD_NAME </td> <td> <code translate=\"no\" dir=\"ltr\">'tensorflow/serving/classify'</code> </td> </tr>\n<tr> <td> CLASSIFY_OUTPUT_CLASSES </td> <td> <code translate=\"no\" dir=\"ltr\">'classes'</code> </td> </tr>\n<tr> <td> CLASSIFY_OUTPUT_SCORES </td> <td> <code translate=\"no\" dir=\"ltr\">'scores'</code> </td> </tr>\n<tr> <td> DEBUG_DIRECTORY </td> <td> <code translate=\"no\" dir=\"ltr\">'debug'</code> </td> </tr>\n<tr> <td> DEBUG_INFO_FILENAME_PB </td> <td> <code translate=\"no\" dir=\"ltr\">'saved_model_debug_info.pb'</code> </td> </tr>\n<tr> <td> DEFAULT_SERVING_SIGNATURE_DEF_KEY </td> <td> <code translate=\"no\" dir=\"ltr\">'serving_default'</code> </td> </tr>\n<tr> <td> GPU </td> <td> <code translate=\"no\" dir=\"ltr\">'gpu'</code> </td> </tr>\n<tr> <td> PREDICT_INPUTS </td> <td> <code translate=\"no\" dir=\"ltr\">'inputs'</code> </td> </tr>\n<tr> <td> PREDICT_METHOD_NAME </td> <td> <code translate=\"no\" dir=\"ltr\">'tensorflow/serving/predict'</code> </td> </tr>\n<tr> <td> PREDICT_OUTPUTS </td> <td> <code translate=\"no\" dir=\"ltr\">'outputs'</code> </td> </tr>\n<tr> <td> REGRESS_INPUTS </td> <td> <code translate=\"no\" dir=\"ltr\">'inputs'</code> </td> </tr>\n<tr> <td> REGRESS_METHOD_NAME </td> <td> <code translate=\"no\" dir=\"ltr\">'tensorflow/serving/regress'</code> </td> </tr>\n<tr> <td> REGRESS_OUTPUTS </td> <td> <code translate=\"no\" dir=\"ltr\">'outputs'</code> </td> </tr>\n<tr> <td> SAVED_MODEL_FILENAME_PB </td> <td> <code translate=\"no\" dir=\"ltr\">'saved_model.pb'</code> </td> </tr>\n<tr> <td> SAVED_MODEL_FILENAME_PBTXT </td> <td> <code translate=\"no\" dir=\"ltr\">'saved_model.pbtxt'</code> </td> </tr>\n<tr> <td> SAVED_MODEL_SCHEMA_VERSION </td> <td> <code translate=\"no\" dir=\"ltr\">1</code> </td> </tr>\n<tr> <td> SERVING </td> <td> <code translate=\"no\" dir=\"ltr\">'serve'</code> </td> </tr>\n<tr> <td> TPU </td> <td> <code translate=\"no\" dir=\"ltr\">'tpu'</code> </td> </tr>\n<tr> <td> TRAINING </td> <td> <code translate=\"no\" dir=\"ltr\">'train'</code> </td> </tr>\n<tr> <td> VARIABLES_DIRECTORY </td> <td> <code translate=\"no\" dir=\"ltr\">'variables'</code> </td> </tr>\n<tr> <td> VARIABLES_FILENAME </td> <td> <code translate=\"no\" dir=\"ltr\">'variables'</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/saved_model\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/saved_model</a>\n  </p>\n</div>\n","sets":"<h1 class=\"devsite-page-title\">Module: tf.sets</h1>       <p>Tensorflow set operations.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"sets/difference\"><code translate=\"no\" dir=\"ltr\">difference(...)</code></a>: Compute set difference of elements in last dimension of <code translate=\"no\" dir=\"ltr\">a</code> and <code translate=\"no\" dir=\"ltr\">b</code>.</p> <p><a href=\"sets/intersection\"><code translate=\"no\" dir=\"ltr\">intersection(...)</code></a>: Compute set intersection of elements in last dimension of <code translate=\"no\" dir=\"ltr\">a</code> and <code translate=\"no\" dir=\"ltr\">b</code>.</p> <p><a href=\"sets/size\"><code translate=\"no\" dir=\"ltr\">size(...)</code></a>: Compute number of unique elements along last dimension of <code translate=\"no\" dir=\"ltr\">a</code>.</p> <p><a href=\"sets/union\"><code translate=\"no\" dir=\"ltr\">union(...)</code></a>: Compute set union of elements in last dimension of <code translate=\"no\" dir=\"ltr\">a</code> and <code translate=\"no\" dir=\"ltr\">b</code>.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sets\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sets</a>\n  </p>\n</div>\n","types":"<h1 class=\"devsite-page-title\">Module: tf.types</h1>       <p>Public TensorFlow type definitions.</p> <p>For details, see <a href=\"https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md\">https://github.com/tensorflow/community/blob/master/rfcs/20200211-tf-types.md</a></p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"types/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.types.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/types\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/types</a>\n  </p>\n</div>\n","summary":"<h1 class=\"devsite-page-title\">Module: tf.summary</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorboard/tree/master/tensorboard/summary/_tf/summary/__init__.py\">  View source on GitHub </a> </td> </table> <p>Operations for writing summary data, for use in analysis and visualization.</p> <p>The <a href=\"summary\"><code translate=\"no\" dir=\"ltr\">tf.summary</code></a> module provides APIs for writing summary data. This data can be visualized in TensorBoard, the visualization toolkit that comes with TensorFlow. See the <a href=\"https://www.tensorflow.org/tensorboard\">TensorBoard website</a> for more detailed tutorials about how to use these APIs, or some quick examples below.</p> <p>Example usage with eager execution, the default in TF 2.0:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">writer = tf.summary.create_file_writer(\"/tmp/mylogs\")\nwith writer.as_default():\n  for step in range(100):\n    # other model code would go here\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\n    writer.flush()\n</pre> <p>Example usage with <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a> graph execution:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">writer = tf.summary.create_file_writer(\"/tmp/mylogs\")\n\n@tf.function\ndef my_func(step):\n  # other model code would go here\n  with writer.as_default():\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\n\nfor step in range(100):\n  my_func(step)\n  writer.flush()\n</pre> <p>Example usage with legacy TF 1.x graph execution:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with tf.compat.v1.Graph().as_default():\n  step = tf.Variable(0, dtype=tf.int64)\n  step_update = step.assign_add(1)\n  writer = tf.summary.create_file_writer(\"/tmp/mylogs\")\n  with writer.as_default():\n    tf.summary.scalar(\"my_metric\", 0.5, step=step)\n  all_summary_ops = tf.compat.v1.summary.all_v2_summary_ops()\n  writer_flush = writer.flush()\n\n  sess = tf.compat.v1.Session()\n  sess.run([writer.init(), step.initializer])\n  for i in range(100):\n    sess.run(all_summary_ops)\n    sess.run(step_update)\n    sess.run(writer_flush)\n</pre> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"summary/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.summary.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"summary/summarywriter\"><code translate=\"no\" dir=\"ltr\">class SummaryWriter</code></a>: Interface representing a stateful summary writer object.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"summary/audio\"><code translate=\"no\" dir=\"ltr\">audio(...)</code></a>: Write an audio summary.</p> <p><a href=\"summary/create_file_writer\"><code translate=\"no\" dir=\"ltr\">create_file_writer(...)</code></a>: Creates a summary file writer for the given log directory.</p> <p><a href=\"summary/create_noop_writer\"><code translate=\"no\" dir=\"ltr\">create_noop_writer(...)</code></a>: Returns a summary writer that does nothing.</p> <p><a href=\"summary/flush\"><code translate=\"no\" dir=\"ltr\">flush(...)</code></a>: Forces summary writer to send any buffered data to storage.</p> <p><a href=\"summary/histogram\"><code translate=\"no\" dir=\"ltr\">histogram(...)</code></a>: Write a histogram summary.</p> <p><a href=\"summary/image\"><code translate=\"no\" dir=\"ltr\">image(...)</code></a>: Write an image summary.</p> <p><a href=\"summary/record_if\"><code translate=\"no\" dir=\"ltr\">record_if(...)</code></a>: Sets summary recording on or off per the provided boolean value.</p> <p><a href=\"summary/scalar\"><code translate=\"no\" dir=\"ltr\">scalar(...)</code></a>: Write a scalar summary.</p> <p><a href=\"summary/should_record_summaries\"><code translate=\"no\" dir=\"ltr\">should_record_summaries(...)</code></a>: Returns boolean Tensor which is true if summaries should be recorded.</p> <p><a href=\"summary/text\"><code translate=\"no\" dir=\"ltr\">text(...)</code></a>: Write a text summary.</p> <p><a href=\"summary/trace_export\"><code translate=\"no\" dir=\"ltr\">trace_export(...)</code></a>: Stops and exports the active trace as a Summary and/or profile file.</p> <p><a href=\"summary/trace_off\"><code translate=\"no\" dir=\"ltr\">trace_off(...)</code></a>: Stops the current trace and discards any collected information.</p> <p><a href=\"summary/trace_on\"><code translate=\"no\" dir=\"ltr\">trace_on(...)</code></a>: Starts a trace to record computation graphs and profiling information.</p> <p><a href=\"summary/write\"><code translate=\"no\" dir=\"ltr\">write(...)</code></a>: Writes a generic summary to the default SummaryWriter if one exists.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/summary\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/summary</a>\n  </p>\n</div>\n","sysconfig":"<h1 class=\"devsite-page-title\">Module: tf.sysconfig</h1>       <p>System configuration library.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"sysconfig/get_build_info\"><code translate=\"no\" dir=\"ltr\">get_build_info(...)</code></a>: Get a dictionary describing TensorFlow's build environment.</p> <p><a href=\"sysconfig/get_compile_flags\"><code translate=\"no\" dir=\"ltr\">get_compile_flags(...)</code></a>: Get the compilation flags for custom operators.</p> <p><a href=\"sysconfig/get_include\"><code translate=\"no\" dir=\"ltr\">get_include(...)</code></a>: Get the directory containing the TensorFlow C++ header files.</p> <p><a href=\"sysconfig/get_lib\"><code translate=\"no\" dir=\"ltr\">get_lib(...)</code></a>: Get the directory containing the TensorFlow framework library.</p> <p><a href=\"sysconfig/get_link_flags\"><code translate=\"no\" dir=\"ltr\">get_link_flags(...)</code></a>: Get the link flags for custom operators.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> CXX11_ABI_FLAG </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> MONOLITHIC_BUILD </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sysconfig\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sysconfig</a>\n  </p>\n</div>\n","tpu":"<h1 class=\"devsite-page-title\">Module: tf.tpu</h1>       <p>Ops related to Tensor Processing Units.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"tpu/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.tpu.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/tpu\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/tpu</a>\n  </p>\n</div>\n","sparse":"<h1 class=\"devsite-page-title\">Module: tf.sparse</h1>       <p>Sparse Tensor Representation.</p> <p>See also <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">class SparseTensor</code></a>: Represents a sparse tensor.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"sparse/add\"><code translate=\"no\" dir=\"ltr\">add(...)</code></a>: Adds two tensors, at least one of each is a <code translate=\"no\" dir=\"ltr\">SparseTensor</code>.</p> <p><a href=\"sparse/bincount\"><code translate=\"no\" dir=\"ltr\">bincount(...)</code></a>: Count the number of times an integer value appears in a tensor.</p> <p><a href=\"sparse/concat\"><code translate=\"no\" dir=\"ltr\">concat(...)</code></a>: Concatenates a list of <code translate=\"no\" dir=\"ltr\">SparseTensor</code> along the specified dimension. (deprecated arguments)</p> <p><a href=\"sparse/cross\"><code translate=\"no\" dir=\"ltr\">cross(...)</code></a>: Generates sparse cross from a list of sparse and dense tensors.</p> <p><a href=\"sparse/cross_hashed\"><code translate=\"no\" dir=\"ltr\">cross_hashed(...)</code></a>: Generates hashed sparse cross from a list of sparse and dense tensors.</p> <p><a href=\"sparse/expand_dims\"><code translate=\"no\" dir=\"ltr\">expand_dims(...)</code></a>: Returns a tensor with an length 1 axis inserted at index <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"sparse/eye\"><code translate=\"no\" dir=\"ltr\">eye(...)</code></a>: Creates a two-dimensional sparse tensor with ones along the diagonal.</p> <p><a href=\"sparse/fill_empty_rows\"><code translate=\"no\" dir=\"ltr\">fill_empty_rows(...)</code></a>: Fills empty rows in the input 2-D <code translate=\"no\" dir=\"ltr\">SparseTensor</code> with a default value.</p> <p><a href=\"sparse/from_dense\"><code translate=\"no\" dir=\"ltr\">from_dense(...)</code></a>: Converts a dense tensor into a sparse tensor.</p> <p><a href=\"sparse/map_values\"><code translate=\"no\" dir=\"ltr\">map_values(...)</code></a>: Applies <code translate=\"no\" dir=\"ltr\">op</code> to the <code translate=\"no\" dir=\"ltr\">.values</code> tensor of one or more <code translate=\"no\" dir=\"ltr\">SparseTensor</code>s.</p> <p><a href=\"sparse/mask\"><code translate=\"no\" dir=\"ltr\">mask(...)</code></a>: Masks elements of <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>.</p> <p><a href=\"sparse/maximum\"><code translate=\"no\" dir=\"ltr\">maximum(...)</code></a>: Returns the element-wise max of two SparseTensors.</p> <p><a href=\"sparse/minimum\"><code translate=\"no\" dir=\"ltr\">minimum(...)</code></a>: Returns the element-wise min of two SparseTensors.</p> <p><a href=\"sparse/reduce_max\"><code translate=\"no\" dir=\"ltr\">reduce_max(...)</code></a>: Computes the max of elements across dimensions of a SparseTensor.</p> <p><a href=\"sparse/reduce_sum\"><code translate=\"no\" dir=\"ltr\">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a SparseTensor.</p> <p><a href=\"sparse/reorder\"><code translate=\"no\" dir=\"ltr\">reorder(...)</code></a>: Reorders a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> into the canonical, row-major ordering.</p> <p><a href=\"sparse/reset_shape\"><code translate=\"no\" dir=\"ltr\">reset_shape(...)</code></a>: Resets the shape of a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> with indices and values unchanged.</p> <p><a href=\"sparse/reshape\"><code translate=\"no\" dir=\"ltr\">reshape(...)</code></a>: Reshapes a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> to represent values in a new dense shape.</p> <p><a href=\"sparse/retain\"><code translate=\"no\" dir=\"ltr\">retain(...)</code></a>: Retains specified non-empty values within a <code translate=\"no\" dir=\"ltr\">SparseTensor</code>.</p> <p><a href=\"sparse/segment_mean\"><code translate=\"no\" dir=\"ltr\">segment_mean(...)</code></a>: Computes the mean along sparse segments of a tensor.</p> <p><a href=\"sparse/segment_sqrt_n\"><code translate=\"no\" dir=\"ltr\">segment_sqrt_n(...)</code></a>: Computes the sum along sparse segments of a tensor divided by the sqrt(N).</p> <p><a href=\"sparse/segment_sum\"><code translate=\"no\" dir=\"ltr\">segment_sum(...)</code></a>: Computes the sum along sparse segments of a tensor.</p> <p><a href=\"sparse/slice\"><code translate=\"no\" dir=\"ltr\">slice(...)</code></a>: Slice a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> based on the <code translate=\"no\" dir=\"ltr\">start</code> and `size.</p> <p><a href=\"sparse/softmax\"><code translate=\"no\" dir=\"ltr\">softmax(...)</code></a>: Applies softmax to a batched N-D <code translate=\"no\" dir=\"ltr\">SparseTensor</code>.</p> <p><a href=\"sparse/sparse_dense_matmul\"><code translate=\"no\" dir=\"ltr\">sparse_dense_matmul(...)</code></a>: Multiply SparseTensor (or dense Matrix) (of rank 2) \"A\" by dense matrix</p> <p><a href=\"sparse/split\"><code translate=\"no\" dir=\"ltr\">split(...)</code></a>: Split a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> into <code translate=\"no\" dir=\"ltr\">num_split</code> tensors along <code translate=\"no\" dir=\"ltr\">axis</code>.</p> <p><a href=\"sparse/to_dense\"><code translate=\"no\" dir=\"ltr\">to_dense(...)</code></a>: Converts a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> into a dense tensor.</p> <p><a href=\"sparse/to_indicator\"><code translate=\"no\" dir=\"ltr\">to_indicator(...)</code></a>: Converts a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> of ids into a dense bool indicator tensor.</p> <p><a href=\"sparse/transpose\"><code translate=\"no\" dir=\"ltr\">transpose(...)</code></a>: Transposes a <code translate=\"no\" dir=\"ltr\">SparseTensor</code></p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sparse\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sparse</a>\n  </p>\n</div>\n","strings":"<h1 class=\"devsite-page-title\">Module: tf.strings</h1>       <p>Operations for working with string Tensors.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"strings/as_string\"><code translate=\"no\" dir=\"ltr\">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href=\"strings/bytes_split\"><code translate=\"no\" dir=\"ltr\">bytes_split(...)</code></a>: Split string elements of <code translate=\"no\" dir=\"ltr\">input</code> into bytes.</p> <p><a href=\"strings/format\"><code translate=\"no\" dir=\"ltr\">format(...)</code></a>: Formats a string template using a list of tensors.</p> <p><a href=\"strings/join\"><code translate=\"no\" dir=\"ltr\">join(...)</code></a>: Perform element-wise concatenation of a list of string tensors.</p> <p><a href=\"strings/length\"><code translate=\"no\" dir=\"ltr\">length(...)</code></a>: String lengths of <code translate=\"no\" dir=\"ltr\">input</code>.</p> <p><a href=\"strings/lower\"><code translate=\"no\" dir=\"ltr\">lower(...)</code></a>: Converts all uppercase characters into their respective lowercase replacements.</p> <p><a href=\"strings/ngrams\"><code translate=\"no\" dir=\"ltr\">ngrams(...)</code></a>: Create a tensor of n-grams based on <code translate=\"no\" dir=\"ltr\">data</code>.</p> <p><a href=\"strings/reduce_join\"><code translate=\"no\" dir=\"ltr\">reduce_join(...)</code></a>: Joins all strings into a single string, or joins along an axis.</p> <p><a href=\"strings/regex_full_match\"><code translate=\"no\" dir=\"ltr\">regex_full_match(...)</code></a>: Check if the input matches the regex pattern.</p> <p><a href=\"strings/regex_replace\"><code translate=\"no\" dir=\"ltr\">regex_replace(...)</code></a>: Replace elements of <code translate=\"no\" dir=\"ltr\">input</code> matching regex <code translate=\"no\" dir=\"ltr\">pattern</code> with <code translate=\"no\" dir=\"ltr\">rewrite</code>.</p> <p><a href=\"strings/split\"><code translate=\"no\" dir=\"ltr\">split(...)</code></a>: Split elements of <code translate=\"no\" dir=\"ltr\">input</code> based on <code translate=\"no\" dir=\"ltr\">sep</code> into a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p><a href=\"strings/strip\"><code translate=\"no\" dir=\"ltr\">strip(...)</code></a>: Strip leading and trailing whitespaces from the Tensor.</p> <p><a href=\"strings/substr\"><code translate=\"no\" dir=\"ltr\">substr(...)</code></a>: Return substrings from <code translate=\"no\" dir=\"ltr\">Tensor</code> of strings.</p> <p><a href=\"strings/to_hash_bucket\"><code translate=\"no\" dir=\"ltr\">to_hash_bucket(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href=\"strings/to_hash_bucket_fast\"><code translate=\"no\" dir=\"ltr\">to_hash_bucket_fast(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href=\"strings/to_hash_bucket_strong\"><code translate=\"no\" dir=\"ltr\">to_hash_bucket_strong(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href=\"strings/to_number\"><code translate=\"no\" dir=\"ltr\">to_number(...)</code></a>: Converts each string in the input Tensor to the specified numeric type.</p> <p><a href=\"strings/unicode_decode\"><code translate=\"no\" dir=\"ltr\">unicode_decode(...)</code></a>: Decodes each string in <code translate=\"no\" dir=\"ltr\">input</code> into a sequence of Unicode code points.</p> <p><a href=\"strings/unicode_decode_with_offsets\"><code translate=\"no\" dir=\"ltr\">unicode_decode_with_offsets(...)</code></a>: Decodes each string into a sequence of code points with start offsets.</p> <p><a href=\"strings/unicode_encode\"><code translate=\"no\" dir=\"ltr\">unicode_encode(...)</code></a>: Encodes each sequence of Unicode code points in <code translate=\"no\" dir=\"ltr\">input</code> into a string.</p> <p><a href=\"strings/unicode_script\"><code translate=\"no\" dir=\"ltr\">unicode_script(...)</code></a>: Determine the script codes of a given tensor of Unicode integer code points.</p> <p><a href=\"strings/unicode_split\"><code translate=\"no\" dir=\"ltr\">unicode_split(...)</code></a>: Splits each string in <code translate=\"no\" dir=\"ltr\">input</code> into a sequence of Unicode code points.</p> <p><a href=\"strings/unicode_split_with_offsets\"><code translate=\"no\" dir=\"ltr\">unicode_split_with_offsets(...)</code></a>: Splits each string into a sequence of code points with start offsets.</p> <p><a href=\"strings/unicode_transcode\"><code translate=\"no\" dir=\"ltr\">unicode_transcode(...)</code></a>: Transcode the input text from a source encoding to a destination encoding.</p> <p><a href=\"strings/unsorted_segment_join\"><code translate=\"no\" dir=\"ltr\">unsorted_segment_join(...)</code></a>: Joins the elements of <code translate=\"no\" dir=\"ltr\">inputs</code> based on <code translate=\"no\" dir=\"ltr\">segment_ids</code>.</p> <p><a href=\"strings/upper\"><code translate=\"no\" dir=\"ltr\">upper(...)</code></a>: Converts all lowercase characters into their respective uppercase replacements.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/strings\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/strings</a>\n  </p>\n</div>\n","signal":"<h1 class=\"devsite-page-title\">Module: tf.signal</h1>       <p>Signal processing operations.</p> <p>See the <a href=\"https://tensorflow.org/api_guides/python/contrib.signal\">tf.signal</a> guide.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"signal/dct\"><code translate=\"no\" dir=\"ltr\">dct(...)</code></a>: Computes the 1D [Discrete Cosine Transform (DCT)][dct] of <code translate=\"no\" dir=\"ltr\">input</code>.</p> <p><a href=\"signal/fft\"><code translate=\"no\" dir=\"ltr\">fft(...)</code></a>: Fast Fourier transform.</p> <p><a href=\"signal/fft2d\"><code translate=\"no\" dir=\"ltr\">fft2d(...)</code></a>: 2D fast Fourier transform.</p> <p><a href=\"signal/fft3d\"><code translate=\"no\" dir=\"ltr\">fft3d(...)</code></a>: 3D fast Fourier transform.</p> <p><a href=\"signal/fftshift\"><code translate=\"no\" dir=\"ltr\">fftshift(...)</code></a>: Shift the zero-frequency component to the center of the spectrum.</p> <p><a href=\"signal/frame\"><code translate=\"no\" dir=\"ltr\">frame(...)</code></a>: Expands <code translate=\"no\" dir=\"ltr\">signal</code>'s <code translate=\"no\" dir=\"ltr\">axis</code> dimension into frames of <code translate=\"no\" dir=\"ltr\">frame_length</code>.</p> <p><a href=\"signal/hamming_window\"><code translate=\"no\" dir=\"ltr\">hamming_window(...)</code></a>: Generate a <a href=\"https://en.wikipedia.org/wiki/Window_function#Hamming_window\">Hamming</a> window.</p> <p><a href=\"signal/hann_window\"><code translate=\"no\" dir=\"ltr\">hann_window(...)</code></a>: Generate a <a href=\"https://en.wikipedia.org/wiki/Window_function#Hann_window\">Hann window</a>.</p> <p><a href=\"signal/idct\"><code translate=\"no\" dir=\"ltr\">idct(...)</code></a>: Computes the 1D [Inverse Discrete Cosine Transform (DCT)][idct] of <code translate=\"no\" dir=\"ltr\">input</code>.</p> <p><a href=\"signal/ifft\"><code translate=\"no\" dir=\"ltr\">ifft(...)</code></a>: Inverse fast Fourier transform.</p> <p><a href=\"signal/ifft2d\"><code translate=\"no\" dir=\"ltr\">ifft2d(...)</code></a>: Inverse 2D fast Fourier transform.</p> <p><a href=\"signal/ifft3d\"><code translate=\"no\" dir=\"ltr\">ifft3d(...)</code></a>: Inverse 3D fast Fourier transform.</p> <p><a href=\"signal/ifftshift\"><code translate=\"no\" dir=\"ltr\">ifftshift(...)</code></a>: The inverse of fftshift.</p> <p><a href=\"signal/inverse_mdct\"><code translate=\"no\" dir=\"ltr\">inverse_mdct(...)</code></a>: Computes the inverse modified DCT of <code translate=\"no\" dir=\"ltr\">mdcts</code>.</p> <p><a href=\"signal/inverse_stft\"><code translate=\"no\" dir=\"ltr\">inverse_stft(...)</code></a>: Computes the inverse <a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\">Short-time Fourier Transform</a> of <code translate=\"no\" dir=\"ltr\">stfts</code>.</p> <p><a href=\"signal/inverse_stft_window_fn\"><code translate=\"no\" dir=\"ltr\">inverse_stft_window_fn(...)</code></a>: Generates a window function that can be used in <code translate=\"no\" dir=\"ltr\">inverse_stft</code>.</p> <p><a href=\"signal/irfft\"><code translate=\"no\" dir=\"ltr\">irfft(...)</code></a>: Inverse real-valued fast Fourier transform.</p> <p><a href=\"signal/irfft2d\"><code translate=\"no\" dir=\"ltr\">irfft2d(...)</code></a>: Inverse 2D real-valued fast Fourier transform.</p> <p><a href=\"signal/irfft3d\"><code translate=\"no\" dir=\"ltr\">irfft3d(...)</code></a>: Inverse 3D real-valued fast Fourier transform.</p> <p><a href=\"signal/kaiser_bessel_derived_window\"><code translate=\"no\" dir=\"ltr\">kaiser_bessel_derived_window(...)</code></a>: Generate a [Kaiser Bessel derived window][kbd].</p> <p><a href=\"signal/kaiser_window\"><code translate=\"no\" dir=\"ltr\">kaiser_window(...)</code></a>: Generate a [Kaiser window][kaiser].</p> <p><a href=\"signal/linear_to_mel_weight_matrix\"><code translate=\"no\" dir=\"ltr\">linear_to_mel_weight_matrix(...)</code></a>: Returns a matrix to warp linear scale spectrograms to the <a href=\"https://en.wikipedia.org/wiki/Mel_scale\">mel scale</a>.</p> <p><a href=\"signal/mdct\"><code translate=\"no\" dir=\"ltr\">mdct(...)</code></a>: Computes the [Modified Discrete Cosine Transform][mdct] of <code translate=\"no\" dir=\"ltr\">signals</code>.</p> <p><a href=\"signal/mfccs_from_log_mel_spectrograms\"><code translate=\"no\" dir=\"ltr\">mfccs_from_log_mel_spectrograms(...)</code></a>: Computes <a href=\"https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\">MFCCs</a> of <code translate=\"no\" dir=\"ltr\">log_mel_spectrograms</code>.</p> <p><a href=\"signal/overlap_and_add\"><code translate=\"no\" dir=\"ltr\">overlap_and_add(...)</code></a>: Reconstructs a signal from a framed representation.</p> <p><a href=\"signal/rfft\"><code translate=\"no\" dir=\"ltr\">rfft(...)</code></a>: Real-valued fast Fourier transform.</p> <p><a href=\"signal/rfft2d\"><code translate=\"no\" dir=\"ltr\">rfft2d(...)</code></a>: 2D real-valued fast Fourier transform.</p> <p><a href=\"signal/rfft3d\"><code translate=\"no\" dir=\"ltr\">rfft3d(...)</code></a>: 3D real-valued fast Fourier transform.</p> <p><a href=\"signal/stft\"><code translate=\"no\" dir=\"ltr\">stft(...)</code></a>: Computes the <a href=\"https://en.wikipedia.org/wiki/Short-time_Fourier_transform\">Short-time Fourier Transform</a> of <code translate=\"no\" dir=\"ltr\">signals</code>.</p> <p><a href=\"signal/vorbis_window\"><code translate=\"no\" dir=\"ltr\">vorbis_window(...)</code></a>: Generate a [Vorbis power complementary window][vorbis].</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/signal\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/signal</a>\n  </p>\n</div>\n","test":"<h1 class=\"devsite-page-title\">Module: tf.test</h1>       <p>Testing.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"test/benchmark\"><code translate=\"no\" dir=\"ltr\">class Benchmark</code></a>: Abstract class that provides helpers for TensorFlow benchmarks.</p> <p><a href=\"test/testcase\"><code translate=\"no\" dir=\"ltr\">class TestCase</code></a>: Base class for tests that need to test TensorFlow.</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"test/assert_equal_graph_def\"><code translate=\"no\" dir=\"ltr\">assert_equal_graph_def(...)</code></a>: Asserts that two <code translate=\"no\" dir=\"ltr\">GraphDef</code>s are (mostly) the same.</p> <p><a href=\"test/benchmark_config\"><code translate=\"no\" dir=\"ltr\">benchmark_config(...)</code></a>: Returns a tf.compat.v1.ConfigProto for disabling the dependency optimizer.</p> <p><a href=\"test/compute_gradient\"><code translate=\"no\" dir=\"ltr\">compute_gradient(...)</code></a>: Computes the theoretical and numeric Jacobian of <code translate=\"no\" dir=\"ltr\">f</code>.</p> <p><a href=\"test/create_local_cluster\"><code translate=\"no\" dir=\"ltr\">create_local_cluster(...)</code></a>: Create and start local servers and return the associated <code translate=\"no\" dir=\"ltr\">Server</code> objects.</p> <p><a href=\"test/gpu_device_name\"><code translate=\"no\" dir=\"ltr\">gpu_device_name(...)</code></a>: Returns the name of a GPU device if available or the empty string.</p> <p><a href=\"test/is_built_with_cuda\"><code translate=\"no\" dir=\"ltr\">is_built_with_cuda(...)</code></a>: Returns whether TensorFlow was built with CUDA (GPU) support.</p> <p><a href=\"test/is_built_with_gpu_support\"><code translate=\"no\" dir=\"ltr\">is_built_with_gpu_support(...)</code></a>: Returns whether TensorFlow was built with GPU (i.e. CUDA or ROCm) support.</p> <p><a href=\"test/is_built_with_rocm\"><code translate=\"no\" dir=\"ltr\">is_built_with_rocm(...)</code></a>: Returns whether TensorFlow was built with ROCm (GPU) support.</p> <p><a href=\"test/is_built_with_xla\"><code translate=\"no\" dir=\"ltr\">is_built_with_xla(...)</code></a>: Returns whether TensorFlow was built with XLA support.</p> <p><a href=\"test/is_gpu_available\"><code translate=\"no\" dir=\"ltr\">is_gpu_available(...)</code></a>: Returns whether TensorFlow can access a GPU. (deprecated)</p> <p><a href=\"test/main\"><code translate=\"no\" dir=\"ltr\">main(...)</code></a>: Runs all unit tests.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/test\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/test</a>\n  </p>\n</div>\n","train":"<h1 class=\"devsite-page-title\">Module: tf.train</h1>       <p>Support for training models.</p> <p>See the <a href=\"https://tensorflow.org/api_guides/python/train\">Training</a> guide.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"train/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.train.experimental namespace.</p> <h2 id=\"classes\" data-text=\"Classes\">Classes</h2> <p><a href=\"train/byteslist\"><code translate=\"no\" dir=\"ltr\">class BytesList</code></a>: A ProtocolMessage</p> <p><a href=\"train/checkpoint\"><code translate=\"no\" dir=\"ltr\">class Checkpoint</code></a>: Manages saving/restoring trackable values to disk.</p> <p><a href=\"train/checkpointmanager\"><code translate=\"no\" dir=\"ltr\">class CheckpointManager</code></a>: Manages multiple checkpoints by keeping some and deleting unneeded ones.</p> <p><a href=\"train/checkpointoptions\"><code translate=\"no\" dir=\"ltr\">class CheckpointOptions</code></a>: Options for constructing a Checkpoint.</p> <p><a href=\"train/clusterdef\"><code translate=\"no\" dir=\"ltr\">class ClusterDef</code></a>: A ProtocolMessage</p> <p><a href=\"train/clusterspec\"><code translate=\"no\" dir=\"ltr\">class ClusterSpec</code></a>: Represents a cluster as a set of \"tasks\", organized into \"jobs\".</p> <p><a href=\"train/coordinator\"><code translate=\"no\" dir=\"ltr\">class Coordinator</code></a>: A coordinator for threads.</p> <p><a href=\"train/example\"><code translate=\"no\" dir=\"ltr\">class Example</code></a>: A ProtocolMessage</p> <p><a href=\"train/exponentialmovingaverage\"><code translate=\"no\" dir=\"ltr\">class ExponentialMovingAverage</code></a>: Maintains moving averages of variables by employing an exponential decay.</p> <p><a href=\"train/feature\"><code translate=\"no\" dir=\"ltr\">class Feature</code></a>: A ProtocolMessage</p> <p><a href=\"train/featurelist\"><code translate=\"no\" dir=\"ltr\">class FeatureList</code></a>: A ProtocolMessage</p> <p><a href=\"train/featurelists\"><code translate=\"no\" dir=\"ltr\">class FeatureLists</code></a>: A ProtocolMessage</p> <p><a href=\"train/features\"><code translate=\"no\" dir=\"ltr\">class Features</code></a>: A ProtocolMessage</p> <p><a href=\"train/floatlist\"><code translate=\"no\" dir=\"ltr\">class FloatList</code></a>: A ProtocolMessage</p> <p><a href=\"train/int64list\"><code translate=\"no\" dir=\"ltr\">class Int64List</code></a>: A ProtocolMessage</p> <p><a href=\"train/jobdef\"><code translate=\"no\" dir=\"ltr\">class JobDef</code></a>: A ProtocolMessage</p> <p><a href=\"train/sequenceexample\"><code translate=\"no\" dir=\"ltr\">class SequenceExample</code></a>: A ProtocolMessage</p> <p><a href=\"train/serverdef\"><code translate=\"no\" dir=\"ltr\">class ServerDef</code></a>: A ProtocolMessage</p> <h2 id=\"functions\" data-text=\"Functions\">Functions</h2> <p><a href=\"train/checkpoints_iterator\"><code translate=\"no\" dir=\"ltr\">checkpoints_iterator(...)</code></a>: Continuously yield new checkpoint files as they appear.</p> <p><a href=\"train/get_checkpoint_state\"><code translate=\"no\" dir=\"ltr\">get_checkpoint_state(...)</code></a>: Returns CheckpointState proto from the \"checkpoint\" file.</p> <p><a href=\"train/latest_checkpoint\"><code translate=\"no\" dir=\"ltr\">latest_checkpoint(...)</code></a>: Finds the filename of latest saved checkpoint file.</p> <p><a href=\"train/list_variables\"><code translate=\"no\" dir=\"ltr\">list_variables(...)</code></a>: Lists the checkpoint keys and shapes of variables in a checkpoint.</p> <p><a href=\"train/load_checkpoint\"><code translate=\"no\" dir=\"ltr\">load_checkpoint(...)</code></a>: Returns <code translate=\"no\" dir=\"ltr\">CheckpointReader</code> for checkpoint found in <code translate=\"no\" dir=\"ltr\">ckpt_dir_or_file</code>.</p> <p><a href=\"train/load_variable\"><code translate=\"no\" dir=\"ltr\">load_variable(...)</code></a>: Returns the tensor value of the given variable in the checkpoint.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/train\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/train</a>\n  </p>\n</div>\n","devicespec":"<h1 class=\"devsite-page-title\">tf.DeviceSpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L52-L394\">  View source on GitHub </a> </td> </table> <p>Represents a (possibly partial) specification for a TensorFlow device.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.DeviceSpec(\n    job=None, replica=None, task=None, device_type=None, device_index=None\n)\n</pre>  <p><code translate=\"no\" dir=\"ltr\">DeviceSpec</code>s are used throughout TensorFlow to describe where state is stored and computations occur. Using <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> allows you to parse device spec strings to verify their validity, merge them or compose them programmatically.</p> <h4 id=\"example\" data-text=\"Example:\">Example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\"># Place the operations on device \"GPU:0\" in the \"ps\" job.\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(device_spec.to_string()):\n  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n</pre> <p>With eager execution disabled (by default in TensorFlow 1.x and by calling disable_eager_execution() in TensorFlow 2.x), the following syntax can be used:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">tf.compat.v1.disable_eager_execution()\n\n# Same as previous\ndevice_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\n# No need of .to_string() method.\nwith tf.device(device_spec):\n  my_var = tf.Variable(..., name=\"my_variable\")\n  squared_var = tf.square(my_var)\n ```\n\nIf a `DeviceSpec` is partially specified, it will be merged with other\n`DeviceSpec`s according to the scope in which it is defined. `DeviceSpec`\ncomponents defined in inner scopes take precedence over those defined in\nouter scopes.\n\n```python\ngpu0_spec = DeviceSpec(job=\"ps\", device_type=\"GPU\", device_index=0)\nwith tf.device(DeviceSpec(job=\"train\").to_string()):\n  with tf.device(gpu0_spec.to_string()):\n    # Nodes created here will be assigned to /job:ps/device:GPU:0.\n  with tf.device(DeviceSpec(device_type=\"GPU\", device_index=1).to_string()):\n    # Nodes created here will be assigned to /job:train/device:GPU:1.\n</pre> <p>A <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> consists of 5 components -- each of which is optionally specified:</p> <ul> <li>Job: The job name.</li> <li>Replica: The replica index.</li> <li>Task: The task index.</li> <li>Device type: The device type string (e.g. \"CPU\" or \"GPU\").</li> <li>Device index: The device index.</li> </ul>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">job</code> </td> <td> string. Optional job name. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">replica</code> </td> <td> int. Optional replica index. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">task</code> </td> <td> int. Optional task index. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">device_type</code> </td> <td> Optional device type string (e.g. \"CPU\" or \"GPU\") </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">device_index</code> </td> <td> int. Optional device index. If left unspecified, device represents 'any' device_index. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">device_index</code> </td> <td> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">device_type</code> </td> <td> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">job</code> </td> <td> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">replica</code> </td> <td> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">task</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"from_string\" data-text=\"from_string\"><code translate=\"no\" dir=\"ltr\">from_string</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L143-L158\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_string(\n    spec\n)\n</pre> <p>Construct a <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> from a string.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">spec</code> </td> <td> a string of the form /job:<name>/replica:<id>/task:<id>/device:CPU:<id> or /job:<name>/replica:<id>/task:<id>/device:GPU:<id> as cpu and gpu are mutually exclusive. All entries are optional. </id></id></id></name></id></id></id></name>\n</td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A DeviceSpec. </td> </tr> \n</table> <h3 id=\"make_merged_spec\" data-text=\"make_merged_spec\"><code translate=\"no\" dir=\"ltr\">make_merged_spec</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L213-L235\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmake_merged_spec(\n    dev\n)\n</pre> <p>Returns a new DeviceSpec which incorporates <code translate=\"no\" dir=\"ltr\">dev</code>.</p> <p>When combining specs, <code translate=\"no\" dir=\"ltr\">dev</code> will take precedence over the current spec. So for instance:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">first_spec = tf.DeviceSpec(job=0, device_type=\"CPU\")\nsecond_spec = tf.DeviceSpec(device_type=\"GPU\")\ncombined_spec = first_spec.make_merged_spec(second_spec)\n</pre> <p>is equivalent to:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">combined_spec = tf.DeviceSpec(job=0, device_type=\"GPU\")\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dev</code> </td> <td> a <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A new <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> which combines <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">dev</code> </td> </tr> \n</table> <h3 id=\"parse_from_string\" data-text=\"parse_from_string\"><code translate=\"no\" dir=\"ltr\">parse_from_string</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L160-L211\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nparse_from_string(\n    spec\n)\n</pre> <p>Parse a <code translate=\"no\" dir=\"ltr\">DeviceSpec</code> name into its components.</p> <p>2.x behavior change: In TensorFlow 1.x, this function mutates its own state and returns itself. In 2.x, DeviceSpecs are immutable, and this function will return a DeviceSpec which contains the spec.</p> <p>Recommended:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">```\n# my_spec and my_updated_spec are unrelated.\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = tf.DeviceSpec.from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n```\n</pre> <p>Will work in 1.x and 2.x (though deprecated in 2.x):</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">```\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_updated_spec = my_spec.parse_from_string(\"/GPU:0\")\nwith tf.device(my_updated_spec):\n  ...\n```\n</pre> <p>Will NOT work in 2.x:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">```\nmy_spec = tf.DeviceSpec.from_string(\"/CPU:0\")\nmy_spec.parse_from_string(\"/GPU:0\")  # &lt;== Will not update my_spec\nwith tf.device(my_spec):\n  ...\n```\n</pre> <p>In general, <a href=\"devicespec#from_string\"><code translate=\"no\" dir=\"ltr\">DeviceSpec.from_string</code></a> should completely replace <a href=\"devicespec#parse_from_string\"><code translate=\"no\" dir=\"ltr\">DeviceSpec.parse_from_string</code></a>, and <a href=\"devicespec#replace\"><code translate=\"no\" dir=\"ltr\">DeviceSpec.replace</code></a> should completely replace setting attributes directly.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">spec</code> </td> <td> an optional string of the form /job:<name>/replica:<id>/task:<id>/device:CPU:<id> or /job:<name>/replica:<id>/task:<id>/device:GPU:<id> as cpu and gpu are mutually exclusive. All entries are optional. </id></id></id></name></id></id></id></name>\n</td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The <code translate=\"no\" dir=\"ltr\">DeviceSpec</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if the spec was not valid. </td> </tr> </table> <h3 id=\"replace\" data-text=\"replace\"><code translate=\"no\" dir=\"ltr\">replace</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L237-L259\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nreplace(\n    **kwargs\n)\n</pre> <p>Convenience method for making a new DeviceSpec by overriding fields.</p> <h4 id=\"for_instance\" data-text=\"For instance:\">For instance:</h4> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">my_spec = DeviceSpec=(job=\"my_job\", device=\"CPU\")\nmy_updated_spec = my_spec.replace(device=\"GPU\")\nmy_other_spec = my_spec.replace(device=None)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">**kwargs</code> </td> <td> This method takes the same args as the DeviceSpec constructor </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A DeviceSpec with the fields specified in kwargs overridden. </td> </tr> \n</table> <h3 id=\"to_string\" data-text=\"to_string\"><code translate=\"no\" dir=\"ltr\">to_string</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L134-L141\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nto_string()\n</pre> <p>Return a string representation of this <code translate=\"no\" dir=\"ltr\">DeviceSpec</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> a string of the form /job:<name>/replica:<id>/task:<id>/device:<device_type>:<id>. </id></device_type></id></id></name>\n</td> </tr> \n</table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/device_spec.py#L377-L391\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Checks if the <code translate=\"no\" dir=\"ltr\">other</code> DeviceSpec is same as the current instance, eg have</p> <p>same value for all the internal fields.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> Another DeviceSpec </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> Return <code translate=\"no\" dir=\"ltr\">True</code> if <code translate=\"no\" dir=\"ltr\">other</code> is also a DeviceSpec instance and has same value as the current instance. Return <code translate=\"no\" dir=\"ltr\">False</code> otherwise. </td> </tr> \n</table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/DeviceSpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/DeviceSpec</a>\n  </p>\n</div>\n","dtypes/dtype":"<h1 class=\"devsite-page-title\">tf.dtypes.DType</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/dtypes.py#L37-L216\">  View source on GitHub </a> </td> </table> <p>Represents the type of the elements in a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a></p> <b>Compat aliases for migration</b> <p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.DType</code></a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/dtypes/DType\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.dtypes.DType</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.dtypes.DType()\n</pre>  <p>The following <code translate=\"no\" dir=\"ltr\">DType</code> objects are defined:</p> <ul> <li>\n<a href=\"../../tf#float16\"><code translate=\"no\" dir=\"ltr\">tf.float16</code></a>: 16-bit half-precision floating-point.</li> <li>\n<a href=\"../../tf#float32\"><code translate=\"no\" dir=\"ltr\">tf.float32</code></a>: 32-bit single-precision floating-point.</li> <li>\n<a href=\"../../tf#float64\"><code translate=\"no\" dir=\"ltr\">tf.float64</code></a>: 64-bit double-precision floating-point.</li> <li>\n<a href=\"../../tf#bfloat16\"><code translate=\"no\" dir=\"ltr\">tf.bfloat16</code></a>: 16-bit truncated floating-point.</li> <li>\n<a href=\"../../tf#complex64\"><code translate=\"no\" dir=\"ltr\">tf.complex64</code></a>: 64-bit single-precision complex.</li> <li>\n<a href=\"../../tf#complex128\"><code translate=\"no\" dir=\"ltr\">tf.complex128</code></a>: 128-bit double-precision complex.</li> <li>\n<a href=\"../../tf#int8\"><code translate=\"no\" dir=\"ltr\">tf.int8</code></a>: 8-bit signed integer.</li> <li>\n<a href=\"../../tf#uint8\"><code translate=\"no\" dir=\"ltr\">tf.uint8</code></a>: 8-bit unsigned integer.</li> <li>\n<a href=\"../../tf#uint16\"><code translate=\"no\" dir=\"ltr\">tf.uint16</code></a>: 16-bit unsigned integer.</li> <li>\n<a href=\"../../tf#uint32\"><code translate=\"no\" dir=\"ltr\">tf.uint32</code></a>: 32-bit unsigned integer.</li> <li>\n<a href=\"../../tf#uint64\"><code translate=\"no\" dir=\"ltr\">tf.uint64</code></a>: 64-bit unsigned integer.</li> <li>\n<a href=\"../../tf#int16\"><code translate=\"no\" dir=\"ltr\">tf.int16</code></a>: 16-bit signed integer.</li> <li>\n<a href=\"../../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a>: 32-bit signed integer.</li> <li>\n<a href=\"../../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>: 64-bit signed integer.</li> <li>\n<a href=\"../../tf#bool\"><code translate=\"no\" dir=\"ltr\">tf.bool</code></a>: Boolean.</li> <li>\n<a href=\"../../tf#string\"><code translate=\"no\" dir=\"ltr\">tf.string</code></a>: String.</li> <li>\n<a href=\"../../tf#qint8\"><code translate=\"no\" dir=\"ltr\">tf.qint8</code></a>: Quantized 8-bit signed integer.</li> <li>\n<a href=\"../../tf#quint8\"><code translate=\"no\" dir=\"ltr\">tf.quint8</code></a>: Quantized 8-bit unsigned integer.</li> <li>\n<a href=\"../../tf#qint16\"><code translate=\"no\" dir=\"ltr\">tf.qint16</code></a>: Quantized 16-bit signed integer.</li> <li>\n<a href=\"../../tf#quint16\"><code translate=\"no\" dir=\"ltr\">tf.quint16</code></a>: Quantized 16-bit unsigned integer.</li> <li>\n<a href=\"../../tf#qint32\"><code translate=\"no\" dir=\"ltr\">tf.qint32</code></a>: Quantized 32-bit signed integer.</li> <li>\n<a href=\"../../tf#resource\"><code translate=\"no\" dir=\"ltr\">tf.resource</code></a>: Handle to a mutable resource.</li> <li>\n<a href=\"../../tf#variant\"><code translate=\"no\" dir=\"ltr\">tf.variant</code></a>: Values of arbitrary types.</li> </ul> <p>The <a href=\"as_dtype\"><code translate=\"no\" dir=\"ltr\">tf.as_dtype()</code></a> function converts numpy types and string type names to a <code translate=\"no\" dir=\"ltr\">DType</code> object.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">as_datatype_enum</code> </td> <td> Returns a <code translate=\"no\" dir=\"ltr\">types_pb2.DataType</code> enum value based on this data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">as_numpy_dtype</code> </td> <td> Returns a Python <code translate=\"no\" dir=\"ltr\">type</code> object based on this <code translate=\"no\" dir=\"ltr\">DType</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">base_dtype</code> </td> <td> Returns a non-reference <code translate=\"no\" dir=\"ltr\">DType</code> based on this <code translate=\"no\" dir=\"ltr\">DType</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_bool</code> </td> <td> Returns whether this is a boolean data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_complex</code> </td> <td> Returns whether this is a complex floating point type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_floating</code> </td> <td> Returns whether this is a (non-quantized, real) floating point type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_integer</code> </td> <td> Returns whether this is a (non-quantized) integer type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_numpy_compatible</code> </td> <td> Returns whether this data type has a compatible NumPy data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_quantized</code> </td> <td> Returns whether this is a quantized data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">is_unsigned</code> </td> <td> Returns whether this type is unsigned. <p>Non-numeric, unordered, and quantized types are not considered unsigned, and this function returns <code translate=\"no\" dir=\"ltr\">False</code>. </p>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">limits</code> </td> <td> Return intensity limits, i.e. <p>(min, max) tuple, of the dtype. Args: clip_negative : bool, optional If True, clip the negative range (i.e. return 0 for min intensity) even if the image dtype allows negative values. Returns min, max : tuple Lower and upper intensity limits. </p>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">max</code> </td> <td> Returns the maximum representable value in this data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">min</code> </td> <td> Returns the minimum representable value in this data type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">real_dtype</code> </td> <td> Returns the <code translate=\"no\" dir=\"ltr\">DType</code> corresponding to this <code translate=\"no\" dir=\"ltr\">DType</code>'s real part. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">size</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/dtypes.py#L172-L190\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    other\n)\n</pre> <p>Returns True if the <code translate=\"no\" dir=\"ltr\">other</code> DType will be converted to this DType.</p> <p>The conversion rules are as follows:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">DType(T)       .is_compatible_with(DType(T))        == True\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">DType</code> (or object that may be converted to a <code translate=\"no\" dir=\"ltr\">DType</code>). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> True if a Tensor of the <code translate=\"no\" dir=\"ltr\">other</code> <code translate=\"no\" dir=\"ltr\">DType</code> will be implicitly converted to this <code translate=\"no\" dir=\"ltr\">DType</code>. </td> </tr> \n</table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/dtypes.py#L192-L203\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Returns True iff this DType refers to the same type as <code translate=\"no\" dir=\"ltr\">other</code>.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/dtypes.py#L205-L207\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Returns True iff self != other.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/dtypes/DType\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/dtypes/DType</a>\n  </p>\n</div>\n","gradienttape":"<h1 class=\"devsite-page-title\">tf.GradientTape</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L736-L1360\">  View source on GitHub </a> </td> </table> <p>Record operations for automatic differentiation.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/GradientTape\"><code translate=\"no\" dir=\"ltr\">tf.autodiff.GradientTape</code></a></p> <b>Compat aliases for migration</b> <p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/GradientTape\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.GradientTape</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n</pre>  <p>Operations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".</p> <p>Trainable variables (created by <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a> or <a href=\"compat/v1/get_variable\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.get_variable</code></a>, where <code translate=\"no\" dir=\"ltr\">trainable=True</code> is default in both cases) are automatically watched. Tensors can be manually watched by invoking the <code translate=\"no\" dir=\"ltr\">watch</code> method on this context manager.</p> <p>For example, consider the function <code translate=\"no\" dir=\"ltr\">y = x * x</code>. The gradient at <code translate=\"no\" dir=\"ltr\">x = 3.0</code> can be computed as:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n</pre> <p>GradientTapes can be nested to compute higher-order derivatives. For example,</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(5.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  with tf.GradientTape() as gg:\n    gg.watch(x)\n    y = x * x\n  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\nd2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\nprint(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\nprint(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\n</pre> <p>By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as g:\n  g.watch(x)\n  y = x * x\n  z = y * y\ndz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\nprint(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n</pre> <p>By default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing <code translate=\"no\" dir=\"ltr\">watch_accessed_variables=False</code> to the tape constructor:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.Variable(2.0)\nw = tf.Variable(5.0)\nwith tf.GradientTape(\n    watch_accessed_variables=False, persistent=True) as tape:\n  tape.watch(x)\n  y = x ** 2  # Gradients will be available for `x`.\n  z = w ** 3  # No gradients will be available as `w` isn't being watched.\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n# No gradients will be available as `w` isn't being watched.\ndz_dy = tape.gradient(z, w)\nprint(dz_dy)\nNone\n</pre> <p>Note that when using models you should ensure that your variables exist when using <code translate=\"no\" dir=\"ltr\">watch_accessed_variables=False</code>. Otherwise it's quite easy to make your first iteration not have any gradients:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">a = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\n</pre> <p>Note that only tensors with real or complex dtypes are differentiable.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">persistent</code> </td> <td> Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">watch_accessed_variables</code> </td> <td> Boolean controlling whether the tape will automatically <code translate=\"no\" dir=\"ltr\">watch</code> any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable <code translate=\"no\" dir=\"ltr\">Variable</code>. If False users must explicitly <code translate=\"no\" dir=\"ltr\">watch</code> any <code translate=\"no\" dir=\"ltr\">Variable</code>s they want to request gradients from. </td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"batch_jacobian\" data-text=\"batch_jacobian\"><code translate=\"no\" dir=\"ltr\">batch_jacobian</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L1221-L1360\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nbatch_jacobian(\n    target, source, unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None, experimental_use_pfor=True\n)\n</pre> <p>Computes and stacks per-example jacobians.</p> <p>See <a href=\"http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant\">wikipedia article</a> for the definition of a Jacobian. This function is essentially an efficient implementation of the following:</p> <p><code translate=\"no\" dir=\"ltr\">tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])</code>.</p> <p>Note that compared to <a href=\"gradienttape#jacobian\"><code translate=\"no\" dir=\"ltr\">GradientTape.jacobian</code></a> which computes gradient of each output value w.r.t each input value, this function is useful when <code translate=\"no\" dir=\"ltr\">target[i,...]</code> is independent of <code translate=\"no\" dir=\"ltr\">source[j,...]</code> for <code translate=\"no\" dir=\"ltr\">j != i</code>. This assumption allows more efficient computation as compared to <a href=\"gradienttape#jacobian\"><code translate=\"no\" dir=\"ltr\">GradientTape.jacobian</code></a>. The output, as well as intermediate activations, are lower dimensional and avoid a bunch of redundant zeros which would result in the jacobian computation given the independence assumption.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Unless you set <code translate=\"no\" dir=\"ltr\">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>\n</blockquote> <h4 id=\"example_usage\" data-text=\"Example usage:\">Example usage:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with tf.GradientTape() as g:\n  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n  g.watch(x)\n  y = x * x\nbatch_jacobian = g.batch_jacobian(y, x)\n# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">target</code> </td> <td> A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. <code translate=\"no\" dir=\"ltr\">target[i,...]</code> should only depend on <code translate=\"no\" dir=\"ltr\">source[i,...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">source</code> </td> <td> A tensor with rank 2 or higher and with shape [b, x1, ..., x_m]. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">parallel_iterations</code> </td> <td> A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">experimental_use_pfor</code> </td> <td> If true, uses pfor for computing the Jacobian. Else uses a tf.while_loop. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A tensor <code translate=\"no\" dir=\"ltr\">t</code> with shape [b, y_1, ..., y_n, x1, ..., x_m] where <code translate=\"no\" dir=\"ltr\">t[i, ...]</code> is the jacobian of <code translate=\"no\" dir=\"ltr\">target[i, ...]</code> w.r.t. <code translate=\"no\" dir=\"ltr\">source[i, ...]</code>, i.e. stacked per-example jacobians. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If vectorization of jacobian computation fails or if first dimension of <code translate=\"no\" dir=\"ltr\">target</code> and <code translate=\"no\" dir=\"ltr\">source</code> do not match. </td> </tr> </table> <h3 id=\"gradient\" data-text=\"gradient\"><code translate=\"no\" dir=\"ltr\">gradient</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L993-L1101\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ngradient(\n    target, sources, output_gradients=None,\n    unconnected_gradients=tf.UnconnectedGradients.NONE\n)\n</pre> <p>Computes the gradient using operations recorded in context of this tape.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Unless you set <code translate=\"no\" dir=\"ltr\">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">target</code> </td> <td> a list or nested structure of Tensors or Variables to be differentiated. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sources</code> </td> <td> a list or nested structure of Tensors or Variables. <code translate=\"no\" dir=\"ltr\">target</code> will be differentiated against elements in <code translate=\"no\" dir=\"ltr\">sources</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">output_gradients</code> </td> <td> a list of gradients, one for each element of target. Defaults to None. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> a list or nested structure of Tensors (or IndexedSlices, or None), one for each element in <code translate=\"no\" dir=\"ltr\">sources</code>. Returned structure is the same as the structure of <code translate=\"no\" dir=\"ltr\">sources</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called inside the context of the tape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If the target is a variable or if unconnected gradients is called with an unknown value. </td> </tr> </table> <h3 id=\"jacobian\" data-text=\"jacobian\"><code translate=\"no\" dir=\"ltr\">jacobian</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L1103-L1219\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\njacobian(\n    target, sources, unconnected_gradients=tf.UnconnectedGradients.NONE,\n    parallel_iterations=None, experimental_use_pfor=True\n)\n</pre> <p>Computes the jacobian using operations recorded in context of this tape.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Unless you set <code translate=\"no\" dir=\"ltr\">persistent=True</code> a GradientTape can only be used to compute one set of gradients (or jacobians).</span>\n</blockquote> <p>See<a href=\"http://en.wikipedia.org/wiki/jacobian_matrix_and_determinant\">wikipedia article</a> for the definition of a Jacobian.</p> <h4 id=\"example_usage_2\" data-text=\"Example usage:\">Example usage:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with tf.GradientTape() as g:\n  x  = tf.constant([1.0, 2.0])\n  g.watch(x)\n  y = x * x\njacobian = g.jacobian(y, x)\n# jacobian value is [[2., 0.], [0., 4.]]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">target</code> </td> <td> Tensor to be differentiated. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sources</code> </td> <td> a list or nested structure of Tensors or Variables. <code translate=\"no\" dir=\"ltr\">target</code> will be differentiated against elements in <code translate=\"no\" dir=\"ltr\">sources</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">unconnected_gradients</code> </td> <td> a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">parallel_iterations</code> </td> <td> A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">experimental_use_pfor</code> </td> <td> If true, vectorizes the jacobian computation. Else falls back to a sequential while_loop. Vectorization can sometimes fail or lead to excessive memory usage. This option can be used to disable vectorization in such cases. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A list or nested structure of Tensors (or None), one for each element in <code translate=\"no\" dir=\"ltr\">sources</code>. Returned structure is the same as the structure of <code translate=\"no\" dir=\"ltr\">sources</code>. Note if any gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called on a used, non-persistent tape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If vectorization of jacobian computation fails. </td> </tr> </table> <h3 id=\"reset\" data-text=\"reset\"><code translate=\"no\" dir=\"ltr\">reset</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L951-L985\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nreset()\n</pre> <p>Clears all information stored in this tape.</p> <p>Equivalent to exiting and reentering the tape context manager with a new tape. For example, the two following code blocks are equivalent:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">with tf.GradientTape() as t:\n  loss = loss_fn()\nwith tf.GradientTape() as t:\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n\n# The following is equivalent to the above\nwith tf.GradientTape() as t:\n  loss = loss_fn()\n  t.reset()\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n</pre> <p>This is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">with tf.GradientTape() as t:\n  loss = ...\n  if loss &gt; k:\n    t.reset()\n</pre> <h3 id=\"stop_recording\" data-text=\"stop_recording\"><code translate=\"no\" dir=\"ltr\">stop_recording</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L919-L949\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\nstop_recording()\n</pre> <p>Temporarily stops recording operations on this tape.</p> <p>Operations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations.</p> <h4 id=\"for_example\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(4.0)\nwith tf.GradientTape() as tape:\n  with tape.stop_recording():\n    y = x ** 2\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\nNone\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Yields</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> None </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> if the tape is not currently recording. </td> </tr> </table> <h3 id=\"watch\" data-text=\"watch\"><code translate=\"no\" dir=\"ltr\">watch</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L894-L917\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwatch(\n    tensor\n)\n</pre> <p>Ensures that <code translate=\"no\" dir=\"ltr\">tensor</code> is being traced by this tape.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">tensor</code> </td> <td> a Tensor or list of Tensors. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if it encounters something that is not a tensor. </td> </tr> </table> <h3 id=\"watched_variables\" data-text=\"watched_variables\"><code translate=\"no\" dir=\"ltr\">watched_variables</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L987-L991\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwatched_variables()\n</pre> <p>Returns variables watched by this tape in order of construction.</p> <h3 id=\"__enter__\" data-text=\"__enter__\"><code translate=\"no\" dir=\"ltr\">__enter__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L856-L859\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__enter__()\n</pre> <p>Enters a context inside which operations are recorded on this tape.</p> <h3 id=\"__exit__\" data-text=\"__exit__\"><code translate=\"no\" dir=\"ltr\">__exit__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/eager/backprop.py#L861-L864\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__exit__(\n    typ, value, traceback\n)\n</pre> <p>Exits the recording context, no further operations are traced.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/GradientTape\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/GradientTape</a>\n  </p>\n</div>\n","version":"<h1 class=\"devsite-page-title\">Module: tf.version</h1>       <p>Public API for tf.version namespace.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Other Members</th></tr> \n<tr> <td> COMPILER_VERSION </td> <td> <code translate=\"no\" dir=\"ltr\">'7.3.1 20180303'</code> </td> </tr>\n<tr> <td> GIT_VERSION </td> <td> <code translate=\"no\" dir=\"ltr\">'v2.4.0-rc4-71-g582c8d236cb'</code> </td> </tr>\n<tr> <td> GRAPH_DEF_VERSION </td> <td> <code translate=\"no\" dir=\"ltr\">561</code> </td> </tr>\n<tr> <td> GRAPH_DEF_VERSION_MIN_CONSUMER </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> GRAPH_DEF_VERSION_MIN_PRODUCER </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> VERSION </td> <td> <code translate=\"no\" dir=\"ltr\">'2.4.0'</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/version\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/version</a>\n  </p>\n</div>\n","criticalsection":"<h1 class=\"devsite-page-title\">tf.CriticalSection</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/critical_section_ops.py#L126-L421\">  View source on GitHub </a> </td> </table> <p>Critical section.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/CriticalSection\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.CriticalSection</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.CriticalSection(\n    name=None, shared_name=None, critical_section_def=None, import_scope=None\n)\n</pre>  <p>A <code translate=\"no\" dir=\"ltr\">CriticalSection</code> object is a resource in the graph which executes subgraphs in <strong>serial</strong> order. A common example of a subgraph one may wish to run exclusively is the one given by the following function:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">v = resource_variable_ops.ResourceVariable(0.0, name=\"v\")\n\ndef count():\n  value = v.read_value()\n  with tf.control_dependencies([value]):\n    with tf.control_dependencies([v.assign_add(1)]):\n      return tf.identity(value)\n</pre> <p>Here, a snapshot of <code translate=\"no\" dir=\"ltr\">v</code> is captured in <code translate=\"no\" dir=\"ltr\">value</code>; and then <code translate=\"no\" dir=\"ltr\">v</code> is updated. The snapshot value is returned.</p> <p>If multiple workers or threads all execute <code translate=\"no\" dir=\"ltr\">count</code> in parallel, there is no guarantee that access to the variable <code translate=\"no\" dir=\"ltr\">v</code> is atomic at any point within any thread's calculation of <code translate=\"no\" dir=\"ltr\">count</code>. In fact, even implementing an atomic counter that guarantees that the user will see each value <code translate=\"no\" dir=\"ltr\">0, 1, ...,</code> is currently impossible.</p> <p>The solution is to ensure any access to the underlying resource <code translate=\"no\" dir=\"ltr\">v</code> is only processed through a critical section:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">cs = CriticalSection()\nf1 = cs.execute(count)\nf2 = cs.execute(count)\noutput = f1 + f2\nsession.run(output)\n</pre> <p>The functions <code translate=\"no\" dir=\"ltr\">f1</code> and <code translate=\"no\" dir=\"ltr\">f2</code> will be executed serially, and updates to <code translate=\"no\" dir=\"ltr\">v</code> will be atomic.</p> <p><strong>NOTES</strong></p> <p>All resource objects, including the critical section and any captured variables of functions executed on that critical section, will be colocated to the same device (host and cpu/gpu).</p> <p>When using multiple critical sections on the same resources, there is no guarantee of exclusive access to those resources. This behavior is disallowed by default (but see the kwarg <code translate=\"no\" dir=\"ltr\">exclusive_resource_access</code>).</p> <p>For example, running the same function in two separate critical sections will not ensure serial execution:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">v = tf.compat.v1.get_variable(\"v\", initializer=0.0, use_resource=True)\ndef accumulate(up):\n  x = v.read_value()\n  with tf.control_dependencies([x]):\n    with tf.control_dependencies([v.assign_add(up)]):\n      return tf.identity(x)\nex1 = CriticalSection().execute(\n  accumulate, 1.0, exclusive_resource_access=False)\nex2 = CriticalSection().execute(\n  accumulate, 1.0, exclusive_resource_access=False)\nbad_sum = ex1 + ex2\nsess.run(v.initializer)\nsess.run(bad_sum)  # May return 0.0\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"execute\" data-text=\"execute\"><code translate=\"no\" dir=\"ltr\">execute</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/critical_section_ops.py#L234-L337\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nexecute(\n    fn, exclusive_resource_access=True, name=None\n)\n</pre> <p>Execute function <code translate=\"no\" dir=\"ltr\">fn()</code> inside the critical section.</p> <p><code translate=\"no\" dir=\"ltr\">fn</code> should not accept any arguments. To add extra arguments to when calling <code translate=\"no\" dir=\"ltr\">fn</code> in the critical section, create a lambda:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">critical_section.execute(lambda: fn(*my_args, **my_kwargs))\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">fn</code> </td> <td> The function to execute. Must return at least one tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">exclusive_resource_access</code> </td> <td> Whether the resources required by <code translate=\"no\" dir=\"ltr\">fn</code> should be exclusive to this <code translate=\"no\" dir=\"ltr\">CriticalSection</code>. Default: <code translate=\"no\" dir=\"ltr\">True</code>. You may want to set this to <code translate=\"no\" dir=\"ltr\">False</code> if you will be accessing a resource in read-only mode in two different CriticalSections. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name to use when creating the execute operation. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The tensors returned from <code translate=\"no\" dir=\"ltr\">fn()</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">fn</code> attempts to lock this <code translate=\"no\" dir=\"ltr\">CriticalSection</code> in any nested or lazy way that may cause a deadlock. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">exclusive_resource_access == True</code> and another <code translate=\"no\" dir=\"ltr\">CriticalSection</code> has an execution requesting the same resources as <code translate=\"no\" dir=\"ltr\">fn</code><code translate=\"no\" dir=\"ltr\">. Note, even if</code>exclusive_resource_access<code translate=\"no\" dir=\"ltr\">is</code>True<code translate=\"no\" dir=\"ltr\">, if another execution in another</code>CriticalSection<code translate=\"no\" dir=\"ltr\">was created without</code>exclusive_resource_access=True<code translate=\"no\" dir=\"ltr\">, a</code>ValueError` will be raised. </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/CriticalSection\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/CriticalSection</a>\n  </p>\n</div>\n","operation":"<h1 class=\"devsite-page-title\">tf.Operation</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L1862-L2596\">  View source on GitHub </a> </td> </table> <p>Represents a graph node that performs computation on tensors.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/Operation\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Operation</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.Operation(\n    node_def, g, inputs=None, output_types=None, control_inputs=None,\n    input_types=None, original_op=None, op_def=None\n)\n</pre>  <p>An <code translate=\"no\" dir=\"ltr\">Operation</code> is a node in a <a href=\"graph\"><code translate=\"no\" dir=\"ltr\">tf.Graph</code></a> that takes zero or more <code translate=\"no\" dir=\"ltr\">Tensor</code> objects as input, and produces zero or more <code translate=\"no\" dir=\"ltr\">Tensor</code> objects as output. Objects of type <code translate=\"no\" dir=\"ltr\">Operation</code> are created by calling a Python op constructor (such as <a href=\"linalg/matmul\"><code translate=\"no\" dir=\"ltr\">tf.matmul</code></a>) within a <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a> or under a <a href=\"graph#as_default\"><code translate=\"no\" dir=\"ltr\">tf.Graph.as_default</code></a> context manager.</p> <p>For example, within a <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>, <code translate=\"no\" dir=\"ltr\">c = tf.matmul(a, b)</code> creates an <code translate=\"no\" dir=\"ltr\">Operation</code> of type \"MatMul\" that takes tensors <code translate=\"no\" dir=\"ltr\">a</code> and <code translate=\"no\" dir=\"ltr\">b</code> as input, and produces <code translate=\"no\" dir=\"ltr\">c</code> as output.</p> <p>If a <a href=\"compat/v1/session\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Session</code></a> is used, an <code translate=\"no\" dir=\"ltr\">Operation</code> of a <a href=\"graph\"><code translate=\"no\" dir=\"ltr\">tf.Graph</code></a> can be executed by passing it to <code translate=\"no\" dir=\"ltr\">tf.Session.run</code>. <code translate=\"no\" dir=\"ltr\">op.run()</code> is a shortcut for calling <code translate=\"no\" dir=\"ltr\">tf.compat.v1.get_default_session().run(op)</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">node_def</code> </td> <td> <code translate=\"no\" dir=\"ltr\">node_def_pb2.NodeDef</code>. <code translate=\"no\" dir=\"ltr\">NodeDef</code> for the <code translate=\"no\" dir=\"ltr\">Operation</code>. Used for attributes of <code translate=\"no\" dir=\"ltr\">node_def_pb2.NodeDef</code>, typically <code translate=\"no\" dir=\"ltr\">name</code>, <code translate=\"no\" dir=\"ltr\">op</code>, and <code translate=\"no\" dir=\"ltr\">device</code>. The <code translate=\"no\" dir=\"ltr\">input</code> attribute is irrelevant here as it will be computed when generating the model. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">g</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Graph</code>. The parent graph. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">inputs</code> </td> <td> list of <code translate=\"no\" dir=\"ltr\">Tensor</code> objects. The inputs to this <code translate=\"no\" dir=\"ltr\">Operation</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">output_types</code> </td> <td> list of <code translate=\"no\" dir=\"ltr\">DType</code> objects. List of the types of the <code translate=\"no\" dir=\"ltr\">Tensors</code> computed by this operation. The length of this list indicates the number of output endpoints of the <code translate=\"no\" dir=\"ltr\">Operation</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">control_inputs</code> </td> <td> list of operations or tensors from which to have a control dependency. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">input_types</code> </td> <td> List of <code translate=\"no\" dir=\"ltr\">DType</code> objects representing the types of the tensors accepted by the <code translate=\"no\" dir=\"ltr\">Operation</code>. By default uses <code translate=\"no\" dir=\"ltr\">[x.dtype.base_dtype for x in inputs]</code>. Operations that expect reference-typed inputs must specify these explicitly. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">original_op</code> </td> <td> Optional. Used to associate the new <code translate=\"no\" dir=\"ltr\">Operation</code> with an existing <code translate=\"no\" dir=\"ltr\">Operation</code> (for example, a replica with the op that was replicated). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">op_def</code> </td> <td> Optional. The <code translate=\"no\" dir=\"ltr\">op_def_pb2.OpDef</code> proto that describes the op type that this <code translate=\"no\" dir=\"ltr\">Operation</code> represents. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> if control inputs are not Operations or Tensors, or if <code translate=\"no\" dir=\"ltr\">node_def</code> is not a <code translate=\"no\" dir=\"ltr\">NodeDef</code>, or if <code translate=\"no\" dir=\"ltr\">g</code> is not a <code translate=\"no\" dir=\"ltr\">Graph</code>, or if <code translate=\"no\" dir=\"ltr\">inputs</code> are not tensors, or if <code translate=\"no\" dir=\"ltr\">inputs</code> and <code translate=\"no\" dir=\"ltr\">input_types</code> are incompatible. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if the <code translate=\"no\" dir=\"ltr\">node_def</code> name is not valid. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">control_inputs</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Operation</code> objects on which this op has a control dependency. <p>Before this op is executed, TensorFlow will ensure that the operations in <code translate=\"no\" dir=\"ltr\">self.control_inputs</code> have finished executing. This mechanism can be used to run ops sequentially for performance reasons, or to ensure that the side effects of an op are observed in the correct order. </p>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">device</code> </td> <td> The name of the device to which this op has been assigned, if any. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">graph</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Graph</code> that contains this operation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">inputs</code> </td> <td> The sequence of <code translate=\"no\" dir=\"ltr\">Tensor</code> objects representing the data inputs of this op. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The full name of this operation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">node_def</code> </td> <td> Returns the <code translate=\"no\" dir=\"ltr\">NodeDef</code> representation of this operation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">op_def</code> </td> <td> Returns the <code translate=\"no\" dir=\"ltr\">OpDef</code> proto that represents the type of this op. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">outputs</code> </td> <td> The list of <code translate=\"no\" dir=\"ltr\">Tensor</code> objects representing the outputs of this op. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">traceback</code> </td> <td> Returns the call stack from when this operation was constructed. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">type</code> </td> <td> The type of the op (e.g. <code translate=\"no\" dir=\"ltr\">\"MatMul\"</code>). </td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"colocation_groups\" data-text=\"colocation_groups\"><code translate=\"no\" dir=\"ltr\">colocation_groups</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L2054-L2071\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ncolocation_groups()\n</pre> <p>Returns the list of colocation groups of the op.</p> <h3 id=\"get_attr\" data-text=\"get_attr\"><code translate=\"no\" dir=\"ltr\">get_attr</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L2516-L2553\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_attr(\n    name\n)\n</pre> <p>Returns the value of the attr of this op with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name of the attr to fetch. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The value of the attr, as a Python object. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If this op does not have an attr with the given <code translate=\"no\" dir=\"ltr\">name</code>. </td> </tr> </table> <h3 id=\"run\" data-text=\"run\"><code translate=\"no\" dir=\"ltr\">run</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L2580-L2596\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrun(\n    feed_dict=None, session=None\n)\n</pre> <p>Runs this operation in a <code translate=\"no\" dir=\"ltr\">Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for this operation.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Before invoking <a href=\"operation#run\"><code translate=\"no\" dir=\"ltr\">Operation.run()</code></a>, its graph must have been launched in a session, and either a default session must be available, or <code translate=\"no\" dir=\"ltr\">session</code> must be specified explicitly.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">feed_dict</code> </td> <td> A dictionary that maps <code translate=\"no\" dir=\"ltr\">Tensor</code> objects to feed values. See <code translate=\"no\" dir=\"ltr\">tf.Session.run</code> for a description of the valid feed values. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">session</code> </td> <td> (Optional.) The <code translate=\"no\" dir=\"ltr\">Session</code> to be used to run to this operation. If none, the default session will be used. </td> </tr> </table> <h3 id=\"values\" data-text=\"values\"><code translate=\"no\" dir=\"ltr\">values</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L2073-L2075\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues()\n</pre> <p>DEPRECATED: Use outputs.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Operation\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Operation</a>\n  </p>\n</div>\n","module":"<h1 class=\"devsite-page-title\">tf.Module</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/module/module.py#L35-L302\">  View source on GitHub </a> </td> </table> <p>Base neural network module class.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/Module\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Module</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.Module(\n    name=None\n)\n</pre>  <p>A module is a named container for <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a>s, other <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a>s and functions which apply to user input. For example a dense layer in a neural network might be implemented as a <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nclass Dense(tf.Module):\n  def __init__(self, input_dim, output_size, name=None):\n    super(Dense, self).__init__(name=name)\n    self.w = tf.Variable(\n      tf.random.normal([input_dim, output_size]), name='w')\n    self.b = tf.Variable(tf.zeros([output_size]), name='b')\n  def __call__(self, x):\n    y = tf.matmul(x, self.w) + self.b\n    return tf.nn.relu(y)\n</pre> <p>You can use the Dense layer as you would expect:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nd = Dense(input_dim=3, output_size=2)\nd(tf.ones([1, 3]))\n&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)&gt;\n</pre> <p>By subclassing <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a> instead of <code translate=\"no\" dir=\"ltr\">object</code> any <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a> or <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a> instances assigned to object properties can be collected using the <code translate=\"no\" dir=\"ltr\">variables</code>, <code translate=\"no\" dir=\"ltr\">trainable_variables</code> or <code translate=\"no\" dir=\"ltr\">submodules</code> property:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nd.variables\n    (&lt;tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=...,\n    dtype=float32)&gt;,\n    &lt;tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=..., dtype=float32)&gt;)\n</pre> <p>Subclasses of <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a> can also take advantage of the <code translate=\"no\" dir=\"ltr\">_flatten</code> method which can be used to implement tracking of any other types.</p> <p>All <a href=\"module\"><code translate=\"no\" dir=\"ltr\">tf.Module</code></a> classes have an associated <a href=\"name_scope\"><code translate=\"no\" dir=\"ltr\">tf.name_scope</code></a> which can be used to group operations in TensorBoard and create hierarchies for variable names which can help with debugging. We suggest using the name scope when creating nested submodules/parameters or for forward methods whose graph you might want to inspect in TensorBoard. You can enter the name scope explicitly using <code translate=\"no\" dir=\"ltr\">with self.name_scope:</code> or you can annotate methods (apart from <code translate=\"no\" dir=\"ltr\">__init__</code>) with <code translate=\"no\" dir=\"ltr\">@tf.Module.with_name_scope</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nclass MLP(tf.Module):\n  def __init__(self, input_size, sizes, name=None):\n    super(MLP, self).__init__(name=name)\n    self.layers = []\n    with self.name_scope:\n      for size in sizes:\n        self.layers.append(Dense(input_dim=input_size, output_size=size))\n        input_size = size\n  @tf.Module.with_name_scope\n  def __call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmodule = MLP(input_size=5, sizes=[5, 5])\nmodule.variables\n(&lt;tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)&gt;,\n&lt;tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)&gt;,\n&lt;tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)&gt;,\n&lt;tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)&gt;)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> Returns the name of this module as passed or determined in the ctor. <blockquote class=\"note\">\n<strong>Note:</strong><span> This is not the same as the <code translate=\"no\" dir=\"ltr\">self.name_scope.name</code> which includes parent module names. </span>\n</blockquote>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name_scope</code> </td> <td> Returns a <a href=\"name_scope\"><code translate=\"no\" dir=\"ltr\">tf.name_scope</code></a> instance for this class. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">submodules</code> </td> <td> Sequence of all sub-modules. <p>Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on).</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.Module()\nb = tf.Module()\nc = tf.Module()\na.b = b\nb.c = c\nlist(a.submodules) == [b, c]\nTrue\nlist(b.submodules) == [c]\nTrue\nlist(c.submodules) == []\nTrue\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">trainable_variables</code> </td> <td> Sequence of trainable variables owned by this module and its submodules. <blockquote class=\"note\">\n<strong>Note:</strong><span> this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. </span>\n</blockquote>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">variables</code> </td> <td> Sequence of variables owned by this module and its submodules.<blockquote class=\"note\">\n<strong>Note:</strong><span> this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. </span>\n</blockquote>\n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"with_name_scope\" data-text=\"with_name_scope\"><code translate=\"no\" dir=\"ltr\">with_name_scope</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/module/module.py#L271-L302\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nwith_name_scope(\n    method\n)\n</pre> <p>Decorator to automatically enter the module name scope.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nclass MyModule(tf.Module):\n  @tf.Module.with_name_scope\n  def __call__(self, x):\n    if not hasattr(self, 'w'):\n      self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n    return tf.matmul(x, self.w)\n</pre> <p>Using the above module would produce <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a>s and <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>s whose names included the module name:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmod = MyModule()\nmod(tf.ones([1, 2]))\n&lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)&gt;\nmod.w\n&lt;tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\nnumpy=..., dtype=float32)&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">method</code> </td> <td> The method to wrap. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The original method wrapped such that it enters the module's name scope. </td> </tr> \n</table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Module\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Module</a>\n  </p>\n</div>\n","experimental/optional":"<h1 class=\"devsite-page-title\">tf.experimental.Optional</h1>       <p>Represents a value that may or may not be present.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/Optional\"><code translate=\"no\" dir=\"ltr\">tf.data.experimental.Optional</code></a></p> <b>Compat aliases for migration</b> <p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/Optional\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.data.experimental.Optional</code></a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/experimental/Optional\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.experimental.Optional</code></a></p> </section>  <p>A <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> can represent the result of an operation that may fail as a value, rather than raising an exception and halting execution. For example, <a href=\"../data/iterator#get_next_as_optional\"><code translate=\"no\" dir=\"ltr\">tf.data.Iterator.get_next_as_optional()</code></a> returns a <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> that either contains the next element of an iterator if one exists, or an \"empty\" value that indicates the end of the sequence has been reached.</p> <p><a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> can only be used with values that are convertible to <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> or <code translate=\"no\" dir=\"ltr\">tf.CompositeTensor</code>.</p> <p>One can create a <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> from a value using the <code translate=\"no\" dir=\"ltr\">from_value()</code> method:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.from_value(42)\nprint(optional.has_value())\ntf.Tensor(True, shape=(), dtype=bool)\nprint(optional.get_value())\ntf.Tensor(42, shape=(), dtype=int32)\n</pre> <p>or without a value using the <code translate=\"no\" dir=\"ltr\">empty()</code> method:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.empty(\n  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))\nprint(optional.has_value())\ntf.Tensor(False, shape=(), dtype=bool)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_spec</code> </td> <td> The type specification of an element of this optional. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.from_value(42)\nprint(optional.element_spec)\ntf.TensorSpec(shape=(), dtype=tf.int32, name=None)\n</pre> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"empty\" data-text=\"empty\"><code translate=\"no\" dir=\"ltr\">empty</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L118-L137\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nempty(\n    element_spec\n)\n</pre> <p>Returns an <code translate=\"no\" dir=\"ltr\">Optional</code> that has no value.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> This method takes an argument that defines the structure of the value that would be contained in the returned <code translate=\"no\" dir=\"ltr\">Optional</code> if it had a value.</span>\n</blockquote> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.empty(\n  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))\nprint(optional.has_value())\ntf.Tensor(False, shape=(), dtype=bool)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_spec</code> </td> <td> A nested structure of <a href=\"../typespec\"><code translate=\"no\" dir=\"ltr\">tf.TypeSpec</code></a> objects matching the structure of an element of this optional. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> with no value. </td> </tr> \n</table> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L139-L163\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_value(\n    value\n)\n</pre> <p>Returns a <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> that wraps the given value.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.from_value(42)\nprint(optional.has_value())\ntf.Tensor(True, shape=(), dtype=bool)\nprint(optional.get_value())\ntf.Tensor(42, shape=(), dtype=int32)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">value</code> </td> <td> A value to wrap. The value must be convertible to <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> or <code translate=\"no\" dir=\"ltr\">tf.CompositeTensor</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> that wraps <code translate=\"no\" dir=\"ltr\">value</code>. </td> </tr> \n</table> <h3 id=\"get_value\" data-text=\"get_value\"><code translate=\"no\" dir=\"ltr\">get_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L84-L102\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@abc.abstractmethod\nget_value(\n    name=None\n)\n</pre> <p>Returns the value wrapped by this optional.</p> <p>If this optional does not have a value (i.e. <code translate=\"no\" dir=\"ltr\">self.has_value()</code> evaluates to <code translate=\"no\" dir=\"ltr\">False</code>), this operation will raise <a href=\"../errors/invalidargumenterror\"><code translate=\"no\" dir=\"ltr\">tf.errors.InvalidArgumentError</code></a> at runtime.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.from_value(42)\nprint(optional.get_value())\ntf.Tensor(42, shape=(), dtype=int32)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> (Optional.) A name for the created operation. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The wrapped value. </td> </tr> \n</table> <h3 id=\"has_value\" data-text=\"has_value\"><code translate=\"no\" dir=\"ltr\">has_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L68-L82\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@abc.abstractmethod\nhas_value(\n    name=None\n)\n</pre> <p>Returns a tensor that evaluates to <code translate=\"no\" dir=\"ltr\">True</code> if this optional has a value.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\noptional = tf.experimental.Optional.from_value(42)\nprint(optional.has_value())\ntf.Tensor(True, shape=(), dtype=bool)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> (Optional.) A name for the created operation. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A scalar <a href=\"../tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type <a href=\"../../tf#bool\"><code translate=\"no\" dir=\"ltr\">tf.bool</code></a>. </td> </tr> \n</table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental/Optional\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental/Optional</a>\n  </p>\n</div>\n","aggregationmethod":"<h1 class=\"devsite-page-title\">tf.AggregationMethod</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/gradients_util.py#L894-L923\">  View source on GitHub </a> </td> </table> <p>A class listing aggregation methods used to combine gradients.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/AggregationMethod\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.AggregationMethod</code></a></p> </section>  <p>Computing partial derivatives can require aggregating gradient contributions. This class lists the various methods that can be used to combine gradients in the graph.</p> <p>The following aggregation methods are part of the stable API for aggregating gradients:</p> <ul> <li>\n<code translate=\"no\" dir=\"ltr\">ADD_N</code>: All of the gradient terms are summed as part of one operation using the \"AddN\" op (see <a href=\"math/add_n\"><code translate=\"no\" dir=\"ltr\">tf.add_n</code></a>). This method has the property that all gradients must be ready and buffered separately in memory before any aggregation is performed.</li> <li>\n<code translate=\"no\" dir=\"ltr\">DEFAULT</code>: The system-chosen default aggregation method.</li> </ul> <p>The following aggregation methods are experimental and may not be supported in future releases:</p> <ul> <li>\n<code translate=\"no\" dir=\"ltr\">EXPERIMENTAL_TREE</code>: Gradient terms are summed in pairs using using the \"AddN\" op. This method of summing gradients may reduce performance, but it can improve memory utilization because the gradients can be released earlier.</li> </ul>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Class Variables</th></tr> \n<tr> <td> ADD_N </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> DEFAULT </td> <td> <code translate=\"no\" dir=\"ltr\">0</code> </td> </tr>\n<tr> <td> EXPERIMENTAL_ACCUMULATE_N </td> <td> <code translate=\"no\" dir=\"ltr\">2</code> </td> </tr>\n<tr> <td> EXPERIMENTAL_TREE </td> <td> <code translate=\"no\" dir=\"ltr\">1</code> </td> </tr> </table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/AggregationMethod\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/AggregationMethod</a>\n  </p>\n</div>\n","indexedslices":"<h1 class=\"devsite-page-title\">tf.IndexedSlices</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/indexed_slices.py#L61-L185\">  View source on GitHub </a> </td> </table> <p>A sparse representation of a set of tensor slices at given indices.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/IndexedSlices\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.IndexedSlices</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.IndexedSlices(\n    values, indices, dense_shape=None\n)\n</pre>  <p>This class is a simple wrapper for a pair of <code translate=\"no\" dir=\"ltr\">Tensor</code> objects:</p> <ul> <li>\n<code translate=\"no\" dir=\"ltr\">values</code>: A <code translate=\"no\" dir=\"ltr\">Tensor</code> of any dtype with shape <code translate=\"no\" dir=\"ltr\">[D0, D1, ..., Dn]</code>.</li> <li>\n<code translate=\"no\" dir=\"ltr\">indices</code>: A 1-D integer <code translate=\"no\" dir=\"ltr\">Tensor</code> with shape <code translate=\"no\" dir=\"ltr\">[D0]</code>.</li> </ul> <p>An <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> is typically used to represent a subset of a larger tensor <code translate=\"no\" dir=\"ltr\">dense</code> of shape <code translate=\"no\" dir=\"ltr\">[LARGE0, D1, .. , DN]</code> where <code translate=\"no\" dir=\"ltr\">LARGE0 &gt;&gt; D0</code>. The values in <code translate=\"no\" dir=\"ltr\">indices</code> are the indices in the first dimension of the slices that have been extracted from the larger tensor.</p> <p>The dense tensor <code translate=\"no\" dir=\"ltr\">dense</code> represented by an <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> <code translate=\"no\" dir=\"ltr\">slices</code> has</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n</pre> <p>The <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> class is used principally in the definition of gradients for operations that have sparse gradients (e.g. <a href=\"gather\"><code translate=\"no\" dir=\"ltr\">tf.gather</code></a>).</p> <p>Contrast this representation with <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>, which uses multi-dimensional indices and scalar values.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense_shape</code> </td> <td> A 1-D <code translate=\"no\" dir=\"ltr\">Tensor</code> containing the shape of the corresponding dense tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">device</code> </td> <td> The name of the device on which <code translate=\"no\" dir=\"ltr\">values</code> will be produced, or <code translate=\"no\" dir=\"ltr\">None</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">DType</code> of elements in this tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">graph</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Graph</code> that contains the values, indices, and shape tensors. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">indices</code> </td> <td> A 1-D <code translate=\"no\" dir=\"ltr\">Tensor</code> containing the indices of the slices. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name of this <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">op</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Operation</code> that produces <code translate=\"no\" dir=\"ltr\">values</code> as an output. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> Gets the <a href=\"tensorshape\"><code translate=\"no\" dir=\"ltr\">tf.TensorShape</code></a> representing the shape of the dense tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> containing the values of the slices. </td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"consumers\" data-text=\"consumers\"><code translate=\"no\" dir=\"ltr\">consumers</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/indexed_slices.py#L184-L185\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nconsumers()\n</pre> <h3 id=\"__neg__\" data-text=\"__neg__\"><code translate=\"no\" dir=\"ltr\">__neg__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/indexed_slices.py#L153-L154\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__neg__()\n</pre>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/IndexedSlices\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/IndexedSlices</a>\n  </p>\n</div>\n","raggedtensor":"<h1 class=\"devsite-page-title\">tf.RaggedTensor</h1>   <p><devsite-mathjax config=\"TeX-AMS-MML_SVG\"></devsite-mathjax> </p>   <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L61-L2119\">  View source on GitHub </a> </td> </table> <p>Represents a ragged tensor.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/RaggedTensor\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.RaggedTensor</code></a></p> </section>  <p>A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> is a tensor with one or more <em>ragged dimensions</em>, which are dimensions whose slices may have different lengths. For example, the inner (column) dimension of <code translate=\"no\" dir=\"ltr\">rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code> is ragged, since the column slices (<code translate=\"no\" dir=\"ltr\">rt[0, :]</code>, ..., <code translate=\"no\" dir=\"ltr\">rt[4, :]</code>) have different lengths. Dimensions whose slices all have the same length are called <em>uniform dimensions</em>. The outermost dimension of a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> is always uniform, since it consists of a single slice (and so there is no possibility for differing slice lengths).</p> <p>The total number of dimensions in a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> is called its <em>rank</em>, and the number of ragged dimensions in a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> is called its <em>ragged-rank</em>. A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>'s ragged-rank is fixed at graph creation time: it can't depend on the runtime values of <code translate=\"no\" dir=\"ltr\">Tensor</code>s, and can't vary dynamically for different session runs.</p> <p>Note that the <code translate=\"no\" dir=\"ltr\">__init__</code> constructor is private. Please use one of the following methods to construct a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">* &lt;a href=\"../tf/RaggedTensor#from_row_lengths\"&gt;&lt;code&gt;tf.RaggedTensor.from_row_lengths&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_value_rowids\"&gt;&lt;code&gt;tf.RaggedTensor.from_value_rowids&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_row_splits\"&gt;&lt;code&gt;tf.RaggedTensor.from_row_splits&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_row_starts\"&gt;&lt;code&gt;tf.RaggedTensor.from_row_starts&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_row_limits\"&gt;&lt;code&gt;tf.RaggedTensor.from_row_limits&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_nested_row_splits\"&gt;&lt;code&gt;tf.RaggedTensor.from_nested_row_splits&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_nested_row_lengths\"&gt;&lt;code&gt;tf.RaggedTensor.from_nested_row_lengths&lt;/code&gt;&lt;/a&gt;\n* &lt;a href=\"../tf/RaggedTensor#from_nested_value_rowids\"&gt;&lt;code&gt;tf.RaggedTensor.from_nested_value_rowids&lt;/code&gt;&lt;/a&gt;\n</pre> <h3 id=\"potentially_ragged_tensors\" data-text=\"Potentially Ragged Tensors\">Potentially Ragged Tensors</h3> <p>Many ops support both <code translate=\"no\" dir=\"ltr\">Tensor</code>s and <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s. The term \"potentially ragged tensor\" may be used to refer to a tensor that might be either a <code translate=\"no\" dir=\"ltr\">Tensor</code> or a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. The ragged-rank of a <code translate=\"no\" dir=\"ltr\">Tensor</code> is zero.</p> <h3 id=\"documenting_raggedtensor_shapes\" data-text=\"Documenting RaggedTensor Shapes\">Documenting RaggedTensor Shapes</h3> <p>When documenting the shape of a RaggedTensor, ragged dimensions can be indicated by enclosing them in parentheses. For example, the shape of a 3-D <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> that stores the fixed-size word embedding for each word in a sentence, for each sentence in a batch, could be written as <code translate=\"no\" dir=\"ltr\">[num_sentences, (num_words), embedding_size]</code>. The parentheses around <code translate=\"no\" dir=\"ltr\">(num_words)</code> indicate that dimension is ragged, and that the length of each element list in that dimension may vary for each item.</p> <h3 id=\"component_tensors\" data-text=\"Component Tensors\">Component Tensors</h3> <p>Internally, a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> consists of a concatenated list of values that are partitioned into variable-length rows. In particular, each <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> consists of:</p> <ul> <li><p>A <code translate=\"no\" dir=\"ltr\">values</code> tensor, which concatenates the variable-length rows into a flattened list. For example, the <code translate=\"no\" dir=\"ltr\">values</code> tensor for <code translate=\"no\" dir=\"ltr\">[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code> is <code translate=\"no\" dir=\"ltr\">[3, 1, 4, 1, 5, 9, 2, 6]</code>.</p></li> <li><p>A <code translate=\"no\" dir=\"ltr\">row_splits</code> vector, which indicates how those flattened values are divided into rows. In particular, the values for row <code translate=\"no\" dir=\"ltr\">rt[i]</code> are stored in the slice <code translate=\"no\" dir=\"ltr\">rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</code>.</p></li> </ul> <h4 id=\"example\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_row_splits(\n      values=[3, 1, 4, 1, 5, 9, 2, 6],\n      row_splits=[0, 4, 4, 7, 8, 8]))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"alternative_row-partitioning_schemes\" data-text=\"Alternative Row-Partitioning Schemes\">Alternative Row-Partitioning Schemes</h3> <p>In addition to <code translate=\"no\" dir=\"ltr\">row_splits</code>, ragged tensors provide support for five other row-partitioning schemes:</p> <ul> <li><p><code translate=\"no\" dir=\"ltr\">row_lengths</code>: a vector with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>, which specifies the length of each row.</p></li> <li><p><code translate=\"no\" dir=\"ltr\">value_rowids</code> and <code translate=\"no\" dir=\"ltr\">nrows</code>: <code translate=\"no\" dir=\"ltr\">value_rowids</code> is a vector with shape <code translate=\"no\" dir=\"ltr\">[nvals]</code>, corresponding one-to-one with <code translate=\"no\" dir=\"ltr\">values</code>, which specifies each value's row index. In particular, the row <code translate=\"no\" dir=\"ltr\">rt[row]</code> consists of the values <code translate=\"no\" dir=\"ltr\">rt.values[j]</code> where <code translate=\"no\" dir=\"ltr\">value_rowids[j]==row</code>. <code translate=\"no\" dir=\"ltr\">nrows</code> is an integer scalar that specifies the number of rows in the <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. (<code translate=\"no\" dir=\"ltr\">nrows</code> is used to indicate trailing empty rows.)</p></li> <li><p><code translate=\"no\" dir=\"ltr\">row_starts</code>: a vector with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>, which specifies the start offset of each row. Equivalent to <code translate=\"no\" dir=\"ltr\">row_splits[:-1]</code>.</p></li> <li><p><code translate=\"no\" dir=\"ltr\">row_limits</code>: a vector with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>, which specifies the stop offset of each row. Equivalent to <code translate=\"no\" dir=\"ltr\">row_splits[1:]</code>.</p></li> <li><p><code translate=\"no\" dir=\"ltr\">uniform_row_length</code>: A scalar tensor, specifying the length of every row. This row-partitioning scheme may only be used if all rows have the same length.</p></li> </ul> <p>Example: The following ragged tensors are equivalent, and all represent the nested list <code translate=\"no\" dir=\"ltr\">[[3, 1, 4, 1], [], [5, 9, 2], [6], []]</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues = [3, 1, 4, 1, 5, 9, 2, 6]\nrt1 = RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])\nrt2 = RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])\nrt3 = RaggedTensor.from_value_rowids(\n    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)\nrt4 = RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])\nrt5 = RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])\n</pre> <h3 id=\"multiple_ragged_dimensions\" data-text=\"Multiple Ragged Dimensions\">Multiple Ragged Dimensions</h3> <p><code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s with multiple ragged dimensions can be defined by using a nested <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> for the <code translate=\"no\" dir=\"ltr\">values</code> tensor. Each nested <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> adds a single ragged dimension.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ninner_rt = RaggedTensor.from_row_splits(  # =rt1 from above\n    values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\nouter_rt = RaggedTensor.from_row_splits(\n    values=inner_rt, row_splits=[0, 3, 3, 5])\nprint(outer_rt.to_list())\n[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\nprint(outer_rt.ragged_rank)\n2\n</pre> <p>The factory function <a href=\"raggedtensor#from_nested_row_splits\"><code translate=\"no\" dir=\"ltr\">RaggedTensor.from_nested_row_splits</code></a> may be used to construct a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with multiple ragged dimensions directly, by providing a list of <code translate=\"no\" dir=\"ltr\">row_splits</code> tensors:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nRaggedTensor.from_nested_row_splits(\n    flat_values=[3, 1, 4, 1, 5, 9, 2, 6],\n    nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8])).to_list()\n[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]\n</pre> <h3 id=\"uniform_inner_dimensions\" data-text=\"Uniform Inner Dimensions\">Uniform Inner Dimensions</h3> <p><code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s with uniform inner dimensions can be defined by using a multidimensional <code translate=\"no\" dir=\"ltr\">Tensor</code> for <code translate=\"no\" dir=\"ltr\">values</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = RaggedTensor.from_row_splits(values=tf.ones([5, 3], tf.int32),\n                                  row_splits=[0, 2, 5])\nprint(rt.to_list())\n[[[1, 1, 1], [1, 1, 1]],\n [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]\nprint(rt.shape)\n(2, None, 3)\n</pre> <h3 id=\"uniform_outer_dimensions\" data-text=\"Uniform Outer Dimensions\">Uniform Outer Dimensions</h3> <p><code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s with uniform outer dimensions can be defined by using one or more <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with a <code translate=\"no\" dir=\"ltr\">uniform_row_length</code> row-partitioning tensor. For example, a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with shape <code translate=\"no\" dir=\"ltr\">[2, 2, None]</code> can be constructed with this method from a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> values with shape <code translate=\"no\" dir=\"ltr\">[4, None]</code>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(values.shape)\n(4, None)\nrt6 = tf.RaggedTensor.from_uniform_row_length(values, 2)\nprint(rt6)\n&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;\nprint(rt6.shape)\n(2, 2, None)\n</pre> <p>Note that <code translate=\"no\" dir=\"ltr\">rt6</code> only contains one ragged dimension (the innermost dimension). In contrast, if <code translate=\"no\" dir=\"ltr\">from_row_splits</code> is used to construct a similar <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>, then that <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> will have two ragged dimensions:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt7 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\nprint(rt7.shape)\n(2, None, None)\n</pre> <p>Uniform and ragged outer dimensions may be interleaved, meaning that a tensor with any combination of ragged and uniform dimensions may be created. For example, a RaggedTensor <code translate=\"no\" dir=\"ltr\">t4</code> with shape <code translate=\"no\" dir=\"ltr\">[3, None, 4, 8, None, 2]</code> could be constructed as follows:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">t0 = tf.zeros([1000, 2])                           # Shape:         [1000, 2]\nt1 = RaggedTensor.from_row_lengths(t0, [...])      #           [160, None, 2]\nt2 = RaggedTensor.from_uniform_row_length(t1, 8)   #         [20, 8, None, 2]\nt3 = RaggedTensor.from_uniform_row_length(t2, 4)   #       [5, 4, 8, None, 2]\nt4 = RaggedTensor.from_row_lengths(t3, [...])      # [3, None, 4, 8, None, 2]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">DType</code> of values in this tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values</code> </td> <td> The innermost <code translate=\"no\" dir=\"ltr\">values</code> tensor for this ragged tensor. <p>Concretely, if <code translate=\"no\" dir=\"ltr\">rt.values</code> is a <code translate=\"no\" dir=\"ltr\">Tensor</code>, then <code translate=\"no\" dir=\"ltr\">rt.flat_values</code> is <code translate=\"no\" dir=\"ltr\">rt.values</code>; otherwise, <code translate=\"no\" dir=\"ltr\">rt.flat_values</code> is <code translate=\"no\" dir=\"ltr\">rt.values.flat_values</code>.</p> <p>Conceptually, <code translate=\"no\" dir=\"ltr\">flat_values</code> is the tensor formed by flattening the outermost dimension and all of the ragged dimensions into a single dimension.</p> <p><code translate=\"no\" dir=\"ltr\">rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]</code> (where <code translate=\"no\" dir=\"ltr\">nvals</code> is the number of items in the flattened dimensions).</p> <h4 id=\"example_2\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])\nprint(rt.flat_values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nested_row_splits</code> </td> <td> A tuple containing the row_splits for all ragged dimensions. <p><code translate=\"no\" dir=\"ltr\">rt.nested_row_splits</code> is a tuple containing the <code translate=\"no\" dir=\"ltr\">row_splits</code> tensors for all ragged dimensions in <code translate=\"no\" dir=\"ltr\">rt</code>, ordered from outermost to innermost. In particular, <code translate=\"no\" dir=\"ltr\">rt.nested_row_splits = (rt.row_splits,) + value_splits</code> where:</p> <ul> <li>\n<code translate=\"no\" dir=\"ltr\">value_splits = ()</code> if <code translate=\"no\" dir=\"ltr\">rt.values</code> is a <code translate=\"no\" dir=\"ltr\">Tensor</code>.</li> <li>\n<code translate=\"no\" dir=\"ltr\">value_splits = rt.values.nested_row_splits</code> otherwise.</li> </ul> <h4 id=\"example_3\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant(\n    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\nfor i, splits in enumerate(rt.nested_row_splits):\n  print('Splits for dimension %d: %s' % (i+1, splits.numpy()))\nSplits for dimension 1: [0 3]\nSplits for dimension 2: [0 3 3 5]\nSplits for dimension 3: [0 4 4 7 8 8]\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ragged_rank</code> </td> <td> The number of times the RaggedTensor's flat_values is partitioned. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nvalues.ragged_rank\n1\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.RaggedTensor.from_uniform_row_length(values, 2)\nrt.ragged_rank\n2\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits</code> </td> <td> The row-split indices for this ragged tensor's <code translate=\"no\" dir=\"ltr\">values</code>. <p><code translate=\"no\" dir=\"ltr\">rt.row_splits</code> specifies where the values for each row begin and end in <code translate=\"no\" dir=\"ltr\">rt.values</code>. In particular, the values for row <code translate=\"no\" dir=\"ltr\">rt[i]</code> are stored in the slice <code translate=\"no\" dir=\"ltr\">rt.values[rt.row_splits[i]:rt.row_splits[i+1]]</code>.</p> <h4 id=\"example_4\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.row_splits)  # indices of row splits in rt.values\ntf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The statically known shape of this ragged tensor. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant([[0], [1, 2]]).shape\nTensorShape([2, None])\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).shape\nTensorShape([2, None, 2])\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">uniform_row_length</code> </td> <td> The length of each row in this ragged tensor, or None if rows are ragged. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt1 = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(rt1.uniform_row_length)  # rows are ragged.\nNone\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt2 = tf.RaggedTensor.from_uniform_row_length(\n    values=rt1, uniform_row_length=2)\nprint(rt2)\n&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;\nprint(rt2.uniform_row_length)  # rows are not ragged (all have size 2).\ntf.Tensor(2, shape=(), dtype=int64)\n</pre> <p>A RaggedTensor's rows are only considered to be uniform (i.e. non-ragged) if it can be determined statically (at graph construction time) that the rows all have the same length. </p>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> The concatenated rows for this ragged tensor. <p><code translate=\"no\" dir=\"ltr\">rt.values</code> is a potentially ragged tensor formed by flattening the two outermost dimensions of <code translate=\"no\" dir=\"ltr\">rt</code> into a single dimension.</p> <p><code translate=\"no\" dir=\"ltr\">rt.values.shape = [nvals] + rt.shape[2:]</code> (where <code translate=\"no\" dir=\"ltr\">nvals</code> is the number of items in the outer two dimensions of <code translate=\"no\" dir=\"ltr\">rt</code>).</p> <p><code translate=\"no\" dir=\"ltr\">rt.ragged_rank = self.ragged_rank - 1</code></p> <h4 id=\"example_5\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\n</pre> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"bounding_shape\" data-text=\"bounding_shape\"><code translate=\"no\" dir=\"ltr\">bounding_shape</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1244-L1299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nbounding_shape(\n    axis=None, name=None, out_type=None\n)\n</pre> <p>Returns the tight bounding box shape for this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">axis</code> </td> <td> An integer scalar or vector indicating which axes to return the bounding box for. If not specified, then the full bounding box is returned. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">out_type</code> </td> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> for the returned tensor. Defaults to <code translate=\"no\" dir=\"ltr\">self.row_splits.dtype</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> An integer <code translate=\"no\" dir=\"ltr\">Tensor</code> (<code translate=\"no\" dir=\"ltr\">dtype=self.row_splits.dtype</code>). If <code translate=\"no\" dir=\"ltr\">axis</code> is not specified, then <code translate=\"no\" dir=\"ltr\">output</code> is a vector with <code translate=\"no\" dir=\"ltr\">output.shape=[self.shape.ndims]</code>. If <code translate=\"no\" dir=\"ltr\">axis</code> is a scalar, then the <code translate=\"no\" dir=\"ltr\">output</code> is a scalar. If <code translate=\"no\" dir=\"ltr\">axis</code> is a vector, then <code translate=\"no\" dir=\"ltr\">output</code> is a vector, where <code translate=\"no\" dir=\"ltr\">output[i]</code> is the bounding size for dimension <code translate=\"no\" dir=\"ltr\">axis[i]</code>. </td> </tr> \n</table> <h4 id=\"example_6\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[1, 2, 3, 4], [5], [], [6, 7, 8, 9], [10]])\nrt.bounding_shape().numpy()\narray([5, 4])\n</pre> <h3 id=\"consumers\" data-text=\"consumers\"><code translate=\"no\" dir=\"ltr\">consumers</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L2118-L2119\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nconsumers()\n</pre> <h3 id=\"from_nested_row_lengths\" data-text=\"from_nested_row_lengths\"><code translate=\"no\" dir=\"ltr\">from_nested_row_lengths</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L742-L780\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_nested_row_lengths(\n    flat_values, nested_row_lengths, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> from a nested list of <code translate=\"no\" dir=\"ltr\">row_lengths</code> tensors.</p> <h4 id=\"equivalent_to\" data-text=\"Equivalent to:\">Equivalent to:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = flat_values\nfor row_lengths in reversed(nested_row_lengths):\n  result = from_row_lengths(result, row_lengths)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values</code> </td> <td> A potentially ragged tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nested_row_lengths</code> </td> <td> A list of 1-D integer tensors. The <code translate=\"no\" dir=\"ltr\">i</code>th tensor is used as the <code translate=\"no\" dir=\"ltr\">row_lengths</code> for the <code translate=\"no\" dir=\"ltr\">i</code>th ragged dimension. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> (or <code translate=\"no\" dir=\"ltr\">flat_values</code> if <code translate=\"no\" dir=\"ltr\">nested_row_lengths</code> is empty). </td> </tr> \n</table> <h3 id=\"from_nested_row_splits\" data-text=\"from_nested_row_splits\"><code translate=\"no\" dir=\"ltr\">from_nested_row_splits</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L702-L740\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_nested_row_splits(\n    flat_values, nested_row_splits, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> from a nested list of <code translate=\"no\" dir=\"ltr\">row_splits</code> tensors.</p> <h4 id=\"equivalent_to_2\" data-text=\"Equivalent to:\">Equivalent to:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = flat_values\nfor row_splits in reversed(nested_row_splits):\n  result = from_row_splits(result, row_splits)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values</code> </td> <td> A potentially ragged tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nested_row_splits</code> </td> <td> A list of 1-D integer tensors. The <code translate=\"no\" dir=\"ltr\">i</code>th tensor is used as the <code translate=\"no\" dir=\"ltr\">row_splits</code> for the <code translate=\"no\" dir=\"ltr\">i</code>th ragged dimension. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> (or <code translate=\"no\" dir=\"ltr\">flat_values</code> if <code translate=\"no\" dir=\"ltr\">nested_row_splits</code> is empty). </td> </tr> \n</table> <h3 id=\"from_nested_value_rowids\" data-text=\"from_nested_value_rowids\"><code translate=\"no\" dir=\"ltr\">from_nested_value_rowids</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L645-L700\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_nested_value_rowids(\n    flat_values, nested_value_rowids, nested_nrows=None, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> from a nested list of <code translate=\"no\" dir=\"ltr\">value_rowids</code> tensors.</p> <h4 id=\"equivalent_to_3\" data-text=\"Equivalent to:\">Equivalent to:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = flat_values\nfor (rowids, nrows) in reversed(zip(nested_value_rowids, nested_nrows)):\n  result = from_value_rowids(result, rowids, nrows)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values</code> </td> <td> A potentially ragged tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nested_value_rowids</code> </td> <td> A list of 1-D integer tensors. The <code translate=\"no\" dir=\"ltr\">i</code>th tensor is used as the <code translate=\"no\" dir=\"ltr\">value_rowids</code> for the <code translate=\"no\" dir=\"ltr\">i</code>th ragged dimension. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nested_nrows</code> </td> <td> A list of integer scalars. The <code translate=\"no\" dir=\"ltr\">i</code>th scalar is used as the <code translate=\"no\" dir=\"ltr\">nrows</code> for the <code translate=\"no\" dir=\"ltr\">i</code>th ragged dimension. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> (or <code translate=\"no\" dir=\"ltr\">flat_values</code> if <code translate=\"no\" dir=\"ltr\">nested_value_rowids</code> is empty). </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">len(nested_values_rowids) != len(nested_nrows)</code>. </td> </tr> </table> <h3 id=\"from_row_lengths\" data-text=\"from_row_lengths\"><code translate=\"no\" dir=\"ltr\">from_row_lengths</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L450-L491\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_row_lengths(\n    values, row_lengths, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">row_lengths</code>.</p> <p>The returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> corresponds with the python list defined by:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = [[values.pop(0) for i in range(length)]\n          for length in row_lengths]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_lengths</code> </td> <td> A 1-D integer tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>. Must be nonnegative. <code translate=\"no\" dir=\"ltr\">sum(row_lengths)</code> must be <code translate=\"no\" dir=\"ltr\">nvals</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </td> </tr> \n</table> <h4 id=\"example_7\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_row_lengths(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_lengths=[4, 0, 3, 1, 0]))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"from_row_limits\" data-text=\"from_row_limits\"><code translate=\"no\" dir=\"ltr\">from_row_limits</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L533-L568\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_row_limits(\n    values, row_limits, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">row_limits</code>.</p> <p>Equivalent to: <code translate=\"no\" dir=\"ltr\">from_row_splits(values, concat([0, row_limits]))</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_limits</code> </td> <td> A 1-D integer tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>. Must be sorted in ascending order. If <code translate=\"no\" dir=\"ltr\">nrows&gt;0</code>, then <code translate=\"no\" dir=\"ltr\">row_limits[-1]</code> must be <code translate=\"no\" dir=\"ltr\">nvals</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </td> </tr> \n</table> <h4 id=\"example_8\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_row_limits(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_limits=[4, 4, 7, 8, 8]))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"from_row_splits\" data-text=\"from_row_splits\"><code translate=\"no\" dir=\"ltr\">from_row_splits</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L403-L448\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_row_splits(\n    values, row_splits, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">row_splits</code>.</p> <p>The returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> corresponds with the python list defined by:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = [values[row_splits[i]:row_splits[i + 1]]\n          for i in range(len(row_splits) - 1)]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits</code> </td> <td> A 1-D integer tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows+1]</code>. Must not be empty, and must be sorted in ascending order. <code translate=\"no\" dir=\"ltr\">row_splits[0]</code> must be zero and <code translate=\"no\" dir=\"ltr\">row_splits[-1]</code> must be <code translate=\"no\" dir=\"ltr\">nvals</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">row_splits</code> is an empty list. </td> </tr> </table> <h4 id=\"example_9\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_row_splits(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_splits=[0, 4, 4, 7, 8, 8]))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"from_row_starts\" data-text=\"from_row_starts\"><code translate=\"no\" dir=\"ltr\">from_row_starts</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L493-L531\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_row_starts(\n    values, row_starts, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">row_starts</code>.</p> <p>Equivalent to: <code translate=\"no\" dir=\"ltr\">from_row_splits(values, concat([row_starts, nvals]))</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_starts</code> </td> <td> A 1-D integer tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>. Must be nonnegative and sorted in ascending order. If <code translate=\"no\" dir=\"ltr\">nrows&gt;0</code>, then <code translate=\"no\" dir=\"ltr\">row_starts[0]</code> must be zero. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </td> </tr> \n</table> <h4 id=\"example_10\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_row_starts(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    row_starts=[0, 4, 4, 7, 8]))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"from_sparse\" data-text=\"from_sparse\"><code translate=\"no\" dir=\"ltr\">from_sparse</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1770-L1832\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_sparse(\n    st_input, name=None, row_splits_dtype=tf.dtypes.int64\n)\n</pre> <p>Converts a 2D <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a> to a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p>Each row of the <code translate=\"no\" dir=\"ltr\">output</code> <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> will contain the explicit values from the same row in <code translate=\"no\" dir=\"ltr\">st_input</code>. <code translate=\"no\" dir=\"ltr\">st_input</code> must be ragged-right. If not it is not ragged-right, then an error will be generated.</p> <h4 id=\"example_11\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nindices = [[0, 0], [0, 1], [0, 2], [1, 0], [3, 0]]\nst = tf.sparse.SparseTensor(indices=indices,\n                            values=[1, 2, 3, 4, 5],\n                            dense_shape=[4, 3])\ntf.RaggedTensor.from_sparse(st).to_list()\n[[1, 2, 3], [4], [], [5]]\n</pre> <p>Currently, only two-dimensional <code translate=\"no\" dir=\"ltr\">SparseTensors</code> are supported.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">st_input</code> </td> <td> The sparse tensor to convert. Must have rank 2. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits_dtype</code> </td> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> for the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>'s <code translate=\"no\" dir=\"ltr\">row_splits</code> tensor. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> or <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with the same values as <code translate=\"no\" dir=\"ltr\">st_input</code>. <code translate=\"no\" dir=\"ltr\">output.ragged_rank = rank(st_input) - 1</code>. <code translate=\"no\" dir=\"ltr\">output.shape = [st_input.dense_shape[0], None]</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If the number of dimensions in <code translate=\"no\" dir=\"ltr\">st_input</code> is not known statically, or is not two. </td> </tr> </table> <h3 id=\"from_tensor\" data-text=\"from_tensor\"><code translate=\"no\" dir=\"ltr\">from_tensor</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1492-L1697\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_tensor(\n    tensor, lengths=None, padding=None, ragged_rank=1, name=None,\n    row_splits_dtype=tf.dtypes.int64\n)\n</pre> <p>Converts a <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> into a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p>The set of absent/default values may be specified using a vector of lengths or a padding value (but not both). If <code translate=\"no\" dir=\"ltr\">lengths</code> is specified, then the output tensor will satisfy <code translate=\"no\" dir=\"ltr\">output[row] = tensor[row][:lengths[row]]</code>. If 'lengths' is a list of lists or tuple of lists, those lists will be used as nested row lengths. If <code translate=\"no\" dir=\"ltr\">padding</code> is specified, then any row <em>suffix</em> consisting entirely of <code translate=\"no\" dir=\"ltr\">padding</code> will be excluded from the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. If neither <code translate=\"no\" dir=\"ltr\">lengths</code> nor <code translate=\"no\" dir=\"ltr\">padding</code> is specified, then the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> will have no absent/default values.</p> <h4 id=\"examples\" data-text=\"Examples:\">Examples:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndt = tf.constant([[5, 7, 0], [0, 3, 0], [6, 0, 0]])\ntf.RaggedTensor.from_tensor(dt)\n&lt;tf.RaggedTensor [[5, 7, 0], [0, 3, 0], [6, 0, 0]]&gt;\ntf.RaggedTensor.from_tensor(dt, lengths=[1, 0, 3])\n&lt;tf.RaggedTensor [[5], [], [6, 0, 0]]&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.RaggedTensor.from_tensor(dt, padding=0)\n&lt;tf.RaggedTensor [[5, 7], [0, 3], [6]]&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ndt = tf.constant([[[5, 0], [7, 0], [0, 0]],\n                  [[0, 0], [3, 0], [0, 0]],\n                  [[6, 0], [0, 0], [0, 0]]])\ntf.RaggedTensor.from_tensor(dt, lengths=([2, 0, 3], [1, 1, 2, 0, 1]))\n&lt;tf.RaggedTensor [[[5], [7]], [], [[6, 0], [], [0]]]&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">tensor</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Tensor</code> to convert. Must have rank <code translate=\"no\" dir=\"ltr\">ragged_rank + 1</code> or higher. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">lengths</code> </td> <td> An optional set of row lengths, specified using a 1-D integer <code translate=\"no\" dir=\"ltr\">Tensor</code> whose length is equal to <code translate=\"no\" dir=\"ltr\">tensor.shape[0]</code> (the number of rows in <code translate=\"no\" dir=\"ltr\">tensor</code>). If specified, then <code translate=\"no\" dir=\"ltr\">output[row]</code> will contain <code translate=\"no\" dir=\"ltr\">tensor[row][:lengths[row]]</code>. Negative lengths are treated as zero. You may optionally pass a list or tuple of lengths to this argument, which will be used as nested row lengths to construct a ragged tensor with multiple ragged dimensions. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">padding</code> </td> <td> An optional padding value. If specified, then any row suffix consisting entirely of <code translate=\"no\" dir=\"ltr\">padding</code> will be excluded from the returned RaggedTensor. <code translate=\"no\" dir=\"ltr\">padding</code> is a <code translate=\"no\" dir=\"ltr\">Tensor</code> with the same dtype as <code translate=\"no\" dir=\"ltr\">tensor</code> and with <code translate=\"no\" dir=\"ltr\">shape=tensor.shape[ragged_rank + 1:]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ragged_rank</code> </td> <td> Integer specifying the ragged rank for the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Must be greater than zero. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits_dtype</code> </td> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> for the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>'s <code translate=\"no\" dir=\"ltr\">row_splits</code> tensor. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> or <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with the specified <code translate=\"no\" dir=\"ltr\">ragged_rank</code>. The shape of the returned ragged tensor is compatible with the shape of <code translate=\"no\" dir=\"ltr\">tensor</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If both <code translate=\"no\" dir=\"ltr\">lengths</code> and <code translate=\"no\" dir=\"ltr\">padding</code> are specified. </td> </tr> </table> <h3 id=\"from_uniform_row_length\" data-text=\"from_uniform_row_length\"><code translate=\"no\" dir=\"ltr\">from_uniform_row_length</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L570-L643\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_uniform_row_length(\n    values, uniform_row_length, nrows=None, validate=True, name=None\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">uniform_row_length</code>.</p> <p>This method can be used to create <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s with multiple uniform outer dimensions. For example, a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with shape <code translate=\"no\" dir=\"ltr\">[2, 2, None]</code> can be constructed with this method from a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> values with shape <code translate=\"no\" dir=\"ltr\">[4, None]</code>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\nprint(values.shape)\n(4, None)\nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\nprint(rt1)\n&lt;tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]&gt;\nprint(rt1.shape)\n(2, 2, None)\n</pre> <p>Note that <code translate=\"no\" dir=\"ltr\">rt1</code> only contains one ragged dimension (the innermost dimension). In contrast, if <code translate=\"no\" dir=\"ltr\">from_row_splits</code> is used to construct a similar <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>, then that <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> will have two ragged dimensions:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt2 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])\nprint(rt2.shape)\n(2, None, None)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">uniform_row_length</code> </td> <td> A scalar integer tensor. Must be nonnegative. The size of the outer axis of <code translate=\"no\" dir=\"ltr\">values</code> must be evenly divisible by <code translate=\"no\" dir=\"ltr\">uniform_row_length</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nrows</code> </td> <td> The number of rows in the constructed RaggedTensor. If not specified, then it defaults to <code translate=\"no\" dir=\"ltr\">nvals/uniform_row_length</code> (or <code translate=\"no\" dir=\"ltr\">0</code> if <code translate=\"no\" dir=\"ltr\">uniform_row_length==0</code>). <code translate=\"no\" dir=\"ltr\">nrows</code> only needs to be specified if <code translate=\"no\" dir=\"ltr\">uniform_row_length</code> might be zero. <code translate=\"no\" dir=\"ltr\">uniform_row_length*nrows</code> must be <code translate=\"no\" dir=\"ltr\">nvals</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> that corresponds with the python list defined by: <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = [[values.pop(0) for i in range(uniform_row_length)]\nfor _ in range(nrows)]\n</pre> <p><code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </p>\n</td> </tr> \n</table> <h3 id=\"from_value_rowids\" data-text=\"from_value_rowids\"><code translate=\"no\" dir=\"ltr\">from_value_rowids</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L344-L401\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_value_rowids(\n    values, value_rowids, nrows=None, name=None, validate=True\n)\n</pre> <p>Creates a <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> with rows partitioned by <code translate=\"no\" dir=\"ltr\">value_rowids</code>.</p> <p>The returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> corresponds with the python list defined by:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">result = [[values[i] for i in range(len(values)) if value_rowids[i] == row]\n          for row in range(nrows)]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A potentially ragged tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals, ...]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_rowids</code> </td> <td> A 1-D integer tensor with shape <code translate=\"no\" dir=\"ltr\">[nvals]</code>, which corresponds one-to-one with <code translate=\"no\" dir=\"ltr\">values</code>, and specifies each value's row index. Must be nonnegative, and must be sorted in ascending order. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">nrows</code> </td> <td> An integer scalar specifying the number of rows. This should be specified if the <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> may containing empty training rows. Must be greater than <code translate=\"no\" dir=\"ltr\">value_rowids[-1]</code> (or zero if <code translate=\"no\" dir=\"ltr\">value_rowids</code> is empty). Defaults to <code translate=\"no\" dir=\"ltr\">value_rowids[-1]</code> (or zero if <code translate=\"no\" dir=\"ltr\">value_rowids</code> is empty). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the RaggedTensor (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">validate</code> </td> <td> If true, then use assertions to check that the arguments form a valid <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Note: these assertions incur a runtime cost, since they must be checked for each tensor value. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = values.rank + 1</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = values.ragged_rank + 1</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">nrows</code> is incompatible with <code translate=\"no\" dir=\"ltr\">value_rowids</code>. </td> </tr> </table> <h4 id=\"example_12\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprint(tf.RaggedTensor.from_value_rowids(\n    values=[3, 1, 4, 1, 5, 9, 2, 6],\n    value_rowids=[0, 0, 0, 0, 2, 2, 2, 3],\n    nrows=5))\n&lt;tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]&gt;\n</pre> <h3 id=\"get_shape\" data-text=\"get_shape\"><code translate=\"no\" dir=\"ltr\">get_shape</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L849-L868\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_shape()\n</pre> <p>The statically known shape of this ragged tensor.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">TensorShape</code> containing the statically known shape of this ragged tensor. Ragged dimensions have a size of <code translate=\"no\" dir=\"ltr\">None</code>. </td> </tr> \n</table> <p>Alias for <code translate=\"no\" dir=\"ltr\">shape</code> property.</p> <h4 id=\"examples_2\" data-text=\"Examples:\">Examples:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant([[0], [1, 2]]).get_shape()\nTensorShape([2, None])\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant(\n   [[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1).get_shape()\nTensorShape([2, None, 2])\n</pre> <h3 id=\"merge_dims\" data-text=\"merge_dims\"><code translate=\"no\" dir=\"ltr\">merge_dims</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1386-L1432\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmerge_dims(\n    outer_axis, inner_axis\n)\n</pre> <p>Merges outer_axis...inner_axis into a single dimension.</p> <p>Returns a copy of this RaggedTensor with the specified range of dimensions flattened into a single dimension, with elements in row-major order.</p> <h4 id=\"examples_3\" data-text=\"Examples:\">Examples:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[[1, 2], [3]], [[4, 5, 6]]])\nprint(rt.merge_dims(0, 1))\n&lt;tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]&gt;\nprint(rt.merge_dims(1, 2))\n&lt;tf.RaggedTensor [[1, 2, 3], [4, 5, 6]]&gt;\nprint(rt.merge_dims(0, 2))\ntf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32)\n</pre> <p>To mimic the behavior of <code translate=\"no\" dir=\"ltr\">np.flatten</code> (which flattens all dimensions), use <code translate=\"no\" dir=\"ltr\">rt.merge_dims(0, -1). To mimic the behavior of</code>tf.layers.Flatten<code translate=\"no\" dir=\"ltr\">(which flattens all dimensions except the outermost batch dimension), use</code>rt.merge_dims(1, -1)`.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">outer_axis</code> </td> <td> <code translate=\"no\" dir=\"ltr\">int</code>: The first dimension in the range of dimensions to merge. May be negative if <code translate=\"no\" dir=\"ltr\">self.shape.rank</code> is statically known. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">inner_axis</code> </td> <td> <code translate=\"no\" dir=\"ltr\">int</code>: The last dimension in the range of dimensions to merge. May be negative if <code translate=\"no\" dir=\"ltr\">self.shape.rank</code> is statically known. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A copy of this tensor, with the specified dimensions merged into a single dimension. The shape of the returned tensor will be <code translate=\"no\" dir=\"ltr\">self.shape[:outer_axis] + [N] + self.shape[inner_axis + 1:]</code>, where <code translate=\"no\" dir=\"ltr\">N</code> is the total number of slices in the merged dimensions. </td> </tr> \n</table> <h3 id=\"nested_row_lengths\" data-text=\"nested_row_lengths\"><code translate=\"no\" dir=\"ltr\">nested_row_lengths</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1223-L1242\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nnested_row_lengths(\n    name=None\n)\n</pre> <p>Returns a tuple containing the row_lengths for all ragged dimensions.</p> <p><code translate=\"no\" dir=\"ltr\">rt.nested_row_lengths()</code> is a tuple containing the <code translate=\"no\" dir=\"ltr\">row_lengths</code> tensors for all ragged dimensions in <code translate=\"no\" dir=\"ltr\">rt</code>, ordered from outermost to innermost.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">tuple</code> of 1-D integer <code translate=\"no\" dir=\"ltr\">Tensors</code>. The length of the tuple is equal to <code translate=\"no\" dir=\"ltr\">self.ragged_rank</code>. </td> </tr> \n</table> <h3 id=\"nested_value_rowids\" data-text=\"nested_value_rowids\"><code translate=\"no\" dir=\"ltr\">nested_value_rowids</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1064-L1099\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nnested_value_rowids(\n    name=None\n)\n</pre> <p>Returns a tuple containing the value_rowids for all ragged dimensions.</p> <p><code translate=\"no\" dir=\"ltr\">rt.nested_value_rowids</code> is a tuple containing the <code translate=\"no\" dir=\"ltr\">value_rowids</code> tensors for all ragged dimensions in <code translate=\"no\" dir=\"ltr\">rt</code>, ordered from outermost to innermost. In particular, <code translate=\"no\" dir=\"ltr\">rt.nested_value_rowids = (rt.value_rowids(),) + value_ids</code> where:</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">* `value_ids = ()` if `rt.values` is a `Tensor`.\n* `value_ids = rt.values.nested_value_rowids` otherwise.\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">tuple</code> of 1-D integer <code translate=\"no\" dir=\"ltr\">Tensor</code>s. </td> </tr> \n</table> <h4 id=\"example_13\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant(\n    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])\nfor i, ids in enumerate(rt.nested_value_rowids()):\n  print('row ids for dimension %d: %s' % (i+1, ids.numpy()))\nrow ids for dimension 1: [0 0 0]\nrow ids for dimension 2: [0 0 0 2 2]\nrow ids for dimension 3: [0 0 0 0 2 2 2 3]\n</pre> <h3 id=\"nrows\" data-text=\"nrows\"><code translate=\"no\" dir=\"ltr\">nrows</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1101-L1122\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nnrows(\n    out_type=None, name=None\n)\n</pre> <p>Returns the number of rows in this ragged tensor.</p> <p>I.e., the size of the outermost dimension of the tensor.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">out_type</code> </td> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> for the returned tensor. Defaults to <code translate=\"no\" dir=\"ltr\">self.row_splits.dtype</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A scalar <code translate=\"no\" dir=\"ltr\">Tensor</code> with dtype <code translate=\"no\" dir=\"ltr\">out_type</code>. </td> </tr> \n</table> <h4 id=\"example_14\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.nrows())  # rt has 5 rows.\ntf.Tensor(5, shape=(), dtype=int64)\n</pre> <h3 id=\"numpy\" data-text=\"numpy\"><code translate=\"no\" dir=\"ltr\">numpy</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1974-L2012\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nnumpy()\n</pre> <p>Returns a numpy <code translate=\"no\" dir=\"ltr\">array</code> with the values for this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p>Requires that this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> was constructed in eager execution mode.</p> <p>Ragged dimensions are encoded using numpy <code translate=\"no\" dir=\"ltr\">arrays</code> with <code translate=\"no\" dir=\"ltr\">dtype=object</code> and <code translate=\"no\" dir=\"ltr\">rank=1</code>, where each element is a single row.</p> <h4 id=\"examples\" data-text=\"Examples\">Examples</h4> <p>In the following example, the value returned by <a href=\"raggedtensor#numpy\"><code translate=\"no\" dir=\"ltr\">RaggedTensor.numpy()</code></a> contains three numpy <code translate=\"no\" dir=\"ltr\">array</code> objects: one for each row (with <code translate=\"no\" dir=\"ltr\">rank=1</code> and <code translate=\"no\" dir=\"ltr\">dtype=int64</code>), and one to combine them (with <code translate=\"no\" dir=\"ltr\">rank=1</code> and <code translate=\"no\" dir=\"ltr\">dtype=object</code>):</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant([[1, 2, 3], [4, 5]], dtype=tf.int64).numpy()\narray([array([1, 2, 3]), array([4, 5])], dtype=object)\n</pre> <p>Uniform dimensions are encoded using multidimensional numpy <code translate=\"no\" dir=\"ltr\">array</code>s. In the following example, the value returned by <a href=\"raggedtensor#numpy\"><code translate=\"no\" dir=\"ltr\">RaggedTensor.numpy()</code></a> contains a single numpy <code translate=\"no\" dir=\"ltr\">array</code> object, with <code translate=\"no\" dir=\"ltr\">rank=2</code> and <code translate=\"no\" dir=\"ltr\">dtype=int64</code>:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.ragged.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int64).numpy()\narray([[1, 2, 3], [4, 5, 6]])\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A numpy <code translate=\"no\" dir=\"ltr\">array</code>. </td> </tr> \n</table> <h3 id=\"row_lengths\" data-text=\"row_lengths\"><code translate=\"no\" dir=\"ltr\">row_lengths</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1174-L1221\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrow_lengths(\n    axis=1, name=None\n)\n</pre> <p>Returns the lengths of the rows in this ragged tensor.</p> <p><code translate=\"no\" dir=\"ltr\">rt.row_lengths()[i]</code> indicates the number of values in the <code translate=\"no\" dir=\"ltr\">i</code>th row of <code translate=\"no\" dir=\"ltr\">rt</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">axis</code> </td> <td> An integer constant indicating the axis whose row lengths should be returned. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A potentially ragged integer Tensor with shape <code translate=\"no\" dir=\"ltr\">self.shape[:axis]</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">axis</code> is out of bounds. </td> </tr> </table> <h4 id=\"example_15\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant(\n    [[[3, 1, 4], [1]], [], [[5, 9], [2]], [[6]], []])\nprint(rt.row_lengths())  # lengths of rows in rt\ntf.Tensor([2 0 2 1 0], shape=(5,), dtype=int64)\nprint(rt.row_lengths(axis=2))  # lengths of axis=2 rows.\n&lt;tf.RaggedTensor [[3, 1], [], [2, 1], [1], []]&gt;\n</pre> <h3 id=\"row_limits\" data-text=\"row_limits\"><code translate=\"no\" dir=\"ltr\">row_limits</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1149-L1172\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrow_limits(\n    name=None\n)\n</pre> <p>Returns the limit indices for rows in this ragged tensor.</p> <p>These indices specify where the values for each row end in <code translate=\"no\" dir=\"ltr\">self.values</code>. <code translate=\"no\" dir=\"ltr\">rt.row_limits(self)</code> is equal to <code translate=\"no\" dir=\"ltr\">rt.row_splits[:-1]</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A 1-D integer Tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>. The returned tensor is nonnegative, and is sorted in ascending order. </td> </tr> \n</table> <h4 id=\"example_16\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.row_limits())  # indices of row limits in rt.values\ntf.Tensor([4 4 7 8 8], shape=(5,), dtype=int64)\n</pre> <h3 id=\"row_starts\" data-text=\"row_starts\"><code translate=\"no\" dir=\"ltr\">row_starts</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1124-L1147\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrow_starts(\n    name=None\n)\n</pre> <p>Returns the start indices for rows in this ragged tensor.</p> <p>These indices specify where the values for each row begin in <code translate=\"no\" dir=\"ltr\">self.values</code>. <code translate=\"no\" dir=\"ltr\">rt.row_starts()</code> is equal to <code translate=\"no\" dir=\"ltr\">rt.row_splits[:-1]</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A 1-D integer Tensor with shape <code translate=\"no\" dir=\"ltr\">[nrows]</code>. The returned tensor is nonnegative, and is sorted in ascending order. </td> </tr> \n</table> <h4 id=\"example_17\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.row_starts())  # indices of row starts in rt.values\ntf.Tensor([0 4 4 7 8], shape=(5,), dtype=int64)\n</pre> <h3 id=\"to_list\" data-text=\"to_list\"><code translate=\"no\" dir=\"ltr\">to_list</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L2014-L2027\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nto_list()\n</pre> <p>Returns a nested Python <code translate=\"no\" dir=\"ltr\">list</code> with the values for this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>.</p> <p>Requires that <code translate=\"no\" dir=\"ltr\">rt</code> was constructed in eager execution mode.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A nested Python <code translate=\"no\" dir=\"ltr\">list</code>. </td> </tr> \n</table> <h3 id=\"to_sparse\" data-text=\"to_sparse\"><code translate=\"no\" dir=\"ltr\">to_sparse</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1834-L1858\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nto_sparse(\n    name=None\n)\n</pre> <p>Converts this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> into a <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>.</p> <h4 id=\"example_18\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[1, 2, 3], [4], [], [5, 6]])\nprint(rt.to_sparse())\nSparseTensor(indices=tf.Tensor(\n                 [[0 0] [0 1] [0 2] [1 0] [3 0] [3 1]],\n                 shape=(6, 2), dtype=int64),\n             values=tf.Tensor([1 2 3 4 5 6], shape=(6,), dtype=int32),\n             dense_shape=tf.Tensor([4 3], shape=(2,), dtype=int64))\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A SparseTensor with the same values as <code translate=\"no\" dir=\"ltr\">self</code>. </td> </tr> \n</table> <h3 id=\"to_tensor\" data-text=\"to_tensor\"><code translate=\"no\" dir=\"ltr\">to_tensor</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1699-L1768\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nto_tensor(\n    default_value=None, name=None, shape=None\n)\n</pre> <p>Converts this <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> into a <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a>.</p> <p>If <code translate=\"no\" dir=\"ltr\">shape</code> is specified, then the result is padded and/or truncated to the specified shape.</p> <h4 id=\"examples_4\" data-text=\"Examples:\">Examples:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[9, 8, 7], [], [6, 5], [4]])\nprint(rt.to_tensor())\ntf.Tensor(\n    [[9 8 7] [0 0 0] [6 5 0] [4 0 0]], shape=(4, 3), dtype=int32)\nprint(rt.to_tensor(shape=[5, 2]))\ntf.Tensor(\n    [[9 8] [0 0] [6 5] [4 0] [0 0]], shape=(5, 2), dtype=int32)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">default_value</code> </td> <td> Value to set for indices not specified in <code translate=\"no\" dir=\"ltr\">self</code>. Defaults to zero. <code translate=\"no\" dir=\"ltr\">default_value</code> must be broadcastable to <code translate=\"no\" dir=\"ltr\">self.shape[self.ragged_rank + 1:]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensors (optional). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The shape of the resulting dense tensor. In particular, <code translate=\"no\" dir=\"ltr\">result.shape[i]</code> is <code translate=\"no\" dir=\"ltr\">shape[i]</code> (if <code translate=\"no\" dir=\"ltr\">shape[i]</code> is not None), or <code translate=\"no\" dir=\"ltr\">self.bounding_shape(i)</code> (otherwise).<code translate=\"no\" dir=\"ltr\">shape.rank</code> must be <code translate=\"no\" dir=\"ltr\">None</code> or equal to <code translate=\"no\" dir=\"ltr\">self.rank</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> with shape <code translate=\"no\" dir=\"ltr\">ragged.bounding_shape(self)</code> and the values specified by the non-empty values in <code translate=\"no\" dir=\"ltr\">self</code>. Empty values are assigned <code translate=\"no\" dir=\"ltr\">default_value</code>. </td> </tr> \n</table> <h3 id=\"value_rowids\" data-text=\"value_rowids\"><code translate=\"no\" dir=\"ltr\">value_rowids</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1037-L1062\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalue_rowids(\n    name=None\n)\n</pre> <p>Returns the row indices for the <code translate=\"no\" dir=\"ltr\">values</code> in this ragged tensor.</p> <p><code translate=\"no\" dir=\"ltr\">rt.value_rowids()</code> corresponds one-to-one with the outermost dimension of <code translate=\"no\" dir=\"ltr\">rt.values</code>, and specifies the row containing each value. In particular, the row <code translate=\"no\" dir=\"ltr\">rt[row]</code> consists of the values <code translate=\"no\" dir=\"ltr\">rt.values[j]</code> where <code translate=\"no\" dir=\"ltr\">rt.value_rowids()[j] == row</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name prefix for the returned tensor (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A 1-D integer <code translate=\"no\" dir=\"ltr\">Tensor</code> with shape <code translate=\"no\" dir=\"ltr\">self.values.shape[:1]</code>. The returned tensor is nonnegative, and is sorted in ascending order. </td> </tr> \n</table> <h4 id=\"example_19\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])\nprint(rt.values)\ntf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)\nprint(rt.value_rowids())  # corresponds 1:1 with rt.values\ntf.Tensor([0 0 0 0 2 2 2 3], shape=(8,), dtype=int64)\n</pre> <h3 id=\"with_flat_values\" data-text=\"with_flat_values\"><code translate=\"no\" dir=\"ltr\">with_flat_values</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1334-L1354\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwith_flat_values(\n    new_values\n)\n</pre> <p>Returns a copy of <code translate=\"no\" dir=\"ltr\">self</code> with <code translate=\"no\" dir=\"ltr\">flat_values</code> replaced by <code translate=\"no\" dir=\"ltr\">new_value</code>.</p> <p>Preserves cached row-partitioning tensors such as <code translate=\"no\" dir=\"ltr\">self.cached_nrows</code> and <code translate=\"no\" dir=\"ltr\">self.cached_value_rowids</code> if they have values.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">new_values</code> </td> <td> Potentially ragged tensor that should replace <code translate=\"no\" dir=\"ltr\">self.flat_values</code>. Must have <code translate=\"no\" dir=\"ltr\">rank &gt; 0</code>, and must have the same number of rows as <code translate=\"no\" dir=\"ltr\">self.flat_values</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = self.ragged_rank + new_values.rank</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = self.ragged_rank + new_values.ragged_rank</code>. </td> </tr> \n</table> <h3 id=\"with_row_splits_dtype\" data-text=\"with_row_splits_dtype\"><code translate=\"no\" dir=\"ltr\">with_row_splits_dtype</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1356-L1384\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwith_row_splits_dtype(\n    dtype\n)\n</pre> <p>Returns a copy of this RaggedTensor with the given <code translate=\"no\" dir=\"ltr\">row_splits</code> dtype.</p> <p>For RaggedTensors with multiple ragged dimensions, the <code translate=\"no\" dir=\"ltr\">row_splits</code> for all nested <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> objects are cast to the given dtype.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The dtype for <code translate=\"no\" dir=\"ltr\">row_splits</code>. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> or <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A copy of this RaggedTensor, with the <code translate=\"no\" dir=\"ltr\">row_splits</code> cast to the given type. </td> </tr> \n</table> <h3 id=\"with_values\" data-text=\"with_values\"><code translate=\"no\" dir=\"ltr\">with_values</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L1305-L1332\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwith_values(\n    new_values\n)\n</pre> <p>Returns a copy of <code translate=\"no\" dir=\"ltr\">self</code> with <code translate=\"no\" dir=\"ltr\">values</code> replaced by <code translate=\"no\" dir=\"ltr\">new_value</code>.</p> <p>Preserves cached row-partitioning tensors such as <code translate=\"no\" dir=\"ltr\">self.cached_nrows</code> and <code translate=\"no\" dir=\"ltr\">self.cached_value_rowids</code> if they have values.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">new_values</code> </td> <td> Potentially ragged tensor to use as the <code translate=\"no\" dir=\"ltr\">values</code> for the returned <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Must have <code translate=\"no\" dir=\"ltr\">rank &gt; 0</code>, and must have the same number of rows as <code translate=\"no\" dir=\"ltr\">self.values</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. <code translate=\"no\" dir=\"ltr\">result.rank = 1 + new_values.rank</code>. <code translate=\"no\" dir=\"ltr\">result.ragged_rank = 1 + new_values.ragged_rank</code> </td> </tr> \n</table> <h3 id=\"__abs__\" data-text=\"__abs__\"><code translate=\"no\" dir=\"ltr\">__abs__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L358-L401\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__abs__(\n    x, name=None\n)\n</pre> <p>Computes the absolute value of a tensor.</p> <p>Given a tensor of integer or floating-point values, this operation returns a tensor of the same type, where each element contains the absolute value of the corresponding element in the input.</p> <p>Given a tensor <code translate=\"no\" dir=\"ltr\">x</code> of complex numbers, this operation returns a tensor of type <code translate=\"no\" dir=\"ltr\">float32</code> or <code translate=\"no\" dir=\"ltr\">float64</code> that is the absolute value of each element in <code translate=\"no\" dir=\"ltr\">x</code>. For a complex number \\(a + bj\\), its absolute value is computed as \\(\\sqrt{a^2 + b^2}\\).</p> <h4 id=\"for_example\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# real number\nx = tf.constant([-2.25, 3.25])\ntf.abs(x)\n&lt;tf.Tensor: shape=(2,), dtype=float32,\nnumpy=array([2.25, 3.25], dtype=float32)&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# complex number\nx = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])\ntf.abs(x)\n&lt;tf.Tensor: shape=(2, 1), dtype=float64, numpy=\narray([[5.25594901],\n       [6.60492241]])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">SparseTensor</code> of type <code translate=\"no\" dir=\"ltr\">float16</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code> or <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">SparseTensor</code> of the same size, type and sparsity as <code translate=\"no\" dir=\"ltr\">x</code>, with absolute values. Note, for <code translate=\"no\" dir=\"ltr\">complex64</code> or <code translate=\"no\" dir=\"ltr\">complex128</code> input, the returned <code translate=\"no\" dir=\"ltr\">Tensor</code> will be of type <code translate=\"no\" dir=\"ltr\">float32</code> or <code translate=\"no\" dir=\"ltr\">float64</code>, respectively. <p>If <code translate=\"no\" dir=\"ltr\">x</code> is a <code translate=\"no\" dir=\"ltr\">SparseTensor</code>, returns <code translate=\"no\" dir=\"ltr\">SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)</code> </p>\n</td> </tr> \n</table> <h3 id=\"__add__\" data-text=\"__add__\"><code translate=\"no\" dir=\"ltr\">__add__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__add__(\n    x, y, name=None\n)\n</pre> <p>Returns x + y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">math.add</code></a> supports broadcasting. <code translate=\"no\" dir=\"ltr\">AddN</code> does not. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <p>Given two input tensors, the <a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">tf.add</code></a> operation computes the sum for every element in the tensor.</p> <p>Both input and output have a range <code translate=\"no\" dir=\"ltr\">(-inf, inf)</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">string</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__and__\" data-text=\"__and__\"><code translate=\"no\" dir=\"ltr\">__and__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1568-L1607\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__and__(\n    x, y, name=None\n)\n</pre> <p>Logical AND function.</p> <p>The operation works for the following input types:</p> <ul> <li>Two single elements of type <code translate=\"no\" dir=\"ltr\">bool</code>\n</li> <li>One <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type <code translate=\"no\" dir=\"ltr\">bool</code> and one single <code translate=\"no\" dir=\"ltr\">bool</code>, where the result will be calculated by applying logical AND with the single element to each element in the larger Tensor.</li> <li>Two <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects of type <code translate=\"no\" dir=\"ltr\">bool</code> of the same shape. In this case, the result will be the element-wise logical AND of the two input tensors.</li> </ul> <h4 id=\"usage\" data-text=\"Usage:\">Usage:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_and(a, b)\n&lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_and(c, x)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_and(y, z)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool with the same size as that of x or y. </td> </tr> \n</table> <h3 id=\"__bool__\" data-text=\"__bool__\"><code translate=\"no\" dir=\"ltr\">__bool__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_operators.py#L89-L91\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__bool__(\n    _\n)\n</pre> <p>Dummy method to prevent a RaggedTensor from being used as a Python bool.</p> <h3 id=\"__div__\" data-text=\"__div__\"><code translate=\"no\" dir=\"ltr\">__div__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1339-L1363\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__div__(\n    x, y, name=None\n)\n</pre> <p>Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p> <aside class=\"warning\"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.</span></aside><blockquote class=\"note\">\n<strong>Note:</strong><span> Prefer using the Tensor division operator or tf.divide which obey Python 3 division operator semantics.</span>\n</blockquote> <p>This function divides <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>, forcing Python 2 semantics. That is, if <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> are both integers then the result will be an integer. This is in contrast to Python 3, where division with <code translate=\"no\" dir=\"ltr\">/</code> is always a float while division with <code translate=\"no\" dir=\"ltr\">//</code> is always an integer.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> returns the quotient of x and y. </td> </tr> \n</table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1718-L1753\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>The operation invoked by the <a href=\"raggedtensor#__eq__\"><code translate=\"no\" dir=\"ltr\">Tensor.<strong>eq</strong></code></a> operator.</p> <p>Compares two tensors element-wise for equality if they are broadcast-compatible; or returns False if they are not broadcast-compatible. (Note that this behavior differs from <a href=\"math/equal\"><code translate=\"no\" dir=\"ltr\">tf.math.equal</code></a>, which raises an exception if the two tensors are not broadcast-compatible.)</p> <h4 id=\"purpose_in_the_api\" data-text=\"Purpose in the API:\">Purpose in the API:</h4> <p>This method is exposed in TensorFlow's API so that library developers can register dispatching for <a href=\"raggedtensor#__eq__\"><code translate=\"no\" dir=\"ltr\">Tensor.<strong>eq</strong></code></a> to allow it to handle custom composite tensors &amp; other custom objects.</p> <p>The API symbol is not intended to be called by users directly and does appear in TensorFlow's generated documentation.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">self</code> </td> <td> The left-hand side of the <code translate=\"no\" dir=\"ltr\">==</code> operator. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> The right-hand side of the <code translate=\"no\" dir=\"ltr\">==</code> operator. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The result of the elementwise <code translate=\"no\" dir=\"ltr\">==</code> operation, or <code translate=\"no\" dir=\"ltr\">False</code> if the arguments are not broadcast-compatible. </td> </tr> \n</table> <h3 id=\"__floordiv__\" data-text=\"__floordiv__\"><code translate=\"no\" dir=\"ltr\">__floordiv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1419-L1447\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__floordiv__(\n    x, y, name=None\n)\n</pre> <p>Divides <code translate=\"no\" dir=\"ltr\">x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <a href=\"raggedtensor#__div__\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.div(x,y)</code></a> for integers, but uses <code translate=\"no\" dir=\"ltr\">tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code translate=\"no\" dir=\"ltr\">x // y</code> floor division in Python 3 and in Python 2.7 with <code translate=\"no\" dir=\"ltr\">from __future__ import division</code>.</p> <p><code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> must have the same type, and the result will have the same type as well.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> rounded down. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If the inputs are complex. </td> </tr> </table> <h3 id=\"__ge__\" data-text=\"__ge__\"><code translate=\"no\" dir=\"ltr\">__ge__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ge__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of (x &gt;= y) element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/greater_equal\"><code translate=\"no\" dir=\"ltr\">math.greater_equal</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <h4 id=\"example_20\" data-text=\"Example:\">Example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==&gt; [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==&gt; [True, False, True, True]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__getitem__\" data-text=\"__getitem__\"><code translate=\"no\" dir=\"ltr\">__getitem__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_getitem.py#L35-L103\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__getitem__(\n    key\n)\n</pre> <p>Returns the specified piece of this RaggedTensor.</p> <p>Supports multidimensional indexing and slicing, with one restriction: indexing into a ragged inner dimension is not allowed. This case is problematic because the indicated value may exist in some rows but not others. In such cases, it's not obvious whether we should (1) report an IndexError; (2) use a default value; or (3) skip that value and return a tensor with fewer rows than we started with. Following the guiding principles of Python (\"In the face of ambiguity, refuse the temptation to guess\"), we simply disallow this operation.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">self</code> </td> <td> The RaggedTensor to slice. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">key</code> </td> <td> Indicates which piece of the RaggedTensor to return, using standard Python semantics (e.g., negative values index from the end). <code translate=\"no\" dir=\"ltr\">key</code> may have any of the following types: <ul> <li>\n<code translate=\"no\" dir=\"ltr\">int</code> constant</li> <li>Scalar integer <code translate=\"no\" dir=\"ltr\">Tensor</code>\n</li> <li>\n<code translate=\"no\" dir=\"ltr\">slice</code> containing integer constants and/or scalar integer <code translate=\"no\" dir=\"ltr\">Tensor</code>s</li> <li><code translate=\"no\" dir=\"ltr\">Ellipsis</code></li> <li><a href=\"../tf#newaxis\"><code translate=\"no\" dir=\"ltr\">tf.newaxis</code></a></li> <li>\n<code translate=\"no\" dir=\"ltr\">tuple</code> containing any of the above (for multidimensional indexing) </li>\n</ul>\n</td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">RaggedTensor</code> object. Values that include at least one ragged dimension are returned as <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>. Values that include no ragged dimensions are returned as <code translate=\"no\" dir=\"ltr\">Tensor</code>. See above for examples of expressions that return <code translate=\"no\" dir=\"ltr\">Tensor</code>s vs <code translate=\"no\" dir=\"ltr\">RaggedTensor</code>s. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">key</code> is out of bounds. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">key</code> is not supported. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If the indices in <code translate=\"no\" dir=\"ltr\">key</code> have an unsupported type. </td> </tr> </table> <h4 id=\"examples_5\" data-text=\"Examples:\">Examples:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# A 2-D ragged tensor with 1 ragged dimension.\nrt = tf.ragged.constant([['a', 'b', 'c'], ['d', 'e'], ['f'], ['g']])\nrt[0].numpy()                 # First row (1-D `Tensor`)\narray([b'a', b'b', b'c'], dtype=object)\nrt[:3].to_list()              # First three rows (2-D RaggedTensor)\n[[b'a', b'b', b'c'], [b'd', b'e'], [b'f']]\nrt[3, 0].numpy()              # 1st element of 4th row (scalar)\nb'g'\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n# A 3-D ragged tensor with 2 ragged dimensions.\nrt = tf.ragged.constant([[[1, 2, 3], [4]],\n                         [[5], [], [6]],\n                         [[7]],\n                         [[8, 9], [10]]])\nrt[1].to_list()               # Second row (2-D RaggedTensor)\n[[5], [], [6]]\nrt[3, 0].numpy()              # First element of fourth row (1-D Tensor)\narray([8, 9], dtype=int32)\nrt[:, 1:3].to_list()          # Items 1-3 of each row (3-D RaggedTensor)\n[[[4]], [[], [6]], [], [[10]]]\nrt[:, -1:].to_list()          # Last item of each row (3-D RaggedTensor)\n[[[4]], [[6]], [[7]], [[10]]]\n</pre> <h3 id=\"__gt__\" data-text=\"__gt__\"><code translate=\"no\" dir=\"ltr\">__gt__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__gt__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of (x &gt; y) element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/greater\"><code translate=\"no\" dir=\"ltr\">math.greater</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <h4 id=\"example_21\" data-text=\"Example:\">Example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==&gt; [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==&gt; [False, False, True]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__invert__\" data-text=\"__invert__\"><code translate=\"no\" dir=\"ltr\">__invert__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__invert__(\n    x, name=None\n)\n</pre> <p>Returns the truth value of <code translate=\"no\" dir=\"ltr\">NOT x</code> element-wise.</p> <h4 id=\"example_22\" data-text=\"Example:\">Example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.math.logical_not(tf.constant([True, False]))\n&lt;tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__le__\" data-text=\"__le__\"><code translate=\"no\" dir=\"ltr\">__le__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__le__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of (x &lt;= y) element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/less_equal\"><code translate=\"no\" dir=\"ltr\">math.less_equal</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <h4 id=\"example_23\" data-text=\"Example:\">Example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==&gt; [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==&gt; [True, True, True]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__lt__\" data-text=\"__lt__\"><code translate=\"no\" dir=\"ltr\">__lt__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__lt__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of (x &lt; y) element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/less\"><code translate=\"no\" dir=\"ltr\">math.less</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <h4 id=\"example_24\" data-text=\"Example:\">Example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==&gt; [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==&gt; [False, True, True]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__mod__\" data-text=\"__mod__\"><code translate=\"no\" dir=\"ltr\">__mod__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__mod__(\n    x, y, name=None\n)\n</pre> <p>Returns element-wise remainder of division. When <code translate=\"no\" dir=\"ltr\">x &lt; 0</code> xor <code translate=\"no\" dir=\"ltr\">y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code translate=\"no\" dir=\"ltr\">floor(x / y) * y + mod(x, y) = x</code>.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/floormod\"><code translate=\"no\" dir=\"ltr\">math.floormod</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__mul__\" data-text=\"__mul__\"><code translate=\"no\" dir=\"ltr\">__mul__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L472-L518\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__mul__(\n    x, y, name=None\n)\n</pre> <p>Returns an element-wise x * y.</p> <h4 id=\"for_example_2\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(([1, 2, 3, 4]))\ntf.math.multiply(x, x)\n&lt;tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)&gt;\n</pre> <p>Since <a href=\"math/multiply\"><code translate=\"no\" dir=\"ltr\">tf.math.multiply</code></a> will convert its arguments to <code translate=\"no\" dir=\"ltr\">Tensor</code>s, you can also pass in non-<code translate=\"no\" dir=\"ltr\">Tensor</code> arguments:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.math.multiply(7,6)\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;\n</pre> <p>If <code translate=\"no\" dir=\"ltr\">x.shape</code> is not thes same as <code translate=\"no\" dir=\"ltr\">y.shape</code>, they will be broadcast to a compatible shape. (More about broadcasting <a href=\"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a>.)</p> <h4 id=\"for_example_3\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.ones([1, 2]);\ny = tf.ones([2, 1]);\nx * y  # Taking advantage of operator overriding\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1., 1.],\n     [1., 1.]], dtype=float32)&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A Tensor. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n</table> <p>A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <ul> <li>InvalidArgumentError: When <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> have incomptatible shapes or types. </li>\n</ul>\n</td> </tr> \n</table> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1756-L1789\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>The operation invoked by the <a href=\"raggedtensor#__ne__\"><code translate=\"no\" dir=\"ltr\">Tensor.<strong>ne</strong></code></a> operator.</p> <p>Compares two tensors element-wise for inequality if they are broadcast-compatible; or returns True if they are not broadcast-compatible. (Note that this behavior differs from <a href=\"math/not_equal\"><code translate=\"no\" dir=\"ltr\">tf.math.not_equal</code></a>, which raises an exception if the two tensors are not broadcast-compatible.)</p> <h4 id=\"purpose_in_the_api_2\" data-text=\"Purpose in the API:\">Purpose in the API:</h4> <p>This method is exposed in TensorFlow's API so that library developers can register dispatching for <a href=\"raggedtensor#__ne__\"><code translate=\"no\" dir=\"ltr\">Tensor.<strong>ne</strong></code></a> to allow it to handle custom composite tensors &amp; other custom objects.</p> <p>The API symbol is not intended to be called by users directly and does appear in TensorFlow's generated documentation.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">self</code> </td> <td> The left-hand side of the <code translate=\"no\" dir=\"ltr\">!=</code> operator. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> The right-hand side of the <code translate=\"no\" dir=\"ltr\">!=</code> operator. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The result of the elementwise <code translate=\"no\" dir=\"ltr\">!=</code> operation, or <code translate=\"no\" dir=\"ltr\">True</code> if the arguments are not broadcast-compatible. </td> </tr> \n</table> <h3 id=\"__neg__\" data-text=\"__neg__\"><code translate=\"no\" dir=\"ltr\">__neg__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__neg__(\n    x, name=None\n)\n</pre> <p>Computes numerical negative value element-wise.</p> <p>I.e., \\(y = -x\\).</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. <p>If <code translate=\"no\" dir=\"ltr\">x</code> is a <code translate=\"no\" dir=\"ltr\">SparseTensor</code>, returns <code translate=\"no\" dir=\"ltr\">SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)</code> </p>\n</td> </tr> \n</table> <h3 id=\"__nonzero__\" data-text=\"__nonzero__\"><code translate=\"no\" dir=\"ltr\">__nonzero__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_operators.py#L89-L91\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__nonzero__(\n    _\n)\n</pre> <p>Dummy method to prevent a RaggedTensor from being used as a Python bool.</p> <h3 id=\"__or__\" data-text=\"__or__\"><code translate=\"no\" dir=\"ltr\">__or__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__or__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of x OR y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/logical_or\"><code translate=\"no\" dir=\"ltr\">math.logical_or</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__pow__\" data-text=\"__pow__\"><code translate=\"no\" dir=\"ltr\">__pow__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L645-L670\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__pow__(\n    x, y, name=None\n)\n</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code translate=\"no\" dir=\"ltr\">x</code> and a tensor <code translate=\"no\" dir=\"ltr\">y</code>, this operation computes \\(x^y\\) for corresponding elements in <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>. For example:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([[2, 2], [3, 3]])\ny = tf.constant([[8, 16], [2, 3]])\ntf.pow(x, y)  # [[256, 65536], [9, 27]]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">float16</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, or <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">float16</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, or <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. </td> </tr> \n</table> <h3 id=\"__radd__\" data-text=\"__radd__\"><code translate=\"no\" dir=\"ltr\">__radd__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__radd__(\n    x, y, name=None\n)\n</pre> <p>Returns x + y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">math.add</code></a> supports broadcasting. <code translate=\"no\" dir=\"ltr\">AddN</code> does not. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote> <p>Given two input tensors, the <a href=\"math/add\"><code translate=\"no\" dir=\"ltr\">tf.add</code></a> operation computes the sum for every element in the tensor.</p> <p>Both input and output have a range <code translate=\"no\" dir=\"ltr\">(-inf, inf)</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">string</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__rand__\" data-text=\"__rand__\"><code translate=\"no\" dir=\"ltr\">__rand__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1568-L1607\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rand__(\n    x, y, name=None\n)\n</pre> <p>Logical AND function.</p> <p>The operation works for the following input types:</p> <ul> <li>Two single elements of type <code translate=\"no\" dir=\"ltr\">bool</code>\n</li> <li>One <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type <code translate=\"no\" dir=\"ltr\">bool</code> and one single <code translate=\"no\" dir=\"ltr\">bool</code>, where the result will be calculated by applying logical AND with the single element to each element in the larger Tensor.</li> <li>Two <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects of type <code translate=\"no\" dir=\"ltr\">bool</code> of the same shape. In this case, the result will be the element-wise logical AND of the two input tensors.</li> </ul> <h4 id=\"usage_2\" data-text=\"Usage:\">Usage:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_and(a, b)\n&lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_and(c, x)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_and(y, z)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False, False, False,  True])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool with the same size as that of x or y. </td> </tr> \n</table> <h3 id=\"__rdiv__\" data-text=\"__rdiv__\"><code translate=\"no\" dir=\"ltr\">__rdiv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1339-L1363\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rdiv__(\n    x, y, name=None\n)\n</pre> <p>Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p> <aside class=\"warning\"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Deprecated in favor of operator or tf.math.divide.</span></aside><blockquote class=\"note\">\n<strong>Note:</strong><span> Prefer using the Tensor division operator or tf.divide which obey Python 3 division operator semantics.</span>\n</blockquote> <p>This function divides <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>, forcing Python 2 semantics. That is, if <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> are both integers then the result will be an integer. This is in contrast to Python 3, where division with <code translate=\"no\" dir=\"ltr\">/</code> is always a float while division with <code translate=\"no\" dir=\"ltr\">//</code> is always an integer.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> returns the quotient of x and y. </td> </tr> \n</table> <h3 id=\"__rfloordiv__\" data-text=\"__rfloordiv__\"><code translate=\"no\" dir=\"ltr\">__rfloordiv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1419-L1447\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rfloordiv__(\n    x, y, name=None\n)\n</pre> <p>Divides <code translate=\"no\" dir=\"ltr\">x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <a href=\"raggedtensor#__div__\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.div(x,y)</code></a> for integers, but uses <code translate=\"no\" dir=\"ltr\">tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code translate=\"no\" dir=\"ltr\">x // y</code> floor division in Python 3 and in Python 2.7 with <code translate=\"no\" dir=\"ltr\">from __future__ import division</code>.</p> <p><code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> must have the same type, and the result will have the same type as well.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of real numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> rounded down. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If the inputs are complex. </td> </tr> </table> <h3 id=\"__rmod__\" data-text=\"__rmod__\"><code translate=\"no\" dir=\"ltr\">__rmod__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rmod__(\n    x, y, name=None\n)\n</pre> <p>Returns element-wise remainder of division. When <code translate=\"no\" dir=\"ltr\">x &lt; 0</code> xor <code translate=\"no\" dir=\"ltr\">y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code translate=\"no\" dir=\"ltr\">floor(x / y) * y + mod(x, y) = x</code>.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/floormod\"><code translate=\"no\" dir=\"ltr\">math.floormod</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__rmul__\" data-text=\"__rmul__\"><code translate=\"no\" dir=\"ltr\">__rmul__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L472-L518\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rmul__(\n    x, y, name=None\n)\n</pre> <p>Returns an element-wise x * y.</p> <h4 id=\"for_example_4\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.constant(([1, 2, 3, 4]))\ntf.math.multiply(x, x)\n&lt;tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)&gt;\n</pre> <p>Since <a href=\"math/multiply\"><code translate=\"no\" dir=\"ltr\">tf.math.multiply</code></a> will convert its arguments to <code translate=\"no\" dir=\"ltr\">Tensor</code>s, you can also pass in non-<code translate=\"no\" dir=\"ltr\">Tensor</code> arguments:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.math.multiply(7,6)\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=42&gt;\n</pre> <p>If <code translate=\"no\" dir=\"ltr\">x.shape</code> is not thes same as <code translate=\"no\" dir=\"ltr\">y.shape</code>, they will be broadcast to a compatible shape. (More about broadcasting <a href=\"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a>.)</p> <h4 id=\"for_example_5\" data-text=\"For example:\">For example:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nx = tf.ones([1, 2]);\ny = tf.ones([2, 1]);\nx * y  # Taking advantage of operator overriding\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1., 1.],\n     [1., 1.]], dtype=float32)&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A Tensor. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> \n</table> <p>A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <ul> <li>InvalidArgumentError: When <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> have incomptatible shapes or types. </li>\n</ul>\n</td> </tr> \n</table> <h3 id=\"__ror__\" data-text=\"__ror__\"><code translate=\"no\" dir=\"ltr\">__ror__</code></h3> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ror__(\n    x, y, name=None\n)\n</pre> <p>Returns the truth value of x OR y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <a href=\"math/logical_or\"><code translate=\"no\" dir=\"ltr\">math.logical_or</code></a> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">bool</code>. </td> </tr> \n</table> <h3 id=\"__rpow__\" data-text=\"__rpow__\"><code translate=\"no\" dir=\"ltr\">__rpow__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L645-L670\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rpow__(\n    x, y, name=None\n)\n</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code translate=\"no\" dir=\"ltr\">x</code> and a tensor <code translate=\"no\" dir=\"ltr\">y</code>, this operation computes \\(x^y\\) for corresponding elements in <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code>. For example:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">x = tf.constant([[2, 2], [3, 3]])\ny = tf.constant([[8, 16], [2, 3]])\ntf.pow(x, y)  # [[256, 65536], [9, 27]]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">float16</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, or <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">float16</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, or <code translate=\"no\" dir=\"ltr\">complex128</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. </td> </tr> \n</table> <h3 id=\"__rsub__\" data-text=\"__rsub__\"><code translate=\"no\" dir=\"ltr\">__rsub__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L533-L561\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rsub__(\n    x, y, name=None\n)\n</pre> <p>Returns x - y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <code translate=\"no\" dir=\"ltr\">Subtract</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__rtruediv__\" data-text=\"__rtruediv__\"><code translate=\"no\" dir=\"ltr\">__rtruediv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1306-L1336\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rtruediv__(\n    x, y, name=None\n)\n</pre> <p>Divides x / y elementwise (using Python 3 division operator semantics).</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.</span>\n</blockquote> <p>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal <code translate=\"no\" dir=\"ltr\">x / y</code> division in Python 3 and in Python 2.7 with <code translate=\"no\" dir=\"ltr\">from __future__ import division</code>. If you want integer division that rounds down, use <code translate=\"no\" dir=\"ltr\">x // y</code> or <code translate=\"no\" dir=\"ltr\">tf.math.floordiv</code>.</p> <p><code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to <code translate=\"no\" dir=\"ltr\">float32</code> for <code translate=\"no\" dir=\"ltr\">int8</code> and <code translate=\"no\" dir=\"ltr\">int16</code> and <code translate=\"no\" dir=\"ltr\">float64</code> for <code translate=\"no\" dir=\"ltr\">int32</code> and <code translate=\"no\" dir=\"ltr\">int64</code> (matching the behavior of Numpy).</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> evaluated in floating point. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> have different dtypes. </td> </tr> </table> <h3 id=\"__rxor__\" data-text=\"__rxor__\"><code translate=\"no\" dir=\"ltr\">__rxor__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1519-L1565\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__rxor__(\n    x, y, name='LogicalXor'\n)\n</pre> <p>Logical XOR function.</p> <p>x ^ y = (x | y) &amp; ~(x &amp; y)</p> <p>The operation works for the following input types:</p> <ul> <li>Two single elements of type <code translate=\"no\" dir=\"ltr\">bool</code>\n</li> <li>One <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type <code translate=\"no\" dir=\"ltr\">bool</code> and one single <code translate=\"no\" dir=\"ltr\">bool</code>, where the result will be calculated by applying logical XOR with the single element to each element in the larger Tensor.</li> <li>Two <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects of type <code translate=\"no\" dir=\"ltr\">bool</code> of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.</li> </ul> <h4 id=\"usage_3\" data-text=\"Usage:\">Usage:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_xor(a, b)\n&lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_xor(c, x)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_xor(y, z)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool with the same size as that of x or y. </td> </tr> \n</table> <h3 id=\"__sub__\" data-text=\"__sub__\"><code translate=\"no\" dir=\"ltr\">__sub__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L533-L561\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__sub__(\n    x, y, name=None\n)\n</pre> <p>Returns x - y element-wise.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> <code translate=\"no\" dir=\"ltr\">Subtract</code> supports broadcasting. More about broadcasting <a href=\"http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\">here</a></span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">x</code>. </td> </tr> \n</table> <h3 id=\"__truediv__\" data-text=\"__truediv__\"><code translate=\"no\" dir=\"ltr\">__truediv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1306-L1336\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__truediv__(\n    x, y, name=None\n)\n</pre> <p>Divides x / y elementwise (using Python 3 division operator semantics).</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Prefer using the Tensor operator or tf.divide which obey Python division operator semantics.</span>\n</blockquote> <p>This function forces Python 3 division operator semantics where all integer arguments are cast to floating types first. This op is generated by normal <code translate=\"no\" dir=\"ltr\">x / y</code> division in Python 3 and in Python 2.7 with <code translate=\"no\" dir=\"ltr\">from __future__ import division</code>. If you want integer division that rounds down, use <code translate=\"no\" dir=\"ltr\">x // y</code> or <code translate=\"no\" dir=\"ltr\">tf.math.floordiv</code>.</p> <p><code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> must have the same numeric type. If the inputs are floating point, the output will have the same type. If the inputs are integral, the inputs are cast to <code translate=\"no\" dir=\"ltr\">float32</code> for <code translate=\"no\" dir=\"ltr\">int8</code> and <code translate=\"no\" dir=\"ltr\">int16</code> and <code translate=\"no\" dir=\"ltr\">float64</code> for <code translate=\"no\" dir=\"ltr\">int32</code> and <code translate=\"no\" dir=\"ltr\">int64</code> (matching the behavior of Numpy).</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> numerator of numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> <code translate=\"no\" dir=\"ltr\">Tensor</code> denominator of numeric type. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> <code translate=\"no\" dir=\"ltr\">x / y</code> evaluated in floating point. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">x</code> and <code translate=\"no\" dir=\"ltr\">y</code> have different dtypes. </td> </tr> </table> <h3 id=\"__xor__\" data-text=\"__xor__\"><code translate=\"no\" dir=\"ltr\">__xor__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1519-L1565\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__xor__(\n    x, y, name='LogicalXor'\n)\n</pre> <p>Logical XOR function.</p> <p>x ^ y = (x | y) &amp; ~(x &amp; y)</p> <p>The operation works for the following input types:</p> <ul> <li>Two single elements of type <code translate=\"no\" dir=\"ltr\">bool</code>\n</li> <li>One <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type <code translate=\"no\" dir=\"ltr\">bool</code> and one single <code translate=\"no\" dir=\"ltr\">bool</code>, where the result will be calculated by applying logical XOR with the single element to each element in the larger Tensor.</li> <li>Two <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects of type <code translate=\"no\" dir=\"ltr\">bool</code> of the same shape. In this case, the result will be the element-wise logical XOR of the two input tensors.</li> </ul> <h4 id=\"usage_4\" data-text=\"Usage:\">Usage:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\na = tf.constant([True])\nb = tf.constant([False])\ntf.math.logical_xor(a, b)\n&lt;tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nc = tf.constant([True])\nx = tf.constant([False, True, True, False])\ntf.math.logical_xor(c, x)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([ True, False, False,  True])&gt;\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ny = tf.constant([False, False, True, True])\nz = tf.constant([False, True, False, True])\ntf.math.logical_xor(y, z)\n&lt;tf.Tensor: shape=(4,), dtype=bool, numpy=array([False,  True,  True, False])&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">x</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">y</code> </td> <td> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> of type bool with the same size as that of x or y. </td> </tr> \n</table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/RaggedTensor\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/RaggedTensor</a>\n  </p>\n</div>\n","xla":"<h1 class=\"devsite-page-title\">Module: tf.xla</h1>       <p>Public API for tf.xla namespace.</p> <h2 id=\"modules\" data-text=\"Modules\">Modules</h2> <p><a href=\"xla/experimental\"><code translate=\"no\" dir=\"ltr\">experimental</code></a> module: Public API for tf.xla.experimental namespace.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/xla\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/xla</a>\n  </p>\n</div>\n","graph":"<h1 class=\"devsite-page-title\">tf.Graph</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L2869-L5195\">  View source on GitHub </a> </td> </table> <p>A TensorFlow computation, represented as a dataflow graph.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/Graph\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Graph</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.Graph()\n</pre>  <p>Graphs are used by <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>s to represent the function's computations. Each graph contains a set of <a href=\"operation\"><code translate=\"no\" dir=\"ltr\">tf.Operation</code></a> objects, which represent units of computation; and <a href=\"tensor\"><code translate=\"no\" dir=\"ltr\">tf.Tensor</code></a> objects, which represent the units of data that flow between operations.</p> <h3 id=\"using_graphs_directly_deprecated\" data-text=\"Using graphs directly (deprecated)\">Using graphs directly (deprecated)</h3> <p>A <a href=\"graph\"><code translate=\"no\" dir=\"ltr\">tf.Graph</code></a> can be constructed and used directly without a <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a>, as was required in TensorFlow 1, but this is deprecated and it is recommended to use a <a href=\"function\"><code translate=\"no\" dir=\"ltr\">tf.function</code></a> instead. If a graph is directly used, other deprecated TensorFlow 1 classes are also required to execute the graph, such as a <a href=\"compat/v1/session\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.Session</code></a>.</p> <p>A default graph can be registered with the <a href=\"graph#as_default\"><code translate=\"no\" dir=\"ltr\">tf.Graph.as_default</code></a> context manager. Then, operations will be added to the graph instead of being executed eagerly. For example:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">g = tf.Graph()\nwith g.as_default():\n  # Define operations and tensors in `g`.\n  c = tf.constant(30.0)\n  assert c.graph is g\n</pre> <p><a href=\"compat/v1/get_default_graph\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.get_default_graph()</code></a> can be used to obtain the default graph.</p> <p>Important note: This class <em>is not</em> thread-safe for graph construction. All operations should be created from a single thread, or external synchronization must be provided. Unless otherwise specified, all methods are not thread-safe.</p> <p>A <code translate=\"no\" dir=\"ltr\">Graph</code> instance supports an arbitrary number of \"collections\" that are identified by name. For convenience when building a large graph, collections can store groups of related objects: for example, the <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a> uses a collection (named <code translate=\"no\" dir=\"ltr\">tf.GraphKeys.GLOBAL_VARIABLES</code>) for all variables that are created during the construction of a graph. The caller may define additional collections by specifying a new name.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">building_function</code> </td> <td> Returns True iff this graph represents a function. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">collections</code> </td> <td> Returns the names of the collections known to this graph. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">finalized</code> </td> <td> True if this graph has been finalized. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">graph_def_versions</code> </td> <td> The GraphDef version information of this graph. <p>For details on the meaning of each version, see <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\"><code translate=\"no\" dir=\"ltr\">GraphDef</code></a>. </p>\n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">seed</code> </td> <td> The graph-level random seed of this graph. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">version</code> </td> <td> Returns a version number that increases as ops are added to the graph. <p>Note that this is unrelated to the <a href=\"graph#graph_def_versions\"><code translate=\"no\" dir=\"ltr\">tf.Graph.graph_def_versions</code></a>. </p>\n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"add_to_collection\" data-text=\"add_to_collection\"><code translate=\"no\" dir=\"ltr\">add_to_collection</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3989-L4005\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nadd_to_collection(\n    name, value\n)\n</pre> <p>Stores <code translate=\"no\" dir=\"ltr\">value</code> in the collection with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p> <p>Note that collections are not sets, so it is possible to add a value to a collection several times.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The key for the collection. The <code translate=\"no\" dir=\"ltr\">GraphKeys</code> class contains many standard names for collections. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value</code> </td> <td> The value to add to the collection. </td> </tr> </table> <h3 id=\"add_to_collections\" data-text=\"add_to_collections\"><code translate=\"no\" dir=\"ltr\">add_to_collections</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4007-L4026\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nadd_to_collections(\n    names, value\n)\n</pre> <p>Stores <code translate=\"no\" dir=\"ltr\">value</code> in the collections given by <code translate=\"no\" dir=\"ltr\">names</code>.</p> <p>Note that collections are not sets, so it is possible to add a value to a collection several times. This function makes sure that duplicates in <code translate=\"no\" dir=\"ltr\">names</code> are ignored, but it will not check for pre-existing membership of <code translate=\"no\" dir=\"ltr\">value</code> in any of the collections in <code translate=\"no\" dir=\"ltr\">names</code>.</p> <p><code translate=\"no\" dir=\"ltr\">names</code> can be any iterable, but if <code translate=\"no\" dir=\"ltr\">names</code> is a string, it is treated as a single collection name.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">names</code> </td> <td> The keys for the collections to add to. The <code translate=\"no\" dir=\"ltr\">GraphKeys</code> class contains many standard names for collections. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value</code> </td> <td> The value to add to the collections. </td> </tr> </table> <h3 id=\"as_default\" data-text=\"as_default\"><code translate=\"no\" dir=\"ltr\">as_default</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3942-L3982\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nas_default()\n</pre> <p>Returns a context manager that makes this <code translate=\"no\" dir=\"ltr\">Graph</code> the default graph.</p> <p>This method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly.</p> <p>Use this method with the <code translate=\"no\" dir=\"ltr\">with</code> keyword to specify that ops created within the scope of a block should be added to this graph. In this case, once the scope of the <code translate=\"no\" dir=\"ltr\">with</code> is exited, the previous default graph is set again as default. There is a stack, so it's ok to have multiple nested levels of <code translate=\"no\" dir=\"ltr\">as_default</code> calls.</p> <p>The default graph is a property of the current thread. If you create a new thread, and wish to use the default graph in that thread, you must explicitly add a <code translate=\"no\" dir=\"ltr\">with g.as_default():</code> in that thread's function.</p> <p>The following code examples are equivalent:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\"># 1. Using Graph.as_default():\ng = tf.Graph()\nwith g.as_default():\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n# 2. Constructing and making default:\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  assert c.graph is g\n</pre> <p>If eager execution is enabled ops created under this context manager will be added to the graph instead of executed eagerly.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager for using this graph as the default graph. </td> </tr> \n</table> <h3 id=\"as_graph_def\" data-text=\"as_graph_def\"><code translate=\"no\" dir=\"ltr\">as_graph_def</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3319-L3346\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nas_graph_def(\n    from_version=None, add_shapes=False\n)\n</pre> <p>Returns a serialized <code translate=\"no\" dir=\"ltr\">GraphDef</code> representation of this graph.</p> <p>The serialized <code translate=\"no\" dir=\"ltr\">GraphDef</code> can be imported into another <code translate=\"no\" dir=\"ltr\">Graph</code> (using <a href=\"graph_util/import_graph_def\"><code translate=\"no\" dir=\"ltr\">tf.import_graph_def</code></a>) or used with the <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/api_docs/cc/index\">C++ Session API</a>.</p> <p>This method is thread-safe.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">from_version</code> </td> <td> Optional. If this is set, returns a <code translate=\"no\" dir=\"ltr\">GraphDef</code> containing only the nodes that were added to this graph since its <code translate=\"no\" dir=\"ltr\">version</code> property had the given value. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">add_shapes</code> </td> <td> If true, adds an \"_output_shapes\" list attr to each node with the inferred shapes of each of its outputs. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <a href=\"https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto\"><code translate=\"no\" dir=\"ltr\">GraphDef</code></a> protocol buffer. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If the <code translate=\"no\" dir=\"ltr\">graph_def</code> would be too large. </td> </tr> </table> <h3 id=\"as_graph_element\" data-text=\"as_graph_element\"><code translate=\"no\" dir=\"ltr\">as_graph_element</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3692-L3726\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nas_graph_element(\n    obj, allow_tensor=True, allow_operation=True\n)\n</pre> <p>Returns the object referred to by <code translate=\"no\" dir=\"ltr\">obj</code>, as an <code translate=\"no\" dir=\"ltr\">Operation</code> or <code translate=\"no\" dir=\"ltr\">Tensor</code>.</p> <p>This function validates that <code translate=\"no\" dir=\"ltr\">obj</code> represents an element of this graph, and gives an informative error message if it is not.</p> <p>This function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.</p> <p>This method may be called concurrently from multiple threads.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">obj</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>, an <code translate=\"no\" dir=\"ltr\">Operation</code>, or the name of a tensor or operation. Can also be any object with an <code translate=\"no\" dir=\"ltr\">_as_graph_element()</code> method that returns a value of one of these types. Note: <code translate=\"no\" dir=\"ltr\">_as_graph_element</code> will be called inside the graph's lock and so may not modify the graph. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">allow_tensor</code> </td> <td> If true, <code translate=\"no\" dir=\"ltr\">obj</code> may refer to a <code translate=\"no\" dir=\"ltr\">Tensor</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">allow_operation</code> </td> <td> If true, <code translate=\"no\" dir=\"ltr\">obj</code> may refer to an <code translate=\"no\" dir=\"ltr\">Operation</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The <code translate=\"no\" dir=\"ltr\">Tensor</code> or <code translate=\"no\" dir=\"ltr\">Operation</code> in the Graph corresponding to <code translate=\"no\" dir=\"ltr\">obj</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">obj</code> is not a type we support attempting to convert to types. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">obj</code> is of an appropriate type but invalid. For example, an invalid string. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">KeyError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">obj</code> is not an object in the graph. </td> </tr> </table> <h3 id=\"clear_collection\" data-text=\"clear_collection\"><code translate=\"no\" dir=\"ltr\">clear_collection</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4098-L4108\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nclear_collection(\n    name\n)\n</pre> <p>Clears all values in a collection.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The key for the collection. The <code translate=\"no\" dir=\"ltr\">GraphKeys</code> class contains many standard names for collections. </td> </tr> </table> <h3 id=\"colocate_with\" data-text=\"colocate_with\"><code translate=\"no\" dir=\"ltr\">colocate_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4348-L4425\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\ncolocate_with(\n    op, ignore_existing=False\n)\n</pre> <p>Returns a context manager that specifies an op to colocate with.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> this function is not for public use, only for internal libraries.</span>\n</blockquote> <h4 id=\"for_example\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">a = tf.Variable([1.0])\nwith g.colocate_with(a):\n  b = tf.constant(1.0)\n  c = tf.add(a, b)\n</pre> <p><code translate=\"no\" dir=\"ltr\">b</code> and <code translate=\"no\" dir=\"ltr\">c</code> will always be colocated with <code translate=\"no\" dir=\"ltr\">a</code>, no matter where <code translate=\"no\" dir=\"ltr\">a</code> is eventually placed.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Using a colocation scope resets any existing device constraints.</span>\n</blockquote> <p>If <code translate=\"no\" dir=\"ltr\">op</code> is <code translate=\"no\" dir=\"ltr\">None</code> then <code translate=\"no\" dir=\"ltr\">ignore_existing</code> must be <code translate=\"no\" dir=\"ltr\">True</code> and the new scope resets all colocation and device constraints.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">op</code> </td> <td> The op to colocate all created ops with, or <code translate=\"no\" dir=\"ltr\">None</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ignore_existing</code> </td> <td> If true, only applies colocation of this op within the context, rather than applying all colocation properties on the stack. If <code translate=\"no\" dir=\"ltr\">op</code> is <code translate=\"no\" dir=\"ltr\">None</code>, this value must be <code translate=\"no\" dir=\"ltr\">True</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if op is None but ignore_existing is False. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Yields</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager that specifies the op with which to colocate newly created ops. </td> </tr> \n</table> <h3 id=\"container\" data-text=\"container\"><code translate=\"no\" dir=\"ltr\">container</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4531-L4581\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\ncontainer(\n    container_name\n)\n</pre> <p>Returns a context manager that specifies the resource container to use.</p> <p>Stateful operations, such as variables and queues, can maintain their states on devices so that they can be shared by multiple processes. A resource container is a string name under which these stateful operations are tracked. These resources can be released or cleared with <code translate=\"no\" dir=\"ltr\">tf.Session.reset()</code>.</p> <h4 id=\"for_example_2\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with g.container('experiment0'):\n  # All stateful Operations constructed in this context will be placed\n  # in resource container \"experiment0\".\n  v1 = tf.Variable([1.0])\n  v2 = tf.Variable([2.0])\n  with g.container(\"experiment1\"):\n    # All stateful Operations constructed in this context will be\n    # placed in resource container \"experiment1\".\n    v3 = tf.Variable([3.0])\n    q1 = tf.queue.FIFOQueue(10, tf.float32)\n  # All stateful Operations constructed in this context will be\n  # be created in the \"experiment0\".\n  v4 = tf.Variable([4.0])\n  q1 = tf.queue.FIFOQueue(20, tf.float32)\n  with g.container(\"\"):\n    # All stateful Operations constructed in this context will be\n    # be placed in the default resource container.\n    v5 = tf.Variable([5.0])\n    q3 = tf.queue.FIFOQueue(30, tf.float32)\n\n# Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n# will become undefined (such as uninitialized).\ntf.Session.reset(target, [\"experiment0\"])\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">container_name</code> </td> <td> container name string. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager for defining resource containers for stateful ops, yields the container name. </td> </tr> \n</table> <h3 id=\"control_dependencies\" data-text=\"control_dependencies\"><code translate=\"no\" dir=\"ltr\">control_dependencies</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4712-L4823\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ncontrol_dependencies(\n    control_inputs\n)\n</pre> <p>Returns a context manager that specifies control dependencies.</p> <p>Use with the <code translate=\"no\" dir=\"ltr\">with</code> keyword to specify that all operations constructed within the context should have control dependencies on <code translate=\"no\" dir=\"ltr\">control_inputs</code>. For example:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with g.control_dependencies([a, b, c]):\n  # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n  d = ...\n  e = ...\n</pre> <p>Multiple calls to <code translate=\"no\" dir=\"ltr\">control_dependencies()</code> can be nested, and in that case a new <code translate=\"no\" dir=\"ltr\">Operation</code> will have control dependencies on the union of <code translate=\"no\" dir=\"ltr\">control_inputs</code> from all active contexts.</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies([c, d]):\n    # Ops constructed here run after `a`, `b`, `c`, and `d`.\n</pre> <p>You can pass None to clear the control dependencies:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies(None):\n    # Ops constructed here run normally, not waiting for either `a` or `b`.\n    with g.control_dependencies([c, d]):\n      # Ops constructed here run after `c` and `d`, also not waiting\n      # for either `a` or `b`.\n</pre>\n<blockquote class=\"note\">\n<strong>Note:</strong><span> The control dependencies context applies <em>only</em> to ops that are constructed within the context. Merely using an op or tensor in the context does not add a control dependency. The following example illustrates this point:</span>\n</blockquote>\n<pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\"># WRONG\ndef my_func(pred, tensor):\n  t = tf.matmul(tensor, tensor)\n  with tf.control_dependencies([pred]):\n    # The matmul op is created outside the context, so no control\n    # dependency will be added.\n    return t\n\n# RIGHT\ndef my_func(pred, tensor):\n  with tf.control_dependencies([pred]):\n    # The matmul op is created in the context, so a control dependency\n    # will be added.\n    return tf.matmul(tensor, tensor)\n</pre> <p>Also note that though execution of ops created under this scope will trigger execution of the dependencies, the ops created under this scope might still be pruned from a normal tensorflow graph. For example, in the following snippet of code the dependencies are never executed:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">loss = model.loss()\nwith tf.control_dependencies(dependencies):\n  loss = loss + tf.constant(1)  # note: dependencies ignored in the\n                                # backward pass\nreturn tf.gradients(loss, model.variables)\n</pre> <p>This is because evaluating the gradient graph does not require evaluating the constant(1) op created in the forward pass.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">control_inputs</code> </td> <td> A list of <code translate=\"no\" dir=\"ltr\">Operation</code> or <code translate=\"no\" dir=\"ltr\">Tensor</code> objects which must be executed or computed before running the operations defined in the context. Can also be <code translate=\"no\" dir=\"ltr\">None</code> to clear the control dependencies. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager that specifies control dependencies for all operations constructed within the context. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">control_inputs</code> is not a list of <code translate=\"no\" dir=\"ltr\">Operation</code> or <code translate=\"no\" dir=\"ltr\">Tensor</code> objects. </td> </tr> </table> <h3 id=\"create_op\" data-text=\"create_op\"><code translate=\"no\" dir=\"ltr\">create_op</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3411-L3467\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ncreate_op(\n    op_type, inputs, dtypes=None, input_types=None, name=None, attrs=None,\n    op_def=None, compute_shapes=True, compute_device=True\n)\n</pre> <p>Creates an <code translate=\"no\" dir=\"ltr\">Operation</code> in this graph. (deprecated arguments)</p> <aside class=\"warning\"><strong>Warning:</strong><span> SOME ARGUMENTS ARE DEPRECATED: <code translate=\"no\" dir=\"ltr\">(compute_shapes)</code>. They will be removed in a future version. Instructions for updating: Shapes are always computed; don't use the compute_shapes as it has no effect.</span></aside> <p>This is a low-level interface for creating an <code translate=\"no\" dir=\"ltr\">Operation</code>. Most programs will not call this method directly, and instead use the Python op constructors, such as <a href=\"constant\"><code translate=\"no\" dir=\"ltr\">tf.constant()</code></a>, which add ops to the default graph.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">op_type</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Operation</code> type to create. This corresponds to the <code translate=\"no\" dir=\"ltr\">OpDef.name</code> field for the proto that defines the operation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">inputs</code> </td> <td> A list of <code translate=\"no\" dir=\"ltr\">Tensor</code> objects that will be inputs to the <code translate=\"no\" dir=\"ltr\">Operation</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtypes</code> </td> <td> (Optional) A list of <code translate=\"no\" dir=\"ltr\">DType</code> objects that will be the types of the tensors that the operation produces. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">input_types</code> </td> <td> (Optional.) A list of <code translate=\"no\" dir=\"ltr\">DType</code>s that will be the types of the tensors that the operation consumes. By default, uses the base <code translate=\"no\" dir=\"ltr\">DType</code> of each input in <code translate=\"no\" dir=\"ltr\">inputs</code>. Operations that expect reference-typed inputs must specify <code translate=\"no\" dir=\"ltr\">input_types</code> explicitly. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> (Optional.) A string name for the operation. If not specified, a name is generated based on <code translate=\"no\" dir=\"ltr\">op_type</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">attrs</code> </td> <td> (Optional.) A dictionary where the key is the attribute name (a string) and the value is the respective <code translate=\"no\" dir=\"ltr\">attr</code> attribute of the <code translate=\"no\" dir=\"ltr\">NodeDef</code> proto that will represent the operation (an <code translate=\"no\" dir=\"ltr\">AttrValue</code> proto). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">op_def</code> </td> <td> (Optional.) The <code translate=\"no\" dir=\"ltr\">OpDef</code> proto that describes the <code translate=\"no\" dir=\"ltr\">op_type</code> that the operation will have. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">compute_shapes</code> </td> <td> (Optional.) Deprecated. Has no effect (shapes are always computed). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">compute_device</code> </td> <td> (Optional.) If True, device functions will be executed to compute the device property of the Operation. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> if any of the inputs is not a <code translate=\"no\" dir=\"ltr\">Tensor</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> if colocation conflicts with existing device assignment. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> An <code translate=\"no\" dir=\"ltr\">Operation</code> object. </td> </tr> \n</table> <h3 id=\"device\" data-text=\"device\"><code translate=\"no\" dir=\"ltr\">device</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4434-L4503\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\ndevice(\n    device_name_or_function\n)\n</pre> <p>Returns a context manager that specifies the default device to use.</p> <p>The <code translate=\"no\" dir=\"ltr\">device_name_or_function</code> argument may either be a device name string, a device function, or None:</p> <ul> <li>If it is a device name string, all operations constructed in this context will be assigned to the device with that name, unless overridden by a nested <code translate=\"no\" dir=\"ltr\">device()</code> context.</li> <li>If it is a function, it will be treated as a function from Operation objects to device name strings, and invoked each time a new Operation is created. The Operation will be assigned to the device with the returned name.</li> <li>If it is None, all <code translate=\"no\" dir=\"ltr\">device()</code> invocations from the enclosing context will be ignored.</li> </ul> <p>For information about the valid syntax of device name strings, see the documentation in <a href=\"https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h\"><code translate=\"no\" dir=\"ltr\">DeviceNameUtils</code></a>.</p> <h4 id=\"for_example_3\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with g.device('/device:GPU:0'):\n  # All operations constructed in this context will be placed\n  # on GPU 0.\n  with g.device(None):\n    # All operations constructed in this context will have no\n    # assigned device.\n\n# Defines a function from `Operation` to device string.\ndef matmul_on_gpu(n):\n  if n.type == \"MatMul\":\n    return \"/device:GPU:0\"\n  else:\n    return \"/cpu:0\"\n\nwith g.device(matmul_on_gpu):\n  # All operations of type \"MatMul\" constructed in this context\n  # will be placed on GPU 0; all other operations will be placed\n  # on CPU 0.\n</pre>\n<blockquote class=\"note\">\n<strong>Note:</strong><span> The device scope may be overridden by op wrappers or other library code. For example, a variable assignment op <code translate=\"no\" dir=\"ltr\">v.assign()</code> must be colocated with the <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a> <code translate=\"no\" dir=\"ltr\">v</code>, and incompatible device scopes will be ignored.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">device_name_or_function</code> </td> <td> The device name or function to use in the context. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Yields</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager that specifies the default device to use for newly created ops. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">RuntimeError</code> </td> <td> If device scopes are not properly nested. </td> </tr> </table> <h3 id=\"finalize\" data-text=\"finalize\"><code translate=\"no\" dir=\"ltr\">finalize</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3177-L3185\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nfinalize()\n</pre> <p>Finalizes this graph, making it read-only.</p> <p>After calling <code translate=\"no\" dir=\"ltr\">g.finalize()</code>, no new operations can be added to <code translate=\"no\" dir=\"ltr\">g</code>. This method is used to ensure that no operations are added to a graph when it is shared between multiple threads, for example when using a <a href=\"compat/v1/train/queuerunner\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.train.QueueRunner</code></a>.</p> <h3 id=\"get_all_collection_keys\" data-text=\"get_all_collection_keys\"><code translate=\"no\" dir=\"ltr\">get_all_collection_keys</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4093-L4096\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_all_collection_keys()\n</pre> <p>Returns a list of collections used in this graph.</p> <h3 id=\"get_collection\" data-text=\"get_collection\"><code translate=\"no\" dir=\"ltr\">get_collection</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4053-L4091\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_collection(\n    name, scope=None\n)\n</pre> <p>Returns a list of values in the collection with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p> <p>This is different from <code translate=\"no\" dir=\"ltr\">get_collection_ref()</code> which always returns the actual collection list if it exists in that it returns a new list each time it is called.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The key for the collection. For example, the <code translate=\"no\" dir=\"ltr\">GraphKeys</code> class contains many standard names for collections. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">scope</code> </td> <td> (Optional.) A string. If supplied, the resulting list is filtered to include only items whose <code translate=\"no\" dir=\"ltr\">name</code> attribute matches <code translate=\"no\" dir=\"ltr\">scope</code> using <code translate=\"no\" dir=\"ltr\">re.match</code>. Items without a <code translate=\"no\" dir=\"ltr\">name</code> attribute are never returned if a scope is supplied. The choice of <code translate=\"no\" dir=\"ltr\">re.match</code> means that a <code translate=\"no\" dir=\"ltr\">scope</code> without special tokens filters by prefix. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The list of values in the collection with the given <code translate=\"no\" dir=\"ltr\">name</code>, or an empty list if no value has been added to that collection. The list contains the values in the order under which they were collected. </td> </tr> \n</table> <h3 id=\"get_collection_ref\" data-text=\"get_collection_ref\"><code translate=\"no\" dir=\"ltr\">get_collection_ref</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4028-L4051\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_collection_ref(\n    name\n)\n</pre> <p>Returns a list of values in the collection with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p> <p>If the collection exists, this returns the list itself, which can be modified in place to change the collection. If the collection does not exist, it is created as an empty list and the list is returned.</p> <p>This is different from <code translate=\"no\" dir=\"ltr\">get_collection()</code> which always returns a copy of the collection list if it exists and never creates an empty collection.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The key for the collection. For example, the <code translate=\"no\" dir=\"ltr\">GraphKeys</code> class contains many standard names for collections. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The list of values in the collection with the given <code translate=\"no\" dir=\"ltr\">name</code>, or an empty list if no value has been added to that collection. </td> </tr> \n</table> <h3 id=\"get_name_scope\" data-text=\"get_name_scope\"><code translate=\"no\" dir=\"ltr\">get_name_scope</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4314-L4329\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_name_scope()\n</pre> <p>Returns the current name scope.</p> <h4 id=\"for_example_4\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with tf.name_scope('scope1'):\n  with tf.name_scope('scope2'):\n    print(tf.compat.v1.get_default_graph().get_name_scope())\n</pre> <p>would print the string <code translate=\"no\" dir=\"ltr\">scope1/scope2</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A string representing the current name scope. </td> </tr> \n</table> <h3 id=\"get_operation_by_name\" data-text=\"get_operation_by_name\"><code translate=\"no\" dir=\"ltr\">get_operation_by_name</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3835-L3854\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_operation_by_name(\n    name\n)\n</pre> <p>Returns the <code translate=\"no\" dir=\"ltr\">Operation</code> with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p> <p>This method may be called concurrently from multiple threads.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name of the <code translate=\"no\" dir=\"ltr\">Operation</code> to return. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The <code translate=\"no\" dir=\"ltr\">Operation</code> with the given <code translate=\"no\" dir=\"ltr\">name</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">name</code> is not a string. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">KeyError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">name</code> does not correspond to an operation in this graph. </td> </tr> </table> <h3 id=\"get_operations\" data-text=\"get_operations\"><code translate=\"no\" dir=\"ltr\">get_operations</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3817-L3833\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_operations()\n</pre> <p>Return the list of operations in the graph.</p> <p>You can modify the operations in place, but modifications to the list such as inserts/delete have no effect on the list of operations known to the graph.</p> <p>This method may be called concurrently from multiple threads.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A list of Operations. </td> </tr> \n</table> <h3 id=\"get_tensor_by_name\" data-text=\"get_tensor_by_name\"><code translate=\"no\" dir=\"ltr\">get_tensor_by_name</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L3883-L3902\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_tensor_by_name(\n    name\n)\n</pre> <p>Returns the <code translate=\"no\" dir=\"ltr\">Tensor</code> with the given <code translate=\"no\" dir=\"ltr\">name</code>.</p> <p>This method may be called concurrently from multiple threads.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name of the <code translate=\"no\" dir=\"ltr\">Tensor</code> to return. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> The <code translate=\"no\" dir=\"ltr\">Tensor</code> with the given <code translate=\"no\" dir=\"ltr\">name</code>. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">name</code> is not a string. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">KeyError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">name</code> does not correspond to a tensor in this graph. </td> </tr> </table> <h3 id=\"gradient_override_map\" data-text=\"gradient_override_map\"><code translate=\"no\" dir=\"ltr\">gradient_override_map</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4965-L5024\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\ngradient_override_map(\n    op_type_map\n)\n</pre> <p>EXPERIMENTAL: A context manager for overriding gradient functions.</p> <p>This context manager can be used to override the gradient function that will be used for ops within the scope of the context.</p> <h4 id=\"for_example_5\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">@tf.RegisterGradient(\"CustomSquare\")\ndef _custom_square_grad(op, grad):\n  # ...\n\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n                          # gradient of s_2.\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">op_type_map</code> </td> <td> A dictionary mapping op type strings to alternative op type strings. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager that sets the alternative op type to be used for one or more ops created in that context. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">TypeError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">op_type_map</code> is not a dictionary mapping strings to strings. </td> </tr> </table> <h3 id=\"is_feedable\" data-text=\"is_feedable\"><code translate=\"no\" dir=\"ltr\">is_feedable</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L5032-L5034\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_feedable(\n    tensor\n)\n</pre> <p>Returns <code translate=\"no\" dir=\"ltr\">True</code> if and only if <code translate=\"no\" dir=\"ltr\">tensor</code> is feedable.</p> <h3 id=\"is_fetchable\" data-text=\"is_fetchable\"><code translate=\"no\" dir=\"ltr\">is_fetchable</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L5040-L5045\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_fetchable(\n    tensor_or_op\n)\n</pre> <p>Returns <code translate=\"no\" dir=\"ltr\">True</code> if and only if <code translate=\"no\" dir=\"ltr\">tensor_or_op</code> is fetchable.</p> <h3 id=\"name_scope\" data-text=\"name_scope\"><code translate=\"no\" dir=\"ltr\">name_scope</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4147-L4258\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf_contextlib.contextmanager\nname_scope(\n    name\n)\n</pre> <p>Returns a context manager that creates hierarchical names for operations.</p> <p>A graph maintains a stack of name scopes. A <code translate=\"no\" dir=\"ltr\">with name_scope(...):</code> statement pushes a new name onto the stack for the lifetime of the context.</p> <p>The <code translate=\"no\" dir=\"ltr\">name</code> argument will be interpreted as follows:</p> <ul> <li>A string (not ending with '/') will create a new name scope, in which <code translate=\"no\" dir=\"ltr\">name</code> is appended to the prefix of all operations created in the context. If <code translate=\"no\" dir=\"ltr\">name</code> has been used before, it will be made unique by calling <code translate=\"no\" dir=\"ltr\">self.unique_name(name)</code>.</li> <li>A scope previously captured from a <code translate=\"no\" dir=\"ltr\">with g.name_scope(...) as scope:</code> statement will be treated as an \"absolute\" name scope, which makes it possible to re-enter existing scopes.</li> <li>A value of <code translate=\"no\" dir=\"ltr\">None</code> or the empty string will reset the current name scope to the top-level (empty) name scope.</li> </ul> <h4 id=\"for_example_6\" data-text=\"For example:\">For example:</h4> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">with tf.Graph().as_default() as g:\n  c = tf.constant(5.0, name=\"c\")\n  assert c.op.name == \"c\"\n  c_1 = tf.constant(6.0, name=\"c\")\n  assert c_1.op.name == \"c_1\"\n\n  # Creates a scope called \"nested\"\n  with g.name_scope(\"nested\") as scope:\n    nested_c = tf.constant(10.0, name=\"c\")\n    assert nested_c.op.name == \"nested/c\"\n\n    # Creates a nested scope called \"inner\".\n    with g.name_scope(\"inner\"):\n      nested_inner_c = tf.constant(20.0, name=\"c\")\n      assert nested_inner_c.op.name == \"nested/inner/c\"\n\n    # Create a nested scope called \"inner_1\".\n    with g.name_scope(\"inner\"):\n      nested_inner_1_c = tf.constant(30.0, name=\"c\")\n      assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n\n      # Treats `scope` as an absolute name scope, and\n      # switches to the \"nested/\" scope.\n      with g.name_scope(scope):\n        nested_d = tf.constant(40.0, name=\"d\")\n        assert nested_d.op.name == \"nested/d\"\n\n        with g.name_scope(\"\"):\n          e = tf.constant(50.0, name=\"e\")\n          assert e.op.name == \"e\"\n</pre> <p>The name of the scope itself can be captured by <code translate=\"no\" dir=\"ltr\">with g.name_scope(...) as scope:</code>, which stores the name of the scope in the variable <code translate=\"no\" dir=\"ltr\">scope</code>. This value can be used to name an operation that represents the overall result of executing the ops in a scope. For example:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">inputs = tf.constant(...)\nwith g.name_scope('my_layer') as scope:\n  weights = tf.Variable(..., name=\"weights\")\n  biases = tf.Variable(..., name=\"biases\")\n  affine = tf.matmul(inputs, weights) + biases\n  output = tf.nn.relu(affine, name=scope)\n</pre>\n<blockquote class=\"note\">\n<strong>Note:</strong><span> This constructor validates the given <code translate=\"no\" dir=\"ltr\">name</code>. Valid scope names match one of the following regular expressions:</span>\n</blockquote>\n<pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">[A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)\n[A-Za-z0-9_.\\-/]* (for other scopes)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the scope. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A context manager that installs <code translate=\"no\" dir=\"ltr\">name</code> as a new name scope. </td> </tr> \n</table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If <code translate=\"no\" dir=\"ltr\">name</code> is not a valid scope name, according to the rules above. </td> </tr> </table> <h3 id=\"prevent_feeding\" data-text=\"prevent_feeding\"><code translate=\"no\" dir=\"ltr\">prevent_feeding</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L5028-L5030\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprevent_feeding(\n    tensor\n)\n</pre> <p>Marks the given <code translate=\"no\" dir=\"ltr\">tensor</code> as unfeedable in this graph.</p> <h3 id=\"prevent_fetching\" data-text=\"prevent_fetching\"><code translate=\"no\" dir=\"ltr\">prevent_fetching</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L5036-L5038\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nprevent_fetching(\n    op\n)\n</pre> <p>Marks the given <code translate=\"no\" dir=\"ltr\">op</code> as unfetchable in this graph.</p> <h3 id=\"switch_to_thread_local\" data-text=\"switch_to_thread_local\"><code translate=\"no\" dir=\"ltr\">switch_to_thread_local</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L5047-L5062\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nswitch_to_thread_local()\n</pre> <p>Make device, colocation and dependencies stacks thread-local.</p> <p>Device, colocation and dependencies stacks are not thread-local be default. If multiple threads access them, then the state is shared. This means that one thread may affect the behavior of another thread.</p> <p>After this method is called, the stacks become thread-local. If multiple threads access them, then the state is not shared. Each thread uses its own value; a thread doesn't affect other threads by mutating such a stack.</p> <p>The initial value for every thread's stack is set to the current value of the stack when <code translate=\"no\" dir=\"ltr\">switch_to_thread_local()</code> was first called.</p> <h3 id=\"unique_name\" data-text=\"unique_name\"><code translate=\"no\" dir=\"ltr\">unique_name</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/ops.py#L4262-L4312\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nunique_name(\n    name, mark_as_used=True\n)\n</pre> <p>Return a unique operation name for <code translate=\"no\" dir=\"ltr\">name</code>.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> You rarely need to call <code translate=\"no\" dir=\"ltr\">unique_name()</code> directly. Most of the time you just need to create <code translate=\"no\" dir=\"ltr\">with g.name_scope()</code> blocks to generate structured names.</span>\n</blockquote> <p><code translate=\"no\" dir=\"ltr\">unique_name</code> is used to generate structured names, separated by <code translate=\"no\" dir=\"ltr\">\"/\"</code>, to help identify operations when debugging a graph. Operation names are displayed in error messages reported by the TensorFlow runtime, and in various visualization tools such as TensorBoard.</p> <p>If <code translate=\"no\" dir=\"ltr\">mark_as_used</code> is set to <code translate=\"no\" dir=\"ltr\">True</code>, which is the default, a new unique name is created and marked as in use. If it's set to <code translate=\"no\" dir=\"ltr\">False</code>, the unique name is returned without actually being marked as used. This is useful when the caller simply wants to know what the name to be created will be.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> The name for an operation. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">mark_as_used</code> </td> <td> Whether to mark this name as being used. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A string to be passed to <code translate=\"no\" dir=\"ltr\">create_op()</code> that will be used to name the operation being created. </td> </tr> \n</table>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Graph\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/Graph</a>\n  </p>\n</div>\n","indexedslicesspec":"<h1 class=\"devsite-page-title\">tf.IndexedSlicesSpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/indexed_slices.py#L193-L256\">  View source on GitHub </a> </td> </table> <p>Type specification for a <a href=\"indexedslices\"><code translate=\"no\" dir=\"ltr\">tf.IndexedSlices</code></a>.</p> <p>Inherits From: <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">TypeSpec</code></a></p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/IndexedSlicesSpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.IndexedSlicesSpec</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.IndexedSlicesSpec(\n    shape=None, dtype=tf.dtypes.float32, indices_dtype=tf.dtypes.int64,\n    dense_shape_dtype=None, indices_shape=None\n)\n</pre>   \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The dense shape of the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>, or <code translate=\"no\" dir=\"ltr\">None</code> to allow any dense shape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> of values in the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">indices_dtype</code> </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> of the <code translate=\"no\" dir=\"ltr\">indices</code> in the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> or <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense_shape_dtype</code> </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> of the <code translate=\"no\" dir=\"ltr\">dense_shape</code> in the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a>, <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>, or <code translate=\"no\" dir=\"ltr\">None</code> (if the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code> has no <code translate=\"no\" dir=\"ltr\">dense_shape</code> tensor). </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">indices_shape</code> </td> <td> The shape of the <code translate=\"no\" dir=\"ltr\">indices</code> component, which indicates how many slices are in the <code translate=\"no\" dir=\"ltr\">IndexedSlices</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_type</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L93-L108\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    spec_or_value\n)\n</pre> <p>Returns true if <code translate=\"no\" dir=\"ltr\">spec_or_value</code> is compatible with this TypeSpec.</p> <h3 id=\"most_specific_compatible_type\" data-text=\"most_specific_compatible_type\"><code translate=\"no\" dir=\"ltr\">most_specific_compatible_type</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L110-L132\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmost_specific_compatible_type(\n    other\n)\n</pre> <p>Returns the most specific TypeSpec compatible with <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">TypeSpec</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If there is no TypeSpec that is compatible with both <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>. </td> </tr> </table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L293-L296\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Return self==value.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L298-L299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Return self!=value.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/IndexedSlicesSpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/IndexedSlicesSpec</a>\n  </p>\n</div>\n","optionalspec":"<h1 class=\"devsite-page-title\">tf.OptionalSpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L207-L263\">  View source on GitHub </a> </td> </table> <p>Type specification for <a href=\"experimental/optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a>.</p> <p>Inherits From: <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">TypeSpec</code></a></p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/OptionalSpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.OptionalSpec</code></a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/OptionalSpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.data.experimental.OptionalStructure</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.OptionalSpec(\n    element_spec\n)\n</pre>  <p>For instance, <a href=\"optionalspec\"><code translate=\"no\" dir=\"ltr\">tf.OptionalSpec</code></a> can be used to define a tf.function that takes <a href=\"experimental/optional\"><code translate=\"no\" dir=\"ltr\">tf.experimental.Optional</code></a> as an input argument:</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf.function(input_signature=[tf.OptionalSpec(\n  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])\ndef maybe_square(optional):\n  if optional.has_value():\n    x = optional.get_value()\n    return x * x\n  return -1\noptional = tf.experimental.Optional.from_value(5)\nprint(maybe_square(optional))\ntf.Tensor(25, shape=(), dtype=int32)\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_spec</code> </td> <td> A nested structure of <code translate=\"no\" dir=\"ltr\">TypeSpec</code> objects that represents the type specification of the optional element. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_type</code> </td> <td> The Python type for values that are compatible with this TypeSpec. <p>In particular, all values that are compatible with this TypeSpec must be an instance of this type. </p>\n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/ops/optional_ops.py#L252-L254\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_value(\n    value\n)\n</pre> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L93-L108\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    spec_or_value\n)\n</pre> <p>Returns true if <code translate=\"no\" dir=\"ltr\">spec_or_value</code> is compatible with this TypeSpec.</p> <h3 id=\"most_specific_compatible_type\" data-text=\"most_specific_compatible_type\"><code translate=\"no\" dir=\"ltr\">most_specific_compatible_type</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L110-L132\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmost_specific_compatible_type(\n    other\n)\n</pre> <p>Returns the most specific TypeSpec compatible with <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">TypeSpec</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If there is no TypeSpec that is compatible with both <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>. </td> </tr> </table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L293-L296\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Return self==value.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L298-L299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Return self!=value.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/OptionalSpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/OptionalSpec</a>\n  </p>\n</div>\n","tensorarrayspec":"<h1 class=\"devsite-page-title\">tf.TensorArraySpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/tensor_array_ops.py#L1319-L1417\">  View source on GitHub </a> </td> </table> <p>Type specification for a <a href=\"tensorarray\"><code translate=\"no\" dir=\"ltr\">tf.TensorArray</code></a>.</p> <p>Inherits From: <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">TypeSpec</code></a></p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/TensorArraySpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.TensorArraySpec</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.TensorArraySpec(\n    element_shape=None, dtype=tf.dtypes.float32, dynamic_size=False,\n    infer_shape=True\n)\n</pre>   \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_shape</code> </td> <td> The shape of each element in the <code translate=\"no\" dir=\"ltr\">TensorArray</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> Data type of the <code translate=\"no\" dir=\"ltr\">TensorArray</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dynamic_size</code> </td> <td> Whether the <code translate=\"no\" dir=\"ltr\">TensorArray</code> can grow past its initial size. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">infer_shape</code> </td> <td> Whether shape inference is enabled. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_type</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/tensor_array_ops.py#L1396-L1406\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@staticmethod\nfrom_value(\n    value\n)\n</pre> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/tensor_array_ops.py#L1341-L1350\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    other\n)\n</pre> <p>Returns true if <code translate=\"no\" dir=\"ltr\">spec_or_value</code> is compatible with this TypeSpec.</p> <h3 id=\"most_specific_compatible_type\" data-text=\"most_specific_compatible_type\"><code translate=\"no\" dir=\"ltr\">most_specific_compatible_type</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/tensor_array_ops.py#L1352-L1360\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmost_specific_compatible_type(\n    other\n)\n</pre> <p>Returns the most specific TypeSpec compatible with <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">TypeSpec</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If there is no TypeSpec that is compatible with both <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>. </td> </tr> </table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L293-L296\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Return self==value.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L298-L299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Return self!=value.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/TensorArraySpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/TensorArraySpec</a>\n  </p>\n</div>\n","raggedtensorspec":"<h1 class=\"devsite-page-title\">tf.RaggedTensorSpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L2179-L2492\">  View source on GitHub </a> </td> </table> <p>Type specification for a <a href=\"raggedtensor\"><code translate=\"no\" dir=\"ltr\">tf.RaggedTensor</code></a>.</p> <p>Inherits From: <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">TypeSpec</code></a></p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/RaggedTensorSpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.RaggedTensorSpec</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.RaggedTensorSpec(\n    shape=None, dtype=tf.dtypes.float32, ragged_rank=None,\n    row_splits_dtype=tf.dtypes.int64, flat_values_spec=None\n)\n</pre>   \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The shape of the RaggedTensor, or <code translate=\"no\" dir=\"ltr\">None</code> to allow any shape. If a shape is specified, then all ragged dimensions must have size <code translate=\"no\" dir=\"ltr\">None</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> of values in the RaggedTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ragged_rank</code> </td> <td> Python integer, the number of times the RaggedTensor's flat_values is partitioned. Defaults to <code translate=\"no\" dir=\"ltr\">shape.ndims - 1</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits_dtype</code> </td> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> for the RaggedTensor's <code translate=\"no\" dir=\"ltr\">row_splits</code> tensor. One of <a href=\"../tf#int32\"><code translate=\"no\" dir=\"ltr\">tf.int32</code></a> or <a href=\"../tf#int64\"><code translate=\"no\" dir=\"ltr\">tf.int64</code></a>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values_spec</code> </td> <td> TypeSpec for flat_value of the RaggedTensor. It shall be provided when the flat_values is a CompositeTensor rather then Tensor. If both <code translate=\"no\" dir=\"ltr\">dtype</code> and <code translate=\"no\" dir=\"ltr\">flat_values_spec</code> and are provided, <code translate=\"no\" dir=\"ltr\">dtype</code> must be the same as <code translate=\"no\" dir=\"ltr\">flat_values_spec.dtype</code>. (experimental) </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> specified by this type for the RaggedTensor. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[\"a\"], [\"b\", \"c\"]], dtype=tf.string)\ntf.type_spec_from_value(rt).dtype\ntf.string\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">flat_values_spec</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">TypeSpec</code> of the flat_values of RaggedTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">ragged_rank</code> </td> <td> The number of times the RaggedTensor's flat_values is partitioned. <p>Defaults to <code translate=\"no\" dir=\"ltr\">shape.ndims - 1</code>.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nvalues = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])\ntf.type_spec_from_value(values).ragged_rank\n1\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt1 = tf.RaggedTensor.from_uniform_row_length(values, 2)\ntf.type_spec_from_value(rt1).ragged_rank\n2\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">row_splits_dtype</code> </td> <td> The <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> of the RaggedTensor's <code translate=\"no\" dir=\"ltr\">row_splits</code>. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[1, 2, 3], [4]], row_splits_dtype=tf.int64)\ntf.type_spec_from_value(rt).row_splits_dtype\ntf.int64\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The statically known shape of the RaggedTensor. <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[0], [1, 2]])\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None])\n</pre> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nrt = tf.ragged.constant([[[0, 1]], [[1, 2], [3, 4]]], ragged_rank=1)\ntf.type_spec_from_value(rt).shape\nTensorShape([2, None, 2])\n</pre> \n</td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_type</code> </td> <td> The Python type for values that are compatible with this TypeSpec. <p>In particular, all values that are compatible with this TypeSpec must be an instance of this type. </p>\n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L2477-L2492\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_value(\n    value\n)\n</pre> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/ragged/ragged_tensor.py#L2323-L2332\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    spec_or_value\n)\n</pre> <p>Returns true if <code translate=\"no\" dir=\"ltr\">spec_or_value</code> is compatible with this TypeSpec.</p> <h3 id=\"most_specific_compatible_type\" data-text=\"most_specific_compatible_type\"><code translate=\"no\" dir=\"ltr\">most_specific_compatible_type</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L110-L132\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmost_specific_compatible_type(\n    other\n)\n</pre> <p>Returns the most specific TypeSpec compatible with <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">TypeSpec</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If there is no TypeSpec that is compatible with both <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>. </td> </tr> </table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L293-L296\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Return self==value.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L298-L299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Return self!=value.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/RaggedTensorSpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/RaggedTensorSpec</a>\n  </p>\n</div>\n","sparse/sparsetensor":"<h1 class=\"devsite-page-title\">tf.sparse.SparseTensor</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L47-L285\">  View source on GitHub </a> </td> </table> <p>Represents a sparse tensor.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Main aliases</b> </p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor\"><code translate=\"no\" dir=\"ltr\">tf.SparseTensor</code></a></p> <b>Compat aliases for migration</b> <p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.SparseTensor</code></a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.sparse.SparseTensor</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n</pre>  <p>TensorFlow represents a sparse tensor as three separate dense tensors: <code translate=\"no\" dir=\"ltr\">indices</code>, <code translate=\"no\" dir=\"ltr\">values</code>, and <code translate=\"no\" dir=\"ltr\">dense_shape</code>. In Python, the three tensors are collected into a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> class for ease of use. If you have separate <code translate=\"no\" dir=\"ltr\">indices</code>, <code translate=\"no\" dir=\"ltr\">values</code>, and <code translate=\"no\" dir=\"ltr\">dense_shape</code> tensors, wrap them in a <code translate=\"no\" dir=\"ltr\">SparseTensor</code> object before passing to the ops below.</p> <p>Concretely, the sparse tensor <code translate=\"no\" dir=\"ltr\">SparseTensor(indices, values, dense_shape)</code> comprises the following components, where <code translate=\"no\" dir=\"ltr\">N</code> and <code translate=\"no\" dir=\"ltr\">ndims</code> are the number of values and number of dimensions in the <code translate=\"no\" dir=\"ltr\">SparseTensor</code>, respectively:</p> <ul> <li><p><code translate=\"no\" dir=\"ltr\">indices</code>: A 2-D int64 tensor of shape <code translate=\"no\" dir=\"ltr\">[N, ndims]</code>, which specifies the indices of the elements in the sparse tensor that contain nonzero values (elements are zero-indexed). For example, <code translate=\"no\" dir=\"ltr\">indices=[[1,3], [2,4]]</code> specifies that the elements with indexes of [1,3] and [2,4] have nonzero values.</p></li> <li><p><code translate=\"no\" dir=\"ltr\">values</code>: A 1-D tensor of any type and shape <code translate=\"no\" dir=\"ltr\">[N]</code>, which supplies the values for each element in <code translate=\"no\" dir=\"ltr\">indices</code>. For example, given <code translate=\"no\" dir=\"ltr\">indices=[[1,3], [2,4]]</code>, the parameter <code translate=\"no\" dir=\"ltr\">values=[18, 3.6]</code> specifies that element [1,3] of the sparse tensor has a value of 18, and element [2,4] of the tensor has a value of 3.6.</p></li> <li><p><code translate=\"no\" dir=\"ltr\">dense_shape</code>: A 1-D int64 tensor of shape <code translate=\"no\" dir=\"ltr\">[ndims]</code>, which specifies the dense_shape of the sparse tensor. Takes a list indicating the number of elements in each dimension. For example, <code translate=\"no\" dir=\"ltr\">dense_shape=[3,6]</code> specifies a two-dimensional 3x6 tensor, <code translate=\"no\" dir=\"ltr\">dense_shape=[2,3,4]</code> specifies a three-dimensional 2x3x4 tensor, and <code translate=\"no\" dir=\"ltr\">dense_shape=[9]</code> specifies a one-dimensional tensor with 9 elements.</p></li> </ul> <p>The corresponding dense tensor satisfies:</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">dense.shape = dense_shape\ndense[tuple(indices[i])] = values[i]\n</pre> <p>By convention, <code translate=\"no\" dir=\"ltr\">indices</code> should be sorted in row-major order (or equivalently lexicographic order on the tuples <code translate=\"no\" dir=\"ltr\">indices[i]</code>). This is not enforced when <code translate=\"no\" dir=\"ltr\">SparseTensor</code> objects are constructed, but most ops assume correct ordering. If the ordering of sparse tensor <code translate=\"no\" dir=\"ltr\">st</code> is wrong, a fixed version can be obtained by calling <a href=\"reorder\"><code translate=\"no\" dir=\"ltr\">tf.sparse.reorder(st)</code></a>.</p> <p>Example: The sparse tensor</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n</pre> <p>represents the dense tensor</p> <pre class=\"prettyprint lang-python\" translate=\"no\" dir=\"ltr\" data-language=\"python\">[[1, 0, 0, 0]\n [0, 0, 2, 0]\n [0, 0, 0, 0]]\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">indices</code> </td> <td> A 2-D int64 tensor of shape <code translate=\"no\" dir=\"ltr\">[N, ndims]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> A 1-D tensor of any type and shape <code translate=\"no\" dir=\"ltr\">[N]</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense_shape</code> </td> <td> A 1-D int64 tensor of shape <code translate=\"no\" dir=\"ltr\">[ndims]</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> When building an eager SparseTensor if <code translate=\"no\" dir=\"ltr\">dense_shape</code> is unknown or contains unknown elements (None or -1). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense_shape</code> </td> <td> A 1-D Tensor of int64 representing the shape of the dense tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">DType</code> of elements in this tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">graph</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Graph</code> that contains the index, value, and dense_shape tensors. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">indices</code> </td> <td> The indices of non-zero values in the represented dense tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">op</code> </td> <td> The <code translate=\"no\" dir=\"ltr\">Operation</code> that produces <code translate=\"no\" dir=\"ltr\">values</code> as an output. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> Get the <code translate=\"no\" dir=\"ltr\">TensorShape</code> representing the shape of the dense tensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">values</code> </td> <td> The non-zero values in the represented dense tensor. </td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"consumers\" data-text=\"consumers\"><code translate=\"no\" dir=\"ltr\">consumers</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L284-L285\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nconsumers()\n</pre> <h3 id=\"eval\" data-text=\"eval\"><code translate=\"no\" dir=\"ltr\">eval</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L239-L262\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\neval(\n    feed_dict=None, session=None\n)\n</pre> <p>Evaluates this sparse tensor in a <code translate=\"no\" dir=\"ltr\">Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.</p> <blockquote class=\"note\">\n<strong>Note:</strong><span> Before invoking <a href=\"sparsetensor#eval\"><code translate=\"no\" dir=\"ltr\">SparseTensor.eval()</code></a>, its graph must have been launched in a session, and either a default session must be available, or <code translate=\"no\" dir=\"ltr\">session</code> must be specified explicitly.</span>\n</blockquote>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">feed_dict</code> </td> <td> A dictionary that maps <code translate=\"no\" dir=\"ltr\">Tensor</code> objects to feed values. See <code translate=\"no\" dir=\"ltr\">tf.Session.run</code> for a description of the valid feed values. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">session</code> </td> <td> (Optional.) The <code translate=\"no\" dir=\"ltr\">Session</code> to be used to evaluate this sparse tensor. If none, the default session will be used. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">SparseTensorValue</code> object. </td> </tr> \n</table> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L106-L114\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_value(\n    sparse_tensor_value\n)\n</pre> <h3 id=\"get_shape\" data-text=\"get_shape\"><code translate=\"no\" dir=\"ltr\">get_shape</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L154-L160\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nget_shape()\n</pre> <p>Get the <code translate=\"no\" dir=\"ltr\">TensorShape</code> representing the shape of the dense tensor.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">TensorShape</code> object. </td> </tr> \n</table> <h3 id=\"with_values\" data-text=\"with_values\"><code translate=\"no\" dir=\"ltr\">with_values</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L181-L204\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nwith_values(\n    new_values\n)\n</pre> <p>Returns a copy of <code translate=\"no\" dir=\"ltr\">self</code> with <code translate=\"no\" dir=\"ltr\">values</code> replaced by <code translate=\"no\" dir=\"ltr\">new_values</code>.</p> <p>This method produces a new <code translate=\"no\" dir=\"ltr\">SparseTensor</code> that has the same nonzero <code translate=\"no\" dir=\"ltr\">indices</code> and same <code translate=\"no\" dir=\"ltr\">dense_shape</code>, but updated values.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">new_values</code> </td> <td> The values of the new <code translate=\"no\" dir=\"ltr\">SparseTensor</code>. Needs to have the same shape as the current <code translate=\"no\" dir=\"ltr\">.values</code> <code translate=\"no\" dir=\"ltr\">Tensor</code>. May have a different type than the current <code translate=\"no\" dir=\"ltr\">values</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">SparseTensor</code> with identical indices and shape but updated values. </td> </tr> \n</table> <h4 id=\"example_usage\" data-text=\"Example usage:\">Example usage:</h4> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nst = tf.sparse.from_dense([[1, 0, 2, 0], [3, 0, 0, 4]])\ntf.sparse.to_dense(st.with_values([10, 20, 30, 40]))  # 4 nonzero values\n&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[10,  0, 20,  0],\n       [30,  0,  0, 40]], dtype=int32)&gt;\n</pre> <h3 id=\"__div__\" data-text=\"__div__\"><code translate=\"no\" dir=\"ltr\">__div__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1184-L1190\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__div__(\n    sp_x, y\n)\n</pre> <p>Component-wise divides a SparseTensor by a dense Tensor.</p> <p><em>Limitation</em>: this Op only broadcasts the dense side to the sparse side, but not the other direction.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_indices</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">int64</code>. 2-D. <code translate=\"no\" dir=\"ltr\">N x R</code> matrix with the indices of non-empty values in a SparseTensor, possibly not in canonical ordering. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_values</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">qint8</code>, <code translate=\"no\" dir=\"ltr\">quint8</code>, <code translate=\"no\" dir=\"ltr\">qint32</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. 1-D. <code translate=\"no\" dir=\"ltr\">N</code> non-empty values corresponding to <code translate=\"no\" dir=\"ltr\">sp_indices</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_shape</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">int64</code>. 1-D. Shape of the input SparseTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">sp_values</code>. <code translate=\"no\" dir=\"ltr\">R</code>-D. The dense Tensor operand. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">sp_values</code>. </td> </tr> \n</table> <h3 id=\"__mul__\" data-text=\"__mul__\"><code translate=\"no\" dir=\"ltr\">__mul__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1184-L1190\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__mul__(\n    sp_x, y\n)\n</pre> <p>Component-wise multiplies a SparseTensor by a dense Tensor.</p> <p>The output locations corresponding to the implicitly zero elements in the sparse tensor will be zero (i.e., will not take up storage space), regardless of the contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).</p> <p><em>Limitation</em>: this Op only broadcasts the dense side to the sparse side, but not the other direction.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_indices</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">int64</code>. 2-D. <code translate=\"no\" dir=\"ltr\">N x R</code> matrix with the indices of non-empty values in a SparseTensor, possibly not in canonical ordering. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_values</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must be one of the following types: <code translate=\"no\" dir=\"ltr\">float32</code>, <code translate=\"no\" dir=\"ltr\">float64</code>, <code translate=\"no\" dir=\"ltr\">int32</code>, <code translate=\"no\" dir=\"ltr\">uint8</code>, <code translate=\"no\" dir=\"ltr\">int16</code>, <code translate=\"no\" dir=\"ltr\">int8</code>, <code translate=\"no\" dir=\"ltr\">complex64</code>, <code translate=\"no\" dir=\"ltr\">int64</code>, <code translate=\"no\" dir=\"ltr\">qint8</code>, <code translate=\"no\" dir=\"ltr\">quint8</code>, <code translate=\"no\" dir=\"ltr\">qint32</code>, <code translate=\"no\" dir=\"ltr\">bfloat16</code>, <code translate=\"no\" dir=\"ltr\">uint16</code>, <code translate=\"no\" dir=\"ltr\">complex128</code>, <code translate=\"no\" dir=\"ltr\">half</code>, <code translate=\"no\" dir=\"ltr\">uint32</code>, <code translate=\"no\" dir=\"ltr\">uint64</code>. 1-D. <code translate=\"no\" dir=\"ltr\">N</code> non-empty values corresponding to <code translate=\"no\" dir=\"ltr\">sp_indices</code>. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">sp_shape</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code> of type <code translate=\"no\" dir=\"ltr\">int64</code>. 1-D. Shape of the input SparseTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dense</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Must have the same type as <code translate=\"no\" dir=\"ltr\">sp_values</code>. <code translate=\"no\" dir=\"ltr\">R</code>-D. The dense Tensor operand. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Returns</th></tr> <tr class=\"alt\"> <td colspan=\"2\"> A <code translate=\"no\" dir=\"ltr\">Tensor</code>. Has the same type as <code translate=\"no\" dir=\"ltr\">sp_values</code>. </td> </tr> \n</table> <h3 id=\"__truediv__\" data-text=\"__truediv__\"><code translate=\"no\" dir=\"ltr\">__truediv__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/math_ops.py#L1184-L1190\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__truediv__(\n    sp_x, y\n)\n</pre> <p>Internal helper function for 'sp_t / dense_t'.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sparse/SparseTensor\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/sparse/SparseTensor</a>\n  </p>\n</div>\n","sparsetensorspec":"<h1 class=\"devsite-page-title\">tf.SparseTensorSpec</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L295-L427\">  View source on GitHub </a> </td> </table> <p>Type specification for a <a href=\"sparse/sparsetensor\"><code translate=\"no\" dir=\"ltr\">tf.sparse.SparseTensor</code></a>.</p> <p>Inherits From: <a href=\"typespec\"><code translate=\"no\" dir=\"ltr\">TypeSpec</code></a></p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/SparseTensorSpec\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.SparseTensorSpec</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.SparseTensorSpec(\n    shape=None, dtype=tf.dtypes.float32\n)\n</pre>   \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The dense shape of the <code translate=\"no\" dir=\"ltr\">SparseTensor</code>, or <code translate=\"no\" dir=\"ltr\">None</code> to allow any dense shape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.DType</code></a> of values in the <code translate=\"no\" dir=\"ltr\">SparseTensor</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Attributes</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> The <a href=\"dtypes/dtype\"><code translate=\"no\" dir=\"ltr\">tf.dtypes.DType</code></a> specified by this type for the SparseTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">shape</code> </td> <td> The <a href=\"tensorshape\"><code translate=\"no\" dir=\"ltr\">tf.TensorShape</code></a> specified by this type for the SparseTensor. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">value_type</code> </td> <td> \n</td> </tr> </table> <h2 id=\"methods\" data-text=\"Methods\">Methods</h2> <h3 id=\"from_value\" data-text=\"from_value\"><code translate=\"no\" dir=\"ltr\">from_value</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/sparse_tensor.py#L417-L427\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@classmethod\nfrom_value(\n    value\n)\n</pre> <h3 id=\"is_compatible_with\" data-text=\"is_compatible_with\"><code translate=\"no\" dir=\"ltr\">is_compatible_with</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L93-L108\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nis_compatible_with(\n    spec_or_value\n)\n</pre> <p>Returns true if <code translate=\"no\" dir=\"ltr\">spec_or_value</code> is compatible with this TypeSpec.</p> <h3 id=\"most_specific_compatible_type\" data-text=\"most_specific_compatible_type\"><code translate=\"no\" dir=\"ltr\">most_specific_compatible_type</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L110-L132\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nmost_specific_compatible_type(\n    other\n)\n</pre> <p>Returns the most specific TypeSpec compatible with <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>.</p>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">other</code> </td> <td> A <code translate=\"no\" dir=\"ltr\">TypeSpec</code>. </td> </tr> </table>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Raises</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">ValueError</code> </td> <td> If there is no TypeSpec that is compatible with both <code translate=\"no\" dir=\"ltr\">self</code> and <code translate=\"no\" dir=\"ltr\">other</code>. </td> </tr> </table> <h3 id=\"__eq__\" data-text=\"__eq__\"><code translate=\"no\" dir=\"ltr\">__eq__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L293-L296\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__eq__(\n    other\n)\n</pre> <p>Return self==value.</p> <h3 id=\"__ne__\" data-text=\"__ne__\"><code translate=\"no\" dir=\"ltr\">__ne__</code></h3> <p><a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/framework/type_spec.py#L298-L299\">View source</a></p> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n__ne__(\n    other\n)\n</pre> <p>Return self!=value.</p>  <devsite-page-rating position=\"footer\" selected-rating=\"0\" hover-rating-star=\"0\"> </devsite-page-rating><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>\n    <a href=\"https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/SparseTensorSpec\" class=\"_attribution-link\">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/SparseTensorSpec</a>\n  </p>\n</div>\n","tensorarray":"<h1 class=\"devsite-page-title\">tf.TensorArray</h1>      <table class=\"tfo-notebook-buttons tfo-api nocontent\" align=\"left\">  <td> <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/tensor_array_ops.py#L947-L1271\">  View source on GitHub </a> </td> </table> <p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <section class=\"expandable\"> <h4 class=\"showalways\" id=\"view-aliases\" data-text=\"View aliases\">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>\n<p>See <a href=\"https://www.tensorflow.org/guide/migrate\">Migration guide</a> for more details.</p> <p><a href=\"https://www.tensorflow.org/api_docs/python/tf/TensorArray\"><code translate=\"no\" dir=\"ltr\">tf.compat.v1.TensorArray</code></a></p> </section> <pre class=\"devsite-click-to-copy prettyprint lang-py tfo-signature-link\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\ntf.TensorArray(\n    dtype, size=None, dynamic_size=None, clear_after_read=None,\n    tensor_array_name=None, handle=None, flow=None, infer_shape=True,\n    element_shape=None, colocate_with_first_write_call=True, name=None\n)\n</pre>  <p>This class is meant to be used with dynamic iteration primitives such as <code translate=\"no\" dir=\"ltr\">while_loop</code> and <code translate=\"no\" dir=\"ltr\">map_fn</code>. It supports gradient back-propagation via special \"flow\" control flow dependencies.</p> <p>Example 1: Plain reading and writing.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\nta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)\nta = ta.write(0, 10)\nta = ta.write(1, 20)\nta = ta.write(2, 30)\n\nta.read(0)\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=10.0&gt;\nta.read(1)\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=20.0&gt;\nta.read(2)\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=30.0&gt;\nta.stack()\n&lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],\ndtype=float32)&gt;\n</pre> <p>Example 2: Fibonacci sequence algorithm that writes in a loop then returns.</p> <pre class=\"devsite-click-to-copy prettyprint lang-py\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">\n@tf.function\ndef fibonacci(n):\n  ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True)\n  ta = ta.unstack([0., 1.])\n\n  for i in range(2, n):\n    ta = ta.write(i, ta.read(i - 1) + ta.read(i - 2))\n\n  return ta.stack()\n\nfibonacci(7)\n&lt;tf.Tensor: shape=(7,), dtype=float32,\nnumpy=array([0., 1., 1., 2., 3., 5., 8.], dtype=float32)&gt;\n</pre> <p>Example 3: A simple loop interacting with a <a href=\"variable\"><code translate=\"no\" dir=\"ltr\">tf.Variable</code></a>.</p> <pre class=\"prettyprint\" translate=\"no\" dir=\"ltr\" data-language=\"cpp\">v = tf.Variable(1)\n@tf.function\ndef f(x):\n  ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True)\n  for i in tf.range(x):\n    v.assign_add(i)\n    ta = ta.write(i, v)\n  return ta.stack()\nf(5)\n&lt;tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 1,  2,  4,  7, 11],\ndtype=int32)&gt;\n</pre>  \n<table class=\"responsive fixed orange\"> <colgroup>\n<col width=\"214px\">\n<col>\n</colgroup> <tr><th colspan=\"2\">Args</th></tr> \n<tr> <td> <code translate=\"no\" dir=\"ltr\">dtype</code> </td> <td> (required) data type of the TensorArray. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">size</code> </td> <td> (optional) int32 scalar <code translate=\"no\" dir=\"ltr\">Tensor</code>: the size of the TensorArray. Required if handle is not provided. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">dynamic_size</code> </td> <td> (optional) Python bool: If true, writes to the TensorArray can grow the TensorArray past its initial size. Default: False. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">clear_after_read</code> </td> <td> Boolean (optional, default: True). If True, clear TensorArray values after reading them. This disables read-many semantics, but allows early release of memory. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">tensor_array_name</code> </td> <td> (optional) Python string: the name of the TensorArray. This is used when creating the TensorArray handle. If this value is set, handle should be None. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">handle</code> </td> <td> (optional) A <code translate=\"no\" dir=\"ltr\">Tensor</code> handle to an existing TensorArray. If this is set, tensor_array_name should be None. Only supported in graph mode. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">flow</code> </td> <td> (optional) A float <code translate=\"no\" dir=\"ltr\">Tensor</code> scalar coming from an existing <a href=\"tensorarray#flow\"><code translate=\"no\" dir=\"ltr\">TensorArray.flow</code></a>. Only supported in graph mode. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">infer_shape</code> </td> <td> (optional, default: True) If True, shape inference is enabled. In this case, all elements must have the same shape. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">element_shape</code> </td> <td> (optional, default: None) A <code translate=\"no\" dir=\"ltr\">TensorShape</code> object specifying the shape constraints of each of the elements of the TensorArray. Need not be fully defined. </td> </tr>\n<tr> <td> <code translate=\"no\" dir=\"ltr\">colocate_with_firs