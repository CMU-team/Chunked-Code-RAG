{"index":"<h1 id=\"pytorch-documentation\">PyTorch documentation</h1> <p>PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.</p> <p>Features described in this documentation are classified by release status:</p>  <p><em>Stable:</em> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).</p> <p><em>Beta:</em> Features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.</p> <p><em>Prototype:</em> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.</p>   <p class=\"caption\"><span class=\"caption-text\">Notes</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html\">Automatic Mixed Precision examples</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/autograd.html\">Autograd mechanics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html\">Broadcasting semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cpu_threading_torchscript_inference.html\">CPU threading and TorchScript inference</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html\">CUDA semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/ddp.html\">Distributed Data Parallel</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/extending.html\">Extending PyTorch</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/faq.html\">Frequently Asked Questions</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/large_scale_deployments.html\">Features for large-scale deployments</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/modules.html\">Modules</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/multiprocessing.html\">Multiprocessing best practices</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\">Reproducibility</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/serialization.html\">Serialization semantics</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/windows.html\">Windows FAQ</a></li> </ul>   <p class=\"caption\"><span class=\"caption-text\">Language Bindings</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/cpp_index.html\">C++</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/javadoc/\">Javadoc</a></li> </ul>   <p class=\"caption\"><span class=\"caption-text\">Python API</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch\">torch</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"nn\">torch.nn</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"nn.functional\">torch.nn.functional</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tensors\">torch.Tensor</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tensor_attributes\">Tensor Attributes</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tensor_view\">Tensor Views</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"autograd\">torch.autograd</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"cuda\">torch.cuda</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"amp\">torch.cuda.amp</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"backends\">torch.backends</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"distributed\">torch.distributed</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"distributions\">torch.distributions</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"fft\">torch.fft</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"futures\">torch.futures</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"fx\">torch.fx</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"hub\">torch.hub</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"jit\">torch.jit</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"linalg\">torch.linalg</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"torch.overrides\">torch.overrides</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"nn.init\">torch.nn.init</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"onnx\">torch.onnx</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"optim\">torch.optim</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"complex_numbers\">Complex Numbers</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"ddp_comm_hooks\">DDP Communication Hooks</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"pipeline\">Pipeline Parallelism</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"quantization\">Quantization</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"rpc\">Distributed RPC Framework</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"random\">torch.random</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"sparse\">torch.sparse</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"storage\">torch.Storage</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"benchmark_utils\">torch.utils.benchmark</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"bottleneck\">torch.utils.bottleneck</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"checkpoint\">torch.utils.checkpoint</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"cpp_extension\">torch.utils.cpp_extension</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"data\">torch.utils.data</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"dlpack\">torch.utils.dlpack</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"mobile_optimizer\">torch.utils.mobile_optimizer</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"model_zoo\">torch.utils.model_zoo</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"tensorboard\">torch.utils.tensorboard</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"type_info\">Type Info</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"named_tensor\">Named Tensors</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"name_inference\">Named Tensors operator coverage</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"__config__\">torch.__config__</a></li> </ul>   <p class=\"caption\"><span class=\"caption-text\">Libraries</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/audio/stable\">torchaudio</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/text/stable\">torchtext</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/vision/stable\">torchvision</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/elastic/\">TorchElastic</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"https://pytorch.org/serve\">TorchServe</a></li> <li class=\"toctree-l1\"><a class=\"reference external\" href=\"http://pytorch.org/xla/\">PyTorch on XLA Devices</a></li> </ul>   <p class=\"caption\"><span class=\"caption-text\">Community</span></p> <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/community/contribution_guide.html\">PyTorch Contribution Guide</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/community/governance.html\">PyTorch Governance</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/community/persons_of_interest.html\">PyTorch Governance | Persons of Interest</a></li> </ul>    <h1 id=\"indices-and-tables\">Indices and tables</h1> <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/genindex.html\"><span class=\"std std-ref\">Index</span></a></li> <li><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/py-modindex.html\"><span class=\"std std-ref\">Module Index</span></a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/</a>\n  </p>\n</div>\n","torch":"<h1 id=\"torch\">torch</h1> <p>The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities.</p> <p>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &gt;= 3.0</p>  <h2 id=\"tensors\">Tensors</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_tensor#torch.is_tensor\" title=\"torch.is_tensor\"><code>is_tensor</code></a>\n</td> <td><p>Returns True if <code>obj</code> is a PyTorch tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_storage#torch.is_storage\" title=\"torch.is_storage\"><code>is_storage</code></a>\n</td> <td><p>Returns True if <code>obj</code> is a PyTorch storage object.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_complex#torch.is_complex\" title=\"torch.is_complex\"><code>is_complex</code></a>\n</td> <td><p>Returns True if the data type of <code>input</code> is a complex data type i.e., one of <code>torch.complex64</code>, and <code>torch.complex128</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_floating_point#torch.is_floating_point\" title=\"torch.is_floating_point\"><code>is_floating_point</code></a>\n</td> <td><p>Returns True if the data type of <code>input</code> is a floating point data type i.e., one of <code>torch.float64</code>, <code>torch.float32</code>, <code>torch.float16</code>, and <code>torch.bfloat16</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.is_nonzero#torch.is_nonzero\" title=\"torch.is_nonzero\"><code>is_nonzero</code></a>\n</td> <td><p>Returns True if the <code>input</code> is a single element tensor which is not equal to zero after type conversions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_default_dtype#torch.set_default_dtype\" title=\"torch.set_default_dtype\"><code>set_default_dtype</code></a>\n</td> <td><p>Sets the default floating point dtype to <code>d</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_default_dtype#torch.get_default_dtype\" title=\"torch.get_default_dtype\"><code>get_default_dtype</code></a>\n</td> <td><p>Get the current default floating point <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_default_tensor_type#torch.set_default_tensor_type\" title=\"torch.set_default_tensor_type\"><code>set_default_tensor_type</code></a>\n</td> <td><p>Sets the default <code>torch.Tensor</code> type to floating point tensor type <code>t</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.numel#torch.numel\" title=\"torch.numel\"><code>numel</code></a>\n</td> <td><p>Returns the total number of elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_printoptions#torch.set_printoptions\" title=\"torch.set_printoptions\"><code>set_printoptions</code></a>\n</td> <td><p>Set options for printing.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_flush_denormal#torch.set_flush_denormal\" title=\"torch.set_flush_denormal\"><code>set_flush_denormal</code></a>\n</td> <td><p>Disables denormal floating numbers on CPU.</p></td> </tr>  </table>  <h3 id=\"tensor-creation-ops\">Creation Ops</h3> <div class=\"admonition note\" id=\"creation-ops\"> <p class=\"admonition-title\">Note</p> <p>Random sampling creation ops are listed under <a class=\"reference internal\" href=\"#random-sampling\"><span class=\"std std-ref\">Random sampling</span></a> and include: <a class=\"reference internal\" href=\"generated/torch.rand#torch.rand\" title=\"torch.rand\"><code>torch.rand()</code></a> <a class=\"reference internal\" href=\"generated/torch.rand_like#torch.rand_like\" title=\"torch.rand_like\"><code>torch.rand_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>torch.randn()</code></a> <a class=\"reference internal\" href=\"generated/torch.randn_like#torch.randn_like\" title=\"torch.randn_like\"><code>torch.randn_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randint#torch.randint\" title=\"torch.randint\"><code>torch.randint()</code></a> <a class=\"reference internal\" href=\"generated/torch.randint_like#torch.randint_like\" title=\"torch.randint_like\"><code>torch.randint_like()</code></a> <a class=\"reference internal\" href=\"generated/torch.randperm#torch.randperm\" title=\"torch.randperm\"><code>torch.randperm()</code></a> You may also use <a class=\"reference internal\" href=\"generated/torch.empty#torch.empty\" title=\"torch.empty\"><code>torch.empty()</code></a> with the <a class=\"reference internal\" href=\"#inplace-random-sampling\"><span class=\"std std-ref\">In-place random sampling</span></a> methods to create <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> s with values sampled from a broader range of distributions.</p> </div> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a>\n</td> <td><p>Constructs a tensor with <code>data</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor\" title=\"torch.sparse_coo_tensor\"><code>sparse_coo_tensor</code></a>\n</td> <td><p>Constructs a <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code>indices</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.as_tensor#torch.as_tensor\" title=\"torch.as_tensor\"><code>as_tensor</code></a>\n</td> <td><p>Convert the data into a <code>torch.Tensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.as_strided#torch.as_strided\" title=\"torch.as_strided\"><code>as_strided</code></a>\n</td> <td><p>Create a view of an existing <code>torch.Tensor</code> <code>input</code> with specified <code>size</code>, <code>stride</code> and <code>storage_offset</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.from_numpy#torch.from_numpy\" title=\"torch.from_numpy\"><code>from_numpy</code></a>\n</td> <td><p>Creates a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>Tensor</code></a> from a <a class=\"reference external\" href=\"https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray\" title=\"(in NumPy v1.20)\"><code>numpy.ndarray</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.zeros#torch.zeros\" title=\"torch.zeros\"><code>zeros</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.zeros_like#torch.zeros_like\" title=\"torch.zeros_like\"><code>zeros_like</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ones#torch.ones\" title=\"torch.ones\"><code>ones</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ones_like#torch.ones_like\" title=\"torch.ones_like\"><code>ones_like</code></a>\n</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arange#torch.arange\" title=\"torch.arange\"><code>arange</code></a>\n</td> <td><p>Returns a 1-D tensor of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo fence=\"true\">⌈</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence=\"true\">⌉</mo></mrow><annotation encoding=\"application/x-tex\">\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil</annotation></semantics></math></span></span> </span> with values from the interval <code>[start, end)</code> taken with common difference <code>step</code> beginning from <code>start</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.range#torch.range\" title=\"torch.range\"><code>range</code></a>\n</td> <td><p>Returns a 1-D tensor of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mrow><mo fence=\"true\">⌊</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence=\"true\">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1</annotation></semantics></math></span></span> </span> with values from <code>start</code> to <code>end</code> with step <code>step</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.linspace#torch.linspace\" title=\"torch.linspace\"><code>linspace</code></a>\n</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <code>start</code> to <code>end</code>, inclusive.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logspace#torch.logspace\" title=\"torch.logspace\"><code>logspace</code></a>\n</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding=\"application/x-tex\">{{\\text{{base}}}}^{{\\text{{start}}}}</annotation></semantics></math></span></span> </span> to <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding=\"application/x-tex\">{{\\text{{base}}}}^{{\\text{{end}}}}</annotation></semantics></math></span></span> </span>, inclusive, on a logarithmic scale with base <code>base</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.eye#torch.eye\" title=\"torch.eye\"><code>eye</code></a>\n</td> <td><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty#torch.empty\" title=\"torch.empty\"><code>empty</code></a>\n</td> <td><p>Returns a tensor filled with uninitialized data.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty_like#torch.empty_like\" title=\"torch.empty_like\"><code>empty_like</code></a>\n</td> <td><p>Returns an uninitialized tensor with the same size as <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.empty_strided#torch.empty_strided\" title=\"torch.empty_strided\"><code>empty_strided</code></a>\n</td> <td><p>Returns a tensor filled with uninitialized data.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.full#torch.full\" title=\"torch.full\"><code>full</code></a>\n</td> <td><p>Creates a tensor of size <code>size</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.full_like#torch.full_like\" title=\"torch.full_like\"><code>full_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantize_per_tensor#torch.quantize_per_tensor\" title=\"torch.quantize_per_tensor\"><code>quantize_per_tensor</code></a>\n</td> <td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantize_per_channel#torch.quantize_per_channel\" title=\"torch.quantize_per_channel\"><code>quantize_per_channel</code></a>\n</td> <td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dequantize#torch.dequantize\" title=\"torch.dequantize\"><code>dequantize</code></a>\n</td> <td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.complex#torch.complex\" title=\"torch.complex\"><code>complex</code></a>\n</td> <td><p>Constructs a complex tensor with its real part equal to <a class=\"reference internal\" href=\"generated/torch.real#torch.real\" title=\"torch.real\"><code>real</code></a> and its imaginary part equal to <a class=\"reference internal\" href=\"generated/torch.imag#torch.imag\" title=\"torch.imag\"><code>imag</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.polar#torch.polar\" title=\"torch.polar\"><code>polar</code></a>\n</td> <td><p>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs</code></a> and angle <a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>angle</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.heaviside#torch.heaviside\" title=\"torch.heaviside\"><code>heaviside</code></a>\n</td> <td><p>Computes the Heaviside step function for each element in <code>input</code>.</p></td> </tr>  </table>   <h3 id=\"indexing-slicing-joining-mutating-ops\">Indexing, Slicing, Joining, Mutating Ops</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cat#torch.cat\" title=\"torch.cat\"><code>cat</code></a>\n</td> <td><p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.chunk#torch.chunk\" title=\"torch.chunk\"><code>chunk</code></a>\n</td> <td><p>Splits a tensor into a specific number of chunks.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.column_stack#torch.column_stack\" title=\"torch.column_stack\"><code>column_stack</code></a>\n</td> <td><p>Creates a new tensor by horizontally stacking the tensors in <code>tensors</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dstack#torch.dstack\" title=\"torch.dstack\"><code>dstack</code></a>\n</td> <td><p>Stack tensors in sequence depthwise (along third axis).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gather#torch.gather\" title=\"torch.gather\"><code>gather</code></a>\n</td> <td><p>Gathers values along an axis specified by <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hstack#torch.hstack\" title=\"torch.hstack\"><code>hstack</code></a>\n</td> <td><p>Stack tensors in sequence horizontally (column wise).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.index_select#torch.index_select\" title=\"torch.index_select\"><code>index_select</code></a>\n</td> <td><p>Returns a new tensor which indexes the <code>input</code> tensor along dimension <code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.masked_select#torch.masked_select\" title=\"torch.masked_select\"><code>masked_select</code></a>\n</td> <td><p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the boolean mask <code>mask</code> which is a <code>BoolTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>movedim</code></a>\n</td> <td><p>Moves the dimension(s) of <code>input</code> at the position(s) in <code>source</code> to the position(s) in <code>destination</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.moveaxis#torch.moveaxis\" title=\"torch.moveaxis\"><code>moveaxis</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>torch.movedim()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.narrow#torch.narrow\" title=\"torch.narrow\"><code>narrow</code></a>\n</td> <td><p>Returns a new tensor that is a narrowed version of <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nonzero#torch.nonzero\" title=\"torch.nonzero\"><code>nonzero</code></a>\n</td> <td></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.reshape#torch.reshape\" title=\"torch.reshape\"><code>reshape</code></a>\n</td> <td><p>Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.row_stack#torch.row_stack\" title=\"torch.row_stack\"><code>row_stack</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.vstack#torch.vstack\" title=\"torch.vstack\"><code>torch.vstack()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.scatter#torch.scatter\" title=\"torch.scatter\"><code>scatter</code></a>\n</td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"tensors#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>torch.Tensor.scatter_()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.scatter_add#torch.scatter_add\" title=\"torch.scatter_add\"><code>scatter_add</code></a>\n</td> <td><p>Out-of-place version of <a class=\"reference internal\" href=\"tensors#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>torch.Tensor.scatter_add_()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.split#torch.split\" title=\"torch.split\"><code>split</code></a>\n</td> <td><p>Splits the tensor into chunks.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.squeeze#torch.squeeze\" title=\"torch.squeeze\"><code>squeeze</code></a>\n</td> <td><p>Returns a tensor with all the dimensions of <code>input</code> of size <code>1</code> removed.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.stack#torch.stack\" title=\"torch.stack\"><code>stack</code></a>\n</td> <td><p>Concatenates a sequence of tensors along a new dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.swapaxes#torch.swapaxes\" title=\"torch.swapaxes\"><code>swapaxes</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.swapdims#torch.swapdims\" title=\"torch.swapdims\"><code>swapdims</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.t#torch.t\" title=\"torch.t\"><code>t</code></a>\n</td> <td><p>Expects <code>input</code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.take#torch.take\" title=\"torch.take\"><code>take</code></a>\n</td> <td><p>Returns a new tensor with the elements of <code>input</code> at the given indices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensor_split#torch.tensor_split\" title=\"torch.tensor_split\"><code>tensor_split</code></a>\n</td> <td><p>Splits a tensor into multiple sub-tensors, all of which are views of <code>input</code>, along dimension <code>dim</code> according to the indices or number of sections specified by <code>indices_or_sections</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tile#torch.tile\" title=\"torch.tile\"><code>tile</code></a>\n</td> <td><p>Constructs a tensor by repeating the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>transpose</code></a>\n</td> <td><p>Returns a tensor that is a transposed version of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unbind#torch.unbind\" title=\"torch.unbind\"><code>unbind</code></a>\n</td> <td><p>Removes a tensor dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unsqueeze#torch.unsqueeze\" title=\"torch.unsqueeze\"><code>unsqueeze</code></a>\n</td> <td><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vstack#torch.vstack\" title=\"torch.vstack\"><code>vstack</code></a>\n</td> <td><p>Stack tensors in sequence vertically (row wise).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.where#torch.where\" title=\"torch.where\"><code>where</code></a>\n</td> <td><p>Return a tensor of elements selected from either <code>x</code> or <code>y</code>, depending on <code>condition</code>.</p></td> </tr>  </table>    <h2 id=\"id1\">Generators</h2> <table class=\"longtable docutils colwidths-auto align-default\" id=\"generators\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.generator#torch.Generator\" title=\"torch.Generator\"><code>Generator</code></a>\n</td> <td><p>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</p></td> </tr>  </table>   <h2 id=\"id2\">Random sampling</h2> <table class=\"longtable docutils colwidths-auto align-default\" id=\"random-sampling\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.seed#torch.seed\" title=\"torch.seed\"><code>seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers to a non-deterministic random number.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.manual_seed#torch.manual_seed\" title=\"torch.manual_seed\"><code>manual_seed</code></a>\n</td> <td><p>Sets the seed for generating random numbers.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.initial_seed#torch.initial_seed\" title=\"torch.initial_seed\"><code>initial_seed</code></a>\n</td> <td><p>Returns the initial seed for generating random numbers as a Python <code>long</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_rng_state#torch.get_rng_state\" title=\"torch.get_rng_state\"><code>get_rng_state</code></a>\n</td> <td><p>Returns the random number generator state as a <code>torch.ByteTensor</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_rng_state#torch.set_rng_state\" title=\"torch.set_rng_state\"><code>set_rng_state</code></a>\n</td> <td><p>Sets the random number generator state.</p></td> </tr>  </table> <dl class=\"attribute\"> <dt id=\"torch.torch.default_generator\">\n<code>torch.default_generator Returns the default CPU torch.Generator</code> </dt> \n</dl> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>bernoulli</code></a>\n</td> <td><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.multinomial#torch.multinomial\" title=\"torch.multinomial\"><code>multinomial</code></a>\n</td> <td><p>Returns a tensor where each row contains <code>num_samples</code> indices sampled from the multinomial probability distribution located in the corresponding row of tensor <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.normal#torch.normal\" title=\"torch.normal\"><code>normal</code></a>\n</td> <td><p>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.poisson#torch.poisson\" title=\"torch.poisson\"><code>poisson</code></a>\n</td> <td><p>Returns a tensor of the same size as <code>input</code> with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in <code>input</code> i.e.,</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rand#torch.rand\" title=\"torch.rand\"><code>rand</code></a>\n</td> <td><p>Returns a tensor filled with random numbers from a uniform distribution on the interval <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0, 1)</annotation></semantics></math></span></span> </span></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rand_like#torch.rand_like\" title=\"torch.rand_like\"><code>rand_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a uniform distribution on the interval <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">[0, 1)</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randint#torch.randint\" title=\"torch.randint\"><code>randint</code></a>\n</td> <td><p>Returns a tensor filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randint_like#torch.randint_like\" title=\"torch.randint_like\"><code>randint_like</code></a>\n</td> <td><p>Returns a tensor with the same shape as Tensor <code>input</code> filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>randn</code></a>\n</td> <td><p>Returns a tensor filled with random numbers from a normal distribution with mean <code>0</code> and variance <code>1</code> (also called the standard normal distribution).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randn_like#torch.randn_like\" title=\"torch.randn_like\"><code>randn_like</code></a>\n</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a normal distribution with mean 0 and variance 1.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.randperm#torch.randperm\" title=\"torch.randperm\"><code>randperm</code></a>\n</td> <td><p>Returns a random permutation of integers from <code>0</code> to <code>n - 1</code>.</p></td> </tr>  </table>  <h3 id=\"inplace-random-sampling\">In-place random sampling</h3> <p id=\"in-place-random-sampling\">There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p> <ul class=\"simple\"> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.bernoulli_\" title=\"torch.Tensor.bernoulli_\"><code>torch.Tensor.bernoulli_()</code></a> - in-place version of <a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>torch.bernoulli()</code></a>\n</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.cauchy_\" title=\"torch.Tensor.cauchy_\"><code>torch.Tensor.cauchy_()</code></a> - numbers drawn from the Cauchy distribution</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.exponential_\" title=\"torch.Tensor.exponential_\"><code>torch.Tensor.exponential_()</code></a> - numbers drawn from the exponential distribution</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.geometric_\" title=\"torch.Tensor.geometric_\"><code>torch.Tensor.geometric_()</code></a> - elements drawn from the geometric distribution</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.log_normal_\" title=\"torch.Tensor.log_normal_\"><code>torch.Tensor.log_normal_()</code></a> - samples from the log-normal distribution</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.normal_\" title=\"torch.Tensor.normal_\"><code>torch.Tensor.normal_()</code></a> - in-place version of <a class=\"reference internal\" href=\"generated/torch.normal#torch.normal\" title=\"torch.normal\"><code>torch.normal()</code></a>\n</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.random_\" title=\"torch.Tensor.random_\"><code>torch.Tensor.random_()</code></a> - numbers sampled from the discrete uniform distribution</li> <li>\n<a class=\"reference internal\" href=\"tensors#torch.Tensor.uniform_\" title=\"torch.Tensor.uniform_\"><code>torch.Tensor.uniform_()</code></a> - numbers sampled from the continuous uniform distribution</li> </ul>   <h3 id=\"quasi-random-sampling\">Quasi-random sampling</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine\" title=\"torch.quasirandom.SobolEngine\"><code>quasirandom.SobolEngine</code></a></p></td> <td><p>The <a class=\"reference internal\" href=\"generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine\" title=\"torch.quasirandom.SobolEngine\"><code>torch.quasirandom.SobolEngine</code></a> is an engine for generating (scrambled) Sobol sequences.</p></td> </tr>  </table>    <h2 id=\"serialization\">Serialization</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.save#torch.save\" title=\"torch.save\"><code>save</code></a>\n</td> <td><p>Saves an object to a disk file.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.load#torch.load\" title=\"torch.load\"><code>load</code></a>\n</td> <td><p>Loads an object saved with <a class=\"reference internal\" href=\"generated/torch.save#torch.save\" title=\"torch.save\"><code>torch.save()</code></a> from a file.</p></td> </tr>  </table>   <h2 id=\"parallelism\">Parallelism</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_num_threads#torch.get_num_threads\" title=\"torch.get_num_threads\"><code>get_num_threads</code></a>\n</td> <td><p>Returns the number of threads used for parallelizing CPU operations</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_num_threads#torch.set_num_threads\" title=\"torch.set_num_threads\"><code>set_num_threads</code></a>\n</td> <td><p>Sets the number of threads used for intraop parallelism on CPU.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.get_num_interop_threads#torch.get_num_interop_threads\" title=\"torch.get_num_interop_threads\"><code>get_num_interop_threads</code></a>\n</td> <td><p>Returns the number of threads used for inter-op parallelism on CPU (e.g.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_num_interop_threads#torch.set_num_interop_threads\" title=\"torch.set_num_interop_threads\"><code>set_num_interop_threads</code></a>\n</td> <td><p>Sets the number of threads used for interop parallelism (e.g.</p></td> </tr>  </table>   <h2 id=\"locally-disabling-gradient-computation\">Locally disabling gradient computation</h2> <p>The context managers <a class=\"reference internal\" href=\"generated/torch.no_grad#torch.no_grad\" title=\"torch.no_grad\"><code>torch.no_grad()</code></a>, <a class=\"reference internal\" href=\"generated/torch.enable_grad#torch.enable_grad\" title=\"torch.enable_grad\"><code>torch.enable_grad()</code></a>, and <a class=\"reference internal\" href=\"generated/torch.set_grad_enabled#torch.set_grad_enabled\" title=\"torch.set_grad_enabled\"><code>torch.set_grad_enabled()</code></a> are helpful for locally disabling and enabling gradient computation. See <a class=\"reference internal\" href=\"autograd#locally-disable-grad\"><span class=\"std std-ref\">Locally disabling gradient computation</span></a> for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the <code>threading</code> module, etc.</p> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; is_train = False\n&gt;&gt;&gt; with torch.set_grad_enabled(is_train):\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n\n&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n\n&gt;&gt;&gt; torch.set_grad_enabled(False)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n</pre> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.no_grad#torch.no_grad\" title=\"torch.no_grad\"><code>no_grad</code></a>\n</td> <td><p>Context-manager that disabled gradient calculation.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.enable_grad#torch.enable_grad\" title=\"torch.enable_grad\"><code>enable_grad</code></a>\n</td> <td><p>Context-manager that enables gradient calculation.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.set_grad_enabled#torch.set_grad_enabled\" title=\"torch.set_grad_enabled\"><code>set_grad_enabled</code></a>\n</td> <td><p>Context-manager that sets gradient calculation to on or off.</p></td> </tr>  </table>   <h2 id=\"math-operations\">Math operations</h2>  <h3 id=\"pointwise-ops\">Pointwise Ops</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs</code></a>\n</td> <td><p>Computes the absolute value of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.absolute#torch.absolute\" title=\"torch.absolute\"><code>absolute</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>acos</code></a>\n</td> <td><p>Computes the inverse cosine of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arccos#torch.arccos\" title=\"torch.arccos\"><code>arccos</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>acosh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arccosh#torch.arccosh\" title=\"torch.arccosh\"><code>arccosh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>torch.acosh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.add#torch.add\" title=\"torch.add\"><code>add</code></a>\n</td> <td><p>Adds the scalar <code>other</code> to each element of the input <code>input</code> and returns a new resulting tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addcdiv#torch.addcdiv\" title=\"torch.addcdiv\"><code>addcdiv</code></a>\n</td> <td><p>Performs the element-wise division of <code>tensor1</code> by <code>tensor2</code>, multiply the result by the scalar <code>value</code> and add it to <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addcmul#torch.addcmul\" title=\"torch.addcmul\"><code>addcmul</code></a>\n</td> <td><p>Performs the element-wise multiplication of <code>tensor1</code> by <code>tensor2</code>, multiply the result by the scalar <code>value</code> and add it to <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>angle</code></a>\n</td> <td><p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>asin</code></a>\n</td> <td><p>Returns a new tensor with the arcsine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arcsin#torch.arcsin\" title=\"torch.arcsin\"><code>arcsin</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>asinh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arcsinh#torch.arcsinh\" title=\"torch.arcsinh\"><code>arcsinh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>torch.asinh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>atan</code></a>\n</td> <td><p>Returns a new tensor with the arctangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arctan#torch.arctan\" title=\"torch.arctan\"><code>arctan</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atanh#torch.atanh\" title=\"torch.atanh\"><code>atanh</code></a>\n</td> <td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.arctanh#torch.arctanh\" title=\"torch.arctanh\"><code>arctanh</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.atanh#torch.atanh\" title=\"torch.atanh\"><code>torch.atanh()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atan2#torch.atan2\" title=\"torch.atan2\"><code>atan2</code></a>\n</td> <td><p>Element-wise arctangent of <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant=\"normal\">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\text{input}_{i} / \\text{other}_{i}</annotation></semantics></math></span></span> </span> with consideration of the quadrant.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_not#torch.bitwise_not\" title=\"torch.bitwise_not\"><code>bitwise_not</code></a>\n</td> <td><p>Computes the bitwise NOT of the given input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_and#torch.bitwise_and\" title=\"torch.bitwise_and\"><code>bitwise_and</code></a>\n</td> <td><p>Computes the bitwise AND of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_or#torch.bitwise_or\" title=\"torch.bitwise_or\"><code>bitwise_or</code></a>\n</td> <td><p>Computes the bitwise OR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bitwise_xor#torch.bitwise_xor\" title=\"torch.bitwise_xor\"><code>bitwise_xor</code></a>\n</td> <td><p>Computes the bitwise XOR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>ceil</code></a>\n</td> <td><p>Returns a new tensor with the ceil of the elements of <code>input</code>, the smallest integer greater than or equal to each element.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>clamp</code></a>\n</td> <td><p>Clamp all elements in <code>input</code> into the range <code>[</code> <a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>min</code></a>, <a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>max</code></a> <code>]</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clip#torch.clip\" title=\"torch.clip\"><code>clip</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>torch.clamp()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.conj#torch.conj\" title=\"torch.conj\"><code>conj</code></a>\n</td> <td><p>Computes the element-wise conjugate of the given <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.copysign#torch.copysign\" title=\"torch.copysign\"><code>copysign</code></a>\n</td> <td><p>Create a new floating-point tensor with the magnitude of <code>input</code> and the sign of <code>other</code>, elementwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>cos</code></a>\n</td> <td><p>Returns a new tensor with the cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>cosh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.deg2rad#torch.deg2rad\" title=\"torch.deg2rad\"><code>deg2rad</code></a>\n</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in degrees to radians.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>div</code></a>\n</td> <td><p>Divides each element of the input <code>input</code> by the corresponding element of <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.divide#torch.divide\" title=\"torch.divide\"><code>divide</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.digamma#torch.digamma\" title=\"torch.digamma\"><code>digamma</code></a>\n</td> <td><p>Computes the logarithmic derivative of the gamma function on <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>erf</code></a>\n</td> <td><p>Computes the error function of each element.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>erfc</code></a>\n</td> <td><p>Computes the complementary error function of each element of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.erfinv#torch.erfinv\" title=\"torch.erfinv\"><code>erfinv</code></a>\n</td> <td><p>Computes the inverse error function of each element of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>exp</code></a>\n</td> <td><p>Returns a new tensor with the exponential of the elements of the input tensor <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.exp2#torch.exp2\" title=\"torch.exp2\"><code>exp2</code></a>\n</td> <td><p>Computes the base two exponential function of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>expm1</code></a>\n</td> <td><p>Returns a new tensor with the exponential of the elements minus 1 of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine\" title=\"torch.fake_quantize_per_channel_affine\"><code>fake_quantize_per_channel_affine</code></a>\n</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized per channel using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>, across the channel specified by <code>axis</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine\" title=\"torch.fake_quantize_per_tensor_affine\"><code>fake_quantize_per_tensor_affine</code></a>\n</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fix#torch.fix\" title=\"torch.fix\"><code>fix</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>torch.trunc()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.float_power#torch.float_power\" title=\"torch.float_power\"><code>float_power</code></a>\n</td> <td><p>Raises <code>input</code> to the power of <code>exponent</code>, elementwise, in double precision.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>floor</code></a>\n</td> <td><p>Returns a new tensor with the floor of the elements of <code>input</code>, the largest integer less than or equal to each element.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.floor_divide#torch.floor_divide\" title=\"torch.floor_divide\"><code>floor_divide</code></a>\n</td> <td></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmod#torch.fmod\" title=\"torch.fmod\"><code>fmod</code></a>\n</td> <td><p>Computes the element-wise remainder of division.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>frac</code></a>\n</td> <td><p>Computes the fractional portion of each element in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.imag#torch.imag\" title=\"torch.imag\"><code>imag</code></a>\n</td> <td><p>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ldexp#torch.ldexp\" title=\"torch.ldexp\"><code>ldexp</code></a>\n</td> <td><p>Multiplies <code>input</code> by 2**:attr:<code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lerp#torch.lerp\" title=\"torch.lerp\"><code>lerp</code></a>\n</td> <td><p>Does a linear interpolation of two tensors <code>start</code> (given by <code>input</code>) and <code>end</code> based on a scalar or tensor <code>weight</code> and returns the resulting <code>out</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>lgamma</code></a>\n</td> <td><p>Computes the logarithm of the gamma function on <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>log</code></a>\n</td> <td><p>Returns a new tensor with the natural logarithm of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>log10</code></a>\n</td> <td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>log1p</code></a>\n</td> <td><p>Returns a new tensor with the natural logarithm of (1 + <code>input</code>).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>log2</code></a>\n</td> <td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logaddexp#torch.logaddexp\" title=\"torch.logaddexp\"><code>logaddexp</code></a>\n</td> <td><p>Logarithm of the sum of exponentiations of the inputs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logaddexp2#torch.logaddexp2\" title=\"torch.logaddexp2\"><code>logaddexp2</code></a>\n</td> <td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_and#torch.logical_and\" title=\"torch.logical_and\"><code>logical_and</code></a>\n</td> <td><p>Computes the element-wise logical AND of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_not#torch.logical_not\" title=\"torch.logical_not\"><code>logical_not</code></a>\n</td> <td><p>Computes the element-wise logical NOT of the given input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_or#torch.logical_or\" title=\"torch.logical_or\"><code>logical_or</code></a>\n</td> <td><p>Computes the element-wise logical OR of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logical_xor#torch.logical_xor\" title=\"torch.logical_xor\"><code>logical_xor</code></a>\n</td> <td><p>Computes the element-wise logical XOR of the given input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logit#torch.logit\" title=\"torch.logit\"><code>logit</code></a>\n</td> <td><p>Returns a new tensor with the logit of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hypot#torch.hypot\" title=\"torch.hypot\"><code>hypot</code></a>\n</td> <td><p>Given the legs of a right triangle, return its hypotenuse.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.i0#torch.i0\" title=\"torch.i0\"><code>i0</code></a>\n</td> <td><p>Computes the zeroth order modified Bessel function of the first kind for each element of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.igamma#torch.igamma\" title=\"torch.igamma\"><code>igamma</code></a>\n</td> <td><p>Computes the regularized lower incomplete gamma function:</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.igammac#torch.igammac\" title=\"torch.igammac\"><code>igammac</code></a>\n</td> <td><p>Computes the regularized upper incomplete gamma function:</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>mul</code></a>\n</td> <td><p>Multiplies each element of the input <code>input</code> with the scalar <code>other</code> and returns a new resulting tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.multiply#torch.multiply\" title=\"torch.multiply\"><code>multiply</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>torch.mul()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mvlgamma#torch.mvlgamma\" title=\"torch.mvlgamma\"><code>mvlgamma</code></a>\n</td> <td><p>Computes the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Multivariate_gamma_function\">multivariate log-gamma function</a>) with dimension <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></span> </span> element-wise, given by</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nan_to_num#torch.nan_to_num\" title=\"torch.nan_to_num\"><code>nan_to_num</code></a>\n</td> <td><p>Replaces <code>NaN</code>, positive infinity, and negative infinity values in <code>input</code> with the values specified by <code>nan</code>, <code>posinf</code>, and <code>neginf</code>, respectively.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>neg</code></a>\n</td> <td><p>Returns a new tensor with the negative of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.negative#torch.negative\" title=\"torch.negative\"><code>negative</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nextafter#torch.nextafter\" title=\"torch.nextafter\"><code>nextafter</code></a>\n</td> <td><p>Return the next floating-point value after <code>input</code> towards <code>other</code>, elementwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.polygamma#torch.polygamma\" title=\"torch.polygamma\"><code>polygamma</code></a>\n</td> <td><p>Computes the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi>n</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">n^{th}</annotation></semantics></math></span></span> </span> derivative of the digamma function on <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pow#torch.pow\" title=\"torch.pow\"><code>pow</code></a>\n</td> <td><p>Takes the power of each element in <code>input</code> with <code>exponent</code> and returns a tensor with the result.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rad2deg#torch.rad2deg\" title=\"torch.rad2deg\"><code>rad2deg</code></a>\n</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in radians to degrees.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.real#torch.real\" title=\"torch.real\"><code>real</code></a>\n</td> <td><p>Returns a new tensor containing real values of the <code>self</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>reciprocal</code></a>\n</td> <td><p>Returns a new tensor with the reciprocal of the elements of <code>input</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.remainder#torch.remainder\" title=\"torch.remainder\"><code>remainder</code></a>\n</td> <td><p>Computes the element-wise remainder of division.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>round</code></a>\n</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> rounded to the closest integer.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rsqrt#torch.rsqrt\" title=\"torch.rsqrt\"><code>rsqrt</code></a>\n</td> <td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>sigmoid</code></a>\n</td> <td><p>Returns a new tensor with the sigmoid of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sign#torch.sign\" title=\"torch.sign\"><code>sign</code></a>\n</td> <td><p>Returns a new tensor with the signs of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sgn#torch.sgn\" title=\"torch.sgn\"><code>sgn</code></a>\n</td> <td><p>For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of <code>input</code> and absolute value 1.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.signbit#torch.signbit\" title=\"torch.signbit\"><code>signbit</code></a>\n</td> <td><p>Tests if each element of <code>input</code> has its sign bit set (is less than zero) or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>sin</code></a>\n</td> <td><p>Returns a new tensor with the sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sinc#torch.sinc\" title=\"torch.sinc\"><code>sinc</code></a>\n</td> <td><p>Computes the normalized sinc of <code>input.</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>sinh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sqrt#torch.sqrt\" title=\"torch.sqrt\"><code>sqrt</code></a>\n</td> <td><p>Returns a new tensor with the square-root of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.square#torch.square\" title=\"torch.square\"><code>square</code></a>\n</td> <td><p>Returns a new tensor with the square of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sub#torch.sub\" title=\"torch.sub\"><code>sub</code></a>\n</td> <td><p>Subtracts <code>other</code>, scaled by <code>alpha</code>, from <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.subtract#torch.subtract\" title=\"torch.subtract\"><code>subtract</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.sub#torch.sub\" title=\"torch.sub\"><code>torch.sub()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tan#torch.tan\" title=\"torch.tan\"><code>tan</code></a>\n</td> <td><p>Returns a new tensor with the tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tanh#torch.tanh\" title=\"torch.tanh\"><code>tanh</code></a>\n</td> <td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.true_divide#torch.true_divide\" title=\"torch.true_divide\"><code>true_divide</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a> with <code>rounding_mode=None</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>trunc</code></a>\n</td> <td><p>Returns a new tensor with the truncated integer values of the elements of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.xlogy#torch.xlogy\" title=\"torch.xlogy\"><code>xlogy</code></a>\n</td> <td><p>Computes <code>input * log(other)</code> with the following cases.</p></td> </tr>  </table>   <h3 id=\"reduction-ops\">Reduction Ops</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argmax#torch.argmax\" title=\"torch.argmax\"><code>argmax</code></a>\n</td> <td><p>Returns the indices of the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argmin#torch.argmin\" title=\"torch.argmin\"><code>argmin</code></a>\n</td> <td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.amax#torch.amax\" title=\"torch.amax\"><code>amax</code></a>\n</td> <td><p>Returns the maximum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.amin#torch.amin\" title=\"torch.amin\"><code>amin</code></a>\n</td> <td><p>Returns the minimum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.all#torch.all\" title=\"torch.all\"><code>all</code></a>\n</td> <td><p>Tests if all elements in <code>input</code> evaluate to <code>True</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.any#torch.any\" title=\"torch.any\"><code>any</code></a>\n</td> <td>\n\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">param input</dt> <dd class=\"field-odd\">\n<p>the input tensor.</p> </dd> </dl> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>max</code></a>\n</td> <td><p>Returns the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>min</code></a>\n</td> <td><p>Returns the minimum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dist#torch.dist\" title=\"torch.dist\"><code>dist</code></a>\n</td> <td><p>Returns the p-norm of (<code>input</code> - <code>other</code>)</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logsumexp#torch.logsumexp\" title=\"torch.logsumexp\"><code>logsumexp</code></a>\n</td> <td><p>Returns the log of summed exponentials of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>mean</code></a>\n</td> <td><p>Returns the mean value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.median#torch.median\" title=\"torch.median\"><code>median</code></a>\n</td> <td><p>Returns the median of the values in <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nanmedian#torch.nanmedian\" title=\"torch.nanmedian\"><code>nanmedian</code></a>\n</td> <td><p>Returns the median of the values in <code>input</code>, ignoring <code>NaN</code> values.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mode#torch.mode\" title=\"torch.mode\"><code>mode</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the mode value of each row of the <code>input</code> tensor in the given dimension <code>dim</code>, i.e.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.norm#torch.norm\" title=\"torch.norm\"><code>norm</code></a>\n</td> <td><p>Returns the matrix norm or vector norm of a given tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nansum#torch.nansum\" title=\"torch.nansum\"><code>nansum</code></a>\n</td> <td><p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.prod#torch.prod\" title=\"torch.prod\"><code>prod</code></a>\n</td> <td><p>Returns the product of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>quantile</code></a>\n</td> <td><p>Returns the q-th quantiles of all elements in the <code>input</code> tensor, doing a linear interpolation when the q-th quantile lies between two data points.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nanquantile#torch.nanquantile\" title=\"torch.nanquantile\"><code>nanquantile</code></a>\n</td> <td><p>This is a variant of <a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>torch.quantile()</code></a> that “ignores” <code>NaN</code> values, computing the quantiles <code>q</code> as if <code>NaN</code> values in <code>input</code> did not exist.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>std</code></a>\n</td> <td><p>Returns the standard-deviation of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.std_mean#torch.std_mean\" title=\"torch.std_mean\"><code>std_mean</code></a>\n</td> <td><p>Returns the standard-deviation and mean of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sum#torch.sum\" title=\"torch.sum\"><code>sum</code></a>\n</td> <td><p>Returns the sum of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unique#torch.unique\" title=\"torch.unique\"><code>unique</code></a>\n</td> <td><p>Returns the unique elements of the input tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.unique_consecutive#torch.unique_consecutive\" title=\"torch.unique_consecutive\"><code>unique_consecutive</code></a>\n</td> <td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.var#torch.var\" title=\"torch.var\"><code>var</code></a>\n</td> <td><p>Returns the variance of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.var_mean#torch.var_mean\" title=\"torch.var_mean\"><code>var_mean</code></a>\n</td> <td><p>Returns the variance and mean of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.count_nonzero#torch.count_nonzero\" title=\"torch.count_nonzero\"><code>count_nonzero</code></a>\n</td> <td><p>Counts the number of non-zero values in the tensor <code>input</code> along the given <code>dim</code>.</p></td> </tr>  </table>   <h3 id=\"comparison-ops\">Comparison Ops</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>allclose</code></a>\n</td> <td><p>This function checks if all <code>input</code> and <code>other</code> satisfy the condition:</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.argsort#torch.argsort\" title=\"torch.argsort\"><code>argsort</code></a>\n</td> <td><p>Returns the indices that sort a tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.eq#torch.eq\" title=\"torch.eq\"><code>eq</code></a>\n</td> <td><p>Computes element-wise equality</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.equal#torch.equal\" title=\"torch.equal\"><code>equal</code></a>\n</td> <td><p><code>True</code> if two tensors have the same size and elements, <code>False</code> otherwise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>ge</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\geq \\text{other}</annotation></semantics></math></span></span> </span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.greater_equal#torch.greater_equal\" title=\"torch.greater_equal\"><code>greater_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>torch.ge()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>gt</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} &gt; \\text{other}</annotation></semantics></math></span></span> </span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.greater#torch.greater\" title=\"torch.greater\"><code>greater</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>torch.gt()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isclose#torch.isclose\" title=\"torch.isclose\"><code>isclose</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is “close” to the corresponding element of <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isfinite#torch.isfinite\" title=\"torch.isfinite\"><code>isfinite</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element is <code>finite</code> or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isinf#torch.isinf\" title=\"torch.isinf\"><code>isinf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is infinite (positive or negative infinity) or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isposinf#torch.isposinf\" title=\"torch.isposinf\"><code>isposinf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is positive infinity or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isneginf#torch.isneginf\" title=\"torch.isneginf\"><code>isneginf</code></a>\n</td> <td><p>Tests if each element of <code>input</code> is negative infinity or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isnan#torch.isnan\" title=\"torch.isnan\"><code>isnan</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is NaN or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.isreal#torch.isreal\" title=\"torch.isreal\"><code>isreal</code></a>\n</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is real-valued or not.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kthvalue#torch.kthvalue\" title=\"torch.kthvalue\"><code>kthvalue</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the <code>k</code> th smallest element of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>le</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\leq \\text{other}</annotation></semantics></math></span></span> </span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.less_equal#torch.less_equal\" title=\"torch.less_equal\"><code>less_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>torch.le()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>lt</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} &lt; \\text{other}</annotation></semantics></math></span></span> </span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.less#torch.less\" title=\"torch.less\"><code>less</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>torch.lt()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.maximum#torch.maximum\" title=\"torch.maximum\"><code>maximum</code></a>\n</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.minimum#torch.minimum\" title=\"torch.minimum\"><code>minimum</code></a>\n</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmax#torch.fmax\" title=\"torch.fmax\"><code>fmax</code></a>\n</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fmin#torch.fmin\" title=\"torch.fmin\"><code>fmin</code></a>\n</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>ne</code></a>\n</td> <td><p>Computes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo mathvariant=\"normal\">≠</mo><mtext>other</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{input} \\neq \\text{other}</annotation></semantics></math></span></span> </span> element-wise.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.not_equal#torch.not_equal\" title=\"torch.not_equal\"><code>not_equal</code></a>\n</td> <td><p>Alias for <a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>torch.ne()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.sort#torch.sort\" title=\"torch.sort\"><code>sort</code></a>\n</td> <td><p>Sorts the elements of the <code>input</code> tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.topk#torch.topk\" title=\"torch.topk\"><code>topk</code></a>\n</td> <td><p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along a given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.msort#torch.msort\" title=\"torch.msort\"><code>msort</code></a>\n</td> <td><p>Sorts the elements of the <code>input</code> tensor along its first dimension in ascending order by value.</p></td> </tr>  </table>   <h3 id=\"spectral-ops\">Spectral Ops</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.stft#torch.stft\" title=\"torch.stft\"><code>stft</code></a>\n</td> <td><p>Short-time Fourier transform (STFT).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.istft#torch.istft\" title=\"torch.istft\"><code>istft</code></a>\n</td> <td><p>Inverse short time Fourier Transform.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bartlett_window#torch.bartlett_window\" title=\"torch.bartlett_window\"><code>bartlett_window</code></a>\n</td> <td><p>Bartlett window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.blackman_window#torch.blackman_window\" title=\"torch.blackman_window\"><code>blackman_window</code></a>\n</td> <td><p>Blackman window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hamming_window#torch.hamming_window\" title=\"torch.hamming_window\"><code>hamming_window</code></a>\n</td> <td><p>Hamming window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.hann_window#torch.hann_window\" title=\"torch.hann_window\"><code>hann_window</code></a>\n</td> <td><p>Hann window function.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kaiser_window#torch.kaiser_window\" title=\"torch.kaiser_window\"><code>kaiser_window</code></a>\n</td> <td><p>Computes the Kaiser window with window length <code>window_length</code> and shape parameter <code>beta</code>.</p></td> </tr>  </table>   <h3 id=\"other-operations\">Other Operations</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_1d#torch.atleast_1d\" title=\"torch.atleast_1d\"><code>atleast_1d</code></a>\n</td> <td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_2d#torch.atleast_2d\" title=\"torch.atleast_2d\"><code>atleast_2d</code></a>\n</td> <td><p>Returns a 2-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.atleast_3d#torch.atleast_3d\" title=\"torch.atleast_3d\"><code>atleast_3d</code></a>\n</td> <td><p>Returns a 3-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bincount#torch.bincount\" title=\"torch.bincount\"><code>bincount</code></a>\n</td> <td><p>Count the frequency of each value in an array of non-negative ints.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.block_diag#torch.block_diag\" title=\"torch.block_diag\"><code>block_diag</code></a>\n</td> <td><p>Create a block diagonal matrix from provided tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_tensors#torch.broadcast_tensors\" title=\"torch.broadcast_tensors\"><code>broadcast_tensors</code></a>\n</td> <td><p>Broadcasts the given tensors according to <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">Broadcasting semantics</span></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_to#torch.broadcast_to\" title=\"torch.broadcast_to\"><code>broadcast_to</code></a>\n</td> <td><p>Broadcasts <code>input</code> to the shape <code>shape</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.broadcast_shapes#torch.broadcast_shapes\" title=\"torch.broadcast_shapes\"><code>broadcast_shapes</code></a>\n</td> <td><p>Similar to <a class=\"reference internal\" href=\"generated/torch.broadcast_tensors#torch.broadcast_tensors\" title=\"torch.broadcast_tensors\"><code>broadcast_tensors()</code></a> but for shapes.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bucketize#torch.bucketize\" title=\"torch.bucketize\"><code>bucketize</code></a>\n</td> <td><p>Returns the indices of the buckets to which each value in the <code>input</code> belongs, where the boundaries of the buckets are set by <code>boundaries</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cartesian_prod#torch.cartesian_prod\" title=\"torch.cartesian_prod\"><code>cartesian_prod</code></a>\n</td> <td><p>Do cartesian product of the given sequence of tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cdist#torch.cdist\" title=\"torch.cdist\"><code>cdist</code></a>\n</td> <td><p>Computes batched the p-norm distance between each pair of the two collections of row vectors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.clone#torch.clone\" title=\"torch.clone\"><code>clone</code></a>\n</td> <td><p>Returns a copy of <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.combinations#torch.combinations\" title=\"torch.combinations\"><code>combinations</code></a>\n</td> <td><p>Compute combinations of length <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span></span> </span> of the given tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cross#torch.cross\" title=\"torch.cross\"><code>cross</code></a>\n</td> <td><p>Returns the cross product of vectors in dimension <code>dim</code> of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cummax#torch.cummax\" title=\"torch.cummax\"><code>cummax</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative maximum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cummin#torch.cummin\" title=\"torch.cummin\"><code>cummin</code></a>\n</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative minimum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cumprod#torch.cumprod\" title=\"torch.cumprod\"><code>cumprod</code></a>\n</td> <td><p>Returns the cumulative product of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cumsum#torch.cumsum\" title=\"torch.cumsum\"><code>cumsum</code></a>\n</td> <td><p>Returns the cumulative sum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diag#torch.diag\" title=\"torch.diag\"><code>diag</code></a>\n</td> <td>\n\n<ul class=\"simple\"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diag_embed#torch.diag_embed\" title=\"torch.diag_embed\"><code>diag_embed</code></a>\n</td> <td><p>Creates a tensor whose diagonals of certain 2D planes (specified by <code>dim1</code> and <code>dim2</code>) are filled by <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diagflat#torch.diagflat\" title=\"torch.diagflat\"><code>diagflat</code></a>\n</td> <td>\n\n<ul class=\"simple\"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diagonal#torch.diagonal\" title=\"torch.diagonal\"><code>diagonal</code></a>\n</td> <td><p>Returns a partial view of <code>input</code> with the its diagonal elements with respect to <code>dim1</code> and <code>dim2</code> appended as a dimension at the end of the shape.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.diff#torch.diff\" title=\"torch.diff\"><code>diff</code></a>\n</td> <td><p>Computes the n-th forward difference along the given dimension.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.einsum#torch.einsum\" title=\"torch.einsum\"><code>einsum</code></a>\n</td> <td><p>Sums the product of the elements of the input <code>operands</code> along dimensions specified using a notation based on the Einstein summation convention.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flatten#torch.flatten\" title=\"torch.flatten\"><code>flatten</code></a>\n</td> <td><p>Flattens <code>input</code> by reshaping it into a one-dimensional tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flip#torch.flip\" title=\"torch.flip\"><code>flip</code></a>\n</td> <td><p>Reverse the order of a n-D tensor along given axis in dims.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.fliplr#torch.fliplr\" title=\"torch.fliplr\"><code>fliplr</code></a>\n</td> <td><p>Flip tensor in the left/right direction, returning a new tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.flipud#torch.flipud\" title=\"torch.flipud\"><code>flipud</code></a>\n</td> <td><p>Flip tensor in the up/down direction, returning a new tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.kron#torch.kron\" title=\"torch.kron\"><code>kron</code></a>\n</td> <td><p>Computes the Kronecker product, denoted by <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math></span></span> </span>, of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.rot90#torch.rot90\" title=\"torch.rot90\"><code>rot90</code></a>\n</td> <td><p>Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.gcd#torch.gcd\" title=\"torch.gcd\"><code>gcd</code></a>\n</td> <td><p>Computes the element-wise greatest common divisor (GCD) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.histc#torch.histc\" title=\"torch.histc\"><code>histc</code></a>\n</td> <td><p>Computes the histogram of a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.meshgrid#torch.meshgrid\" title=\"torch.meshgrid\"><code>meshgrid</code></a>\n</td> <td><p>Take <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span></span> </span> tensors, each of which can be either scalar or 1-dimensional vector, and create <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span></span> </span> N-dimensional grids, where the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span> <sup>th</sup> grid is defined by expanding the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span> <sup>th</sup> input over dimensions defined by other inputs.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lcm#torch.lcm\" title=\"torch.lcm\"><code>lcm</code></a>\n</td> <td><p>Computes the element-wise least common multiple (LCM) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logcumsumexp#torch.logcumsumexp\" title=\"torch.logcumsumexp\"><code>logcumsumexp</code></a>\n</td> <td><p>Returns the logarithm of the cumulative summation of the exponentiation of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ravel#torch.ravel\" title=\"torch.ravel\"><code>ravel</code></a>\n</td> <td><p>Return a contiguous flattened tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.renorm#torch.renorm\" title=\"torch.renorm\"><code>renorm</code></a>\n</td> <td><p>Returns a tensor where each sub-tensor of <code>input</code> along dimension <code>dim</code> is normalized such that the <code>p</code>-norm of the sub-tensor is lower than the value <code>maxnorm</code></p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.repeat_interleave#torch.repeat_interleave\" title=\"torch.repeat_interleave\"><code>repeat_interleave</code></a>\n</td> <td><p>Repeat elements of a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.roll#torch.roll\" title=\"torch.roll\"><code>roll</code></a>\n</td> <td><p>Roll the tensor along the given dimension(s).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.searchsorted#torch.searchsorted\" title=\"torch.searchsorted\"><code>searchsorted</code></a>\n</td> <td><p>Find the indices from the <em>innermost</em> dimension of <code>sorted_sequence</code> such that, if the corresponding values in <code>values</code> were inserted before the indices, the order of the corresponding <em>innermost</em> dimension within <code>sorted_sequence</code> would be preserved.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tensordot#torch.tensordot\" title=\"torch.tensordot\"><code>tensordot</code></a>\n</td> <td><p>Returns a contraction of a and b over multiple dimensions.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trace#torch.trace\" title=\"torch.trace\"><code>trace</code></a>\n</td> <td><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tril#torch.tril\" title=\"torch.tril\"><code>tril</code></a>\n</td> <td><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.tril_indices#torch.tril_indices\" title=\"torch.tril_indices\"><code>tril_indices</code></a>\n</td> <td><p>Returns the indices of the lower triangular part of a <code>row</code>-by- <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triu#torch.triu\" title=\"torch.triu\"><code>triu</code></a>\n</td> <td><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triu_indices#torch.triu_indices\" title=\"torch.triu_indices\"><code>triu_indices</code></a>\n</td> <td><p>Returns the indices of the upper triangular part of a <code>row</code> by <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vander#torch.vander\" title=\"torch.vander\"><code>vander</code></a>\n</td> <td><p>Generates a Vandermonde matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.view_as_real#torch.view_as_real\" title=\"torch.view_as_real\"><code>view_as_real</code></a>\n</td> <td><p>Returns a view of <code>input</code> as a real tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.view_as_complex#torch.view_as_complex\" title=\"torch.view_as_complex\"><code>view_as_complex</code></a>\n</td> <td><p>Returns a view of <code>input</code> as a complex tensor.</p></td> </tr>  </table>   <h3 id=\"blas-and-lapack-operations\">BLAS and LAPACK Operations</h3> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addbmm#torch.addbmm\" title=\"torch.addbmm\"><code>addbmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>batch1</code> and <code>batch2</code>, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addmm#torch.addmm\" title=\"torch.addmm\"><code>addmm</code></a>\n</td> <td><p>Performs a matrix multiplication of the matrices <code>mat1</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addmv#torch.addmv\" title=\"torch.addmv\"><code>addmv</code></a>\n</td> <td><p>Performs a matrix-vector product of the matrix <code>mat</code> and the vector <code>vec</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.addr#torch.addr\" title=\"torch.addr\"><code>addr</code></a>\n</td> <td><p>Performs the outer-product of vectors <code>vec1</code> and <code>vec2</code> and adds it to the matrix <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.baddbmm#torch.baddbmm\" title=\"torch.baddbmm\"><code>baddbmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices in <code>batch1</code> and <code>batch2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.bmm#torch.bmm\" title=\"torch.bmm\"><code>bmm</code></a>\n</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.chain_matmul#torch.chain_matmul\" title=\"torch.chain_matmul\"><code>chain_matmul</code></a>\n</td> <td><p>Returns the matrix product of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span></span> </span> 2-D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky#torch.cholesky\" title=\"torch.cholesky\"><code>cholesky</code></a>\n</td> <td><p>Computes the Cholesky decomposition of a symmetric positive-definite matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span> </span> or for batches of symmetric positive-definite matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky_inverse#torch.cholesky_inverse\" title=\"torch.cholesky_inverse\"><code>cholesky_inverse</code></a>\n</td> <td><p>Computes the inverse of a symmetric positive-definite matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span> </span> using its Cholesky factor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span></span> </span>: returns matrix <code>inv</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.cholesky_solve#torch.cholesky_solve\" title=\"torch.cholesky_solve\"><code>cholesky_solve</code></a>\n</td> <td><p>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>u</mi></mrow><annotation encoding=\"application/x-tex\">u</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.dot#torch.dot\" title=\"torch.dot\"><code>dot</code></a>\n</td> <td><p>Computes the dot product of two 1D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.eig#torch.eig\" title=\"torch.eig\"><code>eig</code></a>\n</td> <td><p>Computes the eigenvalues and eigenvectors of a real square matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>geqrf</code></a>\n</td> <td><p>This is a low-level function for calling LAPACK directly.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ger#torch.ger\" title=\"torch.ger\"><code>ger</code></a>\n</td> <td><p>Alias of <a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>torch.outer()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.inner#torch.inner\" title=\"torch.inner\"><code>inner</code></a>\n</td> <td><p>Computes the dot product for 1D tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.inverse#torch.inverse\" title=\"torch.inverse\"><code>inverse</code></a>\n</td> <td><p>Takes the inverse of the square matrix <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.det#torch.det\" title=\"torch.det\"><code>det</code></a>\n</td> <td><p>Calculates determinant of a square matrix or batches of square matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.logdet#torch.logdet\" title=\"torch.logdet\"><code>logdet</code></a>\n</td> <td><p>Calculates log determinant of a square matrix or batches of square matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.slogdet#torch.slogdet\" title=\"torch.slogdet\"><code>slogdet</code></a>\n</td> <td><p>Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lstsq#torch.lstsq\" title=\"torch.lstsq\"><code>lstsq</code></a>\n</td> <td><p>Computes the solution to the least squares and least norm problems for a full rank matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span> </span> of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>×</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(m \\times n)</annotation></semantics></math></span></span> </span> and a matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span></span> </span> of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>m</mi><mo>×</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(m \\times k)</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu#torch.lu\" title=\"torch.lu\"><code>lu</code></a>\n</td> <td><p>Computes the LU factorization of a matrix or batches of matrices <code>A</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu_solve#torch.lu_solve\" title=\"torch.lu_solve\"><code>lu_solve</code></a>\n</td> <td><p>Returns the LU solve of the linear system <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">Ax = b</annotation></semantics></math></span></span> </span> using the partially pivoted LU factorization of A from <a class=\"reference internal\" href=\"generated/torch.lu#torch.lu\" title=\"torch.lu\"><code>torch.lu()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lu_unpack#torch.lu_unpack\" title=\"torch.lu_unpack\"><code>lu_unpack</code></a>\n</td> <td><p>Unpacks the data and pivots from a LU factorization of a tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matmul#torch.matmul\" title=\"torch.matmul\"><code>matmul</code></a>\n</td> <td><p>Matrix product of two tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matrix_power#torch.matrix_power\" title=\"torch.matrix_power\"><code>matrix_power</code></a>\n</td> <td><p>Returns the matrix raised to the power <code>n</code> for square matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matrix_rank#torch.matrix_rank\" title=\"torch.matrix_rank\"><code>matrix_rank</code></a>\n</td> <td><p>Returns the numerical rank of a 2-D tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.matrix_exp#torch.matrix_exp\" title=\"torch.matrix_exp\"><code>matrix_exp</code></a>\n</td> <td><p>Returns the matrix exponential.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mm#torch.mm\" title=\"torch.mm\"><code>mm</code></a>\n</td> <td><p>Performs a matrix multiplication of the matrices <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.mv#torch.mv\" title=\"torch.mv\"><code>mv</code></a>\n</td> <td><p>Performs a matrix-vector product of the matrix <code>input</code> and the vector <code>vec</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.orgqr#torch.orgqr\" title=\"torch.orgqr\"><code>orgqr</code></a>\n</td> <td><p>Computes the orthogonal matrix <code>Q</code> of a QR factorization, from the <code>(input, input2)</code> tuple returned by <a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>torch.geqrf()</code></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.ormqr#torch.ormqr\" title=\"torch.ormqr\"><code>ormqr</code></a>\n</td> <td><p>Multiplies <code>mat</code> (given by <code>input3</code>) by the orthogonal <code>Q</code> matrix of the QR factorization formed by <a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>torch.geqrf()</code></a> that is represented by <code>(a, tau)</code> (given by (<code>input</code>, <code>input2</code>)).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>outer</code></a>\n</td> <td><p>Outer product of <code>input</code> and <code>vec2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pinverse#torch.pinverse\" title=\"torch.pinverse\"><code>pinverse</code></a>\n</td> <td><p>Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.qr#torch.qr\" title=\"torch.qr\"><code>qr</code></a>\n</td> <td><p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>, and returns a namedtuple (Q, R) of tensors such that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">\\text{input} = Q R</annotation></semantics></math></span></span> </span> with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>Q</mi></mrow><annotation encoding=\"application/x-tex\">Q</annotation></semantics></math></span></span> </span> being an orthogonal matrix or batch of orthogonal matrices and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>R</mi></mrow><annotation encoding=\"application/x-tex\">R</annotation></semantics></math></span></span> </span> being an upper triangular matrix or batch of upper triangular matrices.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.solve#torch.solve\" title=\"torch.solve\"><code>solve</code></a>\n</td> <td><p>This function returns the solution to the system of linear equations represented by <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mi>X</mi><mo>=</mo><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">AX = B</annotation></semantics></math></span></span> </span> and the LU factorization of A, in order as a namedtuple <code>solution, LU</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.svd#torch.svd\" title=\"torch.svd\"><code>svd</code></a>\n</td> <td><p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.svd_lowrank#torch.svd_lowrank\" title=\"torch.svd_lowrank\"><code>svd_lowrank</code></a>\n</td> <td><p>Return the singular value decomposition <code>(U, S, V)</code> of a matrix, batches of matrices, or a sparse matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span> </span> such that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mi>S</mi><mo stretchy=\"false\">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding=\"application/x-tex\">A \\approx U diag(S) V^T</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.pca_lowrank#torch.pca_lowrank\" title=\"torch.pca_lowrank\"><code>pca_lowrank</code></a>\n</td> <td><p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.symeig#torch.symeig\" title=\"torch.symeig\"><code>symeig</code></a>\n</td> <td><p>This function returns eigenvalues and eigenvectors of a real symmetric matrix <code>input</code> or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.lobpcg#torch.lobpcg\" title=\"torch.lobpcg\"><code>lobpcg</code></a>\n</td> <td><p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.trapz#torch.trapz\" title=\"torch.trapz\"><code>trapz</code></a>\n</td> <td><p>Estimate <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∫</mo><mi>y</mi><mi>d</mi><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">\\int y\\,dx</annotation></semantics></math></span></span> </span> along <code>dim</code>, using the trapezoid rule.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.triangular_solve#torch.triangular_solve\" title=\"torch.triangular_solve\"><code>triangular_solve</code></a>\n</td> <td><p>Solves a system of equations with a triangular coefficient matrix <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span></span> </span> and multiple right-hand sides <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.vdot#torch.vdot\" title=\"torch.vdot\"><code>vdot</code></a>\n</td> <td><p>Computes the dot product of two 1D tensors.</p></td> </tr>  </table>    <h2 id=\"utilities\">Utilities</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi\" title=\"torch.compiled_with_cxx11_abi\"><code>compiled_with_cxx11_abi</code></a>\n</td> <td><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.result_type#torch.result_type\" title=\"torch.result_type\"><code>result_type</code></a>\n</td> <td><p>Returns the <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> that would result from performing an arithmetic operation on the provided input tensors.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.can_cast#torch.can_cast\" title=\"torch.can_cast\"><code>can_cast</code></a>\n</td> <td><p>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion <a class=\"reference internal\" href=\"tensor_attributes#type-promotion-doc\"><span class=\"std std-ref\">documentation</span></a>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.promote_types#torch.promote_types\" title=\"torch.promote_types\"><code>promote_types</code></a>\n</td> <td><p>Returns the <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> with the smallest size and scalar kind that is not smaller nor of lower kind than either <code>type1</code> or <code>type2</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms\" title=\"torch.use_deterministic_algorithms\"><code>use_deterministic_algorithms</code></a>\n</td> <td><p>Sets whether PyTorch operations must use “deterministic” algorithms.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled\" title=\"torch.are_deterministic_algorithms_enabled\"><code>are_deterministic_algorithms_enabled</code></a>\n</td> <td><p>Returns True if the global deterministic flag is turned on.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch._assert#torch._assert\" title=\"torch._assert\"><code>_assert</code></a>\n</td> <td><p>A wrapper around Python’s assert which is symbolically traceable.</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/torch.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/torch.html</a>\n  </p>\n</div>\n","cuda":"<h1 id=\"torch-cuda\">torch.cuda</h1> <p id=\"module-torch.cuda\">This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</p> <p>It is lazily initialized, so you can always import it, and use <a class=\"reference internal\" href=\"#torch.cuda.is_available\" title=\"torch.cuda.is_available\"><code>is_available()</code></a> to determine if your system supports CUDA.</p> <p><a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-semantics\"><span class=\"std std-ref\">CUDA semantics</span></a> has more details about working with CUDA.</p> <dl class=\"function\"> <dt id=\"torch.cuda.can_device_access_peer\">\n<code>torch.cuda.can_device_access_peer(device, peer_device)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#can_device_access_peer\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if peer access between two devices is possible.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.current_blas_handle\">\n<code>torch.cuda.current_blas_handle()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_blas_handle\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns cublasHandle_t pointer to current cuBLAS handle</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.current_device\">\n<code>torch.cuda.current_device()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_device\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the index of a currently selected device.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.current_stream\">\n<code>torch.cuda.current_stream(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_stream\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the currently selected <a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for a given device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns the currently selected <a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.default_stream\">\n<code>torch.cuda.default_stream(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#default_stream\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the default <a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for a given device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns the default <a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\"><code>Stream</code></a> for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.cuda.device\">\n<code>class torch.cuda.device(device)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that changes the selected device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – device index to select. It’s a no-op if this argument is a negative integer or <code>None</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.device_count\">\n<code>torch.cuda.device_count()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device_count\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the number of GPUs available.</p> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.cuda.device_of\">\n<code>class torch.cuda.device_of(obj)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device_of\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that changes the current device to that of given object.</p> <p>You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>obj</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><em>Storage</em>) – object allocated on the selected device.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_arch_list\">\n<code>torch.cuda.get_arch_list()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_arch_list\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns list CUDA architectures this library was compiled for.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_device_capability\">\n<code>torch.cuda.get_device_capability(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_capability\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gets the cuda capability of a device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>the major and minor cuda capability of the device</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>(<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>, <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>)</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_device_name\">\n<code>torch.cuda.get_device_name(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_name\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gets the name of a device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>the name of the device</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a></p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_device_properties\">\n<code>torch.cuda.get_device_properties(device)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_properties\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gets the properties of a device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – device for which to return the properties of the device.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>the properties of the device</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>_CudaDeviceProperties</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_gencode_flags\">\n<code>torch.cuda.get_gencode_flags()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_gencode_flags\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns NVCC gencode flags this library were compiled with.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.init\">\n<code>torch.cuda.init()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#init\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Initialize PyTorch’s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch’s CUDA methods automatically initialize CUDA state on-demand.</p> <p>Does nothing if the CUDA state is already initialized.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.ipc_collect\">\n<code>torch.cuda.ipc_collect()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#ipc_collect\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Force collects GPU memory after it has been released by CUDA IPC.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.is_available\">\n<code>torch.cuda.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating if CUDA is currently available.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.is_initialized\">\n<code>torch.cuda.is_initialized()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#is_initialized\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch’s CUDA state has been initialized.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.set_device\">\n<code>torch.cuda.set_device(device)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#set_device\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the current device.</p> <p>Usage of this function is discouraged in favor of <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a>. In most cases it’s better to use <code>CUDA_VISIBLE_DEVICES</code> environmental variable.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – selected device. This function is a no-op if this argument is negative.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.stream\">\n<code>torch.cuda.stream(stream)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#stream\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that selects a given stream.</p> <p>All CUDA kernels queued within its context will be enqueued on a selected stream.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>stream</strong> (<a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\">Stream</a>) – selected stream. This manager is a no-op if it’s <code>None</code>.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.synchronize\">\n<code>torch.cuda.synchronize(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#synchronize\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Waits for all kernels in all streams on a CUDA device to complete.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – device for which to synchronize. It uses the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>\n</dl>  <h2 id=\"random-number-generator\">Random Number Generator</h2> <dl class=\"function\"> <dt id=\"torch.cuda.get_rng_state\">\n<code>torch.cuda.get_rng_state(device='cuda')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#get_rng_state\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the random number generator state of the specified GPU as a ByteTensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – The device to return the RNG state of. Default: <code>'cuda'</code> (i.e., <code>torch.device('cuda')</code>, the current CUDA device).</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function eagerly initializes CUDA.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.get_rng_state_all\">\n<code>torch.cuda.get_rng_state_all()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#get_rng_state_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a list of ByteTensor representing the random number states of all devices.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.set_rng_state\">\n<code>torch.cuda.set_rng_state(new_state, device='cuda')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#set_rng_state\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the random number generator state of the specified GPU.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>new_state</strong> (<em>torch.ByteTensor</em>) – The desired state</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – The device to set the RNG state. Default: <code>'cuda'</code> (i.e., <code>torch.device('cuda')</code>, the current CUDA device).</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.set_rng_state_all\">\n<code>torch.cuda.set_rng_state_all(new_states)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#set_rng_state_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the random number generator state of all devices.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_states</strong> (<em>Iterable of torch.ByteTensor</em>) – The desired state for each device</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.manual_seed\">\n<code>torch.cuda.manual_seed(seed)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#manual_seed\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the seed for generating random numbers for the current GPU. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>seed</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The desired seed.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use <a class=\"reference internal\" href=\"#torch.cuda.manual_seed_all\" title=\"torch.cuda.manual_seed_all\"><code>manual_seed_all()</code></a>.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.manual_seed_all\">\n<code>torch.cuda.manual_seed_all(seed)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#manual_seed_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the seed for generating random numbers on all GPUs. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>seed</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The desired seed.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.seed\">\n<code>torch.cuda.seed()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#seed\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the seed for generating random numbers to a random number for the current GPU. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use <a class=\"reference internal\" href=\"#torch.cuda.seed_all\" title=\"torch.cuda.seed_all\"><code>seed_all()</code></a>.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.seed_all\">\n<code>torch.cuda.seed_all()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#seed_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets the seed for generating random numbers to a random number on all GPUs. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.initial_seed\">\n<code>torch.cuda.initial_seed()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#initial_seed\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the current random seed of the current GPU.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function eagerly initializes CUDA.</p> </div> </dd>\n</dl>   <h2 id=\"communication-collectives\">Communication collectives</h2> <dl class=\"function\"> <dt id=\"torch.cuda.comm.broadcast\">\n<code>torch.cuda.comm.broadcast(tensor, devices=None, *, out=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#broadcast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts a tensor to specified GPU devices.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – tensor to broadcast. Can be on CPU or GPU.</li> <li>\n<strong>devices</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em><em>, </em><em>optional</em>) – an iterable of GPU devices, among which to broadcast.</li> <li>\n<strong>out</strong> (<em>Sequence</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to store output results.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Exactly one of <code>devices</code> and <code>out</code> must be specified.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n\n<ul class=\"simple\"> <li>\n<dl class=\"simple\"> <dt>\n<code>If devices is specified,</code> </dt>\n<dd>\n<p>a tuple containing copies of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </li> <li>\n<dl class=\"simple\"> <dt>\n<code>If out is specified,</code> </dt>\n<dd>\n<p>a tuple containing <code>out</code> tensors, each containing a copy of <code>tensor</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.comm.broadcast_coalesced\">\n<code>torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#broadcast_coalesced\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensors</strong> (<em>sequence</em>) – tensors to broadcast. Must be on the same device, either CPU or GPU.</li> <li>\n<strong>devices</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – an iterable of GPU devices, among which to broadcast.</li> <li>\n<strong>buffer_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – maximum size of the buffer used for coalescing</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A tuple containing copies of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.comm.reduce_add\">\n<code>torch.cuda.comm.reduce_add(inputs, destination=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#reduce_add\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sums tensors from multiple GPUs.</p> <p>All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>inputs</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>) – an iterable of tensors to add.</li> <li>\n<strong>destination</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – a device on which the output will be placed (default: current device).</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A tensor containing an elementwise sum of all inputs, placed on the <code>destination</code> device.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.comm.scatter\">\n<code>torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#scatter\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Scatters tensor across multiple GPUs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – tensor to scatter. Can be on CPU or GPU.</li> <li>\n<strong>devices</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em><em>, </em><em>optional</em>) – an iterable of GPU devices, among which to scatter.</li> <li>\n<strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em><em>, </em><em>optional</em>) – sizes of chunks to be placed on each device. It should match <code>devices</code> in length and sums to <code>tensor.size(dim)</code>. If not specified, <code>tensor</code> will be divided into equal chunks.</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – A dimension along which to chunk <code>tensor</code>. Default: <code>0</code>.</li> <li>\n<strong>streams</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\">Stream</a><em>]</em><em>, </em><em>optional</em>) – an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized.</li> <li>\n<strong>out</strong> (<em>Sequence</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to store output results. Sizes of these tensors must match that of <code>tensor</code>, except for <code>dim</code>, where the total size must sum to <code>tensor.size(dim)</code>.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Exactly one of <code>devices</code> and <code>out</code> must be specified. When <code>out</code> is specified, <code>chunk_sizes</code> must not be specified and will be inferred from sizes of <code>out</code>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n\n<ul class=\"simple\"> <li>\n<dl class=\"simple\"> <dt>\n<code>If devices is specified,</code> </dt>\n<dd>\n<p>a tuple containing chunks of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </li> <li>\n<dl class=\"simple\"> <dt>\n<code>If out is specified,</code> </dt>\n<dd>\n<p>a tuple containing <code>out</code> tensors, each containing a chunk of <code>tensor</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.comm.gather\">\n<code>torch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#gather\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers tensors from multiple GPU devices.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensors</strong> (<em>Iterable</em><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>) – an iterable of tensors to gather. Tensor sizes in all dimensions other than <code>dim</code> have to match.</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – a dimension along which the tensors will be concatenated. Default: <code>0</code>.</li> <li>\n<strong>destination</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em>, or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – the output device. Can be CPU or CUDA. Default: the current CUDA device.</li> <li>\n<strong>out</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the tensor to store gather result. Its sizes must match those of <code>tensors</code>, except for <code>dim</code>, where the size must equal <code>sum(tensor.size(dim) for tensor in tensors)</code>. Can be on CPU or CUDA.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><code>destination</code> must not be specified when <code>out</code> is specified.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n\n<ul class=\"simple\"> <li>\n<dl class=\"simple\"> <dt>\n<code>If destination is specified,</code> </dt>\n<dd>\n<p>a tensor located on <code>destination</code> device, that is a result of concatenating <code>tensors</code> along <code>dim</code>.</p> </dd> </dl> </li> <li>\n<dl class=\"simple\"> <dt>\n<code>If out is specified,</code> </dt>\n<dd>\n<p>the <code>out</code> tensor, now containing results of concatenating <code>tensors</code> along <code>dim</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>\n</dl>   <h2 id=\"streams-and-events\">Streams and events</h2> <dl class=\"class\"> <dt id=\"torch.cuda.Stream\">\n<code>class torch.cuda.Stream</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Wrapper around a CUDA stream.</p> <p>A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-semantics\"><span class=\"std std-ref\">CUDA semantics</span></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – a device on which to allocate the stream. If <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default) or a negative integer, this will use the current device.</li> <li>\n<strong>priority</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Although CUDA versions &gt;= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.</p> </div> <dl class=\"method\"> <dt id=\"torch.cuda.Stream.query\">\n<code>query()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.query\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if all the work submitted has been completed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>A boolean indicating if all kernels in this stream are completed.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Stream.record_event\">\n<code>record_event(event=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.record_event\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Records an event.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>event</strong> (<a class=\"reference internal\" href=\"#torch.cuda.Event\" title=\"torch.cuda.Event\">Event</a><em>, </em><em>optional</em>) – event to record. If not given, a new one will be allocated.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Recorded event.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Stream.synchronize\">\n<code>synchronize()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.synchronize\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Wait for all the kernels in this stream to complete.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This is a wrapper around <code>cudaStreamSynchronize()</code>: see <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html\">CUDA Stream documentation</a> for more info.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Stream.wait_event\">\n<code>wait_event(event)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.wait_event\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Makes all future work submitted to the stream wait for an event.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>event</strong> (<a class=\"reference internal\" href=\"#torch.cuda.Event\" title=\"torch.cuda.Event\">Event</a>) – an event to wait for.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This is a wrapper around <code>cudaStreamWaitEvent()</code>: see <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html\">CUDA Stream documentation</a> for more info.</p> <p>This function returns without waiting for <code>event</code>: only future operations are affected.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Stream.wait_stream\">\n<code>wait_stream(stream)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.wait_stream\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Synchronizes with another stream.</p> <p>All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>stream</strong> (<a class=\"reference internal\" href=\"#torch.cuda.Stream\" title=\"torch.cuda.Stream\">Stream</a>) – a stream to synchronize.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function returns without waiting for currently enqueued kernels in <a class=\"reference internal\" href=\"#torch.cuda.stream\" title=\"torch.cuda.stream\"><code>stream</code></a>: only future operations are affected.</p> </div> </dd>\n</dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.cuda.Event\">\n<code>class torch.cuda.Event</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Wrapper around a CUDA event.</p> <p>CUDA events are synchronization markers that can be used to monitor the device’s progress, to accurately measure timing, and to synchronize CUDA streams.</p> <p>The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enable_timing</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – indicates if the event should measure time (default: <code>False</code>)</li> <li>\n<strong>blocking</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, <a class=\"reference internal\" href=\"#torch.cuda.Event.wait\" title=\"torch.cuda.Event.wait\"><code>wait()</code></a> will be blocking (default: <code>False</code>)</li> <li>\n<strong>interprocess</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – if <code>True</code>, the event can be shared between processes (default: <code>False</code>)</li> </ul> </dd> </dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.elapsed_time\">\n<code>elapsed_time(end_event)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.elapsed_time\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.from_ipc_handle\">\n<code>classmethod from_ipc_handle(device, handle)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.from_ipc_handle\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reconstruct an event from an IPC handle on the given device.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.ipc_handle\">\n<code>ipc_handle()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.ipc_handle\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns an IPC handle of this event. If not recorded yet, the event will use the current device.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.query\">\n<code>query()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.query\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if all work currently captured by event has completed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>A boolean indicating if all work currently captured by event has completed.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.record\">\n<code>record(stream=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.record\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Records the event in a given stream.</p> <p>Uses <code>torch.cuda.current_stream()</code> if no stream is specified. The stream’s device must match the event’s device.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.synchronize\">\n<code>synchronize()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.synchronize\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Waits for the event to complete.</p> <p>Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.</p>  <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This is a wrapper around <code>cudaEventSynchronize()</code>: see <a class=\"reference external\" href=\"https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html\">CUDA Event documentation</a> for more info.</p> </div>  </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.Event.wait\">\n<code>wait(stream=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.wait\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Makes all future work submitted to the given stream wait for this event.</p> <p>Use <code>torch.cuda.current_stream()</code> if no stream is specified.</p> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"memory-management\">Memory management</h2> <dl class=\"function\"> <dt id=\"torch.cuda.empty_cache\">\n<code>torch.cuda.empty_cache()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#empty_cache\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in <code>nvidia-smi</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><a class=\"reference internal\" href=\"#torch.cuda.empty_cache\" title=\"torch.cuda.empty_cache\"><code>empty_cache()</code></a> doesn’t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.list_gpu_processes\">\n<code>torch.cuda.list_gpu_processes(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#list_gpu_processes\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a human-readable printout of the running processes and their GPU memory use for a given device.</p> <p>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns printout for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_stats\">\n<code>torch.cuda.memory_stats(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_stats\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p> <p>The return value of this function is a dictionary of statistics, each of which is a non-negative integer.</p> <p>Core statistics:</p> <ul class=\"simple\"> <li>\n<code>\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: number of allocation requests received by the memory allocator.</li> <li>\n<code>\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: amount of allocated memory.</li> <li>\n<code>\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: number of reserved segments from <code>cudaMalloc()</code>.</li> <li>\n<code>\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: amount of reserved memory.</li> <li>\n<code>\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: number of active memory blocks.</li> <li>\n<code>\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: amount of active memory.</li> <li>\n<code>\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: number of inactive, non-releasable memory blocks.</li> <li>\n<code>\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\"</code>: amount of inactive, non-releasable memory.</li> </ul> <p>For these core statistics, values are broken down as follows.</p> <p>Pool type:</p> <ul class=\"simple\"> <li>\n<code>all</code>: combined statistics across all memory pools.</li> <li>\n<code>large_pool</code>: statistics for the large allocation pool (as of October 2019, for size &gt;= 1MB allocations).</li> <li>\n<code>small_pool</code>: statistics for the small allocation pool (as of October 2019, for size &lt; 1MB allocations).</li> </ul> <p>Metric type:</p> <ul class=\"simple\"> <li>\n<code>current</code>: current value of this metric.</li> <li>\n<code>peak</code>: maximum value of this metric.</li> <li>\n<code>allocated</code>: historical total increase in this metric.</li> <li>\n<code>freed</code>: historical total decrease in this metric.</li> </ul> <p>In addition to the core statistics, we also provide some simple event counters:</p> <ul class=\"simple\"> <li>\n<code>\"num_alloc_retries\"</code>: number of failed <code>cudaMalloc</code> calls that result in a cache flush and retry.</li> <li>\n<code>\"num_ooms\"</code>: number of out-of-memory errors thrown.</li> </ul> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistics for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_summary\">\n<code>torch.cuda.memory_summary(device=None, abbreviated=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_summary\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a human-readable printout of the current memory allocator statistics for a given device.</p> <p>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns printout for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</li> <li>\n<strong>abbreviated</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether to return an abbreviated summary (default: False).</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_snapshot\">\n<code>torch.cuda.memory_snapshot()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_snapshot\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a snapshot of the CUDA memory allocator state across all devices.</p> <p>Interpreting the output of this function requires familiarity with the memory allocator internals.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_allocated\">\n<code>torch.cuda.memory_allocated(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_allocated\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the current GPU memory occupied by tensors in bytes for a given device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This is likely less than the amount shown in <code>nvidia-smi</code> since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.max_memory_allocated\">\n<code>torch.cuda.max_memory_allocated(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_allocated\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p> <p>By default, this returns the peak allocated memory since the beginning of this program. <code>reset_peak_stats()</code> can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.reset_max_memory_allocated\">\n<code>torch.cuda.reset_max_memory_allocated(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#reset_max_memory_allocated\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</p> <p>See <a class=\"reference internal\" href=\"#torch.cuda.max_memory_allocated\" title=\"torch.cuda.max_memory_allocated\"><code>max_memory_allocated()</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function now calls <code>reset_peak_memory_stats()</code>, which resets /all/ peak memory stats.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_reserved\">\n<code>torch.cuda.memory_reserved(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_reserved\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.max_memory_reserved\">\n<code>torch.cuda.max_memory_reserved(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_reserved\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p> <p>By default, this returns the peak cached memory since the beginning of this program. <code>reset_peak_stats()</code> can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.set_per_process_memory_fraction\">\n<code>torch.cuda.set_per_process_memory_fraction(fraction, device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#set_per_process_memory_fraction\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>fraction</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – Range: 0~1. Allowed memory equals total_memory * fraction.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. If it is <code>None</code> the default CUDA device is used.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In general, the total available free memory is less than the total capacity.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.memory_cached\">\n<code>torch.cuda.memory_cached(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_cached\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Deprecated; see <a class=\"reference internal\" href=\"#torch.cuda.memory_reserved\" title=\"torch.cuda.memory_reserved\"><code>memory_reserved()</code></a>.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.max_memory_cached\">\n<code>torch.cuda.max_memory_cached(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_cached\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Deprecated; see <a class=\"reference internal\" href=\"#torch.cuda.max_memory_reserved\" title=\"torch.cuda.max_memory_reserved\"><code>max_memory_reserved()</code></a>.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.reset_max_memory_cached\">\n<code>torch.cuda.reset_max_memory_cached(device=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#reset_max_memory_cached\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</p> <p>See <a class=\"reference internal\" href=\"#torch.cuda.max_memory_cached\" title=\"torch.cuda.max_memory_cached\"><code>max_memory_cached()</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class=\"reference internal\" href=\"#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>current_device()</code></a>, if <a class=\"reference internal\" href=\"#torch.cuda.device\" title=\"torch.cuda.device\"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function now calls <code>reset_peak_memory_stats()</code>, which resets /all/ peak memory stats.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management\"><span class=\"std std-ref\">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>\n</dl>   <h2 id=\"nvidia-tools-extension-nvtx\">NVIDIA Tools Extension (NVTX)</h2> <dl class=\"function\"> <dt id=\"torch.cuda.nvtx.mark\">\n<code>torch.cuda.nvtx.mark(msg)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#mark\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Describe an instantaneous event that occurred at some point.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with the event.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.nvtx.range_push\">\n<code>torch.cuda.nvtx.range_push(msg)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#range_push\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with range</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.nvtx.range_pop\">\n<code>torch.cuda.nvtx.range_pop()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#range_pop\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.</p> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/cuda.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/cuda.html</a>\n  </p>\n</div>\n","amp":"<h1 id=\"automatic-mixed-precision-package-torch-cuda-amp\">Automatic Mixed Precision package - torch.cuda.amp</h1> <p id=\"module-torch.cuda.amp\"><code>torch.cuda.amp</code> provides convenience methods for mixed precision, where some operations use the <code>torch.float32</code> (<code>float</code>) datatype and other operations use <code>torch.float16</code> (<code>half</code>). Some ops, like linear layers and convolutions, are much faster in <code>float16</code>. Other ops, like reductions, often require the dynamic range of <code>float32</code>. Mixed precision tries to match each op to its appropriate datatype.</p> <p>Ordinarily, “automatic mixed precision training” uses <a class=\"reference internal\" href=\"#torch.cuda.amp.autocast\" title=\"torch.cuda.amp.autocast\"><code>torch.cuda.amp.autocast</code></a> and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>torch.cuda.amp.GradScaler</code></a> together, as shown in the <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#amp-examples\"><span class=\"std std-ref\">Automatic Mixed Precision examples</span></a> and <a class=\"reference external\" href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\">Automatic Mixed Precision recipe</a>. However, <a class=\"reference internal\" href=\"#torch.cuda.amp.autocast\" title=\"torch.cuda.amp.autocast\"><code>autocast</code></a> and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>GradScaler</code></a> are modular, and may be used separately if desired.</p>  <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"#autocasting\" id=\"id4\">Autocasting</a></li> <li><a class=\"reference internal\" href=\"#gradient-scaling\" id=\"id5\">Gradient Scaling</a></li> <li>\n<p><a class=\"reference internal\" href=\"#autocast-op-reference\" id=\"id6\">Autocast Op Reference</a></p> <ul> <li><a class=\"reference internal\" href=\"#op-eligibility\" id=\"id7\">Op Eligibility</a></li> <li>\n<p><a class=\"reference internal\" href=\"#op-specific-behavior\" id=\"id8\">Op-Specific Behavior</a></p> <ul> <li><a class=\"reference internal\" href=\"#ops-that-can-autocast-to-float16\" id=\"id9\">Ops that can autocast to <code>float16</code></a></li> <li><a class=\"reference internal\" href=\"#ops-that-can-autocast-to-float32\" id=\"id10\">Ops that can autocast to <code>float32</code></a></li> <li><a class=\"reference internal\" href=\"#ops-that-promote-to-the-widest-input-type\" id=\"id11\">Ops that promote to the widest input type</a></li> <li><a class=\"reference internal\" href=\"#prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\" id=\"id12\">Prefer <code>binary_cross_entropy_with_logits</code> over <code>binary_cross_entropy</code></a></li> </ul> </li> </ul> </li> </ul>   <h2 id=\"id1\">Autocasting</h2> <dl class=\"class\" id=\"autocasting\"> <dt id=\"torch.cuda.amp.autocast\">\n<code>class torch.cuda.amp.autocast(enabled=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/autocast_mode.html#autocast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Instances of <a class=\"reference internal\" href=\"#torch.cuda.amp.autocast\" title=\"torch.cuda.amp.autocast\"><code>autocast</code></a> serve as context managers or decorators that allow regions of your script to run in mixed precision.</p> <p>In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the <a class=\"reference internal\" href=\"#autocast-op-reference\"><span class=\"std std-ref\">Autocast Op Reference</span></a> for details.</p> <p>When entering an autocast-enabled region, Tensors may be any type. You should not call <code>.half()</code> on your model(s) or inputs when using autocasting.</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.autocast\" title=\"torch.cuda.amp.autocast\"><code>autocast</code></a> should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.</p> <p>Example:</p> <pre data-language=\"python\"># Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n</pre> <p>See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#amp-examples\"><span class=\"std std-ref\">Automatic Mixed Precision examples</span></a> for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.autocast\" title=\"torch.cuda.amp.autocast\"><code>autocast</code></a> can also be used as a decorator, e.g., on the <code>forward</code> method of your model:</p> <pre data-language=\"python\">class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n</pre> <p>Floating-point Tensors produced in an autocast-enabled region may be <code>float16</code>. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to <code>float32</code> (or other dtype if desired). If a Tensor from the autocast region is already <code>float32</code>, the cast is a no-op, and incurs no additional overhead. Example:</p> <pre data-language=\"python\"># Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n</pre> <p>Type mismatch errors <em>in</em> an autocast-enabled region are a bug; if this is what you observe, please file an issue.</p> <p><code>autocast(enabled=False)</code> subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular <code>dtype</code>. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to <code>dtype</code> before use:</p> <pre data-language=\"python\"># Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n</pre> <p>The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel</code></a> and <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel</code></a> when used with more than one GPU per process (see <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#amp-multigpu\"><span class=\"std std-ref\">Working with Multiple GPUs</span></a>).</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – Whether autocasting should be enabled in the region.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.amp.custom_fwd\">\n<code>torch.cuda.amp.custom_fwd(fwd=None, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/autocast_mode.html#custom_fwd\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Helper decorator for <code>forward</code> methods of custom autograd functions (subclasses of <a class=\"reference internal\" href=\"autograd#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>torch.autograd.Function</code></a>). See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#amp-custom-examples\"><span class=\"std std-ref\">example page</span></a> for more detail.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>cast_inputs</strong> (<code>torch.dtype</code> or None, optional, default=None) – If not <code>None</code>, when <code>forward</code> runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes <code>forward</code> with autocast disabled. If <code>None</code>, <code>forward</code>’s internal ops execute with the current autocast state.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If the decorated <code>forward</code> is called outside an autocast-enabled region, <a class=\"reference internal\" href=\"#torch.cuda.amp.custom_fwd\" title=\"torch.cuda.amp.custom_fwd\"><code>custom_fwd</code></a> is a no-op and <code>cast_inputs</code> has no effect.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.cuda.amp.custom_bwd\">\n<code>torch.cuda.amp.custom_bwd(bwd)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/autocast_mode.html#custom_bwd\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Helper decorator for backward methods of custom autograd functions (subclasses of <a class=\"reference internal\" href=\"autograd#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>torch.autograd.Function</code></a>). Ensures that <code>backward</code> executes with the same autocast state as <code>forward</code>. See the <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#amp-custom-examples\"><span class=\"std std-ref\">example page</span></a> for more detail.</p> </dd>\n</dl>   <h2 id=\"id2\">Gradient Scaling</h2> <p id=\"gradient-scaling\">If the forward pass for a particular op has <code>float16</code> inputs, the backward pass for that op will produce <code>float16</code> gradients. Gradient values with small magnitudes may not be representable in <code>float16</code>. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.</p> <p>To prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.</p> <p>Each parameter’s gradient (<code>.grad</code> attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.</p> <dl class=\"class\"> <dt id=\"torch.cuda.amp.GradScaler\">\n<code>class torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.get_backoff_factor\">\n<code>get_backoff_factor()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_backoff_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the scale backoff factor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.get_growth_factor\">\n<code>get_growth_factor()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the scale growth factor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.get_growth_interval\">\n<code>get_growth_interval()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_growth_interval\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python int containing the growth interval.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.get_scale\">\n<code>get_scale()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.get_scale\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a Python float containing the current scale, or 1.0 if scaling is disabled.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.get_scale\" title=\"torch.cuda.amp.GradScaler.get_scale\"><code>get_scale()</code></a> incurs a CPU-GPU sync.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.is_enabled\">\n<code>is_enabled()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.is_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating whether this instance is enabled.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.load_state_dict\">\n<code>load_state_dict(state_dict)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.load_state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Loads the scaler state. If this instance is disabled, <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.load_state_dict\" title=\"torch.cuda.amp.GradScaler.load_state_dict\"><code>load_state_dict()</code></a> is a no-op.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>state_dict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.9)\">dict</a>) – scaler state. Should be an object returned from a call to <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.state_dict\" title=\"torch.cuda.amp.GradScaler.state_dict\"><code>state_dict()</code></a>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.scale\">\n<code>scale(outputs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.scale\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Multiplies (‘scales’) a tensor or list of tensors by the scale factor.</p> <p>Returns scaled outputs. If this instance of <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler\" title=\"torch.cuda.amp.GradScaler\"><code>GradScaler</code></a> is not enabled, outputs are returned unmodified.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>outputs</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><em>iterable of Tensors</em>) – Outputs to scale.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.set_backoff_factor\">\n<code>set_backoff_factor(new_factor)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_backoff_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – Value to use as the new scale backoff factor.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.set_growth_factor\">\n<code>set_growth_factor(new_factor)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_factor\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – Value to use as the new scale growth factor.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.set_growth_interval\">\n<code>set_growth_interval(new_interval)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.set_growth_interval\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_interval</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Value to use as the new growth interval.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.state_dict\">\n<code>state_dict()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.state_dict\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the state of the scaler as a <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#dict\" title=\"(in Python v3.9)\"><code>dict</code></a>. It contains five entries:</p> <ul class=\"simple\"> <li>\n<code>\"scale\"</code> - a Python float containing the current scale</li> <li>\n<code>\"growth_factor\"</code> - a Python float containing the current growth factor</li> <li>\n<code>\"backoff_factor\"</code> - a Python float containing the current backoff factor</li> <li>\n<code>\"growth_interval\"</code> - a Python int containing the current growth interval</li> <li>\n<code>\"_growth_tracker\"</code> - a Python int containing the number of recent consecutive unskipped steps.</li> </ul> <p>If this instance is not enabled, returns an empty dict.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you wish to checkpoint the scaler’s state after a particular iteration, <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.state_dict\" title=\"torch.cuda.amp.GradScaler.state_dict\"><code>state_dict()</code></a> should be called after <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.update\" title=\"torch.cuda.amp.GradScaler.update\"><code>update()</code></a>.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.step\">\n<code>step(optimizer, *args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.step\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> carries out the following two operations:</p> <ol class=\"arabic simple\"> <li>Internally invokes <code>unscale_(optimizer)</code> (unless <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> was explicitly called for <code>optimizer</code> earlier in the iteration). As part of the <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a>, gradients are checked for infs/NaNs.</li> <li>If no inf/NaN gradients are found, invokes <code>optimizer.step()</code> using the unscaled gradients. Otherwise, <code>optimizer.step()</code> is skipped to avoid corrupting the params.</li> </ol> <p><code>*args</code> and <code>**kwargs</code> are forwarded to <code>optimizer.step()</code>.</p> <p>Returns the return value of <code>optimizer.step(*args, **kwargs)</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>optimizer</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\">torch.optim.Optimizer</a>) – Optimizer that applies the gradients.</li> <li>\n<strong>args</strong> – Any arguments.</li> <li>\n<strong>kwargs</strong> – Any keyword arguments.</li> </ul> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Closure use is not currently supported.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.unscale_\">\n<code>unscale_(optimizer)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.unscale_\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Divides (“unscales”) the optimizer’s gradient tensors by the scale factor.</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> is optional, serving cases where you need to <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/amp_examples.html#working-with-unscaled-gradients\"><span class=\"std std-ref\">modify or inspect gradients</span></a> between the backward pass(es) and <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a>. If <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> is not called explicitly, gradients will be unscaled automatically during <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a>.</p> <p>Simple example, using <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> to enable clipping of unscaled gradients:</p> <pre data-language=\"python\">...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n</pre> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>optimizer</strong> (<a class=\"reference internal\" href=\"optim#torch.optim.Optimizer\" title=\"torch.optim.Optimizer\">torch.optim.Optimizer</a>) – Optimizer that owns the gradients to be unscaled.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> does not incur a CPU-GPU sync.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> should only be called once per optimizer per <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> call, and only after all gradients for that optimizer’s assigned parameters have been accumulated. Calling <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> twice for a given optimizer between each <a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.step\" title=\"torch.cuda.amp.GradScaler.step\"><code>step()</code></a> triggers a RuntimeError.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.unscale_\" title=\"torch.cuda.amp.GradScaler.unscale_\"><code>unscale_()</code></a> may unscale sparse gradients out of place, replacing the <code>.grad</code> attribute.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.cuda.amp.GradScaler.update\">\n<code>update(new_scale=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/cuda/amp/grad_scaler.html#GradScaler.update\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Updates the scale factor.</p> <p>If any optimizer steps were skipped the scale is multiplied by <code>backoff_factor</code> to reduce it. If <code>growth_interval</code> unskipped iterations occurred consecutively, the scale is multiplied by <code>growth_factor</code> to increase it.</p> <p>Passing <code>new_scale</code> sets the scale directly.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>new_scale</strong> (float or <code>torch.cuda.FloatTensor</code>, optional, default=None) – New scale factor.</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.cuda.amp.GradScaler.update\" title=\"torch.cuda.amp.GradScaler.update\"><code>update()</code></a> should only be called at the end of the iteration, after <code>scaler.step(optimizer)</code> has been invoked for all optimizers used this iteration.</p> </div> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"id3\">Autocast Op Reference</h2>  <h3 id=\"autocast-eligibility\">Op Eligibility</h3> <p id=\"autocast-op-reference\">Only CUDA ops are eligible for autocasting.</p> <p>Ops that run in <code>float64</code> or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.</p> <p>Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an <code>out=...</code> Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region <code>a.addmm(b, c)</code> can autocast, but <code>a.addmm_(b, c)</code> and <code>a.addmm(b, c, out=d)</code> cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.</p> <p>Ops called with an explicit <code>dtype=...</code> argument are not eligible, and will produce output that respects the <code>dtype</code> argument.</p>   <h3 id=\"op-specific-behavior\">Op-Specific Behavior</h3> <p>The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a <a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>torch.nn.Module</code></a>, as a function, or as a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</p> <p>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.</p> <p>If an op is unlisted, we assume it’s numerically stable in <code>float16</code>. If you believe an unlisted op is numerically unstable in <code>float16</code>, please file an issue.</p>  <h4 id=\"ops-that-can-autocast-to-float16\">Ops that can autocast to <code>float16</code>\n</h4> <p><code>__matmul__</code>, <code>addbmm</code>, <code>addmm</code>, <code>addmv</code>, <code>addr</code>, <code>baddbmm</code>, <code>bmm</code>, <code>chain_matmul</code>, <code>conv1d</code>, <code>conv2d</code>, <code>conv3d</code>, <code>conv_transpose1d</code>, <code>conv_transpose2d</code>, <code>conv_transpose3d</code>, <code>GRUCell</code>, <code>linear</code>, <code>LSTMCell</code>, <code>matmul</code>, <code>mm</code>, <code>mv</code>, <code>prelu</code>, <code>RNNCell</code></p>   <h4 id=\"ops-that-can-autocast-to-float32\">Ops that can autocast to <code>float32</code>\n</h4> <p><code>__pow__</code>, <code>__rdiv__</code>, <code>__rpow__</code>, <code>__rtruediv__</code>, <code>acos</code>, <code>asin</code>, <code>binary_cross_entropy_with_logits</code>, <code>cosh</code>, <code>cosine_embedding_loss</code>, <code>cdist</code>, <code>cosine_similarity</code>, <code>cross_entropy</code>, <code>cumprod</code>, <code>cumsum</code>, <code>dist</code>, <code>erfinv</code>, <code>exp</code>, <code>expm1</code>, <code>gelu</code>, <code>group_norm</code>, <code>hinge_embedding_loss</code>, <code>kl_div</code>, <code>l1_loss</code>, <code>layer_norm</code>, <code>log</code>, <code>log_softmax</code>, <code>log10</code>, <code>log1p</code>, <code>log2</code>, <code>margin_ranking_loss</code>, <code>mse_loss</code>, <code>multilabel_margin_loss</code>, <code>multi_margin_loss</code>, <code>nll_loss</code>, <code>norm</code>, <code>normalize</code>, <code>pdist</code>, <code>poisson_nll_loss</code>, <code>pow</code>, <code>prod</code>, <code>reciprocal</code>, <code>rsqrt</code>, <code>sinh</code>, <code>smooth_l1_loss</code>, <code>soft_margin_loss</code>, <code>softmax</code>, <code>softmin</code>, <code>softplus</code>, <code>sum</code>, <code>renorm</code>, <code>tan</code>, <code>triplet_margin_loss</code></p>   <h4 id=\"ops-that-promote-to-the-widest-input-type\">Ops that promote to the widest input type</h4> <p>These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are <code>float16</code>, the op runs in <code>float16</code>. If any of the inputs is <code>float32</code>, autocast casts all inputs to <code>float32</code> and runs the op in <code>float32</code>.</p> <p><code>addcdiv</code>, <code>addcmul</code>, <code>atan2</code>, <code>bilinear</code>, <code>cat</code>, <code>cross</code>, <code>dot</code>, <code>equal</code>, <code>index_put</code>, <code>stack</code>, <code>tensordot</code></p> <p>Some ops not listed here (e.g., binary ops like <code>add</code>) natively promote inputs without autocasting’s intervention. If inputs are a mixture of <code>float16</code> and <code>float32</code>, these ops run in <code>float32</code> and produce <code>float32</code> output, regardless of whether autocast is enabled.</p>   <h4 id=\"prefer-binary-cross-entropy-with-logits-over-binary-cross-entropy\">Prefer <code>binary_cross_entropy_with_logits</code> over <code>binary_cross_entropy</code>\n</h4> <p>The backward passes of <a class=\"reference internal\" href=\"nn.functional#torch.nn.functional.binary_cross_entropy\" title=\"torch.nn.functional.binary_cross_entropy\"><code>torch.nn.functional.binary_cross_entropy()</code></a> (and <a class=\"reference internal\" href=\"generated/torch.nn.bceloss#torch.nn.BCELoss\" title=\"torch.nn.BCELoss\"><code>torch.nn.BCELoss</code></a>, which wraps it) can produce gradients that aren’t representable in <code>float16</code>. In autocast-enabled regions, the forward input may be <code>float16</code>, which means the backward gradient must be representable in <code>float16</code> (autocasting <code>float16</code> forward inputs to <code>float32</code> doesn’t help, because that cast must be reversed in backward). Therefore, <code>binary_cross_entropy</code> and <code>BCELoss</code> raise an error in autocast-enabled regions.</p> <p>Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using <a class=\"reference internal\" href=\"nn.functional#torch.nn.functional.binary_cross_entropy_with_logits\" title=\"torch.nn.functional.binary_cross_entropy_with_logits\"><code>torch.nn.functional.binary_cross_entropy_with_logits()</code></a> or <a class=\"reference internal\" href=\"generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\" title=\"torch.nn.BCEWithLogitsLoss\"><code>torch.nn.BCEWithLogitsLoss</code></a>. <code>binary_cross_entropy_with_logits</code> and <code>BCEWithLogits</code> are safe to autocast.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/amp.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/amp.html</a>\n  </p>\n</div>\n","backends":"<h1 id=\"torch-backends\">torch.backends</h1> <p><code>torch.backends</code> controls the behavior of various backends that PyTorch supports.</p> <p>These backends include:</p> <ul class=\"simple\"> <li><code>torch.backends.cuda</code></li> <li><code>torch.backends.cudnn</code></li> <li><code>torch.backends.mkl</code></li> <li><code>torch.backends.mkldnn</code></li> <li><code>torch.backends.openmp</code></li> </ul>  <h2 id=\"torch-backends-cuda\">torch.backends.cuda</h2> <dl class=\"function\"> <dt id=\"torch.backends.cuda.is_built\">\n<code>torch.backends.cuda.is_built()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/cuda.html#is_built\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with CUDA support. Note that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cuda.matmul.allow_tf32\">\n<code>torch.backends.cuda.matmul.allow_tf32</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\"><code>bool</code></a> that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cuda.cufft_plan_cache\">\n<code>torch.backends.cuda.cufft_plan_cache</code> </dt> <dd>\n<p><code>cufft_plan_cache</code> caches the cuFFT plans</p> <dl class=\"attribute\"> <dt id=\"torch.backends.cuda.size\">\n<code>size</code> </dt> <dd>\n<p>A readonly <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\"><code>int</code></a> that shows the number of plans currently in the cuFFT plan cache.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"max_size\">\n<code>max_size</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\"><code>int</code></a> that controls cache capacity of cuFFT plan.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"clear\">\n<code>clear()</code> </dt> <dd>\n<p>Clears the cuFFT plan cache.</p> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"torch-backends-cudnn\">torch.backends.cudnn</h2> <dl class=\"function\"> <dt id=\"torch.backends.cudnn.version\">\n<code>torch.backends.cudnn.version()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/cudnn.html#version\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the version of cuDNN</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.backends.cudnn.is_available\">\n<code>torch.backends.cudnn.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/cudnn.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns a bool indicating if CUDNN is currently available.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cudnn.enabled\">\n<code>torch.backends.cudnn.enabled</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\"><code>bool</code></a> that controls whether cuDNN is enabled.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cudnn.allow_tf32\">\n<code>torch.backends.cudnn.allow_tf32</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\"><code>bool</code></a> that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat-32(TF32) on Ampere devices</span></a>.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cudnn.deterministic\">\n<code>torch.backends.cudnn.deterministic</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\"><code>bool</code></a> that, if True, causes cuDNN to only use deterministic convolution algorithms. See also <a class=\"reference internal\" href=\"generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled\" title=\"torch.are_deterministic_algorithms_enabled\"><code>torch.are_deterministic_algorithms_enabled()</code></a> and <a class=\"reference internal\" href=\"generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms\" title=\"torch.use_deterministic_algorithms\"><code>torch.use_deterministic_algorithms()</code></a>.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.backends.cudnn.benchmark\">\n<code>torch.backends.cudnn.benchmark</code> </dt> <dd>\n<p>A <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\"><code>bool</code></a> that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest.</p> </dd>\n</dl>   <h2 id=\"torch-backends-mkl\">torch.backends.mkl</h2> <dl class=\"function\"> <dt id=\"torch.backends.mkl.is_available\">\n<code>torch.backends.mkl.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/mkl.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with MKL support.</p> </dd>\n</dl>   <h2 id=\"torch-backends-mkldnn\">torch.backends.mkldnn</h2> <dl class=\"function\"> <dt id=\"torch.backends.mkldnn.is_available\">\n<code>torch.backends.mkldnn.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/mkldnn.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with MKL-DNN support.</p> </dd>\n</dl>   <h2 id=\"torch-backends-openmp\">torch.backends.openmp</h2> <dl class=\"function\"> <dt id=\"torch.backends.openmp.is_available\">\n<code>torch.backends.openmp.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/backends/openmp.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns whether PyTorch is built with OpenMP support.</p> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/backends.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/backends.html</a>\n  </p>\n</div>\n","futures":"<h1 id=\"futures-docs\">torch.futures</h1> <div class=\"admonition warning\" id=\"torch-futures\"> <p class=\"admonition-title\">Warning</p> <p>The <code>torch.futures</code> package is experimental and subject to change.</p> </div> <p>This package provides a <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> objects. Currently, the <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> type is primarily used by the <a class=\"reference internal\" href=\"rpc#distributed-rpc-framework\"><span class=\"std std-ref\">Distributed RPC Framework</span></a>.</p> <dl class=\"class\" id=\"module-torch.futures\"> <dt id=\"torch.futures.Future\">\n<code>class torch.futures.Future</code> </dt> <dd>\n<p>Wrapper around a <code>torch._C.Future</code> which encapsulates an asynchronous execution of a callable, e.g. <a class=\"reference internal\" href=\"rpc#torch.distributed.rpc.rpc_async\" title=\"torch.distributed.rpc.rpc_async\"><code>rpc_async()</code></a>. It also exposes a set of APIs to add callback functions and set results.</p> <dl class=\"method\"> <dt id=\"torch.futures.Future.add_done_callback\">\n<code>add_done_callback(self: torch._C.Future, arg0: function) → None</code> </dt> \n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.done\">\n<code>done()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#Future.done\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Return <code>True</code> if this <code>Future</code> is done. A <code>Future</code> is done if it has a result or an exception.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.set_exception\">\n<code>set_exception(result)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#Future.set_exception\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Set an exception for this <code>Future</code>, which will mark this <code>Future</code> as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this <code>Future</code>, the exception set here will be raised inline.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>result</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/exceptions.html#BaseException\" title=\"(in Python v3.9)\">BaseException</a>) – the exception for this <code>Future</code>.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut = torch.futures.Future()\n&gt;&gt;&gt; fut.set_exception(ValueError(\"foo\"))\n&gt;&gt;&gt; fut.wait()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Output:\n&gt;&gt;&gt; # This will run after the future has finished.\n&gt;&gt;&gt; ValueError: foo\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.set_result\">\n<code>set_result(result)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#Future.set_result\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Set the result for this <code>Future</code>, which will mark this <code>Future</code> as completed and trigger all attached callbacks. Note that a <code>Future</code> cannot be marked completed twice.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>result</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#object\" title=\"(in Python v3.9)\">object</a>) – the result object of this <code>Future</code>.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import threading\n&gt;&gt;&gt; import time\n&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; def slow_set_future(fut, value):\n&gt;&gt;&gt;     time.sleep(0.5)\n&gt;&gt;&gt;     fut.set_result(value)\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut = torch.futures.Future()\n&gt;&gt;&gt; t = threading.Thread(\n&gt;&gt;&gt;     target=slow_set_future,\n&gt;&gt;&gt;     args=(fut, torch.ones(2) * 3)\n&gt;&gt;&gt; )\n&gt;&gt;&gt; t.start()\n&gt;&gt;&gt;\n&gt;&gt;&gt; print(fut.wait())  # tensor([3., 3.])\n&gt;&gt;&gt; t.join()\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.then\">\n<code>then(callback)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#Future.then\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Append the given callback function to this <code>Future</code>, which will be run when the <code>Future</code> is completed. Multiple callbacks can be added to the same <code>Future</code>, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this <code>Future</code>. The callback function can use the <code>Future.wait()</code> API to get the value. Note that if this <code>Future</code> is already completed, the given callback will be run immediately inline.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>callback</strong> (<code>Callable</code>) – a <code>Callable</code> that takes this <code>Future</code> as the only argument.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A new <code>Future</code> object that holds the return value of the <code>callback</code> and will be marked as completed when the given <code>callback</code> finishes.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; def callback(fut):\n&gt;&gt;&gt;     print(f\"RPC return value is {fut.wait()}.\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut = torch.futures.Future()\n&gt;&gt;&gt; # The inserted callback will print the return value when\n&gt;&gt;&gt; # receiving the response from \"worker1\"\n&gt;&gt;&gt; cb_fut = fut.then(callback)\n&gt;&gt;&gt; chain_cb_fut = cb_fut.then(\n&gt;&gt;&gt;     lambda x : print(f\"Chained cb done. {x.wait()}\")\n&gt;&gt;&gt; )\n&gt;&gt;&gt; fut.set_result(5)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Outputs are:\n&gt;&gt;&gt; # RPC return value is 5.\n&gt;&gt;&gt; # Chained cb done. None\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.value\">\n<code>value(self: torch._C.Future) → object</code> </dt> \n</dl> <dl class=\"method\"> <dt id=\"torch.futures.Future.wait\">\n<code>wait()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#Future.wait\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Block until the value of this <code>Future</code> is ready.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>The value held by this <code>Future</code>. If the function (callback or RPC) creating the value has thrown an error, this <code>wait</code> method will also throw an error.</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.futures.collect_all\">\n<code>torch.futures.collect_all(futures)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#collect_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Collects the provided <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> objects into a single combined <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> that is completed when all of the sub-futures are completed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>futures</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a>) – a list of <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> objects.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Returns a <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> object to a list of the passed in Futures.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut0 = torch.futures.Future()\n&gt;&gt;&gt; fut1 = torch.futures.Future()\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut = torch.futures.collect_all([fut0, fut1])\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut0.set_result(0)\n&gt;&gt;&gt; fut1.set_result(1)\n&gt;&gt;&gt;\n&gt;&gt;&gt; fut_list = fut.wait()\n&gt;&gt;&gt; print(f\"fut0 result = {fut_list[0].wait()}\")\n&gt;&gt;&gt; print(f\"fut1 result = {fut_list[1].wait()}\")\n&gt;&gt;&gt; # outputs:\n&gt;&gt;&gt; # fut0 result = 0\n&gt;&gt;&gt; # fut1 result = 1\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.futures.wait_all\">\n<code>torch.futures.wait_all(futures)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/futures.html#wait_all\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Waits for all provided futures to be complete, and returns the list of completed values.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>futures</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a>) – a list of <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> object.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A list of the completed <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> results. This method will throw an error if <code>wait</code> on any <a class=\"reference internal\" href=\"#torch.futures.Future\" title=\"torch.futures.Future\"><code>Future</code></a> throws.</p> </dd> </dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/futures.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/futures.html</a>\n  </p>\n</div>\n","hub":"<h1 id=\"torch-hub\">torch.hub</h1> <p>Pytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.</p>  <h2 id=\"publishing-models\">Publishing models</h2> <p>Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a github repository by adding a simple <code>hubconf.py</code> file;</p> <p><code>hubconf.py</code> can have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).</p> <pre data-language=\"python\">def entrypoint_name(*args, **kwargs):\n    # args &amp; kwargs are optional, for models which take positional/keyword arguments.\n    ...\n</pre>  <h3 id=\"how-to-implement-an-entrypoint\">How to implement an entrypoint?</h3> <p>Here is a code snippet specifies an entrypoint for <code>resnet18</code> model if we expand the implementation in <code>pytorch/vision/hubconf.py</code>. In most case importing the right function in <code>hubconf.py</code> is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in <a class=\"reference external\" href=\"https://github.com/pytorch/vision/blob/master/hubconf.py\">pytorch/vision repo</a></p> <pre data-language=\"python\">dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n</pre> <ul class=\"simple\"> <li>\n<code>dependencies</code> variable is a <strong>list</strong> of package names required to <strong>load</strong> the model. Note this might be slightly different from dependencies required for training a model.</li> <li>\n<code>args</code> and <code>kwargs</code> are passed along to the real callable function.</li> <li>Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here.</li> <li>Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.</li> <li>Callables prefixed with underscore are considered as helper functions which won’t show up in <a class=\"reference internal\" href=\"#torch.hub.list\" title=\"torch.hub.list\"><code>torch.hub.list()</code></a>.</li> <li>Pretrained weights can either be stored locally in the github repo, or loadable by <a class=\"reference internal\" href=\"#torch.hub.load_state_dict_from_url\" title=\"torch.hub.load_state_dict_from_url\"><code>torch.hub.load_state_dict_from_url()</code></a>. If less than 2GB, it’s recommended to attach it to a <a class=\"reference external\" href=\"https://help.github.com/en/articles/distributing-large-binaries\">project release</a> and use the url from the release. In the example above <code>torchvision.models.resnet.resnet18</code> handles <code>pretrained</code>, alternatively you can put the following logic in the entrypoint definition.</li> </ul> <pre data-language=\"python\">if pretrained:\n    # For checkpoint saved in local github repo, e.g. &lt;RELATIVE_PATH_TO_CHECKPOINT&gt;=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, &lt;RELATIVE_PATH_TO_CHECKPOINT&gt;)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n</pre>   <h3 id=\"important-notice\">Important Notice</h3> <ul class=\"simple\"> <li>The published models should be at least in a branch/tag. It can’t be a random commit.</li> </ul>    <h2 id=\"loading-models-from-hub\">Loading models from Hub</h2> <p>Pytorch Hub provides convenient APIs to explore all available models in hub through <a class=\"reference internal\" href=\"#torch.hub.list\" title=\"torch.hub.list\"><code>torch.hub.list()</code></a>, show docstring and examples through <a class=\"reference internal\" href=\"#torch.hub.help\" title=\"torch.hub.help\"><code>torch.hub.help()</code></a> and load the pre-trained models using <a class=\"reference internal\" href=\"#torch.hub.load\" title=\"torch.hub.load\"><code>torch.hub.load()</code></a>.</p> <dl class=\"function\" id=\"module-torch.hub\"> <dt id=\"torch.hub.list\">\n<code>torch.hub.list(github, force_reload=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#list\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>List all entrypoints available in <code>github</code> hubconf.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>github</strong> (<em>string</em>) – a string with format “repo_owner/repo_name[:tag_name]” with an optional tag/branch. The default branch is <code>master</code> if not specified. Example: ‘pytorch/vision[:hub]’</li> <li>\n<strong>force_reload</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether to discard the existing cache and force a fresh download. Default is <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>a list of available entrypoint names</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>entrypoints</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.hub.help\">\n<code>torch.hub.help(github, model, force_reload=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#help\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Show the docstring of entrypoint <code>model</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>github</strong> (<em>string</em>) – a string with format &lt;repo_owner/repo_name[:tag_name]&gt; with an optional tag/branch. The default branch is <code>master</code> if not specified. Example: ‘pytorch/vision[:hub]’</li> <li>\n<strong>model</strong> (<em>string</em>) – a string of entrypoint name defined in repo’s hubconf.py</li> <li>\n<strong>force_reload</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether to discard the existing cache and force a fresh download. Default is <code>False</code>.</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.hub.load\">\n<code>torch.hub.load(repo_or_dir, model, *args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#load\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Load a model from a github repo or a local directory.</p> <p>Note: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.</p> <p>If <code>source</code> is <code>'github'</code>, <code>repo_or_dir</code> is expected to be of the form <code>repo_owner/repo_name[:tag_name]</code> with an optional tag/branch.</p> <p>If <code>source</code> is <code>'local'</code>, <code>repo_or_dir</code> is expected to be a path to a local directory.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>repo_or_dir</strong> (<em>string</em>) – repo name (<code>repo_owner/repo_name[:tag_name]</code>), if <code>source = 'github'</code>; or a path to a local directory, if <code>source = 'local'</code>.</li> <li>\n<strong>model</strong> (<em>string</em>) – the name of a callable (entrypoint) defined in the repo/dir’s <code>hubconf.py</code>.</li> <li>\n<strong>*args</strong> (<em>optional</em>) – the corresponding args for callable <code>model</code>.</li> <li>\n<strong>source</strong> (<em>string</em><em>, </em><em>optional</em>) – <code>'github'</code> | <code>'local'</code>. Specifies how <code>repo_or_dir</code> is to be interpreted. Default is <code>'github'</code>.</li> <li>\n<strong>force_reload</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether to force a fresh download of the github repo unconditionally. Does not have any effect if <code>source = 'local'</code>. Default is <code>False</code>.</li> <li>\n<strong>verbose</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if <code>source = 'local'</code>. Default is <code>True</code>.</li> <li>\n<strong>**kwargs</strong> (<em>optional</em>) – the corresponding kwargs for callable <code>model</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The output of the <code>model</code> callable when called with the given <code>*args</code> and <code>**kwargs</code>.</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; # from a github repo\n&gt;&gt;&gt; repo = 'pytorch/vision'\n&gt;&gt;&gt; model = torch.hub.load(repo, 'resnet50', pretrained=True)\n&gt;&gt;&gt; # from a local directory\n&gt;&gt;&gt; path = '/some/local/path/pytorch/vision'\n&gt;&gt;&gt; model = torch.hub.load(path, 'resnet50', pretrained=True)\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.hub.download_url_to_file\">\n<code>torch.hub.download_url_to_file(url, dst, hash_prefix=None, progress=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#download_url_to_file\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Download object at the given URL to a local path.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>url</strong> (<em>string</em>) – URL of the object to download</li> <li>\n<strong>dst</strong> (<em>string</em>) – Full path where object will be saved, e.g. <code>/tmp/temporary_file</code>\n</li> <li>\n<strong>hash_prefix</strong> (<em>string</em><em>, </em><em>optional</em>) – If not None, the SHA256 downloaded file should start with <code>hash_prefix</code>. Default: None</li> <li>\n<strong>progress</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether or not to display a progress bar to stderr Default: True</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.hub.load_state_dict_from_url\">\n<code>torch.hub.load_state_dict_from_url(url, model_dir=None, map_location=None, progress=True, check_hash=False, file_name=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#load_state_dict_from_url\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Loads the Torch serialized object at the given URL.</p> <p>If downloaded file is a zip file, it will be automatically decompressed.</p> <p>If the object is already present in <code>model_dir</code>, it’s deserialized and returned. The default value of <code>model_dir</code> is <code>&lt;hub_dir&gt;/checkpoints</code> where <code>hub_dir</code> is the directory returned by <a class=\"reference internal\" href=\"#torch.hub.get_dir\" title=\"torch.hub.get_dir\"><code>get_dir()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>url</strong> (<em>string</em>) – URL of the object to download</li> <li>\n<strong>model_dir</strong> (<em>string</em><em>, </em><em>optional</em>) – directory in which to save the object</li> <li>\n<strong>map_location</strong> (<em>optional</em>) – a function or a dict specifying how to remap storage locations (see torch.load)</li> <li>\n<strong>progress</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – whether or not to display a progress bar to stderr. Default: True</li> <li>\n<strong>check_hash</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If True, the filename part of the URL should follow the naming convention <code>filename-&lt;sha256&gt;.ext</code> where <code>&lt;sha256&gt;</code> is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False</li> <li>\n<strong>file_name</strong> (<em>string</em><em>, </em><em>optional</em>) – name for the downloaded file. Filename from <code>url</code> will be used if not set.</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n</pre> </dd>\n</dl>  <h3 id=\"running-a-loaded-model\">Running a loaded model:</h3> <p>Note that <code>*args</code> and <code>**kwargs</code> in <a class=\"reference internal\" href=\"#torch.hub.load\" title=\"torch.hub.load\"><code>torch.hub.load()</code></a> are used to <strong>instantiate</strong> a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is</p> <ul class=\"simple\"> <li>\n<code>dir(model)</code> to see all available methods of the model.</li> <li>\n<code>help(model.foo)</code> to check what arguments <code>model.foo</code> takes to run</li> </ul> <p>To help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.</p>   <h3 id=\"where-are-my-downloaded-models-saved\">Where are my downloaded models saved?</h3> <p>The locations are used in the order of</p> <ul class=\"simple\"> <li>Calling <code>hub.set_dir(&lt;PATH_TO_HUB_DIR&gt;)</code>\n</li> <li>\n<code>$TORCH_HOME/hub</code>, if environment variable <code>TORCH_HOME</code> is set.</li> <li>\n<code>$XDG_CACHE_HOME/torch/hub</code>, if environment variable <code>XDG_CACHE_HOME</code> is set.</li> <li><code>~/.cache/torch/hub</code></li> </ul> <dl class=\"function\"> <dt id=\"torch.hub.get_dir\">\n<code>torch.hub.get_dir()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#get_dir\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Get the Torch Hub cache directory used for storing downloaded models &amp; weights.</p> <p>If <a class=\"reference internal\" href=\"#torch.hub.set_dir\" title=\"torch.hub.set_dir\"><code>set_dir()</code></a> is not called, default path is <code>$TORCH_HOME/hub</code> where environment variable <code>$TORCH_HOME</code> defaults to <code>$XDG_CACHE_HOME/torch</code>. <code>$XDG_CACHE_HOME</code> follows the X Design Group specification of the Linux filesystem layout, with a default value <code>~/.cache</code> if the environment variable is not set.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.hub.set_dir\">\n<code>torch.hub.set_dir(d)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/hub.html#set_dir\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Optionally set the Torch Hub directory used to save downloaded models &amp; weights.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>d</strong> (<em>string</em>) – path to a local folder to save downloaded models &amp; weights.</p> </dd> </dl> </dd>\n</dl>   <h3 id=\"caching-logic\">Caching logic</h3> <p>By default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by <a class=\"reference internal\" href=\"#torch.hub.get_dir\" title=\"torch.hub.get_dir\"><code>get_dir()</code></a>.</p> <p>Users can force a reload by calling <code>hub.load(..., force_reload=True)</code>. This will delete the existing github folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.</p>   <h3 id=\"known-limitations\">Known limitations:</h3> <p>Torch hub works by importing the package as if it was installed. There’re some side effects introduced by importing in Python. For example, you can see new items in Python caches <code>sys.modules</code> and <code>sys.path_importer_cache</code> which is normal Python behavior.</p> <p>A known limitation that worth mentioning here is user <strong>CANNOT</strong> load two different branches of the same repo in the <strong>same python process</strong>. It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.</p><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/hub.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/hub.html</a>\n  </p>\n</div>\n","jit":"<h1 id=\"torchscript\">TorchScript</h1>      <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"#creating-torchscript-code\" id=\"id4\">Creating TorchScript Code</a></li> <li><a class=\"reference internal\" href=\"#mixing-tracing-and-scripting\" id=\"id5\">Mixing Tracing and Scripting</a></li> <li><a class=\"reference internal\" href=\"#torchscript-language\" id=\"id6\">TorchScript Language</a></li> <li>\n<p><a class=\"reference internal\" href=\"#built-in-functions-and-modules\" id=\"id7\">Built-in Functions and Modules</a></p> <ul> <li><a class=\"reference internal\" href=\"#pytorch-functions-and-modules\" id=\"id8\">PyTorch Functions and Modules</a></li> <li><a class=\"reference internal\" href=\"#python-functions-and-modules\" id=\"id9\">Python Functions and Modules</a></li> <li><a class=\"reference internal\" href=\"#python-language-reference-comparison\" id=\"id10\">Python Language Reference Comparison</a></li> </ul> </li> <li>\n<p><a class=\"reference internal\" href=\"#debugging\" id=\"id11\">Debugging</a></p> <ul> <li><a class=\"reference internal\" href=\"#disable-jit-for-debugging\" id=\"id12\">Disable JIT for Debugging</a></li> <li><a class=\"reference internal\" href=\"#inspecting-code\" id=\"id13\">Inspecting Code</a></li> <li><a class=\"reference internal\" href=\"#interpreting-graphs\" id=\"id14\">Interpreting Graphs</a></li> <li><a class=\"reference internal\" href=\"#tracer\" id=\"id15\">Tracer</a></li> </ul> </li> <li><a class=\"reference internal\" href=\"#frequently-asked-questions\" id=\"id16\">Frequently Asked Questions</a></li> <li>\n<p><a class=\"reference internal\" href=\"#appendix\" id=\"id17\">Appendix</a></p> <ul> <li><a class=\"reference internal\" href=\"#migrating-to-pytorch-1-2-recursive-scripting-api\" id=\"id18\">Migrating to PyTorch 1.2 Recursive Scripting API</a></li> <li><a class=\"reference internal\" href=\"#references\" id=\"id19\">References</a></li> </ul> </li> </ul>  <p id=\"module-torch.jit\">TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.</p> <p>We provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.</p> <p>For a gentle introduction to TorchScript, see the <a class=\"reference external\" href=\"https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html\">Introduction to TorchScript</a> tutorial.</p> <p>For an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the <a class=\"reference external\" href=\"https://pytorch.org/tutorials/advanced/cpp_export.html\">Loading a PyTorch Model in C++</a> tutorial.</p>  <h2 id=\"creating-torchscript-code\">Creating TorchScript Code</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>script</code></a>(obj[, optimize, _frames_up, _rcb])</p></td> <td><p>Scripting a function or <code>nn.Module</code> will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> or <a class=\"reference internal\" href=\"generated/torch.jit.scriptfunction#torch.jit.ScriptFunction\" title=\"torch.jit.ScriptFunction\"><code>ScriptFunction</code></a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.trace#torch.jit.trace\" title=\"torch.jit.trace\"><code>trace</code></a>(func, example_inputs[, optimize, …])</p></td> <td><p>Trace a function and return an executable or <a class=\"reference internal\" href=\"generated/torch.jit.scriptfunction#torch.jit.ScriptFunction\" title=\"torch.jit.ScriptFunction\"><code>ScriptFunction</code></a> that will be optimized using just-in-time compilation.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.script_if_tracing#torch.jit.script_if_tracing\" title=\"torch.jit.script_if_tracing\"><code>script_if_tracing</code></a>(fn)</p></td> <td><p>Compiles <code>fn</code> when it is first called during tracing.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.trace_module#torch.jit.trace_module\" title=\"torch.jit.trace_module\"><code>trace_module</code></a>(mod, inputs[, optimize, …])</p></td> <td><p>Trace a module and return an executable <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> that will be optimized using just-in-time compilation.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.fork#torch.jit.fork\" title=\"torch.jit.fork\"><code>fork</code></a>(func, *args, **kwargs)</p></td> <td><p>Creates an asynchronous task executing <code>func</code> and a reference to the value of the result of this execution.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.wait#torch.jit.wait\" title=\"torch.jit.wait\"><code>wait</code></a>(future)</p></td> <td><p>Forces completion of a <code>torch.jit.Future[T]</code> asynchronous task, returning the result of the task.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>()</p></td> <td><p>A wrapper around C++ <code>torch::jit::Module</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.jit.scriptfunction#torch.jit.ScriptFunction\" title=\"torch.jit.ScriptFunction\"><code>ScriptFunction</code></a>\n</td> <td><p>Functionally equivalent to a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>, but represents a single function and does not have any attributes or Parameters.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.freeze#torch.jit.freeze\" title=\"torch.jit.freeze\"><code>freeze</code></a>(mod[, preserved_attrs, optimize_numerics])</p></td> <td><p>Freezing a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> will clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.save#torch.jit.save\" title=\"torch.jit.save\"><code>save</code></a>(m, f[, _extra_files])</p></td> <td><p>Save an offline version of this module for use in a separate process.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.load#torch.jit.load\" title=\"torch.jit.load\"><code>load</code></a>(f[, map_location, _extra_files])</p></td> <td><p>Load a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> or <a class=\"reference internal\" href=\"generated/torch.jit.scriptfunction#torch.jit.ScriptFunction\" title=\"torch.jit.ScriptFunction\"><code>ScriptFunction</code></a> previously saved with <a class=\"reference internal\" href=\"generated/torch.jit.save#torch.jit.save\" title=\"torch.jit.save\"><code>torch.jit.save</code></a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>ignore</code></a>([drop])</p></td> <td><p>This decorator indicates to the compiler that a function or method should be ignored and left as a Python function.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.unused#torch.jit.unused\" title=\"torch.jit.unused\"><code>unused</code></a>(fn)</p></td> <td><p>This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.jit.isinstance#torch.jit.isinstance\" title=\"torch.jit.isinstance\"><code>isinstance</code></a>(obj, target_type)</p></td> <td><p>This function provides for conatiner type refinement in TorchScript.</p></td> </tr>  </table>   <h2 id=\"mixing-tracing-and-scripting\">Mixing Tracing and Scripting</h2> <p>In many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.</p> <p>Scripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.</p> <p>Example (calling a traced function in script):</p> <pre data-language=\"python\">import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n</pre> <p>Traced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.</p> <p>Example (calling a script function in a traced function):</p> <pre data-language=\"python\">import torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max() &gt; y.max():\n        r = x\n    else:\n        r = y\n    return r\n\n\ndef bar(x, y, z):\n    return foo(x, y) + z\n\ntraced_bar = torch.jit.trace(bar, (torch.rand(3), torch.rand(3), torch.rand(3)))\n</pre> <p>This composition also works for <code>nn.Module</code>s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.</p> <p>Example (using a traced module):</p> <pre data-language=\"python\">import torch\nimport torchvision\n\nclass MyScriptModule(torch.nn.Module):\n    def __init__(self):\n        super(MyScriptModule, self).__init__()\n        self.means = torch.nn.Parameter(torch.tensor([103.939, 116.779, 123.68])\n                                        .resize_(1, 3, 1, 1))\n        self.resnet = torch.jit.trace(torchvision.models.resnet18(),\n                                      torch.rand(1, 3, 224, 224))\n\n    def forward(self, input):\n        return self.resnet(input - self.means)\n\nmy_script_module = torch.jit.script(MyScriptModule())\n</pre>   <h2 id=\"torchscript-language\">TorchScript Language</h2> <p>TorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full <a class=\"reference internal\" href=\"jit_language_reference#language-reference\"><span class=\"std std-ref\">TorchScript Language Reference</span></a> for details.</p>   <h2 id=\"builtin-functions\">Built-in Functions and Modules</h2> <p id=\"built-in-functions-and-modules\">TorchScript supports the use of most PyTorch functions and many Python built-ins. See <a class=\"reference internal\" href=\"jit_builtin_functions#builtin-functions\"><span class=\"std std-ref\">TorchScript Builtins</span></a> for a full reference of supported functions.</p>  <h3 id=\"pytorch-functions-and-modules\">PyTorch Functions and Modules</h3> <p>TorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the <code>torch</code> namespace, all functions in <code>torch.nn.functional</code> and most modules from <code>torch.nn</code> are supported in TorchScript.</p> <p>See <a class=\"reference internal\" href=\"jit_unsupported#jit-unsupported\"><span class=\"std std-ref\">TorchScript Unsupported Pytorch Constructs</span></a> for a list of unsupported PyTorch functions and modules.</p>   <h3 id=\"python-functions-and-modules\">Python Functions and Modules</h3> <p>Many of Python’s <a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html\">built-in functions</a> are supported in TorchScript. The <a class=\"reference external\" href=\"https://docs.python.org/3/library/math.html#module-math\" title=\"(in Python v3.9)\"><code>math</code></a> module is also supported (see <a class=\"reference internal\" href=\"jit_builtin_functions#math-module\"><span class=\"std std-ref\">math Module</span></a> for details), but no other Python modules (built-in or third party) are supported.</p>   <h3 id=\"python-language-reference-comparison\">Python Language Reference Comparison</h3> <p>For a full listing of supported Python features, see <a class=\"reference internal\" href=\"jit_python_reference#python-language-reference\"><span class=\"std std-ref\">Python Language Reference Coverage</span></a>.</p>    <h2 id=\"debugging\">Debugging</h2>  <h3 id=\"disable-torchscript\">Disable JIT for Debugging</h3> <dl class=\"envvar\" id=\"disable-jit-for-debugging\"> <dt id=\"envvar-PYTORCH_JIT\">\n<code>PYTORCH_JIT</code> </dt> \n</dl> <p>Setting the environment variable <code>PYTORCH_JIT=0</code> will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like <code>pdb</code> to debug the model code. For example:</p> <pre data-language=\"python\">@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))\n</pre> <p>Debugging this script with <code>pdb</code> works except for when we invoke the <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>@torch.jit.script</code></a> function. We can globally disable JIT, so that we can call the <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>@torch.jit.script</code></a> function as a normal Python function and not compile it. If the above script is called <code>disable_jit_example.py</code>, we can invoke it like so:</p> <pre data-language=\"python\">$ PYTORCH_JIT=0 python disable_jit_example.py\n</pre> <p>and we will be able to step into the <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>@torch.jit.script</code></a> function as a normal Python function. To disable the TorchScript compiler for a specific function, see <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a>.</p>   <h3 id=\"id1\">Inspecting Code</h3> <p id=\"inspecting-code\">TorchScript provides a code pretty-printer for all <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> instances. This pretty-printer gives an interpretation of the script method’s code as valid Python syntax. For example:</p> <pre data-language=\"python\">@torch.jit.script\ndef foo(len):\n    # type: (int) -&gt; torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i &lt; 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)\n</pre> <p>A <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> with a single <code>forward</code> method will have an attribute <code>code</code>, which you can use to inspect the <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>’s code. If the <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> has more than one method, you will need to access <code>.code</code> on the method itself and not the module. We can inspect the code of a method named <code>foo</code> on a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> by accessing <code>.foo.code</code>. The example above produces this output:</p> <pre data-language=\"python\">def foo(len: int) -&gt; Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0\n</pre> <p>This is TorchScript’s compilation of the code for the <code>forward</code> method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.</p>   <h3 id=\"id2\">Interpreting Graphs</h3> <p id=\"interpreting-graphs\">TorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.</p> <p>TorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:</p> <pre data-language=\"python\">@torch.jit.script\ndef foo(len):\n    # type: (int) -&gt; torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i &lt; 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)\n</pre> <p><code>graph</code> follows the same rules described in the <a class=\"reference internal\" href=\"#inspecting-code\"><span class=\"std std-ref\">Inspecting Code</span></a> section with regard to <code>forward</code> method lookup.</p> <p>The example script above produces the graph:</p> <pre data-language=\"python\">graph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -&gt; (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -&gt; (%rv.6)\n      -&gt; (%17, %rv.13)\n  return (%rv)\n</pre> <p>Take the instruction <code>%rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10</code> for example.</p> <ul class=\"simple\"> <li>\n<code>%rv.1 : Tensor</code> means we assign the output to a (unique) value named <code>rv.1</code>, that value is of <code>Tensor</code> type and that we do not know its concrete shape.</li> <li>\n<code>aten::zeros</code> is the operator (equivalent to <code>torch.zeros</code>) and the input list <code>(%4, %6, %6, %10, %12)</code> specifies which values in scope should be passed as inputs. The schema for built-in functions like <code>aten::zeros</code> can be found at <a class=\"reference internal\" href=\"#builtin-functions\">Builtin Functions</a>.</li> <li>\n<code># test.py:9:10</code> is the location in the original source file that generated this instruction. In this case, it is a file named <code>test.py</code>, on line 9, and at character 10.</li> </ul> <p>Notice that operators can also have associated <code>blocks</code>, namely the <code>prim::Loop</code> and <code>prim::If</code> operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.</p> <p>Graphs can be inspected as shown to confirm that the computation described by a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> is correct, in both automated and manual fashion, as described below.</p>   <h3 id=\"tracer\">Tracer</h3>  <h4 id=\"tracing-edge-cases\">Tracing Edge Cases</h4> <p>There are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:</p> <ul class=\"simple\"> <li>Tracing of control flow that is dependent on inputs (e.g. tensor shapes)</li> <li>Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)</li> </ul> <p>Note that these cases may in fact be traceable in the future.</p>   <h4 id=\"automatic-trace-checking\">Automatic Trace Checking</h4> <p>One way to automatically catch many errors in traces is by using <code>check_inputs</code> on the <code>torch.jit.trace()</code> API. <code>check_inputs</code> takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:</p> <pre data-language=\"python\">def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n</pre> <p>Gives us the following diagnostic information:</p> <pre data-language=\"python\">ERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }\n</pre> <p>This message indicates to us that the computation differed between when we first traced it and when we traced it with the <code>check_inputs</code>. Indeed, the loop within the body of <code>loop_in_traced_fn</code> depends on the shape of the input <code>x</code>, and thus when we try another <code>x</code> with a different shape, the trace differs.</p> <p>In this case, data-dependent control flow like this can be captured using <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>torch.jit.script()</code></a> instead:</p> <pre data-language=\"python\">def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n</pre> <p>Which produces:</p> <pre data-language=\"python\">graph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -&gt; (%5, %result.2)\n    }\n    return (%result);\n}\n</pre>   <h4 id=\"tracer-warnings\">Tracer Warnings</h4> <p>The tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:</p> <pre data-language=\"python\">def fill_row_zero(x):\n    x[0] = torch.rand(*x.shape[1:2])\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n</pre> <p>Produces several warnings and a graph which simply returns the input:</p> <pre data-language=\"python\">fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}\n</pre> <p>We can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with <code>torch.cat</code>:</p> <pre data-language=\"python\">def fill_row_zero(x):\n    x = torch.cat((torch.rand(1, *x.shape[1:2]), x[1:2]), dim=0)\n    return x\n\ntraced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\nprint(traced.graph)\n</pre>     <h2 id=\"frequently-asked-questions\">Frequently Asked Questions</h2> <p>Q: I would like to train a model on GPU and do inference on CPU. What are the best practices?</p>  <p>First convert your model from GPU to CPU and then save it, like so:</p> <pre data-language=\"python\">cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)\n</pre> <p>This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model <em>before</em> saving it ensures that the tracer has the correct device information.</p>  <p>Q: How do I store attributes on a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>?</p>  <p>Say we have a model like:</p> <pre data-language=\"python\">import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())\n</pre> <p>If <code>Model</code> is instantiated it will result in a compilation error since the compiler doesn’t know about <code>x</code>. There are 4 ways to inform the compiler of attributes on <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>:</p> <p>1. <code>nn.Parameter</code> - Values wrapped in <code>nn.Parameter</code> will work as they do on <code>nn.Module</code>s</p> <p>2. <code>register_buffer</code> - Values wrapped in <code>register_buffer</code> will work as they do on <code>nn.Module</code>s. This is equivalent to an attribute (see 4) of type <code>Tensor</code>.</p> <p>3. Constants - Annotating a class member as <code>Final</code> (or adding it to a list called <code>__constants__</code> at the class definition level) will mark the contained names as constants. Constants are saved directly in the code of the model. See <code>builtin-constants</code> for details.</p> <p>4. Attributes - Values that are a <code>supported type</code> can be added as mutable attributes. Most types can be inferred but some may need to be specified, see <code>module attributes</code> for details.</p>  <p>Q: I would like to trace module’s method but I keep getting this error:</p> <p><code>RuntimeError: Cannot insert a Tensor that requires grad as a constant. Consider making it a parameter or input, or detaching the gradient</code></p>  <p>This error usually means that the method you are tracing uses a module’s parameters and you are passing the module’s method instead of the module instance (e.g. <code>my_module_instance.forward</code> vs <code>my_module_instance</code>).</p>  <ul class=\"simple\"> <li>Invoking <code>trace</code> with a module’s method captures module parameters (which may require gradients) as <strong>constants</strong>.</li> <li>On the other hand, invoking <code>trace</code> with module’s instance (e.g. <code>my_module</code>) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.</li> </ul>  <p>To trace a specific method on a module, see <a class=\"reference internal\" href=\"generated/torch.jit.trace_module#torch.jit.trace_module\" title=\"torch.jit.trace_module\"><code>torch.jit.trace_module</code></a></p>    <h2 id=\"appendix\">Appendix</h2>  <h3 id=\"migrating-to-pytorch-1-2-recursive-scripting-api\">Migrating to PyTorch 1.2 Recursive Scripting API</h3> <p>This section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.</p> <p>1. <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>torch.jit.script</code></a> will now attempt to recursively compile functions, methods, and classes that it encounters. Once you call <code>torch.jit.script</code>, compilation is “opt-out”, rather than “opt-in”.</p> <p>2. <code>torch.jit.script(nn_module_instance)</code> is now the preferred way to create <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>s, instead of inheriting from <code>torch.jit.ScriptModule</code>. These changes combine to provide a simpler, easier-to-use API for converting your <code>nn.Module</code>s into <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a>s, ready to be optimized and executed in a non-Python environment.</p> <p>The new usage looks like this:</p> <pre data-language=\"python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)\n</pre> <ul class=\"simple\"> <li>The module’s <code>forward</code> is compiled by default. Methods called from <code>forward</code> are lazily compiled in the order they are used in <code>forward</code>.</li> <li>To compile a method other than <code>forward</code> that is not called from <code>forward</code>, add <code>@torch.jit.export</code>.</li> <li>To stop the compiler from compiling a method, add <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a> or <a class=\"reference internal\" href=\"generated/torch.jit.unused#torch.jit.unused\" title=\"torch.jit.unused\"><code>@torch.jit.unused</code></a>. <code>@ignore</code> leaves the</li> <li>method as a call to python, and <code>@unused</code> replaces it with an exception. <code>@ignored</code> cannot be exported; <code>@unused</code> can.</li> <li>Most attribute types can be inferred, so <code>torch.jit.Attribute</code> is not necessary. For empty container types, annotate their types using <a class=\"reference external\" href=\"https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations\">PEP 526-style</a> class annotations.</li> <li>Constants can be marked with a <code>Final</code> class annotation instead of adding the name of the member to <code>__constants__</code>.</li> <li>Python 3 type hints can be used in place of <code>torch.jit.annotate</code>\n</li> </ul> <dl class=\"simple\"> <dt>As a result of these changes, the following items are considered deprecated and should not appear in new code:</dt>\n<dd>\n<ul class=\"simple\"> <li>The <code>@torch.jit.script_method</code> decorator</li> <li>Classes that inherit from <code>torch.jit.ScriptModule</code>\n</li> <li>The <code>torch.jit.Attribute</code> wrapper class</li> <li>The <code>__constants__</code> array</li> <li>The <code>torch.jit.annotate</code> function</li> </ul> </dd> </dl>  <h4 id=\"modules\">Modules</h4> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a> annotation’s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use <code>@torch.jit.unused()</code>. <code>@torch.jit.ignore</code> is now equivalent to <code>@torch.jit.ignore(drop=False)</code>. See <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a> and <a class=\"reference internal\" href=\"generated/torch.jit.unused#torch.jit.unused\" title=\"torch.jit.unused\"><code>@torch.jit.unused</code></a> for details.</p> </div> <p>When passed to the <a class=\"reference internal\" href=\"generated/torch.jit.script#torch.jit.script\" title=\"torch.jit.script\"><code>torch.jit.script</code></a> function, a <code>torch.nn.Module</code>’s data is copied to a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> and the TorchScript compiler compiles the module. The module’s <code>forward</code> is compiled by default. Methods called from <code>forward</code> are lazily compiled in the order they are used in <code>forward</code>, as well as any <code>@torch.jit.export</code> methods.</p> <dl class=\"function\"> <dt id=\"torch.jit.export\">\n<code>torch.jit.export(fn)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/_jit_internal.html#export\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This decorator indicates that a method on an <code>nn.Module</code> is used as an entry point into a <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> and should be compiled.</p> <p><code>forward</code> implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from <code>forward</code> are compiled as they are seen by the compiler, so they do not need this decorator either.</p> <p>Example (using <code>@torch.jit.export</code> on a method):</p> <pre data-language=\"python\">import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def implicitly_compiled_method(self, x):\n        return x + 99\n\n    # `forward` is implicitly decorated with `@torch.jit.export`,\n    # so adding it here would have no effect\n    def forward(self, x):\n        return x + 10\n\n    @torch.jit.export\n    def another_forward(self, x):\n        # When the compiler sees this call, it will compile\n        # `implicitly_compiled_method`\n        return self.implicitly_compiled_method(x)\n\n    def unused_method(self, x):\n        return x - 20\n\n# `m` will contain compiled methods:\n#     `forward`\n#     `another_forward`\n#     `implicitly_compiled_method`\n# `unused_method` will not be compiled since it was not called from\n# any compiled methods and wasn't decorated with `@torch.jit.export`\nm = torch.jit.script(MyModule())\n</pre> </dd>\n</dl>   <h4 id=\"functions\">Functions</h4> <p>Functions don’t change much, they can be decorated with <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a> or <a class=\"reference internal\" href=\"generated/torch.jit.unused#torch.jit.unused\" title=\"torch.jit.unused\"><code>torch.jit.unused</code></a> if needed.</p> <pre data-language=\"python\"># Same behavior as pre-PyTorch 1.2\n@torch.jit.script\ndef some_fn():\n    return 2\n\n# Marks a function as ignored, if nothing\n# ever calls it then this has no effect\n@torch.jit.ignore\ndef some_fn2():\n    return 2\n\n# As with ignore, if nothing calls it then it has no effect.\n# If it is called in script it is replaced with an exception.\n@torch.jit.unused\ndef some_fn3():\n  import pdb; pdb.set_trace()\n  return 4\n\n# Doesn't do anything, this function is already\n# the main entry point\n@torch.jit.export\ndef some_fn4():\n    return 2\n</pre>   <h4 id=\"torchscript-classes\">TorchScript Classes</h4> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>TorchScript class support is experimental. Currently it is best suited for simple record-like types (think a <code>NamedTuple</code> with methods attached).</p> </div> <p>Everything in a user defined <a class=\"reference external\" href=\"torchscript-class\">TorchScript Class</a> is exported by default, functions can be decorated with <a class=\"reference internal\" href=\"generated/torch.jit.ignore#torch.jit.ignore\" title=\"torch.jit.ignore\"><code>@torch.jit.ignore</code></a> if needed.</p>   <h4 id=\"attributes\">Attributes</h4> <p>The TorchScript compiler needs to know the types of <code>module attributes</code>. Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with <a class=\"reference external\" href=\"https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations\">PEP 526-style</a> class annotations. If a type cannot be inferred and is not explicitly annotated, it will not be added as an attribute to the resulting <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a></p> <p>Old API:</p> <pre data-language=\"python\">from typing import Dict\nimport torch\n\nclass MyModule(torch.jit.ScriptModule):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_dict = torch.jit.Attribute({}, Dict[str, int])\n        self.my_int = torch.jit.Attribute(20, int)\n\nm = MyModule()\n</pre> <p>New API:</p> <pre data-language=\"python\">from typing import Dict\n\nclass MyModule(torch.nn.Module):\n    my_dict: Dict[str, int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        # This type cannot be inferred and must be specified\n        self.my_dict = {}\n\n        # The attribute type here is inferred to be `int`\n        self.my_int = 20\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n</pre>   <h4 id=\"constants\">Constants</h4> <p>The <code>Final</code> type constructor can be used to mark members as <code>constant</code>. If members are not marked constant, they will be copied to the resulting <a class=\"reference internal\" href=\"generated/torch.jit.scriptmodule#torch.jit.ScriptModule\" title=\"torch.jit.ScriptModule\"><code>ScriptModule</code></a> as an attribute. Using <code>Final</code> opens opportunities for optimization if the value is known to be fixed and gives additional type safety.</p> <p>Old API:</p> <pre data-language=\"python\">class MyModule(torch.jit.ScriptModule):\n    __constants__ = ['my_constant']\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\nm = MyModule()\n</pre> <p>New API:</p> <pre data-language=\"python\">try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n</pre>   <h4 id=\"python-3-type-hints\">Variables</h4> <p id=\"variables\">Containers are assumed to have type <code>Tensor</code> and be non-optional (see <code>Default Types</code> for more information). Previously, <code>torch.jit.annotate</code> was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.</p> <pre data-language=\"python\">import torch\nfrom typing import Dict, Optional\n\n@torch.jit.script\ndef make_dict(flag: bool):\n    x: Dict[str, int] = {}\n    x['hi'] = 2\n    b: Optional[int] = None\n    if flag:\n        b = 2\n    return x, b\n</pre>    <h3 id=\"references\">References</h3>  <ul> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"jit_python_reference\">Python Language Reference Coverage</a></li> <li class=\"toctree-l1\"><a class=\"reference internal\" href=\"jit_unsupported\">TorchScript Unsupported Pytorch Constructs</a></li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/jit.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/jit.html</a>\n  </p>\n</div>\n","nn":"<h1 id=\"torch-nn\">torch.nn</h1> <p>These are the basic building block for graphs</p>  <p class=\"topic-title\">torch.nn</p> <ul class=\"simple\"> <li><a class=\"reference internal\" href=\"#containers\" id=\"id2\">Containers</a></li> <li><a class=\"reference internal\" href=\"#convolution-layers\" id=\"id3\">Convolution Layers</a></li> <li><a class=\"reference internal\" href=\"#pooling-layers\" id=\"id4\">Pooling layers</a></li> <li><a class=\"reference internal\" href=\"#padding-layers\" id=\"id5\">Padding Layers</a></li> <li><a class=\"reference internal\" href=\"#non-linear-activations-weighted-sum-nonlinearity\" id=\"id6\">Non-linear Activations (weighted sum, nonlinearity)</a></li> <li><a class=\"reference internal\" href=\"#non-linear-activations-other\" id=\"id7\">Non-linear Activations (other)</a></li> <li><a class=\"reference internal\" href=\"#normalization-layers\" id=\"id8\">Normalization Layers</a></li> <li><a class=\"reference internal\" href=\"#recurrent-layers\" id=\"id9\">Recurrent Layers</a></li> <li><a class=\"reference internal\" href=\"#transformer-layers\" id=\"id10\">Transformer Layers</a></li> <li><a class=\"reference internal\" href=\"#linear-layers\" id=\"id11\">Linear Layers</a></li> <li><a class=\"reference internal\" href=\"#dropout-layers\" id=\"id12\">Dropout Layers</a></li> <li><a class=\"reference internal\" href=\"#sparse-layers\" id=\"id13\">Sparse Layers</a></li> <li><a class=\"reference internal\" href=\"#distance-functions\" id=\"id14\">Distance Functions</a></li> <li><a class=\"reference internal\" href=\"#loss-functions\" id=\"id15\">Loss Functions</a></li> <li><a class=\"reference internal\" href=\"#vision-layers\" id=\"id16\">Vision Layers</a></li> <li><a class=\"reference internal\" href=\"#shuffle-layers\" id=\"id17\">Shuffle Layers</a></li> <li><a class=\"reference internal\" href=\"#dataparallel-layers-multi-gpu-distributed\" id=\"id18\">DataParallel Layers (multi-GPU, distributed)</a></li> <li><a class=\"reference internal\" href=\"#utilities\" id=\"id19\">Utilities</a></li> <li><a class=\"reference internal\" href=\"#quantized-functions\" id=\"id20\">Quantized Functions</a></li> <li><a class=\"reference internal\" href=\"#lazy-modules-initialization\" id=\"id21\">Lazy Modules Initialization</a></li> </ul>  <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameter.parameter#torch.nn.parameter.Parameter\" title=\"torch.nn.parameter.Parameter\"><code>Parameter</code></a>\n</td> <td><p>A kind of Tensor that is to be considered a module parameter.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameter.uninitializedparameter#torch.nn.parameter.UninitializedParameter\" title=\"torch.nn.parameter.UninitializedParameter\"><code>UninitializedParameter</code></a>\n</td> <td><p>A parameter that is not initialized.</p></td> </tr>  </table>  <h2 id=\"containers\">Containers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\"><code>Module</code></a>\n</td> <td><p>Base class for all neural network modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.sequential#torch.nn.Sequential\" title=\"torch.nn.Sequential\"><code>Sequential</code></a>\n</td> <td><p>A sequential container.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modulelist#torch.nn.ModuleList\" title=\"torch.nn.ModuleList\"><code>ModuleList</code></a>\n</td> <td><p>Holds submodules in a list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.moduledict#torch.nn.ModuleDict\" title=\"torch.nn.ModuleDict\"><code>ModuleDict</code></a>\n</td> <td><p>Holds submodules in a dictionary.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameterlist#torch.nn.ParameterList\" title=\"torch.nn.ParameterList\"><code>ParameterList</code></a>\n</td> <td><p>Holds parameters in a list.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.parameterdict#torch.nn.ParameterDict\" title=\"torch.nn.ParameterDict\"><code>ParameterDict</code></a>\n</td> <td><p>Holds parameters in a dictionary.</p></td> </tr>  </table> <p>Global Hooks For Module</p> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_forward_pre_hook#torch.nn.modules.module.register_module_forward_pre_hook\" title=\"torch.nn.modules.module.register_module_forward_pre_hook\"><code>register_module_forward_pre_hook</code></a>\n</td> <td><p>Registers a forward pre-hook common to all modules.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_forward_hook#torch.nn.modules.module.register_module_forward_hook\" title=\"torch.nn.modules.module.register_module_forward_hook\"><code>register_module_forward_hook</code></a>\n</td> <td><p>Registers a global forward hook for all the modules</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.modules.module.register_module_backward_hook#torch.nn.modules.module.register_module_backward_hook\" title=\"torch.nn.modules.module.register_module_backward_hook\"><code>register_module_backward_hook</code></a>\n</td> <td><p>Registers a backward hook common to all the modules.</p></td> </tr>  </table>   <h2 id=\"convolution-layers\">Convolution Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv1d#torch.nn.Conv1d\" title=\"torch.nn.Conv1d\"><code>nn.Conv1d</code></a></p></td> <td><p>Applies a 1D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv2d#torch.nn.Conv2d\" title=\"torch.nn.Conv2d\"><code>nn.Conv2d</code></a></p></td> <td><p>Applies a 2D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.conv3d#torch.nn.Conv3d\" title=\"torch.nn.Conv3d\"><code>nn.Conv3d</code></a></p></td> <td><p>Applies a 3D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d\" title=\"torch.nn.ConvTranspose1d\"><code>nn.ConvTranspose1d</code></a></p></td> <td><p>Applies a 1D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d\" title=\"torch.nn.ConvTranspose2d\"><code>nn.ConvTranspose2d</code></a></p></td> <td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d\" title=\"torch.nn.ConvTranspose3d\"><code>nn.ConvTranspose3d</code></a></p></td> <td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv1d#torch.nn.LazyConv1d\" title=\"torch.nn.LazyConv1d\"><code>nn.LazyConv1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv1d#torch.nn.Conv1d\" title=\"torch.nn.Conv1d\"><code>torch.nn.Conv1d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv2d#torch.nn.LazyConv2d\" title=\"torch.nn.LazyConv2d\"><code>nn.LazyConv2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv2d#torch.nn.Conv2d\" title=\"torch.nn.Conv2d\"><code>torch.nn.Conv2d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconv3d#torch.nn.LazyConv3d\" title=\"torch.nn.LazyConv3d\"><code>nn.LazyConv3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.conv3d#torch.nn.Conv3d\" title=\"torch.nn.Conv3d\"><code>torch.nn.Conv3d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>Conv3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose1d#torch.nn.LazyConvTranspose1d\" title=\"torch.nn.LazyConvTranspose1d\"><code>nn.LazyConvTranspose1d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d\" title=\"torch.nn.ConvTranspose1d\"><code>torch.nn.ConvTranspose1d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose1d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose2d#torch.nn.LazyConvTranspose2d\" title=\"torch.nn.LazyConvTranspose2d\"><code>nn.LazyConvTranspose2d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d\" title=\"torch.nn.ConvTranspose2d\"><code>torch.nn.ConvTranspose2d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose2d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazyconvtranspose3d#torch.nn.LazyConvTranspose3d\" title=\"torch.nn.LazyConvTranspose3d\"><code>nn.LazyConvTranspose3d</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d\" title=\"torch.nn.ConvTranspose3d\"><code>torch.nn.ConvTranspose3d</code></a> module with lazy initialization of the <code>in_channels</code> argument of the <code>ConvTranspose3d</code> that is inferred from the <code>input.size(1)</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.unfold#torch.nn.Unfold\" title=\"torch.nn.Unfold\"><code>nn.Unfold</code></a></p></td> <td><p>Extracts sliding local blocks from a batched input tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.fold#torch.nn.Fold\" title=\"torch.nn.Fold\"><code>nn.Fold</code></a></p></td> <td><p>Combines an array of sliding local blocks into a large containing tensor.</p></td> </tr>  </table>   <h2 id=\"pooling-layers\">Pooling layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool1d#torch.nn.MaxPool1d\" title=\"torch.nn.MaxPool1d\"><code>nn.MaxPool1d</code></a></p></td> <td><p>Applies a 1D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool2d#torch.nn.MaxPool2d\" title=\"torch.nn.MaxPool2d\"><code>nn.MaxPool2d</code></a></p></td> <td><p>Applies a 2D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxpool3d#torch.nn.MaxPool3d\" title=\"torch.nn.MaxPool3d\"><code>nn.MaxPool3d</code></a></p></td> <td><p>Applies a 3D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d\" title=\"torch.nn.MaxUnpool1d\"><code>nn.MaxUnpool1d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool1d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d\" title=\"torch.nn.MaxUnpool2d\"><code>nn.MaxUnpool2d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool2d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d\" title=\"torch.nn.MaxUnpool3d\"><code>nn.MaxUnpool3d</code></a></p></td> <td><p>Computes a partial inverse of <code>MaxPool3d</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool1d#torch.nn.AvgPool1d\" title=\"torch.nn.AvgPool1d\"><code>nn.AvgPool1d</code></a></p></td> <td><p>Applies a 1D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool2d#torch.nn.AvgPool2d\" title=\"torch.nn.AvgPool2d\"><code>nn.AvgPool2d</code></a></p></td> <td><p>Applies a 2D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.avgpool3d#torch.nn.AvgPool3d\" title=\"torch.nn.AvgPool3d\"><code>nn.AvgPool3d</code></a></p></td> <td><p>Applies a 3D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d\" title=\"torch.nn.FractionalMaxPool2d\"><code>nn.FractionalMaxPool2d</code></a></p></td> <td><p>Applies a 2D fractional max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lppool1d#torch.nn.LPPool1d\" title=\"torch.nn.LPPool1d\"><code>nn.LPPool1d</code></a></p></td> <td><p>Applies a 1D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lppool2d#torch.nn.LPPool2d\" title=\"torch.nn.LPPool2d\"><code>nn.LPPool2d</code></a></p></td> <td><p>Applies a 2D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d\" title=\"torch.nn.AdaptiveMaxPool1d\"><code>nn.AdaptiveMaxPool1d</code></a></p></td> <td><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d\" title=\"torch.nn.AdaptiveMaxPool2d\"><code>nn.AdaptiveMaxPool2d</code></a></p></td> <td><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d\" title=\"torch.nn.AdaptiveMaxPool3d\"><code>nn.AdaptiveMaxPool3d</code></a></p></td> <td><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d\" title=\"torch.nn.AdaptiveAvgPool1d\"><code>nn.AdaptiveAvgPool1d</code></a></p></td> <td><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d\" title=\"torch.nn.AdaptiveAvgPool2d\"><code>nn.AdaptiveAvgPool2d</code></a></p></td> <td><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d\" title=\"torch.nn.AdaptiveAvgPool3d\"><code>nn.AdaptiveAvgPool3d</code></a></p></td> <td><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr>  </table>   <h2 id=\"padding-layers\">Padding Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d\" title=\"torch.nn.ReflectionPad1d\"><code>nn.ReflectionPad1d</code></a></p></td> <td><p>Pads the input tensor using the reflection of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d\" title=\"torch.nn.ReflectionPad2d\"><code>nn.ReflectionPad2d</code></a></p></td> <td><p>Pads the input tensor using the reflection of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad1d#torch.nn.ReplicationPad1d\" title=\"torch.nn.ReplicationPad1d\"><code>nn.ReplicationPad1d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d\" title=\"torch.nn.ReplicationPad2d\"><code>nn.ReplicationPad2d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.replicationpad3d#torch.nn.ReplicationPad3d\" title=\"torch.nn.ReplicationPad3d\"><code>nn.ReplicationPad3d</code></a></p></td> <td><p>Pads the input tensor using replication of the input boundary.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.zeropad2d#torch.nn.ZeroPad2d\" title=\"torch.nn.ZeroPad2d\"><code>nn.ZeroPad2d</code></a></p></td> <td><p>Pads the input tensor boundaries with zero.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad1d#torch.nn.ConstantPad1d\" title=\"torch.nn.ConstantPad1d\"><code>nn.ConstantPad1d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d\" title=\"torch.nn.ConstantPad2d\"><code>nn.ConstantPad2d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.constantpad3d#torch.nn.ConstantPad3d\" title=\"torch.nn.ConstantPad3d\"><code>nn.ConstantPad3d</code></a></p></td> <td><p>Pads the input tensor boundaries with a constant value.</p></td> </tr>  </table>   <h2 id=\"non-linear-activations-weighted-sum-nonlinearity\">Non-linear Activations (weighted sum, nonlinearity)</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.elu#torch.nn.ELU\" title=\"torch.nn.ELU\"><code>nn.ELU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardshrink#torch.nn.Hardshrink\" title=\"torch.nn.Hardshrink\"><code>nn.Hardshrink</code></a></p></td> <td><p>Applies the hard shrinkage function element-wise:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid\" title=\"torch.nn.Hardsigmoid\"><code>nn.Hardsigmoid</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardtanh#torch.nn.Hardtanh\" title=\"torch.nn.Hardtanh\"><code>nn.Hardtanh</code></a></p></td> <td><p>Applies the HardTanh function element-wise</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hardswish#torch.nn.Hardswish\" title=\"torch.nn.Hardswish\"><code>nn.Hardswish</code></a></p></td> <td><p>Applies the hardswish function, element-wise, as described in the paper:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.leakyrelu#torch.nn.LeakyReLU\" title=\"torch.nn.LeakyReLU\"><code>nn.LeakyReLU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.logsigmoid#torch.nn.LogSigmoid\" title=\"torch.nn.LogSigmoid\"><code>nn.LogSigmoid</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multiheadattention#torch.nn.MultiheadAttention\" title=\"torch.nn.MultiheadAttention\"><code>nn.MultiheadAttention</code></a></p></td> <td><p>Allows the model to jointly attend to information from different representation subspaces.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.prelu#torch.nn.PReLU\" title=\"torch.nn.PReLU\"><code>nn.PReLU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.relu#torch.nn.ReLU\" title=\"torch.nn.ReLU\"><code>nn.ReLU</code></a></p></td> <td><p>Applies the rectified linear unit function element-wise:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.relu6#torch.nn.ReLU6\" title=\"torch.nn.ReLU6\"><code>nn.ReLU6</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rrelu#torch.nn.RReLU\" title=\"torch.nn.RReLU\"><code>nn.RReLU</code></a></p></td> <td><p>Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.selu#torch.nn.SELU\" title=\"torch.nn.SELU\"><code>nn.SELU</code></a></p></td> <td><p>Applied element-wise, as:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.celu#torch.nn.CELU\" title=\"torch.nn.CELU\"><code>nn.CELU</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gelu#torch.nn.GELU\" title=\"torch.nn.GELU\"><code>nn.GELU</code></a></p></td> <td><p>Applies the Gaussian Error Linear Units function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.sigmoid#torch.nn.Sigmoid\" title=\"torch.nn.Sigmoid\"><code>nn.Sigmoid</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.silu#torch.nn.SiLU\" title=\"torch.nn.SiLU\"><code>nn.SiLU</code></a></p></td> <td><p>Applies the silu function, element-wise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softplus#torch.nn.Softplus\" title=\"torch.nn.Softplus\"><code>nn.Softplus</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softshrink#torch.nn.Softshrink\" title=\"torch.nn.Softshrink\"><code>nn.Softshrink</code></a></p></td> <td><p>Applies the soft shrinkage function elementwise:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softsign#torch.nn.Softsign\" title=\"torch.nn.Softsign\"><code>nn.Softsign</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tanh#torch.nn.Tanh\" title=\"torch.nn.Tanh\"><code>nn.Tanh</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tanhshrink#torch.nn.Tanhshrink\" title=\"torch.nn.Tanhshrink\"><code>nn.Tanhshrink</code></a></p></td> <td><p>Applies the element-wise function:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.threshold#torch.nn.Threshold\" title=\"torch.nn.Threshold\"><code>nn.Threshold</code></a></p></td> <td><p>Thresholds each element of the input Tensor.</p></td> </tr>  </table>   <h2 id=\"non-linear-activations-other\">Non-linear Activations (other)</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmin#torch.nn.Softmin\" title=\"torch.nn.Softmin\"><code>nn.Softmin</code></a></p></td> <td><p>Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range <code>[0, 1]</code> and sum to 1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmax#torch.nn.Softmax\" title=\"torch.nn.Softmax\"><code>nn.Softmax</code></a></p></td> <td><p>Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmax2d#torch.nn.Softmax2d\" title=\"torch.nn.Softmax2d\"><code>nn.Softmax2d</code></a></p></td> <td><p>Applies SoftMax over features to each spatial location.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.logsoftmax#torch.nn.LogSoftmax\" title=\"torch.nn.LogSoftmax\"><code>nn.LogSoftmax</code></a></p></td> <td><p>Applies the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(\\text{Softmax}(x))</annotation></semantics></math></span></span> </span> function to an n-dimensional input Tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.adaptivelogsoftmaxwithloss#torch.nn.AdaptiveLogSoftmaxWithLoss\" title=\"torch.nn.AdaptiveLogSoftmaxWithLoss\"><code>nn.AdaptiveLogSoftmaxWithLoss</code></a></p></td> <td><p>Efficient softmax approximation as described in <a class=\"reference external\" href=\"https://arxiv.org/abs/1609.04309\">Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Cissé, David Grangier, and Hervé Jégou</a>.</p></td> </tr>  </table>   <h2 id=\"normalization-layers\">Normalization Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d\" title=\"torch.nn.BatchNorm1d\"><code>nn.BatchNorm1d</code></a></p></td> <td><p>Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d\" title=\"torch.nn.BatchNorm2d\"><code>nn.BatchNorm2d</code></a></p></td> <td><p>Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d\" title=\"torch.nn.BatchNorm3d\"><code>nn.BatchNorm3d</code></a></p></td> <td><p>Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.groupnorm#torch.nn.GroupNorm\" title=\"torch.nn.GroupNorm\"><code>nn.GroupNorm</code></a></p></td> <td><p>Applies Group Normalization over a mini-batch of inputs as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1803.08494\">Group Normalization</a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.syncbatchnorm#torch.nn.SyncBatchNorm\" title=\"torch.nn.SyncBatchNorm\"><code>nn.SyncBatchNorm</code></a></p></td> <td><p>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1502.03167\">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d\" title=\"torch.nn.InstanceNorm1d\"><code>nn.InstanceNorm1d</code></a></p></td> <td><p>Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d\" title=\"torch.nn.InstanceNorm2d\"><code>nn.InstanceNorm2d</code></a></p></td> <td><p>Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d\" title=\"torch.nn.InstanceNorm3d\"><code>nn.InstanceNorm3d</code></a></p></td> <td><p>Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.08022\">Instance Normalization: The Missing Ingredient for Fast Stylization</a>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.layernorm#torch.nn.LayerNorm\" title=\"torch.nn.LayerNorm\"><code>nn.LayerNorm</code></a></p></td> <td><p>Applies Layer Normalization over a mini-batch of inputs as described in the paper <a class=\"reference external\" href=\"https://arxiv.org/abs/1607.06450\">Layer Normalization</a></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm\" title=\"torch.nn.LocalResponseNorm\"><code>nn.LocalResponseNorm</code></a></p></td> <td><p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</p></td> </tr>  </table>   <h2 id=\"recurrent-layers\">Recurrent Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnnbase#torch.nn.RNNBase\" title=\"torch.nn.RNNBase\"><code>nn.RNNBase</code></a></p></td> <td></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnn#torch.nn.RNN\" title=\"torch.nn.RNN\"><code>nn.RNN</code></a></p></td> <td><p>Applies a multi-layer Elman RNN with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>tanh</mi><mo>⁡</mo></mrow><annotation encoding=\"application/x-tex\">\\tanh</annotation></semantics></math></span></span> </span> or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>ReLU</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{ReLU}</annotation></semantics></math></span></span> </span> non-linearity to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lstm#torch.nn.LSTM\" title=\"torch.nn.LSTM\"><code>nn.LSTM</code></a></p></td> <td><p>Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gru#torch.nn.GRU\" title=\"torch.nn.GRU\"><code>nn.GRU</code></a></p></td> <td><p>Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.rnncell#torch.nn.RNNCell\" title=\"torch.nn.RNNCell\"><code>nn.RNNCell</code></a></p></td> <td><p>An Elman RNN cell with tanh or ReLU non-linearity.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lstmcell#torch.nn.LSTMCell\" title=\"torch.nn.LSTMCell\"><code>nn.LSTMCell</code></a></p></td> <td><p>A long short-term memory (LSTM) cell.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.grucell#torch.nn.GRUCell\" title=\"torch.nn.GRUCell\"><code>nn.GRUCell</code></a></p></td> <td><p>A gated recurrent unit (GRU) cell</p></td> </tr>  </table>   <h2 id=\"transformer-layers\">Transformer Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformer#torch.nn.Transformer\" title=\"torch.nn.Transformer\"><code>nn.Transformer</code></a></p></td> <td><p>A transformer model.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerencoder#torch.nn.TransformerEncoder\" title=\"torch.nn.TransformerEncoder\"><code>nn.TransformerEncoder</code></a></p></td> <td><p>TransformerEncoder is a stack of N encoder layers</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerdecoder#torch.nn.TransformerDecoder\" title=\"torch.nn.TransformerDecoder\"><code>nn.TransformerDecoder</code></a></p></td> <td><p>TransformerDecoder is a stack of N decoder layers</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerencoderlayer#torch.nn.TransformerEncoderLayer\" title=\"torch.nn.TransformerEncoderLayer\"><code>nn.TransformerEncoderLayer</code></a></p></td> <td><p>TransformerEncoderLayer is made up of self-attn and feedforward network.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.transformerdecoderlayer#torch.nn.TransformerDecoderLayer\" title=\"torch.nn.TransformerDecoderLayer\"><code>nn.TransformerDecoderLayer</code></a></p></td> <td><p>TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.</p></td> </tr>  </table>   <h2 id=\"linear-layers\">Linear Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.identity#torch.nn.Identity\" title=\"torch.nn.Identity\"><code>nn.Identity</code></a></p></td> <td><p>A placeholder identity operator that is argument-insensitive.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.linear#torch.nn.Linear\" title=\"torch.nn.Linear\"><code>nn.Linear</code></a></p></td> <td><p>Applies a linear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = xA^T + b</annotation></semantics></math></span></span> </span></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bilinear#torch.nn.Bilinear\" title=\"torch.nn.Bilinear\"><code>nn.Bilinear</code></a></p></td> <td><p>Applies a bilinear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = x_1^T A x_2 + b</annotation></semantics></math></span></span> </span></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.lazylinear#torch.nn.LazyLinear\" title=\"torch.nn.LazyLinear\"><code>nn.LazyLinear</code></a></p></td> <td><p>A <a class=\"reference internal\" href=\"generated/torch.nn.linear#torch.nn.Linear\" title=\"torch.nn.Linear\"><code>torch.nn.Linear</code></a> module with lazy initialization.</p></td> </tr>  </table>   <h2 id=\"dropout-layers\">Dropout Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><code>nn.Dropout</code></a></p></td> <td><p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout2d#torch.nn.Dropout2d\" title=\"torch.nn.Dropout2d\"><code>nn.Dropout2d</code></a></p></td> <td><p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span> </span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span>-th sample in the batched input is a 2D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span> </span>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dropout3d#torch.nn.Dropout3d\" title=\"torch.nn.Dropout3d\"><code>nn.Dropout3d</code></a></p></td> <td><p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span> </span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span>-th sample in the batched input is a 3D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span> </span>).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.alphadropout#torch.nn.AlphaDropout\" title=\"torch.nn.AlphaDropout\"><code>nn.AlphaDropout</code></a></p></td> <td><p>Applies Alpha Dropout over the input.</p></td> </tr>  </table>   <h2 id=\"sparse-layers\">Sparse Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.embedding#torch.nn.Embedding\" title=\"torch.nn.Embedding\"><code>nn.Embedding</code></a></p></td> <td><p>A simple lookup table that stores embeddings of a fixed dictionary and size.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag\" title=\"torch.nn.EmbeddingBag\"><code>nn.EmbeddingBag</code></a></p></td> <td><p>Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings.</p></td> </tr>  </table>   <h2 id=\"distance-functions\">Distance Functions</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.cosinesimilarity#torch.nn.CosineSimilarity\" title=\"torch.nn.CosineSimilarity\"><code>nn.CosineSimilarity</code></a></p></td> <td><p>Returns cosine similarity between <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span></span> </span> and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span></span> </span>, computed along dim.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance\" title=\"torch.nn.PairwiseDistance\"><code>nn.PairwiseDistance</code></a></p></td> <td><p>Computes the batchwise pairwise distance between vectors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">v_1</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>v</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">v_2</annotation></semantics></math></span></span> </span> using the p-norm:</p></td> </tr>  </table>   <h2 id=\"loss-functions\">Loss Functions</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.l1loss#torch.nn.L1Loss\" title=\"torch.nn.L1Loss\"><code>nn.L1Loss</code></a></p></td> <td><p>Creates a criterion that measures the mean absolute error (MAE) between each element in the input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.mseloss#torch.nn.MSELoss\" title=\"torch.nn.MSELoss\"><code>nn.MSELoss</code></a></p></td> <td><p>Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss\" title=\"torch.nn.CrossEntropyLoss\"><code>nn.CrossEntropyLoss</code></a></p></td> <td><p>This criterion combines <a class=\"reference internal\" href=\"generated/torch.nn.logsoftmax#torch.nn.LogSoftmax\" title=\"torch.nn.LogSoftmax\"><code>LogSoftmax</code></a> and <a class=\"reference internal\" href=\"generated/torch.nn.nllloss#torch.nn.NLLLoss\" title=\"torch.nn.NLLLoss\"><code>NLLLoss</code></a> in one single class.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.ctcloss#torch.nn.CTCLoss\" title=\"torch.nn.CTCLoss\"><code>nn.CTCLoss</code></a></p></td> <td><p>The Connectionist Temporal Classification loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.nllloss#torch.nn.NLLLoss\" title=\"torch.nn.NLLLoss\"><code>nn.NLLLoss</code></a></p></td> <td><p>The negative log likelihood loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss\" title=\"torch.nn.PoissonNLLLoss\"><code>nn.PoissonNLLLoss</code></a></p></td> <td><p>Negative log likelihood loss with Poisson distribution of target.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.gaussiannllloss#torch.nn.GaussianNLLLoss\" title=\"torch.nn.GaussianNLLLoss\"><code>nn.GaussianNLLLoss</code></a></p></td> <td><p>Gaussian negative log likelihood loss.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.kldivloss#torch.nn.KLDivLoss\" title=\"torch.nn.KLDivLoss\"><code>nn.KLDivLoss</code></a></p></td> <td><p>The Kullback-Leibler divergence loss measure</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bceloss#torch.nn.BCELoss\" title=\"torch.nn.BCELoss\"><code>nn.BCELoss</code></a></p></td> <td><p>Creates a criterion that measures the Binary Cross Entropy between the target and the output:</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\" title=\"torch.nn.BCEWithLogitsLoss\"><code>nn.BCEWithLogitsLoss</code></a></p></td> <td><p>This loss combines a <code>Sigmoid</code> layer and the <code>BCELoss</code> in one single class.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss\" title=\"torch.nn.MarginRankingLoss\"><code>nn.MarginRankingLoss</code></a></p></td> <td><p>Creates a criterion that measures the loss given inputs <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x1</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x2</annotation></semantics></math></span></span> </span>, two 1D mini-batch <code>Tensors</code>, and a label 1D mini-batch tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss\" title=\"torch.nn.HingeEmbeddingLoss\"><code>nn.HingeEmbeddingLoss</code></a></p></td> <td><p>Measures the loss given an input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> and a labels tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss\" title=\"torch.nn.MultiLabelMarginLoss\"><code>nn.MultiLabelMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> (a 2D mini-batch <code>Tensor</code>) and output <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> (which is a 2D <code>Tensor</code> of target class indices).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss\" title=\"torch.nn.SmoothL1Loss\"><code>nn.SmoothL1Loss</code></a></p></td> <td><p>Creates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss\" title=\"torch.nn.SoftMarginLoss\"><code>nn.SoftMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a two-class classification logistic loss between input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> and target tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> (containing 1 or -1).</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss\" title=\"torch.nn.MultiLabelSoftMarginLoss\"><code>nn.MultiLabelSoftMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> and target <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> of size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C)</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss\" title=\"torch.nn.CosineEmbeddingLoss\"><code>nn.CosineEmbeddingLoss</code></a></p></td> <td><p>Creates a criterion that measures the loss given input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_1</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mn>2</mn></msub></mrow><annotation encoding=\"application/x-tex\">x_2</annotation></semantics></math></span></span> </span> and a <code>Tensor</code> label <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> with values 1 or -1.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss\" title=\"torch.nn.MultiMarginLoss\"><code>nn.MultiMarginLoss</code></a></p></td> <td><p>Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span></span> </span> (a 2D mini-batch <code>Tensor</code>) and output <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi></mrow><annotation encoding=\"application/x-tex\">y</annotation></semantics></math></span></span> </span> (which is a 1D tensor of target class indices, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>y</mi><mo>≤</mo><mtext>x.size</mtext><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 \\leq y \\leq \\text{x.size}(1)-1</annotation></semantics></math></span></span> </span>):</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss\" title=\"torch.nn.TripletMarginLoss\"><code>nn.TripletMarginLoss</code></a></p></td> <td><p>Creates a criterion that measures the triplet loss given an input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x1</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">x2</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">x3</annotation></semantics></math></span></span> </span> and a margin with a value greater than <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span></span> </span>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss\" title=\"torch.nn.TripletMarginWithDistanceLoss\"><code>nn.TripletMarginWithDistanceLoss</code></a></p></td> <td><p>Creates a criterion that measures the triplet loss given input tensors <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>a</mi></mrow><annotation encoding=\"application/x-tex\">a</annotation></semantics></math></span></span> </span>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi></mrow><annotation encoding=\"application/x-tex\">p</annotation></semantics></math></span></span> </span>, and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>n</mi></mrow><annotation encoding=\"application/x-tex\">n</annotation></semantics></math></span></span> </span> (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (“distance function”) used to compute the relationship between the anchor and positive example (“positive distance”) and the anchor and negative example (“negative distance”).</p></td> </tr>  </table>   <h2 id=\"vision-layers\">Vision Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>nn.PixelShuffle</code></a></p></td> <td><p>Rearranges elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span> </span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span> </span>, where r is an upscale factor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle\" title=\"torch.nn.PixelUnshuffle\"><code>nn.PixelUnshuffle</code></a></p></td> <td><p>Reverses the <a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>PixelShuffle</code></a> operation by rearranging elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span> </span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span> </span>, where r is a downscale factor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsample#torch.nn.Upsample\" title=\"torch.nn.Upsample\"><code>nn.Upsample</code></a></p></td> <td><p>Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsamplingnearest2d#torch.nn.UpsamplingNearest2d\" title=\"torch.nn.UpsamplingNearest2d\"><code>nn.UpsamplingNearest2d</code></a></p></td> <td><p>Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.upsamplingbilinear2d#torch.nn.UpsamplingBilinear2d\" title=\"torch.nn.UpsamplingBilinear2d\"><code>nn.UpsamplingBilinear2d</code></a></p></td> <td><p>Applies a 2D bilinear upsampling to an input signal composed of several input channels.</p></td> </tr>  </table>   <h2 id=\"shuffle-layers\">Shuffle Layers</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.channelshuffle#torch.nn.ChannelShuffle\" title=\"torch.nn.ChannelShuffle\"><code>nn.ChannelShuffle</code></a></p></td> <td><p>Divide the channels in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C , H, W)</annotation></semantics></math></span></span> </span> into g groups and rearrange them as <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mfrac><mi>g</mi><mo separator=\"true\">,</mo></mfrac><mi>g</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\frac g, g, H, W)</annotation></semantics></math></span></span> </span>, while keeping the original tensor shape.</p></td> </tr>  </table>   <h2 id=\"dataparallel-layers-multi-gpu-distributed\">DataParallel Layers (multi-GPU, distributed)</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>nn.DataParallel</code></a></p></td> <td><p>Implements data parallelism at the module level.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>nn.parallel.DistributedDataParallel</code></a></p></td> <td><p>Implements distributed data parallelism that is based on <code>torch.distributed</code> package at the module level.</p></td> </tr>  </table>   <h2 id=\"utilities\">Utilities</h2> <p>From the <code>torch.nn.utils</code> module</p> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.clip_grad_norm_#torch.nn.utils.clip_grad_norm_\" title=\"torch.nn.utils.clip_grad_norm_\"><code>clip_grad_norm_</code></a>\n</td> <td><p>Clips gradient norm of an iterable of parameters.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.clip_grad_value_#torch.nn.utils.clip_grad_value_\" title=\"torch.nn.utils.clip_grad_value_\"><code>clip_grad_value_</code></a>\n</td> <td><p>Clips gradient of an iterable of parameters at specified value.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.parameters_to_vector#torch.nn.utils.parameters_to_vector\" title=\"torch.nn.utils.parameters_to_vector\"><code>parameters_to_vector</code></a>\n</td> <td><p>Convert parameters to one vector</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.vector_to_parameters#torch.nn.utils.vector_to_parameters\" title=\"torch.nn.utils.vector_to_parameters\"><code>vector_to_parameters</code></a>\n</td> <td><p>Convert one vector to the parameters</p></td> </tr>  </table> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.basepruningmethod#torch.nn.utils.prune.BasePruningMethod\" title=\"torch.nn.utils.prune.BasePruningMethod\"><code>prune.BasePruningMethod</code></a></p></td> <td><p>Abstract base class for creation of new pruning techniques.</p></td> </tr>  </table> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.pruningcontainer#torch.nn.utils.prune.PruningContainer\" title=\"torch.nn.utils.prune.PruningContainer\"><code>prune.PruningContainer</code></a></p></td> <td><p>Container holding a sequence of pruning methods for iterative pruning.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.Identity\" title=\"torch.nn.utils.prune.Identity\"><code>prune.Identity</code></a></p></td> <td><p>Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.randomunstructured#torch.nn.utils.prune.RandomUnstructured\" title=\"torch.nn.utils.prune.RandomUnstructured\"><code>prune.RandomUnstructured</code></a></p></td> <td><p>Prune (currently unpruned) units in a tensor at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.l1unstructured#torch.nn.utils.prune.L1Unstructured\" title=\"torch.nn.utils.prune.L1Unstructured\"><code>prune.L1Unstructured</code></a></p></td> <td><p>Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.randomstructured#torch.nn.utils.prune.RandomStructured\" title=\"torch.nn.utils.prune.RandomStructured\"><code>prune.RandomStructured</code></a></p></td> <td><p>Prune entire (currently unpruned) channels in a tensor at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.lnstructured#torch.nn.utils.prune.LnStructured\" title=\"torch.nn.utils.prune.LnStructured\"><code>prune.LnStructured</code></a></p></td> <td><p>Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.customfrommask#torch.nn.utils.prune.CustomFromMask\" title=\"torch.nn.utils.prune.CustomFromMask\"><code>prune.CustomFromMask</code></a></p></td> <td></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.identity#torch.nn.utils.prune.identity\" title=\"torch.nn.utils.prune.identity\"><code>prune.identity</code></a></p></td> <td><p>Applies pruning reparametrization to the tensor corresponding to the parameter called <code>name</code> in <code>module</code> without actually pruning any units.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.random_unstructured#torch.nn.utils.prune.random_unstructured\" title=\"torch.nn.utils.prune.random_unstructured\"><code>prune.random_unstructured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) units selected at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.l1_unstructured#torch.nn.utils.prune.l1_unstructured\" title=\"torch.nn.utils.prune.l1_unstructured\"><code>prune.l1_unstructured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) units with the lowest L1-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.random_structured#torch.nn.utils.prune.random_structured\" title=\"torch.nn.utils.prune.random_structured\"><code>prune.random_structured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) channels along the specified <code>dim</code> selected at random.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.ln_structured#torch.nn.utils.prune.ln_structured\" title=\"torch.nn.utils.prune.ln_structured\"><code>prune.ln_structured</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by removing the specified <code>amount</code> of (currently unpruned) channels along the specified <code>dim</code> with the lowest L``n``-norm.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.global_unstructured#torch.nn.utils.prune.global_unstructured\" title=\"torch.nn.utils.prune.global_unstructured\"><code>prune.global_unstructured</code></a></p></td> <td><p>Globally prunes tensors corresponding to all parameters in <code>parameters</code> by applying the specified <code>pruning_method</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.custom_from_mask#torch.nn.utils.prune.custom_from_mask\" title=\"torch.nn.utils.prune.custom_from_mask\"><code>prune.custom_from_mask</code></a></p></td> <td><p>Prunes tensor corresponding to parameter called <code>name</code> in <code>module</code> by applying the pre-computed mask in <code>mask</code>.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.remove#torch.nn.utils.prune.remove\" title=\"torch.nn.utils.prune.remove\"><code>prune.remove</code></a></p></td> <td><p>Removes the pruning reparameterization from a module and the pruning method from the forward hook.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.prune.is_pruned#torch.nn.utils.prune.is_pruned\" title=\"torch.nn.utils.prune.is_pruned\"><code>prune.is_pruned</code></a></p></td> <td><p>Check whether <code>module</code> is pruned by looking for <code>forward_pre_hooks</code> in its modules that inherit from the <code>BasePruningMethod</code>.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.weight_norm#torch.nn.utils.weight_norm\" title=\"torch.nn.utils.weight_norm\"><code>weight_norm</code></a>\n</td> <td><p>Applies weight normalization to a parameter in the given module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.remove_weight_norm#torch.nn.utils.remove_weight_norm\" title=\"torch.nn.utils.remove_weight_norm\"><code>remove_weight_norm</code></a>\n</td> <td><p>Removes the weight normalization reparameterization from a module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.spectral_norm#torch.nn.utils.spectral_norm\" title=\"torch.nn.utils.spectral_norm\"><code>spectral_norm</code></a>\n</td> <td><p>Applies spectral normalization to a parameter in the given module.</p></td> </tr> <tr>\n<td>\n\n\n<a class=\"reference internal\" href=\"generated/torch.nn.utils.remove_spectral_norm#torch.nn.utils.remove_spectral_norm\" title=\"torch.nn.utils.remove_spectral_norm\"><code>remove_spectral_norm</code></a>\n</td> <td><p>Removes the spectral normalization reparameterization from a module.</p></td> </tr>  </table> <p>Utility functions in other modules</p> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.packedsequence#torch.nn.utils.rnn.PackedSequence\" title=\"torch.nn.utils.rnn.PackedSequence\"><code>nn.utils.rnn.PackedSequence</code></a></p></td> <td><p>Holds the data and list of <code>batch_sizes</code> of a packed sequence.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence\" title=\"torch.nn.utils.rnn.pack_padded_sequence\"><code>nn.utils.rnn.pack_padded_sequence</code></a></p></td> <td><p>Packs a Tensor containing padded sequences of variable length.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence\" title=\"torch.nn.utils.rnn.pad_packed_sequence\"><code>nn.utils.rnn.pad_packed_sequence</code></a></p></td> <td><p>Pads a packed batch of variable length sequences.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pad_sequence#torch.nn.utils.rnn.pad_sequence\" title=\"torch.nn.utils.rnn.pad_sequence\"><code>nn.utils.rnn.pad_sequence</code></a></p></td> <td><p>Pad a list of variable length Tensors with <code>padding_value</code></p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.utils.rnn.pack_sequence#torch.nn.utils.rnn.pack_sequence\" title=\"torch.nn.utils.rnn.pack_sequence\"><code>nn.utils.rnn.pack_sequence</code></a></p></td> <td><p>Packs a list of variable length Tensors</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.flatten#torch.nn.Flatten\" title=\"torch.nn.Flatten\"><code>nn.Flatten</code></a></p></td> <td><p>Flattens a contiguous range of dims into a tensor.</p></td> </tr> <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.unflatten#torch.nn.Unflatten\" title=\"torch.nn.Unflatten\"><code>nn.Unflatten</code></a></p></td> <td><p>Unflattens a tensor dim expanding it to a desired shape.</p></td> </tr>  </table>   <h2 id=\"quantized-functions\">Quantized Functions</h2> <p>Quantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the <a class=\"reference internal\" href=\"quantization#quantization-doc\"><span class=\"std std-ref\">Quantization</span></a> documentation.</p>   <h2 id=\"lazy-modules-initialization\">Lazy Modules Initialization</h2> <table class=\"longtable docutils colwidths-auto align-default\">  <tr>\n<td><p><a class=\"reference internal\" href=\"generated/torch.nn.modules.lazy.lazymodulemixin#torch.nn.modules.lazy.LazyModuleMixin\" title=\"torch.nn.modules.lazy.LazyModuleMixin\"><code>nn.modules.lazy.LazyModuleMixin</code></a></p></td> <td><p>A mixin for modules that lazily initialize parameters, also known as “lazy modules.”</p></td> </tr>  </table><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/nn.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/nn.html</a>\n  </p>\n</div>\n","nn.functional":"<h1 id=\"torch-nn-functional\">torch.nn.functional</h1>  <h2 id=\"convolution-functions\">Convolution functions</h2>  <h3 id=\"conv1d\"><span class=\"hidden-section\">conv1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv1d\">\n<code>torch.nn.functional.conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor</code> </dt> <dd>\n<p>Applies a 1D convolution over an input signal composed of several input planes.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.conv1d#torch.nn.Conv1d\" title=\"torch.nn.Conv1d\"><code>Conv1d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: <code>None</code>\n</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a one-element tuple <code>(sW,)</code>. Default: 1</li> <li>\n<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a one-element tuple <code>(padW,)</code>. Default: 0</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a one-element tuple <code>(dW,)</code>. Default: 1</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; filters = torch.randn(33, 16, 3)\n&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)\n&gt;&gt;&gt; F.conv1d(inputs, filters)\n</pre> </dd>\n</dl>   <h3 id=\"conv2d\"><span class=\"hidden-section\">conv2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv2d\">\n<code>torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor</code> </dt> <dd>\n<p>Applies a 2D convolution over an input image composed of several input planes.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.conv2d#torch.nn.Conv2d\" title=\"torch.nn.Conv2d\"><code>Conv2d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: <code>None</code>\n</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sH, sW)</code>. Default: 1</li> <li>\n<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a tuple <code>(padH, padW)</code>. Default: 0</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dH, dW)</code>. Default: 1</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; filters = torch.randn(8,4,3,3)\n&gt;&gt;&gt; inputs = torch.randn(1,4,5,5)\n&gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)\n</pre> </dd>\n</dl>   <h3 id=\"conv3d\"><span class=\"hidden-section\">conv3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv3d\">\n<code>torch.nn.functional.conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor</code> </dt> <dd>\n<p>Applies a 3D convolution over an input image composed of several input planes.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.conv3d#torch.nn.Conv3d\" title=\"torch.nn.Conv3d\"><code>Conv3d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>T</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>T</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: None</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sT, sH, sW)</code>. Default: 1</li> <li>\n<strong>padding</strong> – implicit paddings on both sides of the input. Can be a single number or a tuple <code>(padT, padH, padW)</code>. Default: 0</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dT, dH, dW)</code>. Default: 1</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)\n&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)\n&gt;&gt;&gt; F.conv3d(inputs, filters)\n</pre> </dd>\n</dl>   <h3 id=\"conv-transpose1d\"><span class=\"hidden-section\">conv_transpose1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv_transpose1d\">\n<code>torch.nn.functional.conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor</code> </dt> <dd>\n<p>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called “deconvolution”.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose1d#torch.nn.ConvTranspose1d\" title=\"torch.nn.ConvTranspose1d\"><code>ConvTranspose1d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>out_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: None</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sW,)</code>. Default: 1</li> <li>\n<strong>padding</strong> – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple <code>(padW,)</code>. Default: 0</li> <li>\n<strong>output_padding</strong> – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple <code>(out_padW)</code>. Default: 0</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dW,)</code>. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; inputs = torch.randn(20, 16, 50)\n&gt;&gt;&gt; weights = torch.randn(16, 33, 5)\n&gt;&gt;&gt; F.conv_transpose1d(inputs, weights)\n</pre> </dd>\n</dl>   <h3 id=\"conv-transpose2d\"><span class=\"hidden-section\">conv_transpose2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv_transpose2d\">\n<code>torch.nn.functional.conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor</code> </dt> <dd>\n<p>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called “deconvolution”.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose2d#torch.nn.ConvTranspose2d\" title=\"torch.nn.ConvTranspose2d\"><code>ConvTranspose2d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>out_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: None</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sH, sW)</code>. Default: 1</li> <li>\n<strong>padding</strong> – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple <code>(padH, padW)</code>. Default: 0</li> <li>\n<strong>output_padding</strong> – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple <code>(out_padH, out_padW)</code>. Default: 0</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dH, dW)</code>. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; # With square kernels and equal stride\n&gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)\n&gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)\n&gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)\n</pre> </dd>\n</dl>   <h3 id=\"conv-transpose3d\"><span class=\"hidden-section\">conv_transpose3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.conv_transpose3d\">\n<code>torch.nn.functional.conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) → Tensor</code> </dt> <dd>\n<p>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called “deconvolution”</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.convtranspose3d#torch.nn.ConvTranspose3d\" title=\"torch.nn.ConvTranspose3d\"><code>ConvTranspose3d</code></a> for details and output shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>T</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>weight</strong> – filters of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mfrac><mtext>out_channels</mtext><mtext>groups</mtext></mfrac><mo separator=\"true\">,</mo><mi>k</mi><mi>T</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>k</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>bias</strong> – optional bias of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_channels</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_channels})</annotation></semantics></math></span></span> </span>. Default: None</li> <li>\n<strong>stride</strong> – the stride of the convolving kernel. Can be a single number or a tuple <code>(sT, sH, sW)</code>. Default: 1</li> <li>\n<strong>padding</strong> – <code>dilation * (kernel_size - 1) - padding</code> zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple <code>(padT, padH, padW)</code>. Default: 0</li> <li>\n<strong>output_padding</strong> – additional size added to one side of each dimension in the output shape. Can be a single number or a tuple <code>(out_padT, out_padH, out_padW)</code>. Default: 0</li> <li>\n<strong>groups</strong> – split input into groups, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>in_channels</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{in\\_channels}</annotation></semantics></math></span></span> </span> should be divisible by the number of groups. Default: 1</li> <li>\n<strong>dilation</strong> – the spacing between kernel elements. Can be a single number or a tuple <code>(dT, dH, dW)</code>. Default: 1</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)\n&gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)\n&gt;&gt;&gt; F.conv_transpose3d(inputs, weights)\n</pre> </dd>\n</dl>   <h3 id=\"unfold\"><span class=\"hidden-section\">unfold</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.unfold\">\n<code>torch.nn.functional.unfold(input, kernel_size, dilation=1, padding=0, stride=1)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#unfold\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Extracts sliding local blocks from a batched input tensor.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Currently, only 4-D input tensors (batched image-like tensors) are supported.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.</p> </div> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.unfold#torch.nn.Unfold\" title=\"torch.nn.Unfold\"><code>torch.nn.Unfold</code></a> for details</p> </dd>\n</dl>   <h3 id=\"fold\"><span class=\"hidden-section\">fold</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.fold\">\n<code>torch.nn.functional.fold(input, output_size, kernel_size, dilation=1, padding=0, stride=1)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#fold\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Combines an array of sliding local blocks into a large containing tensor.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Currently, only 3-D output tensors (unfolded batched image-like tensors) are supported.</p> </div> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.fold#torch.nn.Fold\" title=\"torch.nn.Fold\"><code>torch.nn.Fold</code></a> for details</p> </dd>\n</dl>    <h2 id=\"pooling-functions\">Pooling functions</h2>  <h3 id=\"avg-pool1d\"><span class=\"hidden-section\">avg_pool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.avg_pool1d\">\n<code>torch.nn.functional.avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) → Tensor</code> </dt> <dd>\n<p>Applies a 1D average pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.avgpool1d#torch.nn.AvgPool1d\" title=\"torch.nn.AvgPool1d\"><code>AvgPool1d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>kernel_size</strong> – the size of the window. Can be a single number or a tuple <code>(kW,)</code>\n</li> <li>\n<strong>stride</strong> – the stride of the window. Can be a single number or a tuple <code>(sW,)</code>. Default: <code>kernel_size</code>\n</li> <li>\n<strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a single number or a tuple <code>(padW,)</code>. Default: 0</li> <li>\n<strong>ceil_mode</strong> – when True, will use <code>ceil</code> instead of <code>floor</code> to compute the output shape. Default: <code>False</code>\n</li> <li>\n<strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation. Default: <code>True</code>\n</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n&gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)\ntensor([[[ 2.,  4.,  6.]]])\n</pre> </dd>\n</dl>   <h3 id=\"avg-pool2d\"><span class=\"hidden-section\">avg_pool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.avg_pool2d\">\n<code>torch.nn.functional.avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) → Tensor</code> </dt> <dd>\n<p>Applies 2D average-pooling operation in <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">kH \\times kW</annotation></semantics></math></span></span> </span> regions by step size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">sH \\times sW</annotation></semantics></math></span></span> </span> steps. The number of output features is equal to the number of input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.avgpool2d#torch.nn.AvgPool2d\" title=\"torch.nn.AvgPool2d\"><code>AvgPool2d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>kernel_size</strong> – size of the pooling region. Can be a single number or a tuple <code>(kH, kW)</code>\n</li> <li>\n<strong>stride</strong> – stride of the pooling operation. Can be a single number or a tuple <code>(sH, sW)</code>. Default: <code>kernel_size</code>\n</li> <li>\n<strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a single number or a tuple <code>(padH, padW)</code>. Default: 0</li> <li>\n<strong>ceil_mode</strong> – when True, will use <code>ceil</code> instead of <code>floor</code> in the formula to compute the output shape. Default: <code>False</code>\n</li> <li>\n<strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation. Default: <code>True</code>\n</li> <li>\n<strong>divisor_override</strong> – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"avg-pool3d\"><span class=\"hidden-section\">avg_pool3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.avg_pool3d\">\n<code>torch.nn.functional.avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) → Tensor</code> </dt> <dd>\n<p>Applies 3D average-pooling operation in <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>k</mi><mi>T</mi><mo>×</mo><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">kT \\times kH \\times kW</annotation></semantics></math></span></span> </span> regions by step size <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>T</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">sT \\times sH \\times sW</annotation></semantics></math></span></span> </span> steps. The number of output features is equal to <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">⌊</mo><mfrac><mtext>input planes</mtext><mrow><mi>s</mi><mi>T</mi></mrow></mfrac><mo stretchy=\"false\">⌋</mo></mrow><annotation encoding=\"application/x-tex\">\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.avgpool3d#torch.nn.AvgPool3d\" title=\"torch.nn.AvgPool3d\"><code>AvgPool3d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>minibatch</mtext><mo separator=\"true\">,</mo><mtext>in_channels</mtext><mo separator=\"true\">,</mo><mi>i</mi><mi>T</mi><mo>×</mo><mi>i</mi><mi>H</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)</annotation></semantics></math></span></span> </span>\n</li> <li>\n<strong>kernel_size</strong> – size of the pooling region. Can be a single number or a tuple <code>(kT, kH, kW)</code>\n</li> <li>\n<strong>stride</strong> – stride of the pooling operation. Can be a single number or a tuple <code>(sT, sH, sW)</code>. Default: <code>kernel_size</code>\n</li> <li>\n<strong>padding</strong> – implicit zero paddings on both sides of the input. Can be a single number or a tuple <code>(padT, padH, padW)</code>, Default: 0</li> <li>\n<strong>ceil_mode</strong> – when True, will use <code>ceil</code> instead of <code>floor</code> in the formula to compute the output shape</li> <li>\n<strong>count_include_pad</strong> – when True, will include the zero-padding in the averaging calculation</li> <li>\n<strong>divisor_override</strong> – if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"max-pool1d\"><span class=\"hidden-section\">max_pool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_pool1d\">\n<code>torch.nn.functional.max_pool1d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 1D max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxpool1d#torch.nn.MaxPool1d\" title=\"torch.nn.MaxPool1d\"><code>MaxPool1d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"max-pool2d\"><span class=\"hidden-section\">max_pool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_pool2d\">\n<code>torch.nn.functional.max_pool2d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 2D max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxpool2d#torch.nn.MaxPool2d\" title=\"torch.nn.MaxPool2d\"><code>MaxPool2d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"max-pool3d\"><span class=\"hidden-section\">max_pool3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_pool3d\">\n<code>torch.nn.functional.max_pool3d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 3D max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxpool3d#torch.nn.MaxPool3d\" title=\"torch.nn.MaxPool3d\"><code>MaxPool3d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"max-unpool1d\"><span class=\"hidden-section\">max_unpool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_unpool1d\">\n<code>torch.nn.functional.max_unpool1d(input, indices, kernel_size, stride=None, padding=0, output_size=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#max_unpool1d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes a partial inverse of <code>MaxPool1d</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxunpool1d#torch.nn.MaxUnpool1d\" title=\"torch.nn.MaxUnpool1d\"><code>MaxUnpool1d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"max-unpool2d\"><span class=\"hidden-section\">max_unpool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_unpool2d\">\n<code>torch.nn.functional.max_unpool2d(input, indices, kernel_size, stride=None, padding=0, output_size=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#max_unpool2d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes a partial inverse of <code>MaxPool2d</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxunpool2d#torch.nn.MaxUnpool2d\" title=\"torch.nn.MaxUnpool2d\"><code>MaxUnpool2d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"max-unpool3d\"><span class=\"hidden-section\">max_unpool3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.max_unpool3d\">\n<code>torch.nn.functional.max_unpool3d(input, indices, kernel_size, stride=None, padding=0, output_size=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#max_unpool3d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes a partial inverse of <code>MaxPool3d</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.maxunpool3d#torch.nn.MaxUnpool3d\" title=\"torch.nn.MaxUnpool3d\"><code>MaxUnpool3d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"lp-pool1d\"><span class=\"hidden-section\">lp_pool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.lp_pool1d\">\n<code>torch.nn.functional.lp_pool1d(input, norm_type, kernel_size, stride=None, ceil_mode=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#lp_pool1d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of <code>p</code> is zero, the gradient is set to zero as well.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.lppool1d#torch.nn.LPPool1d\" title=\"torch.nn.LPPool1d\"><code>LPPool1d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"lp-pool2d\"><span class=\"hidden-section\">lp_pool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.lp_pool2d\">\n<code>torch.nn.functional.lp_pool2d(input, norm_type, kernel_size, stride=None, ceil_mode=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#lp_pool2d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of <code>p</code> is zero, the gradient is set to zero as well.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.lppool2d#torch.nn.LPPool2d\" title=\"torch.nn.LPPool2d\"><code>LPPool2d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"adaptive-max-pool1d\"><span class=\"hidden-section\">adaptive_max_pool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_max_pool1d\">\n<code>torch.nn.functional.adaptive_max_pool1d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool1d#torch.nn.AdaptiveMaxPool1d\" title=\"torch.nn.AdaptiveMaxPool1d\"><code>AdaptiveMaxPool1d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output_size</strong> – the target output size (single integer)</li> <li>\n<strong>return_indices</strong> – whether to return pooling indices. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"adaptive-max-pool2d\"><span class=\"hidden-section\">adaptive_max_pool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_max_pool2d\">\n<code>torch.nn.functional.adaptive_max_pool2d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d\" title=\"torch.nn.AdaptiveMaxPool2d\"><code>AdaptiveMaxPool2d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output_size</strong> – the target output size (single integer or double-integer tuple)</li> <li>\n<strong>return_indices</strong> – whether to return pooling indices. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"adaptive-max-pool3d\"><span class=\"hidden-section\">adaptive_max_pool3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_max_pool3d\">\n<code>torch.nn.functional.adaptive_max_pool3d(*args, **kwargs)</code> </dt> <dd>\n<p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptivemaxpool3d#torch.nn.AdaptiveMaxPool3d\" title=\"torch.nn.AdaptiveMaxPool3d\"><code>AdaptiveMaxPool3d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>output_size</strong> – the target output size (single integer or triple-integer tuple)</li> <li>\n<strong>return_indices</strong> – whether to return pooling indices. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"adaptive-avg-pool1d\"><span class=\"hidden-section\">adaptive_avg_pool1d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_avg_pool1d\">\n<code>torch.nn.functional.adaptive_avg_pool1d(input, output_size) → Tensor</code> </dt> <dd>\n<p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool1d#torch.nn.AdaptiveAvgPool1d\" title=\"torch.nn.AdaptiveAvgPool1d\"><code>AdaptiveAvgPool1d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>output_size</strong> – the target output size (single integer)</p> </dd> </dl> </dd>\n</dl>   <h3 id=\"adaptive-avg-pool2d\"><span class=\"hidden-section\">adaptive_avg_pool2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_avg_pool2d\">\n<code>torch.nn.functional.adaptive_avg_pool2d(input, output_size)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#adaptive_avg_pool2d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d\" title=\"torch.nn.AdaptiveAvgPool2d\"><code>AdaptiveAvgPool2d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>output_size</strong> – the target output size (single integer or double-integer tuple)</p> </dd> </dl> </dd>\n</dl>   <h3 id=\"adaptive-avg-pool3d\"><span class=\"hidden-section\">adaptive_avg_pool3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.adaptive_avg_pool3d\">\n<code>torch.nn.functional.adaptive_avg_pool3d(input, output_size)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#adaptive_avg_pool3d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d\" title=\"torch.nn.AdaptiveAvgPool3d\"><code>AdaptiveAvgPool3d</code></a> for details and output shape.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>output_size</strong> – the target output size (single integer or triple-integer tuple)</p> </dd> </dl> </dd>\n</dl>    <h2 id=\"non-linear-activation-functions\">Non-linear activation functions</h2>  <h3 id=\"threshold\"><span class=\"hidden-section\">threshold</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.threshold\">\n<code>torch.nn.functional.threshold(input, threshold, value, inplace=False)</code> </dt> <dd>\n<p>Thresholds each element of the input Tensor.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.threshold#torch.nn.Threshold\" title=\"torch.nn.Threshold\"><code>Threshold</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.threshold_\">\n<code>torch.nn.functional.threshold_(input, threshold, value) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.threshold\" title=\"torch.nn.functional.threshold\"><code>threshold()</code></a>.</p> </dd>\n</dl>   <h3 id=\"relu\"><span class=\"hidden-section\">relu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.relu\">\n<code>torch.nn.functional.relu(input, inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#relu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the rectified linear unit function element-wise. See <a class=\"reference internal\" href=\"generated/torch.nn.relu#torch.nn.ReLU\" title=\"torch.nn.ReLU\"><code>ReLU</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.relu_\">\n<code>torch.nn.functional.relu_(input) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.relu\" title=\"torch.nn.functional.relu\"><code>relu()</code></a>.</p> </dd>\n</dl>   <h3 id=\"hardtanh\"><span class=\"hidden-section\">hardtanh</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.hardtanh\">\n<code>torch.nn.functional.hardtanh(input, min_val=-1., max_val=1., inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#hardtanh\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the HardTanh function element-wise. See <a class=\"reference internal\" href=\"generated/torch.nn.hardtanh#torch.nn.Hardtanh\" title=\"torch.nn.Hardtanh\"><code>Hardtanh</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.hardtanh_\">\n<code>torch.nn.functional.hardtanh_(input, min_val=-1., max_val=1.) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.hardtanh\" title=\"torch.nn.functional.hardtanh\"><code>hardtanh()</code></a>.</p> </dd>\n</dl>   <h3 id=\"hardswish\"><span class=\"hidden-section\">hardswish</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.hardswish\">\n<code>torch.nn.functional.hardswish(input, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#hardswish\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the hardswish function, element-wise, as described in the paper:</p> <p><a class=\"reference external\" href=\"https://arxiv.org/abs/1905.02244\">Searching for MobileNetV3</a>.</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Hardswish</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.3599999999999999em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mo>−</mo><mn>3</mn><mo separator=\"true\">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mi>x</mi></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>≥</mo><mo>+</mo><mn>3</mn><mo separator=\"true\">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>x</mi><mo>⋅</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo>+</mo><mn>3</mn><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">/</mi><mn>6</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Hardswish}(x) = \\begin{cases} 0 &amp; \\text{if~} x \\le -3, \\\\ x &amp; \\text{if~} x \\ge +3, \\\\ x \\cdot (x + 3) /6 &amp; \\text{otherwise} \\end{cases} </annotation></semantics></math></span></span></span> </div>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.hardswish#torch.nn.Hardswish\" title=\"torch.nn.Hardswish\"><code>Hardswish</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"relu6\"><span class=\"hidden-section\">relu6</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.relu6\">\n<code>torch.nn.functional.relu6(input, inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#relu6\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the element-wise function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>ReLU6</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mn>6</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{ReLU6}(x) = \\min(\\max(0,x), 6)</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.relu6#torch.nn.ReLU6\" title=\"torch.nn.ReLU6\"><code>ReLU6</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"elu\"><span class=\"hidden-section\">elu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.elu\">\n<code>torch.nn.functional.elu(input, alpha=1.0, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#elu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>ELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>α</mi><mo>∗</mo><mo stretchy=\"false\">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.elu#torch.nn.ELU\" title=\"torch.nn.ELU\"><code>ELU</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.elu_\">\n<code>torch.nn.functional.elu_(input, alpha=1.) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.elu\" title=\"torch.nn.functional.elu\"><code>elu()</code></a>.</p> </dd>\n</dl>   <h3 id=\"selu\"><span class=\"hidden-section\">selu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.selu\">\n<code>torch.nn.functional.selu(input, inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#selu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>SELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>∗</mo><mo stretchy=\"false\">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>α</mi><mo>∗</mo><mo stretchy=\"false\">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))</annotation></semantics></math></span></span> </span>, with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=1.6732632423543772848170429916717</annotation></semantics></math></span></span> </span> and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding=\"application/x-tex\">scale=1.0507009873554804934193349852946</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.selu#torch.nn.SELU\" title=\"torch.nn.SELU\"><code>SELU</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"celu\"><span class=\"hidden-section\">celu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.celu\">\n<code>torch.nn.functional.celu(input, alpha=1., inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#celu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>CELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>α</mi><mo>∗</mo><mo stretchy=\"false\">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mi mathvariant=\"normal\">/</mi><mi>α</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.celu#torch.nn.CELU\" title=\"torch.nn.CELU\"><code>CELU</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"leaky-relu\"><span class=\"hidden-section\">leaky_relu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.leaky_relu\">\n<code>torch.nn.functional.leaky_relu(input, negative_slope=0.01, inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#leaky_relu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.leakyrelu#torch.nn.LeakyReLU\" title=\"torch.nn.LeakyReLU\"><code>LeakyReLU</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.leaky_relu_\">\n<code>torch.nn.functional.leaky_relu_(input, negative_slope=0.01) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.leaky_relu\" title=\"torch.nn.functional.leaky_relu\"><code>leaky_relu()</code></a>.</p> </dd>\n</dl>   <h3 id=\"prelu\"><span class=\"hidden-section\">prelu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.prelu\">\n<code>torch.nn.functional.prelu(input, weight) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#prelu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>PReLU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>weight</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)</annotation></semantics></math></span></span> </span> where weight is a learnable parameter.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.prelu#torch.nn.PReLU\" title=\"torch.nn.PReLU\"><code>PReLU</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"rrelu\"><span class=\"hidden-section\">rrelu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.rrelu\">\n<code>torch.nn.functional.rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#rrelu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Randomized leaky ReLU.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.rrelu#torch.nn.RReLU\" title=\"torch.nn.RReLU\"><code>RReLU</code></a> for more details.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.nn.functional.rrelu_\">\n<code>torch.nn.functional.rrelu_(input, lower=1./8, upper=1./3, training=False) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.nn.functional.rrelu\" title=\"torch.nn.functional.rrelu\"><code>rrelu()</code></a>.</p> </dd>\n</dl>   <h3 id=\"glu\"><span class=\"hidden-section\">glu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.glu\">\n<code>torch.nn.functional.glu(input, dim=-1) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#glu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>The gated linear unit. Computes:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>GLU</mtext><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>b</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>a</mi><mo>⊗</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>b</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GLU}(a, b) = a \\otimes \\sigma(b) </annotation></semantics></math></span></span></span> </div>\n<p>where <code>input</code> is split in half along <code>dim</code> to form <code>a</code> and <code>b</code>, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span></span> </span> is the sigmoid function and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding=\"application/x-tex\">\\otimes</annotation></semantics></math></span></span> </span> is the element-wise product between matrices.</p> <p>See <a class=\"reference external\" href=\"https://arxiv.org/abs/1612.08083\">Language Modeling with Gated Convolutional Networks</a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input tensor</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – dimension on which to split the input. Default: -1</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"gelu\"><span class=\"hidden-section\">gelu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.gelu\">\n<code>torch.nn.functional.gelu(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#gelu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>GELU</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{GELU}(x) = x * \\Phi(x)</annotation></semantics></math></span></span> </span></p> <p>where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\Phi(x)</annotation></semantics></math></span></span> </span> is the Cumulative Distribution Function for Gaussian Distribution.</p> <p>See <a class=\"reference external\" href=\"https://arxiv.org/abs/1606.08415\">Gaussian Error Linear Units (GELUs)</a>.</p> </dd>\n</dl>   <h3 id=\"logsigmoid\"><span class=\"hidden-section\">logsigmoid</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.logsigmoid\">\n<code>torch.nn.functional.logsigmoid(input) → Tensor</code> </dt> <dd>\n<p>Applies element-wise <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>LogSigmoid</mtext><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.logsigmoid#torch.nn.LogSigmoid\" title=\"torch.nn.LogSigmoid\"><code>LogSigmoid</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"hardshrink\"><span class=\"hidden-section\">hardshrink</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.hardshrink\">\n<code>torch.nn.functional.hardshrink(input, lambd=0.5) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#hardshrink\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the hard shrinkage function element-wise</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.hardshrink#torch.nn.Hardshrink\" title=\"torch.nn.Hardshrink\"><code>Hardshrink</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"tanhshrink\"><span class=\"hidden-section\">tanhshrink</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.tanhshrink\">\n<code>torch.nn.functional.tanhshrink(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#tanhshrink\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Tanhshrink</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi><mo>−</mo><mtext>Tanh</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.tanhshrink#torch.nn.Tanhshrink\" title=\"torch.nn.Tanhshrink\"><code>Tanhshrink</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"softsign\"><span class=\"hidden-section\">softsign</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.softsign\">\n<code>torch.nn.functional.softsign(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#softsign\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>SoftSign</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mi mathvariant=\"normal\">∣</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{SoftSign}(x) = \\frac{x}{1 + |x|}</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.softsign#torch.nn.Softsign\" title=\"torch.nn.Softsign\"><code>Softsign</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"softplus\"><span class=\"hidden-section\">softplus</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.softplus\">\n<code>torch.nn.functional.softplus(input, beta=1, threshold=20) → Tensor</code> </dt> <dd>\n<p>Applies element-wise, the function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Softplus</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>β</mi></mfrac><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>β</mi><mo>∗</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))</annotation></semantics></math></span></span> </span>.</p> <p>For numerical stability the implementation reverts to the linear function when <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>×</mo><mi>β</mi><mo>&gt;</mo><mi>t</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>h</mi><mi>o</mi><mi>l</mi><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">input \\times \\beta &gt; threshold</annotation></semantics></math></span></span> </span>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.softplus#torch.nn.Softplus\" title=\"torch.nn.Softplus\"><code>Softplus</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"softmin\"><span class=\"hidden-section\">softmin</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.softmin\">\n<code>torch.nn.functional.softmin(input, dim=None, _stacklevel=3, dtype=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#softmin\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a softmin function.</p> <p>Note that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Softmin</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Softmin}(x) = \\text{Softmax}(-x)</annotation></semantics></math></span></span> </span>. See softmax definition for mathematical formula.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.softmin#torch.nn.Softmin\" title=\"torch.nn.Softmin\"><code>Softmin</code></a> for more details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – A dimension along which softmin will be computed (so every slice along dim will sum to 1).</li> <li>\n<strong>dtype</strong> (<code>torch.dtype</code>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"softmax\"><span class=\"hidden-section\">softmax</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.softmax\">\n<code>torch.nn.functional.softmax(input, dim=None, _stacklevel=3, dtype=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#softmax\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a softmax function.</p> <p>Softmax is defined as:</p> <p><span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Softmax</mtext><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy=\"false\">)</mo></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>x</mi><mi>j</mi></msub><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}</annotation></semantics></math></span></span> </span></p> <p>It is applied to all slices along dim, and will re-scale them so that the elements lie in the range <code>[0, 1]</code> and sum to 1.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.softmax#torch.nn.Softmax\" title=\"torch.nn.Softmax\"><code>Softmax</code></a> for more details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – A dimension along which softmax will be computed.</li> <li>\n<strong>dtype</strong> (<code>torch.dtype</code>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it’s faster and has better numerical properties).</p> </div> </dd>\n</dl>   <h3 id=\"softshrink\"><span class=\"hidden-section\">softshrink</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.softshrink\">\n<code>torch.nn.functional.softshrink(input, lambd=0.5) → Tensor</code> </dt> <dd>\n<p>Applies the soft shrinkage function elementwise</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.softshrink#torch.nn.Softshrink\" title=\"torch.nn.Softshrink\"><code>Softshrink</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"gumbel-softmax\"><span class=\"hidden-section\">gumbel_softmax</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.gumbel_softmax\">\n<code>torch.nn.functional.gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#gumbel_softmax\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Samples from the Gumbel-Softmax distribution (<a class=\"reference external\" href=\"https://arxiv.org/abs/1611.00712\">Link 1</a> <a class=\"reference external\" href=\"https://arxiv.org/abs/1611.01144\">Link 2</a>) and optionally discretizes.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>logits</strong> – <code>[…, num_features]</code> unnormalized log probabilities</li> <li>\n<strong>tau</strong> – non-negative scalar temperature</li> <li>\n<strong>hard</strong> – if <code>True</code>, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – A dimension along which softmax will be computed. Default: -1.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Sampled tensor of same shape as <code>logits</code> from the Gumbel-Softmax distribution. If <code>hard=True</code>, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across <code>dim</code>.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function is here for legacy reasons, may be removed from nn.Functional in the future.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The main trick for <code>hard</code> is to do <code>y_hard - y_soft.detach() + y_soft</code></p> <p>It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)</p> </div> <dl> <dt>Examples::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; logits = torch.randn(20, 32)\n&gt;&gt;&gt; # Sample soft categorical using reparametrization trick:\n&gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=False)\n&gt;&gt;&gt; # Sample hard categorical using \"Straight-through\" trick:\n&gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=True)\n</pre> </dd> </dl> </dd>\n</dl>   <h3 id=\"log-softmax\"><span class=\"hidden-section\">log_softmax</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.log_softmax\">\n<code>torch.nn.functional.log_softmax(input, dim=None, _stacklevel=3, dtype=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#log_softmax\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a softmax followed by a logarithm.</p> <p>While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.logsoftmax#torch.nn.LogSoftmax\" title=\"torch.nn.LogSoftmax\"><code>LogSoftmax</code></a> for more details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – A dimension along which log_softmax will be computed.</li> <li>\n<strong>dtype</strong> (<code>torch.dtype</code>, optional) – the desired data type of returned tensor. If specified, the input tensor is casted to <code>dtype</code> before the operation is performed. This is useful for preventing data type overflows. Default: None.</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"tanh\"><span class=\"hidden-section\">tanh</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.tanh\">\n<code>torch.nn.functional.tanh(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#tanh\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies element-wise, <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Tanh</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>−</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.tanh#torch.nn.Tanh\" title=\"torch.nn.Tanh\"><code>Tanh</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"sigmoid\"><span class=\"hidden-section\">sigmoid</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.sigmoid\">\n<code>torch.nn.functional.sigmoid(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#sigmoid\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the element-wise function <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Sigmoid</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mo>−</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}</annotation></semantics></math></span></span> </span></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.sigmoid#torch.nn.Sigmoid\" title=\"torch.nn.Sigmoid\"><code>Sigmoid</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"hardsigmoid\"><span class=\"hidden-section\">hardsigmoid</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.hardsigmoid\">\n<code>torch.nn.functional.hardsigmoid(input) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#hardsigmoid\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the element-wise function</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>Hardsigmoid</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.3599999999999999em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>≤</mo><mo>−</mo><mn>3</mn><mo separator=\"true\">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>≥</mo><mo>+</mo><mn>3</mn><mo separator=\"true\">,</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>x</mi><mi mathvariant=\"normal\">/</mi><mn>6</mn><mo>+</mo><mn>1</mn><mi mathvariant=\"normal\">/</mi><mn>2</mn></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">\\text{Hardsigmoid}(x) = \\begin{cases} 0 &amp; \\text{if~} x \\le -3, \\\\ 1 &amp; \\text{if~} x \\ge +3, \\\\ x / 6 + 1 / 2 &amp; \\text{otherwise} \\end{cases} </annotation></semantics></math></span></span></span> </div>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>inplace</strong> – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code></p> </dd> </dl> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.hardsigmoid#torch.nn.Hardsigmoid\" title=\"torch.nn.Hardsigmoid\"><code>Hardsigmoid</code></a> for more details.</p> </dd>\n</dl>   <h3 id=\"silu\"><span class=\"hidden-section\">silu</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.silu\">\n<code>torch.nn.functional.silu(input, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#silu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies the silu function, element-wise.</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>silu</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo separator=\"true\">,</mo><mtext>where </mtext><mi>σ</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mtext> is the logistic sigmoid.</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.} </annotation></semantics></math></span></span></span> </div>\n<div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>See <a class=\"reference external\" href=\"https://arxiv.org/abs/1606.08415\">Gaussian Error Linear Units (GELUs)</a> where the SiLU (Sigmoid Linear Unit) was originally coined, and see <a class=\"reference external\" href=\"https://arxiv.org/abs/1702.03118\">Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning</a> and <a class=\"reference external\" href=\"https://arxiv.org/abs/1710.05941v1\">Swish: a Self-Gated Activation Function</a> where the SiLU was experimented with later.</p> </div> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.silu#torch.nn.SiLU\" title=\"torch.nn.SiLU\"><code>SiLU</code></a> for more details.</p> </dd>\n</dl>    <h2 id=\"normalization-functions\">Normalization functions</h2>  <h3 id=\"batch-norm\"><span class=\"hidden-section\">batch_norm</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.batch_norm\">\n<code>torch.nn.functional.batch_norm(input, running_mean, running_var, weight=None, bias=None, training=False, momentum=0.1, eps=1e-05)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#batch_norm\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies Batch Normalization for each channel across a batch of data.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm1d#torch.nn.BatchNorm1d\" title=\"torch.nn.BatchNorm1d\"><code>BatchNorm1d</code></a>, <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm2d#torch.nn.BatchNorm2d\" title=\"torch.nn.BatchNorm2d\"><code>BatchNorm2d</code></a>, <a class=\"reference internal\" href=\"generated/torch.nn.batchnorm3d#torch.nn.BatchNorm3d\" title=\"torch.nn.BatchNorm3d\"><code>BatchNorm3d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"instance-norm\"><span class=\"hidden-section\">instance_norm</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.instance_norm\">\n<code>torch.nn.functional.instance_norm(input, running_mean=None, running_var=None, weight=None, bias=None, use_input_stats=True, momentum=0.1, eps=1e-05)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#instance_norm\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies Instance Normalization for each channel in each data sample in a batch.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm1d#torch.nn.InstanceNorm1d\" title=\"torch.nn.InstanceNorm1d\"><code>InstanceNorm1d</code></a>, <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm2d#torch.nn.InstanceNorm2d\" title=\"torch.nn.InstanceNorm2d\"><code>InstanceNorm2d</code></a>, <a class=\"reference internal\" href=\"generated/torch.nn.instancenorm3d#torch.nn.InstanceNorm3d\" title=\"torch.nn.InstanceNorm3d\"><code>InstanceNorm3d</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"layer-norm\"><span class=\"hidden-section\">layer_norm</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.layer_norm\">\n<code>torch.nn.functional.layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#layer_norm\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies Layer Normalization for last certain number of dimensions.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.layernorm#torch.nn.LayerNorm\" title=\"torch.nn.LayerNorm\"><code>LayerNorm</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"local-response-norm\"><span class=\"hidden-section\">local_response_norm</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.local_response_norm\">\n<code>torch.nn.functional.local_response_norm(input, size, alpha=0.0001, beta=0.75, k=1.0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#local_response_norm\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.localresponsenorm#torch.nn.LocalResponseNorm\" title=\"torch.nn.LocalResponseNorm\"><code>LocalResponseNorm</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"normalize\"><span class=\"hidden-section\">normalize</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.normalize\">\n<code>torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#normalize\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Performs <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding=\"application/x-tex\">L_p</annotation></semantics></math></span></span> </span> normalization of inputs over specified dimension.</p> <p>For a tensor <code>input</code> of sizes <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mi>n</mi><mn>0</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>n</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi></mrow></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>n</mi><mi>k</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(n_0, ..., n_{dim}, ..., n_k)</annotation></semantics></math></span></span> </span>, each <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>n</mi><mrow><mi>d</mi><mi>i</mi><mi>m</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">n_{dim}</annotation></semantics></math></span></span> </span> -element vector <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>v</mi></mrow><annotation encoding=\"application/x-tex\">v</annotation></semantics></math></span></span> </span> along dimension <code>dim</code> is transformed as</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>v</mi><mo>=</mo><mfrac><mi>v</mi><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">∥</mo><mi>v</mi><msub><mo stretchy=\"false\">∥</mo><mi>p</mi></msub><mo separator=\"true\">,</mo><mi>ϵ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac><mi mathvariant=\"normal\">.</mi></mrow><annotation encoding=\"application/x-tex\">v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}. </annotation></semantics></math></span></span></span> </div>\n<p>With the default arguments it uses the Euclidean norm over vectors along dimension <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1</annotation></semantics></math></span></span> </span> for normalization.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of any shape</li> <li>\n<strong>p</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – the exponent value in the norm formulation. Default: 2</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the dimension to reduce. Default: 1</li> <li>\n<strong>eps</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – small value to avoid division by zero. Default: 1e-12</li> <li>\n<strong>out</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – the output tensor. If <code>out</code> is used, this operation won’t be differentiable.</li> </ul> </dd> </dl> </dd>\n</dl>    <h2 id=\"linear-functions\">Linear functions</h2>  <h3 id=\"linear\"><span class=\"hidden-section\">linear</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.linear\">\n<code>torch.nn.functional.linear(input, weight, bias=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#linear\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a linear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = xA^T + b</annotation></semantics></math></span></span> </span>.</p> <p>This operator supports <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere\"><span class=\"std std-ref\">TensorFloat32</span></a>.</p> <p>Shape:</p>  <ul class=\"simple\"> <li>Input: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>i</mi><mi>n</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, *, in\\_features)</annotation></semantics></math></span></span> </span> N is the batch size, <code>*</code> means any number of additional dimensions</li> <li>Weight: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo separator=\"true\">,</mo><mi>i</mi><mi>n</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(out\\_features, in\\_features)</annotation></semantics></math></span></span> </span>\n</li> <li>Bias: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(out\\_features)</annotation></semantics></math></span></span> </span>\n</li> <li>Output: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant=\"normal\">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, *, out\\_features)</annotation></semantics></math></span></span> </span>\n</li> </ul>  </dd>\n</dl>   <h3 id=\"bilinear\"><span class=\"hidden-section\">bilinear</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.bilinear\">\n<code>torch.nn.functional.bilinear(input1, input2, weight, bias=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#bilinear\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies a bilinear transformation to the incoming data: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">y = x_1^T A x_2 + b</annotation></semantics></math></span></span> </span></p> <p>Shape:</p>  <ul class=\"simple\"> <li>input1: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mo>∗</mo><mo separator=\"true\">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi><mn>1</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, *, H_{in1})</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi><mn>1</mn></mrow></msub><mo>=</mo><mtext>in1_features</mtext></mrow><annotation encoding=\"application/x-tex\">H_{in1}=\\text{in1\\_features}</annotation></semantics></math></span></span> </span> and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∗</mo></mrow><annotation encoding=\"application/x-tex\">*</annotation></semantics></math></span></span> </span> means any number of additional dimensions. All but the last dimension of the inputs should be the same.</li> <li>input2: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mo>∗</mo><mo separator=\"true\">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi><mn>2</mn></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, *, H_{in2})</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi><mn>2</mn></mrow></msub><mo>=</mo><mtext>in2_features</mtext></mrow><annotation encoding=\"application/x-tex\">H_{in2}=\\text{in2\\_features}</annotation></semantics></math></span></span> </span>\n</li> <li>weight: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_features</mtext><mo separator=\"true\">,</mo><mtext>in1_features</mtext><mo separator=\"true\">,</mo><mtext>in2_features</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})</annotation></semantics></math></span></span> </span>\n</li> <li>bias: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>out_features</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{out\\_features})</annotation></semantics></math></span></span> </span>\n</li> <li>output: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mo>∗</mo><mo separator=\"true\">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, *, H_{out})</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mtext>out_features</mtext></mrow><annotation encoding=\"application/x-tex\">H_{out}=\\text{out\\_features}</annotation></semantics></math></span></span> </span> and all but the last dimension are the same shape as the input.</li> </ul>  </dd>\n</dl>    <h2 id=\"dropout-functions\">Dropout functions</h2>  <h3 id=\"dropout\"><span class=\"hidden-section\">dropout</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.dropout\">\n<code>torch.nn.functional.dropout(input, p=0.5, training=True, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#dropout\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.dropout#torch.nn.Dropout\" title=\"torch.nn.Dropout\"><code>Dropout</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>p</strong> – probability of an element to be zeroed. Default: 0.5</li> <li>\n<strong>training</strong> – apply dropout if is <code>True</code>. Default: <code>True</code>\n</li> <li>\n<strong>inplace</strong> – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"alpha-dropout\"><span class=\"hidden-section\">alpha_dropout</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.alpha_dropout\">\n<code>torch.nn.functional.alpha_dropout(input, p=0.5, training=False, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#alpha_dropout\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Applies alpha dropout to the input.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.alphadropout#torch.nn.AlphaDropout\" title=\"torch.nn.AlphaDropout\"><code>AlphaDropout</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"feature-alpha-dropout\"><span class=\"hidden-section\">feature_alpha_dropout</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.feature_alpha_dropout\">\n<code>torch.nn.functional.feature_alpha_dropout(input, p=0.5, training=False, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#feature_alpha_dropout\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Randomly masks out entire channels (a channel is a feature map, e.g. the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span> </span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span>-th sample in the batch input is a tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span> </span>) of the input tensor). Instead of setting activations to zero, as in regular Dropout, the activations are set to the negative saturation value of the SELU activation function.</p> <p>Each element will be masked independently on every forward call with probability <code>p</code> using samples from a Bernoulli distribution. The elements to be masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit variance.</p> <p>See <code>FeatureAlphaDropout</code> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>p</strong> – dropout probability of a channel to be zeroed. Default: 0.5</li> <li>\n<strong>training</strong> – apply dropout if is <code>True</code>. Default: <code>True</code>\n</li> <li>\n<strong>inplace</strong> – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"dropout2d\"><span class=\"hidden-section\">dropout2d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.dropout2d\">\n<code>torch.nn.functional.dropout2d(input, p=0.5, training=True, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#dropout2d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span> </span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span>-th sample in the batched input is a 2D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span> </span>) of the input tensor). Each channel will be zeroed out independently on every forward call with probability <code>p</code> using samples from a Bernoulli distribution.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.dropout2d#torch.nn.Dropout2d\" title=\"torch.nn.Dropout2d\"><code>Dropout2d</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</li> <li>\n<strong>training</strong> – apply dropout if is <code>True</code>. Default: <code>True</code>\n</li> <li>\n<strong>inplace</strong> – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"dropout3d\"><span class=\"hidden-section\">dropout3d</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.dropout3d\">\n<code>torch.nn.functional.dropout3d(input, p=0.5, training=True, inplace=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#dropout3d\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span></span> </span>-th channel of the <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span></span> </span>-th sample in the batched input is a 3D tensor <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input}[i, j]</annotation></semantics></math></span></span> </span>) of the input tensor). Each channel will be zeroed out independently on every forward call with probability <code>p</code> using samples from a Bernoulli distribution.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.dropout3d#torch.nn.Dropout3d\" title=\"torch.nn.Dropout3d\"><code>Dropout3d</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>p</strong> – probability of a channel to be zeroed. Default: 0.5</li> <li>\n<strong>training</strong> – apply dropout if is <code>True</code>. Default: <code>True</code>\n</li> <li>\n<strong>inplace</strong> – If set to <code>True</code>, will do this operation in-place. Default: <code>False</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>    <h2 id=\"sparse-functions\">Sparse functions</h2>  <h3 id=\"embedding\"><span class=\"hidden-section\">embedding</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.embedding\">\n<code>torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#embedding\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p> <p>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.embedding#torch.nn.Embedding\" title=\"torch.nn.Embedding\"><code>torch.nn.Embedding</code></a> for more details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size</li> <li>\n<strong>padding_idx</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – If given, pads the output with the embedding vector at <code>padding_idx</code> (initialized to zeros) whenever it encounters the index.</li> <li>\n<strong>max_norm</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>. Note: this will modify <code>weight</code> in-place.</li> <li>\n<strong>norm_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li> <li>\n<strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>.</li> <li>\n<strong>sparse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, gradient w.r.t. <code>weight</code> will be a sparse tensor. See Notes under <a class=\"reference internal\" href=\"generated/torch.nn.embedding#torch.nn.Embedding\" title=\"torch.nn.Embedding\"><code>torch.nn.Embedding</code></a> for more details regarding sparse gradients.</li> </ul> </dd> </dl> <dl class=\"simple\"> <dt>Shape:</dt>\n<dd>\n<ul class=\"simple\"> <li>Input: LongTensor of arbitrary shape containing the indices to extract</li> <li>\n<dl class=\"simple\"> <dt>\n<code>Weight: Embedding matrix of floating point type with shape (V, embedding_dim),</code> </dt>\n<dd>\n<p>where V = maximum index + 1 and embedding_dim = the embedding size</p> </dd> </dl> </li> <li>Output: <code>(*, embedding_dim)</code>, where <code>*</code> is the input shape</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n&gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3\n&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)\n&gt;&gt;&gt; F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,  0.9625,  0.6753],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.6246,  0.9751,  0.3618],\n         [ 0.4161,  0.2419,  0.7383]],\n\n        [[ 0.6246,  0.9751,  0.3618],\n         [ 0.0237,  0.7794,  0.0528],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.3385,  0.8612,  0.1867]]])\n\n&gt;&gt;&gt; # example with padding_idx\n&gt;&gt;&gt; weights = torch.rand(10, 3)\n&gt;&gt;&gt; weights[0, :].zero_()\n&gt;&gt;&gt; embedding_matrix = weights\n&gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])\n&gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.5609,  0.5384,  0.8720],\n         [ 0.0000,  0.0000,  0.0000],\n         [ 0.6262,  0.2438,  0.7471]]])\n</pre> </dd>\n</dl>   <h3 id=\"embedding-bag\"><span class=\"hidden-section\">embedding_bag</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.embedding_bag\">\n<code>torch.nn.functional.embedding_bag(input, weight, offsets=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, mode='mean', sparse=False, per_sample_weights=None, include_last_offset=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#embedding_bag\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes sums, means or maxes of <code>bags</code> of embeddings, without instantiating the intermediate embeddings.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.embeddingbag#torch.nn.EmbeddingBag\" title=\"torch.nn.EmbeddingBag\"><code>torch.nn.EmbeddingBag</code></a> for more details.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<em>LongTensor</em>) – Tensor containing bags of indices into the embedding matrix</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size</li> <li>\n<strong>offsets</strong> (<em>LongTensor</em><em>, </em><em>optional</em>) – Only used when <code>input</code> is 1D. <code>offsets</code> determines the starting index position of each bag (sequence) in <code>input</code>.</li> <li>\n<strong>max_norm</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>. Note: this will modify <code>weight</code> in-place.</li> <li>\n<strong>norm_type</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – The <code>p</code> in the <code>p</code>-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li> <li>\n<strong>scale_grad_by_freq</strong> (<em>boolean</em><em>, </em><em>optional</em>) – if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>. Note: this option is not supported when <code>mode=\"max\"</code>.</li> <li>\n<strong>mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code>\"sum\"</code>, <code>\"mean\"</code> or <code>\"max\"</code>. Specifies the way to reduce the bag. Default: <code>\"mean\"</code>\n</li> <li>\n<strong>sparse</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, gradient w.r.t. <code>weight</code> will be a sparse tensor. See Notes under <a class=\"reference internal\" href=\"generated/torch.nn.embedding#torch.nn.Embedding\" title=\"torch.nn.Embedding\"><code>torch.nn.Embedding</code></a> for more details regarding sparse gradients. Note: this option is not supported when <code>mode=\"max\"</code>.</li> <li>\n<strong>per_sample_weights</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, <code>per_sample_weights</code> must have exactly the same shape as input and is treated as having the same <code>offsets</code>, if those are not None.</li> <li>\n<strong>include_last_offset</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, the size of offsets is equal to the number of bags + 1.</li> <li>\n<strong>last element is the size of the input, or the ending index position of the last bag</strong> (<em>The</em>) – </li> </ul> </dd> </dl> <p>Shape:</p>  <ul> <li>\n<p><code>input</code> (LongTensor) and <code>offsets</code> (LongTensor, optional)</p> <ul> <li>\n<p>If <code>input</code> is 2D of shape <code>(B, N)</code>,</p> <p>it will be treated as <code>B</code> bags (sequences) each of fixed length <code>N</code>, and this will return <code>B</code> values aggregated in a way depending on the <code>mode</code>. <code>offsets</code> is ignored and required to be <code>None</code> in this case.</p> </li> <li>\n<p>If <code>input</code> is 1D of shape <code>(N)</code>,</p> <p>it will be treated as a concatenation of multiple bags (sequences). <code>offsets</code> is required to be a 1D tensor containing the starting index positions of each bag in <code>input</code>. Therefore, for <code>offsets</code> of shape <code>(B)</code>, <code>input</code> will be viewed as having <code>B</code> bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.</p> </li> </ul> </li> <li>\n<code>weight</code> (Tensor): the learnable weights of the module of shape <code>(num_embeddings, embedding_dim)</code>\n</li> <li>\n<code>per_sample_weights</code> (Tensor, optional). Has the same shape as <code>input</code>.</li> <li>\n<code>output</code>: aggregated embedding values of shape <code>(B, embedding_dim)</code>\n</li> </ul>  <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3\n&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)\n&gt;&gt;&gt; # a batch of 2 samples of 4 indices each\n&gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])\n&gt;&gt;&gt; offsets = torch.tensor([0,4])\n&gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)\ntensor([[ 0.3397,  0.3552,  0.5545],\n        [ 0.5893,  0.4386,  0.5882]])\n</pre> </dd>\n</dl>   <h3 id=\"one-hot\"><span class=\"hidden-section\">one_hot</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.one_hot\">\n<code>torch.nn.functional.one_hot(tensor, num_classes=-1) → LongTensor</code> </dt> <dd>\n<p>Takes LongTensor with index values of shape <code>(*)</code> and returns a tensor of shape <code>(*, num_classes)</code> that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</p> <p>See also <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/One-hot\">One-hot on Wikipedia</a> .</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<em>LongTensor</em>) – class values of any shape.</li> <li>\n<strong>num_classes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3)\ntensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0]])\n&gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\ntensor([[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0]])\n&gt;&gt;&gt; F.one_hot(torch.arange(0, 6).view(3,2) % 3)\ntensor([[[1, 0, 0],\n         [0, 1, 0]],\n        [[0, 0, 1],\n         [1, 0, 0]],\n        [[0, 1, 0],\n         [0, 0, 1]]])\n</pre> </dd>\n</dl>    <h2 id=\"distance-functions\">Distance functions</h2>  <h3 id=\"pairwise-distance\"><span class=\"hidden-section\">pairwise_distance</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.pairwise_distance\">\n<code>torch.nn.functional.pairwise_distance(x1, x2, p=2.0, eps=1e-06, keepdim=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#pairwise_distance\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance\" title=\"torch.nn.PairwiseDistance\"><code>torch.nn.PairwiseDistance</code></a> for details</p> </dd>\n</dl>   <h3 id=\"cosine-similarity\"><span class=\"hidden-section\">cosine_similarity</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.cosine_similarity\">\n<code>torch.nn.functional.cosine_similarity(x1, x2, dim=1, eps=1e-8) → Tensor</code> </dt> <dd>\n<p>Returns cosine similarity between x1 and x2, computed along dim.</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>similarity</mtext><mo>=</mo><mfrac><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>⋅</mo><msub><mi>x</mi><mn>2</mn></msub></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">∥</mi><msub><mi>x</mi><mn>1</mn></msub><msub><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msub><mo>⋅</mo><mi mathvariant=\"normal\">∥</mi><msub><mi>x</mi><mn>2</mn></msub><msub><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi>ϵ</mi><mo stretchy=\"false\">)</mo></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)} </annotation></semantics></math></span></span></span> </div>\n<dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>x1</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – First input.</li> <li>\n<strong>x2</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Second input (of size matching x1).</li> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Dimension of vectors. Default: 1</li> <li>\n<strong>eps</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – Small value to avoid division by zero. Default: 1e-8</li> </ul> </dd> </dl> <dl class=\"simple\"> <dt>Shape:</dt>\n<dd>\n<ul class=\"simple\"> <li>Input: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mo>∗</mo><mn>1</mn></msub><mo separator=\"true\">,</mo><mi>D</mi><mo separator=\"true\">,</mo><msub><mo>∗</mo><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\ast_1, D, \\ast_2)</annotation></semantics></math></span></span> </span> where D is at position <code>dim</code>.</li> <li>Output: <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><msub><mo>∗</mo><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mo>∗</mo><mn>2</mn></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\ast_1, \\ast_2)</annotation></semantics></math></span></span> </span> where 1 is at position <code>dim</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; input1 = torch.randn(100, 128)\n&gt;&gt;&gt; input2 = torch.randn(100, 128)\n&gt;&gt;&gt; output = F.cosine_similarity(input1, input2)\n&gt;&gt;&gt; print(output)\n</pre> </dd>\n</dl>   <h3 id=\"pdist\"><span class=\"hidden-section\">pdist</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.pdist\">\n<code>torch.nn.functional.pdist(input, p=2) → Tensor</code> </dt> <dd>\n<p>Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of <code>torch.norm(input[:, None] - input, dim=2, p=p)</code>. This function will be faster if the rows are contiguous.</p> <p>If input has shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N \\times M</annotation></semantics></math></span></span> </span> then the output will have shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>N</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{1}{2} N (N - 1)</annotation></semantics></math></span></span> </span>.</p> <p>This function is equivalent to <code>scipy.spatial.distance.pdist(input, ‘minkowski’, p=p)</code> if <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo>∈</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">∞</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">p \\in (0, \\infty)</annotation></semantics></math></span></span> </span>. When <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">p = 0</annotation></semantics></math></span></span> </span> it is equivalent to <code>scipy.spatial.distance.pdist(input, ‘hamming’) * M</code>. When <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>p</mi><mo>=</mo><mi mathvariant=\"normal\">∞</mi></mrow><annotation encoding=\"application/x-tex\">p = \\infty</annotation></semantics></math></span></span> </span>, the closest scipy function is <code>scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – input tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">N \\times M</annotation></semantics></math></span></span> </span>.</li> <li>\n<strong>p</strong> – p value for the p-norm distance to calculate between each vector pair <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>∈</mo><mo stretchy=\"false\">[</mo><mn>0</mn><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">∞</mi><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\in [0, \\infty]</annotation></semantics></math></span></span> </span>.</li> </ul> </dd> </dl> </dd>\n</dl>    <h2 id=\"loss-functions\">Loss functions</h2>  <h3 id=\"binary-cross-entropy\"><span class=\"hidden-section\">binary_cross_entropy</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.binary_cross_entropy\">\n<code>torch.nn.functional.binary_cross_entropy(input, target, weight=None, size_average=None, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#binary_cross_entropy\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that measures the Binary Cross Entropy between the target and the output.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.bceloss#torch.nn.BCELoss\" title=\"torch.nn.BCELoss\"><code>BCELoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – Tensor of arbitrary shape</li> <li>\n<strong>target</strong> – Tensor of the same shape as input</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a manual rescaling weight if provided it’s repeated to match input tensor shape</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code>\n</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)\n&gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)\n&gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)\n&gt;&gt;&gt; loss.backward()\n</pre> </dd>\n</dl>   <h3 id=\"binary-cross-entropy-with-logits\"><span class=\"hidden-section\">binary_cross_entropy_with_logits</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.binary_cross_entropy_with_logits\">\n<code>torch.nn.functional.binary_cross_entropy_with_logits(input, target, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#binary_cross_entropy_with_logits\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that measures Binary Cross Entropy between target and output logits.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\" title=\"torch.nn.BCEWithLogitsLoss\"><code>BCEWithLogitsLoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – Tensor of arbitrary shape</li> <li>\n<strong>target</strong> – Tensor of the same shape as input</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a manual rescaling weight if provided it’s repeated to match input tensor shape</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code>\n</li> <li>\n<strong>pos_weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a weight of positive examples. Must be a vector with length equal to the number of classes.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3).random_(2)\n&gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)\n&gt;&gt;&gt; loss.backward()\n</pre> </dd>\n</dl>   <h3 id=\"poisson-nll-loss\"><span class=\"hidden-section\">poisson_nll_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.poisson_nll_loss\">\n<code>torch.nn.functional.poisson_nll_loss(input, target, log_input=True, full=False, size_average=None, eps=1e-08, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#poisson_nll_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Poisson negative log likelihood loss.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.poissonnllloss#torch.nn.PoissonNLLLoss\" title=\"torch.nn.PoissonNLLLoss\"><code>PoissonNLLLoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – expectation of underlying Poisson distribution.</li> <li>\n<strong>target</strong> – random sample <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>t</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>∼</mo><mtext>Poisson</mtext><mo stretchy=\"false\">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">target \\sim \\text{Poisson}(input)</annotation></semantics></math></span></span> </span>.</li> <li>\n<strong>log_input</strong> – if <code>True</code> the loss is computed as <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mtext>input</mtext><mo stretchy=\"false\">)</mo><mo>−</mo><mtext>target</mtext><mo>∗</mo><mtext>input</mtext></mrow><annotation encoding=\"application/x-tex\">\\exp(\\text{input}) - \\text{target} * \\text{input}</annotation></semantics></math></span></span> </span>, if <code>False</code> then loss is <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>input</mtext><mo>−</mo><mtext>target</mtext><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mtext>input</mtext><mo>+</mo><mtext>eps</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})</annotation></semantics></math></span></span> </span>. Default: <code>True</code>\n</li> <li>\n<strong>full</strong> – whether to compute full loss, i. e. to add the Stirling approximation term. Default: <code>False</code> <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>target</mtext><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mtext>target</mtext><mo stretchy=\"false\">)</mo><mo>−</mo><mtext>target</mtext><mo>+</mo><mn>0.5</mn><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>2</mn><mo>∗</mo><mi>π</mi><mo>∗</mo><mtext>target</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})</annotation></semantics></math></span></span> </span>.</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>eps</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – Small value to avoid evaluation of <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>0</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(0)</annotation></semantics></math></span></span> </span> when <code>log_input`=``False`</code>. Default: 1e-8</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code>\n</li> </ul> </dd> </dl> </dd>\n</dl>   <h3 id=\"cosine-embedding-loss\"><span class=\"hidden-section\">cosine_embedding_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.cosine_embedding_loss\">\n<code>torch.nn.functional.cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#cosine_embedding_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss\" title=\"torch.nn.CosineEmbeddingLoss\"><code>CosineEmbeddingLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"cross-entropy\"><span class=\"hidden-section\">cross_entropy</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.cross_entropy\">\n<code>torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#cross_entropy\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>This criterion combines <code>log_softmax</code> and <code>nll_loss</code> in a single function.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.crossentropyloss#torch.nn.CrossEntropyLoss\" title=\"torch.nn.CrossEntropyLoss\"><code>CrossEntropyLoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C)</annotation></semantics></math></span></span> </span> where <code>C = number of classes</code> or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, H, W)</annotation></semantics></math></span></span> </span> in case of 2D Loss, or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, d_1, d_2, ..., d_K)</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K \\geq 1</annotation></semantics></math></span></span> </span> in the case of K-dimensional loss.</li> <li>\n<strong>target</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N)</annotation></semantics></math></span></span> </span> where each value is <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>≤</mo><mtext>targets</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo><mo>≤</mo><mi>C</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 \\leq \\text{targets}[i] \\leq C-1</annotation></semantics></math></span></span> </span>, or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, d_1, d_2, ..., d_K)</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K \\geq 1</annotation></semantics></math></span></span> </span> for K-dimensional loss.</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a manual rescaling weight given to each class. If given, has to be a Tensor of size <code>C</code>\n</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>ignore_index</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets. Default: -100</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code>\n</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)\n&gt;&gt;&gt; loss = F.cross_entropy(input, target)\n&gt;&gt;&gt; loss.backward()\n</pre> </dd>\n</dl>   <h3 id=\"ctc-loss\"><span class=\"hidden-section\">ctc_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.ctc_loss\">\n<code>torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#ctc_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>The Connectionist Temporal Classification loss.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.ctcloss#torch.nn.CTCLoss\" title=\"torch.nn.CTCLoss\"><code>CTCLoss</code></a> for details.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>log_probs</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>T</mi><mo separator=\"true\">,</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(T, N, C)</annotation></semantics></math></span></span> </span> where <code>C = number of characters in alphabet including blank</code>, <code>T = input length</code>, and <code>N = batch size</code>. The logarithmized probabilities of the outputs (e.g. obtained with <a class=\"reference internal\" href=\"#torch.nn.functional.log_softmax\" title=\"torch.nn.functional.log_softmax\"><code>torch.nn.functional.log_softmax()</code></a>).</li> <li>\n<strong>targets</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>S</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, S)</annotation></semantics></math></span></span> </span> or <code>(sum(target_lengths))</code>. Targets cannot be blank. In the second form, the targets are assumed to be concatenated.</li> <li>\n<strong>input_lengths</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N)</annotation></semantics></math></span></span> </span>. Lengths of the inputs (must each be <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo>≤</mo><mi>T</mi></mrow><annotation encoding=\"application/x-tex\">\\leq T</annotation></semantics></math></span></span> </span>)</li> <li>\n<strong>target_lengths</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N)</annotation></semantics></math></span></span> </span>. Lengths of the targets</li> <li>\n<strong>blank</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Blank label. Default <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn></mrow><annotation encoding=\"application/x-tex\">0</annotation></semantics></math></span></span> </span>.</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the output losses will be divided by the target lengths and then the mean over the batch is taken, <code>'sum'</code>: the output will be summed. Default: <code>'mean'</code>\n</li> <li>\n<strong>zero_infinity</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients. Default: <code>False</code> Infinite losses mainly occur when the inputs are too short to be aligned to the targets.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n&gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n&gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)\n&gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n&gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n&gt;&gt;&gt; loss.backward()\n</pre> </dd>\n</dl>   <h3 id=\"hinge-embedding-loss\"><span class=\"hidden-section\">hinge_embedding_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.hinge_embedding_loss\">\n<code>torch.nn.functional.hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#hinge_embedding_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss\" title=\"torch.nn.HingeEmbeddingLoss\"><code>HingeEmbeddingLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"kl-div\"><span class=\"hidden-section\">kl_div</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.kl_div\">\n<code>torch.nn.functional.kl_div(input, target, size_average=None, reduce=None, reduction='mean', log_target=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#kl_div\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>The <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Kullback-Leibler_divergence\">Kullback-Leibler divergence Loss</a></p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.kldivloss#torch.nn.KLDivLoss\" title=\"torch.nn.KLDivLoss\"><code>KLDivLoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – Tensor of arbitrary shape</li> <li>\n<strong>target</strong> – Tensor of the same shape as input</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'batchmean'</code> | <code>'sum'</code> | <code>'mean'</code>. <code>'none'</code>: no reduction will be applied <code>'batchmean'</code>: the sum of the output will be divided by the batchsize <code>'sum'</code>: the output will be summed <code>'mean'</code>: the output will be divided by the number of elements in the output Default: <code>'mean'</code>\n</li> <li>\n<strong>log_target</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – A flag indicating whether <code>target</code> is passed in the log space. It is recommended to pass certain distributions (like <code>softmax</code>) in the log space to avoid numerical issues caused by explicit <code>log</code>. Default: <code>False</code>\n</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>:attr:<code>reduction</code> = <code>'mean'</code> doesn’t return the true kl divergence value, please use :attr:<code>reduction</code> = <code>'batchmean'</code> which aligns with KL math definition. In the next major release, <code>'mean'</code> will be changed to be the same as ‘batchmean’.</p> </div> </dd>\n</dl>   <h3 id=\"l1-loss\"><span class=\"hidden-section\">l1_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.l1_loss\">\n<code>torch.nn.functional.l1_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#l1_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that takes the mean element-wise absolute value difference.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.l1loss#torch.nn.L1Loss\" title=\"torch.nn.L1Loss\"><code>L1Loss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"mse-loss\"><span class=\"hidden-section\">mse_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.mse_loss\">\n<code>torch.nn.functional.mse_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#mse_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Measures the element-wise mean squared error.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.mseloss#torch.nn.MSELoss\" title=\"torch.nn.MSELoss\"><code>MSELoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"margin-ranking-loss\"><span class=\"hidden-section\">margin_ranking_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.margin_ranking_loss\">\n<code>torch.nn.functional.margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#margin_ranking_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss\" title=\"torch.nn.MarginRankingLoss\"><code>MarginRankingLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"multilabel-margin-loss\"><span class=\"hidden-section\">multilabel_margin_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.multilabel_margin_loss\">\n<code>torch.nn.functional.multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#multilabel_margin_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss\" title=\"torch.nn.MultiLabelMarginLoss\"><code>MultiLabelMarginLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"multilabel-soft-margin-loss\"><span class=\"hidden-section\">multilabel_soft_margin_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.multilabel_soft_margin_loss\">\n<code>torch.nn.functional.multilabel_soft_margin_loss(input, target, weight=None, size_average=None) → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#multilabel_soft_margin_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss\" title=\"torch.nn.MultiLabelSoftMarginLoss\"><code>MultiLabelSoftMarginLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"multi-margin-loss\"><span class=\"hidden-section\">multi_margin_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.multi_margin_loss\">\n<code>torch.nn.functional.multi_margin_loss(input, target, p=1, margin=1.0, weight=None, size_average=None, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#multi_margin_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"simple\"> <dt>multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</dt>\n<dd>\n<p>reduce=None, reduction=’mean’) -&gt; Tensor</p> </dd> </dl> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss\" title=\"torch.nn.MultiMarginLoss\"><code>MultiMarginLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"nll-loss\"><span class=\"hidden-section\">nll_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.nll_loss\">\n<code>torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#nll_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>The negative log likelihood loss.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.nllloss#torch.nn.NLLLoss\" title=\"torch.nn.NLLLoss\"><code>NLLLoss</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C)</annotation></semantics></math></span></span> </span> where <code>C = number of classes</code> or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, H, W)</annotation></semantics></math></span></span> </span> in case of 2D Loss, or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, d_1, d_2, ..., d_K)</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K \\geq 1</annotation></semantics></math></span></span> </span> in the case of K-dimensional loss.</li> <li>\n<strong>target</strong> – <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N)</annotation></semantics></math></span></span> </span> where each value is <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>0</mn><mo>≤</mo><mtext>targets</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo><mo>≤</mo><mi>C</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">0 \\leq \\text{targets}[i] \\leq C-1</annotation></semantics></math></span></span> </span>, or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><msub><mi>d</mi><mn>2</mn></msub><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mi mathvariant=\"normal\">.</mi><mo separator=\"true\">,</mo><msub><mi>d</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, d_1, d_2, ..., d_K)</annotation></semantics></math></span></span> </span> where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>K</mi><mo>≥</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">K \\geq 1</annotation></semantics></math></span></span> </span> for K-dimensional loss.</li> <li>\n<strong>weight</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – a manual rescaling weight given to each class. If given, has to be a Tensor of size <code>C</code>\n</li> <li>\n<strong>size_average</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field <code>size_average</code> is set to <code>False</code>, the losses are instead summed for each minibatch. Ignored when reduce is <code>False</code>. Default: <code>True</code>\n</li> <li>\n<strong>ignore_index</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Specifies a target value that is ignored and does not contribute to the input gradient. When <code>size_average</code> is <code>True</code>, the loss is averaged over non-ignored targets. Default: -100</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Deprecated (see <code>reduction</code>). By default, the losses are averaged or summed over observations for each minibatch depending on <code>size_average</code>. When <code>reduce</code> is <code>False</code>, returns a loss per batch element instead and ignores <code>size_average</code>. Default: <code>True</code>\n</li> <li>\n<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the sum of the output will be divided by the number of elements in the output, <code>'sum'</code>: the output will be summed. Note: <code>size_average</code> and <code>reduce</code> are in the process of being deprecated, and in the meantime, specifying either of those two args will override <code>reduction</code>. Default: <code>'mean'</code>\n</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; # input is of size N x C = 3 x 5\n&gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)\n&gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C\n&gt;&gt;&gt; target = torch.tensor([1, 0, 4])\n&gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)\n&gt;&gt;&gt; output.backward()\n</pre> </dd>\n</dl>   <h3 id=\"smooth-l1-loss\"><span class=\"hidden-section\">smooth_l1_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.smooth_l1_loss\">\n<code>torch.nn.functional.smooth_l1_loss(input, target, size_average=None, reduce=None, reduction='mean', beta=1.0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#smooth_l1_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.smoothl1loss#torch.nn.SmoothL1Loss\" title=\"torch.nn.SmoothL1Loss\"><code>SmoothL1Loss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"soft-margin-loss\"><span class=\"hidden-section\">soft_margin_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.soft_margin_loss\">\n<code>torch.nn.functional.soft_margin_loss(input, target, size_average=None, reduce=None, reduction='mean') → Tensor</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#soft_margin_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss\" title=\"torch.nn.SoftMarginLoss\"><code>SoftMarginLoss</code></a> for details.</p> </dd>\n</dl>   <h3 id=\"triplet-margin-loss\"><span class=\"hidden-section\">triplet_margin_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.triplet_margin_loss\">\n<code>torch.nn.functional.triplet_margin_loss(anchor, positive, negative, margin=1.0, p=2, eps=1e-06, swap=False, size_average=None, reduce=None, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#triplet_margin_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss\" title=\"torch.nn.TripletMarginLoss\"><code>TripletMarginLoss</code></a> for details</p> </dd>\n</dl>   <h3 id=\"triplet-margin-with-distance-loss\"><span class=\"hidden-section\">triplet_margin_with_distance_loss</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.triplet_margin_with_distance_loss\">\n<code>torch.nn.functional.triplet_margin_with_distance_loss(anchor, positive, negative, *, distance_function=None, margin=1.0, swap=False, reduction='mean')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#triplet_margin_with_distance_loss\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss\" title=\"torch.nn.TripletMarginWithDistanceLoss\"><code>TripletMarginWithDistanceLoss</code></a> for details.</p> </dd>\n</dl>    <h2 id=\"vision-functions\">Vision functions</h2>  <h3 id=\"pixel-shuffle\"><span class=\"hidden-section\">pixel_shuffle</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.pixel_shuffle\">\n<code>torch.nn.functional.pixel_shuffle(input, upscale_factor) → Tensor</code> </dt> <dd>\n<p>Rearranges elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span> </span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span> </span>, where r is the <code>upscale_factor</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>PixelShuffle</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the input tensor</li> <li>\n<strong>upscale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – factor to increase spatial resolution by</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)\n&gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([1, 1, 12, 12])\n</pre> </dd>\n</dl>   <h3 id=\"pixel-unshuffle\"><span class=\"hidden-section\">pixel_unshuffle</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.pixel_unshuffle\">\n<code>torch.nn.functional.pixel_unshuffle(input, downscale_factor) → Tensor</code> </dt> <dd>\n<p>Reverses the <a class=\"reference internal\" href=\"generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle\" title=\"torch.nn.PixelShuffle\"><code>PixelShuffle</code></a> operation by rearranging elements in a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator=\"true\">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C, H \\times r, W \\times r)</annotation></semantics></math></span></span> </span> to a tensor of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mo>∗</mo><mo separator=\"true\">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator=\"true\">,</mo><mi>H</mi><mo separator=\"true\">,</mo><mi>W</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(*, C \\times r^2, H, W)</annotation></semantics></math></span></span> </span>, where r is the <code>downscale_factor</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.nn.pixelunshuffle#torch.nn.PixelUnshuffle\" title=\"torch.nn.PixelUnshuffle\"><code>PixelUnshuffle</code></a> for details.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the input tensor</li> <li>\n<strong>downscale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – factor to increase spatial resolution by</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; input = torch.randn(1, 1, 12, 12)\n&gt;&gt;&gt; output = torch.nn.functional.pixel_unshuffle(input, 3)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([1, 9, 4, 4])\n</pre> </dd>\n</dl>   <h3 id=\"pad\"><span class=\"hidden-section\">pad</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.pad\">\n<code>torch.nn.functional.pad(input, pad, mode='constant', value=0)</code> </dt> <dd>\n<p>Pads tensor.</p> <dl class=\"simple\"> <dt>Padding size:</dt>\n<dd>\n<p>The padding size by which to pad some dimensions of <code>input</code> are described starting from the last dimension and moving forward. <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo fence=\"true\">⌊</mo><mfrac><mtext>len(pad)</mtext><mn>2</mn></mfrac><mo fence=\"true\">⌋</mo></mrow><annotation encoding=\"application/x-tex\">\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor</annotation></semantics></math></span></span> </span> dimensions of <code>input</code> will be padded. For example, to pad only the last dimension of the input tensor, then <a class=\"reference internal\" href=\"#torch.nn.functional.pad\" title=\"torch.nn.functional.pad\"><code>pad</code></a> has the form <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>padding_left</mtext><mo separator=\"true\">,</mo><mtext>padding_right</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{padding\\_left}, \\text{padding\\_right})</annotation></semantics></math></span></span> </span>; to pad the last 2 dimensions of the input tensor, then use <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>padding_left</mtext><mo separator=\"true\">,</mo><mtext>padding_right</mtext><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{padding\\_left}, \\text{padding\\_right},</annotation></semantics></math></span></span> </span> <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>padding_top</mtext><mo separator=\"true\">,</mo><mtext>padding_bottom</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{padding\\_top}, \\text{padding\\_bottom})</annotation></semantics></math></span></span> </span>; to pad the last 3 dimensions, use <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mtext>padding_left</mtext><mo separator=\"true\">,</mo><mtext>padding_right</mtext><mo separator=\"true\">,</mo></mrow><annotation encoding=\"application/x-tex\">(\\text{padding\\_left}, \\text{padding\\_right},</annotation></semantics></math></span></span> </span> <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>padding_top</mtext><mo separator=\"true\">,</mo><mtext>padding_bottom</mtext></mrow><annotation encoding=\"application/x-tex\">\\text{padding\\_top}, \\text{padding\\_bottom}</annotation></semantics></math></span></span> </span> <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>padding_front</mtext><mo separator=\"true\">,</mo><mtext>padding_back</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{padding\\_front}, \\text{padding\\_back})</annotation></semantics></math></span></span> </span>.</p> </dd> <dt>Padding mode:</dt>\n<dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nn.constantpad2d#torch.nn.ConstantPad2d\" title=\"torch.nn.ConstantPad2d\"><code>torch.nn.ConstantPad2d</code></a>, <a class=\"reference internal\" href=\"generated/torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d\" title=\"torch.nn.ReflectionPad2d\"><code>torch.nn.ReflectionPad2d</code></a>, and <a class=\"reference internal\" href=\"generated/torch.nn.replicationpad2d#torch.nn.ReplicationPad2d\" title=\"torch.nn.ReplicationPad2d\"><code>torch.nn.ReplicationPad2d</code></a> for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for background.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – N-dimensional tensor</li> <li>\n<strong>pad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>) – m-elements tuple, where <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mi>m</mi><mn>2</mn></mfrac><mo>≤</mo></mrow><annotation encoding=\"application/x-tex\">\\frac{m}{2} \\leq</annotation></semantics></math></span></span> </span> input dimensions and <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi></mrow><annotation encoding=\"application/x-tex\">m</annotation></semantics></math></span></span> </span> is even.</li> <li>\n<strong>mode</strong> – <code>'constant'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'constant'</code>\n</li> <li>\n<strong>value</strong> – fill value for <code>'constant'</code> padding. Default: <code>0</code>\n</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)\n&gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side\n&gt;&gt;&gt; out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n&gt;&gt;&gt; print(out.size())\ntorch.Size([3, 3, 4, 4])\n&gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n&gt;&gt;&gt; out = F.pad(t4d, p2d, \"constant\", 0)\n&gt;&gt;&gt; print(out.size())\ntorch.Size([3, 3, 8, 4])\n&gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)\n&gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n&gt;&gt;&gt; out = F.pad(t4d, p3d, \"constant\", 0)\n&gt;&gt;&gt; print(out.size())\ntorch.Size([3, 9, 7, 3])\n</pre> </dd>\n</dl>   <h3 id=\"interpolate\"><span class=\"hidden-section\">interpolate</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.interpolate\">\n<code>torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#interpolate\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Down/up samples the input to either the given <code>size</code> or the given <code>scale_factor</code></p> <p>The algorithm used for interpolation is determined by <code>mode</code>.</p> <p>Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</p> <p>The input dimensions are interpreted in the form: <code>mini-batch x channels x [optional depth] x [optional height] x width</code>.</p> <p>The modes available for resizing are: <code>nearest</code>, <code>linear</code> (3D-only), <code>bilinear</code>, <code>bicubic</code> (4D-only), <code>trilinear</code> (5D-only), <code>area</code></p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the input tensor</li> <li>\n<strong>size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>] or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>] or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – output spatial size.</li> <li>\n<strong>scale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</li> <li>\n<strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – algorithm used for upsampling: <code>'nearest'</code> | <code>'linear'</code> | <code>'bilinear'</code> | <code>'bicubic'</code> | <code>'trilinear'</code> | <code>'area'</code>. Default: <code>'nearest'</code>\n</li> <li>\n<strong>align_corners</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to <code>True</code>, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to <code>False</code>, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation <em>independent</em> of input size when <code>scale_factor</code> is kept the same. This only has an effect when <code>mode</code> is <code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code> or <code>'trilinear'</code>. Default: <code>False</code>\n</li> <li>\n<strong>recompute_scale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – recompute the scale_factor for use in the interpolation calculation. When <code>scale_factor</code> is passed as a parameter, it is used to compute the <code>output_size</code>. If <code>recompute_scale_factor</code> is <code>False</code> or not specified, the passed-in <code>scale_factor</code> will be used in the interpolation computation. Otherwise, a new <code>scale_factor</code> will be computed based on the output and input sizes for use in the interpolation computation (i.e. the computation will be identical to if the computed <code>output_size</code> were passed-in explicitly). Note that when <code>scale_factor</code> is floating-point, the recomputed scale_factor may differ from the one passed in due to rounding and precision issues.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>With <code>mode='bicubic'</code>, it’s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call <code>result.clamp(min=0, max=255)</code> if you want to reduce the overshoot when displaying the image.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>With <code>align_corners = True</code>, the linearly interpolating modes (<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is <code>align_corners = False</code>. See <a class=\"reference internal\" href=\"generated/torch.nn.upsample#torch.nn.Upsample\" title=\"torch.nn.Upsample\"><code>Upsample</code></a> for concrete examples on how this affects the outputs.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When scale_factor is specified, if recompute_scale_factor=True, scale_factor is used to compute the output_size which will then be used to infer new scales for the interpolation. The default behavior for recompute_scale_factor changed to False in 1.6.0, and scale_factor is used in the interpolation calculation.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> </dd>\n</dl>   <h3 id=\"upsample\"><span class=\"hidden-section\">upsample</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.upsample\">\n<code>torch.nn.functional.upsample(input, size=None, scale_factor=None, mode='nearest', align_corners=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#upsample\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Upsamples the input to either the given <code>size</code> or the given <code>scale_factor</code></p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>torch.nn.functional.interpolate()</code></a>. This is equivalent with <code>nn.functional.interpolate(...)</code>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <p>The algorithm used for upsampling is determined by <code>mode</code>.</p> <p>Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape.</p> <p>The input dimensions are interpreted in the form: <code>mini-batch x channels x [optional depth] x [optional height] x width</code>.</p> <p>The modes available for upsampling are: <code>nearest</code>, <code>linear</code> (3D-only), <code>bilinear</code>, <code>bicubic</code> (4D-only), <code>trilinear</code> (5D-only)</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the input tensor</li> <li>\n<strong>size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>] or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>] or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – output spatial size.</li> <li>\n<strong>scale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>]</em>) – multiplier for spatial size. Has to match input size if it is a tuple.</li> <li>\n<strong>mode</strong> (<em>string</em>) – algorithm used for upsampling: <code>'nearest'</code> | <code>'linear'</code> | <code>'bilinear'</code> | <code>'bicubic'</code> | <code>'trilinear'</code>. Default: <code>'nearest'</code>\n</li> <li>\n<strong>align_corners</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the input and output as squares rather than points. If set to <code>True</code>, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to <code>False</code>, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation <em>independent</em> of input size when <code>scale_factor</code> is kept the same. This only has an effect when <code>mode</code> is <code>'linear'</code>, <code>'bilinear'</code>, <code>'bicubic'</code> or <code>'trilinear'</code>. Default: <code>False</code>\n</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>With <code>mode='bicubic'</code>, it’s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call <code>result.clamp(min=0, max=255)</code> if you want to reduce the overshoot when displaying the image.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>With <code>align_corners = True</code>, the linearly interpolating modes (<code>linear</code>, <code>bilinear</code>, and <code>trilinear</code>) don’t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is <code>align_corners = False</code>. See <a class=\"reference internal\" href=\"generated/torch.nn.upsample#torch.nn.Upsample\" title=\"torch.nn.Upsample\"><code>Upsample</code></a> for concrete examples on how this affects the outputs.</p> </div> </dd>\n</dl>   <h3 id=\"upsample-nearest\"><span class=\"hidden-section\">upsample_nearest</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.upsample_nearest\">\n<code>torch.nn.functional.upsample_nearest(input, size=None, scale_factor=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#upsample_nearest\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Upsamples the input, using nearest neighbours’ pixel values.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>torch.nn.functional.interpolate()</code></a>. This is equivalent with <code>nn.functional.interpolate(..., mode='nearest')</code>.</p> </div> <p>Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input</li> <li>\n<strong>size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>] or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – output spatia size.</li> <li>\n<strong>scale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – multiplier for spatial size. Has to be an integer.</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> </dd>\n</dl>   <h3 id=\"upsample-bilinear\"><span class=\"hidden-section\">upsample_bilinear</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.upsample_bilinear\">\n<code>torch.nn.functional.upsample_bilinear(input, size=None, scale_factor=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#upsample_bilinear\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Upsamples the input, using bilinear upsampling.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function is deprecated in favor of <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>torch.nn.functional.interpolate()</code></a>. This is equivalent with <code>nn.functional.interpolate(..., mode='bilinear', align_corners=True)</code>.</p> </div> <p>Expected inputs are spatial (4 dimensional). Use <code>upsample_trilinear</code> fo volumetric (5 dimensional) inputs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input</li> <li>\n<strong>size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – output spatial size.</li> <li>\n<strong>scale_factor</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em> or </em><em>Tuple</em><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – multiplier for spatial size</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> </dd>\n</dl>   <h3 id=\"grid-sample\"><span class=\"hidden-section\">grid_sample</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.grid_sample\">\n<code>torch.nn.functional.grid_sample(input, grid, mode='bilinear', padding_mode='zeros', align_corners=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#grid_sample\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Given an <code>input</code> and a flow-field <code>grid</code>, computes the <code>output</code> using <code>input</code> values and pixel locations from <code>grid</code>.</p> <p>Currently, only spatial (4-D) and volumetric (5-D) <code>input</code> are supported.</p> <p>In the spatial (4-D) case, for <code>input</code> with shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>in</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>in</mtext></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, H_\\text{in}, W_\\text{in})</annotation></semantics></math></span></span> </span> and <code>grid</code> with shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, H_\\text{out}, W_\\text{out}, 2)</annotation></semantics></math></span></span> </span>, the output will have shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>out</mtext></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, H_\\text{out}, W_\\text{out})</annotation></semantics></math></span></span> </span>.</p> <p>For each output location <code>output[n, :, h, w]</code>, the size-2 vector <code>grid[n, h, w]</code> specifies <code>input</code> pixel locations <code>x</code> and <code>y</code>, which are used to interpolate the output value <code>output[n, :, h, w]</code>. In the case of 5D inputs, <code>grid[n, d, h, w]</code> specifies the <code>x</code>, <code>y</code>, <code>z</code> pixel locations for interpolating <code>output[n, :, d, h, w]</code>. <code>mode</code> argument specifies <code>nearest</code> or <code>bilinear</code> interpolation method to sample the input pixels.</p> <p><code>grid</code> specifies the sampling pixel locations normalized by the <code>input</code> spatial dimensions. Therefore, it should have most values in the range of <code>[-1, 1]</code>. For example, values <code>x = -1, y = -1</code> is the left-top pixel of <code>input</code>, and values <code>x = 1, y = 1</code> is the right-bottom pixel of <code>input</code>.</p> <p>If <code>grid</code> has values outside the range of <code>[-1, 1]</code>, the corresponding outputs are handled as defined by <code>padding_mode</code>. Options are</p>  <ul class=\"simple\"> <li>\n<code>padding_mode=\"zeros\"</code>: use <code>0</code> for out-of-bound grid locations,</li> <li>\n<code>padding_mode=\"border\"</code>: use border values for out-of-bound grid locations,</li> <li>\n<code>padding_mode=\"reflection\"</code>: use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location <code>x = -3.5</code> reflects by border <code>-1</code> and becomes <code>x' = 1.5</code>, then reflects by border <code>1</code> and becomes <code>x'' = -0.5</code>.</li> </ul>  <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function is often used in conjunction with <a class=\"reference internal\" href=\"#torch.nn.functional.affine_grid\" title=\"torch.nn.functional.affine_grid\"><code>affine_grid()</code></a> to build <a class=\"reference external\" href=\"https://arxiv.org/abs/1506.02025\">Spatial Transformer Networks</a> .</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for background.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>NaN values in <code>grid</code> would be interpreted as <code>-1</code>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>input</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>in</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>in</mtext></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, H_\\text{in}, W_\\text{in})</annotation></semantics></math></span></span> </span> (4-D case) or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><mi>C</mi><mo separator=\"true\">,</mo><msub><mi>D</mi><mtext>in</mtext></msub><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>in</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>in</mtext></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})</annotation></semantics></math></span></span> </span> (5-D case)</li> <li>\n<strong>grid</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – flow-field of shape <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, H_\\text{out}, W_\\text{out}, 2)</annotation></semantics></math></span></span> </span> (4-D case) or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mo stretchy=\"false\">(</mo><mi>N</mi><mo separator=\"true\">,</mo><msub><mi>D</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><msub><mi>H</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><msub><mi>W</mi><mtext>out</mtext></msub><mo separator=\"true\">,</mo><mn>3</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)</annotation></semantics></math></span></span> </span> (5-D case)</li> <li>\n<strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – interpolation mode to calculate output values <code>'bilinear'</code> | <code>'nearest'</code> | <code>'bicubic'</code>. Default: <code>'bilinear'</code> Note: <code>mode='bicubic'</code> supports only 4-D input. When <code>mode='bilinear'</code> and the input is 5-D, the interpolation mode used internally will actually be trilinear. However, when the input is 4-D, the interpolation mode will legitimately be bilinear.</li> <li>\n<strong>padding_mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – padding mode for outside grid values <code>'zeros'</code> | <code>'border'</code> | <code>'reflection'</code>. Default: <code>'zeros'</code>\n</li> <li>\n<strong>align_corners</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Geometrically, we consider the pixels of the input as squares rather than points. If set to <code>True</code>, the extrema (<code>-1</code> and <code>1</code>) are considered as referring to the center points of the input’s corner pixels. If set to <code>False</code>, they are instead considered as referring to the corner points of the input’s corner pixels, making the sampling more resolution agnostic. This option parallels the <code>align_corners</code> option in <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>interpolate()</code></a>, and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: <code>False</code>\n</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>output Tensor</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When <code>align_corners = True</code>, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by <a class=\"reference internal\" href=\"#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample()</code></a> will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was <code>align_corners = True</code>. Since then, the default behavior has been changed to <code>align_corners = False</code>, in order to bring it in line with the default for <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>interpolate()</code></a>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><code>mode='bicubic'</code> is implemented using the <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Bicubic_interpolation\">cubic convolution algorithm</a> with <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi><mo>=</mo><mo>−</mo><mn>0.75</mn></mrow><annotation encoding=\"application/x-tex\">\\alpha=-0.75</annotation></semantics></math></span></span> </span>. The constant <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span></span> </span> might be different from packages to packages. For example, <a class=\"reference external\" href=\"https://github.com/python-pillow/Pillow/blob/4634eafe3c695a014267eefdce830b4a825beed7/src/libImaging/Resample.c#L51\">PIL</a> and <a class=\"reference external\" href=\"https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\">OpenCV</a> use -0.5 and -0.75 respectively. This algorithm may “overshoot” the range of values it’s interpolating. For example, it may produce negative values or values greater than 255 when interpolating input in [0, 255]. Clamp the results with :func: <code>torch.clamp</code> to ensure they are within the valid range.</p> </div> </dd>\n</dl>   <h3 id=\"affine-grid\"><span class=\"hidden-section\">affine_grid</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.functional.affine_grid\">\n<code>torch.nn.functional.affine_grid(theta, size, align_corners=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/functional.html#affine_grid\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices <code>theta</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function is often used in conjunction with <a class=\"reference internal\" href=\"#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample()</code></a> to build <a class=\"reference external\" href=\"https://arxiv.org/abs/1506.02025\">Spatial Transformer Networks</a> .</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>theta</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – input batch of affine matrices with shape (<span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mn>2</mn><mo>×</mo><mn>3</mn></mrow><annotation encoding=\"application/x-tex\">N \\times 2 \\times 3</annotation></semantics></math></span></span> </span>) for 2D or (<span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mn>3</mn><mo>×</mo><mn>4</mn></mrow><annotation encoding=\"application/x-tex\">N \\times 3 \\times 4</annotation></semantics></math></span></span> </span>) for 3D</li> <li>\n<strong>size</strong> (<em>torch.Size</em>) – the target output image size. (<span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N \\times C \\times H \\times W</annotation></semantics></math></span></span> </span> for 2D or <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>C</mi><mo>×</mo><mi>D</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">N \\times C \\times D \\times H \\times W</annotation></semantics></math></span></span> </span> for 3D) Example: torch.Size((32, 3, 24, 24))</li> <li>\n<strong>align_corners</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, consider <code>-1</code> and <code>1</code> to refer to the centers of the corner pixels rather than the image corners. Refer to <a class=\"reference internal\" href=\"#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample()</code></a> for a more complete description. A grid generated by <a class=\"reference internal\" href=\"#torch.nn.functional.affine_grid\" title=\"torch.nn.functional.affine_grid\"><code>affine_grid()</code></a> should be passed to <a class=\"reference internal\" href=\"#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample()</code></a> with the same setting for this option. Default: <code>False</code>\n</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>output Tensor of size (<span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi><mo>×</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">N \\times H \\times W \\times 2</annotation></semantics></math></span></span> </span>)</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>)</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When <code>align_corners = True</code>, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by <a class=\"reference internal\" href=\"#torch.nn.functional.grid_sample\" title=\"torch.nn.functional.grid_sample\"><code>grid_sample()</code></a> will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was <code>align_corners = True</code>. Since then, the default behavior has been changed to <code>align_corners = False</code>, in order to bring it in line with the default for <a class=\"reference internal\" href=\"#torch.nn.functional.interpolate\" title=\"torch.nn.functional.interpolate\"><code>interpolate()</code></a>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When <code>align_corners = True</code>, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when <code>align_corners = False</code>. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at <code>-1</code>. From version 1.3.0, under <code>align_corners = True</code> all grid points along a unit dimension are considered to be at <code>`0</code> (the center of the input image).</p> </div> </dd>\n</dl>    <h2 id=\"dataparallel-functions-multi-gpu-distributed\">DataParallel functions (multi-GPU, distributed)</h2>  <h3 id=\"data-parallel\"><span class=\"hidden-section\">data_parallel</span></h3> <dl class=\"function\"> <dt id=\"torch.nn.parallel.data_parallel\">\n<code>torch.nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/data_parallel.html#data_parallel\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p> <p>This is the functional version of the DataParallel module.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>module</strong> (<a class=\"reference internal\" href=\"generated/torch.nn.module#torch.nn.Module\" title=\"torch.nn.Module\">Module</a>) – the module to evaluate in parallel</li> <li>\n<strong>inputs</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the module</li> <li>\n<strong>device_ids</strong> (<em>list of python:int</em><em> or </em><a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a>) – GPU ids on which to replicate module</li> <li>\n<strong>output_device</strong> (<em>list of python:int</em><em> or </em><a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\">torch.device</a>) – GPU location of the output Use -1 to indicate the CPU. (default: device_ids[0])</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>a Tensor containing the result of module(input) located on output_device</p> </dd> </dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/nn.functional.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/nn.functional.html</a>\n  </p>\n</div>\n","tensors":"<h1 id=\"tensor-doc\">torch.Tensor</h1> <p id=\"torch-tensor\">A <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is a multi-dimensional matrix containing elements of a single data type.</p> <p>Torch defines 10 tensor types with CPU and GPU variants which are as follows:</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Data type</p></th> <th class=\"head\"><p>dtype</p></th> <th class=\"head\"><p>CPU tensor</p></th> <th class=\"head\"><p>GPU tensor</p></th> </tr> </thead>  <tr>\n<td><p>32-bit floating point</p></td> <td><p><code>torch.float32</code> or <code>torch.float</code></p></td> <td><p><code>torch.FloatTensor</code></p></td> <td><p><code>torch.cuda.FloatTensor</code></p></td> </tr> <tr>\n<td><p>64-bit floating point</p></td> <td><p><code>torch.float64</code> or <code>torch.double</code></p></td> <td><p><code>torch.DoubleTensor</code></p></td> <td><p><code>torch.cuda.DoubleTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id3\" id=\"id1\">1</a></p></td> <td><p><code>torch.float16</code> or <code>torch.half</code></p></td> <td><p><code>torch.HalfTensor</code></p></td> <td><p><code>torch.cuda.HalfTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id4\" id=\"id2\">2</a></p></td> <td><p><code>torch.bfloat16</code></p></td> <td><p><code>torch.BFloat16Tensor</code></p></td> <td><p><code>torch.cuda.BFloat16Tensor</code></p></td> </tr> <tr>\n<td><p>32-bit complex</p></td> <td><p><code>torch.complex32</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>64-bit complex</p></td> <td><p><code>torch.complex64</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>128-bit complex</p></td> <td><p><code>torch.complex128</code> or <code>torch.cdouble</code></p></td> <td></td> <td></td> </tr> <tr>\n<td><p>8-bit integer (unsigned)</p></td> <td><p><code>torch.uint8</code></p></td> <td><p><code>torch.ByteTensor</code></p></td> <td><p><code>torch.cuda.ByteTensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (signed)</p></td> <td><p><code>torch.int8</code></p></td> <td><p><code>torch.CharTensor</code></p></td> <td><p><code>torch.cuda.CharTensor</code></p></td> </tr> <tr>\n<td><p>16-bit integer (signed)</p></td> <td><p><code>torch.int16</code> or <code>torch.short</code></p></td> <td><p><code>torch.ShortTensor</code></p></td> <td><p><code>torch.cuda.ShortTensor</code></p></td> </tr> <tr>\n<td><p>32-bit integer (signed)</p></td> <td><p><code>torch.int32</code> or <code>torch.int</code></p></td> <td><p><code>torch.IntTensor</code></p></td> <td><p><code>torch.cuda.IntTensor</code></p></td> </tr> <tr>\n<td><p>64-bit integer (signed)</p></td> <td><p><code>torch.int64</code> or <code>torch.long</code></p></td> <td><p><code>torch.LongTensor</code></p></td> <td><p><code>torch.cuda.LongTensor</code></p></td> </tr> <tr>\n<td><p>Boolean</p></td> <td><p><code>torch.bool</code></p></td> <td><p><code>torch.BoolTensor</code></p></td> <td><p><code>torch.cuda.BoolTensor</code></p></td> </tr>  </table> <dl class=\"footnote brackets\"> <dt class=\"label\" id=\"id3\">\n<code>1</code> </dt> <dd>\n<p>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important at the expense of range.</p> </dd> <dt class=\"label\" id=\"id4\">\n<code>2</code> </dt> <dd>\n<p>Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as <code>float32</code></p> </dd> </dl> <p><a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is an alias for the default tensor type (<code>torch.FloatTensor</code>).</p> <p>A tensor can be constructed from a Python <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\"><code>list</code></a> or sequence using the <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a> constructor:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n&gt;&gt;&gt; torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n</pre> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a> always copies <code>data</code>. If you have a Tensor <code>data</code> and just want to change its <code>requires_grad</code> flag, use <a class=\"reference internal\" href=\"#torch.Tensor.requires_grad_\" title=\"torch.Tensor.requires_grad_\"><code>requires_grad_()</code></a> or <a class=\"reference internal\" href=\"autograd#torch.Tensor.detach\" title=\"torch.Tensor.detach\"><code>detach()</code></a> to avoid a copy. If you have a numpy array and want to avoid a copy, use <a class=\"reference internal\" href=\"generated/torch.as_tensor#torch.as_tensor\" title=\"torch.as_tensor\"><code>torch.as_tensor()</code></a>.</p> </div> <p>A tensor of specific data type can be constructed by passing a <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and/or a <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> to a constructor or tensor creation op:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n&gt;&gt;&gt; cuda0 = torch.device('cuda:0')\n&gt;&gt;&gt; torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n</pre> <p>The contents of a tensor can be accessed and modified using Python’s indexing and slicing notation:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n&gt;&gt;&gt; print(x[1][2])\ntensor(6)\n&gt;&gt;&gt; x[0][1] = 8\n&gt;&gt;&gt; print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n</pre> <p>Use <a class=\"reference internal\" href=\"#torch.Tensor.item\" title=\"torch.Tensor.item\"><code>torch.Tensor.item()</code></a> to get a Python number from a tensor containing a single value:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1]])\n&gt;&gt;&gt; x\ntensor([[ 1]])\n&gt;&gt;&gt; x.item()\n1\n&gt;&gt;&gt; x = torch.tensor(2.5)\n&gt;&gt;&gt; x\ntensor(2.5000)\n&gt;&gt;&gt; x.item()\n2.5\n</pre> <p>A tensor can be created with <code>requires_grad=True</code> so that <a class=\"reference internal\" href=\"autograd#module-torch.autograd\" title=\"torch.autograd\"><code>torch.autograd</code></a> records operations on them for automatic differentiation.</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n&gt;&gt;&gt; out = x.pow(2).sum()\n&gt;&gt;&gt; out.backward()\n&gt;&gt;&gt; x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n</pre> <p>Each tensor has an associated <code>torch.Storage</code>, which holds its data. The tensor class also provides multi-dimensional, <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\">strided</a> view of a storage and defines numeric operations on it.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For more information on tensor views, see <a class=\"reference internal\" href=\"tensor_view#tensor-view-doc\"><span class=\"std std-ref\">Tensor Views</span></a>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For more information on the <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.layout\" title=\"torch.torch.layout\"><code>torch.layout</code></a> attributes of a <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>, see <a class=\"reference internal\" href=\"tensor_attributes#tensor-attributes-doc\"><span class=\"std std-ref\">Tensor Attributes</span></a>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Methods which mutate a tensor are marked with an underscore suffix. For example, <code>torch.FloatTensor.abs_()</code> computes the absolute value in-place and returns the modified tensor, while <code>torch.FloatTensor.abs()</code> computes the result in a new tensor.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>To change an existing tensor’s <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> and/or <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, consider using <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a> method on the tensor.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Current implementation of <a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> introduces memory overhead, thus it might lead to unexpectedly high memory usage in the applications with many tiny tensors. If this is your case, consider using one large structure.</p> </div> <dl class=\"class\"> <dt id=\"torch.Tensor\">\n<code>class torch.Tensor</code> </dt> <dd>\n<p>There are a few main ways to create a tensor, depending on your use case.</p> <ul class=\"simple\"> <li>To create a tensor with pre-existing data, use <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>torch.tensor()</code></a>.</li> <li>To create a tensor with specific size, use <code>torch.*</code> tensor creation ops (see <a class=\"reference internal\" href=\"torch#tensor-creation-ops\"><span class=\"std std-ref\">Creation Ops</span></a>).</li> <li>To create a tensor with the same size (and similar types) as another tensor, use <code>torch.*_like</code> tensor creation ops (see <a class=\"reference internal\" href=\"torch#tensor-creation-ops\"><span class=\"std std-ref\">Creation Ops</span></a>).</li> <li>To create a tensor with similar type but different size as another tensor, use <code>tensor.new_*</code> creation ops.</li> </ul> <dl class=\"method\"> <dt id=\"torch.Tensor.new_tensor\">\n<code>new_tensor(data, dtype=None, device=None, requires_grad=False) → Tensor</code> </dt> <dd>\n<p>Returns a new Tensor with <code>data</code> as the tensor data. By default, the returned Tensor has the same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.Tensor.new_tensor\" title=\"torch.Tensor.new_tensor\"><code>new_tensor()</code></a> always copies <code>data</code>. If you have a Tensor <code>data</code> and want to avoid a copy, use <a class=\"reference internal\" href=\"#torch.Tensor.requires_grad_\" title=\"torch.Tensor.requires_grad_\"><code>torch.Tensor.requires_grad_()</code></a> or <a class=\"reference internal\" href=\"autograd#torch.Tensor.detach\" title=\"torch.Tensor.detach\"><code>torch.Tensor.detach()</code></a>. If you have a numpy array and want to avoid a copy, use <a class=\"reference internal\" href=\"generated/torch.from_numpy#torch.from_numpy\" title=\"torch.from_numpy\"><code>torch.from_numpy()</code></a>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When data is a tensor <code>x</code>, <a class=\"reference internal\" href=\"#torch.Tensor.new_tensor\" title=\"torch.Tensor.new_tensor\"><code>new_tensor()</code></a> reads out ‘the data’ from whatever it is passed, and constructs a leaf variable. Therefore <code>tensor.new_tensor(x)</code> is equivalent to <code>x.clone().detach()</code> and <code>tensor.new_tensor(x, requires_grad=True)</code> is equivalent to <code>x.clone().detach().requires_grad_(True)</code>. The equivalents using <code>clone()</code> and <code>detach()</code> are recommended.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>data</strong> (<em>array_like</em>) – The returned Tensor copies <code>data</code>.</li> <li>\n<strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> as this tensor.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</li> <li>\n<strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8)\n&gt;&gt;&gt; data = [[0, 1], [2, 3]]\n&gt;&gt;&gt; tensor.new_tensor(data)\ntensor([[ 0,  1],\n        [ 2,  3]], dtype=torch.int8)\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.new_full\">\n<code>new_full(size, fill_value, dtype=None, device=None, requires_grad=False) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> filled with <code>fill_value</code>. By default, the returned Tensor has the same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>fill_value</strong> (<em>scalar</em>) – the number to fill the output tensor with.</li> <li>\n<strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> as this tensor.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</li> <li>\n<strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64)\n&gt;&gt;&gt; tensor.new_full((3, 4), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.new_empty\">\n<code>new_empty(size, dtype=None, device=None, requires_grad=False) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> filled with uninitialized data. By default, the returned Tensor has the same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> as this tensor.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</li> <li>\n<strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.ones(())\n&gt;&gt;&gt; tensor.new_empty((2, 3))\ntensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.new_ones\">\n<code>new_ones(size, dtype=None, device=None, requires_grad=False) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> filled with <code>1</code>. By default, the returned Tensor has the same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>size</strong> (<em>int...</em>) – a list, tuple, or <code>torch.Size</code> of integers defining the shape of the output tensor.</li> <li>\n<strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> as this tensor.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</li> <li>\n<strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32)\n&gt;&gt;&gt; tensor.new_ones((2, 3))\ntensor([[ 1,  1,  1],\n        [ 1,  1,  1]], dtype=torch.int32)\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.new_zeros\">\n<code>new_zeros(size, dtype=None, device=None, requires_grad=False) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> filled with <code>0</code>. By default, the returned Tensor has the same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>size</strong> (<em>int...</em>) – a list, tuple, or <code>torch.Size</code> of integers defining the shape of the output tensor.</li> <li>\n<strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, optional) – the desired type of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> as this tensor.</li> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as this tensor.</li> <li>\n<strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64)\n&gt;&gt;&gt; tensor.new_zeros((2, 3))\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]], dtype=torch.float64)\n</pre> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.is_cuda\">\n<code>is_cuda</code> </dt> <dd>\n<p>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.is_quantized\">\n<code>is_quantized</code> </dt> <dd>\n<p>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.is_meta\">\n<code>is_meta</code> </dt> <dd>\n<p>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise. Meta tensors are like normal tensors, but they carry no data.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.device\">\n<code>device</code> </dt> <dd>\n<p>Is the <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> where this Tensor is.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt>\n<code>grad</code> </dt> <dd>\n<p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a class=\"reference internal\" href=\"autograd#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a> computes gradients for <code>self</code>. The attribute will then contain the gradients computed and future calls to <a class=\"reference internal\" href=\"autograd#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a> will accumulate (add) gradients into it.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.ndim\">\n<code>ndim</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim()</code></a></p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.T\">\n<code>T</code> </dt> <dd>\n<p>Is this Tensor with its dimensions reversed.</p> <p>If <code>n</code> is the number of dimensions in <code>x</code>, <code>x.T</code> is equivalent to <code>x.permute(n-1, n-2, ..., 0)</code>.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.real\">\n<code>real</code> </dt> <dd>\n<p>Returns a new tensor containing real values of the <code>self</code> tensor. The returned tensor and <code>self</code> share the same underlying storage.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"generated/torch.real#torch.real\" title=\"torch.real\"><code>real()</code></a> is only supported for tensors with complex dtypes.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat)\n&gt;&gt;&gt; x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n&gt;&gt;&gt; x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.imag\">\n<code>imag</code> </dt> <dd>\n<p>Returns a new tensor containing imaginary values of the <code>self</code> tensor. The returned tensor and <code>self</code> share the same underlying storage.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"generated/torch.imag#torch.imag\" title=\"torch.imag\"><code>imag()</code></a> is only supported for tensors with complex dtypes.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; x=torch.randn(4, dtype=torch.cfloat)\n&gt;&gt;&gt; x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n&gt;&gt;&gt; x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.abs\">\n<code>abs() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>torch.abs()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.abs_\">\n<code>abs_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.abs\" title=\"torch.Tensor.abs\"><code>abs()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.absolute\">\n<code>absolute() → Tensor</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"generated/torch.abs#torch.abs\" title=\"torch.abs\"><code>abs()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.absolute_\">\n<code>absolute_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.absolute\" title=\"torch.Tensor.absolute\"><code>absolute()</code></a> Alias for <a class=\"reference internal\" href=\"#torch.Tensor.abs_\" title=\"torch.Tensor.abs_\"><code>abs_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.acos\">\n<code>acos() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.acos#torch.acos\" title=\"torch.acos\"><code>torch.acos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.acos_\">\n<code>acos_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.acos\" title=\"torch.Tensor.acos\"><code>acos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arccos\">\n<code>arccos() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.arccos#torch.arccos\" title=\"torch.arccos\"><code>torch.arccos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arccos_\">\n<code>arccos_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arccos\" title=\"torch.Tensor.arccos\"><code>arccos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.add\">\n<code>add(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>Add a scalar or tensor to <code>self</code> tensor. If both <code>alpha</code> and <code>other</code> are specified, each element of <code>other</code> is scaled by <code>alpha</code> before being used.</p> <p>When <code>other</code> is a tensor, the shape of <code>other</code> must be <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">broadcastable</span></a> with the shape of the underlying tensor</p> <p>See <a class=\"reference internal\" href=\"generated/torch.add#torch.add\" title=\"torch.add\"><code>torch.add()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.add_\">\n<code>add_(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.add\" title=\"torch.Tensor.add\"><code>add()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addbmm\">\n<code>addbmm(batch1, batch2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addbmm#torch.addbmm\" title=\"torch.addbmm\"><code>torch.addbmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addbmm_\">\n<code>addbmm_(batch1, batch2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addbmm\" title=\"torch.Tensor.addbmm\"><code>addbmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addcdiv\">\n<code>addcdiv(tensor1, tensor2, *, value=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addcdiv#torch.addcdiv\" title=\"torch.addcdiv\"><code>torch.addcdiv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addcdiv_\">\n<code>addcdiv_(tensor1, tensor2, *, value=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addcdiv\" title=\"torch.Tensor.addcdiv\"><code>addcdiv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addcmul\">\n<code>addcmul(tensor1, tensor2, *, value=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addcmul#torch.addcmul\" title=\"torch.addcmul\"><code>torch.addcmul()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addcmul_\">\n<code>addcmul_(tensor1, tensor2, *, value=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addcmul\" title=\"torch.Tensor.addcmul\"><code>addcmul()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addmm\">\n<code>addmm(mat1, mat2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addmm#torch.addmm\" title=\"torch.addmm\"><code>torch.addmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addmm_\">\n<code>addmm_(mat1, mat2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addmm\" title=\"torch.Tensor.addmm\"><code>addmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>sspaddmm(mat1, mat2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"sparse#torch.sspaddmm\" title=\"torch.sspaddmm\"><code>torch.sspaddmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addmv\">\n<code>addmv(mat, vec, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addmv#torch.addmv\" title=\"torch.addmv\"><code>torch.addmv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addmv_\">\n<code>addmv_(mat, vec, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addmv\" title=\"torch.Tensor.addmv\"><code>addmv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addr\">\n<code>addr(vec1, vec2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.addr#torch.addr\" title=\"torch.addr\"><code>torch.addr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.addr_\">\n<code>addr_(vec1, vec2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.addr\" title=\"torch.Tensor.addr\"><code>addr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.allclose\">\n<code>allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>torch.allclose()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.amax\">\n<code>amax(dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.amax#torch.amax\" title=\"torch.amax\"><code>torch.amax()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.amin\">\n<code>amin(dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.amin#torch.amin\" title=\"torch.amin\"><code>torch.amin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.angle\">\n<code>angle() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.angle#torch.angle\" title=\"torch.angle\"><code>torch.angle()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.apply_\">\n<code>apply_(callable) → Tensor</code> </dt> <dd>\n<p>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function only works with CPU tensors and should not be used in code sections that require high performance.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.argmax\">\n<code>argmax(dim=None, keepdim=False) → LongTensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.argmax#torch.argmax\" title=\"torch.argmax\"><code>torch.argmax()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.argmin\">\n<code>argmin(dim=None, keepdim=False) → LongTensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.argmin#torch.argmin\" title=\"torch.argmin\"><code>torch.argmin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.argsort\">\n<code>argsort(dim=-1, descending=False) → LongTensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.argsort#torch.argsort\" title=\"torch.argsort\"><code>torch.argsort()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.asin\">\n<code>asin() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.asin#torch.asin\" title=\"torch.asin\"><code>torch.asin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.asin_\">\n<code>asin_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.asin\" title=\"torch.Tensor.asin\"><code>asin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arcsin\">\n<code>arcsin() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.arcsin#torch.arcsin\" title=\"torch.arcsin\"><code>torch.arcsin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arcsin_\">\n<code>arcsin_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arcsin\" title=\"torch.Tensor.arcsin\"><code>arcsin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.as_strided\">\n<code>as_strided(size, stride, storage_offset=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.as_strided#torch.as_strided\" title=\"torch.as_strided\"><code>torch.as_strided()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atan\">\n<code>atan() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.atan#torch.atan\" title=\"torch.atan\"><code>torch.atan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atan_\">\n<code>atan_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.atan\" title=\"torch.Tensor.atan\"><code>atan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arctan\">\n<code>arctan() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.arctan#torch.arctan\" title=\"torch.arctan\"><code>torch.arctan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arctan_\">\n<code>arctan_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arctan\" title=\"torch.Tensor.arctan\"><code>arctan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atan2\">\n<code>atan2(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.atan2#torch.atan2\" title=\"torch.atan2\"><code>torch.atan2()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atan2_\">\n<code>atan2_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.atan2\" title=\"torch.Tensor.atan2\"><code>atan2()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.all\">\n<code>all(dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.all#torch.all\" title=\"torch.all\"><code>torch.all()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.any\">\n<code>any(dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.any#torch.any\" title=\"torch.any\"><code>torch.any()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.backward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes the gradient of current tensor w.r.t. graph leaves.</p> <p>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying <code>gradient</code>. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. <code>self</code>.</p> <p>This function accumulates gradients in the leaves - you might need to zero <code>.grad</code> attributes or set them to <code>None</code> before calling it. See <a class=\"reference internal\" href=\"autograd#default-grad-layouts\"><span class=\"std std-ref\">Default gradient layouts</span></a> for details on the memory layout of accumulated gradients.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you run any forward ops, create <code>gradient</code>, and/or call <code>backward</code> in a user-specified CUDA stream context, see <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#bwd-cuda-stream-semantics\"><span class=\"std std-ref\">Stream semantics of backward passes</span></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>gradient</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/constants.html#None\" title=\"(in Python v3.9)\">None</a>) – Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless <code>create_graph</code> is True. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable then this argument is optional.</li> <li>\n<strong>retain_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to <code>False</code>.</li> <li>\n<strong>inputs</strong> (<em>sequence of Tensor</em>) – Inputs w.r.t. which the gradient will be accumulated into <code>.grad</code>. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.baddbmm\">\n<code>baddbmm(batch1, batch2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.baddbmm#torch.baddbmm\" title=\"torch.baddbmm\"><code>torch.baddbmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.baddbmm_\">\n<code>baddbmm_(batch1, batch2, *, beta=1, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.baddbmm\" title=\"torch.Tensor.baddbmm\"><code>baddbmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bernoulli\">\n<code>bernoulli(*, generator=None) → Tensor</code> </dt> <dd>\n<p>Returns a result tensor where each <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext mathvariant=\"monospace\">result[i]</mtext></mrow><annotation encoding=\"application/x-tex\">\\texttt{result[i]}</annotation></semantics></math></span></span> </span> is independently sampled from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">self[i]</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Bernoulli}(\\texttt{self[i]})</annotation></semantics></math></span></span> </span>. <code>self</code> must have floating point <code>dtype</code>, and the result will have the same <code>dtype</code>.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>torch.bernoulli()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bernoulli_\">\n<code>bernoulli_()</code> </dt> <dd>\n<dl class=\"function\"> <dt>\n<code>bernoulli_(p=0.5, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills each location of <code>self</code> with an independent sample from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">p</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Bernoulli}(\\texttt{p})</annotation></semantics></math></span></span> </span>. <code>self</code> can have integral <code>dtype</code>.</p> </dd>\n</dl> <dl class=\"function\"> <dt>\n<code>bernoulli_(p_tensor, *, generator=None) → Tensor</code> </dt> <dd>\n<p><code>p_tensor</code> should be a tensor containing probabilities to be used for drawing the binary random number.</p> <p>The <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mtext>i</mtext><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\text{i}^{th}</annotation></semantics></math></span></span> </span> element of <code>self</code> tensor will be set to a value sampled from <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>Bernoulli</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">p_tensor[i]</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\text{Bernoulli}(\\texttt{p\\_tensor[i]})</annotation></semantics></math></span></span> </span>.</p> <p><code>self</code> can have integral <code>dtype</code>, but <code>p_tensor</code> must have floating point <code>dtype</code>.</p> </dd>\n</dl> <p>See also <a class=\"reference internal\" href=\"#torch.Tensor.bernoulli\" title=\"torch.Tensor.bernoulli\"><code>bernoulli()</code></a> and <a class=\"reference internal\" href=\"generated/torch.bernoulli#torch.bernoulli\" title=\"torch.bernoulli\"><code>torch.bernoulli()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bfloat16\">\n<code>bfloat16(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bincount\">\n<code>bincount(weights=None, minlength=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bincount#torch.bincount\" title=\"torch.bincount\"><code>torch.bincount()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_not\">\n<code>bitwise_not() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_not#torch.bitwise_not\" title=\"torch.bitwise_not\"><code>torch.bitwise_not()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_not_\">\n<code>bitwise_not_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.bitwise_not\" title=\"torch.Tensor.bitwise_not\"><code>bitwise_not()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_and\">\n<code>bitwise_and() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_and#torch.bitwise_and\" title=\"torch.bitwise_and\"><code>torch.bitwise_and()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_and_\">\n<code>bitwise_and_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.bitwise_and\" title=\"torch.Tensor.bitwise_and\"><code>bitwise_and()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_or\">\n<code>bitwise_or() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_or#torch.bitwise_or\" title=\"torch.bitwise_or\"><code>torch.bitwise_or()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_or_\">\n<code>bitwise_or_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.bitwise_or\" title=\"torch.Tensor.bitwise_or\"><code>bitwise_or()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_xor\">\n<code>bitwise_xor() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bitwise_xor#torch.bitwise_xor\" title=\"torch.bitwise_xor\"><code>torch.bitwise_xor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bitwise_xor_\">\n<code>bitwise_xor_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.bitwise_xor\" title=\"torch.Tensor.bitwise_xor\"><code>bitwise_xor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bmm\">\n<code>bmm(batch2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.bmm#torch.bmm\" title=\"torch.bmm\"><code>torch.bmm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.bool\">\n<code>bool(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.byte\">\n<code>byte(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.broadcast_to\">\n<code>broadcast_to(shape) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.broadcast_to#torch.broadcast_to\" title=\"torch.broadcast_to\"><code>torch.broadcast_to()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cauchy_\">\n<code>cauchy_(median=0, sigma=1, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills the tensor with numbers drawn from the Cauchy distribution:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>π</mi></mfrac><mfrac><mi>σ</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo>−</mo><mtext>median</mtext><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup><mo>+</mo><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}</annotation></semantics></math></span></span></span> </div>\n</dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ceil\">\n<code>ceil() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ceil#torch.ceil\" title=\"torch.ceil\"><code>torch.ceil()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ceil_\">\n<code>ceil_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.ceil\" title=\"torch.Tensor.ceil\"><code>ceil()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.char\">\n<code>char(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cholesky\">\n<code>cholesky(upper=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cholesky#torch.cholesky\" title=\"torch.cholesky\"><code>torch.cholesky()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cholesky_inverse\">\n<code>cholesky_inverse(upper=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cholesky_inverse#torch.cholesky_inverse\" title=\"torch.cholesky_inverse\"><code>torch.cholesky_inverse()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cholesky_solve\">\n<code>cholesky_solve(input2, upper=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cholesky_solve#torch.cholesky_solve\" title=\"torch.cholesky_solve\"><code>torch.cholesky_solve()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.chunk\">\n<code>chunk(chunks, dim=0) → List of Tensors</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.chunk#torch.chunk\" title=\"torch.chunk\"><code>torch.chunk()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.clamp\">\n<code>clamp(min, max) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.clamp#torch.clamp\" title=\"torch.clamp\"><code>torch.clamp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.clamp_\">\n<code>clamp_(min, max) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.clamp\" title=\"torch.Tensor.clamp\"><code>clamp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.clip\">\n<code>clip(min, max) → Tensor</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"#torch.Tensor.clamp\" title=\"torch.Tensor.clamp\"><code>clamp()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.clip_\">\n<code>clip_(min, max) → Tensor</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"#torch.Tensor.clamp_\" title=\"torch.Tensor.clamp_\"><code>clamp_()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.clone\">\n<code>clone(*, memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.clone#torch.clone\" title=\"torch.clone\"><code>torch.clone()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.contiguous\">\n<code>contiguous(memory_format=torch.contiguous_format) → Tensor</code> </dt> <dd>\n<p>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor. If <code>self</code> tensor is already in the specified memory format, this function returns the <code>self</code> tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.contiguous_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.copy_\">\n<code>copy_(src, non_blocking=False) → Tensor</code> </dt> <dd>\n<p>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</p> <p>The <code>src</code> tensor must be <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">broadcastable</span></a> with the <code>self</code> tensor. It may be of a different data type or reside on a different device.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>src</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the source tensor to copy from</li> <li>\n<strong>non_blocking</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – if <code>True</code> and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.conj\">\n<code>conj() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.conj#torch.conj\" title=\"torch.conj\"><code>torch.conj()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.copysign\">\n<code>copysign(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.copysign#torch.copysign\" title=\"torch.copysign\"><code>torch.copysign()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.copysign_\">\n<code>copysign_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.copysign\" title=\"torch.Tensor.copysign\"><code>copysign()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cos\">\n<code>cos() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cos#torch.cos\" title=\"torch.cos\"><code>torch.cos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cos_\">\n<code>cos_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.cos\" title=\"torch.Tensor.cos\"><code>cos()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cosh\">\n<code>cosh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cosh#torch.cosh\" title=\"torch.cosh\"><code>torch.cosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cosh_\">\n<code>cosh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.cosh\" title=\"torch.Tensor.cosh\"><code>cosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.count_nonzero\">\n<code>count_nonzero(dim=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.count_nonzero#torch.count_nonzero\" title=\"torch.count_nonzero\"><code>torch.count_nonzero()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.acosh\">\n<code>acosh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.acosh#torch.acosh\" title=\"torch.acosh\"><code>torch.acosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.acosh_\">\n<code>acosh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.acosh\" title=\"torch.Tensor.acosh\"><code>acosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arccosh\">\n<code>arccosh()</code> </dt> <dd>\n<p>acosh() -&gt; Tensor</p> <p>See <a class=\"reference internal\" href=\"generated/torch.arccosh#torch.arccosh\" title=\"torch.arccosh\"><code>torch.arccosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arccosh_\">\n<code>arccosh_()</code> </dt> <dd>\n<p>acosh_() -&gt; Tensor</p> <p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arccosh\" title=\"torch.Tensor.arccosh\"><code>arccosh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cpu\">\n<code>cpu(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p>Returns a copy of this object in CPU memory.</p> <p>If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cross\">\n<code>cross(other, dim=-1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cross#torch.cross\" title=\"torch.cross\"><code>torch.cross()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cuda\">\n<code>cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p>Returns a copy of this object in CUDA memory.</p> <p>If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>device</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>) – The destination GPU device. Defaults to the current CUDA device.</li> <li>\n<strong>non_blocking</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – If <code>True</code> and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: <code>False</code>.</li> <li>\n<strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logcumsumexp\">\n<code>logcumsumexp(dim) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logcumsumexp#torch.logcumsumexp\" title=\"torch.logcumsumexp\"><code>torch.logcumsumexp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cummax\">\n<code>cummax(dim) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cummax#torch.cummax\" title=\"torch.cummax\"><code>torch.cummax()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cummin\">\n<code>cummin(dim) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cummin#torch.cummin\" title=\"torch.cummin\"><code>torch.cummin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cumprod\">\n<code>cumprod(dim, dtype=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cumprod#torch.cumprod\" title=\"torch.cumprod\"><code>torch.cumprod()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cumprod_\">\n<code>cumprod_(dim, dtype=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.cumprod\" title=\"torch.Tensor.cumprod\"><code>cumprod()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cumsum\">\n<code>cumsum(dim, dtype=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.cumsum#torch.cumsum\" title=\"torch.cumsum\"><code>torch.cumsum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.cumsum_\">\n<code>cumsum_(dim, dtype=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.cumsum\" title=\"torch.Tensor.cumsum\"><code>cumsum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.data_ptr\">\n<code>data_ptr() → int</code> </dt> <dd>\n<p>Returns the address of the first element of <code>self</code> tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.deg2rad\">\n<code>deg2rad() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.deg2rad#torch.deg2rad\" title=\"torch.deg2rad\"><code>torch.deg2rad()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.dequantize\">\n<code>dequantize() → Tensor</code> </dt> <dd>\n<p>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.det\">\n<code>det() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.det#torch.det\" title=\"torch.det\"><code>torch.det()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>dense_dim() → int</code> </dt> <dd>\n<p>Return the number of dense dimensions in a <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">sparse tensor</span></a> <code>self</code>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Throws an error if <code>self</code> is not a sparse tensor.</p> </div> <p>See also <a class=\"reference internal\" href=\"sparse#torch.Tensor.sparse_dim\" title=\"torch.Tensor.sparse_dim\"><code>Tensor.sparse_dim()</code></a> and <a class=\"reference internal\" href=\"sparse#sparse-hybrid-coo-docs\"><span class=\"std std-ref\">hybrid tensors</span></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>detach()</code> </dt> <dd>\n<p>Returns a new Tensor, detached from the current graph.</p> <p>The result will never require gradient.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as <code>zero_</code> / <code>copy_</code> / <code>add_</code>) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>detach_()</code> </dt> <dd>\n<p>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.diag\">\n<code>diag(diagonal=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.diag#torch.diag\" title=\"torch.diag\"><code>torch.diag()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.diag_embed\">\n<code>diag_embed(offset=0, dim1=-2, dim2=-1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.diag_embed#torch.diag_embed\" title=\"torch.diag_embed\"><code>torch.diag_embed()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.diagflat\">\n<code>diagflat(offset=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.diagflat#torch.diagflat\" title=\"torch.diagflat\"><code>torch.diagflat()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.diagonal\">\n<code>diagonal(offset=0, dim1=0, dim2=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.diagonal#torch.diagonal\" title=\"torch.diagonal\"><code>torch.diagonal()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fill_diagonal_\">\n<code>fill_diagonal_(fill_value, wrap=False) → Tensor</code> </dt> <dd>\n<p>Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>fill_value</strong> (<em>Scalar</em>) – the fill value</li> <li>\n<strong>wrap</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – the diagonal ‘wrapped’ after N columns for tall matrices.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.zeros(3, 3)\n&gt;&gt;&gt; a.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n&gt;&gt;&gt; b = torch.zeros(7, 3)\n&gt;&gt;&gt; b.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n&gt;&gt;&gt; c = torch.zeros(7, 3)\n&gt;&gt;&gt; c.fill_diagonal_(5, wrap=True)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fmax\">\n<code>fmax(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.fmax#torch.fmax\" title=\"torch.fmax\"><code>torch.fmax()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fmin\">\n<code>fmin(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.fmin#torch.fmin\" title=\"torch.fmin\"><code>torch.fmin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.diff\">\n<code>diff(n=1, dim=-1, prepend=None, append=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.diff#torch.diff\" title=\"torch.diff\"><code>torch.diff()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.digamma\">\n<code>digamma() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.digamma#torch.digamma\" title=\"torch.digamma\"><code>torch.digamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.digamma_\">\n<code>digamma_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.digamma\" title=\"torch.Tensor.digamma\"><code>digamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.dim\">\n<code>dim() → int</code> </dt> <dd>\n<p>Returns the number of dimensions of <code>self</code> tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.dist\">\n<code>dist(other, p=2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.dist#torch.dist\" title=\"torch.dist\"><code>torch.dist()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.div\">\n<code>div(value, *, rounding_mode=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.div#torch.div\" title=\"torch.div\"><code>torch.div()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.div_\">\n<code>div_(value, *, rounding_mode=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.div\" title=\"torch.Tensor.div\"><code>div()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.divide\">\n<code>divide(value, *, rounding_mode=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.divide#torch.divide\" title=\"torch.divide\"><code>torch.divide()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.divide_\">\n<code>divide_(value, *, rounding_mode=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.divide\" title=\"torch.Tensor.divide\"><code>divide()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.dot\">\n<code>dot(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.dot#torch.dot\" title=\"torch.dot\"><code>torch.dot()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.double\">\n<code>double(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.eig\">\n<code>eig(eigenvectors=False) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.eig#torch.eig\" title=\"torch.eig\"><code>torch.eig()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.element_size\">\n<code>element_size() → int</code> </dt> <dd>\n<p>Returns the size in bytes of an individual element.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.tensor([]).element_size()\n4\n&gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size()\n1\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.eq\">\n<code>eq(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.eq#torch.eq\" title=\"torch.eq\"><code>torch.eq()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.eq_\">\n<code>eq_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.eq\" title=\"torch.Tensor.eq\"><code>eq()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.equal\">\n<code>equal(other) → bool</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.equal#torch.equal\" title=\"torch.equal\"><code>torch.equal()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erf\">\n<code>erf() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.erf#torch.erf\" title=\"torch.erf\"><code>torch.erf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erf_\">\n<code>erf_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.erf\" title=\"torch.Tensor.erf\"><code>erf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erfc\">\n<code>erfc() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.erfc#torch.erfc\" title=\"torch.erfc\"><code>torch.erfc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erfc_\">\n<code>erfc_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.erfc\" title=\"torch.Tensor.erfc\"><code>erfc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erfinv\">\n<code>erfinv() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.erfinv#torch.erfinv\" title=\"torch.erfinv\"><code>torch.erfinv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.erfinv_\">\n<code>erfinv_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.erfinv\" title=\"torch.Tensor.erfinv\"><code>erfinv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.exp\">\n<code>exp() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.exp#torch.exp\" title=\"torch.exp\"><code>torch.exp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.exp_\">\n<code>exp_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.exp\" title=\"torch.Tensor.exp\"><code>exp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.expm1\">\n<code>expm1() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.expm1#torch.expm1\" title=\"torch.expm1\"><code>torch.expm1()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.expm1_\">\n<code>expm1_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.expm1\" title=\"torch.Tensor.expm1\"><code>expm1()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.expand\">\n<code>expand(*sizes) → Tensor</code> </dt> <dd>\n<p>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</p> <p>Passing -1 as the size for a dimension means not changing the size of that dimension.</p> <p>Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1.</p> <p>Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the <code>stride</code> to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>*sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired expanded size</p> </dd> </dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.</p> </div> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1], [2], [3]])\n&gt;&gt;&gt; x.size()\ntorch.Size([3, 1])\n&gt;&gt;&gt; x.expand(3, 4)\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n&gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.expand_as\">\n<code>expand_as(other) → Tensor</code> </dt> <dd>\n<p>Expand this tensor to the same size as <code>other</code>. <code>self.expand_as(other)</code> is equivalent to <code>self.expand(other.size())</code>.</p> <p>Please see <a class=\"reference internal\" href=\"#torch.Tensor.expand\" title=\"torch.Tensor.expand\"><code>expand()</code></a> for more information about <code>expand</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>other</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>) – The result tensor has the same size as <code>other</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.exponential_\">\n<code>exponential_(lambd=1, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>λ</mi><msup><mi>e</mi><mrow><mo>−</mo><mi>λ</mi><mi>x</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">f(x) = \\lambda e^{-\\lambda x}</annotation></semantics></math></span></span></span> </div>\n</dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fix\">\n<code>fix() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.fix#torch.fix\" title=\"torch.fix\"><code>torch.fix()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fix_\">\n<code>fix_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.fix\" title=\"torch.Tensor.fix\"><code>fix()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fill_\">\n<code>fill_(value) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with the specified value.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.flatten\">\n<code>flatten(input, start_dim=0, end_dim=-1) → Tensor</code> </dt> <dd>\n<p>see <a class=\"reference internal\" href=\"generated/torch.flatten#torch.flatten\" title=\"torch.flatten\"><code>torch.flatten()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.flip\">\n<code>flip(dims) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.flip#torch.flip\" title=\"torch.flip\"><code>torch.flip()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fliplr\">\n<code>fliplr() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.fliplr#torch.fliplr\" title=\"torch.fliplr\"><code>torch.fliplr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.flipud\">\n<code>flipud() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.flipud#torch.flipud\" title=\"torch.flipud\"><code>torch.flipud()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.float\">\n<code>float(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.float_power\">\n<code>float_power(exponent) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.float_power#torch.float_power\" title=\"torch.float_power\"><code>torch.float_power()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.float_power_\">\n<code>float_power_(exponent) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.float_power\" title=\"torch.Tensor.float_power\"><code>float_power()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.floor\">\n<code>floor() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.floor#torch.floor\" title=\"torch.floor\"><code>torch.floor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.floor_\">\n<code>floor_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.floor\" title=\"torch.Tensor.floor\"><code>floor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.floor_divide\">\n<code>floor_divide(value) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.floor_divide#torch.floor_divide\" title=\"torch.floor_divide\"><code>torch.floor_divide()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.floor_divide_\">\n<code>floor_divide_(value) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.floor_divide\" title=\"torch.Tensor.floor_divide\"><code>floor_divide()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fmod\">\n<code>fmod(divisor) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.fmod#torch.fmod\" title=\"torch.fmod\"><code>torch.fmod()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.fmod_\">\n<code>fmod_(divisor) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.fmod\" title=\"torch.Tensor.fmod\"><code>fmod()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.frac\">\n<code>frac() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.frac#torch.frac\" title=\"torch.frac\"><code>torch.frac()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.frac_\">\n<code>frac_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.frac\" title=\"torch.Tensor.frac\"><code>frac()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.gather\">\n<code>gather(dim, index) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.gather#torch.gather\" title=\"torch.gather\"><code>torch.gather()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.gcd\">\n<code>gcd(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.gcd#torch.gcd\" title=\"torch.gcd\"><code>torch.gcd()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.gcd_\">\n<code>gcd_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.gcd\" title=\"torch.Tensor.gcd\"><code>gcd()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ge\">\n<code>ge(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ge#torch.ge\" title=\"torch.ge\"><code>torch.ge()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ge_\">\n<code>ge_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.ge\" title=\"torch.Tensor.ge\"><code>ge()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.greater_equal\">\n<code>greater_equal(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.greater_equal#torch.greater_equal\" title=\"torch.greater_equal\"><code>torch.greater_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.greater_equal_\">\n<code>greater_equal_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.greater_equal\" title=\"torch.Tensor.greater_equal\"><code>greater_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.geometric_\">\n<code>geometric_(p, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo>=</mo><mi>k</mi><mo stretchy=\"false\">)</mo><mo>=</mo><msup><mi>p</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">f(X=k) = p^{k - 1} (1 - p)</annotation></semantics></math></span></span></span> </div>\n</dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.geqrf\">\n<code>geqrf() -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.geqrf#torch.geqrf\" title=\"torch.geqrf\"><code>torch.geqrf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ger\">\n<code>ger(vec2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ger#torch.ger\" title=\"torch.ger\"><code>torch.ger()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.get_device\">\n<code>get_device() -&gt; Device ordinal (Integer)</code> </dt> <dd>\n<p>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn(3, 4, 5, device='cuda:0')\n&gt;&gt;&gt; x.get_device()\n0\n&gt;&gt;&gt; x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.gt\">\n<code>gt(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.gt#torch.gt\" title=\"torch.gt\"><code>torch.gt()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.gt_\">\n<code>gt_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.gt\" title=\"torch.Tensor.gt\"><code>gt()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.greater\">\n<code>greater(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.greater#torch.greater\" title=\"torch.greater\"><code>torch.greater()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.greater_\">\n<code>greater_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.greater\" title=\"torch.Tensor.greater\"><code>greater()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.half\">\n<code>half(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.hardshrink\">\n<code>hardshrink(lambd=0.5) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"nn.functional#torch.nn.functional.hardshrink\" title=\"torch.nn.functional.hardshrink\"><code>torch.nn.functional.hardshrink()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.heaviside\">\n<code>heaviside(values) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.heaviside#torch.heaviside\" title=\"torch.heaviside\"><code>torch.heaviside()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.histc\">\n<code>histc(bins=100, min=0, max=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.histc#torch.histc\" title=\"torch.histc\"><code>torch.histc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.hypot\">\n<code>hypot(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.hypot#torch.hypot\" title=\"torch.hypot\"><code>torch.hypot()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.hypot_\">\n<code>hypot_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.hypot\" title=\"torch.Tensor.hypot\"><code>hypot()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.i0\">\n<code>i0() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.i0#torch.i0\" title=\"torch.i0\"><code>torch.i0()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.i0_\">\n<code>i0_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.i0\" title=\"torch.Tensor.i0\"><code>i0()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.igamma\">\n<code>igamma(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.igamma#torch.igamma\" title=\"torch.igamma\"><code>torch.igamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.igamma_\">\n<code>igamma_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.igamma\" title=\"torch.Tensor.igamma\"><code>igamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.igammac\">\n<code>igammac(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.igammac#torch.igammac\" title=\"torch.igammac\"><code>torch.igammac()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.igammac_\">\n<code>igammac_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.igammac\" title=\"torch.Tensor.igammac\"><code>igammac()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_add_\">\n<code>index_add_(dim, index, tensor) → Tensor</code> </dt> <dd>\n<p>Accumulate the elements of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>. For example, if <code>dim == 0</code> and <code>index[i] == j</code>, then the <code>i</code>th row of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> is added to the <code>j</code>th row of <code>self</code>.</p> <p>The <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim</code></a>th dimension of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> must have the same size as the length of <code>index</code> (which must be a vector), and all other dimensions must match <code>self</code>, or an error will be raised.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may behave nondeterministically when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – dimension along which to index</li> <li>\n<strong>index</strong> (<em>IntTensor</em><em> or </em><em>LongTensor</em>) – indices of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> to select from</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the tensor containing values to add</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.ones(5, 3)\n&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 4, 2])\n&gt;&gt;&gt; x.index_add_(0, index, t)\ntensor([[  2.,   3.,   4.],\n        [  1.,   1.,   1.],\n        [  8.,   9.,  10.],\n        [  1.,   1.,   1.],\n        [  5.,   6.,   7.]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_add\">\n<code>index_add(tensor1, dim, index, tensor2) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>torch.Tensor.index_add_()</code></a>. <code>tensor1</code> corresponds to <code>self</code> in <a class=\"reference internal\" href=\"#torch.Tensor.index_add_\" title=\"torch.Tensor.index_add_\"><code>torch.Tensor.index_add_()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_copy_\">\n<code>index_copy_(dim, index, tensor) → Tensor</code> </dt> <dd>\n<p>Copies the elements of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>. For example, if <code>dim == 0</code> and <code>index[i] == j</code>, then the <code>i</code>th row of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> is copied to the <code>j</code>th row of <code>self</code>.</p> <p>The <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim</code></a>th dimension of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> must have the same size as the length of <code>index</code> (which must be a vector), and all other dimensions must match <code>self</code>, or an error will be raised.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If <code>index</code> contains duplicate entries, multiple elements from <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> will be copied to the same index of <code>self</code>. The result is nondeterministic since it depends on which copy occurs last.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – dimension along which to index</li> <li>\n<strong>index</strong> (<em>LongTensor</em>) – indices of <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> to select from</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the tensor containing values to copy</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.zeros(5, 3)\n&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 4, 2])\n&gt;&gt;&gt; x.index_copy_(0, index, t)\ntensor([[ 1.,  2.,  3.],\n        [ 0.,  0.,  0.],\n        [ 7.,  8.,  9.],\n        [ 0.,  0.,  0.],\n        [ 4.,  5.,  6.]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_copy\">\n<code>index_copy(tensor1, dim, index, tensor2) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.index_copy_\" title=\"torch.Tensor.index_copy_\"><code>torch.Tensor.index_copy_()</code></a>. <code>tensor1</code> corresponds to <code>self</code> in <a class=\"reference internal\" href=\"#torch.Tensor.index_copy_\" title=\"torch.Tensor.index_copy_\"><code>torch.Tensor.index_copy_()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_fill_\">\n<code>index_fill_(dim, index, val) → Tensor</code> </dt> <dd>\n<p>Fills the elements of the <code>self</code> tensor with value <code>val</code> by selecting the indices in the order given in <code>index</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – dimension along which to index</li> <li>\n<strong>index</strong> (<em>LongTensor</em>) – indices of <code>self</code> tensor to fill in</li> <li>\n<strong>val</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – the value to fill with</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n&gt;&gt;&gt; index = torch.tensor([0, 2])\n&gt;&gt;&gt; x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_fill\">\n<code>index_fill(tensor1, dim, index, value) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.index_fill_\" title=\"torch.Tensor.index_fill_\"><code>torch.Tensor.index_fill_()</code></a>. <code>tensor1</code> corresponds to <code>self</code> in <a class=\"reference internal\" href=\"#torch.Tensor.index_fill_\" title=\"torch.Tensor.index_fill_\"><code>torch.Tensor.index_fill_()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_put_\">\n<code>index_put_(indices, values, accumulate=False) → Tensor</code> </dt> <dd>\n<p>Puts values from the tensor <a class=\"reference internal\" href=\"sparse#torch.Tensor.values\" title=\"torch.Tensor.values\"><code>values</code></a> into the tensor <code>self</code> using the indices specified in <a class=\"reference internal\" href=\"sparse#torch.Tensor.indices\" title=\"torch.Tensor.indices\"><code>indices</code></a> (which is a tuple of Tensors). The expression <code>tensor.index_put_(indices, values)</code> is equivalent to <code>tensor[indices] = values</code>. Returns <code>self</code>.</p> <p>If <code>accumulate</code> is <code>True</code>, the elements in <a class=\"reference internal\" href=\"sparse#torch.Tensor.values\" title=\"torch.Tensor.values\"><code>values</code></a> are added to <code>self</code>. If accumulate is <code>False</code>, the behavior is undefined if indices contain duplicate elements.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>indices</strong> (<em>tuple of LongTensor</em>) – tensors used to index into <code>self</code>.</li> <li>\n<strong>values</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – tensor of same dtype as <code>self</code>.</li> <li>\n<strong>accumulate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – whether to accumulate into self</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_put\">\n<code>index_put(tensor1, indices, values, accumulate=False) → Tensor</code> </dt> <dd>\n<p>Out-place version of <a class=\"reference internal\" href=\"#torch.Tensor.index_put_\" title=\"torch.Tensor.index_put_\"><code>index_put_()</code></a>. <code>tensor1</code> corresponds to <code>self</code> in <a class=\"reference internal\" href=\"#torch.Tensor.index_put_\" title=\"torch.Tensor.index_put_\"><code>torch.Tensor.index_put_()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.index_select\">\n<code>index_select(dim, index) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.index_select#torch.index_select\" title=\"torch.index_select\"><code>torch.index_select()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>indices() → Tensor</code> </dt> <dd>\n<p>Return the indices tensor of a <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">sparse COO tensor</span></a>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Throws an error if <code>self</code> is not a sparse COO tensor.</p> </div> <p>See also <a class=\"reference internal\" href=\"sparse#torch.Tensor.values\" title=\"torch.Tensor.values\"><code>Tensor.values()</code></a>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This method can only be called on a coalesced sparse tensor. See <a class=\"reference internal\" href=\"sparse#torch.Tensor.coalesce\" title=\"torch.Tensor.coalesce\"><code>Tensor.coalesce()</code></a> for details.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.inner\">\n<code>inner(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.inner#torch.inner\" title=\"torch.inner\"><code>torch.inner()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.int\">\n<code>int(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.int_repr\">\n<code>int_repr() → Tensor</code> </dt> <dd>\n<p>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.inverse\">\n<code>inverse() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.inverse#torch.inverse\" title=\"torch.inverse\"><code>torch.inverse()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isclose\">\n<code>isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isclose#torch.isclose\" title=\"torch.isclose\"><code>torch.isclose()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isfinite\">\n<code>isfinite() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isfinite#torch.isfinite\" title=\"torch.isfinite\"><code>torch.isfinite()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isinf\">\n<code>isinf() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isinf#torch.isinf\" title=\"torch.isinf\"><code>torch.isinf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isposinf\">\n<code>isposinf() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isposinf#torch.isposinf\" title=\"torch.isposinf\"><code>torch.isposinf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isneginf\">\n<code>isneginf() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isneginf#torch.isneginf\" title=\"torch.isneginf\"><code>torch.isneginf()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isnan\">\n<code>isnan() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isnan#torch.isnan\" title=\"torch.isnan\"><code>torch.isnan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_contiguous\">\n<code>is_contiguous(memory_format=torch.contiguous_format) → bool</code> </dt> <dd>\n<p>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – Specifies memory allocation order. Default: <code>torch.contiguous_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_complex\">\n<code>is_complex() → bool</code> </dt> <dd>\n<p>Returns True if the data type of <code>self</code> is a complex data type.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_floating_point\">\n<code>is_floating_point() → bool</code> </dt> <dd>\n<p>Returns True if the data type of <code>self</code> is a floating point data type.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt>\n<code>is_leaf</code> </dt> <dd>\n<p>All Tensors that have <a class=\"reference internal\" href=\"autograd#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p> <p>For Tensors that have <a class=\"reference internal\" href=\"autograd#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p> <p>Only leaf Tensors will have their <a class=\"reference internal\" href=\"autograd#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> populated during a call to <a class=\"reference internal\" href=\"autograd#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a>. To get <a class=\"reference internal\" href=\"autograd#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> populated for non-leaf Tensors, you can use <a class=\"reference internal\" href=\"autograd#torch.Tensor.retain_grad\" title=\"torch.Tensor.retain_grad\"><code>retain_grad()</code></a>.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)\n&gt;&gt;&gt; a.is_leaf\nTrue\n&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()\n&gt;&gt;&gt; b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2\n&gt;&gt;&gt; c.is_leaf\nFalse\n# c was created by the addition operation\n&gt;&gt;&gt; d = torch.rand(10).cuda()\n&gt;&gt;&gt; d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()\n&gt;&gt;&gt; e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt; f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_pinned\">\n<code>is_pinned()</code> </dt> <dd>\n<p>Returns true if this tensor resides in pinned memory.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_set_to\">\n<code>is_set_to(tensor) → bool</code> </dt> <dd>\n<p>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_shared\">\n<code>is_shared()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.is_shared\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if tensor is in shared memory.</p> <p>This is always <code>True</code> for CUDA tensors.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.is_signed\">\n<code>is_signed() → bool</code> </dt> <dd>\n<p>Returns True if the data type of <code>self</code> is a signed data type.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt>\n<code>is_sparse</code> </dt> <dd>\n<p>Is <code>True</code> if the Tensor uses sparse storage layout, <code>False</code> otherwise.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.istft\">\n<code>istft(n_fft, hop_length=None, win_length=None, window=None, center=True, normalized=False, onesided=None, length=None, return_complex=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.istft\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.istft#torch.istft\" title=\"torch.istft\"><code>torch.istft()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.isreal\">\n<code>isreal() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.isreal#torch.isreal\" title=\"torch.isreal\"><code>torch.isreal()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.item\">\n<code>item() → number</code> </dt> <dd>\n<p>Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see <a class=\"reference internal\" href=\"#torch.Tensor.tolist\" title=\"torch.Tensor.tolist\"><code>tolist()</code></a>.</p> <p>This operation is not differentiable.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1.0])\n&gt;&gt;&gt; x.item()\n1.0\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.kthvalue\">\n<code>kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.kthvalue#torch.kthvalue\" title=\"torch.kthvalue\"><code>torch.kthvalue()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lcm\">\n<code>lcm(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lcm#torch.lcm\" title=\"torch.lcm\"><code>torch.lcm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lcm_\">\n<code>lcm_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.lcm\" title=\"torch.Tensor.lcm\"><code>lcm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ldexp\">\n<code>ldexp(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ldexp#torch.ldexp\" title=\"torch.ldexp\"><code>torch.ldexp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ldexp_\">\n<code>ldexp_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.ldexp\" title=\"torch.Tensor.ldexp\"><code>ldexp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.le\">\n<code>le(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.le#torch.le\" title=\"torch.le\"><code>torch.le()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.le_\">\n<code>le_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.le\" title=\"torch.Tensor.le\"><code>le()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.less_equal\">\n<code>less_equal(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.less_equal#torch.less_equal\" title=\"torch.less_equal\"><code>torch.less_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.less_equal_\">\n<code>less_equal_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.less_equal\" title=\"torch.Tensor.less_equal\"><code>less_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lerp\">\n<code>lerp(end, weight) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lerp#torch.lerp\" title=\"torch.lerp\"><code>torch.lerp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lerp_\">\n<code>lerp_(end, weight) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.lerp\" title=\"torch.Tensor.lerp\"><code>lerp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lgamma\">\n<code>lgamma() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lgamma#torch.lgamma\" title=\"torch.lgamma\"><code>torch.lgamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lgamma_\">\n<code>lgamma_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.lgamma\" title=\"torch.Tensor.lgamma\"><code>lgamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log\">\n<code>log() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.log#torch.log\" title=\"torch.log\"><code>torch.log()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log_\">\n<code>log_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.log\" title=\"torch.Tensor.log\"><code>log()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logdet\">\n<code>logdet() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logdet#torch.logdet\" title=\"torch.logdet\"><code>torch.logdet()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log10\">\n<code>log10() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.log10#torch.log10\" title=\"torch.log10\"><code>torch.log10()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log10_\">\n<code>log10_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.log10\" title=\"torch.Tensor.log10\"><code>log10()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log1p\">\n<code>log1p() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.log1p#torch.log1p\" title=\"torch.log1p\"><code>torch.log1p()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log1p_\">\n<code>log1p_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.log1p\" title=\"torch.Tensor.log1p\"><code>log1p()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log2\">\n<code>log2() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.log2#torch.log2\" title=\"torch.log2\"><code>torch.log2()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log2_\">\n<code>log2_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.log2\" title=\"torch.Tensor.log2\"><code>log2()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.log_normal_\">\n<code>log_normal_(mean=1, std=2, *, generator=None)</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>μ</mi></mrow><annotation encoding=\"application/x-tex\">\\mu</annotation></semantics></math></span></span> </span> and standard deviation <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>σ</mi></mrow><annotation encoding=\"application/x-tex\">\\sigma</annotation></semantics></math></span></span> </span>. Note that <a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>mean</code></a> and <a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>std</code></a> are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mi>x</mi><mi>σ</mi><msqrt><mrow><mn>2</mn><mi>π</mi></mrow></msqrt></mrow></mfrac><msup><mi>e</mi><mrow><mo>−</mo><mfrac><mrow><mo stretchy=\"false\">(</mo><mi>ln</mi><mo>⁡</mo><mi>x</mi><mo>−</mo><mi>μ</mi><msup><mo stretchy=\"false\">)</mo><mn>2</mn></msup></mrow><mrow><mn>2</mn><msup><mi>σ</mi><mn>2</mn></msup></mrow></mfrac></mrow></msup></mrow><annotation encoding=\"application/x-tex\">f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}</annotation></semantics></math></span></span></span> </div>\n</dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logaddexp\">\n<code>logaddexp(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logaddexp#torch.logaddexp\" title=\"torch.logaddexp\"><code>torch.logaddexp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logaddexp2\">\n<code>logaddexp2(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logaddexp2#torch.logaddexp2\" title=\"torch.logaddexp2\"><code>torch.logaddexp2()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logsumexp\">\n<code>logsumexp(dim, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logsumexp#torch.logsumexp\" title=\"torch.logsumexp\"><code>torch.logsumexp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_and\">\n<code>logical_and() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logical_and#torch.logical_and\" title=\"torch.logical_and\"><code>torch.logical_and()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_and_\">\n<code>logical_and_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.logical_and\" title=\"torch.Tensor.logical_and\"><code>logical_and()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_not\">\n<code>logical_not() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logical_not#torch.logical_not\" title=\"torch.logical_not\"><code>torch.logical_not()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_not_\">\n<code>logical_not_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.logical_not\" title=\"torch.Tensor.logical_not\"><code>logical_not()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_or\">\n<code>logical_or() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logical_or#torch.logical_or\" title=\"torch.logical_or\"><code>torch.logical_or()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_or_\">\n<code>logical_or_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.logical_or\" title=\"torch.Tensor.logical_or\"><code>logical_or()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_xor\">\n<code>logical_xor() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logical_xor#torch.logical_xor\" title=\"torch.logical_xor\"><code>torch.logical_xor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logical_xor_\">\n<code>logical_xor_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.logical_xor\" title=\"torch.Tensor.logical_xor\"><code>logical_xor()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logit\">\n<code>logit() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.logit#torch.logit\" title=\"torch.logit\"><code>torch.logit()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.logit_\">\n<code>logit_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.logit\" title=\"torch.Tensor.logit\"><code>logit()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.long\">\n<code>long(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lstsq\">\n<code>lstsq(A) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lstsq#torch.lstsq\" title=\"torch.lstsq\"><code>torch.lstsq()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lt\">\n<code>lt(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lt#torch.lt\" title=\"torch.lt\"><code>torch.lt()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lt_\">\n<code>lt_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.lt\" title=\"torch.Tensor.lt\"><code>lt()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.less\">\n<code>less()</code> </dt> <dd>\n<p>lt(other) -&gt; Tensor</p> <p>See <a class=\"reference internal\" href=\"generated/torch.less#torch.less\" title=\"torch.less\"><code>torch.less()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.less_\">\n<code>less_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.less\" title=\"torch.Tensor.less\"><code>less()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lu\">\n<code>lu(pivot=True, get_infos=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.lu\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lu#torch.lu\" title=\"torch.lu\"><code>torch.lu()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.lu_solve\">\n<code>lu_solve(LU_data, LU_pivots) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.lu_solve#torch.lu_solve\" title=\"torch.lu_solve\"><code>torch.lu_solve()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.as_subclass\">\n<code>as_subclass(cls) → Tensor</code> </dt> <dd>\n<p>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>. Changes in the output mirror changes in <code>self</code>, and the output stays attached to the autograd graph. <code>cls</code> must be a subclass of <code>Tensor</code>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.map_\">\n<code>map_(tensor, callable)</code> </dt> <dd>\n<p>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> and stores the results in <code>self</code> tensor. <code>self</code> tensor and the given <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> must be <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">broadcastable</span></a>.</p> <p>The <code>callable</code> should have the signature:</p> <pre data-language=\"python\">def callable(a, b) -&gt; number\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.masked_scatter_\">\n<code>masked_scatter_(mask, source)</code> </dt> <dd>\n<p>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True. The shape of <code>mask</code> must be <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">broadcastable</span></a> with the shape of the underlying tensor. The <code>source</code> should have at least as many elements as the number of ones in <code>mask</code></p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>mask</strong> (<em>BoolTensor</em>) – the boolean mask</li> <li>\n<strong>source</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the tensor to copy from</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The <code>mask</code> operates on the <code>self</code> tensor, not on the given <code>source</code> tensor.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.masked_scatter\">\n<code>masked_scatter(mask, tensor) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.masked_scatter_\" title=\"torch.Tensor.masked_scatter_\"><code>torch.Tensor.masked_scatter_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.masked_fill_\">\n<code>masked_fill_(mask, value)</code> </dt> <dd>\n<p>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True. The shape of <code>mask</code> must be <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/broadcasting.html#broadcasting-semantics\"><span class=\"std std-ref\">broadcastable</span></a> with the shape of the underlying tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>mask</strong> (<em>BoolTensor</em>) – the boolean mask</li> <li>\n<strong>value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – the value to fill in with</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.masked_fill\">\n<code>masked_fill(mask, value) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.masked_fill_\" title=\"torch.Tensor.masked_fill_\"><code>torch.Tensor.masked_fill_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.masked_select\">\n<code>masked_select(mask) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.masked_select#torch.masked_select\" title=\"torch.masked_select\"><code>torch.masked_select()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.matmul\">\n<code>matmul(tensor2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.matmul#torch.matmul\" title=\"torch.matmul\"><code>torch.matmul()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.matrix_power\">\n<code>matrix_power(n) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.matrix_power#torch.matrix_power\" title=\"torch.matrix_power\"><code>torch.matrix_power()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.matrix_exp\">\n<code>matrix_exp() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.matrix_exp#torch.matrix_exp\" title=\"torch.matrix_exp\"><code>torch.matrix_exp()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.max\">\n<code>max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.max#torch.max\" title=\"torch.max\"><code>torch.max()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.maximum\">\n<code>maximum(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.maximum#torch.maximum\" title=\"torch.maximum\"><code>torch.maximum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mean\">\n<code>mean(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>torch.mean()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.median\">\n<code>median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.median#torch.median\" title=\"torch.median\"><code>torch.median()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nanmedian\">\n<code>nanmedian(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nanmedian#torch.nanmedian\" title=\"torch.nanmedian\"><code>torch.nanmedian()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.min\">\n<code>min(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.min#torch.min\" title=\"torch.min\"><code>torch.min()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.minimum\">\n<code>minimum(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.minimum#torch.minimum\" title=\"torch.minimum\"><code>torch.minimum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mm\">\n<code>mm(mat2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mm#torch.mm\" title=\"torch.mm\"><code>torch.mm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>smm(mat) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"sparse#torch.smm\" title=\"torch.smm\"><code>torch.smm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mode\">\n<code>mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mode#torch.mode\" title=\"torch.mode\"><code>torch.mode()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.movedim\">\n<code>movedim(source, destination) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.movedim#torch.movedim\" title=\"torch.movedim\"><code>torch.movedim()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.moveaxis\">\n<code>moveaxis(source, destination) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.moveaxis#torch.moveaxis\" title=\"torch.moveaxis\"><code>torch.moveaxis()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.msort\">\n<code>msort() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.msort#torch.msort\" title=\"torch.msort\"><code>torch.msort()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mul\">\n<code>mul(value) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mul#torch.mul\" title=\"torch.mul\"><code>torch.mul()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mul_\">\n<code>mul_(value) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.mul\" title=\"torch.Tensor.mul\"><code>mul()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.multiply\">\n<code>multiply(value) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.multiply#torch.multiply\" title=\"torch.multiply\"><code>torch.multiply()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.multiply_\">\n<code>multiply_(value) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.multiply\" title=\"torch.Tensor.multiply\"><code>multiply()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.multinomial\">\n<code>multinomial(num_samples, replacement=False, *, generator=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.multinomial#torch.multinomial\" title=\"torch.multinomial\"><code>torch.multinomial()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mv\">\n<code>mv(vec) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mv#torch.mv\" title=\"torch.mv\"><code>torch.mv()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mvlgamma\">\n<code>mvlgamma(p) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.mvlgamma#torch.mvlgamma\" title=\"torch.mvlgamma\"><code>torch.mvlgamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.mvlgamma_\">\n<code>mvlgamma_(p) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.mvlgamma\" title=\"torch.Tensor.mvlgamma\"><code>mvlgamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nansum\">\n<code>nansum(dim=None, keepdim=False, dtype=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nansum#torch.nansum\" title=\"torch.nansum\"><code>torch.nansum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.narrow\">\n<code>narrow(dimension, start, length) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.narrow#torch.narrow\" title=\"torch.narrow\"><code>torch.narrow()</code></a></p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n&gt;&gt;&gt; x.narrow(0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n&gt;&gt;&gt; x.narrow(1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.narrow_copy\">\n<code>narrow_copy(dimension, start, length) → Tensor</code> </dt> <dd>\n<p>Same as <a class=\"reference internal\" href=\"#torch.Tensor.narrow\" title=\"torch.Tensor.narrow\"><code>Tensor.narrow()</code></a> except returning a copy rather than shared storage. This is primarily for sparse tensors, which do not have a shared-storage narrow method. Calling <code>`narrow_copy</code> with <code>`dimemsion &gt; self.sparse_dim()`</code> will return a copy with the relevant dense dimension narrowed, and <code>`self.shape`</code> updated accordingly.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ndimension\">\n<code>ndimension() → int</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nan_to_num\">\n<code>nan_to_num(nan=0.0, posinf=None, neginf=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nan_to_num#torch.nan_to_num\" title=\"torch.nan_to_num\"><code>torch.nan_to_num()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nan_to_num_\">\n<code>nan_to_num_(nan=0.0, posinf=None, neginf=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.nan_to_num\" title=\"torch.Tensor.nan_to_num\"><code>nan_to_num()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ne\">\n<code>ne(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ne#torch.ne\" title=\"torch.ne\"><code>torch.ne()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ne_\">\n<code>ne_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.ne\" title=\"torch.Tensor.ne\"><code>ne()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.not_equal\">\n<code>not_equal(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.not_equal#torch.not_equal\" title=\"torch.not_equal\"><code>torch.not_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.not_equal_\">\n<code>not_equal_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.not_equal\" title=\"torch.Tensor.not_equal\"><code>not_equal()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.neg\">\n<code>neg() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.neg#torch.neg\" title=\"torch.neg\"><code>torch.neg()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.neg_\">\n<code>neg_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.neg\" title=\"torch.Tensor.neg\"><code>neg()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.negative\">\n<code>negative() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.negative#torch.negative\" title=\"torch.negative\"><code>torch.negative()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.negative_\">\n<code>negative_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.negative\" title=\"torch.Tensor.negative\"><code>negative()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nelement\">\n<code>nelement() → int</code> </dt> <dd>\n<p>Alias for <a class=\"reference internal\" href=\"#torch.Tensor.numel\" title=\"torch.Tensor.numel\"><code>numel()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nextafter\">\n<code>nextafter(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nextafter#torch.nextafter\" title=\"torch.nextafter\"><code>torch.nextafter()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nextafter_\">\n<code>nextafter_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.nextafter\" title=\"torch.Tensor.nextafter\"><code>nextafter()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nonzero\">\n<code>nonzero() → LongTensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nonzero#torch.nonzero\" title=\"torch.nonzero\"><code>torch.nonzero()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.norm\">\n<code>norm(p='fro', dim=None, keepdim=False, dtype=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.norm\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.norm#torch.norm\" title=\"torch.norm\"><code>torch.norm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.normal_\">\n<code>normal_(mean=0, std=1, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a class=\"reference internal\" href=\"generated/torch.mean#torch.mean\" title=\"torch.mean\"><code>mean</code></a> and <a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>std</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.numel\">\n<code>numel() → int</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.numel#torch.numel\" title=\"torch.numel\"><code>torch.numel()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.numpy\">\n<code>numpy() → numpy.ndarray</code> </dt> <dd>\n<p>Returns <code>self</code> tensor as a NumPy <code>ndarray</code>. This tensor and the returned <code>ndarray</code> share the same underlying storage. Changes to <code>self</code> tensor will be reflected in the <code>ndarray</code> and vice versa.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.orgqr\">\n<code>orgqr(input2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.orgqr#torch.orgqr\" title=\"torch.orgqr\"><code>torch.orgqr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ormqr\">\n<code>ormqr(input2, input3, left=True, transpose=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.ormqr#torch.ormqr\" title=\"torch.ormqr\"><code>torch.ormqr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.outer\">\n<code>outer(vec2) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.outer#torch.outer\" title=\"torch.outer\"><code>torch.outer()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.permute\">\n<code>permute(*dims) → Tensor</code> </dt> <dd>\n<p>Returns a view of the original tensor with its dimensions permuted.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>*dims</strong> (<em>int...</em>) – The desired ordering of dimensions</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn(2, 3, 5)\n&gt;&gt;&gt; x.size()\ntorch.Size([2, 3, 5])\n&gt;&gt;&gt; x.permute(2, 0, 1).size()\ntorch.Size([5, 2, 3])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.pin_memory\">\n<code>pin_memory() → Tensor</code> </dt> <dd>\n<p>Copies the tensor to pinned memory, if it’s not already pinned.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.pinverse\">\n<code>pinverse() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.pinverse#torch.pinverse\" title=\"torch.pinverse\"><code>torch.pinverse()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.polygamma\">\n<code>polygamma(n) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.polygamma#torch.polygamma\" title=\"torch.polygamma\"><code>torch.polygamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.polygamma_\">\n<code>polygamma_(n) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.polygamma\" title=\"torch.Tensor.polygamma\"><code>polygamma()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.pow\">\n<code>pow(exponent) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.pow#torch.pow\" title=\"torch.pow\"><code>torch.pow()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.pow_\">\n<code>pow_(exponent) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.pow\" title=\"torch.Tensor.pow\"><code>pow()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.prod\">\n<code>prod(dim=None, keepdim=False, dtype=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.prod#torch.prod\" title=\"torch.prod\"><code>torch.prod()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.put_\">\n<code>put_(indices, tensor, accumulate=False) → Tensor</code> </dt> <dd>\n<p>Copies the elements from <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> into the positions specified by indices. For the purpose of indexing, the <code>self</code> tensor is treated as if it were a 1-D tensor.</p> <p>If <code>accumulate</code> is <code>True</code>, the elements in <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a> are added to <code>self</code>. If accumulate is <code>False</code>, the behavior is undefined if indices contain duplicate elements.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>indices</strong> (<em>LongTensor</em>) – the indices into self</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the tensor containing values to copy from</li> <li>\n<strong>accumulate</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – whether to accumulate into self</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n&gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\ntensor([[  4,   9,   5],\n        [ 10,   7,   8]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.qr\">\n<code>qr(some=True) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.qr#torch.qr\" title=\"torch.qr\"><code>torch.qr()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.qscheme\">\n<code>qscheme() → torch.qscheme</code> </dt> <dd>\n<p>Returns the quantization scheme of a given QTensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.quantile\">\n<code>quantile(q, dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.quantile#torch.quantile\" title=\"torch.quantile\"><code>torch.quantile()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.nanquantile\">\n<code>nanquantile(q, dim=None, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.nanquantile#torch.nanquantile\" title=\"torch.nanquantile\"><code>torch.nanquantile()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.q_scale\">\n<code>q_scale() → float</code> </dt> <dd>\n<p>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.q_zero_point\">\n<code>q_zero_point() → int</code> </dt> <dd>\n<p>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.q_per_channel_scales\">\n<code>q_per_channel_scales() → Tensor</code> </dt> <dd>\n<p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.q_per_channel_zero_points\">\n<code>q_per_channel_zero_points() → Tensor</code> </dt> <dd>\n<p>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.q_per_channel_axis\">\n<code>q_per_channel_axis() → int</code> </dt> <dd>\n<p>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.rad2deg\">\n<code>rad2deg() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.rad2deg#torch.rad2deg\" title=\"torch.rad2deg\"><code>torch.rad2deg()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.random_\">\n<code>random_(from=0, to=None, *, generator=None) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>. If not specified, the values are usually only bounded by <code>self</code> tensor’s data type. However, for floating point types, if unspecified, range will be <code>[0, 2^mantissa]</code> to ensure that every value is representable. For example, <code>torch.tensor(1, dtype=torch.double).random_()</code> will be uniform in <code>[0, 2^53]</code>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.ravel\">\n<code>ravel(input) → Tensor</code> </dt> <dd>\n<p>see <a class=\"reference internal\" href=\"generated/torch.ravel#torch.ravel\" title=\"torch.ravel\"><code>torch.ravel()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.reciprocal\">\n<code>reciprocal() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.reciprocal#torch.reciprocal\" title=\"torch.reciprocal\"><code>torch.reciprocal()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.reciprocal_\">\n<code>reciprocal_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.reciprocal\" title=\"torch.Tensor.reciprocal\"><code>reciprocal()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.record_stream\">\n<code>record_stream(stream)</code> </dt> <dd>\n<p>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>register_hook(hook)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.register_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Registers a backward hook.</p> <p>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</p> <pre data-language=\"python\">hook(grad) -&gt; Tensor or None\n</pre> <p>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of <a class=\"reference internal\" href=\"autograd#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a>.</p> <p>This function returns a handle with a method <code>handle.remove()</code> that removes the hook from the module.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)\n&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))\n&gt;&gt;&gt; v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n&gt;&gt;&gt; h.remove()  # removes the hook\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.remainder\">\n<code>remainder(divisor) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.remainder#torch.remainder\" title=\"torch.remainder\"><code>torch.remainder()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.remainder_\">\n<code>remainder_(divisor) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.remainder\" title=\"torch.Tensor.remainder\"><code>remainder()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.renorm\">\n<code>renorm(p, dim, maxnorm) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.renorm#torch.renorm\" title=\"torch.renorm\"><code>torch.renorm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.renorm_\">\n<code>renorm_(p, dim, maxnorm) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.renorm\" title=\"torch.Tensor.renorm\"><code>renorm()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.repeat\">\n<code>repeat(*sizes) → Tensor</code> </dt> <dd>\n<p>Repeats this tensor along the specified dimensions.</p> <p>Unlike <a class=\"reference internal\" href=\"#torch.Tensor.expand\" title=\"torch.Tensor.expand\"><code>expand()</code></a>, this function copies the tensor’s data.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.Tensor.repeat\" title=\"torch.Tensor.repeat\"><code>repeat()</code></a> behaves differently from <a class=\"reference external\" href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\">numpy.repeat</a>, but is more similar to <a class=\"reference external\" href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html\">numpy.tile</a>. For the operator similar to <code>numpy.repeat</code>, see <a class=\"reference internal\" href=\"generated/torch.repeat_interleave#torch.repeat_interleave\" title=\"torch.repeat_interleave\"><code>torch.repeat_interleave()</code></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – The number of times to repeat this tensor along each dimension</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1, 2, 3])\n&gt;&gt;&gt; x.repeat(4, 2)\ntensor([[ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3]])\n&gt;&gt;&gt; x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.repeat_interleave\">\n<code>repeat_interleave(repeats, dim=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.repeat_interleave#torch.repeat_interleave\" title=\"torch.repeat_interleave\"><code>torch.repeat_interleave()</code></a>.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt>\n<code>requires_grad</code> </dt> <dd>\n<p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The fact that gradients need to be computed for a Tensor do not mean that the <a class=\"reference internal\" href=\"autograd#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> attribute will be populated, see <a class=\"reference internal\" href=\"autograd#torch.Tensor.is_leaf\" title=\"torch.Tensor.is_leaf\"><code>is_leaf</code></a> for more details.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.requires_grad_\">\n<code>requires_grad_(requires_grad=True) → Tensor</code> </dt> <dd>\n<p>Change if autograd should record operations on this tensor: sets this tensor’s <a class=\"reference internal\" href=\"autograd#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>requires_grad</code></a> attribute in-place. Returns this tensor.</p> <p><a class=\"reference internal\" href=\"#torch.Tensor.requires_grad_\" title=\"torch.Tensor.requires_grad_\"><code>requires_grad_()</code></a>’s main use case is to tell autograd to begin recording operations on a Tensor <code>tensor</code>. If <code>tensor</code> has <code>requires_grad=False</code> (because it was obtained through a DataLoader, or required preprocessing or initialization), <code>tensor.requires_grad_()</code> makes it so that autograd will begin to record operations on <code>tensor</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>requires_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – If autograd should record operations on this tensor. Default: <code>True</code>.</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; # Let's say we want to preprocess some saved weights and use\n&gt;&gt;&gt; # the result as new weights.\n&gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25]\n&gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights)\n&gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function\n&gt;&gt;&gt; weights\ntensor([-0.5503,  0.4926, -2.1158, -0.8303])\n\n&gt;&gt;&gt; # Now, start to record operations done to weights\n&gt;&gt;&gt; weights.requires_grad_()\n&gt;&gt;&gt; out = weights.pow(2).sum()\n&gt;&gt;&gt; out.backward()\n&gt;&gt;&gt; weights.grad\ntensor([-1.1007,  0.9853, -4.2316, -1.6606])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.reshape\">\n<code>reshape(*shape) → Tensor</code> </dt> <dd>\n<p>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape. This method returns a view if <code>shape</code> is compatible with the current shape. See <a class=\"reference internal\" href=\"#torch.Tensor.view\" title=\"torch.Tensor.view\"><code>torch.Tensor.view()</code></a> on when it is possible to return a view.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.reshape#torch.reshape\" title=\"torch.reshape\"><code>torch.reshape()</code></a></p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>shape</strong> (<em>tuple of python:ints</em><em> or </em><em>int...</em>) – the desired shape</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.reshape_as\">\n<code>reshape_as(other) → Tensor</code> </dt> <dd>\n<p>Returns this tensor as the same shape as <code>other</code>. <code>self.reshape_as(other)</code> is equivalent to <code>self.reshape(other.sizes())</code>. This method returns a view if <code>other.sizes()</code> is compatible with the current shape. See <a class=\"reference internal\" href=\"#torch.Tensor.view\" title=\"torch.Tensor.view\"><code>torch.Tensor.view()</code></a> on when it is possible to return a view.</p> <p>Please see <a class=\"reference internal\" href=\"generated/torch.reshape#torch.reshape\" title=\"torch.reshape\"><code>reshape()</code></a> for more information about <code>reshape</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>other</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>) – The result tensor has the same shape as <code>other</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.resize_\">\n<code>resize_(*sizes, memory_format=torch.contiguous_format) → Tensor</code> </dt> <dd>\n<p>Resizes <code>self</code> tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use <a class=\"reference internal\" href=\"#torch.Tensor.view\" title=\"torch.Tensor.view\"><code>view()</code></a>, which checks for contiguity, or <a class=\"reference internal\" href=\"#torch.Tensor.reshape\" title=\"torch.Tensor.reshape\"><code>reshape()</code></a>, which copies data if needed. To change the size in-place with custom strides, see <a class=\"reference internal\" href=\"#torch.Tensor.set_\" title=\"torch.Tensor.set_\"><code>set_()</code></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>sizes</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</li> <li>\n<strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of Tensor. Default: <code>torch.contiguous_format</code>. Note that memory format of <code>self</code> is going to be unaffected if <code>self.size()</code> matches <code>sizes</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n&gt;&gt;&gt; x.resize_(2, 2)\ntensor([[ 1,  2],\n        [ 3,  4]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.resize_as_\">\n<code>resize_as_(tensor, memory_format=torch.contiguous_format) → Tensor</code> </dt> <dd>\n<p>Resizes the <code>self</code> tensor to be the same size as the specified <a class=\"reference internal\" href=\"generated/torch.tensor#torch.tensor\" title=\"torch.tensor\"><code>tensor</code></a>. This is equivalent to <code>self.resize_(tensor.size())</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of Tensor. Default: <code>torch.contiguous_format</code>. Note that memory format of <code>self</code> is going to be unaffected if <code>self.size()</code> matches <code>tensor.size()</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>retain_grad()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.retain_grad\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Enables .grad attribute for non-leaf Tensors.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.roll\">\n<code>roll(shifts, dims) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.roll#torch.roll\" title=\"torch.roll\"><code>torch.roll()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.rot90\">\n<code>rot90(k, dims) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.rot90#torch.rot90\" title=\"torch.rot90\"><code>torch.rot90()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.round\">\n<code>round() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.round#torch.round\" title=\"torch.round\"><code>torch.round()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.round_\">\n<code>round_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.round\" title=\"torch.Tensor.round\"><code>round()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.rsqrt\">\n<code>rsqrt() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.rsqrt#torch.rsqrt\" title=\"torch.rsqrt\"><code>torch.rsqrt()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.rsqrt_\">\n<code>rsqrt_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.rsqrt\" title=\"torch.Tensor.rsqrt\"><code>rsqrt()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.scatter\">\n<code>scatter(dim, index, src) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>torch.Tensor.scatter_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.scatter_\">\n<code>scatter_(dim, index, src, reduce=None) → Tensor</code> </dt> <dd>\n<p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor. For each value in <code>src</code>, its output index is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p> <p>For a 3-D tensor, <code>self</code> is updated as:</p> <pre data-language=\"python\">self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n</pre> <p>This is the reverse operation of the manner described in <a class=\"reference internal\" href=\"#torch.Tensor.gather\" title=\"torch.Tensor.gather\"><code>gather()</code></a>.</p> <p><code>self</code>, <code>index</code> and <code>src</code> (if it is a Tensor) should all have the same number of dimensions. It is also required that <code>index.size(d) &lt;= src.size(d)</code> for all dimensions <code>d</code>, and that <code>index.size(d) &lt;= self.size(d)</code> for all dimensions <code>d != dim</code>. Note that <code>index</code> and <code>src</code> do not broadcast.</p> <p>Moreover, as for <a class=\"reference internal\" href=\"#torch.Tensor.gather\" title=\"torch.Tensor.gather\"><code>gather()</code></a>, the values of <code>index</code> must be between <code>0</code> and <code>self.size(dim) - 1</code> inclusive.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When indices are not unique, the behavior is non-deterministic (one of the values from <code>src</code> will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The backward pass is implemented only for <code>src.shape == index.shape</code>.</p> </div> <p>Additionally accepts an optional <code>reduce</code> argument that allows specification of an optional reduction operation, which is applied to all values in the tensor <code>src</code> into <code>self</code> at the indicies specified in the <code>index</code>. For each value in <code>src</code>, the reduction operation is applied to an index in <code>self</code> which is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p> <p>Given a 3-D tensor and reduction using the multiplication operation, <code>self</code> is updated as:</p> <pre data-language=\"python\">self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n</pre> <p>Reducing with the addition operation is the same as using <a class=\"reference internal\" href=\"#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>scatter_add_()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the axis along which to index</li> <li>\n<strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter, can be either empty or of the same dimensionality as <code>src</code>. When empty, the operation returns <code>self</code> unchanged.</li> <li>\n<strong>src</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a>) – the source element(s) to scatter.</li> <li>\n<strong>reduce</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em>, </em><em>optional</em>) – reduction operation to apply, can be either <code>'add'</code> or <code>'multiply'</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; src = torch.arange(1, 11).reshape((2, 5))\n&gt;&gt;&gt; src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n&gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0]])\n&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n&gt;&gt;&gt; index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n&gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n&gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.scatter_add_\">\n<code>scatter_add_(dim, index, src) → Tensor</code> </dt> <dd>\n<p>Adds all values from the tensor <code>other</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a class=\"reference internal\" href=\"#torch.Tensor.scatter_\" title=\"torch.Tensor.scatter_\"><code>scatter_()</code></a>. For each value in <code>src</code>, it is added to an index in <code>self</code> which is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p> <p>For a 3-D tensor, <code>self</code> is updated as:</p> <pre data-language=\"python\">self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n</pre> <p><code>self</code>, <code>index</code> and <code>src</code> should have same number of dimensions. It is also required that <code>index.size(d) &lt;= src.size(d)</code> for all dimensions <code>d</code>, and that <code>index.size(d) &lt;= self.size(d)</code> for all dimensions <code>d != dim</code>. Note that <code>index</code> and <code>src</code> do not broadcast.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This operation may behave nondeterministically when given tensors on a CUDA device. See <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/randomness.html\"><span class=\"doc\">Reproducibility</span></a> for more information.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The backward pass is implemented only for <code>src.shape == index.shape</code>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the axis along which to index</li> <li>\n<strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter and add, can be either empty or of the same dimensionality as <code>src</code>. When empty, the operation returns <code>self</code> unchanged.</li> <li>\n<strong>src</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the source elements to scatter and add</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; src = torch.ones((2, 5))\n&gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0, 0]])\n&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n&gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.scatter_add\">\n<code>scatter_add(dim, index, src) → Tensor</code> </dt> <dd>\n<p>Out-of-place version of <a class=\"reference internal\" href=\"#torch.Tensor.scatter_add_\" title=\"torch.Tensor.scatter_add_\"><code>torch.Tensor.scatter_add_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.select\">\n<code>select(dim, index) → Tensor</code> </dt> <dd>\n<p>Slices the <code>self</code> tensor along the selected dimension at the given index. This function returns a view of the original tensor with the given dimension removed.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the dimension to slice</li> <li>\n<strong>index</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the index to select with</li> </ul> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p><a class=\"reference internal\" href=\"#torch.Tensor.select\" title=\"torch.Tensor.select\"><code>select()</code></a> is equivalent to slicing. For example, <code>tensor.select(0, index)</code> is equivalent to <code>tensor[index]</code> and <code>tensor.select(2, index)</code> is equivalent to <code>tensor[:,:,index]</code>.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.set_\">\n<code>set_(source=None, storage_offset=0, size=None, stride=None) → Tensor</code> </dt> <dd>\n<p>Sets the underlying storage, size, and strides. If <code>source</code> is a tensor, <code>self</code> tensor will share the same storage and have the same size and strides as <code>source</code>. Changes to elements in one tensor will be reflected in the other.</p> <p>If <code>source</code> is a <code>Storage</code>, the method sets the underlying storage, offset, size, and stride.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>source</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><em>Storage</em>) – the tensor or storage to use</li> <li>\n<strong>storage_offset</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – the offset in the storage</li> <li>\n<strong>size</strong> (<em>torch.Size</em><em>, </em><em>optional</em>) – the desired size. Defaults to the size of the source.</li> <li>\n<strong>stride</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a><em>, </em><em>optional</em>) – the desired stride. Defaults to C-contiguous strides.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.share_memory_\">\n<code>share_memory_()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.share_memory_\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Moves the underlying storage to shared memory.</p> <p>This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.short\">\n<code>short(memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>. See <a class=\"reference internal\" href=\"#torch.Tensor.to\" title=\"torch.Tensor.to\"><code>to()</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>memory_format</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional) – the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sigmoid\">\n<code>sigmoid() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sigmoid#torch.sigmoid\" title=\"torch.sigmoid\"><code>torch.sigmoid()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sigmoid_\">\n<code>sigmoid_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sigmoid\" title=\"torch.Tensor.sigmoid\"><code>sigmoid()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sign\">\n<code>sign() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sign#torch.sign\" title=\"torch.sign\"><code>torch.sign()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sign_\">\n<code>sign_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sign\" title=\"torch.Tensor.sign\"><code>sign()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.signbit\">\n<code>signbit() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.signbit#torch.signbit\" title=\"torch.signbit\"><code>torch.signbit()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sgn\">\n<code>sgn() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sgn#torch.sgn\" title=\"torch.sgn\"><code>torch.sgn()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sgn_\">\n<code>sgn_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sgn\" title=\"torch.Tensor.sgn\"><code>sgn()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sin\">\n<code>sin() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sin#torch.sin\" title=\"torch.sin\"><code>torch.sin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sin_\">\n<code>sin_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sin\" title=\"torch.Tensor.sin\"><code>sin()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sinc\">\n<code>sinc() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sinc#torch.sinc\" title=\"torch.sinc\"><code>torch.sinc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sinc_\">\n<code>sinc_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sinc\" title=\"torch.Tensor.sinc\"><code>sinc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sinh\">\n<code>sinh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sinh#torch.sinh\" title=\"torch.sinh\"><code>torch.sinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sinh_\">\n<code>sinh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sinh\" title=\"torch.Tensor.sinh\"><code>sinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.asinh\">\n<code>asinh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.asinh#torch.asinh\" title=\"torch.asinh\"><code>torch.asinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.asinh_\">\n<code>asinh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.asinh\" title=\"torch.Tensor.asinh\"><code>asinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arcsinh\">\n<code>arcsinh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.arcsinh#torch.arcsinh\" title=\"torch.arcsinh\"><code>torch.arcsinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arcsinh_\">\n<code>arcsinh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arcsinh\" title=\"torch.Tensor.arcsinh\"><code>arcsinh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.size\">\n<code>size() → torch.Size</code> </dt> <dd>\n<p>Returns the size of the <code>self</code> tensor. The returned value is a subclass of <a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\"><code>tuple</code></a>.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.empty(3, 4, 5).size()\ntorch.Size([3, 4, 5])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.slogdet\">\n<code>slogdet() -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.slogdet#torch.slogdet\" title=\"torch.slogdet\"><code>torch.slogdet()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.solve\">\n<code>solve(A) → Tensor, Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.solve#torch.solve\" title=\"torch.solve\"><code>torch.solve()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sort\">\n<code>sort(dim=-1, descending=False) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sort#torch.sort\" title=\"torch.sort\"><code>torch.sort()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.split\">\n<code>split(split_size, dim=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.split\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.split#torch.split\" title=\"torch.split\"><code>torch.split()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>sparse_mask(mask) → Tensor</code> </dt> <dd>\n<p>Returns a new <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">sparse tensor</span></a> with values from a strided tensor <code>self</code> filtered by the indices of the sparse tensor <code>mask</code>. The values of <code>mask</code> sparse tensor are ignored. <code>self</code> and <code>mask</code> tensors must have the same shape.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The returned sparse tensor has the same indices as the sparse tensor <code>mask</code>, even when the corresponding values in <code>self</code> are zeros.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>mask</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – a sparse tensor whose indices are used as a filter</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; nse = 5\n&gt;&gt;&gt; dims = (5, 5, 2, 2)\n&gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n&gt;&gt;&gt; V = torch.randn(nse, dims[2], dims[3])\n&gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n&gt;&gt;&gt; D = torch.randn(dims)\n&gt;&gt;&gt; D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n                       [0, 1, 4, 3]]),\n       values=tensor([[[ 1.6550,  0.2397],\n                       [-0.1611, -0.0779]],\n\n                      [[ 0.2326, -1.0558],\n                       [ 1.4711,  1.9678]],\n\n                      [[-0.5138, -0.0411],\n                       [ 1.9417,  0.5158]],\n\n                      [[ 0.0793,  0.0036],\n                       [-0.2569, -0.1055]]]),\n       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>sparse_dim() → int</code> </dt> <dd>\n<p>Return the number of sparse dimensions in a <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">sparse tensor</span></a> <code>self</code>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Throws an error if <code>self</code> is not a sparse tensor.</p> </div> <p>See also <a class=\"reference internal\" href=\"sparse#torch.Tensor.dense_dim\" title=\"torch.Tensor.dense_dim\"><code>Tensor.dense_dim()</code></a> and <a class=\"reference internal\" href=\"sparse#sparse-hybrid-coo-docs\"><span class=\"std std-ref\">hybrid tensors</span></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sqrt\">\n<code>sqrt() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sqrt#torch.sqrt\" title=\"torch.sqrt\"><code>torch.sqrt()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sqrt_\">\n<code>sqrt_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sqrt\" title=\"torch.Tensor.sqrt\"><code>sqrt()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.square\">\n<code>square() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.square#torch.square\" title=\"torch.square\"><code>torch.square()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.square_\">\n<code>square_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.square\" title=\"torch.Tensor.square\"><code>square()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.squeeze\">\n<code>squeeze(dim=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.squeeze#torch.squeeze\" title=\"torch.squeeze\"><code>torch.squeeze()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.squeeze_\">\n<code>squeeze_(dim=None) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.squeeze\" title=\"torch.Tensor.squeeze\"><code>squeeze()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.std\">\n<code>std(dim=None, unbiased=True, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.std#torch.std\" title=\"torch.std\"><code>torch.std()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.stft\">\n<code>stft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=None, return_complex=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.stft\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.stft#torch.stft\" title=\"torch.stft\"><code>torch.stft()</code></a></p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.storage\">\n<code>storage() → torch.Storage</code> </dt> <dd>\n<p>Returns the underlying storage.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.storage_offset\">\n<code>storage_offset() → int</code> </dt> <dd>\n<p>Returns <code>self</code> tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5])\n&gt;&gt;&gt; x.storage_offset()\n0\n&gt;&gt;&gt; x[3:].storage_offset()\n3\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.storage_type\">\n<code>storage_type() → type</code> </dt> <dd>\n<p>Returns the type of the underlying storage.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.stride\">\n<code>stride(dim) → tuple or int</code> </dt> <dd>\n<p>Returns the stride of <code>self</code> tensor.</p> <p>Stride is the jump necessary to go from one element to the next one in the specified dimension <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim</code></a>. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension <a class=\"reference internal\" href=\"#torch.Tensor.dim\" title=\"torch.Tensor.dim\"><code>dim</code></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>dim</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – the desired dimension in which stride is required</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; x.stride()\n(5, 1)\n&gt;&gt;&gt; x.stride(0)\n5\n&gt;&gt;&gt; x.stride(-1)\n1\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sub\">\n<code>sub(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sub#torch.sub\" title=\"torch.sub\"><code>torch.sub()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sub_\">\n<code>sub_(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.sub\" title=\"torch.Tensor.sub\"><code>sub()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.subtract\">\n<code>subtract(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.subtract#torch.subtract\" title=\"torch.subtract\"><code>torch.subtract()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.subtract_\">\n<code>subtract_(other, *, alpha=1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.subtract\" title=\"torch.Tensor.subtract\"><code>subtract()</code></a>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sum\">\n<code>sum(dim=None, keepdim=False, dtype=None) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.sum#torch.sum\" title=\"torch.sum\"><code>torch.sum()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.sum_to_size\">\n<code>sum_to_size(*size) → Tensor</code> </dt> <dd>\n<p>Sum <code>this</code> tensor to <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a>. <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> must be broadcastable to <code>this</code> tensor size.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>size</strong> (<em>int...</em>) – a sequence of integers defining the shape of the output tensor.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.svd\">\n<code>svd(some=True, compute_uv=True) -&gt; (Tensor, Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.svd#torch.svd\" title=\"torch.svd\"><code>torch.svd()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.swapaxes\">\n<code>swapaxes(axis0, axis1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.swapaxes#torch.swapaxes\" title=\"torch.swapaxes\"><code>torch.swapaxes()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.swapdims\">\n<code>swapdims(dim0, dim1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.swapdims#torch.swapdims\" title=\"torch.swapdims\"><code>torch.swapdims()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.symeig\">\n<code>symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.symeig#torch.symeig\" title=\"torch.symeig\"><code>torch.symeig()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.t\">\n<code>t() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.t#torch.t\" title=\"torch.t\"><code>torch.t()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.t_\">\n<code>t_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.t\" title=\"torch.Tensor.t\"><code>t()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tensor_split\">\n<code>tensor_split(indices_or_sections, dim=0) → List of Tensors</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.tensor_split#torch.tensor_split\" title=\"torch.tensor_split\"><code>torch.tensor_split()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tile\">\n<code>tile(*reps) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.tile#torch.tile\" title=\"torch.tile\"><code>torch.tile()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.to\">\n<code>to(*args, **kwargs) → Tensor</code> </dt> <dd>\n<p>Performs Tensor dtype and/or device conversion. A <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> are inferred from the arguments of <code>self.to(*args, **kwargs)</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If the <code>self</code> Tensor already has the correct <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, then <code>self</code> is returned. Otherwise, the returned tensor is a copy of <code>self</code> with the desired <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>.</p> </div> <p>Here are the ways to call <code>to</code>:</p> <dl class=\"function\"> <dt>\n<code>to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor with the specified <code>dtype</code></p> <dl class=\"simple\"> <dt>Args:</dt>\n<dd>\n<p>memory_format (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional): the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt>\n<code>to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor with the specified <a class=\"reference internal\" href=\"#torch.Tensor.device\" title=\"torch.Tensor.device\"><code>device</code></a> and (optional) <code>dtype</code>. If <code>dtype</code> is <code>None</code> it is inferred to be <code>self.dtype</code>. When <code>non_blocking</code>, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When <code>copy</code> is set, a new Tensor is created even when the Tensor already matches the desired conversion.</p> <dl class=\"simple\"> <dt>Args:</dt>\n<dd>\n<p>memory_format (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a>, optional): the desired memory format of returned Tensor. Default: <code>torch.preserve_format</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt>\n<code>to(other, non_blocking=False, copy=False) → Tensor</code> </dt> <dd>\n<p>Returns a Tensor with same <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> and <a class=\"reference internal\" href=\"tensor_attributes#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> as the Tensor <code>other</code>. When <code>non_blocking</code>, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When <code>copy</code> is set, a new Tensor is created even when the Tensor already matches the desired conversion.</p> </dd>\n</dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n&gt;&gt;&gt; tensor.to(torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64)\n\n&gt;&gt;&gt; cuda0 = torch.device('cuda:0')\n&gt;&gt;&gt; tensor.to(cuda0)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], device='cuda:0')\n\n&gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n&gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0)\n&gt;&gt;&gt; tensor.to(other, non_blocking=True)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.to_mkldnn\">\n<code>to_mkldnn() → Tensor</code> </dt> <dd>\n<p>Returns a copy of the tensor in <code>torch.mkldnn</code> layout.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.take\">\n<code>take(indices) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.take#torch.take\" title=\"torch.take\"><code>torch.take()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tan\">\n<code>tan() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.tan#torch.tan\" title=\"torch.tan\"><code>torch.tan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tan_\">\n<code>tan_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.tan\" title=\"torch.Tensor.tan\"><code>tan()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tanh\">\n<code>tanh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.tanh#torch.tanh\" title=\"torch.tanh\"><code>torch.tanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tanh_\">\n<code>tanh_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.tanh\" title=\"torch.Tensor.tanh\"><code>tanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atanh\">\n<code>atanh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.atanh#torch.atanh\" title=\"torch.atanh\"><code>torch.atanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.atanh_\">\n<code>atanh_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.atanh\" title=\"torch.Tensor.atanh\"><code>atanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arctanh\">\n<code>arctanh() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.arctanh#torch.arctanh\" title=\"torch.arctanh\"><code>torch.arctanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.arctanh_\">\n<code>arctanh_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.arctanh\" title=\"torch.Tensor.arctanh\"><code>arctanh()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tolist\">\n<code>tolist() → list or number</code> </dt> <dd>\n<p>Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with <a class=\"reference internal\" href=\"#torch.Tensor.item\" title=\"torch.Tensor.item\"><code>item()</code></a>. Tensors are automatically moved to the CPU first if necessary.</p> <p>This operation is not differentiable.</p> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.randn(2, 2)\n&gt;&gt;&gt; a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n&gt;&gt;&gt; a[0,0].tolist()\n0.012766935862600803\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.topk\">\n<code>topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.topk#torch.topk\" title=\"torch.topk\"><code>torch.topk()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>to_sparse(sparseDims) → Tensor</code> </dt> <dd>\n<p>Returns a sparse copy of the tensor. PyTorch supports sparse tensors in <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">coordinate format</span></a>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>sparseDims</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – the number of sparse dimensions to include in the new sparse tensor</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n&gt;&gt;&gt; d\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n&gt;&gt;&gt; d.to_sparse()\ntensor(indices=tensor([[1, 1],\n                       [0, 2]]),\n       values=tensor([ 9, 10]),\n       size=(3, 3), nnz=2, layout=torch.sparse_coo)\n&gt;&gt;&gt; d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n       values=tensor([[ 9,  0, 10]]),\n       size=(3, 3), nnz=1, layout=torch.sparse_coo)\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.trace\">\n<code>trace() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.trace#torch.trace\" title=\"torch.trace\"><code>torch.trace()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.transpose\">\n<code>transpose(dim0, dim1) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.transpose#torch.transpose\" title=\"torch.transpose\"><code>torch.transpose()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.transpose_\">\n<code>transpose_(dim0, dim1) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.transpose\" title=\"torch.Tensor.transpose\"><code>transpose()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.triangular_solve\">\n<code>triangular_solve(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor)</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.triangular_solve#torch.triangular_solve\" title=\"torch.triangular_solve\"><code>torch.triangular_solve()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tril\">\n<code>tril(k=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.tril#torch.tril\" title=\"torch.tril\"><code>torch.tril()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.tril_\">\n<code>tril_(k=0) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.tril\" title=\"torch.Tensor.tril\"><code>tril()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.triu\">\n<code>triu(k=0) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.triu#torch.triu\" title=\"torch.triu\"><code>torch.triu()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.triu_\">\n<code>triu_(k=0) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.triu\" title=\"torch.Tensor.triu\"><code>triu()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.true_divide\">\n<code>true_divide(value) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.true_divide#torch.true_divide\" title=\"torch.true_divide\"><code>torch.true_divide()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.true_divide_\">\n<code>true_divide_(value) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.true_divide_\" title=\"torch.Tensor.true_divide_\"><code>true_divide_()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.trunc\">\n<code>trunc() → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.trunc#torch.trunc\" title=\"torch.trunc\"><code>torch.trunc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.trunc_\">\n<code>trunc_() → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.trunc\" title=\"torch.Tensor.trunc\"><code>trunc()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.type\">\n<code>type(dtype=None, non_blocking=False, **kwargs) → str or Tensor</code> </dt> <dd>\n<p>Returns the type if <code>dtype</code> is not provided, else casts this object to the specified type.</p> <p>If this is already of the correct type, no copy is performed and the original object is returned.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dtype</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#type\" title=\"(in Python v3.9)\">type</a><em> or </em><em>string</em>) – The desired type</li> <li>\n<strong>non_blocking</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – If <code>True</code>, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.</li> <li>\n<strong>**kwargs</strong> – For compatibility, may contain the key <code>async</code> in place of the <code>non_blocking</code> argument. The <code>async</code> arg is deprecated.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.type_as\">\n<code>type_as(tensor) → Tensor</code> </dt> <dd>\n<p>Returns this tensor cast to the type of the given tensor.</p> <p>This is a no-op if the tensor is already of the correct type. This is equivalent to <code>self.type(tensor.type())</code></p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>tensor</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – the tensor which has the desired type</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unbind\">\n<code>unbind(dim=0) → seq</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.unbind#torch.unbind\" title=\"torch.unbind\"><code>torch.unbind()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unfold\">\n<code>unfold(dimension, size, step) → Tensor</code> </dt> <dd>\n<p>Returns a view of the original tensor which contains all slices of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> from <code>self</code> tensor in the dimension <code>dimension</code>.</p> <p>Step between two slices is given by <code>step</code>.</p> <p>If <code>sizedim</code> is the size of dimension <code>dimension</code> for <code>self</code>, the size of dimension <code>dimension</code> in the returned tensor will be <code>(sizedim - size) / step + 1</code>.</p> <p>An additional dimension of size <a class=\"reference internal\" href=\"#torch.Tensor.size\" title=\"torch.Tensor.size\"><code>size</code></a> is appended in the returned tensor.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>dimension</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – dimension in which unfolding happens</li> <li>\n<strong>size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the size of each slice that is unfolded</li> <li>\n<strong>step</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – the step between each slice</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.arange(1., 8)\n&gt;&gt;&gt; x\ntensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n&gt;&gt;&gt; x.unfold(0, 2, 1)\ntensor([[ 1.,  2.],\n        [ 2.,  3.],\n        [ 3.,  4.],\n        [ 4.,  5.],\n        [ 5.,  6.],\n        [ 6.,  7.]])\n&gt;&gt;&gt; x.unfold(0, 2, 2)\ntensor([[ 1.,  2.],\n        [ 3.,  4.],\n        [ 5.,  6.]])\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.uniform_\">\n<code>uniform_(from=0, to=1) → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with numbers sampled from the continuous uniform distribution:</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>P</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mtext>to</mtext><mo>−</mo><mtext>from</mtext></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">P(x) = \\dfrac{1}{\\text{to} - \\text{from}} </annotation></semantics></math></span></span></span> </div>\n</dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unique\">\n<code>unique(sorted=True, return_inverse=False, return_counts=False, dim=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.unique\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the unique elements of the input tensor.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.unique#torch.unique\" title=\"torch.unique\"><code>torch.unique()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unique_consecutive\">\n<code>unique_consecutive(return_inverse=False, return_counts=False, dim=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.unique_consecutive\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Eliminates all but the first element from every consecutive group of equivalent elements.</p> <p>See <a class=\"reference internal\" href=\"generated/torch.unique_consecutive#torch.unique_consecutive\" title=\"torch.unique_consecutive\"><code>torch.unique_consecutive()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unsqueeze\">\n<code>unsqueeze(dim) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.unsqueeze#torch.unsqueeze\" title=\"torch.unsqueeze\"><code>torch.unsqueeze()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.unsqueeze_\">\n<code>unsqueeze_(dim) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.unsqueeze\" title=\"torch.Tensor.unsqueeze\"><code>unsqueeze()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt>\n<code>values() → Tensor</code> </dt> <dd>\n<p>Return the values tensor of a <a class=\"reference internal\" href=\"sparse#sparse-coo-docs\"><span class=\"std std-ref\">sparse COO tensor</span></a>.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Throws an error if <code>self</code> is not a sparse COO tensor.</p> </div> <p>See also <a class=\"reference internal\" href=\"sparse#torch.Tensor.indices\" title=\"torch.Tensor.indices\"><code>Tensor.indices()</code></a>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This method can only be called on a coalesced sparse tensor. See <a class=\"reference internal\" href=\"sparse#torch.Tensor.coalesce\" title=\"torch.Tensor.coalesce\"><code>Tensor.coalesce()</code></a> for details.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.var\">\n<code>var(dim=None, unbiased=True, keepdim=False) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.var#torch.var\" title=\"torch.var\"><code>torch.var()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.vdot\">\n<code>vdot(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.vdot#torch.vdot\" title=\"torch.vdot\"><code>torch.vdot()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.view\">\n<code>view(*shape) → Tensor</code> </dt> <dd>\n<p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</p> <p>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo separator=\"true\">,</mo><mi>d</mi><mo>+</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>d</mi><mo>+</mo><mi>k</mi></mrow><annotation encoding=\"application/x-tex\">d, d+1, \\dots, d+k</annotation></semantics></math></span></span> </span> that satisfy the following contiguity-like condition that <span class=\"math\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∀</mi><mi>i</mi><mo>=</mo><mi>d</mi><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><mi>d</mi><mo>+</mo><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">\\forall i = d, \\dots, d+k-1</annotation></semantics></math></span></span> </span>,</p> <div class=\"math\"> <span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mtext>stride</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo stretchy=\"false\">]</mo><mo>=</mo><mtext>stride</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">]</mo><mo>×</mo><mtext>size</mtext><mo stretchy=\"false\">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]</annotation></semantics></math></span></span></span> </div>\n<p>Otherwise, it will not be possible to view <code>self</code> tensor as <code>shape</code> without copying it (e.g., via <a class=\"reference internal\" href=\"#torch.Tensor.contiguous\" title=\"torch.Tensor.contiguous\"><code>contiguous()</code></a>). When it is unclear whether a <a class=\"reference internal\" href=\"#torch.Tensor.view\" title=\"torch.Tensor.view\"><code>view()</code></a> can be performed, it is advisable to use <a class=\"reference internal\" href=\"generated/torch.reshape#torch.reshape\" title=\"torch.reshape\"><code>reshape()</code></a>, which returns a view if the shapes are compatible, and copies (equivalent to calling <a class=\"reference internal\" href=\"#torch.Tensor.contiguous\" title=\"torch.Tensor.contiguous\"><code>contiguous()</code></a>) otherwise.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>shape</strong> (<em>torch.Size</em><em> or </em><em>int...</em>) – the desired size</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn(4, 4)\n&gt;&gt;&gt; x.size()\ntorch.Size([4, 4])\n&gt;&gt;&gt; y = x.view(16)\n&gt;&gt;&gt; y.size()\ntorch.Size([16])\n&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n&gt;&gt;&gt; z.size()\ntorch.Size([2, 8])\n\n&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4)\n&gt;&gt;&gt; a.size()\ntorch.Size([1, 2, 3, 4])\n&gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n&gt;&gt;&gt; b.size()\ntorch.Size([1, 3, 2, 4])\n&gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n&gt;&gt;&gt; c.size()\ntorch.Size([1, 3, 2, 4])\n&gt;&gt;&gt; torch.equal(b, c)\nFalse\n</pre> <dl class=\"function\"> <dt>\n<code>view(dtype) → Tensor</code> </dt> \n</dl> <p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>dtype</code>. <code>dtype</code> must have the same number of bytes per element as <code>self</code>’s dtype.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>dtype</strong> (<a class=\"reference internal\" href=\"tensor_attributes#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>) – the desired dtype</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn(4, 4)\n&gt;&gt;&gt; x\ntensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n&gt;&gt;&gt; x.dtype\ntorch.float32\n\n&gt;&gt;&gt; y = x.view(torch.int32)\n&gt;&gt;&gt; y\ntensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n        [-1105482831,  1061112040,  1057999968, -1084397505],\n        [-1071760287, -1123489973, -1097310419, -1084649136],\n        [-1101533110,  1073668768, -1082790149, -1088634448]],\n    dtype=torch.int32)\n&gt;&gt;&gt; y[0, 0] = 1000000000\n&gt;&gt;&gt; x\ntensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n        [-0.1520,  0.7472,  0.5617, -0.8649],\n        [-2.4724, -0.0334, -0.2976, -0.8499],\n        [-0.2109,  1.9913, -0.9607, -0.6123]])\n\n&gt;&gt;&gt; x.view(torch.int16)\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nRuntimeError: Viewing a tensor as a new dtype with a different number of bytes per element is not supported.\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.view_as\">\n<code>view_as(other) → Tensor</code> </dt> <dd>\n<p>View this tensor as the same size as <code>other</code>. <code>self.view_as(other)</code> is equivalent to <code>self.view(other.size())</code>.</p> <p>Please see <a class=\"reference internal\" href=\"#torch.Tensor.view\" title=\"torch.Tensor.view\"><code>view()</code></a> for more information about <code>view</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>other</strong> (<a class=\"reference internal\" href=\"#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>) – The result tensor has the same size as <code>other</code>.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.where\">\n<code>where(condition, y) → Tensor</code> </dt> <dd>\n<p><code>self.where(condition, y)</code> is equivalent to <code>torch.where(condition, self, y)</code>. See <a class=\"reference internal\" href=\"generated/torch.where#torch.where\" title=\"torch.where\"><code>torch.where()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.xlogy\">\n<code>xlogy(other) → Tensor</code> </dt> <dd>\n<p>See <a class=\"reference internal\" href=\"generated/torch.xlogy#torch.xlogy\" title=\"torch.xlogy\"><code>torch.xlogy()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.xlogy_\">\n<code>xlogy_(other) → Tensor</code> </dt> <dd>\n<p>In-place version of <a class=\"reference internal\" href=\"#torch.Tensor.xlogy\" title=\"torch.Tensor.xlogy\"><code>xlogy()</code></a></p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.zero_\">\n<code>zero_() → Tensor</code> </dt> <dd>\n<p>Fills <code>self</code> tensor with zeros.</p> </dd>\n</dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/tensors.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/tensors.html</a>\n  </p>\n</div>\n","tensor_attributes":"<h1 id=\"tensor-attributes-doc\">Tensor Attributes</h1> <p id=\"tensor-attributes\">Each <code>torch.Tensor</code> has a <a class=\"reference internal\" href=\"#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a>, <a class=\"reference internal\" href=\"#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a>, and <a class=\"reference internal\" href=\"#torch.torch.layout\" title=\"torch.torch.layout\"><code>torch.layout</code></a>.</p>  <h2 id=\"dtype-doc\">torch.dtype</h2> <dl class=\"class\" id=\"torch-dtype\"> <dt id=\"torch.torch.dtype\">\n<code>class torch.dtype</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> is an object that represents the data type of a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>. PyTorch has twelve different data types:</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Data type</p></th> <th class=\"head\"><p>dtype</p></th> <th class=\"head\"><p>Legacy Constructors</p></th> </tr> </thead>  <tr>\n<td><p>32-bit floating point</p></td> <td><p><code>torch.float32</code> or <code>torch.float</code></p></td> <td><p><code>torch.*.FloatTensor</code></p></td> </tr> <tr>\n<td><p>64-bit floating point</p></td> <td><p><code>torch.float64</code> or <code>torch.double</code></p></td> <td><p><code>torch.*.DoubleTensor</code></p></td> </tr> <tr>\n<td><p>64-bit complex</p></td> <td><p><code>torch.complex64</code> or <code>torch.cfloat</code></p></td> <td></td> </tr> <tr>\n<td><p>128-bit complex</p></td> <td><p><code>torch.complex128</code> or <code>torch.cdouble</code></p></td> <td></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id3\" id=\"id1\">1</a></p></td> <td><p><code>torch.float16</code> or <code>torch.half</code></p></td> <td><p><code>torch.*.HalfTensor</code></p></td> </tr> <tr>\n<td><p>16-bit floating point <a class=\"footnote-reference brackets\" href=\"#id4\" id=\"id2\">2</a></p></td> <td><p><code>torch.bfloat16</code></p></td> <td><p><code>torch.*.BFloat16Tensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (unsigned)</p></td> <td><p><code>torch.uint8</code></p></td> <td><p><code>torch.*.ByteTensor</code></p></td> </tr> <tr>\n<td><p>8-bit integer (signed)</p></td> <td><p><code>torch.int8</code></p></td> <td><p><code>torch.*.CharTensor</code></p></td> </tr> <tr>\n<td><p>16-bit integer (signed)</p></td> <td><p><code>torch.int16</code> or <code>torch.short</code></p></td> <td><p><code>torch.*.ShortTensor</code></p></td> </tr> <tr>\n<td><p>32-bit integer (signed)</p></td> <td><p><code>torch.int32</code> or <code>torch.int</code></p></td> <td><p><code>torch.*.IntTensor</code></p></td> </tr> <tr>\n<td><p>64-bit integer (signed)</p></td> <td><p><code>torch.int64</code> or <code>torch.long</code></p></td> <td><p><code>torch.*.LongTensor</code></p></td> </tr> <tr>\n<td><p>Boolean</p></td> <td><p><code>torch.bool</code></p></td> <td><p><code>torch.*.BoolTensor</code></p></td> </tr>  </table> <dl class=\"footnote brackets\"> <dt class=\"label\" id=\"id3\">\n<code>1</code> </dt> <dd>\n<p>Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10 significand bits. Useful when precision is important.</p> </dd> <dt class=\"label\" id=\"id4\">\n<code>2</code> </dt> <dd>\n<p>Sometimes referred to as Brain Floating Point: use 1 sign, 8 exponent and 7 significand bits. Useful when range is important, since it has the same number of exponent bits as <code>float32</code></p> </dd> </dl> <p>To find out if a <a class=\"reference internal\" href=\"#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> is a floating point data type, the property <a class=\"reference internal\" href=\"generated/torch.is_floating_point#torch.is_floating_point\" title=\"torch.is_floating_point\"><code>is_floating_point</code></a> can be used, which returns <code>True</code> if the data type is a floating point data type.</p> <p>To find out if a <a class=\"reference internal\" href=\"#torch.torch.dtype\" title=\"torch.torch.dtype\"><code>torch.dtype</code></a> is a complex data type, the property <a class=\"reference internal\" href=\"generated/torch.is_complex#torch.is_complex\" title=\"torch.is_complex\"><code>is_complex</code></a> can be used, which returns <code>True</code> if the data type is a complex data type.</p> <p id=\"type-promotion-doc\">When the dtypes of inputs to an arithmetic operation (<code>add</code>, <code>sub</code>, <code>div</code>, <code>mul</code>) differ, we promote by finding the minimum dtype that satisfies the following rules:</p> <ul class=\"simple\"> <li>If the type of a scalar operand is of a higher category than tensor operands (where complex &gt; floating &gt; integral &gt; boolean), we promote to a type with sufficient size to hold all scalar operands of that category.</li> <li>If a zero-dimension tensor operand has a higher category than dimensioned operands, we promote to a type with sufficient size and category to hold all zero-dim tensor operands of that category.</li> <li>If there are no higher-category zero-dim operands, we promote to a type with sufficient size and category to hold all dimensioned operands.</li> </ul> <p>A floating point scalar operand has dtype <code>torch.get_default_dtype()</code> and an integral non-boolean scalar operand has dtype <code>torch.int64</code>. Unlike numpy, we do not inspect values when determining the minimum <code>dtypes</code> of an operand. Quantized and complex types are not yet supported.</p> <p>Promotion Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; float_tensor = torch.ones(1, dtype=torch.float)\n&gt;&gt;&gt; double_tensor = torch.ones(1, dtype=torch.double)\n&gt;&gt;&gt; complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n&gt;&gt;&gt; complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n&gt;&gt;&gt; int_tensor = torch.ones(1, dtype=torch.int)\n&gt;&gt;&gt; long_tensor = torch.ones(1, dtype=torch.long)\n&gt;&gt;&gt; uint_tensor = torch.ones(1, dtype=torch.uint8)\n&gt;&gt;&gt; double_tensor = torch.ones(1, dtype=torch.double)\n&gt;&gt;&gt; bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n&gt;&gt;&gt; long_zerodim = torch.tensor(1, dtype=torch.long)\n&gt;&gt;&gt; int_zerodim = torch.tensor(1, dtype=torch.int)\n\n&gt;&gt;&gt; torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n&gt;&gt;&gt; (int_tensor + 5).dtype\ntorch.int32\n&gt;&gt;&gt; (int_tensor + long_zerodim).dtype\ntorch.int32\n&gt;&gt;&gt; (long_tensor + int_tensor).dtype\ntorch.int64\n&gt;&gt;&gt; (bool_tensor + long_tensor).dtype\ntorch.int64\n&gt;&gt;&gt; (bool_tensor + uint_tensor).dtype\ntorch.uint8\n&gt;&gt;&gt; (float_tensor + double_tensor).dtype\ntorch.float64\n&gt;&gt;&gt; (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n&gt;&gt;&gt; (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n&gt;&gt;&gt; torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n</pre> <dl class=\"simple\"> <dt>\n<code>When the output tensor of an arithmetic operation is specified, we allow casting to its dtype except that:</code> </dt>\n<dd>\n<ul class=\"simple\"> <li>An integral output tensor cannot accept a floating point tensor.</li> <li>A boolean output tensor cannot accept a non-boolean tensor.</li> <li>A non-complex output tensor cannot accept a complex tensor</li> </ul> </dd> </dl> <p>Casting Examples:</p> <pre data-language=\"python\"># allowed:\n&gt;&gt;&gt; float_tensor *= double_tensor\n&gt;&gt;&gt; float_tensor *= int_tensor\n&gt;&gt;&gt; float_tensor *= uint_tensor\n&gt;&gt;&gt; float_tensor *= bool_tensor\n&gt;&gt;&gt; float_tensor *= double_tensor\n&gt;&gt;&gt; int_tensor *= long_tensor\n&gt;&gt;&gt; int_tensor *= uint_tensor\n&gt;&gt;&gt; uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n&gt;&gt;&gt; int_tensor *= float_tensor\n&gt;&gt;&gt; bool_tensor *= int_tensor\n&gt;&gt;&gt; bool_tensor *= uint_tensor\n&gt;&gt;&gt; float_tensor *= complex_float_tensor\n</pre>   <h2 id=\"device-doc\">torch.device</h2> <dl class=\"class\" id=\"torch-device\"> <dt id=\"torch.torch.device\">\n<code>class torch.device</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> is an object representing the device on which a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is or will be allocated.</p> <p>The <a class=\"reference internal\" href=\"#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> contains a device type (<code>'cpu'</code> or <code>'cuda'</code>) and optional device ordinal for the device type. If the device ordinal is not present, this object will always represent the current device for the device type, even after <a class=\"reference internal\" href=\"cuda#torch.cuda.set_device\" title=\"torch.cuda.set_device\"><code>torch.cuda.set_device()</code></a> is called; e.g., a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> constructed with device <code>'cuda'</code> is equivalent to <code>'cuda:X'</code> where X is the result of <a class=\"reference internal\" href=\"cuda#torch.cuda.current_device\" title=\"torch.cuda.current_device\"><code>torch.cuda.current_device()</code></a>.</p> <p>A <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>’s device can be accessed via the <a class=\"reference internal\" href=\"tensors#torch.Tensor.device\" title=\"torch.Tensor.device\"><code>Tensor.device</code></a> property.</p> <p>A <a class=\"reference internal\" href=\"#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> can be constructed via a string or via a string and device ordinal</p> <p>Via a string:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n&gt;&gt;&gt; torch.device('cpu')\ndevice(type='cpu')\n\n&gt;&gt;&gt; torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n</pre> <p>Via a string and device ordinal:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n&gt;&gt;&gt; torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n</pre> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The <a class=\"reference internal\" href=\"#torch.torch.device\" title=\"torch.torch.device\"><code>torch.device</code></a> argument in functions can generally be substituted with a string. This allows for fast prototyping of code.</p> <pre data-language=\"python\">&gt;&gt;&gt; # Example of a function that takes in a torch.device\n&gt;&gt;&gt; cuda1 = torch.device('cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=cuda1)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # You can substitute the torch.device with a string\n&gt;&gt;&gt; torch.randn((2,3), device='cuda:1')\n</pre> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For legacy reasons, a device can be constructed via a single device ordinal, which is treated as a cuda device. This matches <a class=\"reference internal\" href=\"tensors#torch.Tensor.get_device\" title=\"torch.Tensor.get_device\"><code>Tensor.get_device()</code></a>, which returns an ordinal for cuda tensors and is not supported for cpu tensors.</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.device(1)\ndevice(type='cuda', index=1)\n</pre> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Methods which take a device will generally accept a (properly formatted) string or (legacy) integer device ordinal, i.e. the following are all equivalent:</p> <pre data-language=\"python\">&gt;&gt;&gt; torch.randn((2,3), device=torch.device('cuda:1'))\n&gt;&gt;&gt; torch.randn((2,3), device='cuda:1')\n&gt;&gt;&gt; torch.randn((2,3), device=1)  # legacy\n</pre> </div>   <h2 id=\"layout-doc\">torch.layout</h2> <dl class=\"class\" id=\"torch-layout\"> <dt id=\"torch.torch.layout\">\n<code>class torch.layout</code> </dt> \n</dl> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The <code>torch.layout</code> class is in beta and subject to change.</p> </div> <p>A <a class=\"reference internal\" href=\"#torch.torch.layout\" title=\"torch.torch.layout\"><code>torch.layout</code></a> is an object that represents the memory layout of a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a>. Currently, we support <code>torch.strided</code> (dense Tensors) and have beta support for <code>torch.sparse_coo</code> (sparse COO Tensors).</p> <p><code>torch.strided</code> represents dense Tensors and is the memory layout that is most commonly used. Each strided tensor has an associated <code>torch.Storage</code>, which holds its data. These tensors provide multi-dimensional, <a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stride_of_an_array\">strided</a> view of a storage. Strides are a list of integers: the k-th stride represents the jump in the memory necessary to go from one element to the next one in the k-th dimension of the Tensor. This concept makes it possible to perform many tensor operations efficiently.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n&gt;&gt;&gt; x.stride()\n(5, 1)\n\n&gt;&gt;&gt; x.t().stride()\n(1, 5)\n</pre> <p>For more information on <code>torch.sparse_coo</code> tensors, see <a class=\"reference internal\" href=\"sparse#sparse-docs\"><span class=\"std std-ref\">torch.sparse</span></a>.</p>   <h2 id=\"torch-memory-format\">torch.memory_format</h2> <dl class=\"class\"> <dt id=\"torch.torch.memory_format\">\n<code>class torch.memory_format</code> </dt> \n</dl> <p>A <a class=\"reference internal\" href=\"#torch.torch.memory_format\" title=\"torch.torch.memory_format\"><code>torch.memory_format</code></a> is an object representing the memory format on which a <a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\"><code>torch.Tensor</code></a> is or will be allocated.</p> <p>Possible values are:</p> <ul class=\"simple\"> <li>\n<code>torch.contiguous_format</code>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in decreasing order.</li> <li>\n<code>torch.channels_last</code>: Tensor is or will be allocated in dense non-overlapping memory. Strides represented by values in <code>strides[0] &gt; strides[2] &gt; strides[3] &gt; strides[1] == 1</code> aka NHWC order.</li> <li>\n<code>torch.preserve_format</code>: Used in functions like <code>clone</code> to preserve the memory format of the input tensor. If input tensor is allocated in dense non-overlapping memory, the output tensor strides will be copied from the input. Otherwise output strides will follow <code>torch.contiguous_format</code>\n</li> </ul><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/tensor_attributes.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/tensor_attributes.html</a>\n  </p>\n</div>\n","autograd":"<h1 id=\"automatic-differentiation-package-torch-autograd\">Automatic differentiation package - torch.autograd</h1> <p id=\"module-torch.autograd\"><code>torch.autograd</code> provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare <code>Tensor</code> s for which gradients should be computed with the <code>requires_grad=True</code> keyword. As of now, we only support autograd for floating point <code>Tensor</code> types ( half, float, double and bfloat16) and complex <code>Tensor</code> types (cfloat, cdouble).</p> <dl class=\"function\"> <dt id=\"torch.autograd.backward\">\n<code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd.html#backward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes the sum of gradients of given tensors w.r.t. graph leaves.</p> <p>The graph is differentiated using the chain rule. If any of <code>tensors</code> are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying <code>grad_tensors</code>. It should be a sequence of matching length, that contains the “vector” in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (<code>None</code> is an acceptable value for all tensors that don’t need gradient tensors).</p> <p>This function accumulates gradients in the leaves - you might need to zero <code>.grad</code> attributes or set them to <code>None</code> before calling it. See <a class=\"reference internal\" href=\"#default-grad-layouts\"><span class=\"std std-ref\">Default gradient layouts</span></a> for details on the memory layout of accumulated gradients.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Using this method with <code>create_graph=True</code> will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using <code>autograd.grad</code> when creating the graph to avoid this. If you have to use this function, make sure to reset the <code>.grad</code> fields of your parameters to <code>None</code> after use to break the cycle and avoid the leak.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you run any forward ops, create <code>grad_tensors</code>, and/or call <code>backward</code> in a user-specified CUDA stream context, see <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#bwd-cuda-stream-semantics\"><span class=\"std std-ref\">Stream semantics of backward passes</span></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensors</strong> (<em>sequence of Tensor</em>) – Tensors of which the derivative will be computed.</li> <li>\n<strong>grad_tensors</strong> (<em>sequence of</em><em> (</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/constants.html#None\" title=\"(in Python v3.9)\">None</a><em>)</em>) – The “vector” in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional.</li> <li>\n<strong>retain_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to <code>True</code> is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to <code>False</code>.</li> <li>\n<strong>inputs</strong> (<em>sequence of Tensor</em>) – Inputs w.r.t. which the gradient will be accumulated into <code>.grad</code>. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.grad\">\n<code>torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd.html#grad\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes and returns the sum of gradients of outputs w.r.t. the inputs.</p> <p><code>grad_outputs</code> should be a sequence of length matching <code>output</code> containing the “vector” in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn’t require_grad, then the gradient can be <code>None</code>).</p> <p>If <code>only_inputs</code> is <code>True</code>, the function will only return a list of gradients w.r.t the specified inputs. If it’s <code>False</code>, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their <code>.grad</code> attribute.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you run any forward ops, create <code>grad_outputs</code>, and/or call <code>grad</code> in a user-specified CUDA stream context, see <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#bwd-cuda-stream-semantics\"><span class=\"std std-ref\">Stream semantics of backward passes</span></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>outputs</strong> (<em>sequence of Tensor</em>) – outputs of the differentiated function.</li> <li>\n<strong>inputs</strong> (<em>sequence of Tensor</em>) – Inputs w.r.t. which the gradient will be returned (and not accumulated into <code>.grad</code>).</li> <li>\n<strong>grad_outputs</strong> (<em>sequence of Tensor</em>) – The “vector” in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None.</li> <li>\n<strong>retain_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to <code>True</code> is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: <code>False</code>.</li> <li>\n<strong>allow_unused</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to <code>False</code>.</li> </ul> </dd> </dl> </dd>\n</dl>  <h2 id=\"functional-api\">Functional higher level API</h2> <div class=\"admonition warning\" id=\"functional-higher-level-api\"> <p class=\"admonition-title\">Warning</p> <p>This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.</p> </div> <p>This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.</p> <p>This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function <code>f</code> that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as <code>f(input, constant, flag=flag)</code> you can use it as <code>functional.jacobian(lambda x: f(x, constant, flag=flag), input)</code>.</p> <dl class=\"function\"> <dt id=\"torch.autograd.functional.jacobian\">\n<code>torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#jacobian\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the Jacobian of a given function.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the Jacobian will be computed in a differentiable manner. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> <li>\n<strong>vectorize</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – This feature is experimental, please use at your own risk. When computing the jacobian, usually we invoke <code>autograd.grad</code> once per row of the jacobian. If this flag is <code>True</code>, we use the vmap prototype feature as the backend to vectorize calls to <code>autograd.grad</code> so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use <code>torch._C._debug_only_display_vmap_fallback_warnings(True)</code> to show any performance warnings and file us issues if warnings exist for your use case. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where <code>Jacobian[i][j]</code> will contain the Jacobian of the <code>i</code>th output and <code>j</code>th input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>Jacobian (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a> or nested tuple of Tensors)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n&gt;&gt;&gt; inputs = torch.rand(2, 2)\n&gt;&gt;&gt; jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=&lt;ViewBackward&gt;)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; def exp_adder(x, y):\n...   return 2 * x.exp() + 3 * y\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.functional.hessian\">\n<code>torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#hessian\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the Hessian of a given scalar function.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor with a single element.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the Hessian will be computed in a differentiable manner. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> <li>\n<strong>vectorize</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – This feature is experimental, please use at your own risk. When computing the hessian, usually we invoke <code>autograd.grad</code> once per row of the hessian. If this flag is <code>True</code>, we use the vmap prototype feature as the backend to vectorize calls to <code>autograd.grad</code> so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use <code>torch._C._debug_only_display_vmap_fallback_warnings(True)</code> to show any performance warnings and file us issues if warnings exist for your use case. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where <code>Hessian[i][j]</code> will contain the Hessian of the <code>i</code>th input and <code>j</code>th input with size the sum of the size of the <code>i</code>th input plus the size of the <code>j</code>th input. <code>Hessian[i][j]</code> will have the same dtype and device as the corresponding <code>i</code>th input.</p> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>Hessian (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a> or a tuple of tuple of Tensors)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def pow_reducer(x):\n...   return x.pow(3).sum()\n&gt;&gt;&gt; inputs = torch.rand(2, 2)\n&gt;&gt;&gt; hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=&lt;ViewBackward&gt;)\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.functional.vjp\">\n<code>torch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#vjp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the dot product between a vector <code>v</code> and the Jacobian of the given function at the point given by the inputs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>v</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The vector for which the vector Jacobian product is computed. Must be the same size as the output of <code>func</code>. This argument is optional when the output of <code>func</code> contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n\n<dl> <dt>tuple with:</dt>\n<dd>\n<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.</p> </dd> </dl> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n&gt;&gt;&gt; inputs = torch.rand(4, 4)\n&gt;&gt;&gt; v = torch.ones(4)\n&gt;&gt;&gt; vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=&lt;SumBackward1&gt;),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=&lt;MulBackward0&gt;))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; def adder(x, y):\n...   return 2 * x + 3 * y\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; v = torch.ones(2)\n&gt;&gt;&gt; vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.functional.jvp\">\n<code>torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#jvp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector <code>v</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>v</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The vector for which the Jacobian vector product is computed. Must be the same size as the input of <code>func</code>. This argument is optional when the input to <code>func</code> contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n\n<dl> <dt>tuple with:</dt>\n<dd>\n<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.</p> </dd> </dl> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n&gt;&gt;&gt; inputs = torch.rand(4, 4)\n&gt;&gt;&gt; v = torch.ones(4, 4)\n&gt;&gt;&gt; jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SumBackward1&gt;),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SqueezeBackward1&gt;))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; def adder(x, y):\n...   return 2 * x + 3 * y\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; v = (torch.ones(2), torch.ones(2))\n&gt;&gt;&gt; jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n</pre> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don’t have support for forward mode AD in PyTorch at the moment.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.functional.vhp\">\n<code>torch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#vhp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the dot product between a vector <code>v</code> and the Hessian of a given scalar function at the point given by the inputs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor with a single element.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>v</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The vector for which the vector Hessian product is computed. Must be the same size as the input of <code>func</code>. This argument is optional when <code>func</code>’s input contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n\n<dl> <dt>tuple with:</dt>\n<dd>\n<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.</p> </dd> </dl> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def pow_reducer(x):\n...   return x.pow(3).sum()\n&gt;&gt;&gt; inputs = torch.rand(2, 2)\n&gt;&gt;&gt; v = torch.ones(2, 2)\n&gt;&gt;&gt; vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n&gt;&gt;&gt; vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=&lt;SumBackward0&gt;),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=&lt;MulBackward0&gt;))\n&gt;&gt;&gt; def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; v = (torch.zeros(2), torch.ones(2))\n&gt;&gt;&gt; vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.functional.hvp\">\n<code>torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/functional.html#hvp\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Function that computes the dot product between the Hessian of a given scalar function and a vector <code>v</code> at the point given by the inputs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor with a single element.</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>\n<strong>v</strong> (<em>tuple of Tensors</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – The vector for which the Hessian vector product is computed. Must be the same size as the input of <code>func</code>. This argument is optional when <code>func</code>’s input contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>\n<strong>strict</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n\n<dl> <dt>tuple with:</dt>\n<dd>\n<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.</p> </dd> </dl> </dd> <dt class=\"field-odd\">Return type</dt> <dd class=\"field-odd\">\n<p>output (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#tuple\" title=\"(in Python v3.9)\">tuple</a>)</p> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; def pow_reducer(x):\n...   return x.pow(3).sum()\n&gt;&gt;&gt; inputs = torch.rand(2, 2)\n&gt;&gt;&gt; v = torch.ones(2, 2)\n&gt;&gt;&gt; hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=&lt;SumBackward0&gt;),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=&lt;MulBackward0&gt;))\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))\n&gt;&gt;&gt; v = (torch.zeros(2), torch.ones(2))\n&gt;&gt;&gt; hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n</pre> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>This function is significantly slower than <code>vhp</code> due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.</p> </div> </dd>\n</dl>   <h2 id=\"locally-disable-grad\">Locally disabling gradient computation</h2> <dl class=\"class\" id=\"locally-disabling-gradient-computation\"> <dt id=\"torch.autograd.no_grad\">\n<code>class torch.autograd.no_grad</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/grad_mode.html#no_grad\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that disabled gradient calculation.</p> <p>Disabling gradient calculation is useful for inference, when you are sure that you will not call <code>Tensor.backward()</code>. It will reduce memory consumption for computations that would otherwise have <code>requires_grad=True</code>.</p> <p>In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <p>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...   y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n&gt;&gt;&gt; @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n&gt;&gt;&gt; z = doubler(x)\n&gt;&gt;&gt; z.requires_grad\nFalse\n</pre> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.autograd.enable_grad\">\n<code>class torch.autograd.enable_grad</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/grad_mode.html#enable_grad\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that enables gradient calculation.</p> <p>Enables gradient calculation, if it has been disabled via <a class=\"reference internal\" href=\"#torch.autograd.no_grad\" title=\"torch.autograd.no_grad\"><code>no_grad</code></a> or <a class=\"reference internal\" href=\"#torch.autograd.set_grad_enabled\" title=\"torch.autograd.set_grad_enabled\"><code>set_grad_enabled</code></a>.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <p>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n&gt;&gt;&gt; y.backward()\n&gt;&gt;&gt; x.grad\n&gt;&gt;&gt; @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n&gt;&gt;&gt; with torch.no_grad():\n...     z = doubler(x)\n&gt;&gt;&gt; z.requires_grad\nTrue\n</pre> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.autograd.set_grad_enabled\">\n<code>class torch.autograd.set_grad_enabled(mode)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/grad_mode.html#set_grad_enabled\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that sets gradient calculation to on or off.</p> <p><code>set_grad_enabled</code> will enable or disable grads based on its argument <code>mode</code>. It can be used as a context-manager or as a function.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – Flag whether to enable grad (<code>True</code>), or disable (<code>False</code>). This can be used to conditionally enable gradients.</p> </dd> </dl> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)\n&gt;&gt;&gt; is_train = False\n&gt;&gt;&gt; with torch.set_grad_enabled(is_train):\n...   y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n&gt;&gt;&gt; torch.set_grad_enabled(True)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nTrue\n&gt;&gt;&gt; torch.set_grad_enabled(False)\n&gt;&gt;&gt; y = x * 2\n&gt;&gt;&gt; y.requires_grad\nFalse\n</pre> </dd>\n</dl>   <h2 id=\"default-grad-layouts\">Default gradient layouts</h2> <p id=\"default-gradient-layouts\">When a non-sparse <code>param</code> receives a non-sparse gradient during <a class=\"reference internal\" href=\"#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>torch.autograd.backward()</code></a> or <a class=\"reference internal\" href=\"#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>torch.Tensor.backward()</code></a> <code>param.grad</code> is accumulated as follows.</p> <p>If <code>param.grad</code> is initially <code>None</code>:</p> <ol class=\"arabic simple\"> <li>If <code>param</code>’s memory is non-overlapping and dense, <code>.grad</code> is created with strides matching <code>param</code> (thus matching <code>param</code>’s layout).</li> <li>Otherwise, <code>.grad</code> is created with rowmajor-contiguous strides.</li> </ol> <p>If <code>param</code> already has a non-sparse <code>.grad</code> attribute:</p> <ol class=\"arabic simple\" start=\"3\"> <li>If <code>create_graph=False</code>, <code>backward()</code> accumulates into <code>.grad</code> in-place, which preserves its strides.</li> <li>If <code>create_graph=True</code>, <code>backward()</code> replaces <code>.grad</code> with a new tensor <code>.grad + new grad</code>, which attempts (but does not guarantee) matching the preexisting <code>.grad</code>’s strides.</li> </ol> <p>The default behavior (letting <code>.grad</code>s be <code>None</code> before the first <code>backward()</code>, such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to <code>model.zero_grad()</code> or <code>optimizer.zero_grad()</code> will not affect <code>.grad</code> layouts.</p> <p>In fact, resetting all <code>.grad</code>s to <code>None</code> before each accumulation phase, e.g.:</p> <pre data-language=\"python\">for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n</pre> <p>such that they’re recreated according to 1 or 2 every time, is a valid alternative to <code>model.zero_grad()</code> or <code>optimizer.zero_grad()</code> that may improve performance for some networks.</p>  <h3 id=\"manual-gradient-layouts\">Manual gradient layouts</h3> <p>If you need manual control over <code>.grad</code>’s strides, assign <code>param.grad =</code> a zeroed tensor with desired strides before the first <code>backward()</code>, and never reset it to <code>None</code>. 3 guarantees your layout is preserved as long as <code>create_graph=False</code>. 4 indicates your layout is <em>likely</em> preserved even if <code>create_graph=True</code>.</p>    <h2 id=\"in-place-operations-on-tensors\">In-place operations on Tensors</h2> <p>Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.</p>  <h3 id=\"in-place-correctness-checks\">In-place correctness checks</h3> <p>All <code>Tensor</code> s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.</p>    <h2 id=\"variable-deprecated\">Variable (deprecated)</h2> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with <code>requires_grad</code> set to <code>True</code>. Below please find a quick guide on what has changed:</p> <ul class=\"simple\"> <li>\n<code>Variable(tensor)</code> and <code>Variable(tensor, requires_grad)</code> still work as expected, but they return Tensors instead of Variables.</li> <li>\n<code>var.data</code> is the same thing as <code>tensor.data</code>.</li> <li>Methods such as <code>var.backward(), var.detach(), var.register_hook()</code> now work on tensors with the same method names.</li> </ul> <p>In addition, one can now create tensors with <code>requires_grad=True</code> using factory methods such as <a class=\"reference internal\" href=\"generated/torch.randn#torch.randn\" title=\"torch.randn\"><code>torch.randn()</code></a>, <a class=\"reference internal\" href=\"generated/torch.zeros#torch.zeros\" title=\"torch.zeros\"><code>torch.zeros()</code></a>, <a class=\"reference internal\" href=\"generated/torch.ones#torch.ones\" title=\"torch.ones\"><code>torch.ones()</code></a>, and others like the following:</p> <p><code>autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)</code></p> </div>   <h2 id=\"tensor-autograd-functions\">Tensor autograd functions</h2> <dl class=\"class\"> <dt>\n<code>class torch.Tensor</code> </dt> <dd>\n<dl class=\"attribute\"> <dt id=\"torch.Tensor.grad\">\n<code>grad</code> </dt> <dd>\n<p>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <a class=\"reference internal\" href=\"#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a> computes gradients for <code>self</code>. The attribute will then contain the gradients computed and future calls to <a class=\"reference internal\" href=\"#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a> will accumulate (add) gradients into it.</p> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.requires_grad\">\n<code>requires_grad</code> </dt> <dd>\n<p>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The fact that gradients need to be computed for a Tensor do not mean that the <a class=\"reference internal\" href=\"#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> attribute will be populated, see <a class=\"reference internal\" href=\"#torch.Tensor.is_leaf\" title=\"torch.Tensor.is_leaf\"><code>is_leaf</code></a> for more details.</p> </div> </dd>\n</dl> <dl class=\"attribute\"> <dt id=\"torch.Tensor.is_leaf\">\n<code>is_leaf</code> </dt> <dd>\n<p>All Tensors that have <a class=\"reference internal\" href=\"#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>requires_grad</code></a> which is <code>False</code> will be leaf Tensors by convention.</p> <p>For Tensors that have <a class=\"reference internal\" href=\"#torch.Tensor.requires_grad\" title=\"torch.Tensor.requires_grad\"><code>requires_grad</code></a> which is <code>True</code>, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so <code>grad_fn</code> is None.</p> <p>Only leaf Tensors will have their <a class=\"reference internal\" href=\"#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> populated during a call to <a class=\"reference internal\" href=\"#torch.Tensor.backward\" title=\"torch.Tensor.backward\"><code>backward()</code></a>. To get <a class=\"reference internal\" href=\"#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a> populated for non-leaf Tensors, you can use <a class=\"reference internal\" href=\"#torch.Tensor.retain_grad\" title=\"torch.Tensor.retain_grad\"><code>retain_grad()</code></a>.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; a = torch.rand(10, requires_grad=True)\n&gt;&gt;&gt; a.is_leaf\nTrue\n&gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda()\n&gt;&gt;&gt; b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n&gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2\n&gt;&gt;&gt; c.is_leaf\nFalse\n# c was created by the addition operation\n&gt;&gt;&gt; d = torch.rand(10).cuda()\n&gt;&gt;&gt; d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n&gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_()\n&gt;&gt;&gt; e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n&gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=\"cuda\")\n&gt;&gt;&gt; f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.backward\">\n<code>backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.backward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Computes the gradient of current tensor w.r.t. graph leaves.</p> <p>The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying <code>gradient</code>. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. <code>self</code>.</p> <p>This function accumulates gradients in the leaves - you might need to zero <code>.grad</code> attributes or set them to <code>None</code> before calling it. See <a class=\"reference internal\" href=\"#default-grad-layouts\"><span class=\"std std-ref\">Default gradient layouts</span></a> for details on the memory layout of accumulated gradients.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>If you run any forward ops, create <code>gradient</code>, and/or call <code>backward</code> in a user-specified CUDA stream context, see <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/cuda.html#bwd-cuda-stream-semantics\"><span class=\"std std-ref\">Stream semantics of backward passes</span></a>.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>gradient</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em> or </em><a class=\"reference external\" href=\"https://docs.python.org/3/library/constants.html#None\" title=\"(in Python v3.9)\">None</a>) – Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless <code>create_graph</code> is True. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable then this argument is optional.</li> <li>\n<strong>retain_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li> <li>\n<strong>create_graph</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to <code>False</code>.</li> <li>\n<strong>inputs</strong> (<em>sequence of Tensor</em>) – Inputs w.r.t. which the gradient will be accumulated into <code>.grad</code>. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.detach\">\n<code>detach()</code> </dt> <dd>\n<p>Returns a new Tensor, detached from the current graph.</p> <p>The result will never require gradient.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as <code>resize_</code> / <code>resize_as_</code> / <code>set_</code> / <code>transpose_</code>) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as <code>zero_</code> / <code>copy_</code> / <code>add_</code>) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.</p> </div> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.detach_\">\n<code>detach_()</code> </dt> <dd>\n<p>Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.register_hook\">\n<code>register_hook(hook)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.register_hook\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Registers a backward hook.</p> <p>The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature:</p> <pre data-language=\"python\">hook(grad) -&gt; Tensor or None\n</pre> <p>The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of <a class=\"reference internal\" href=\"#torch.Tensor.grad\" title=\"torch.Tensor.grad\"><code>grad</code></a>.</p> <p>This function returns a handle with a method <code>handle.remove()</code> that removes the hook from the module.</p> <p>Example:</p> <pre data-language=\"python\">&gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)\n&gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n&gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))\n&gt;&gt;&gt; v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n&gt;&gt;&gt; h.remove()  # removes the hook\n</pre> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.Tensor.retain_grad\">\n<code>retain_grad()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/tensor.html#Tensor.retain_grad\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Enables .grad attribute for non-leaf Tensors.</p> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"function\"><span class=\"hidden-section\">Function</span></h2> <dl class=\"class\"> <dt id=\"torch.autograd.Function\">\n<code>class torch.autograd.Function</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#Function\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Records operation history and defines formulas for differentiating ops.</p> <p>See the Note on extending the autograd engine for more details on how to use this class: <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd\">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p> <p>Every operation performed on <code>Tensor</code> s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is called, the graph is processed in the topological ordering, by calling <a class=\"reference internal\" href=\"#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward()</code></a> methods of each <a class=\"reference internal\" href=\"#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>Function</code></a> object, and passing returned gradients on to next <a class=\"reference internal\" href=\"#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>Function</code></a> s.</p> <p>Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd.</p> <p>Examples:</p> <pre data-language=\"python\">&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; #Use it by calling the apply method:\n&gt;&gt;&gt; output = Exp.apply(input)\n</pre> <dl class=\"method\"> <dt id=\"torch.autograd.Function.backward\">\n<code>static backward(ctx, *grad_outputs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#Function.backward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Defines a formula for differentiating the operation.</p> <p>This function is to be overridden by all subclasses.</p> <p>It must accept a context <code>ctx</code> as the first argument, followed by as many outputs did <a class=\"reference internal\" href=\"#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>forward()</code></a> return, and it should return as many tensors, as there were inputs to <a class=\"reference internal\" href=\"#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>forward()</code></a>. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.</p> <p>The context can be used to retrieve tensors saved during the forward pass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple of booleans representing whether each input needs gradient. E.g., <a class=\"reference internal\" href=\"#torch.autograd.backward\" title=\"torch.autograd.backward\"><code>backward()</code></a> will have <code>ctx.needs_input_grad[0] = True</code> if the first input to <a class=\"reference internal\" href=\"#torch.autograd.Function.forward\" title=\"torch.autograd.Function.forward\"><code>forward()</code></a> needs gradient computated w.r.t. the output.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.Function.forward\">\n<code>static forward(ctx, *args, **kwargs)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#Function.forward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Performs the operation.</p> <p>This function is to be overridden by all subclasses.</p> <p>It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types).</p> <p>The context can be used to store tensors that can be then retrieved during the backward pass.</p> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"context-method-mixins\">Context method mixins</h2> <p>When creating a new <a class=\"reference internal\" href=\"#torch.autograd.Function\" title=\"torch.autograd.Function\"><code>Function</code></a>, the following methods are available to <code>ctx</code>.</p> <dl class=\"class\"> <dt id=\"torch.autograd.function._ContextMethodMixin\">\n<code>class torch.autograd.function._ContextMethodMixin</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#_ContextMethodMixin\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<dl class=\"method\"> <dt id=\"torch.autograd.function._ContextMethodMixin.mark_dirty\">\n<code>mark_dirty(*args)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_dirty\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Marks given tensors as modified in an in-place operation.</p> <p><strong>This should be called at most once, only from inside the</strong> <code>forward()</code> <strong>method, and all arguments should be inputs.</strong></p> <p>Every tensor that’s been modified in-place in a call to <code>forward()</code> should be given to this function, to ensure correctness of our checks. It doesn’t matter whether the function is called before or after modification.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.function._ContextMethodMixin.mark_non_differentiable\">\n<code>mark_non_differentiable(*args)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#_ContextMethodMixin.mark_non_differentiable\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Marks outputs as non-differentiable.</p> <p><strong>This should be called at most once, only from inside the</strong> <code>forward()</code> <strong>method, and all arguments should be outputs.</strong></p> <p>This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in <code>backward()</code>, but it’s always going to be a zero tensor with the same shape as the shape of a corresponding output.</p> <p>This is used e.g. for indices returned from a max <code>Function</code>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.function._ContextMethodMixin.save_for_backward\">\n<code>save_for_backward(*tensors)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#_ContextMethodMixin.save_for_backward\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Saves given tensors for a future call to <code>backward()</code>.</p> <p><strong>This should be called at most once, and only from inside the</strong> <code>forward()</code> <strong>method.</strong></p> <p>Later, saved tensors can be accessed through the <code>saved_tensors</code> attribute. Before returning them to the user, a check is made to ensure they weren’t used in any in-place operation that modified their content.</p> <p>Arguments can also be <code>None</code>.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.function._ContextMethodMixin.set_materialize_grads\">\n<code>set_materialize_grads(value)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/function.html#_ContextMethodMixin.set_materialize_grads\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sets whether to materialize output grad tensors. Default is true.</p> <p><strong>This should be called only from inside the</strong> <code>forward()</code> <strong>method</strong></p> <p>If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the <code>backward()</code> method.</p> </dd>\n</dl> </dd>\n</dl>   <h2 id=\"grad-check\">Numerical gradient checking</h2> <dl class=\"function\" id=\"numerical-gradient-checking\"> <dt id=\"torch.autograd.gradcheck\">\n<code>torch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=False, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/gradcheck.html#gradcheck\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in <code>inputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p> <p>The check between numerical and analytical gradients uses <a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>allclose()</code></a>.</p> <p>For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of <code>grad_output</code>: 1 and 1j. For more details, check out <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/notes/autograd.html#complex-autograd-doc\"><span class=\"std std-ref\">Autograd for Complex Numbers</span></a>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The default values are designed for <code>input</code> of double precision. This check will likely fail if <code>input</code> is of less precision, e.g., <code>FloatTensor</code>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>If any checked tensor in <code>input</code> has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from <code>torch.expand()</code>), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensor</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function</li> <li>\n<strong>eps</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – perturbation for finite differences</li> <li>\n<strong>atol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – absolute tolerance</li> <li>\n<strong>rtol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – relative tolerance</li> <li>\n<strong>raise_exception</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks.</li> <li>\n<strong>check_sparse_nnz</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only.</li> <li>\n<strong>nondet_tol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.</li> <li>\n<strong>check_undefined_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if True, check if undefined output grads are supported and treated as zeros, for <code>Tensor</code> outputs.</li> <li>\n<strong>check_batched_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>True if all differences satisfy allclose condition</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.gradgradcheck\">\n<code>torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/gradcheck.html#gradgradcheck\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in <code>inputs</code> and <code>grad_outputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p> <p>This function checks that backpropagating through the gradients computed to the given <code>grad_outputs</code> are correct.</p> <p>The check between numerical and analytical gradients uses <a class=\"reference internal\" href=\"generated/torch.allclose#torch.allclose\" title=\"torch.allclose\"><code>allclose()</code></a>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The default values are designed for <code>input</code> and <code>grad_outputs</code> of double precision. This check will likely fail if they are of less precision, e.g., <code>FloatTensor</code>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>If any checked tensor in <code>input</code> and <code>grad_outputs</code> has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from <code>torch.expand()</code>), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors</li> <li>\n<strong>inputs</strong> (<em>tuple of Tensor</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – inputs to the function</li> <li>\n<strong>grad_outputs</strong> (<em>tuple of Tensor</em><em> or </em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>, </em><em>optional</em>) – The gradients with respect to the function’s outputs.</li> <li>\n<strong>eps</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – perturbation for finite differences</li> <li>\n<strong>atol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – absolute tolerance</li> <li>\n<strong>rtol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – relative tolerance</li> <li>\n<strong>gen_non_contig_grad_outputs</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if <code>grad_outputs</code> is <code>None</code> and <code>gen_non_contig_grad_outputs</code> is <code>True</code>, the randomly generated gradient outputs are made to be noncontiguous</li> <li>\n<strong>raise_exception</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks.</li> <li>\n<strong>nondet_tol</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#float\" title=\"(in Python v3.9)\">float</a><em>, </em><em>optional</em>) – tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative.</li> <li>\n<strong>check_undefined_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if True, check if undefined output grads are supported and treated as zeros</li> <li>\n<strong>check_batched_grad</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>True if all differences satisfy allclose condition</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"profiler\">Profiler</h2> <p>Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using <a class=\"reference internal\" href=\"#torch.autograd.profiler.profile\" title=\"torch.autograd.profiler.profile\"><code>profile</code></a>. and nvprof based (registers both CPU and GPU activity) using <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a>.</p> <dl class=\"class\"> <dt id=\"torch.autograd.profiler.profile\">\n<code>class torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, use_kineto=False, use_cpu=True)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#profile\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Setting this to False makes this context manager a no-op.</li> <li>\n<strong>use_cuda</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation.</li> <li>\n<strong>record_shapes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.</li> <li>\n<strong>with_flops</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator’s input shape and total time. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.</li> <li>\n<strong>profile_memory</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – track tensor memory allocation/deallocation.</li> <li>\n<strong>with_stack</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – record source information (file and line number) for the ops.</li> <li>\n<strong>use_kineto</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – experimental, enable profiling with Kineto profiler.</li> <li>\n<strong>use_cpu</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – profile CPU events; setting to <code>False</code> requires <code>use_kineto=True</code> and can be used to lower the overhead for GPU-only profiling.</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; x = torch.randn((1, 1), requires_grad=True)\n&gt;&gt;&gt; with torch.autograd.profiler.profile() as prof:\n&gt;&gt;&gt;     for _ in range(100):  # any normal python code, really!\n&gt;&gt;&gt;         y = x ** 2\n&gt;&gt;          y.backward()\n&gt;&gt;&gt; # NOTE: some columns were removed for brevity\n&gt;&gt;&gt; print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n</pre> <dl class=\"method\"> <dt id=\"torch.autograd.profiler.profile.export_chrome_trace\">\n<code>export_chrome_trace(path)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#profile.export_chrome_trace\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Exports an EventList as a Chrome tracing tools file.</p> <p>The checkpoint can be later loaded and inspected under <code>chrome://tracing</code> URL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>path</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – Path where the trace will be written.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.profiler.profile.key_averages\">\n<code>key_averages(group_by_input_shape=False, group_by_stack_n=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#profile.key_averages\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Averages all function events over their keys.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>group_by_input_shapes</strong> – group entries by</li> <li>\n<strong>name, input shapes) rather than just event name.</strong> (<em>(</em><em>event</em>) – </li> <li>\n<strong>is useful to see which input shapes contribute to the runtime</strong> (<em>This</em>) – </li> <li>\n<strong>most and may help with size-specific optimizations or</strong> (<em>the</em>) – </li> <li>\n<strong>the best candidates for quantization</strong> (<em>choosing</em>) – </li> <li>\n<strong>group_by_stack_n</strong> – group by top n stack trace entries</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>An EventList containing FunctionEventAvg objects.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.profiler.profile.self_cpu_time_total\">\n<code>property self_cpu_time_total</code> </dt> <dd>\n<p>Returns total time spent on CPU obtained as a sum of all self times across all the events.</p> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.profiler.profile.table\">\n<code>table(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#profile.table\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Prints an EventList as a nicely formatted table.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>sort_by</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em>, </em><em>optional</em>) – Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: <code>cpu_time</code>, <code>cuda_time</code>, <code>cpu_time_total</code>, <code>cuda_time_total</code>, <code>cpu_memory_usage</code>, <code>cuda_memory_usage</code>, <code>self_cpu_memory_usage</code>, <code>self_cuda_memory_usage</code>, <code>count</code>.</li> <li>\n<strong>top_level_events_only</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python <code>lstm</code>, python <code>add</code> or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A string containing the table.</p> </dd> </dl> </dd>\n</dl> <dl class=\"method\"> <dt id=\"torch.autograd.profiler.profile.total_average\">\n<code>total_average()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#profile.total_average\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Averages all events.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>A FunctionEventAvg object.</p> </dd> </dl> </dd>\n</dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.autograd.profiler.emit_nvtx\">\n<code>class torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#emit_nvtx\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context manager that makes every autograd operation emit an NVTX range.</p> <p>It is useful when running the program under nvprof:</p> <pre data-language=\"python\">nvprof --profile-from-start off -o trace_name.prof -- &lt;regular command here&gt;\n</pre> <p>Unfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or <a class=\"reference internal\" href=\"#torch.autograd.profiler.load_nvprof\" title=\"torch.autograd.profiler.load_nvprof\"><code>torch.autograd.profiler.load_nvprof()</code></a> can load the results for inspection e.g. in Python REPL.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>enabled</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em><em>, </em><em>default=True</em>) – Setting <code>enabled=False</code> makes this context manager a no-op. Default: <code>True</code>.</li> <li>\n<strong>record_shapes</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If <code>record_shapes=True</code>, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: <code>[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]</code> Non-tensor arguments will be represented by <code>[]</code>. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.</li> </ul> </dd> </dl> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n</pre> <p><strong>Forward-backward correlation</strong></p> <p>When viewing a profile created using <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a> in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, <a class=\"reference internal\" href=\"#torch.autograd.profiler.emit_nvtx\" title=\"torch.autograd.profiler.emit_nvtx\"><code>emit_nvtx</code></a> appends sequence number information to the ranges it generates.</p> <p>During the forward pass, each function range is decorated with <code>seq=&lt;N&gt;</code>. <code>seq</code> is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the <code>seq=&lt;N&gt;</code> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s <code>apply()</code> call is decorated with <code>stashed seq=&lt;M&gt;</code>. <code>M</code> is the sequence number that the backward object was created with. By comparing <code>stashed seq</code> numbers in backward with <code>seq</code> numbers in forward, you can track down which forward op created each backward Function.</p> <p>Any functions executed during the backward pass are also decorated with <code>seq=&lt;N&gt;</code>. During default backward (with <code>create_graph=False</code>) this information is irrelevant, and in fact, <code>N</code> may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ <code>apply()</code> methods are useful, as a way to correlate these Function objects with the earlier forward pass.</p> <p><strong>Double-backward</strong></p> <p>If, on the other hand, a backward pass with <code>create_graph=True</code> is underway (in other words, if you are setting up for a double-backward), each function’s execution during backward is given a nonzero, useful <code>seq=&lt;N&gt;</code>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ <code>apply()</code> ranges are still tagged with <code>stashed seq</code> numbers, which can be compared to <code>seq</code> numbers from the backward pass.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.autograd.profiler.load_nvprof\">\n<code>torch.autograd.profiler.load_nvprof(path)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/profiler.html#load_nvprof\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Opens an nvprof trace file and parses autograd annotations.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>path</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – path to nvprof trace</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"anomaly-detection\">Anomaly detection</h2> <dl class=\"class\"> <dt id=\"torch.autograd.detect_anomaly\">\n<code>class torch.autograd.detect_anomaly</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/anomaly_mode.html#detect_anomaly\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that enable anomaly detection for the autograd engine.</p> <p>This does two things:</p> <ul class=\"simple\"> <li>Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.</li> <li>Any backward computation that generate “nan” value will raise an error.</li> </ul> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This mode should be enabled only for debugging as the different tests will slow down your program execution.</p> </div> <h4 class=\"rubric\">Example</h4> <pre data-language=\"python\">&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from torch import autograd\n&gt;&gt;&gt; class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n&gt;&gt;&gt; def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n&gt;&gt;&gt; inp = torch.rand(10, 10, requires_grad=True)\n&gt;&gt;&gt; out = run_fn(inp)\n&gt;&gt;&gt; out.backward()\n    Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"&lt;stdin&gt;\", line 8, in backward\n    RuntimeError: Some error in backward\n&gt;&gt;&gt; with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in &lt;module&gt;\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"&lt;stdin&gt;\", line 4, in &lt;module&gt;\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"&lt;stdin&gt;\", line 8, in backward\n    RuntimeError: Some error in backward\n</pre> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.autograd.set_detect_anomaly\">\n<code>class torch.autograd.set_detect_anomaly(mode)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/autograd/anomaly_mode.html#set_detect_anomaly\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Context-manager that sets the anomaly detection for the autograd engine on or off.</p> <p><code>set_detect_anomaly</code> will enable or disable the autograd anomaly detection based on its argument <code>mode</code>. It can be used as a context-manager or as a function.</p> <p>See <code>detect_anomaly</code> above for details of the anomaly detection behaviour.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>mode</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – Flag whether to enable anomaly detection (<code>True</code>), or disable (<code>False</code>).</p> </dd> </dl> </dd>\n</dl><div class=\"_attribution\">\n  <p class=\"_attribution-p\">\n    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>\n    <a href=\"https://pytorch.org/docs/1.8.0/autograd.html\" class=\"_attribution-link\">https://pytorch.org/docs/1.8.0/autograd.html</a>\n  </p>\n</div>\n","distributed":"<h1 id=\"distributed-communication-package-torch-distributed\">Distributed communication package - torch.distributed</h1> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Please refer to <a class=\"reference external\" href=\"https://pytorch.org/tutorials/beginner/dist_overview.html\">PyTorch Distributed Overview</a> for a brief introduction to all features related to distributed training.</p> </div>  <h2 id=\"backends\">Backends</h2> <p><code>torch.distributed</code> supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.</p> <table class=\"docutils colwidths-auto align-default\"> <thead> <tr>\n<th class=\"head\"><p>Backend</p></th> <th class=\"head\" colspan=\"2\"><p><code>gloo</code></p></th> <th class=\"head\" colspan=\"2\"><p><code>mpi</code></p></th> <th class=\"head\" colspan=\"2\"><p><code>nccl</code></p></th> </tr> <tr>\n<th class=\"head\"><p>Device</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> <th class=\"head\"><p>CPU</p></th> <th class=\"head\"><p>GPU</p></th> </tr> </thead>  <tr>\n<td><p>send</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✘</p></td> </tr> <tr>\n<td><p>recv</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✘</p></td> </tr> <tr>\n<td><p>broadcast</p></td> <td><p>✓</p></td> <td><p>✓</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr> <tr>\n<td><p>all_reduce</p></td> <td><p>✓</p></td> <td><p>✓</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr> <tr>\n<td><p>reduce</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr> <tr>\n<td><p>all_gather</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr> <tr>\n<td><p>gather</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✘</p></td> </tr> <tr>\n<td><p>scatter</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✘</p></td> </tr> <tr>\n<td><p>reduce_scatter</p></td> <td><p>✘</p></td> <td><p>✘</p></td> <td><p>✘</p></td> <td><p>✘</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr> <tr>\n<td><p>all_to_all</p></td> <td><p>✘</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✘</p></td> </tr> <tr>\n<td><p>barrier</p></td> <td><p>✓</p></td> <td><p>✘</p></td> <td><p>✓</p></td> <td><p>?</p></td> <td><p>✘</p></td> <td><p>✓</p></td> </tr>  </table>  <h3 id=\"backends-that-come-with-pytorch\">Backends that come with PyTorch</h3> <p>PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the <code>init_method</code> argument of <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> points to a file it must adhere to the following schema:</p> <ul class=\"simple\"> <li>Local file system, <code>init_method=\"file:///d:/tmp/some_file\"</code>\n</li> <li>Shared file system, <code>init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"</code>\n</li> </ul> <p>Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.</p> </div>   <h3 id=\"which-backend-to-use\">Which backend to use?</h3> <p>In the past, we were often asked: “which backend should I use?”.</p> <ul class=\"simple\"> <li>\n<p>Rule of thumb</p> <ul> <li>Use the NCCL backend for distributed <strong>GPU</strong> training</li> <li>Use the Gloo backend for distributed <strong>CPU</strong> training.</li> </ul> </li> <li>\n<p>GPU hosts with InfiniBand interconnect</p> <ul> <li>Use NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect.</li> </ul> </li> <li>\n<p>GPU hosts with Ethernet interconnect</p> <ul> <li>Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)</li> </ul> </li> <li>\n<p>CPU hosts with InfiniBand interconnect</p> <ul> <li>If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.</li> </ul> </li> <li>\n<p>CPU hosts with Ethernet interconnect</p> <ul> <li>Use Gloo, unless you have specific reasons to use MPI.</li> </ul> </li> </ul>   <h3 id=\"common-environment-variables\">Common environment variables</h3>  <h4 id=\"choosing-the-network-interface-to-use\">Choosing the network interface to use</h4> <p>By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):</p> <ul class=\"simple\"> <li>\n<strong>NCCL_SOCKET_IFNAME</strong>, for example <code>export NCCL_SOCKET_IFNAME=eth0</code>\n</li> <li>\n<strong>GLOO_SOCKET_IFNAME</strong>, for example <code>export GLOO_SOCKET_IFNAME=eth0</code>\n</li> </ul> <p>If you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: <code>export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3</code>. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.</p>   <h4 id=\"other-nccl-environment-variables\">Other NCCL environment variables</h4> <p>NCCL has also provided a number of environment variables for fine-tuning purposes.</p> <p>Commonly used ones include the following for debugging purposes:</p> <ul class=\"simple\"> <li><code>export NCCL_DEBUG=INFO</code></li> <li><code>export NCCL_DEBUG_SUBSYS=ALL</code></li> </ul> <p>For the full list of NCCL environment variables, please refer to <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/docs/env.html\">NVIDIA NCCL’s official documentation</a></p>     <h2 id=\"distributed-basics\">Basics</h2> <p id=\"basics\">The <code>torch.distributed</code> package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by <a class=\"reference internal\" href=\"multiprocessing\"><span class=\"doc\">Multiprocessing package - torch.multiprocessing</span></a> and <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel()</code></a> in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.</p> <p>In the single-machine synchronous case, <code>torch.distributed</code> or the <a class=\"reference internal\" href=\"generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel\" title=\"torch.nn.parallel.DistributedDataParallel\"><code>torch.nn.parallel.DistributedDataParallel()</code></a> wrapper may still have advantages over other approaches to data-parallelism, including <a class=\"reference internal\" href=\"generated/torch.nn.dataparallel#torch.nn.DataParallel\" title=\"torch.nn.DataParallel\"><code>torch.nn.DataParallel()</code></a>:</p> <ul class=\"simple\"> <li>Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.</li> <li>Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.</li> </ul>   <h2 id=\"initialization\">Initialization</h2> <p>The package needs to be initialized using the <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> function before calling any other methods. This blocks until all processes have joined.</p> <dl class=\"function\"> <dt id=\"torch.distributed.is_available\">\n<code>torch.distributed.is_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed.html#is_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns <code>True</code> if the distributed package is available. Otherwise, <code>torch.distributed</code> does not expose any other APIs. Currently, <code>torch.distributed</code> is available on Linux, MacOS and Windows. Set <code>USE_DISTRIBUTED=1</code> to enable it when building PyTorch from source. Currently, the default value is <code>USE_DISTRIBUTED=1</code> for Linux and Windows, <code>USE_DISTRIBUTED=0</code> for MacOS.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.init_process_group\">\n<code>torch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='')</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#init_process_group\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Initializes the default distributed process group, and this will also initialize the distributed package.</p> <dl class=\"simple\"> <dt>There are 2 main ways to initialize a process group:</dt>\n<dd>\n<ol class=\"arabic simple\"> <li>Specify <code>store</code>, <code>rank</code>, and <code>world_size</code> explicitly.</li> <li>Specify <code>init_method</code> (a URL string) which indicates where/how to discover peers. Optionally specify <code>rank</code> and <code>world_size</code>, or encode all required parameters in the URL and omit them.</li> </ol> </dd> </dl> <p>If neither is specified, <code>init_method</code> is assumed to be “env://”.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>backend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em> or </em><a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\">Backend</a>) – The backend to use. Depending on build-time configurations, valid values include <code>mpi</code>, <code>gloo</code>, and <code>nccl</code>. This field should be given as a lowercase string (e.g., <code>\"gloo\"</code>), which can also be accessed via <a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\"><code>Backend</code></a> attributes (e.g., <code>Backend.GLOO</code>). If using multiple processes per machine with <code>nccl</code> backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks.</li> <li>\n<strong>init_method</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em>, </em><em>optional</em>) – URL specifying how to initialize the process group. Default is “env://” if no <code>init_method</code> or <code>store</code> is specified. Mutually exclusive with <code>store</code>.</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Number of processes participating in the job. Required if <code>store</code> is specified.</li> <li>\n<strong>rank</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Rank of the current process (it should be a number between 0 and <code>world_size</code>-1). Required if <code>store</code> is specified.</li> <li>\n<strong>store</strong> (<a class=\"reference internal\" href=\"#torch.distributed.Store\" title=\"torch.distributed.Store\">Store</a><em>, </em><em>optional</em>) – Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with <code>init_method</code>.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em><em>, </em><em>optional</em>) – Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the <code>gloo</code> backend. For <code>nccl</code>, this is applicable only if the environment variable <code>NCCL_BLOCKING_WAIT</code> or <code>NCCL_ASYNC_ERROR_HANDLING</code> is set to 1. When <code>NCCL_BLOCKING_WAIT</code> is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When <code>NCCL_ASYNC_ERROR_HANDLING</code> is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. <code>NCCL_BLOCKING_WAIT</code> will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, <code>NCCL_ASYNC_ERROR_HANDLING</code> has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set.</li> <li>\n<strong>group_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em>, </em><em>optional</em><em>, </em><em>deprecated</em>) – Group name.</li> </ul> </dd> </dl> <p>To enable <code>backend == Backend.MPI</code>, PyTorch needs to be built from source on a system that supports MPI.</p> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.distributed.Backend\">\n<code>class torch.distributed.Backend</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#Backend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>An enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends.</p> <p>The values of this class are lowercase strings, e.g., <code>\"gloo\"</code>. They can be accessed as attributes, e.g., <code>Backend.NCCL</code>.</p> <p>This class can be directly called to parse the string, e.g., <code>Backend(backend_str)</code> will check if <code>backend_str</code> is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., <code>Backend(\"GLOO\")</code> returns <code>\"gloo\"</code>.</p> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>The entry <code>Backend.UNDEFINED</code> is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.</p> </div> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.get_backend\">\n<code>torch.distributed.get_backend(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#get_backend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the backend of the given process group.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of <code>group</code>.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The backend of the given process group as a lower case string.</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.get_rank\">\n<code>torch.distributed.get_rank(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#get_rank\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the rank of current process group</p> <p>Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to <code>world_size</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The rank of the process group -1, if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.get_world_size\">\n<code>torch.distributed.get_world_size(group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#get_world_size\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Returns the number of processes in the current process group</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>The world size of the process group -1, if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.is_initialized\">\n<code>torch.distributed.is_initialized()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#is_initialized\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checking if the default process group has been initialized</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.is_mpi_available\">\n<code>torch.distributed.is_mpi_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#is_mpi_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if the MPI backend is available.</p> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.is_nccl_available\">\n<code>torch.distributed.is_nccl_available()</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#is_nccl_available\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Checks if the NCCL backend is available.</p> </dd>\n</dl>  <p>Currently three initialization methods are supported:</p>  <h3 id=\"tcp-initialization\">TCP initialization</h3> <p>There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired <code>world_size</code>. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.</p> <p>Note that multicast address is not supported anymore in the latest distributed package. <code>group_name</code> is deprecated as well.</p> <pre data-language=\"python\">import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n</pre>   <h3 id=\"shared-file-system-initialization\">Shared file-system initialization</h3> <p>Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired <code>world_size</code>. The URL should start with <code>file://</code> and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> call on the same file path/name.</p> <p>Note that automatic rank assignment is not supported anymore in the latest distributed package and <code>group_name</code> is deprecated as well.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This method assumes that the file system supports locking using <code>fcntl</code> - most local systems and NFS support it.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>init_process_group()</code></a> is called.</p> </div> <pre data-language=\"python\">import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n</pre>   <h3 id=\"environment-variable-initialization\">Environment variable initialization</h3> <p>This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:</p> <ul class=\"simple\"> <li>\n<code>MASTER_PORT</code> - required; has to be a free port on machine with rank 0</li> <li>\n<code>MASTER_ADDR</code> - required (except for rank 0); address of rank 0 node</li> <li>\n<code>WORLD_SIZE</code> - required; can be set either here, or in a call to init function</li> <li>\n<code>RANK</code> - required; can be set either here, or in a call to init function</li> </ul> <p>The machine with rank 0 will be used to set up all connections.</p> <p>This is the default method, meaning that <code>init_method</code> does not have to be specified (or can be <code>env://</code>).</p>    <h2 id=\"distributed-key-value-store\">Distributed Key-Value Store</h2> <p>The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in <a class=\"reference internal\" href=\"#torch.distributed.init_process_group\" title=\"torch.distributed.init_process_group\"><code>torch.distributed.init_process_group()</code></a> (by explicitly creating the store as an alternative to specifying <code>init_method</code>.) There are 3 choices for Key-Value Stores: <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>.</p> <dl class=\"class\"> <dt id=\"torch.distributed.Store\">\n<code>class torch.distributed.Store</code> </dt> <dd>\n<p>Base class for all store implementations, such as the 3 provided by PyTorch distributed: (<a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>).</p> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.distributed.TCPStore\">\n<code>class torch.distributed.TCPStore</code> </dt> <dd>\n<p>A TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as <code>set()</code> to insert a key-value pair, <code>get()</code> to retrieve a key-value pair, etc.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>host_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The hostname or IP Address the server store should run on.</li> <li>\n<strong>port</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The port on which the server store should listen for incoming requests.</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The total number of store users (number of clients + 1 for the server).</li> <li>\n<strong>is_master</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a>) – True when initializing the server store, False for client stores.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em>) – Timeout used by the store during initialization and for methods such as <code>get()</code> and <code>wait()</code>.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Run on process 1 (server)\n&gt;&gt;&gt; server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n&gt;&gt;&gt; # Run on process 2 (client)\n&gt;&gt;&gt; client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n&gt;&gt;&gt; # Use any of the store methods from either the client or server after initialization\n&gt;&gt;&gt; server_store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; client_store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.distributed.HashStore\">\n<code>class torch.distributed.HashStore</code> </dt> <dd>\n<p>A thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.</p> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; store = dist.HashStore()\n&gt;&gt;&gt; # store can be used from other threads\n&gt;&gt;&gt; # Use any of the store methods after initialization\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.distributed.FileStore\">\n<code>class torch.distributed.FileStore</code> </dt> <dd>\n<p>A store implementation that uses a file to store the underlying key-value pairs.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>file_name</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – path of the file in which to store the key-value pairs</li> <li>\n<strong>world_size</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The total number of processes using the store</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; store1 = dist.FileStore(\"/tmp/filestore\", 2)\n&gt;&gt;&gt; store2 = dist.FileStore(\"/tmp/filestore\", 2)\n&gt;&gt;&gt; # Use any of the store methods from either the client or server after initialization\n&gt;&gt;&gt; store1.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; store2.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"class\"> <dt id=\"torch.distributed.PrefixStore\">\n<code>class torch.distributed.PrefixStore</code> </dt> <dd>\n<p>A wrapper around any of the 3 key-value stores (<a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a>, and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>) that adds a prefix to each key inserted to the store.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>prefix</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The prefix string that is prepended to each key before being inserted into the store.</li> <li>\n<strong>store</strong> (<em>torch.distributed.store</em>) – A store object that forms the underlying key-value store.</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.set\">\n<code>torch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) → None</code> </dt> <dd>\n<p>Inserts the key-value pair into the store based on the supplied <code>key</code> and <code>value</code>. If <code>key</code> already exists in the store, it will overwrite the old value with the new supplied <code>value</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The key to be added to the store.</li> <li>\n<strong>value</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The value associated with <code>key</code> to be added to the store.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # Should return \"first_value\"\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.get\">\n<code>torch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) → bytes</code> </dt> <dd>\n<p>Retrieves the value associated with the given <code>key</code> in the store. If <code>key</code> is not present in the store, the function will wait for <code>timeout</code>, which is defined when initializing the store, before throwing an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The function will return the value associated with this key.</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Value associated with <code>key</code> if <code>key</code> is in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # Should return \"first_value\"\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.add\">\n<code>torch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) → int</code> </dt> <dd>\n<p>The first call to add for a given <code>key</code> creates a counter associated with <code>key</code> in the store, initialized to <code>amount</code>. Subsequent calls to add with the same <code>key</code> increment the counter by the specified <code>amount</code>. Calling <code>add()</code> with a key that has already been set in the store by <code>set()</code> will result in an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The key in the store whose counter will be incremented.</li> <li>\n<strong>amount</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – The quantity by which the counter will be incremented.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.add(\"first_key\", 1)\n&gt;&gt;&gt; store.add(\"first_key\", 6)\n&gt;&gt;&gt; # Should return 7\n&gt;&gt;&gt; store.get(\"first_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.wait\">\n<code>torch.distributed.Store.wait(*args, **kwargs)</code> </dt> <dd>\n<p>Overloaded function.</p> <ol class=\"arabic simple\"> <li>wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -&gt; None</li> </ol> <p>Waits for each key in <code>keys</code> to be added to the store. If not all keys are set before the <code>timeout</code> (set during store initialization), then <code>wait</code> will throw an exception.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>keys</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a>) – List of keys on which to wait until they are set in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; # This will throw an exception after 30 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"])\n</pre> </dd> </dl> <ol class=\"arabic simple\" start=\"2\"> <li>wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -&gt; None</li> </ol> <p>Waits for each key in <code>keys</code> to be added to the store, and throws an exception if the keys have not been set by the supplied <code>timeout</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>keys</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a>) – List of keys on which to wait until they are set in the store.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em>) – Time to wait for the keys to be added before throwing an exception.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; # This will throw an exception after 10 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"], timedelta(seconds=10))\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.num_keys\">\n<code>torch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) → int</code> </dt> <dd>\n<p>Returns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by <code>set()</code> and <code>add()</code> since one key is used to coordinate all the workers using the store.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>When used with the <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a>, <code>num_keys</code> returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Returns</dt> <dd class=\"field-odd\">\n<p>The number of keys present in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\", \"first_value\")\n&gt;&gt;&gt; # This should return 2\n&gt;&gt;&gt; store.num_keys()\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.delete_key\">\n<code>torch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) → bool</code> </dt> <dd>\n<p>Deletes the key-value pair associated with <code>key</code> from the store. Returns <code>true</code> if the key was successfully deleted, and <code>false</code> if it was not.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>The <code>delete_key</code> API is only supported by the <a class=\"reference internal\" href=\"#torch.distributed.TCPStore\" title=\"torch.distributed.TCPStore\"><code>TCPStore</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.HashStore\" title=\"torch.distributed.HashStore\"><code>HashStore</code></a>. Using this API with the <a class=\"reference internal\" href=\"#torch.distributed.FileStore\" title=\"torch.distributed.FileStore\"><code>FileStore</code></a> will result in an exception.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>key</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a>) – The key to be deleted from the store</p> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>True</code> if <code>key</code> was deleted, otherwise <code>False</code>.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, HashStore can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set(\"first_key\")\n&gt;&gt;&gt; # This should return true\n&gt;&gt;&gt; store.delete_key(\"first_key\")\n&gt;&gt;&gt; # This should return false\n&gt;&gt;&gt; store.delete_key(\"bad_key\")\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.Store.set_timeout\">\n<code>torch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) → None</code> </dt> <dd>\n<p>Sets the store’s default timeout. This timeout is used during initialization and in <code>wait()</code> and <code>get()</code>.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<p><strong>timeout</strong> (<em>timedelta</em>) – timeout to be set in the store.</p> </dd> </dl> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; from datetime import timedelta\n&gt;&gt;&gt; # Using TCPStore as an example, other store types can also be used\n&gt;&gt;&gt; store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n&gt;&gt;&gt; store.set_timeout(timedelta(seconds=10))\n&gt;&gt;&gt; # This will throw an exception after 10 seconds\n&gt;&gt;&gt; store.wait([\"bad_key\"])\n</pre> </dd> </dl> </dd>\n</dl>   <h2 id=\"groups\">Groups</h2> <p>By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. <a class=\"reference internal\" href=\"#torch.distributed.new_group\" title=\"torch.distributed.new_group\"><code>new_group()</code></a> function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a <code>group</code> argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).</p> <dl class=\"function\"> <dt id=\"torch.distributed.new_group\">\n<code>torch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#new_group\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Creates a new distributed group.</p> <p>This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.</p> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p>Using multiple process groups with the <code>NCCL</code> backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See <a class=\"reference external\" href=\"https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/communicators.html#using-multiple-nccl-communicators-concurrently\">Using multiple NCCL communicators concurrently</a> for more details.</p> </div> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>ranks</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a><em>[</em><a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>]</em>) – List of ranks of group members. If <code>None</code>, will be set to all ranks. Default is <code>None</code>.</li> <li>\n<strong>timeout</strong> (<em>timedelta</em><em>, </em><em>optional</em>) – Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the <code>gloo</code> backend.</li> <li>\n<strong>backend</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#str\" title=\"(in Python v3.9)\">str</a><em> or </em><a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\">Backend</a><em>, </em><em>optional</em>) – The backend to use. Depending on build-time configurations, valid values are <code>gloo</code> and <code>nccl</code>. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., <code>\"gloo\"</code>), which can also be accessed via <a class=\"reference internal\" href=\"#torch.distributed.Backend\" title=\"torch.distributed.Backend\"><code>Backend</code></a> attributes (e.g., <code>Backend.GLOO</code>).</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A handle of distributed group that can be given to collective calls.</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"point-to-point-communication\">Point-to-point communication</h2> <dl class=\"function\"> <dt id=\"torch.distributed.send\">\n<code>torch.distributed.send(tensor, dst, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#send\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sends a tensor synchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Tensor to send.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Destination rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Tag to match send with remote recv</li> </ul> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.recv\">\n<code>torch.distributed.recv(tensor, src=None, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#recv\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Receives a tensor synchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Tensor to fill with received data.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Source rank. Will receive from any process if unspecified.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Tag to match recv with remote send</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Sender rank -1, if not part of the group</p> </dd> </dl> </dd>\n</dl> <p><a class=\"reference internal\" href=\"#torch.distributed.isend\" title=\"torch.distributed.isend\"><code>isend()</code></a> and <a class=\"reference internal\" href=\"#torch.distributed.irecv\" title=\"torch.distributed.irecv\"><code>irecv()</code></a> return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:</p> <ul class=\"simple\"> <li>\n<code>is_completed()</code> - returns True if the operation has finished</li> <li>\n<code>wait()</code> - will block the process until the operation is finished. <code>is_completed()</code> is guaranteed to return True once it returns.</li> </ul> <dl class=\"function\"> <dt id=\"torch.distributed.isend\">\n<code>torch.distributed.isend(tensor, dst, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#isend\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Sends a tensor asynchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Tensor to send.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Destination rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Tag to match send with remote recv</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A distributed request object. None, if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.irecv\">\n<code>torch.distributed.irecv(tensor, src=None, group=None, tag=0)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#irecv\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Receives a tensor asynchronously.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Tensor to fill with received data.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Source rank. Will receive from any process if unspecified.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>tag</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Tag to match recv with remote send</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>A distributed request object. None, if not part of the group</p> </dd> </dl> </dd>\n</dl>   <h2 id=\"synchronous-and-asynchronous-collective-operations\">Synchronous and asynchronous collective operations</h2> <p>Every collective operation function supports the following two kinds of operations, depending on the setting of the <code>async_op</code> flag passed into the collective:</p> <p><strong>Synchronous operation</strong> - the default mode, when <code>async_op</code> is set to <code>False</code>. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see <a class=\"reference external\" href=\"https://pytorch.org/docs/stable/notes/cuda.html\">CUDA Semantics</a>. See the below script to see examples of differences in these semantics for CPU and CUDA operations.</p> <p><strong>Asynchronous operation</strong> - when <code>async_op</code> is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:</p> <ul class=\"simple\"> <li>\n<code>is_completed()</code> - in the case of CPU collectives, returns <code>True</code> if completed. In the case of CUDA operations, returns <code>True</code> if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.</li> <li>\n<code>wait()</code> - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.</li> </ul> <p><strong>Example</strong></p> <p>The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:</p> <pre data-language=\"python\"># Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n</pre>   <h2 id=\"collective-functions\">Collective functions</h2> <dl class=\"function\"> <dt id=\"torch.distributed.broadcast\">\n<code>torch.distributed.broadcast(tensor, src, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#broadcast\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts the tensor to the whole group.</p> <p><code>tensor</code> must have the same number of elements in all processes participating in the collective.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Data to be sent if <code>src</code> is the rank of current process, and tensor to be used to save received data otherwise.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Source rank.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.broadcast_object_list\">\n<code>torch.distributed.broadcast_object_list(object_list, src=0, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#broadcast_object_list\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Broadcasts picklable objects in <code>object_list</code> to the whole group. Similar to <a class=\"reference internal\" href=\"#torch.distributed.broadcast\" title=\"torch.distributed.broadcast\"><code>broadcast()</code></a>, but Python objects can be passed in. Note that all objects in <code>object_list</code> must be picklable in order to be broadcasted.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>object_list</strong> (<em>List</em><em>[</em><em>Any</em><em>]</em>) – List of input objects to broadcast. Each object must be picklable. Only objects on the <code>src</code> rank will be broadcast, but each rank must provide lists of equal sizes.</li> <li>\n<strong>src</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Source rank from which to broadcast <code>object_list</code>.</li> <li>\n<strong>group</strong> – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p><code>None</code>. If rank is part of the group, <code>object_list</code> will contain the broadcasted objects from <code>src</code> rank.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by <code>torch.cuda.current_device()</code> and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via <code>torch.cuda.set_device()</code>.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a> collective since it does not provide an <code>async_op</code> handle and thus will be a blocking call.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.broadcast_object_list\" title=\"torch.distributed.broadcast_object_list\"><code>broadcast_object_list()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; if dist.get_rank() == 0:\n&gt;&gt;&gt;     # Assumes world_size of 3.\n&gt;&gt;&gt;     objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; else:\n&gt;&gt;&gt;     objects = [None, None, None]\n&gt;&gt;&gt; dist.broadcast_object_list(objects, src=0)\n&gt;&gt;&gt; broadcast_objects\n['foo', 12, {1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.all_reduce\">\n<code>torch.distributed.all_reduce(tensor, op=&lt;ReduceOp.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#all_reduce\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data across all machines in such a way that all get the final result.</p> <p>After the call <code>tensor</code> is going to be bitwise identical in all processes.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Input and output of the collective. The function operates in-place.</li> <li>\n<strong>op</strong> (<em>optional</em>) – One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 type.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n&gt;&gt;&gt; tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n&gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)\n&gt;&gt;&gt; tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.cfloat type.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n&gt;&gt;&gt; tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n&gt;&gt;&gt; dist.all_reduce(tensor, op=ReduceOp.SUM)\n&gt;&gt;&gt; tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.reduce\">\n<code>torch.distributed.reduce(tensor, dst, op=&lt;ReduceOp.SUM: 0&gt;, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#reduce\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Reduces the tensor data across all machines.</p> <p>Only the process with rank <code>dst</code> is going to receive the final result.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Input and output of the collective. The function operates in-place.</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a>) – Destination rank</li> <li>\n<strong>op</strong> (<em>optional</em>) – One of the values from <code>torch.distributed.ReduceOp</code> enum. Specifies an operation used for element-wise reductions.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.all_gather\">\n<code>torch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#all_gather\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers tensors from the whole group in a list.</p> <p>Complex tensors are supported.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em>) – Output list. It should contain correctly-sized tensors to be used for output of the collective.</li> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Tensor to be broadcast from current process.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> <h4 class=\"rubric\">Examples</h4> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.int64 dtype.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n&gt;&gt;&gt; tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n&gt;&gt;&gt; tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n&gt;&gt;&gt; tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n&gt;&gt;&gt; dist.all_gather(tensor_list, tensor)\n&gt;&gt;&gt; tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n</pre> <pre data-language=\"python\">&gt;&gt;&gt; # All tensors below are of torch.cfloat dtype.\n&gt;&gt;&gt; # We have 2 process groups, 2 ranks.\n&gt;&gt;&gt; tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n&gt;&gt;&gt; tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n&gt;&gt;&gt; tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n&gt;&gt;&gt; tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n&gt;&gt;&gt; dist.all_gather(tensor_list, tensor)\n&gt;&gt;&gt; tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n</pre> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.all_gather_object\">\n<code>torch.distributed.all_gather_object(object_list, obj, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#all_gather_object\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers picklable objects from the whole group into a list. Similar to <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a>, but Python objects can be passed in. Note that the object must be picklable in order to be gathered.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>object_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a><em>[</em><em>Any</em><em>]</em>) – Output list. It should be correctly sized as the size of the group for this collective and will contain the output.</li> <li>\n<strong>object</strong> (<em>Any</em>) – Pickable Python object to be broadcast from current process.</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None. If the calling rank is part of this group, the output of the collective will be populated into the input <code>object_list</code>. If the calling rank is not part of the group, the passed in <code>object_list</code> will be unmodified.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the <a class=\"reference internal\" href=\"#torch.distributed.all_gather\" title=\"torch.distributed.all_gather\"><code>all_gather()</code></a> collective since it does not provide an <code>async_op</code> handle and thus will be a blocking call.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by <code>torch.cuda.current_device()</code> and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via <code>torch.cuda.set_device()</code>.</p> </div> <div class=\"admonition warning\"> <p class=\"admonition-title\">Warning</p> <p><a class=\"reference internal\" href=\"#torch.distributed.all_gather_object\" title=\"torch.distributed.all_gather_object\"><code>all_gather_object()</code></a> uses <code>pickle</code> module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.</p> </div> <dl> <dt>Example::</dt>\n<dd>\n<pre data-language=\"python\">&gt;&gt;&gt; # Note: Process group initialization omitted on each rank.\n&gt;&gt;&gt; import torch.distributed as dist\n&gt;&gt;&gt; # Assumes world_size of 3.\n&gt;&gt;&gt; gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n&gt;&gt;&gt; output = [None for _ in gather_objects]\n&gt;&gt;&gt; dist.all_gather_object(output, gather_objects[dist.get_rank()])\n&gt;&gt;&gt; output\n['foo', 12, {1: 2}]\n</pre> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.gather\">\n<code>torch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#gather\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers a list of tensors in a single process.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>tensor</strong> (<a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a>) – Input tensor.</li> <li>\n<strong>gather_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a><em>[</em><a class=\"reference internal\" href=\"tensors#torch.Tensor\" title=\"torch.Tensor\">Tensor</a><em>]</em><em>, </em><em>optional</em>) – List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank)</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Destination rank (default is 0)</li> <li>\n<strong>group</strong> (<em>ProcessGroup</em><em>, </em><em>optional</em>) – The process group to work on. If None, the default process group will be used.</li> <li>\n<strong>async_op</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#bool\" title=\"(in Python v3.9)\">bool</a><em>, </em><em>optional</em>) – Whether this op should be an async op</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>Async work handle, if async_op is set to True. None, if not async_op or if not part of the group</p> </dd> </dl> </dd>\n</dl> <dl class=\"function\"> <dt id=\"torch.distributed.gather_object\">\n<code>torch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None)</code> <a class=\"reference internal\" href=\"https://pytorch.org/docs/1.8.0/_modules/torch/distributed/distributed_c10d.html#gather_object\"><span class=\"viewcode-link\">[source]</span></a>\n</dt> <dd>\n<p>Gathers picklable objects from the whole group in a single process. Similar to <a class=\"reference internal\" href=\"#torch.distributed.gather\" title=\"torch.distributed.gather\"><code>gather()</code></a>, but Python objects can be passed in. Note that the object must be picklable in order to be gathered.</p> <dl class=\"field-list simple\"> <dt class=\"field-odd\">Parameters</dt> <dd class=\"field-odd\">\n<ul class=\"simple\"> <li>\n<strong>obj</strong> (<em>Any</em>) – Input object. Must be picklable.</li> <li>\n<strong>object_gather_list</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/stdtypes.html#list\" title=\"(in Python v3.9)\">list</a><em>[</em><em>Any</em><em>]</em>) – Output list. On the <code>dst</code> rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be <code>None</code> on non-dst ranks. (default is <code>None</code>)</li> <li>\n<strong>dst</strong> (<a class=\"reference external\" href=\"https://docs.python.org/3/library/functions.html#int\" title=\"(in Python v3.9)\">int</a><em>, </em><em>optional</em>) – Destination rank. (default is 0)</li> <li>\n<strong>group</strong> – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is <code>None</code>.</li> </ul> </dd> <dt class=\"field-even\">Returns</dt> <dd class=\"field-even\">\n<p>None. On the <code>dst</code> rank, <code>object_gather_list</code> will contain the output of the collective.</p> </dd> </dl> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.</p> </div> <div class=\"admonition note\"> <p class=\"admonition-title\">Note</p> <p>Note that this API is not supported when using the NCCL backend.</p> </di