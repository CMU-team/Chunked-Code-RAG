[{"name": "clear()", "path": "backends#clear", "type": "torch.backends", "text": " \nclear()  \nClears the cuFFT plan cache. \n"}, {"name": "max_size", "path": "backends#max_size", "type": "torch.backends", "text": " \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n"}, {"name": "torch", "path": "torch", "type": "torch", "text": "torch The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serializing of Tensors and arbitrary types, and other useful utilities. It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0 Tensors  \n\nis_tensor\n Returns True if obj is a PyTorch tensor.  \n\nis_storage\n Returns True if obj is a PyTorch storage object.  \n\nis_complex\n Returns True if the data type of input is a complex data type i.e., one of torch.complex64, and torch.complex128.  \n\nis_floating_point\n Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32, torch.float16, and torch.bfloat16.  \n\nis_nonzero\n Returns True if the input is a single element tensor which is not equal to zero after type conversions.  \n\nset_default_dtype\n Sets the default floating point dtype to d.  \n\nget_default_dtype\n Get the current default floating point torch.dtype.  \n\nset_default_tensor_type\n Sets the default torch.Tensor type to floating point tensor type t.  \n\nnumel\n Returns the total number of elements in the input tensor.  \n\nset_printoptions\n Set options for printing.  \n\nset_flush_denormal\n Disables denormal floating numbers on CPU.   Creation Ops  Note Random sampling creation ops are listed under Random sampling and include: torch.rand() torch.rand_like() torch.randn() torch.randn_like() torch.randint() torch.randint_like() torch.randperm() You may also use torch.empty() with the In-place random sampling methods to create torch.Tensor s with values sampled from a broader range of distributions.   \n\ntensor\n Constructs a tensor with data.  \n\nsparse_coo_tensor\n Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.  \n\nas_tensor\n Convert the data into a torch.Tensor.  \n\nas_strided\n Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.  \n\nfrom_numpy\n Creates a Tensor from a numpy.ndarray.  \n\nzeros\n Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.  \n\nzeros_like\n Returns a tensor filled with the scalar value 0, with the same size as input.  \n\nones\n Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.  \n\nones_like\n Returns a tensor filled with the scalar value 1, with the same size as input.  \n\narange\n Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start.  \n\nrange\n Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1  with values from start to end with step step.  \n\nlinspace\n Creates a one-dimensional tensor of size steps whose values are evenly spaced from start to end, inclusive.  \n\nlogspace\n Creates a one-dimensional tensor of size steps whose values are evenly spaced from basestart{{\\text{{base}}}}^{{\\text{{start}}}}  to baseend{{\\text{{base}}}}^{{\\text{{end}}}} , inclusive, on a logarithmic scale with base base.  \n\neye\n Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.  \n\nempty\n Returns a tensor filled with uninitialized data.  \n\nempty_like\n Returns an uninitialized tensor with the same size as input.  \n\nempty_strided\n Returns a tensor filled with uninitialized data.  \n\nfull\n Creates a tensor of size size filled with fill_value.  \n\nfull_like\n Returns a tensor with the same size as input filled with fill_value.  \n\nquantize_per_tensor\n Converts a float tensor to a quantized tensor with given scale and zero point.  \n\nquantize_per_channel\n Converts a float tensor to a per-channel quantized tensor with given scales and zero points.  \n\ndequantize\n Returns an fp32 Tensor by dequantizing a quantized Tensor  \n\ncomplex\n Constructs a complex tensor with its real part equal to real and its imaginary part equal to imag.  \n\npolar\n Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value abs and angle angle.  \n\nheaviside\n Computes the Heaviside step function for each element in input.   Indexing, Slicing, Joining, Mutating Ops  \n\ncat\n Concatenates the given sequence of seq tensors in the given dimension.  \n\nchunk\n Splits a tensor into a specific number of chunks.  \n\ncolumn_stack\n Creates a new tensor by horizontally stacking the tensors in tensors.  \n\ndstack\n Stack tensors in sequence depthwise (along third axis).  \n\ngather\n Gathers values along an axis specified by dim.  \n\nhstack\n Stack tensors in sequence horizontally (column wise).  \n\nindex_select\n Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor.  \n\nmasked_select\n Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor.  \n\nmovedim\n Moves the dimension(s) of input at the position(s) in source to the position(s) in destination.  \n\nmoveaxis\n Alias for torch.movedim().  \n\nnarrow\n Returns a new tensor that is a narrowed version of input tensor.  \n\nnonzero\n   \n\nreshape\n Returns a tensor with the same data and number of elements as input, but with the specified shape.  \n\nrow_stack\n Alias of torch.vstack().  \n\nscatter\n Out-of-place version of torch.Tensor.scatter_()  \n\nscatter_add\n Out-of-place version of torch.Tensor.scatter_add_()  \n\nsplit\n Splits the tensor into chunks.  \n\nsqueeze\n Returns a tensor with all the dimensions of input of size 1 removed.  \n\nstack\n Concatenates a sequence of tensors along a new dimension.  \n\nswapaxes\n Alias for torch.transpose().  \n\nswapdims\n Alias for torch.transpose().  \n\nt\n Expects input to be <= 2-D tensor and transposes dimensions 0 and 1.  \n\ntake\n Returns a new tensor with the elements of input at the given indices.  \n\ntensor_split\n Splits a tensor into multiple sub-tensors, all of which are views of input, along dimension dim according to the indices or number of sections specified by indices_or_sections.  \n\ntile\n Constructs a tensor by repeating the elements of input.  \n\ntranspose\n Returns a tensor that is a transposed version of input.  \n\nunbind\n Removes a tensor dimension.  \n\nunsqueeze\n Returns a new tensor with a dimension of size one inserted at the specified position.  \n\nvstack\n Stack tensors in sequence vertically (row wise).  \n\nwhere\n Return a tensor of elements selected from either x or y, depending on condition.   Generators  \n\nGenerator\n Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.   Random sampling  \n\nseed\n Sets the seed for generating random numbers to a non-deterministic random number.  \n\nmanual_seed\n Sets the seed for generating random numbers.  \n\ninitial_seed\n Returns the initial seed for generating random numbers as a Python long.  \n\nget_rng_state\n Returns the random number generator state as a torch.ByteTensor.  \n\nset_rng_state\n Sets the random number generator state.    \ntorch.default_generator Returns the default CPU torch.Generator \n  \n\nbernoulli\n Draws binary random numbers (0 or 1) from a Bernoulli distribution.  \n\nmultinomial\n Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.  \n\nnormal\n Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.  \n\npoisson\n Returns a tensor of the same size as input with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in input i.e.,  \n\nrand\n Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)   \n\nrand_like\n Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1) .  \n\nrandint\n Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).  \n\nrandint_like\n Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).  \n\nrandn\n Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).  \n\nrandn_like\n Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1.  \n\nrandperm\n Returns a random permutation of integers from 0 to n - 1.   In-place random sampling There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:  \ntorch.Tensor.bernoulli_() - in-place version of torch.bernoulli()\n \ntorch.Tensor.cauchy_() - numbers drawn from the Cauchy distribution \ntorch.Tensor.exponential_() - numbers drawn from the exponential distribution \ntorch.Tensor.geometric_() - elements drawn from the geometric distribution \ntorch.Tensor.log_normal_() - samples from the log-normal distribution \ntorch.Tensor.normal_() - in-place version of torch.normal()\n \ntorch.Tensor.random_() - numbers sampled from the discrete uniform distribution \ntorch.Tensor.uniform_() - numbers sampled from the continuous uniform distribution  Quasi-random sampling  \nquasirandom.SobolEngine The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences.   Serialization  \n\nsave\n Saves an object to a disk file.  \n\nload\n Loads an object saved with torch.save() from a file.   Parallelism  \n\nget_num_threads\n Returns the number of threads used for parallelizing CPU operations  \n\nset_num_threads\n Sets the number of threads used for intraop parallelism on CPU.  \n\nget_num_interop_threads\n Returns the number of threads used for inter-op parallelism on CPU (e.g.  \n\nset_num_interop_threads\n Sets the number of threads used for interop parallelism (e.g.   Locally disabling gradient computation The context managers torch.no_grad(), torch.enable_grad(), and torch.set_grad_enabled() are helpful for locally disabling and enabling gradient computation. See Locally disabling gradient computation for more details on their usage. These context managers are thread local, so they won\u2019t work if you send work to another thread using the threading module, etc. Examples: >>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n  \n\nno_grad\n Context-manager that disabled gradient calculation.  \n\nenable_grad\n Context-manager that enables gradient calculation.  \n\nset_grad_enabled\n Context-manager that sets gradient calculation to on or off.   Math operations Pointwise Ops  \n\nabs\n Computes the absolute value of each element in input.  \n\nabsolute\n Alias for torch.abs()  \n\nacos\n Computes the inverse cosine of each element in input.  \n\narccos\n Alias for torch.acos().  \n\nacosh\n Returns a new tensor with the inverse hyperbolic cosine of the elements of input.  \n\narccosh\n Alias for torch.acosh().  \n\nadd\n Adds the scalar other to each element of the input input and returns a new resulting tensor.  \n\naddcdiv\n Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  \n\naddcmul\n Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  \n\nangle\n Computes the element-wise angle (in radians) of the given input tensor.  \n\nasin\n Returns a new tensor with the arcsine of the elements of input.  \n\narcsin\n Alias for torch.asin().  \n\nasinh\n Returns a new tensor with the inverse hyperbolic sine of the elements of input.  \n\narcsinh\n Alias for torch.asinh().  \n\natan\n Returns a new tensor with the arctangent of the elements of input.  \n\narctan\n Alias for torch.atan().  \n\natanh\n Returns a new tensor with the inverse hyperbolic tangent of the elements of input.  \n\narctanh\n Alias for torch.atanh().  \n\natan2\n Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant.  \n\nbitwise_not\n Computes the bitwise NOT of the given input tensor.  \n\nbitwise_and\n Computes the bitwise AND of input and other.  \n\nbitwise_or\n Computes the bitwise OR of input and other.  \n\nbitwise_xor\n Computes the bitwise XOR of input and other.  \n\nceil\n Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.  \n\nclamp\n Clamp all elements in input into the range [ min, max ].  \n\nclip\n Alias for torch.clamp().  \n\nconj\n Computes the element-wise conjugate of the given input tensor.  \n\ncopysign\n Create a new floating-point tensor with the magnitude of input and the sign of other, elementwise.  \n\ncos\n Returns a new tensor with the cosine of the elements of input.  \n\ncosh\n Returns a new tensor with the hyperbolic cosine of the elements of input.  \n\ndeg2rad\n Returns a new tensor with each of the elements of input converted from angles in degrees to radians.  \n\ndiv\n Divides each element of the input input by the corresponding element of other.  \n\ndivide\n Alias for torch.div().  \n\ndigamma\n Computes the logarithmic derivative of the gamma function on input.  \n\nerf\n Computes the error function of each element.  \n\nerfc\n Computes the complementary error function of each element of input.  \n\nerfinv\n Computes the inverse error function of each element of input.  \n\nexp\n Returns a new tensor with the exponential of the elements of the input tensor input.  \n\nexp2\n Computes the base two exponential function of input.  \n\nexpm1\n Returns a new tensor with the exponential of the elements minus 1 of input.  \n\nfake_quantize_per_channel_affine\n Returns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.  \n\nfake_quantize_per_tensor_affine\n Returns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.  \n\nfix\n Alias for torch.trunc()  \n\nfloat_power\n Raises input to the power of exponent, elementwise, in double precision.  \n\nfloor\n Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.  \n\nfloor_divide\n   \n\nfmod\n Computes the element-wise remainder of division.  \n\nfrac\n Computes the fractional portion of each element in input.  \n\nimag\n Returns a new tensor containing imaginary values of the self tensor.  \n\nldexp\n Multiplies input by 2**:attr:other.  \n\nlerp\n Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.  \n\nlgamma\n Computes the logarithm of the gamma function on input.  \n\nlog\n Returns a new tensor with the natural logarithm of the elements of input.  \n\nlog10\n Returns a new tensor with the logarithm to the base 10 of the elements of input.  \n\nlog1p\n Returns a new tensor with the natural logarithm of (1 + input).  \n\nlog2\n Returns a new tensor with the logarithm to the base 2 of the elements of input.  \n\nlogaddexp\n Logarithm of the sum of exponentiations of the inputs.  \n\nlogaddexp2\n Logarithm of the sum of exponentiations of the inputs in base-2.  \n\nlogical_and\n Computes the element-wise logical AND of the given input tensors.  \n\nlogical_not\n Computes the element-wise logical NOT of the given input tensor.  \n\nlogical_or\n Computes the element-wise logical OR of the given input tensors.  \n\nlogical_xor\n Computes the element-wise logical XOR of the given input tensors.  \n\nlogit\n Returns a new tensor with the logit of the elements of input.  \n\nhypot\n Given the legs of a right triangle, return its hypotenuse.  \n\ni0\n Computes the zeroth order modified Bessel function of the first kind for each element of input.  \n\nigamma\n Computes the regularized lower incomplete gamma function:  \n\nigammac\n Computes the regularized upper incomplete gamma function:  \n\nmul\n Multiplies each element of the input input with the scalar other and returns a new resulting tensor.  \n\nmultiply\n Alias for torch.mul().  \n\nmvlgamma\n Computes the multivariate log-gamma function) with dimension pp  element-wise, given by  \n\nnan_to_num\n Replaces NaN, positive infinity, and negative infinity values in input with the values specified by nan, posinf, and neginf, respectively.  \n\nneg\n Returns a new tensor with the negative of the elements of input.  \n\nnegative\n Alias for torch.neg()  \n\nnextafter\n Return the next floating-point value after input towards other, elementwise.  \n\npolygamma\n Computes the nthn^{th}  derivative of the digamma function on input.  \n\npow\n Takes the power of each element in input with exponent and returns a tensor with the result.  \n\nrad2deg\n Returns a new tensor with each of the elements of input converted from angles in radians to degrees.  \n\nreal\n Returns a new tensor containing real values of the self tensor.  \n\nreciprocal\n Returns a new tensor with the reciprocal of the elements of input  \n\nremainder\n Computes the element-wise remainder of division.  \n\nround\n Returns a new tensor with each of the elements of input rounded to the closest integer.  \n\nrsqrt\n Returns a new tensor with the reciprocal of the square-root of each of the elements of input.  \n\nsigmoid\n Returns a new tensor with the sigmoid of the elements of input.  \n\nsign\n Returns a new tensor with the signs of the elements of input.  \n\nsgn\n For complex tensors, this function returns a new tensor whose elemants have the same angle as that of the elements of input and absolute value 1.  \n\nsignbit\n Tests if each element of input has its sign bit set (is less than zero) or not.  \n\nsin\n Returns a new tensor with the sine of the elements of input.  \n\nsinc\n Computes the normalized sinc of input.  \n\nsinh\n Returns a new tensor with the hyperbolic sine of the elements of input.  \n\nsqrt\n Returns a new tensor with the square-root of the elements of input.  \n\nsquare\n Returns a new tensor with the square of the elements of input.  \n\nsub\n Subtracts other, scaled by alpha, from input.  \n\nsubtract\n Alias for torch.sub().  \n\ntan\n Returns a new tensor with the tangent of the elements of input.  \n\ntanh\n Returns a new tensor with the hyperbolic tangent of the elements of input.  \n\ntrue_divide\n Alias for torch.div() with rounding_mode=None.  \n\ntrunc\n Returns a new tensor with the truncated integer values of the elements of input.  \n\nxlogy\n Computes input * log(other) with the following cases.   Reduction Ops  \n\nargmax\n Returns the indices of the maximum value of all elements in the input tensor.  \n\nargmin\n Returns the indices of the minimum value(s) of the flattened tensor or along a dimension  \n\namax\n Returns the maximum value of each slice of the input tensor in the given dimension(s) dim.  \n\namin\n Returns the minimum value of each slice of the input tensor in the given dimension(s) dim.  \n\nall\n Tests if all elements in input evaluate to True.  \n\nany\n \n param input \nthe input tensor.     \n\nmax\n Returns the maximum value of all elements in the input tensor.  \n\nmin\n Returns the minimum value of all elements in the input tensor.  \n\ndist\n Returns the p-norm of (input - other)  \n\nlogsumexp\n Returns the log of summed exponentials of each row of the input tensor in the given dimension dim.  \n\nmean\n Returns the mean value of all elements in the input tensor.  \n\nmedian\n Returns the median of the values in input.  \n\nnanmedian\n Returns the median of the values in input, ignoring NaN values.  \n\nmode\n Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e.  \n\nnorm\n Returns the matrix norm or vector norm of a given tensor.  \n\nnansum\n Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.  \n\nprod\n Returns the product of all elements in the input tensor.  \n\nquantile\n Returns the q-th quantiles of all elements in the input tensor, doing a linear interpolation when the q-th quantile lies between two data points.  \n\nnanquantile\n This is a variant of torch.quantile() that \u201cignores\u201d NaN values, computing the quantiles q as if NaN values in input did not exist.  \n\nstd\n Returns the standard-deviation of all elements in the input tensor.  \n\nstd_mean\n Returns the standard-deviation and mean of all elements in the input tensor.  \n\nsum\n Returns the sum of all elements in the input tensor.  \n\nunique\n Returns the unique elements of the input tensor.  \n\nunique_consecutive\n Eliminates all but the first element from every consecutive group of equivalent elements.  \n\nvar\n Returns the variance of all elements in the input tensor.  \n\nvar_mean\n Returns the variance and mean of all elements in the input tensor.  \n\ncount_nonzero\n Counts the number of non-zero values in the tensor input along the given dim.   Comparison Ops  \n\nallclose\n This function checks if all input and other satisfy the condition:  \n\nargsort\n Returns the indices that sort a tensor along a given dimension in ascending order by value.  \n\neq\n Computes element-wise equality  \n\nequal\n True if two tensors have the same size and elements, False otherwise.  \n\nge\n Computes input\u2265other\\text{input} \\geq \\text{other}  element-wise.  \n\ngreater_equal\n Alias for torch.ge().  \n\ngt\n Computes input>other\\text{input} > \\text{other}  element-wise.  \n\ngreater\n Alias for torch.gt().  \n\nisclose\n Returns a new tensor with boolean elements representing if each element of input is \u201cclose\u201d to the corresponding element of other.  \n\nisfinite\n Returns a new tensor with boolean elements representing if each element is finite or not.  \n\nisinf\n Tests if each element of input is infinite (positive or negative infinity) or not.  \n\nisposinf\n Tests if each element of input is positive infinity or not.  \n\nisneginf\n Tests if each element of input is negative infinity or not.  \n\nisnan\n Returns a new tensor with boolean elements representing if each element of input is NaN or not.  \n\nisreal\n Returns a new tensor with boolean elements representing if each element of input is real-valued or not.  \n\nkthvalue\n Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim.  \n\nle\n Computes input\u2264other\\text{input} \\leq \\text{other}  element-wise.  \n\nless_equal\n Alias for torch.le().  \n\nlt\n Computes input<other\\text{input} < \\text{other}  element-wise.  \n\nless\n Alias for torch.lt().  \n\nmaximum\n Computes the element-wise maximum of input and other.  \n\nminimum\n Computes the element-wise minimum of input and other.  \n\nfmax\n Computes the element-wise maximum of input and other.  \n\nfmin\n Computes the element-wise minimum of input and other.  \n\nne\n Computes input\u2260other\\text{input} \\neq \\text{other}  element-wise.  \n\nnot_equal\n Alias for torch.ne().  \n\nsort\n Sorts the elements of the input tensor along a given dimension in ascending order by value.  \n\ntopk\n Returns the k largest elements of the given input tensor along a given dimension.  \n\nmsort\n Sorts the elements of the input tensor along its first dimension in ascending order by value.   Spectral Ops  \n\nstft\n Short-time Fourier transform (STFT).  \n\nistft\n Inverse short time Fourier Transform.  \n\nbartlett_window\n Bartlett window function.  \n\nblackman_window\n Blackman window function.  \n\nhamming_window\n Hamming window function.  \n\nhann_window\n Hann window function.  \n\nkaiser_window\n Computes the Kaiser window with window length window_length and shape parameter beta.   Other Operations  \n\natleast_1d\n Returns a 1-dimensional view of each input tensor with zero dimensions.  \n\natleast_2d\n Returns a 2-dimensional view of each input tensor with zero dimensions.  \n\natleast_3d\n Returns a 3-dimensional view of each input tensor with zero dimensions.  \n\nbincount\n Count the frequency of each value in an array of non-negative ints.  \n\nblock_diag\n Create a block diagonal matrix from provided tensors.  \n\nbroadcast_tensors\n Broadcasts the given tensors according to Broadcasting semantics.  \n\nbroadcast_to\n Broadcasts input to the shape shape.  \n\nbroadcast_shapes\n Similar to broadcast_tensors() but for shapes.  \n\nbucketize\n Returns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries.  \n\ncartesian_prod\n Do cartesian product of the given sequence of tensors.  \n\ncdist\n Computes batched the p-norm distance between each pair of the two collections of row vectors.  \n\nclone\n Returns a copy of input.  \n\ncombinations\n Compute combinations of length rr  of the given tensor.  \n\ncross\n Returns the cross product of vectors in dimension dim of input and other.  \n\ncummax\n Returns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim.  \n\ncummin\n Returns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim.  \n\ncumprod\n Returns the cumulative product of elements of input in the dimension dim.  \n\ncumsum\n Returns the cumulative sum of elements of input in the dimension dim.  \n\ndiag\n \n If input is a vector (1-D tensor), then returns a 2-D square tensor    \n\ndiag_embed\n Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input.  \n\ndiagflat\n \n If input is a vector (1-D tensor), then returns a 2-D square tensor    \n\ndiagonal\n Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape.  \n\ndiff\n Computes the n-th forward difference along the given dimension.  \n\neinsum\n Sums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention.  \n\nflatten\n Flattens input by reshaping it into a one-dimensional tensor.  \n\nflip\n Reverse the order of a n-D tensor along given axis in dims.  \n\nfliplr\n Flip tensor in the left/right direction, returning a new tensor.  \n\nflipud\n Flip tensor in the up/down direction, returning a new tensor.  \n\nkron\n Computes the Kronecker product, denoted by \u2297\\otimes , of input and other.  \n\nrot90\n Rotate a n-D tensor by 90 degrees in the plane specified by dims axis.  \n\ngcd\n Computes the element-wise greatest common divisor (GCD) of input and other.  \n\nhistc\n Computes the histogram of a tensor.  \n\nmeshgrid\n Take NN  tensors, each of which can be either scalar or 1-dimensional vector, and create NN  N-dimensional grids, where the ii  th grid is defined by expanding the ii  th input over dimensions defined by other inputs.  \n\nlcm\n Computes the element-wise least common multiple (LCM) of input and other.  \n\nlogcumsumexp\n Returns the logarithm of the cumulative summation of the exponentiation of elements of input in the dimension dim.  \n\nravel\n Return a contiguous flattened tensor.  \n\nrenorm\n Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm  \n\nrepeat_interleave\n Repeat elements of a tensor.  \n\nroll\n Roll the tensor along the given dimension(s).  \n\nsearchsorted\n Find the indices from the innermost dimension of sorted_sequence such that, if the corresponding values in values were inserted before the indices, the order of the corresponding innermost dimension within sorted_sequence would be preserved.  \n\ntensordot\n Returns a contraction of a and b over multiple dimensions.  \n\ntrace\n Returns the sum of the elements of the diagonal of the input 2-D matrix.  \n\ntril\n Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.  \n\ntril_indices\n Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \n\ntriu\n Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0.  \n\ntriu_indices\n Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \n\nvander\n Generates a Vandermonde matrix.  \n\nview_as_real\n Returns a view of input as a real tensor.  \n\nview_as_complex\n Returns a view of input as a complex tensor.   BLAS and LAPACK Operations  \n\naddbmm\n Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).  \n\naddmm\n Performs a matrix multiplication of the matrices mat1 and mat2.  \n\naddmv\n Performs a matrix-vector product of the matrix mat and the vector vec.  \n\naddr\n Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input.  \n\nbaddbmm\n Performs a batch matrix-matrix product of matrices in batch1 and batch2.  \n\nbmm\n Performs a batch matrix-matrix product of matrices stored in input and mat2.  \n\nchain_matmul\n Returns the matrix product of the NN  2-D tensors.  \n\ncholesky\n Computes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices.  \n\ncholesky_inverse\n Computes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv.  \n\ncholesky_solve\n Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu .  \n\ndot\n Computes the dot product of two 1D tensors.  \n\neig\n Computes the eigenvalues and eigenvectors of a real square matrix.  \n\ngeqrf\n This is a low-level function for calling LAPACK directly.  \n\nger\n Alias of torch.outer().  \n\ninner\n Computes the dot product for 1D tensors.  \n\ninverse\n Takes the inverse of the square matrix input.  \n\ndet\n Calculates determinant of a square matrix or batches of square matrices.  \n\nlogdet\n Calculates log determinant of a square matrix or batches of square matrices.  \n\nslogdet\n Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.  \n\nlstsq\n Computes the solution to the least squares and least norm problems for a full rank matrix AA  of size (m\u00d7n)(m \\times n)  and a matrix BB  of size (m\u00d7k)(m \\times k) .  \n\nlu\n Computes the LU factorization of a matrix or batches of matrices A.  \n\nlu_solve\n Returns the LU solve of the linear system Ax=bAx = b  using the partially pivoted LU factorization of A from torch.lu().  \n\nlu_unpack\n Unpacks the data and pivots from a LU factorization of a tensor.  \n\nmatmul\n Matrix product of two tensors.  \n\nmatrix_power\n Returns the matrix raised to the power n for square matrices.  \n\nmatrix_rank\n Returns the numerical rank of a 2-D tensor.  \n\nmatrix_exp\n Returns the matrix exponential.  \n\nmm\n Performs a matrix multiplication of the matrices input and mat2.  \n\nmv\n Performs a matrix-vector product of the matrix input and the vector vec.  \n\norgqr\n Computes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf().  \n\normqr\n Multiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)).  \n\nouter\n Outer product of input and vec2.  \n\npinverse\n Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor.  \n\nqr\n Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q R  with QQ  being an orthogonal matrix or batch of orthogonal matrices and RR  being an upper triangular matrix or batch of upper triangular matrices.  \n\nsolve\n This function returns the solution to the system of linear equations represented by AX=BAX = B  and the LU factorization of A, in order as a namedtuple solution, LU.  \n\nsvd\n Computes the singular value decomposition of either a matrix or batch of matrices input.  \n\nsvd_lowrank\n Return the singular value decomposition (U, S, V) of a matrix, batches of matrices, or a sparse matrix AA  such that A\u2248Udiag(S)VTA \\approx U diag(S) V^T .  \n\npca_lowrank\n Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.  \n\nsymeig\n This function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors).  \n\nlobpcg\n Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.  \n\ntrapz\n Estimate \u222bydx\\int y\\,dx  along dim, using the trapezoid rule.  \n\ntriangular_solve\n Solves a system of equations with a triangular coefficient matrix AA  and multiple right-hand sides bb .  \n\nvdot\n Computes the dot product of two 1D tensors.   Utilities  \n\ncompiled_with_cxx11_abi\n Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1  \n\nresult_type\n Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors.  \n\ncan_cast\n Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  \n\npromote_types\n Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2.  \n\nuse_deterministic_algorithms\n Sets whether PyTorch operations must use \u201cdeterministic\u201d algorithms.  \n\nare_deterministic_algorithms_enabled\n Returns True if the global deterministic flag is turned on.  \n\n_assert\n A wrapper around Python\u2019s assert which is symbolically traceable.  \n"}, {"name": "torch.abs()", "path": "generated/torch.abs#torch.abs", "type": "torch", "text": " \ntorch.abs(input, *, out=None) \u2192 Tensor  \nComputes the absolute value of each element in input.  outi=\u2223inputi\u2223\\text{out}_{i} = |\\text{input}_{i}|  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n \n"}, {"name": "torch.absolute()", "path": "generated/torch.absolute#torch.absolute", "type": "torch", "text": " \ntorch.absolute(input, *, out=None) \u2192 Tensor  \nAlias for torch.abs() \n"}, {"name": "torch.acos()", "path": "generated/torch.acos#torch.acos", "type": "torch", "text": " \ntorch.acos(input, *, out=None) \u2192 Tensor  \nComputes the inverse cosine of each element in input.  outi=cos\u2061\u22121(inputi)\\text{out}_{i} = \\cos^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n \n"}, {"name": "torch.acosh()", "path": "generated/torch.acosh#torch.acosh", "type": "torch", "text": " \ntorch.acosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic cosine of the elements of input.  Note The domain of the inverse hyperbolic cosine is [1, inf) and values outside this range will be mapped to NaN, except for + INF for which the output is mapped to + INF.   outi=cosh\u2061\u22121(inputi)\\text{out}_{i} = \\cosh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\n \n"}, {"name": "torch.add()", "path": "generated/torch.add#torch.add", "type": "torch", "text": " \ntorch.add(input, other, *, out=None)  \nAdds the scalar other to each element of the input input and returns a new resulting tensor.  out=input+other\\text{out} = \\text{input} + \\text{other}  \nIf input is of type FloatTensor or DoubleTensor, other must be a real number, otherwise it should be an integer.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nvalue (Number) \u2013 the number to be added to each element of input\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n  \ntorch.add(input, other, *, alpha=1, out=None) \n Each element of the tensor other is multiplied by the scalar alpha and added to each element of the tensor input. The resulting tensor is returned. The shapes of input and other must be broadcastable.  out=input+alpha\u00d7other\\text{out} = \\text{input} + \\text{alpha} \\times \\text{other}  \nIf other is of type FloatTensor or DoubleTensor, alpha must be a real number, otherwise it should be an integer.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \n \nalpha (Number) \u2013 the scalar multiplier for other\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n \n"}, {"name": "torch.addbmm()", "path": "generated/torch.addbmm#torch.addbmm", "type": "torch", "text": " \ntorch.addbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor.  out=\u03b2 input+\u03b1(\u2211i=0b\u22121batch1i@batch2i)out = \\beta\\ \\text{input} + \\alpha\\ (\\sum_{i=0}^{b-1} \\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \ninput (Tensor) \u2013 matrix to be added \nalpha (Number, optional) \u2013 multiplier for batch1 @ batch2 (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n \n"}, {"name": "torch.addcdiv()", "path": "generated/torch.addcdiv#torch.addcdiv", "type": "torch", "text": " \ntorch.addcdiv(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  Warning Integer division with addcdiv is no longer supported, and in a future release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.   outi=inputi+value\u00d7tensor1itensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}  \nThe shapes of input, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the numerator tensor \ntensor2 (Tensor) \u2013 the denominator tensor   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2} \n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n \n"}, {"name": "torch.addcmul()", "path": "generated/torch.addcmul#torch.addcmul", "type": "torch", "text": " \ntorch.addcmul(input, tensor1, tensor2, *, value=1, out=None) \u2192 Tensor  \nPerforms the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  outi=inputi+value\u00d7tensor1i\u00d7tensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\text{tensor1}_i \\times \\text{tensor2}_i  \nThe shapes of tensor, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \ntensor1 (Tensor) \u2013 the tensor to be multiplied \ntensor2 (Tensor) \u2013 the tensor to be multiplied   Keyword Arguments \n \nvalue (Number, optional) \u2013 multiplier for tensor1.\u2217tensor2tensor1 .* tensor2 \n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n \n"}, {"name": "torch.addmm()", "path": "generated/torch.addmm#torch.addmm", "type": "torch", "text": " \ntorch.addmm(input, mat1, mat2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result. If mat1 is a (n\u00d7m)(n \\times m)  tensor, mat2 is a (m\u00d7p)(m \\times p)  tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p)  tensor and out will be a (n\u00d7p)(n \\times p)  tensor. alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(mat1i@mat2i)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nmat1 (Tensor) \u2013 the first matrix to be matrix multiplied \nmat2 (Tensor) \u2013 the second matrix to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n \n"}, {"name": "torch.addmv()", "path": "generated/torch.addmv#torch.addmv", "type": "torch", "text": " \ntorch.addmv(input, mat, vec, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result. If mat is a (n\u00d7m)(n \\times m)  tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n. alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.  out=\u03b2 input+\u03b1(mat@vec)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat} \\mathbin{@} \\text{vec})  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers  Parameters \n \ninput (Tensor) \u2013 vector to be added \nmat (Tensor) \u2013 matrix to be matrix multiplied \nvec (Tensor) \u2013 vector to be matrix multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for mat@vecmat @ vec  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n \n"}, {"name": "torch.addr()", "path": "generated/torch.addr#torch.addr", "type": "torch", "text": " \ntorch.addr(input, vec1, vec2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms the outer-product of vectors vec1 and vec2 and adds it to the matrix input. Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.  out=\u03b2 input+\u03b1(vec1\u2297vec2)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{vec1} \\otimes \\text{vec2})  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m)  and out will be a matrix of size (n\u00d7m)(n \\times m) .  Parameters \n \ninput (Tensor) \u2013 matrix to be added \nvec1 (Tensor) \u2013 the first vector of the outer product \nvec2 (Tensor) \u2013 the second vector of the outer product   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for vec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n \n"}, {"name": "torch.all()", "path": "generated/torch.all#torch.all", "type": "torch", "text": " \ntorch.all(input) \u2192 Tensor  \nTests if all elements in input evaluate to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)\n  \ntorch.all(input, dim, keepdim=False, *, out=None) \u2192 Tensor \n For each row of input in the given dimension dim, returns True if all elements in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)\n \n"}, {"name": "torch.allclose()", "path": "generated/torch.allclose#torch.allclose", "type": "torch", "text": " \ntorch.allclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 bool  \nThis function checks if all input and other satisfy the condition:  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert  \nelementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose  Parameters \n \ninput (Tensor) \u2013 first tensor to compare \nother (Tensor) \u2013 second tensor to compare \natol (float, optional) \u2013 absolute tolerance. Default: 1e-08 \nrtol (float, optional) \u2013 relative tolerance. Default: 1e-05 \nequal_nan (bool, optional) \u2013 if True, then two NaN s will be considered equal. Default: False\n    Example: >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n \n"}, {"name": "torch.amax()", "path": "generated/torch.amax#torch.amax", "type": "torch", "text": " \ntorch.amax(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the maximum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is ``True`, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\n \n"}, {"name": "torch.amin()", "path": "generated/torch.amin#torch.amin", "type": "torch", "text": " \ntorch.amin(input, dim, keepdim=False, *, out=None) \u2192 Tensor  \nReturns the minimum value of each slice of the input tensor in the given dimension(s) dim.  Note  \nThe difference between max/min and amax/amin is: \n\n \namax/amin supports reducing on multiple dimensions, \namax/amin does not return indices, \namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.     If keepdim is True, the output tensors are of the same size as input except in the dimension(s) dim where they are of size 1. Otherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in the output tensors having fewer dimensions than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\n \n"}, {"name": "torch.angle()", "path": "generated/torch.angle#torch.angle", "type": "torch", "text": " \ntorch.angle(input, *, out=None) \u2192 Tensor  \nComputes the element-wise angle (in radians) of the given input tensor.  outi=angle(inputi)\\text{out}_{i} = angle(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Note Starting in PyTorch 1.8, angle returns pi for negative real numbers, zero for non-negative real numbers, and propagates NaNs. Previously the function would return zero for all real numbers and not propagate floating-point NaNs.  Example: >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n \n"}, {"name": "torch.any()", "path": "generated/torch.any#torch.any", "type": "torch", "text": " \ntorch.any(input) \u2192 Tensor  \n Parameters \ninput (Tensor) \u2013 the input tensor.   Tests if any element in input evaluates to True.  Note This function matches the behaviour of NumPy in returning output of dtype bool for all supported dtypes except uint8. For uint8 the dtype of output is uint8 itself.  Example: >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)\n  \ntorch.any(input, dim, keepdim=False, *, out=None) \u2192 Tensor \n For each row of input in the given dimension dim, returns True if any element in the row evaluate to True and False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 2) < 0\n>>> a\ntensor([[ True,  True],\n        [False,  True],\n        [ True,  True],\n        [False, False]])\n>>> torch.any(a, 1)\ntensor([ True,  True,  True, False])\n>>> torch.any(a, 0)\ntensor([True, True])\n \n"}, {"name": "torch.arange()", "path": "generated/torch.arange#torch.arange", "type": "torch", "text": " \ntorch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil  with values from the interval [start, end) taken with common difference step beginning from start. Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise adding a small epsilon to end in such cases.  outi+1=outi+step\\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}  \n Parameters \n \nstart (Number) \u2013 the starting value for the set of points. Default: 0. \nend (Number) \u2013 the ending value for the set of points \nstep (Number) \u2013 the gap between each pair of adjacent points. Default: 1.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n \n"}, {"name": "torch.arccos()", "path": "generated/torch.arccos#torch.arccos", "type": "torch", "text": " \ntorch.arccos(input, *, out=None) \u2192 Tensor  \nAlias for torch.acos(). \n"}, {"name": "torch.arccosh()", "path": "generated/torch.arccosh#torch.arccosh", "type": "torch", "text": " \ntorch.arccosh(input, *, out=None) \u2192 Tensor  \nAlias for torch.acosh(). \n"}, {"name": "torch.arcsin()", "path": "generated/torch.arcsin#torch.arcsin", "type": "torch", "text": " \ntorch.arcsin(input, *, out=None) \u2192 Tensor  \nAlias for torch.asin(). \n"}, {"name": "torch.arcsinh()", "path": "generated/torch.arcsinh#torch.arcsinh", "type": "torch", "text": " \ntorch.arcsinh(input, *, out=None) \u2192 Tensor  \nAlias for torch.asinh(). \n"}, {"name": "torch.arctan()", "path": "generated/torch.arctan#torch.arctan", "type": "torch", "text": " \ntorch.arctan(input, *, out=None) \u2192 Tensor  \nAlias for torch.atan(). \n"}, {"name": "torch.arctanh()", "path": "generated/torch.arctanh#torch.arctanh", "type": "torch", "text": " \ntorch.arctanh(input, *, out=None) \u2192 Tensor  \nAlias for torch.atanh(). \n"}, {"name": "torch.are_deterministic_algorithms_enabled()", "path": "generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled", "type": "torch", "text": " \ntorch.are_deterministic_algorithms_enabled() [source]\n \nReturns True if the global deterministic flag is turned on. Refer to torch.use_deterministic_algorithms() documentation for more details. \n"}, {"name": "torch.argmax()", "path": "generated/torch.argmax#torch.argmax", "type": "torch", "text": " \ntorch.argmax(input) \u2192 LongTensor  \nReturns the indices of the maximum value of all elements in the input tensor. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Note If there are multiple minimal values then the indices of the first minimal value are returned.   Parameters \ninput (Tensor) \u2013 the input tensor.   Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)\n  \ntorch.argmax(input, dim, keepdim=False) \u2192 LongTensor \n Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmax of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n \n"}, {"name": "torch.argmin()", "path": "generated/torch.argmin#torch.argmin", "type": "torch", "text": " \ntorch.argmin(input, dim=None, keepdim=False) \u2192 LongTensor  \nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.  Note If there are multiple minimal values then the indices of the first minimal value are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to reduce. If None, the argmin of the flattened input is returned. \nkeepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])\n \n"}, {"name": "torch.argsort()", "path": "generated/torch.argsort#torch.argsort", "type": "torch", "text": " \ntorch.argsort(input, dim=-1, descending=False) \u2192 LongTensor  \nReturns the indices that sort a tensor along a given dimension in ascending order by value. This is the second value returned by torch.sort(). See its documentation for the exact semantics of this method.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int, optional) \u2013 the dimension to sort along \ndescending (bool, optional) \u2013 controls the sorting order (ascending or descending)    Example: >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n \n"}, {"name": "torch.asin()", "path": "generated/torch.asin#torch.asin", "type": "torch", "text": " \ntorch.asin(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arcsine of the elements of input.  outi=sin\u2061\u22121(inputi)\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n \n"}, {"name": "torch.asinh()", "path": "generated/torch.asinh#torch.asinh", "type": "torch", "text": " \ntorch.asinh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic sine of the elements of input.  outi=sinh\u2061\u22121(inputi)\\text{out}_{i} = \\sinh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\n \n"}, {"name": "torch.as_strided()", "path": "generated/torch.as_strided#torch.as_strided", "type": "torch", "text": " \ntorch.as_strided(input, size, stride, storage_offset=0) \u2192 Tensor  \nCreate a view of an existing torch.Tensor input with specified size, stride and storage_offset.  Warning More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like torch.Tensor.expand(), are easier to read and are therefore more advisable to use.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nsize (tuple or ints) \u2013 the shape of the output tensor \nstride (tuple or ints) \u2013 the stride of the output tensor \nstorage_offset (int, optional) \u2013 the offset in the underlying storage of the output tensor    Example: >>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n \n"}, {"name": "torch.as_tensor()", "path": "generated/torch.as_tensor#torch.as_tensor", "type": "torch", "text": " \ntorch.as_tensor(data, dtype=None, device=None) \u2192 Tensor  \nConvert the data into a torch.Tensor. If the data is already a Tensor with the same dtype and device, no copy will be performed, otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True. Similarly, if the data is an ndarray of the corresponding dtype and the device is the cpu, no copy will be performed.  Parameters \n \ndata (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.    Example: >>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])\n \n"}, {"name": "torch.atan()", "path": "generated/torch.atan#torch.atan", "type": "torch", "text": " \ntorch.atan(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the arctangent of the elements of input.  outi=tan\u2061\u22121(inputi)\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n \n"}, {"name": "torch.atan2()", "path": "generated/torch.atan2#torch.atan2", "type": "torch", "text": " \ntorch.atan2(input, other, *, out=None) \u2192 Tensor  \nElement-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}  with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})  and vector (1,0)(1, 0) . (Note that otheri\\text{other}_{i} , the second parameter, is the x-coordinate, while inputi\\text{input}_{i} , the first parameter, is the y-coordinate.) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the first input tensor \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n \n"}, {"name": "torch.atanh()", "path": "generated/torch.atanh#torch.atanh", "type": "torch", "text": " \ntorch.atanh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the inverse hyperbolic tangent of the elements of input.  Note The domain of the inverse hyperbolic tangent is (-1, 1) and values outside this range will be mapped to NaN, except for the values 1 and -1 for which the output is mapped to +/-INF respectively.   outi=tanh\u2061\u22121(inputi)\\text{out}_{i} = \\tanh^{-1}(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\n \n"}, {"name": "torch.atleast_1d()", "path": "generated/torch.atleast_1d#torch.atleast_1d", "type": "torch", "text": " \ntorch.atleast_1d(*tensors) [source]\n \nReturns a 1-dimensional view of each input tensor with zero dimensions. Input tensors with one or more dimensions are returned as-is.  Parameters \ninput (Tensor or list of Tensors) \u2013   Returns \noutput (Tensor or tuple of Tensors)    Example::\n\n>>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))\n   \n"}, {"name": "torch.atleast_2d()", "path": "generated/torch.atleast_2d#torch.atleast_2d", "type": "torch", "text": " \ntorch.atleast_2d(*tensors) [source]\n \nReturns a 2-dimensional view of each input tensor with zero dimensions. Input tensors with two or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)    Example::\n\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))\n   \n"}, {"name": "torch.atleast_3d()", "path": "generated/torch.atleast_3d#torch.atleast_3d", "type": "torch", "text": " \ntorch.atleast_3d(*tensors) [source]\n \nReturns a 3-dimensional view of each input tensor with zero dimensions. Input tensors with three or more dimensions are returned as-is. :param input: :type input: Tensor or list of Tensors  Returns \noutput (Tensor or tuple of Tensors)   Example >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\n \n"}, {"name": "torch.autograd", "path": "autograd", "type": "torch.autograd", "text": "Automatic differentiation package - torch.autograd torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s for which gradients should be computed with the requires_grad=True keyword. As of now, we only support autograd for floating point Tensor types ( half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).  \ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None) [source]\n \nComputes the sum of gradients of given tensors w.r.t. graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.   Note If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ntensors (sequence of Tensor) \u2013 Tensors of which the derivative will be computed. \ngrad_tensors (sequence of (Tensor or None)) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n  \ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) [source]\n \nComputes and returns the sum of gradients of outputs w.r.t. the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None). If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.  Note If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \noutputs (sequence of Tensor) \u2013 outputs of the differentiated function. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). \ngrad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. \nallow_unused (bool, optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.    \n Functional higher level API  Warning This API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.  This section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set, you can use a lambda to capture them. For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input).  \ntorch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Jacobian of a given function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Jacobian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the jacobian, usually we invoke autograd.grad once per row of the jacobian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.  Return type \nJacobian (Tensor or nested tuple of Tensors)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(2, 2)\n>>> jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n >>> jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n >>> def exp_adder(x, y):\n...   return 2 * x.exp() + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n \n  \ntorch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Hessian of a given scalar function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Hessian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the hessian, usually we invoke autograd.grad once per row of the hessian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.  Return type \nHessian (Tensor or a tuple of tuple of Tensors)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n >>> hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n \n  \ntorch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Jacobian product is computed. Must be the same size as the output of func. This argument is optional when the output of func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4)\n>>> vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n >>> vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = torch.ones(2)\n>>> vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n \n  \ntorch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Jacobian vector product is computed. Must be the same size as the input of func. This argument is optional when the input to func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4, 4)\n>>> jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n >>> jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.ones(2), torch.ones(2))\n>>> jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n  Note The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment.  \n  \ntorch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Hessian product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n>>> vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=<SumBackward0>),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=<MulBackward0>))\n>>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n \n  \ntorch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Hessian vector product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n >>> hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=<SumBackward0>),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=<MulBackward0>))\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n  Note This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.  \n Locally disabling gradient computation  \nclass torch.autograd.no_grad [source]\n \nContext-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n \n  \nclass torch.autograd.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n  \nclass torch.autograd.set_grad_enabled(mode) [source]\n \nContext-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation in other threads.  Parameters \nmode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n \n Default gradient layouts When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None:  If param\u2019s memory is non-overlapping and dense, .grad is created with strides matching param (thus matching param\u2019s layout). Otherwise, .grad is created with rowmajor-contiguous strides.  If param already has a non-sparse .grad attribute:  If create_graph=False, backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a new tensor .grad + new grad, which attempts (but does not guarantee) matching the preexisting .grad\u2019s strides.  The default behavior (letting .grads be None before the first backward(), such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grads to None before each accumulation phase, e.g.: for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n such that they\u2019re recreated according to 1 or 2 every time, is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks. Manual gradient layouts If you need manual control over .grad\u2019s strides, assign param.grad = a zeroed tensor with desired strides before the first backward(), and never reset it to None. 3 guarantees your layout is preserved as long as create_graph=False. 4 indicates your layout is likely preserved even if create_graph=True. In-place operations on Tensors Supporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd\u2019s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you\u2019re operating under heavy memory pressure, you might never need to use them. In-place correctness checks All Tensor s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you\u2019re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct. Variable (deprecated)  Warning The Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True. Below please find a quick guide on what has changed:  \nVariable(tensor) and Variable(tensor, requires_grad) still work as expected, but they return Tensors instead of Variables. \nvar.data is the same thing as tensor.data. Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors with the same method names.  In addition, one can now create tensors with requires_grad=True using factory methods such as torch.randn(), torch.zeros(), torch.ones(), and others like the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True)  Tensor autograd functions  \nclass torch.Tensor  \n \ngrad  \nThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it. \n  \nrequires_grad  \nIs True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.  \n  \nis_leaf  \nAll Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: >>> a = torch.rand(10, requires_grad=True)\n>>> a.is_leaf\nTrue\n>>> b = torch.rand(10, requires_grad=True).cuda()\n>>> b.is_leaf\nFalse\n# b was created by the operation that cast a cpu Tensor into a cuda Tensor\n>>> c = torch.rand(10, requires_grad=True) + 2\n>>> c.is_leaf\nFalse\n# c was created by the addition operation\n>>> d = torch.rand(10).cuda()\n>>> d.is_leaf\nTrue\n# d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n>>> e = torch.rand(10).cuda().requires_grad_()\n>>> e.is_leaf\nTrue\n# e requires gradients and has no operations creating it\n>>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n>>> f.is_leaf\nTrue\n# f requires grad, has no operation creating it\n \n  \nbackward(gradient=None, retain_graph=None, create_graph=False, inputs=None) [source]\n \nComputes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ngradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n  \ndetach()  \nReturns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  \n  \ndetach_()  \nDetaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. \n  \nregister_hook(hook) [source]\n \nRegisters a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -> Tensor or None\n The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n>>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n>>> v.backward(torch.tensor([1., 2., 3.]))\n>>> v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n>>> h.remove()  # removes the hook\n \n  \nretain_grad() [source]\n \nEnables .grad attribute for non-leaf Tensors. \n \n Function  \nclass torch.autograd.Function [source]\n \nRecords operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n  \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n  \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n \n Context method mixins When creating a new Function, the following methods are available to ctx.  \nclass torch.autograd.function._ContextMethodMixin [source]\n \n \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n  \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n  \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n  \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n \n Numerical gradient checking  \ntorch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=False, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True. The check between numerical and analytical gradients uses allclose(). For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \ncheck_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros, for Tensor outputs. \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n  \ntorch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \ngrad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \ngen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n Profiler Autograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are two modes implemented at the moment - CPU-only using profile. and nvprof based (registers both CPU and GPU activity) using emit_nvtx.  \nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, use_kineto=False, use_cpu=True) [source]\n \nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks  Parameters \n \nenabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. \nuse_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. \nrecord_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection. \nwith_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators. \nprofile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. \nwith_stack (bool, optional) \u2013 record source information (file and line number) for the ops. \nuse_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. \nuse_cpu (bool, optional) \u2013 profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling.    Example >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n  \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n  \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n  \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n  \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n  \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n \n  \nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.  Parameters \n \nenabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.    Example >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. \n  \ntorch.autograd.profiler.load_nvprof(path) [source]\n \nOpens an nvprof trace file and parses autograd annotations.  Parameters \npath (str) \u2013 path to nvprof trace   \n Anomaly detection  \nclass torch.autograd.detect_anomaly [source]\n \nContext-manager that enable anomaly detection for the autograd engine. This does two things:  Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. Any backward computation that generate \u201cnan\u201d value will raise an error.   Warning This mode should be enabled only for debugging as the different tests will slow down your program execution.  Example >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n \n  \nclass torch.autograd.set_detect_anomaly(mode) [source]\n \nContext-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.  Parameters \nmode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False).   \n\n"}, {"name": "torch.autograd.backward()", "path": "autograd#torch.autograd.backward", "type": "torch.autograd", "text": " \ntorch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None) [source]\n \nComputes the sum of gradients of given tensors w.r.t. graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.  Note Using this method with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak.   Note If you run any forward ops, create grad_tensors, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \ntensors (sequence of Tensor) \u2013 Tensors of which the derivative will be computed. \ngrad_tensors (sequence of (Tensor or None)) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors.    \n"}, {"name": "torch.autograd.detect_anomaly", "path": "autograd#torch.autograd.detect_anomaly", "type": "torch.autograd", "text": " \nclass torch.autograd.detect_anomaly [source]\n \nContext-manager that enable anomaly detection for the autograd engine. This does two things:  Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function. Any backward computation that generate \u201cnan\u201d value will raise an error.   Warning This mode should be enabled only for debugging as the different tests will slow down your program execution.  Example >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n \n"}, {"name": "torch.autograd.enable_grad", "path": "autograd#torch.autograd.enable_grad", "type": "torch.autograd", "text": " \nclass torch.autograd.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n"}, {"name": "torch.autograd.Function", "path": "autograd#torch.autograd.Function", "type": "torch.autograd", "text": " \nclass torch.autograd.Function [source]\n \nRecords operation history and defines formulas for differentiating ops. See the Note on extending the autograd engine for more details on how to use this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd Every operation performed on Tensor s creates a new function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s. Normally, the only way users interact with functions is by creating subclasses and defining new operations. This is a recommended way of extending torch.autograd. Examples: >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n  \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n  \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n \n"}, {"name": "torch.autograd.Function.backward()", "path": "autograd#torch.autograd.Function.backward", "type": "torch.autograd", "text": " \nstatic backward(ctx, *grad_outputs) [source]\n \nDefines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. \n"}, {"name": "torch.autograd.Function.forward()", "path": "autograd#torch.autograd.Function.forward", "type": "torch.autograd", "text": " \nstatic forward(ctx, *args, **kwargs) [source]\n \nPerforms the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \n"}, {"name": "torch.autograd.function._ContextMethodMixin", "path": "autograd#torch.autograd.function._ContextMethodMixin", "type": "torch.autograd", "text": " \nclass torch.autograd.function._ContextMethodMixin [source]\n \n \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n  \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n  \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n  \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n \n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_dirty()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_dirty", "type": "torch.autograd", "text": " \nmark_dirty(*args) [source]\n \nMarks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable()", "path": "autograd#torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "type": "torch.autograd", "text": " \nmark_non_differentiable(*args) [source]\n \nMarks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.save_for_backward()", "path": "autograd#torch.autograd.function._ContextMethodMixin.save_for_backward", "type": "torch.autograd", "text": " \nsave_for_backward(*tensors) [source]\n \nSaves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. \n"}, {"name": "torch.autograd.function._ContextMethodMixin.set_materialize_grads()", "path": "autograd#torch.autograd.function._ContextMethodMixin.set_materialize_grads", "type": "torch.autograd", "text": " \nset_materialize_grads(value) [source]\n \nSets whether to materialize output grad tensors. Default is true. This should be called only from inside the forward() method If true, undefined output grad tensors will be expanded to tensors full of zeros prior to calling the backward() method. \n"}, {"name": "torch.autograd.functional.hessian()", "path": "autograd#torch.autograd.functional.hessian", "type": "torch.autograd", "text": " \ntorch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Hessian of a given scalar function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Hessian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the hessian, usually we invoke autograd.grad once per row of the hessian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where Hessian[i][j] will contain the Hessian of the ith input and jth input with size the sum of the size of the ith input plus the size of the jth input. Hessian[i][j] will have the same dtype and device as the corresponding ith input.  Return type \nHessian (Tensor or a tuple of tuple of Tensors)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> hessian(pow_reducer, inputs)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]])\n >>> hessian(pow_reducer, inputs, create_graph=True)\ntensor([[[[5.2265, 0.0000],\n          [0.0000, 0.0000]],\n         [[0.0000, 4.8221],\n          [0.0000, 0.0000]]],\n        [[[0.0000, 0.0000],\n          [1.9456, 0.0000]],\n         [[0.0000, 0.0000],\n          [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> hessian(pow_adder_reducer, inputs)\n((tensor([[4., 0.],\n          [0., 4.]]),\n  tensor([[0., 0.],\n          [0., 0.]])),\n (tensor([[0., 0.],\n          [0., 0.]]),\n  tensor([[6., 0.],\n          [0., 6.]])))\n \n"}, {"name": "torch.autograd.functional.hvp()", "path": "autograd#torch.autograd.functional.hvp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Hessian vector product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> hvp(pow_reducer, inputs, v)\n(tensor(0.1448),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]]))\n >>> hvp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.1448, grad_fn=<SumBackward0>),\n tensor([[2.0239, 1.6456],\n         [2.4988, 1.4310]], grad_fn=<MulBackward0>))\n >>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> hvp(pow_adder_reducer, inputs, v)\n(tensor(2.3030),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n  Note This function is significantly slower than vhp due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.  \n"}, {"name": "torch.autograd.functional.jacobian()", "path": "autograd#torch.autograd.functional.jacobian", "type": "torch.autograd", "text": " \ntorch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False) [source]\n \nFunction that computes the Jacobian of a given function.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \ncreate_graph (bool, optional) \u2013 If True, the Jacobian will be computed in a differentiable manner. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to False. \nvectorize (bool, optional) \u2013 This feature is experimental, please use at your own risk. When computing the jacobian, usually we invoke autograd.grad once per row of the jacobian. If this flag is True, we use the vmap prototype feature as the backend to vectorize calls to autograd.grad so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use torch._C._debug_only_display_vmap_fallback_warnings(True) to show any performance warnings and file us issues if warnings exist for your use case. Defaults to False.   Returns \nif there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where Jacobian[i][j] will contain the Jacobian of the ith output and jth input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input.  Return type \nJacobian (Tensor or nested tuple of Tensors)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(2, 2)\n>>> jacobian(exp_reducer, inputs)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]])\n >>> jacobian(exp_reducer, inputs, create_graph=True)\ntensor([[[1.4917, 2.4352],\n         [0.0000, 0.0000]],\n        [[0.0000, 0.0000],\n         [2.4369, 2.3799]]], grad_fn=<ViewBackward>)\n >>> def exp_adder(x, y):\n...   return 2 * x.exp() + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> jacobian(exp_adder, inputs)\n(tensor([[2.8052, 0.0000],\n        [0.0000, 3.3963]]),\n tensor([[3., 0.],\n         [0., 3.]]))\n \n"}, {"name": "torch.autograd.functional.jvp()", "path": "autograd#torch.autograd.functional.jvp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector v.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the Jacobian vector product is computed. Must be the same size as the input of func. This argument is optional when the input to func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4, 4)\n>>> jvp(exp_reducer, inputs, v)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106]),\n tensor([6.3090, 4.6742, 7.9114, 8.2106]))\n >>> jvp(exp_reducer, inputs, v, create_graph=True)\n(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),\n tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.ones(2), torch.ones(2))\n>>> jvp(adder, inputs, v)\n(tensor([2.2399, 2.5005]),\n tensor([5., 5.]))\n  Note The jvp is currently computed by using the backward of the backward (sometimes called the double backwards trick) as we don\u2019t have support for forward mode AD in PyTorch at the moment.  \n"}, {"name": "torch.autograd.functional.vhp()", "path": "autograd#torch.autograd.functional.vhp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.vhp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor with a single element. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Hessian product is computed. Must be the same size as the input of func. This argument is optional when func\u2019s input contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vhp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def pow_reducer(x):\n...   return x.pow(3).sum()\n>>> inputs = torch.rand(2, 2)\n>>> v = torch.ones(2, 2)\n>>> vhp(pow_reducer, inputs, v)\n(tensor(0.5591),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]]))\n>>> vhp(pow_reducer, inputs, v, create_graph=True)\n(tensor(0.5591, grad_fn=<SumBackward0>),\n tensor([[1.0689, 1.2431],\n         [3.0989, 4.4456]], grad_fn=<MulBackward0>))\n>>> def pow_adder_reducer(x, y):\n...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = (torch.zeros(2), torch.ones(2))\n>>> vhp(pow_adder_reducer, inputs, v)\n(tensor(4.8053),\n (tensor([0., 0.]),\n  tensor([6., 6.])))\n \n"}, {"name": "torch.autograd.functional.vjp()", "path": "autograd#torch.autograd.functional.vjp", "type": "torch.autograd", "text": " \ntorch.autograd.functional.vjp(func, inputs, v=None, create_graph=False, strict=False) [source]\n \nFunction that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs.  Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor. \ninputs (tuple of Tensors or Tensor) \u2013 inputs to the function func. \nv (tuple of Tensors or Tensor) \u2013 The vector for which the vector Jacobian product is computed. Must be the same size as the output of func. This argument is optional when the output of func contains a single element and (if it is not provided) will be set as a Tensor containing a single 1. \ncreate_graph (bool, optional) \u2013 If True, both the output and result will be computed in a differentiable way. Note that when strict is False, the result can not require gradients or be disconnected from the inputs. Defaults to False. \nstrict (bool, optional) \u2013 If True, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If False, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to False.   Returns \n tuple with:\n\nfunc_output (tuple of Tensors or Tensor): output of func(inputs) vjp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.    Return type \noutput (tuple)   Example >>> def exp_reducer(x):\n...   return x.exp().sum(dim=1)\n>>> inputs = torch.rand(4, 4)\n>>> v = torch.ones(4)\n>>> vjp(exp_reducer, inputs, v)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782]),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]]))\n >>> vjp(exp_reducer, inputs, v, create_graph=True)\n(tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),\n tensor([[1.4458, 1.3962, 1.3042, 1.6354],\n        [2.1288, 1.0652, 1.5483, 2.5035],\n        [2.2046, 1.1292, 1.1432, 1.3059],\n        [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))\n >>> def adder(x, y):\n...   return 2 * x + 3 * y\n>>> inputs = (torch.rand(2), torch.rand(2))\n>>> v = torch.ones(2)\n>>> vjp(adder, inputs, v)\n(tensor([2.4225, 2.3340]),\n (tensor([2., 2.]), tensor([3., 3.])))\n \n"}, {"name": "torch.autograd.grad()", "path": "autograd#torch.autograd.grad", "type": "torch.autograd", "text": " \ntorch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False) [source]\n \nComputes and returns the sum of gradients of outputs w.r.t. the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None). If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.  Note If you run any forward ops, create grad_outputs, and/or call grad in a user-specified CUDA stream context, see Stream semantics of backward passes.   Parameters \n \noutputs (sequence of Tensor) \u2013 outputs of the differentiated function. \ninputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). \ngrad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. \ncreate_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. \nallow_unused (bool, optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.    \n"}, {"name": "torch.autograd.gradcheck()", "path": "autograd#torch.autograd.gradcheck", "type": "torch.autograd", "text": " \ntorch.autograd.gradcheck(func, inputs, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=False, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point or complex type and with requires_grad=True. The check between numerical and analytical gradients uses allclose(). For complex functions, no notion of Jacobian exists. Gradcheck verifies if the numerical and analytical values of Wirtinger and Conjugate Wirtinger derivative are consistent. The gradient computation is done under the assumption that the overall function has a real valued output. For functions with complex output, gradcheck compares the numerical and analytical gradients for two values of grad_output: 1 and 1j. For more details, check out Autograd for Complex Numbers.  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \ncheck_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros, for Tensor outputs. \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n"}, {"name": "torch.autograd.gradgradcheck()", "path": "autograd#torch.autograd.gradgradcheck", "type": "torch.autograd", "text": " \ntorch.autograd.gradgradcheck(func, inputs, grad_outputs=None, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False) [source]\n \nCheck gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters \n \nfunc (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors \ninputs (tuple of Tensor or Tensor) \u2013 inputs to the function \ngrad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. \neps (float, optional) \u2013 perturbation for finite differences \natol (float, optional) \u2013 absolute tolerance \nrtol (float, optional) \u2013 relative tolerance \ngen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous \nraise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. \nnondet_tol (float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative. \ncheck_undefined_grad (bool, optional) \u2013 if True, check if undefined output grads are supported and treated as zeros \ncheck_batched_grad (bool, optional) \u2013 if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.   Returns \nTrue if all differences satisfy allclose condition   \n"}, {"name": "torch.autograd.no_grad", "path": "autograd#torch.autograd.no_grad", "type": "torch.autograd", "text": " \nclass torch.autograd.no_grad [source]\n \nContext-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n \n"}, {"name": "torch.autograd.profiler.emit_nvtx", "path": "autograd#torch.autograd.profiler.emit_nvtx", "type": "torch.autograd", "text": " \nclass torch.autograd.profiler.emit_nvtx(enabled=True, record_shapes=False) [source]\n \nContext manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n Unfortunately, there\u2019s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection e.g. in Python REPL.  Parameters \n \nenabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op. Default: True. \nrecord_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by []. Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation.    Example >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, emit_nvtx appends sequence number information to the ranges it generates. During the forward pass, each function range is decorated with seq=<N>. seq is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the seq=<N> annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function\u2019s apply() call is decorated with stashed seq=<M>. M is the sequence number that the backward object was created with. By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N>. During default backward (with create_graph=False) this information is irrelevant, and in fact, N may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects\u2019 apply() methods are useful, as a way to correlate these Function objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words, if you are setting up for a double-backward), each function\u2019s execution during backward is given a nonzero, useful seq=<N>. Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects\u2019 apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. \n"}, {"name": "torch.autograd.profiler.load_nvprof()", "path": "autograd#torch.autograd.profiler.load_nvprof", "type": "torch.autograd", "text": " \ntorch.autograd.profiler.load_nvprof(path) [source]\n \nOpens an nvprof trace file and parses autograd annotations.  Parameters \npath (str) \u2013 path to nvprof trace   \n"}, {"name": "torch.autograd.profiler.profile", "path": "autograd#torch.autograd.profiler.profile", "type": "torch.autograd", "text": " \nclass torch.autograd.profiler.profile(enabled=True, *, use_cuda=False, record_shapes=False, with_flops=False, profile_memory=False, with_stack=False, use_kineto=False, use_cpu=True) [source]\n \nContext manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks  Parameters \n \nenabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. \nuse_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API. Adds approximately 4us of overhead to each tensor operation. \nrecord_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection. \nwith_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators. \nprofile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. \nwith_stack (bool, optional) \u2013 record source information (file and line number) for the ops. \nuse_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. \nuse_cpu (bool, optional) \u2013 profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling.    Example >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n  \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n  \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n  \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n  \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n  \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n \n"}, {"name": "torch.autograd.profiler.profile.export_chrome_trace()", "path": "autograd#torch.autograd.profiler.profile.export_chrome_trace", "type": "torch.autograd", "text": " \nexport_chrome_trace(path) [source]\n \nExports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters \npath (str) \u2013 Path where the trace will be written.   \n"}, {"name": "torch.autograd.profiler.profile.key_averages()", "path": "autograd#torch.autograd.profiler.profile.key_averages", "type": "torch.autograd", "text": " \nkey_averages(group_by_input_shape=False, group_by_stack_n=0) [source]\n \nAverages all function events over their keys.  Parameters \n \ngroup_by_input_shapes \u2013 group entries by \nname, input shapes) rather than just event name. ((event) \u2013  \nis useful to see which input shapes contribute to the runtime (This) \u2013  \nmost and may help with size-specific optimizations or (the) \u2013  \nthe best candidates for quantization (choosing) \u2013  \ngroup_by_stack_n \u2013 group by top n stack trace entries   Returns \nAn EventList containing FunctionEventAvg objects.   \n"}, {"name": "torch.autograd.profiler.profile.self_cpu_time_total()", "path": "autograd#torch.autograd.profiler.profile.self_cpu_time_total", "type": "torch.autograd", "text": " \nproperty self_cpu_time_total  \nReturns total time spent on CPU obtained as a sum of all self times across all the events. \n"}, {"name": "torch.autograd.profiler.profile.table()", "path": "autograd#torch.autograd.profiler.profile.table", "type": "torch.autograd", "text": " \ntable(sort_by=None, row_limit=100, max_src_column_width=75, header=None, top_level_events_only=False) [source]\n \nPrints an EventList as a nicely formatted table.  Parameters \n \nsort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, cpu_memory_usage, cuda_memory_usage, self_cpu_memory_usage, self_cuda_memory_usage, count. \ntop_level_events_only (bool, optional) \u2013 Boolean flag to determine the selection of events to display. If true, the profiler will only display events at top level like top-level invocation of python lstm, python add or other functions, nested events like low-level cpu/cuda ops events are omitted for profiler result readability.   Returns \nA string containing the table.   \n"}, {"name": "torch.autograd.profiler.profile.total_average()", "path": "autograd#torch.autograd.profiler.profile.total_average", "type": "torch.autograd", "text": " \ntotal_average() [source]\n \nAverages all events.  Returns \nA FunctionEventAvg object.   \n"}, {"name": "torch.autograd.set_detect_anomaly", "path": "autograd#torch.autograd.set_detect_anomaly", "type": "torch.autograd", "text": " \nclass torch.autograd.set_detect_anomaly(mode) [source]\n \nContext-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection based on its argument mode. It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour.  Parameters \nmode (bool) \u2013 Flag whether to enable anomaly detection (True), or disable (False).   \n"}, {"name": "torch.autograd.set_grad_enabled", "path": "autograd#torch.autograd.set_grad_enabled", "type": "torch.autograd", "text": " \nclass torch.autograd.set_grad_enabled(mode) [source]\n \nContext-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation in other threads.  Parameters \nmode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n \n"}, {"name": "torch.backends", "path": "backends", "type": "torch.backends", "text": "torch.backends torch.backends controls the behavior of various backends that PyTorch supports. These backends include:  torch.backends.cuda torch.backends.cudnn torch.backends.mkl torch.backends.mkldnn torch.backends.openmp  torch.backends.cuda  \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n  \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache caches the cuFFT plans  \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n  \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n  \nclear()  \nClears the cuFFT plan cache. \n \n torch.backends.cudnn  \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n  \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n  \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n  \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n  \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n  \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n torch.backends.mkl  \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n torch.backends.mkldnn  \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n torch.backends.openmp  \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n\n"}, {"name": "torch.backends.cuda.cufft_plan_cache", "path": "backends#torch.backends.cuda.cufft_plan_cache", "type": "torch.backends", "text": " \ntorch.backends.cuda.cufft_plan_cache  \ncufft_plan_cache caches the cuFFT plans  \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n  \nmax_size  \nA int that controls cache capacity of cuFFT plan. \n  \nclear()  \nClears the cuFFT plan cache. \n \n"}, {"name": "torch.backends.cuda.is_built()", "path": "backends#torch.backends.cuda.is_built", "type": "torch.backends", "text": " \ntorch.backends.cuda.is_built() [source]\n \nReturns whether PyTorch is built with CUDA support. Note that this doesn\u2019t necessarily mean CUDA is available; just that if this PyTorch binary were run a machine with working CUDA drivers and devices, we would be able to use it. \n"}, {"name": "torch.backends.cuda.matmul.allow_tf32", "path": "backends#torch.backends.cuda.matmul.allow_tf32", "type": "torch.backends", "text": " \ntorch.backends.cuda.matmul.allow_tf32  \nA bool that controls whether TensorFloat-32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cuda.size", "path": "backends#torch.backends.cuda.size", "type": "torch.backends", "text": " \nsize  \nA readonly int that shows the number of plans currently in the cuFFT plan cache. \n"}, {"name": "torch.backends.cudnn.allow_tf32", "path": "backends#torch.backends.cudnn.allow_tf32", "type": "torch.backends", "text": " \ntorch.backends.cudnn.allow_tf32  \nA bool that controls where TensorFloat-32 tensor cores may be used in cuDNN convolutions on Ampere or newer GPUs. See TensorFloat-32(TF32) on Ampere devices. \n"}, {"name": "torch.backends.cudnn.benchmark", "path": "backends#torch.backends.cudnn.benchmark", "type": "torch.backends", "text": " \ntorch.backends.cudnn.benchmark  \nA bool that, if True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest. \n"}, {"name": "torch.backends.cudnn.deterministic", "path": "backends#torch.backends.cudnn.deterministic", "type": "torch.backends", "text": " \ntorch.backends.cudnn.deterministic  \nA bool that, if True, causes cuDNN to only use deterministic convolution algorithms. See also torch.are_deterministic_algorithms_enabled() and torch.use_deterministic_algorithms(). \n"}, {"name": "torch.backends.cudnn.enabled", "path": "backends#torch.backends.cudnn.enabled", "type": "torch.backends", "text": " \ntorch.backends.cudnn.enabled  \nA bool that controls whether cuDNN is enabled. \n"}, {"name": "torch.backends.cudnn.is_available()", "path": "backends#torch.backends.cudnn.is_available", "type": "torch.backends", "text": " \ntorch.backends.cudnn.is_available() [source]\n \nReturns a bool indicating if CUDNN is currently available. \n"}, {"name": "torch.backends.cudnn.version()", "path": "backends#torch.backends.cudnn.version", "type": "torch.backends", "text": " \ntorch.backends.cudnn.version() [source]\n \nReturns the version of cuDNN \n"}, {"name": "torch.backends.mkl.is_available()", "path": "backends#torch.backends.mkl.is_available", "type": "torch.backends", "text": " \ntorch.backends.mkl.is_available() [source]\n \nReturns whether PyTorch is built with MKL support. \n"}, {"name": "torch.backends.mkldnn.is_available()", "path": "backends#torch.backends.mkldnn.is_available", "type": "torch.backends", "text": " \ntorch.backends.mkldnn.is_available() [source]\n \nReturns whether PyTorch is built with MKL-DNN support. \n"}, {"name": "torch.backends.openmp.is_available()", "path": "backends#torch.backends.openmp.is_available", "type": "torch.backends", "text": " \ntorch.backends.openmp.is_available() [source]\n \nReturns whether PyTorch is built with OpenMP support. \n"}, {"name": "torch.baddbmm()", "path": "generated/torch.baddbmm#torch.baddbmm", "type": "torch", "text": " \ntorch.baddbmm(input, batch1, batch2, *, beta=1, alpha=1, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().  outi=\u03b2inputi+\u03b1(batch1i@batch2i)\\text{out}_i = \\beta\\ \\text{input}_i + \\alpha\\ (\\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  \nIf beta is 0, then input will be ignored, and nan and inf in it will not be propagated. For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers. This operator supports TensorFloat32.  Parameters \n \ninput (Tensor) \u2013 the tensor to be added \nbatch1 (Tensor) \u2013 the first batch of matrices to be multiplied \nbatch2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \nbeta (Number, optional) \u2013 multiplier for input (\u03b2\\beta ) \nalpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}  (\u03b1\\alpha ) \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n \n"}, {"name": "torch.bartlett_window()", "path": "generated/torch.bartlett_window#torch.bartlett_window", "type": "torch", "text": " \ntorch.bartlett_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBartlett window function.  w[n]=1\u2212\u22232nN\u22121\u22121\u2223={2nN\u22121if 0\u2264n\u2264N\u2212122\u22122nN\u22121if N\u221212<n<N,w[n] = 1 - \\left| \\frac{2n}{N-1} - 1 \\right| = \\begin{cases} \\frac{2n}{N - 1} & \\text{if } 0 \\leq n \\leq \\frac{N - 1}{2} \\\\ 2 - \\frac{2n}{N - 1} & \\text{if } \\frac{N - 1}{2} < n < N \\\\ \\end{cases},  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.bernoulli()", "path": "generated/torch.bernoulli#torch.bernoulli", "type": "torch", "text": " \ntorch.bernoulli(input, *, generator=None, out=None) \u2192 Tensor  \nDraws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 1 . The ith\\text{i}^{th}  element of the output tensor will draw a value 11  according to the ith\\text{i}^{th}  probability value given in input.  outi\u223cBernoulli(p=inputi)\\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})  \nThe returned out tensor only has values 0 or 1 and is of the same shape as input. out can have integral dtype, but input must have floating point dtype.  Parameters \ninput (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution  Keyword Arguments \n \ngenerator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n \n"}, {"name": "torch.bincount()", "path": "generated/torch.bincount#torch.bincount", "type": "torch", "text": " \ntorch.bincount(input, weights=None, minlength=0) \u2192 Tensor  \nCount the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.  Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information.   Parameters \n \ninput (Tensor) \u2013 1-d int tensor \nweights (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input tensor. \nminlength (int) \u2013 optional, minimum number of bins. Should be non-negative.   Returns \na tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)  Return type \noutput (Tensor)   Example: >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n \n"}, {"name": "torch.bitwise_and()", "path": "generated/torch.bitwise_and#torch.bitwise_and", "type": "torch", "text": " \ntorch.bitwise_and(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise AND of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical AND.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])\n \n"}, {"name": "torch.bitwise_not()", "path": "generated/torch.bitwise_not#torch.bitwise_not", "type": "torch", "text": " \ntorch.bitwise_not(input, *, out=None) \u2192 Tensor  \nComputes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n \n"}, {"name": "torch.bitwise_or()", "path": "generated/torch.bitwise_or#torch.bitwise_or", "type": "torch", "text": " \ntorch.bitwise_or(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise OR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical OR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])\n \n"}, {"name": "torch.bitwise_xor()", "path": "generated/torch.bitwise_xor#torch.bitwise_xor", "type": "torch", "text": " \ntorch.bitwise_xor(input, other, *, out=None) \u2192 Tensor  \nComputes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.  Parameters \n \ninput \u2013 the first input tensor \nother \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n \n"}, {"name": "torch.blackman_window()", "path": "generated/torch.blackman_window#torch.blackman_window", "type": "torch", "text": " \ntorch.blackman_window(window_length, periodic=True, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nBlackman window function.  w[n]=0.42\u22120.5cos\u2061(2\u03c0nN\u22121)+0.08cos\u2061(4\u03c0nN\u22121)w[n] = 0.42 - 0.5 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right) + 0.08 \\cos \\left( \\frac{4 \\pi n}{N - 1} \\right)  \nwhere NN  is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NN  in above formula is in fact window_length+1\\text{window\\_length} + 1 . Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1 , the returned window contains a single value 1.   Parameters \n \nwindow_length (int) \u2013 the size of returned window \nperiodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. \nlayout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 1-D tensor of size (window_length,)(\\text{window\\_length},)  containing the window  Return type \nTensor   \n"}, {"name": "torch.block_diag()", "path": "generated/torch.block_diag#torch.block_diag", "type": "torch", "text": " \ntorch.block_diag(*tensors) [source]\n \nCreate a block diagonal matrix from provided tensors.  Parameters \n*tensors \u2013 One or more tensors with 0, 1, or 2 dimensions.  Returns \n A 2 dimensional tensor with all the input tensors arranged in\n\norder such that their upper left and lower right corners are diagonally adjacent. All other elements are set to 0.    Return type \nTensor   Example: >>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n \n"}, {"name": "torch.bmm()", "path": "generated/torch.bmm#torch.bmm", "type": "torch", "text": " \ntorch.bmm(input, mat2, *, deterministic=False, out=None) \u2192 Tensor  \nPerforms a batch matrix-matrix product of matrices stored in input and mat2. input and mat2 must be 3-D tensors each containing the same number of matrices. If input is a (b\u00d7n\u00d7m)(b \\times n \\times m)  tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)  tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)  tensor.  outi=inputi@mat2i\\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i  \nThis operator supports TensorFloat32.  Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().   Parameters \n \ninput (Tensor) \u2013 the first batch of matrices to be multiplied \nmat2 (Tensor) \u2013 the second batch of matrices to be multiplied   Keyword Arguments \n \ndeterministic (bool, optional) \u2013 flag to choose between a faster non-deterministic calculation, or a slower deterministic calculation. This argument is only available for sparse-dense CUDA bmm. Default: False\n \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])\n \n"}, {"name": "torch.broadcast_shapes()", "path": "generated/torch.broadcast_shapes#torch.broadcast_shapes", "type": "torch", "text": " \ntorch.broadcast_shapes(*shapes) \u2192 Size [source]\n \nSimilar to broadcast_tensors() but for shapes. This is equivalent to torch.broadcast_tensors(*map(torch.empty, shapes))[0].shape but avoids the need create to intermediate tensors. This is useful for broadcasting tensors of common batch shape but different rightmost shape, e.g. to broadcast mean vectors with covariance matrices. Example: >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])\n  Parameters \n*shapes (torch.Size) \u2013 Shapes of tensors.  Returns \nA shape compatible with all input shapes.  Return type \nshape (torch.Size)  Raises \nRuntimeError \u2013 If shapes are incompatible.   \n"}, {"name": "torch.broadcast_tensors()", "path": "generated/torch.broadcast_tensors#torch.broadcast_tensors", "type": "torch", "text": " \ntorch.broadcast_tensors(*tensors) \u2192 List of Tensors [source]\n \nBroadcasts the given tensors according to Broadcasting semantics.  Parameters \n*tensors \u2013 any number of tensors of the same type    Warning More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: >>> x = torch.arange(3).view(1, 3)\n>>> y = torch.arange(2).view(2, 1)\n>>> a, b = torch.broadcast_tensors(x, y)\n>>> a.size()\ntorch.Size([2, 3])\n>>> a\ntensor([[0, 1, 2],\n        [0, 1, 2]])\n \n"}, {"name": "torch.broadcast_to()", "path": "generated/torch.broadcast_to#torch.broadcast_to", "type": "torch", "text": " \ntorch.broadcast_to(input, shape) \u2192 Tensor  \nBroadcasts input to the shape shape. Equivalent to calling input.expand(shape). See expand() for details.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nshape (list, tuple, or torch.Size) \u2013 the new shape.    Example: >>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n \n"}, {"name": "torch.bucketize()", "path": "generated/torch.bucketize#torch.bucketize", "type": "torch", "text": " \ntorch.bucketize(input, boundaries, *, out_int32=False, right=False, out=None) \u2192 Tensor  \nReturns the indices of the buckets to which each value in the input belongs, where the boundaries of the buckets are set by boundaries. Return a new tensor with the same size as input. If right is False (default), then the left boundary is closed. More formally, the returned index satisfies the following rules:   \nright returned index satisfies   \nFalse boundaries[i-1] < input[m][n]...[l][x] <= boundaries[i]  \nTrue boundaries[i-1] <= input[m][n]...[l][x] < boundaries[i]    Parameters \n \ninput (Tensor or Scalar) \u2013 N-D tensor or a Scalar containing the search value(s). \nboundaries (Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence.   Keyword Arguments \n \nout_int32 (bool, optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise. Default value is False, i.e. default output data type is torch.int64. \nright (bool, optional) \u2013 if False, return the first suitable location that is found. If True, return the last such index. If no suitable index found, return 0 for non-numerical value (eg. nan, inf) or the size of boundaries (one pass the last index). In other words, if False, gets the lower bound index for each value in input from boundaries. If True, gets the upper bound index instead. Default value is False. \nout (Tensor, optional) \u2013 the output tensor, must be the same size as input if provided.    Example: >>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])\n \n"}, {"name": "torch.can_cast()", "path": "generated/torch.can_cast#torch.can_cast", "type": "torch", "text": " \ntorch.can_cast(from, to) \u2192 bool  \nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  Parameters \n \nfrom (dpython:type) \u2013 The original torch.dtype. \nto (dpython:type) \u2013 The target torch.dtype.    Example: >>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse\n \n"}, {"name": "torch.cartesian_prod()", "path": "generated/torch.cartesian_prod#torch.cartesian_prod", "type": "torch", "text": " \ntorch.cartesian_prod(*tensors) [source]\n \nDo cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.  Parameters \n*tensors \u2013 any number of 1 dimensional tensors.  Returns \n A tensor equivalent to converting all the input tensors into lists,\n\ndo itertools.product on these lists, and finally convert the resulting list into tensor.    Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n \n"}, {"name": "torch.cat()", "path": "generated/torch.cat#torch.cat", "type": "torch", "text": " \ntorch.cat(tensors, dim=0, *, out=None) \u2192 Tensor  \nConcatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk(). torch.cat() can be best understood via examples.  Parameters \n \ntensors (sequence of Tensors) \u2013 any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension. \ndim (int, optional) \u2013 the dimension over which the tensors are concatenated   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n \n"}, {"name": "torch.cdist()", "path": "generated/torch.cdist#torch.cdist", "type": "torch", "text": " \ntorch.cdist(x1, x2, p=2.0, compute_mode='use_mm_for_euclid_dist_if_necessary') [source]\n \nComputes batched the p-norm distance between each pair of the two collections of row vectors.  Parameters \n \nx1 (Tensor) \u2013 input tensor of shape B\u00d7P\u00d7MB \\times P \\times M . \nx2 (Tensor) \u2013 input tensor of shape B\u00d7R\u00d7MB \\times R \\times M . \np \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty] . \ncompute_mode \u2013 \u2018use_mm_for_euclid_dist_if_necessary\u2019 - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P > 25 or R > 25 \u2018use_mm_for_euclid_dist\u2019 - will always use matrix multiplication approach to calculate euclidean distance (p = 2) \u2018donot_use_mm_for_euclid_dist\u2019 - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.    If x1 has shape B\u00d7P\u00d7MB \\times P \\times M  and x2 has shape B\u00d7R\u00d7MB \\times R \\times M  then the output will have shape B\u00d7P\u00d7RB \\times P \\times R . This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty) . When p=0p = 0  it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\infty , the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). Example >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n \n"}, {"name": "torch.ceil()", "path": "generated/torch.ceil#torch.ceil", "type": "torch", "text": " \ntorch.ceil(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.  outi=\u2308inputi\u2309=\u230ainputi\u230b+1\\text{out}_{i} = \\left\\lceil \\text{input}_{i} \\right\\rceil = \\left\\lfloor \\text{input}_{i} \\right\\rfloor + 1  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n \n"}, {"name": "torch.chain_matmul()", "path": "generated/torch.chain_matmul#torch.chain_matmul", "type": "torch", "text": " \ntorch.chain_matmul(*matrices) [source]\n \nReturns the matrix product of the NN  2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NN  needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NN  is 1, then this is a no-op - the original matrix is returned as is.  Parameters \nmatrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined.  Returns \nif the ithi^{th}  tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1} , then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1} .  Return type \nTensor   Example: >>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n \n"}, {"name": "torch.cholesky()", "path": "generated/torch.cholesky#torch.cholesky", "type": "torch", "text": " \ntorch.cholesky(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the Cholesky decomposition of a symmetric positive-definite matrix AA  or for batches of symmetric positive-definite matrices. If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:  A=UTUA = U^TU \nIf upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:  A=LLTA = LL^T \nIf upper is True, and AA  is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.  Note torch.linalg.cholesky() should be used over torch.cholesky when possible. Note however that torch.linalg.cholesky() does not yet support the upper parameter and instead always returns the lower triangular matrix.   Parameters \n \ninput (Tensor) \u2013 the input tensor AA  of size (\u2217,n,n)(*, n, n)  where * is zero or more batch dimensions consisting of symmetric positive-definite matrices. \nupper (bool, optional) \u2013 flag that indicates whether to return a upper or lower triangular matrix. Default: False\n   Keyword Arguments \nout (Tensor, optional) \u2013 the output matrix   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n \n"}, {"name": "torch.cholesky_inverse()", "path": "generated/torch.cholesky_inverse#torch.cholesky_inverse", "type": "torch", "text": " \ntorch.cholesky_inverse(input, upper=False, *, out=None) \u2192 Tensor  \nComputes the inverse of a symmetric positive-definite matrix AA  using its Cholesky factor uu : returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uu  is lower triangular such that the returned tensor is  inv=(uuT)\u22121inv = (uu^{{T}})^{{-1}}  \nIf upper is True or not provided, uu  is upper triangular such that the returned tensor is  inv=(uTu)\u22121inv = (u^T u)^{{-1}}  \n Parameters \n \ninput (Tensor) \u2013 the input 2-D tensor uu , a upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for inv   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n \n"}, {"name": "torch.cholesky_solve()", "path": "generated/torch.cholesky_solve#torch.cholesky_solve", "type": "torch", "text": " \ntorch.cholesky_solve(input, input2, upper=False, *, out=None) \u2192 Tensor  \nSolves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uu . If upper is False, uu  is and lower triangular and c is returned such that:  c=(uuT)\u22121bc = (u u^T)^{{-1}} b  \nIf upper is True or not provided, uu  is upper triangular and c is returned such that:  c=(uTu)\u22121bc = (u^T u)^{{-1}} b  \ntorch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c Supports real-valued and complex-valued inputs. For the complex-valued inputs the transpose operator above is the conjugate transpose.  Parameters \n \ninput (Tensor) \u2013 input matrix bb  of size (\u2217,m,k)(*, m, k) , where \u2217*  is zero or more batch dimensions \ninput2 (Tensor) \u2013 input matrix uu  of size (\u2217,m,m)(*, m, m) , where \u2217*  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor \nupper (bool, optional) \u2013 whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: False.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor for c   Example: >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n \n"}, {"name": "torch.chunk()", "path": "generated/torch.chunk#torch.chunk", "type": "torch", "text": " \ntorch.chunk(input, chunks, dim=0) \u2192 List of Tensors  \nSplits a tensor into a specific number of chunks. Each chunk is a view of the input tensor. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.  Parameters \n \ninput (Tensor) \u2013 the tensor to split \nchunks (int) \u2013 number of chunks to return \ndim (int) \u2013 dimension along which to split the tensor    \n"}, {"name": "torch.clamp()", "path": "generated/torch.clamp#torch.clamp", "type": "torch", "text": " \ntorch.clamp(input, min, max, *, out=None) \u2192 Tensor  \nClamp all elements in input into the range [ min, max ]. Let min_value and max_value be min and max, respectively, this returns:  yi=min\u2061(max\u2061(xi,min_value),max_value)y_i = \\min(\\max(x_i, \\text{min\\_value}), \\text{max\\_value})  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \nmin (Number) \u2013 lower-bound of the range to be clamped to \nmax (Number) \u2013 upper-bound of the range to be clamped to   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n  \ntorch.clamp(input, *, min, out=None) \u2192 Tensor \n Clamps all elements in input to be larger or equal min.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \n \nmin (Number) \u2013 minimal value of each element in the output \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.0299, -2.3184,  2.1593, -0.8883])\n>>> torch.clamp(a, min=0.5)\ntensor([ 0.5000,  0.5000,  2.1593,  0.5000])\n  \ntorch.clamp(input, *, max, out=None) \u2192 Tensor \n Clamps all elements in input to be smaller or equal max.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \n \nmax (Number) \u2013 maximal value of each element in the output \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.7753, -0.4702, -0.4599,  1.1899])\n>>> torch.clamp(a, max=0.5)\ntensor([ 0.5000, -0.4702, -0.4599,  0.5000])\n \n"}, {"name": "torch.clip()", "path": "generated/torch.clip#torch.clip", "type": "torch", "text": " \ntorch.clip(input, min, max, *, out=None) \u2192 Tensor  \nAlias for torch.clamp(). \n"}, {"name": "torch.clone()", "path": "generated/torch.clone#torch.clone", "type": "torch", "text": " \ntorch.clone(input, *, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a copy of input.  Note This function is differentiable, so gradients will flow back from the result of this operation to input. To create a tensor without an autograd relationship to input see detach().   Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned tensor. Default: torch.preserve_format.   \n"}, {"name": "torch.column_stack()", "path": "generated/torch.column_stack#torch.column_stack", "type": "torch", "text": " \ntorch.column_stack(tensors, *, out=None) \u2192 Tensor  \nCreates a new tensor by horizontally stacking the tensors in tensors. Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t in tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally.  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])\n \n"}, {"name": "torch.combinations()", "path": "generated/torch.combinations#torch.combinations", "type": "torch", "text": " \ntorch.combinations(input, r=2, with_replacement=False) \u2192 seq  \nCompute combinations of length rr  of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters \n \ninput (Tensor) \u2013 1D vector. \nr (int, optional) \u2013 number of elements to combine \nwith_replacement (boolean, optional) \u2013 whether to allow duplication in combination   Returns \nA tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type \nTensor   Example: >>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n \n"}, {"name": "torch.compiled_with_cxx11_abi()", "path": "generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi", "type": "torch", "text": " \ntorch.compiled_with_cxx11_abi() [source]\n \nReturns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1 \n"}, {"name": "torch.complex()", "path": "generated/torch.complex#torch.complex", "type": "torch", "text": " \ntorch.complex(real, imag, *, out=None) \u2192 Tensor  \nConstructs a complex tensor with its real part equal to real and its imaginary part equal to imag.  Parameters \n \nreal (Tensor) \u2013 The real part of the complex tensor. Must be float or double. \nimag (Tensor) \u2013 The imaginary part of the complex tensor. Must be same dtype as real.   Keyword Arguments \nout (Tensor) \u2013 If the inputs are torch.float32, must be torch.complex64. If the inputs are torch.float64, must be torch.complex128.    Example::\n\n>>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64\n   \n"}, {"name": "torch.conj()", "path": "generated/torch.conj#torch.conj", "type": "torch", "text": " \ntorch.conj(input, *, out=None) \u2192 Tensor  \nComputes the element-wise conjugate of the given input tensor. If :attr`input` has a non-complex dtype, this function just returns input.  Warning In the future, torch.conj() may return a non-writeable view for an input of non-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj() when input is of non-complex dtype to be compatible with this change.   outi=conj(inputi)\\text{out}_{i} = conj(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n \n"}, {"name": "torch.copysign()", "path": "generated/torch.copysign#torch.copysign", "type": "torch", "text": " \ntorch.copysign(input, other, *, out=None) \u2192 Tensor  \nCreate a new floating-point tensor with the magnitude of input and the sign of other, elementwise.  outi={\u2212\u2223inputi\u2223ifotheri\u2264\u22120.0\u2223inputi\u2223ifotheri\u22650.0\\text{out}_{i} = \\begin{cases} -|\\text{input}_{i}| & \\text{if} \\text{other}_{i} \\leq -0.0 \\\\ |\\text{input}_{i}| & \\text{if} \\text{other}_{i} \\geq 0.0 \\\\ \\end{cases}  \nSupports broadcasting to a common shape, and integer and float inputs.  Parameters \n \ninput (Tensor) \u2013 magnitudes. \nother (Tensor or Number) \u2013 contains value(s) whose signbit(s) are applied to the magnitudes in input.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\n \n"}, {"name": "torch.cos()", "path": "generated/torch.cos#torch.cos", "type": "torch", "text": " \ntorch.cos(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the cosine of the elements of input.  outi=cos\u2061(inputi)\\text{out}_{i} = \\cos(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n \n"}, {"name": "torch.cosh()", "path": "generated/torch.cosh#torch.cosh", "type": "torch", "text": " \ntorch.cosh(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the hyperbolic cosine of the elements of input.  outi=cosh\u2061(inputi)\\text{out}_{i} = \\cosh(\\text{input}_{i})  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n  Note When input is on the CPU, the implementation of torch.cosh may use the Sleef library, which rounds very large results to infinity or negative infinity. See here for details.  \n"}, {"name": "torch.count_nonzero()", "path": "generated/torch.count_nonzero#torch.count_nonzero", "type": "torch", "text": " \ntorch.count_nonzero(input, dim=None) \u2192 Tensor  \nCounts the number of non-zero values in the tensor input along the given dim. If no dim is specified then all non-zeros in the tensor are counted.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros.    Example: >>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])\n \n"}, {"name": "torch.cross()", "path": "generated/torch.cross#torch.cross", "type": "torch", "text": " \ntorch.cross(input, other, dim=None, *, out=None) \u2192 Tensor  \nReturns the cross product of vectors in dimension dim of input and other. input and other must have the same size, and the size of their dim dimension should be 3. If dim is not given, it defaults to the first dimension found with the size 3. Note that this might be unexpected.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor \ndim (int, optional) \u2013 the dimension to take the cross-product in.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n \n"}, {"name": "torch.cuda", "path": "cuda", "type": "torch.cuda", "text": "torch.cuda This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.  \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible. \n  \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n  \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device. \n  \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).   \n  \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).   \n  \nclass torch.cuda.device(device) [source]\n \nContext-manager that changes the selected device.  Parameters \ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   \n  \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available. \n  \nclass torch.cuda.device_of(obj) [source]\n \nContext-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters \nobj (Tensor or Storage) \u2013 object allocated on the selected device.   \n  \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for. \n  \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n  \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n  \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n  \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library were compiled with. \n  \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n  \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n  \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available. \n  \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n  \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n  \ntorch.cuda.stream(stream) [source]\n \nContext-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.  \n  \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n Random Number Generator  \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    Warning This function eagerly initializes CUDA.  \n  \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices. \n  \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n  \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n  \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n  \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n  \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n  \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n  \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.  \n Communication collectives  \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n  \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n  \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n  \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n  \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n Streams and events  \nclass torch.cuda.Stream [source]\n \nWrapper around a CUDA stream. A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.  Parameters \n \ndevice (torch.device or int, optional) \u2013 a device on which to allocate the stream. If device is None (default) or a negative integer, this will use the current device. \npriority (int, optional) \u2013 priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.     Note Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.   \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n  \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n  \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n  \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n  \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n \n  \nclass torch.cuda.Event [source]\n \nWrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters \n \nenable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) \nblocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) \ninterprocess (bool) \u2013 if True, the event can be shared between processes (default: False)     \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n  \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n  \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n  \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n  \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n  \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n  \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n \n Memory management  \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).   \n  \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).     Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n  \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n  \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved(). \n  \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved(). \n  \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n NVIDIA Tools Extension (NVTX)  \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (string) \u2013 ASCII message to associate with the event.   \n  \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (string) \u2013 ASCII message to associate with range   \n  \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n\n"}, {"name": "torch.cuda.amp", "path": "amp", "type": "torch.cuda.amp", "text": "Automatic Mixed Precision package - torch.cuda.amp torch.cuda.amp provides convenience methods for mixed precision, where some operations use the torch.float32 (float) datatype and other operations use torch.float16 (half). Some ops, like linear layers and convolutions, are much faster in float16. Other ops, like reductions, often require the dynamic range of float32. Mixed precision tries to match each op to its appropriate datatype. Ordinarily, \u201cautomatic mixed precision training\u201d uses torch.cuda.amp.autocast and torch.cuda.amp.GradScaler together, as shown in the Automatic Mixed Precision examples and Automatic Mixed Precision recipe. However, autocast and GradScaler are modular, and may be used separately if desired.  Autocasting Gradient Scaling \nAutocast Op Reference  Op Eligibility \nOp-Specific Behavior  Ops that can autocast to float16 Ops that can autocast to float32 Ops that promote to the widest input type Prefer binary_cross_entropy_with_logits over binary_cross_entropy      Autocasting  \nclass torch.cuda.amp.autocast(enabled=True) [source]\n \nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details. When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops. Example: # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions). autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue. autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).  Parameters \nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region.   \n  \ntorch.cuda.amp.custom_fwd(fwd=None, **kwargs) [source]\n \nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.  Parameters \ncast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.    Note If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.  \n  \ntorch.cuda.amp.custom_bwd(bwd) [source]\n \nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail. \n Gradient Scaling If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (\u201cunderflow\u201d), so the update for the corresponding parameters will be lost. To prevent underflow, \u201cgradient scaling\u201d multiplies the network\u2019s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don\u2019t flush to zero. Each parameter\u2019s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.  \nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True) [source]\n \n \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n  \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n  \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n  \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n  \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n  \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n  \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n  \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n  \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n  \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n  \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n  \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n  \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n  \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n \n Autocast Op Reference Op Eligibility Only CUDA ops are eligible for autocasting. Ops that run in float64 or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled. Only out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an out=... Tensor are allowed in autocast-enabled regions, but won\u2019t go through autocasting. For example, in an autocast-enabled region a.addmm(b, c) can autocast, but a.addmm_(b, c) and a.addmm(b, c, out=d) cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions. Ops called with an explicit dtype=... argument are not eligible, and will produce output that respects the dtype argument. Op-Specific Behavior The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a torch.nn.Module, as a function, or as a torch.Tensor method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace. Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they\u2019re downstream from autocasted ops. If an op is unlisted, we assume it\u2019s numerically stable in float16. If you believe an unlisted op is numerically unstable in float16, please file an issue. Ops that can autocast to float16\n __matmul__, addbmm, addmm, addmv, addr, baddbmm, bmm, chain_matmul, conv1d, conv2d, conv3d, conv_transpose1d, conv_transpose2d, conv_transpose3d, GRUCell, linear, LSTMCell, matmul, mm, mv, prelu, RNNCell Ops that can autocast to float32\n __pow__, __rdiv__, __rpow__, __rtruediv__, acos, asin, binary_cross_entropy_with_logits, cosh, cosine_embedding_loss, cdist, cosine_similarity, cross_entropy, cumprod, cumsum, dist, erfinv, exp, expm1, gelu, group_norm, hinge_embedding_loss, kl_div, l1_loss, layer_norm, log, log_softmax, log10, log1p, log2, margin_ranking_loss, mse_loss, multilabel_margin_loss, multi_margin_loss, nll_loss, norm, normalize, pdist, poisson_nll_loss, pow, prod, reciprocal, rsqrt, sinh, smooth_l1_loss, soft_margin_loss, softmax, softmin, softplus, sum, renorm, tan, triplet_margin_loss Ops that promote to the widest input type These ops don\u2019t require a particular dtype for stability, but take multiple inputs and require that the inputs\u2019 dtypes match. If all of the inputs are float16, the op runs in float16. If any of the inputs is float32, autocast casts all inputs to float32 and runs the op in float32. addcdiv, addcmul, atan2, bilinear, cat, cross, dot, equal, index_put, stack, tensordot Some ops not listed here (e.g., binary ops like add) natively promote inputs without autocasting\u2019s intervention. If inputs are a mixture of float16 and float32, these ops run in float32 and produce float32 output, regardless of whether autocast is enabled. Prefer binary_cross_entropy_with_logits over binary_cross_entropy\n The backward passes of torch.nn.functional.binary_cross_entropy() (and torch.nn.BCELoss, which wraps it) can produce gradients that aren\u2019t representable in float16. In autocast-enabled regions, the forward input may be float16, which means the backward gradient must be representable in float16 (autocasting float16 forward inputs to float32 doesn\u2019t help, because that cast must be reversed in backward). Therefore, binary_cross_entropy and BCELoss raise an error in autocast-enabled regions. Many models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits() or torch.nn.BCEWithLogitsLoss. binary_cross_entropy_with_logits and BCEWithLogits are safe to autocast.\n"}, {"name": "torch.cuda.amp.autocast", "path": "amp#torch.cuda.amp.autocast", "type": "torch.cuda.amp", "text": " \nclass torch.cuda.amp.autocast(enabled=True) [source]\n \nInstances of autocast serve as context managers or decorators that allow regions of your script to run in mixed precision. In these regions, CUDA ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the Autocast Op Reference for details. When entering an autocast-enabled region, Tensors may be any type. You should not call .half() on your model(s) or inputs when using autocasting. autocast should wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops. Example: # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n See the Automatic Mixed Precision examples for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions). autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n Floating-point Tensors produced in an autocast-enabled region may be float16. After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to float32 (or other dtype if desired). If a Tensor from the autocast region is already float32, the cast is a no-op, and incurs no additional overhead. Example: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe, please file an issue. autocast(enabled=False) subregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular dtype. Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n The autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects torch.nn.DataParallel and torch.nn.parallel.DistributedDataParallel when used with more than one GPU per process (see Working with Multiple GPUs).  Parameters \nenabled (bool, optional, default=True) \u2013 Whether autocasting should be enabled in the region.   \n"}, {"name": "torch.cuda.amp.custom_bwd()", "path": "amp#torch.cuda.amp.custom_bwd", "type": "torch.cuda.amp", "text": " \ntorch.cuda.amp.custom_bwd(bwd) [source]\n \nHelper decorator for backward methods of custom autograd functions (subclasses of torch.autograd.Function). Ensures that backward executes with the same autocast state as forward. See the example page for more detail. \n"}, {"name": "torch.cuda.amp.custom_fwd()", "path": "amp#torch.cuda.amp.custom_fwd", "type": "torch.cuda.amp", "text": " \ntorch.cuda.amp.custom_fwd(fwd=None, **kwargs) [source]\n \nHelper decorator for forward methods of custom autograd functions (subclasses of torch.autograd.Function). See the example page for more detail.  Parameters \ncast_inputs (torch.dtype or None, optional, default=None) \u2013 If not None, when forward runs in an autocast-enabled region, casts incoming floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected), then executes forward with autocast disabled. If None, forward\u2019s internal ops execute with the current autocast state.    Note If the decorated forward is called outside an autocast-enabled region, custom_fwd is a no-op and cast_inputs has no effect.  \n"}, {"name": "torch.cuda.amp.GradScaler", "path": "amp#torch.cuda.amp.GradScaler", "type": "torch.cuda.amp", "text": " \nclass torch.cuda.amp.GradScaler(init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000, enabled=True) [source]\n \n \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n  \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n  \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n  \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n  \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n  \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n  \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n  \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n  \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n  \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n  \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n  \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n  \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n  \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n \n"}, {"name": "torch.cuda.amp.GradScaler.get_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_backoff_factor", "type": "torch.cuda.amp", "text": " \nget_backoff_factor() [source]\n \nReturns a Python float containing the scale backoff factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_factor", "type": "torch.cuda.amp", "text": " \nget_growth_factor() [source]\n \nReturns a Python float containing the scale growth factor. \n"}, {"name": "torch.cuda.amp.GradScaler.get_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.get_growth_interval", "type": "torch.cuda.amp", "text": " \nget_growth_interval() [source]\n \nReturns a Python int containing the growth interval. \n"}, {"name": "torch.cuda.amp.GradScaler.get_scale()", "path": "amp#torch.cuda.amp.GradScaler.get_scale", "type": "torch.cuda.amp", "text": " \nget_scale() [source]\n \nReturns a Python float containing the current scale, or 1.0 if scaling is disabled.  Warning get_scale() incurs a CPU-GPU sync.  \n"}, {"name": "torch.cuda.amp.GradScaler.is_enabled()", "path": "amp#torch.cuda.amp.GradScaler.is_enabled", "type": "torch.cuda.amp", "text": " \nis_enabled() [source]\n \nReturns a bool indicating whether this instance is enabled. \n"}, {"name": "torch.cuda.amp.GradScaler.load_state_dict()", "path": "amp#torch.cuda.amp.GradScaler.load_state_dict", "type": "torch.cuda.amp", "text": " \nload_state_dict(state_dict) [source]\n \nLoads the scaler state. If this instance is disabled, load_state_dict() is a no-op.  Parameters \nstate_dict (dict) \u2013 scaler state. Should be an object returned from a call to state_dict().   \n"}, {"name": "torch.cuda.amp.GradScaler.scale()", "path": "amp#torch.cuda.amp.GradScaler.scale", "type": "torch.cuda.amp", "text": " \nscale(outputs) [source]\n \nMultiplies (\u2018scales\u2019) a tensor or list of tensors by the scale factor. Returns scaled outputs. If this instance of GradScaler is not enabled, outputs are returned unmodified.  Parameters \noutputs (Tensor or iterable of Tensors) \u2013 Outputs to scale.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_backoff_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_backoff_factor", "type": "torch.cuda.amp", "text": " \nset_backoff_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale backoff factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_factor()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_factor", "type": "torch.cuda.amp", "text": " \nset_growth_factor(new_factor) [source]\n \n Parameters \nnew_scale (float) \u2013 Value to use as the new scale growth factor.   \n"}, {"name": "torch.cuda.amp.GradScaler.set_growth_interval()", "path": "amp#torch.cuda.amp.GradScaler.set_growth_interval", "type": "torch.cuda.amp", "text": " \nset_growth_interval(new_interval) [source]\n \n Parameters \nnew_interval (int) \u2013 Value to use as the new growth interval.   \n"}, {"name": "torch.cuda.amp.GradScaler.state_dict()", "path": "amp#torch.cuda.amp.GradScaler.state_dict", "type": "torch.cuda.amp", "text": " \nstate_dict() [source]\n \nReturns the state of the scaler as a dict. It contains five entries:  \n\"scale\" - a Python float containing the current scale \n\"growth_factor\" - a Python float containing the current growth factor \n\"backoff_factor\" - a Python float containing the current backoff factor \n\"growth_interval\" - a Python int containing the current growth interval \n\"_growth_tracker\" - a Python int containing the number of recent consecutive unskipped steps.  If this instance is not enabled, returns an empty dict.  Note If you wish to checkpoint the scaler\u2019s state after a particular iteration, state_dict() should be called after update().  \n"}, {"name": "torch.cuda.amp.GradScaler.step()", "path": "amp#torch.cuda.amp.GradScaler.step", "type": "torch.cuda.amp", "text": " \nstep(optimizer, *args, **kwargs) [source]\n \nstep() carries out the following two operations:  Internally invokes unscale_(optimizer) (unless unscale_() was explicitly called for optimizer earlier in the iteration). As part of the unscale_(), gradients are checked for infs/NaNs. If no inf/NaN gradients are found, invokes optimizer.step() using the unscaled gradients. Otherwise, optimizer.step() is skipped to avoid corrupting the params.  *args and **kwargs are forwarded to optimizer.step(). Returns the return value of optimizer.step(*args, **kwargs).  Parameters \n \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that applies the gradients. \nargs \u2013 Any arguments. \nkwargs \u2013 Any keyword arguments.     Warning Closure use is not currently supported.  \n"}, {"name": "torch.cuda.amp.GradScaler.unscale_()", "path": "amp#torch.cuda.amp.GradScaler.unscale_", "type": "torch.cuda.amp", "text": " \nunscale_(optimizer) [source]\n \nDivides (\u201cunscales\u201d) the optimizer\u2019s gradient tensors by the scale factor. unscale_() is optional, serving cases where you need to modify or inspect gradients between the backward pass(es) and step(). If unscale_() is not called explicitly, gradients will be unscaled automatically during step(). Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n  Parameters \noptimizer (torch.optim.Optimizer) \u2013 Optimizer that owns the gradients to be unscaled.    Note unscale_() does not incur a CPU-GPU sync.   Warning unscale_() should only be called once per optimizer per step() call, and only after all gradients for that optimizer\u2019s assigned parameters have been accumulated. Calling unscale_() twice for a given optimizer between each step() triggers a RuntimeError.   Warning unscale_() may unscale sparse gradients out of place, replacing the .grad attribute.  \n"}, {"name": "torch.cuda.amp.GradScaler.update()", "path": "amp#torch.cuda.amp.GradScaler.update", "type": "torch.cuda.amp", "text": " \nupdate(new_scale=None) [source]\n \nUpdates the scale factor. If any optimizer steps were skipped the scale is multiplied by backoff_factor to reduce it. If growth_interval unskipped iterations occurred consecutively, the scale is multiplied by growth_factor to increase it. Passing new_scale sets the scale directly.  Parameters \nnew_scale (float or torch.cuda.FloatTensor, optional, default=None) \u2013 New scale factor.    Warning update() should only be called at the end of the iteration, after scaler.step(optimizer) has been invoked for all optimizers used this iteration.  \n"}, {"name": "torch.cuda.can_device_access_peer()", "path": "cuda#torch.cuda.can_device_access_peer", "type": "torch.cuda", "text": " \ntorch.cuda.can_device_access_peer(device, peer_device) [source]\n \nChecks if peer access between two devices is possible. \n"}, {"name": "torch.cuda.comm.broadcast()", "path": "cuda#torch.cuda.comm.broadcast", "type": "torch.cuda", "text": " \ntorch.cuda.comm.broadcast(tensor, devices=None, *, out=None) [source]\n \nBroadcasts a tensor to specified GPU devices.  Parameters \n \ntensor (Tensor) \u2013 tensor to broadcast. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to broadcast. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results.     Note Exactly one of devices and out must be specified.   Returns \n \n \nIf devices is specified, \n\na tuple containing copies of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a copy of tensor.       \n"}, {"name": "torch.cuda.comm.broadcast_coalesced()", "path": "cuda#torch.cuda.comm.broadcast_coalesced", "type": "torch.cuda", "text": " \ntorch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760) [source]\n \nBroadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters \n \ntensors (sequence) \u2013 tensors to broadcast. Must be on the same device, either CPU or GPU. \ndevices (Iterable[torch.device, str or int]) \u2013 an iterable of GPU devices, among which to broadcast. \nbuffer_size (int) \u2013 maximum size of the buffer used for coalescing   Returns \nA tuple containing copies of tensor, placed on devices.   \n"}, {"name": "torch.cuda.comm.gather()", "path": "cuda#torch.cuda.comm.gather", "type": "torch.cuda", "text": " \ntorch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None) [source]\n \nGathers tensors from multiple GPU devices.  Parameters \n \ntensors (Iterable[Tensor]) \u2013 an iterable of tensors to gather. Tensor sizes in all dimensions other than dim have to match. \ndim (int, optional) \u2013 a dimension along which the tensors will be concatenated. Default: 0. \ndestination (torch.device, str, or int, optional) \u2013 the output device. Can be CPU or CUDA. Default: the current CUDA device. \nout (Tensor, optional, keyword-only) \u2013 the tensor to store gather result. Its sizes must match those of tensors, except for dim, where the size must equal sum(tensor.size(dim) for tensor in tensors). Can be on CPU or CUDA.     Note destination must not be specified when out is specified.   Returns \n \n \nIf destination is specified, \n\na tensor located on destination device, that is a result of concatenating tensors along dim.    \n \nIf out is specified, \n\nthe out tensor, now containing results of concatenating tensors along dim.       \n"}, {"name": "torch.cuda.comm.reduce_add()", "path": "cuda#torch.cuda.comm.reduce_add", "type": "torch.cuda", "text": " \ntorch.cuda.comm.reduce_add(inputs, destination=None) [source]\n \nSums tensors from multiple GPUs. All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.  Parameters \n \ninputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. \ndestination (int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns \nA tensor containing an elementwise sum of all inputs, placed on the destination device.   \n"}, {"name": "torch.cuda.comm.scatter()", "path": "cuda#torch.cuda.comm.scatter", "type": "torch.cuda", "text": " \ntorch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None) [source]\n \nScatters tensor across multiple GPUs.  Parameters \n \ntensor (Tensor) \u2013 tensor to scatter. Can be on CPU or GPU. \ndevices (Iterable[torch.device, str or int], optional) \u2013 an iterable of GPU devices, among which to scatter. \nchunk_sizes (Iterable[int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sums to tensor.size(dim). If not specified, tensor will be divided into equal chunks. \ndim (int, optional) \u2013 A dimension along which to chunk tensor. Default: 0. \nstreams (Iterable[Stream], optional) \u2013 an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized. \nout (Sequence[Tensor], optional, keyword-only) \u2013 the GPU tensors to store output results. Sizes of these tensors must match that of tensor, except for dim, where the total size must sum to tensor.size(dim).     Note Exactly one of devices and out must be specified. When out is specified, chunk_sizes must not be specified and will be inferred from sizes of out.   Returns \n \n \nIf devices is specified, \n\na tuple containing chunks of tensor, placed on devices.    \n \nIf out is specified, \n\na tuple containing out tensors, each containing a chunk of tensor.       \n"}, {"name": "torch.cuda.current_blas_handle()", "path": "cuda#torch.cuda.current_blas_handle", "type": "torch.cuda", "text": " \ntorch.cuda.current_blas_handle() [source]\n \nReturns cublasHandle_t pointer to current cuBLAS handle \n"}, {"name": "torch.cuda.current_device()", "path": "cuda#torch.cuda.current_device", "type": "torch.cuda", "text": " \ntorch.cuda.current_device() [source]\n \nReturns the index of a currently selected device. \n"}, {"name": "torch.cuda.current_stream()", "path": "cuda#torch.cuda.current_stream", "type": "torch.cuda", "text": " \ntorch.cuda.current_stream(device=None) [source]\n \nReturns the currently selected Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.default_stream()", "path": "cuda#torch.cuda.default_stream", "type": "torch.cuda", "text": " \ntorch.cuda.default_stream(device=None) [source]\n \nReturns the default Stream for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.device", "path": "cuda#torch.cuda.device", "type": "torch.cuda", "text": " \nclass torch.cuda.device(device) [source]\n \nContext-manager that changes the selected device.  Parameters \ndevice (torch.device or int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   \n"}, {"name": "torch.cuda.device_count()", "path": "cuda#torch.cuda.device_count", "type": "torch.cuda", "text": " \ntorch.cuda.device_count() [source]\n \nReturns the number of GPUs available. \n"}, {"name": "torch.cuda.device_of", "path": "cuda#torch.cuda.device_of", "type": "torch.cuda", "text": " \nclass torch.cuda.device_of(obj) [source]\n \nContext-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters \nobj (Tensor or Storage) \u2013 object allocated on the selected device.   \n"}, {"name": "torch.cuda.empty_cache()", "path": "cuda#torch.cuda.empty_cache", "type": "torch.cuda", "text": " \ntorch.cuda.empty_cache() [source]\n \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.Event", "path": "cuda#torch.cuda.Event", "type": "torch.cuda", "text": " \nclass torch.cuda.Event [source]\n \nWrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters \n \nenable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) \nblocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) \ninterprocess (bool) \u2013 if True, the event can be shared between processes (default: False)     \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n  \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n  \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n  \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n  \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n  \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n  \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n \n"}, {"name": "torch.cuda.Event.elapsed_time()", "path": "cuda#torch.cuda.Event.elapsed_time", "type": "torch.cuda", "text": " \nelapsed_time(end_event) [source]\n \nReturns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. \n"}, {"name": "torch.cuda.Event.from_ipc_handle()", "path": "cuda#torch.cuda.Event.from_ipc_handle", "type": "torch.cuda", "text": " \nclassmethod from_ipc_handle(device, handle) [source]\n \nReconstruct an event from an IPC handle on the given device. \n"}, {"name": "torch.cuda.Event.ipc_handle()", "path": "cuda#torch.cuda.Event.ipc_handle", "type": "torch.cuda", "text": " \nipc_handle() [source]\n \nReturns an IPC handle of this event. If not recorded yet, the event will use the current device. \n"}, {"name": "torch.cuda.Event.query()", "path": "cuda#torch.cuda.Event.query", "type": "torch.cuda", "text": " \nquery() [source]\n \nChecks if all work currently captured by event has completed.  Returns \nA boolean indicating if all work currently captured by event has completed.   \n"}, {"name": "torch.cuda.Event.record()", "path": "cuda#torch.cuda.Event.record", "type": "torch.cuda", "text": " \nrecord(stream=None) [source]\n \nRecords the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. \n"}, {"name": "torch.cuda.Event.synchronize()", "path": "cuda#torch.cuda.Event.synchronize", "type": "torch.cuda", "text": " \nsynchronize() [source]\n \nWaits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.  Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.  \n"}, {"name": "torch.cuda.Event.wait()", "path": "cuda#torch.cuda.Event.wait", "type": "torch.cuda", "text": " \nwait(stream=None) [source]\n \nMakes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. \n"}, {"name": "torch.cuda.get_arch_list()", "path": "cuda#torch.cuda.get_arch_list", "type": "torch.cuda", "text": " \ntorch.cuda.get_arch_list() [source]\n \nReturns list CUDA architectures this library was compiled for. \n"}, {"name": "torch.cuda.get_device_capability()", "path": "cuda#torch.cuda.get_device_capability", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_capability(device=None) [source]\n \nGets the cuda capability of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe major and minor cuda capability of the device  Return type \ntuple(int, int)   \n"}, {"name": "torch.cuda.get_device_name()", "path": "cuda#torch.cuda.get_device_name", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_name(device=None) [source]\n \nGets the name of a device.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns \nthe name of the device  Return type \nstr   \n"}, {"name": "torch.cuda.get_device_properties()", "path": "cuda#torch.cuda.get_device_properties", "type": "torch.cuda", "text": " \ntorch.cuda.get_device_properties(device) [source]\n \nGets the properties of a device.  Parameters \ndevice (torch.device or int or str) \u2013 device for which to return the properties of the device.  Returns \nthe properties of the device  Return type \n_CudaDeviceProperties   \n"}, {"name": "torch.cuda.get_gencode_flags()", "path": "cuda#torch.cuda.get_gencode_flags", "type": "torch.cuda", "text": " \ntorch.cuda.get_gencode_flags() [source]\n \nReturns NVCC gencode flags this library were compiled with. \n"}, {"name": "torch.cuda.get_rng_state()", "path": "cuda#torch.cuda.get_rng_state", "type": "torch.cuda", "text": " \ntorch.cuda.get_rng_state(device='cuda') [source]\n \nReturns the random number generator state of the specified GPU as a ByteTensor.  Parameters \ndevice (torch.device or int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    Warning This function eagerly initializes CUDA.  \n"}, {"name": "torch.cuda.get_rng_state_all()", "path": "cuda#torch.cuda.get_rng_state_all", "type": "torch.cuda", "text": " \ntorch.cuda.get_rng_state_all() [source]\n \nReturns a list of ByteTensor representing the random number states of all devices. \n"}, {"name": "torch.cuda.init()", "path": "cuda#torch.cuda.init", "type": "torch.cuda", "text": " \ntorch.cuda.init() [source]\n \nInitialize PyTorch\u2019s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. \n"}, {"name": "torch.cuda.initial_seed()", "path": "cuda#torch.cuda.initial_seed", "type": "torch.cuda", "text": " \ntorch.cuda.initial_seed() [source]\n \nReturns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.  \n"}, {"name": "torch.cuda.ipc_collect()", "path": "cuda#torch.cuda.ipc_collect", "type": "torch.cuda", "text": " \ntorch.cuda.ipc_collect() [source]\n \nForce collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  \n"}, {"name": "torch.cuda.is_available()", "path": "cuda#torch.cuda.is_available", "type": "torch.cuda", "text": " \ntorch.cuda.is_available() [source]\n \nReturns a bool indicating if CUDA is currently available. \n"}, {"name": "torch.cuda.is_initialized()", "path": "cuda#torch.cuda.is_initialized", "type": "torch.cuda", "text": " \ntorch.cuda.is_initialized() [source]\n \nReturns whether PyTorch\u2019s CUDA state has been initialized. \n"}, {"name": "torch.cuda.list_gpu_processes()", "path": "cuda#torch.cuda.list_gpu_processes", "type": "torch.cuda", "text": " \ntorch.cuda.list_gpu_processes(device=None) [source]\n \nReturns a human-readable printout of the running processes and their GPU memory use for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cuda.manual_seed()", "path": "cuda#torch.cuda.manual_seed", "type": "torch.cuda", "text": " \ntorch.cuda.manual_seed(seed) [source]\n \nSets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use manual_seed_all().  \n"}, {"name": "torch.cuda.manual_seed_all()", "path": "cuda#torch.cuda.manual_seed_all", "type": "torch.cuda", "text": " \ntorch.cuda.manual_seed_all(seed) [source]\n \nSets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters \nseed (int) \u2013 The desired seed.   \n"}, {"name": "torch.cuda.max_memory_allocated()", "path": "cuda#torch.cuda.max_memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_allocated(device=None) [source]\n \nReturns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.max_memory_cached()", "path": "cuda#torch.cuda.max_memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_cached(device=None) [source]\n \nDeprecated; see max_memory_reserved(). \n"}, {"name": "torch.cuda.max_memory_reserved()", "path": "cuda#torch.cuda.max_memory_reserved", "type": "torch.cuda", "text": " \ntorch.cuda.max_memory_reserved(device=None) [source]\n \nReturns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_allocated()", "path": "cuda#torch.cuda.memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.memory_allocated(device=None) [source]\n \nReturns the current GPU memory occupied by tensors in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_cached()", "path": "cuda#torch.cuda.memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.memory_cached(device=None) [source]\n \nDeprecated; see memory_reserved(). \n"}, {"name": "torch.cuda.memory_reserved()", "path": "cuda#torch.cuda.memory_reserved", "type": "torch.cuda", "text": " \ntorch.cuda.memory_reserved(device=None) [source]\n \nReturns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_snapshot()", "path": "cuda#torch.cuda.memory_snapshot", "type": "torch.cuda", "text": " \ntorch.cuda.memory_snapshot() [source]\n \nReturns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_stats()", "path": "cuda#torch.cuda.memory_stats", "type": "torch.cuda", "text": " \ntorch.cuda.memory_stats(device=None) [source]\n \nReturns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \n\"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \n\"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \n\"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \n\"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \n\"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \n\"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \n\"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \n\"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  \nall: combined statistics across all memory pools. \nlarge_pool: statistics for the large allocation pool (as of October 2019, for size >= 1MB allocations). \nsmall_pool: statistics for the small allocation pool (as of October 2019, for size < 1MB allocations).  Metric type:  \ncurrent: current value of this metric. \npeak: maximum value of this metric. \nallocated: historical total increase in this metric. \nfreed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \n\"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \n\"num_ooms\": number of out-of-memory errors thrown.   Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.memory_summary()", "path": "cuda#torch.cuda.memory_summary", "type": "torch.cuda", "text": " \ntorch.cuda.memory_summary(device=None, abbreviated=False) [source]\n \nReturns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters \n \ndevice (torch.device or int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). \nabbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).     Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.nvtx.mark()", "path": "cuda#torch.cuda.nvtx.mark", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.mark(msg) [source]\n \nDescribe an instantaneous event that occurred at some point.  Parameters \nmsg (string) \u2013 ASCII message to associate with the event.   \n"}, {"name": "torch.cuda.nvtx.range_pop()", "path": "cuda#torch.cuda.nvtx.range_pop", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.range_pop() [source]\n \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended. \n"}, {"name": "torch.cuda.nvtx.range_push()", "path": "cuda#torch.cuda.nvtx.range_push", "type": "torch.cuda", "text": " \ntorch.cuda.nvtx.range_push(msg) [source]\n \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.  Parameters \nmsg (string) \u2013 ASCII message to associate with range   \n"}, {"name": "torch.cuda.reset_max_memory_allocated()", "path": "cuda#torch.cuda.reset_max_memory_allocated", "type": "torch.cuda", "text": " \ntorch.cuda.reset_max_memory_allocated(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.reset_max_memory_cached()", "path": "cuda#torch.cuda.reset_max_memory_cached", "type": "torch.cuda", "text": " \ntorch.cuda.reset_max_memory_cached(device=None) [source]\n \nResets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters \ndevice (torch.device or int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  \n"}, {"name": "torch.cuda.seed()", "path": "cuda#torch.cuda.seed", "type": "torch.cuda", "text": " \ntorch.cuda.seed() [source]\n \nSets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use seed_all().  \n"}, {"name": "torch.cuda.seed_all()", "path": "cuda#torch.cuda.seed_all", "type": "torch.cuda", "text": " \ntorch.cuda.seed_all() [source]\n \nSets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. \n"}, {"name": "torch.cuda.set_device()", "path": "cuda#torch.cuda.set_device", "type": "torch.cuda", "text": " \ntorch.cuda.set_device(device) [source]\n \nSets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters \ndevice (torch.device or int) \u2013 selected device. This function is a no-op if this argument is negative.   \n"}, {"name": "torch.cuda.set_per_process_memory_fraction()", "path": "cuda#torch.cuda.set_per_process_memory_fraction", "type": "torch.cuda", "text": " \ntorch.cuda.set_per_process_memory_fraction(fraction, device=None) [source]\n \nSet memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.  Parameters \n \nfraction (float) \u2013 Range: 0~1. Allowed memory equals total_memory * fraction. \ndevice (torch.device or int, optional) \u2013 selected device. If it is None the default CUDA device is used.     Note In general, the total available free memory is less than the total capacity.  \n"}, {"name": "torch.cuda.set_rng_state()", "path": "cuda#torch.cuda.set_rng_state", "type": "torch.cuda", "text": " \ntorch.cuda.set_rng_state(new_state, device='cuda') [source]\n \nSets the random number generator state of the specified GPU.  Parameters \n \nnew_state (torch.ByteTensor) \u2013 The desired state \ndevice (torch.device or int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    \n"}, {"name": "torch.cuda.set_rng_state_all()", "path": "cuda#torch.cuda.set_rng_state_all", "type": "torch.cuda", "text": " \ntorch.cuda.set_rng_state_all(new_states) [source]\n \nSets the random number generator state of all devices.  Parameters \nnew_states (Iterable of torch.ByteTensor) \u2013 The desired state for each device   \n"}, {"name": "torch.cuda.Stream", "path": "cuda#torch.cuda.Stream", "type": "torch.cuda", "text": " \nclass torch.cuda.Stream [source]\n \nWrapper around a CUDA stream. A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See CUDA semantics for details.  Parameters \n \ndevice (torch.device or int, optional) \u2013 a device on which to allocate the stream. If device is None (default) or a negative integer, this will use the current device. \npriority (int, optional) \u2013 priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.     Note Although CUDA versions >= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.   \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n  \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n  \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n  \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n  \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n \n"}, {"name": "torch.cuda.stream()", "path": "cuda#torch.cuda.stream", "type": "torch.cuda", "text": " \ntorch.cuda.stream(stream) [source]\n \nContext-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters \nstream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.  \n"}, {"name": "torch.cuda.Stream.query()", "path": "cuda#torch.cuda.Stream.query", "type": "torch.cuda", "text": " \nquery() [source]\n \nChecks if all the work submitted has been completed.  Returns \nA boolean indicating if all kernels in this stream are completed.   \n"}, {"name": "torch.cuda.Stream.record_event()", "path": "cuda#torch.cuda.Stream.record_event", "type": "torch.cuda", "text": " \nrecord_event(event=None) [source]\n \nRecords an event.  Parameters \nevent (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns \nRecorded event.   \n"}, {"name": "torch.cuda.Stream.synchronize()", "path": "cuda#torch.cuda.Stream.synchronize", "type": "torch.cuda", "text": " \nsynchronize() [source]\n \nWait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  \n"}, {"name": "torch.cuda.Stream.wait_event()", "path": "cuda#torch.cuda.Stream.wait_event", "type": "torch.cuda", "text": " \nwait_event(event) [source]\n \nMakes all future work submitted to the stream wait for an event.  Parameters \nevent (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  \n"}, {"name": "torch.cuda.Stream.wait_stream()", "path": "cuda#torch.cuda.Stream.wait_stream", "type": "torch.cuda", "text": " \nwait_stream(stream) [source]\n \nSynchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters \nstream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  \n"}, {"name": "torch.cuda.synchronize()", "path": "cuda#torch.cuda.synchronize", "type": "torch.cuda", "text": " \ntorch.cuda.synchronize(device=None) [source]\n \nWaits for all kernels in all streams on a CUDA device to complete.  Parameters \ndevice (torch.device or int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   \n"}, {"name": "torch.cummax()", "path": "generated/torch.cummax#torch.cummax", "type": "torch", "text": " \ntorch.cummax(input, dim, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the cumulative maximum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=max(x1,x2,x3,\u2026,xi)y_i = max(x_1, x_2, x_3, \\dots, x_i)  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\n \n"}, {"name": "torch.cummin()", "path": "generated/torch.cummin#torch.cummin", "type": "torch", "text": " \ntorch.cummin(input, dim, *, out=None) -> (Tensor, LongTensor)  \nReturns a namedtuple (values, indices) where values is the cumulative minimum of elements of input in the dimension dim. And indices is the index location of each maximum value found in the dimension dim.  yi=min(x1,x2,x3,\u2026,xi)y_i = min(x_1, x_2, x_3, \\dots, x_i)  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \nout (tuple, optional) \u2013 the result tuple of two output tensors (values, indices)   Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\n \n"}, {"name": "torch.cumprod()", "path": "generated/torch.cumprod#torch.cumprod", "type": "torch", "text": " \ntorch.cumprod(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative product of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1\u00d7x2\u00d7x3\u00d7\u22ef\u00d7xiy_i = x_1 \\times x_2\\times x_3\\times \\dots \\times x_i  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n \n"}, {"name": "torch.cumsum()", "path": "generated/torch.cumsum#torch.cumsum", "type": "torch", "text": " \ntorch.cumsum(input, dim, *, dtype=None, out=None) \u2192 Tensor  \nReturns the cumulative sum of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1+x2+x3+\u22ef+xiy_i = x_1 + x_2 + x_3 + \\dots + x_i  \n Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndim (int) \u2013 the dimension to do the operation over   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. \nout (Tensor, optional) \u2013 the output tensor.    Example: >>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n \n"}, {"name": "torch.deg2rad()", "path": "generated/torch.deg2rad#torch.deg2rad", "type": "torch", "text": " \ntorch.deg2rad(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with each of the elements of input converted from angles in degrees to radians.  Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])\n \n"}, {"name": "torch.dequantize()", "path": "generated/torch.dequantize#torch.dequantize", "type": "torch", "text": " \ntorch.dequantize(tensor) \u2192 Tensor  \nReturns an fp32 Tensor by dequantizing a quantized Tensor  Parameters \ntensor (Tensor) \u2013 A quantized Tensor    \ntorch.dequantize(tensors) \u2192 sequence of Tensors \n Given a list of quantized Tensors, dequantize them and return a list of fp32 Tensors  Parameters \ntensors (sequence of Tensors) \u2013 A list of quantized Tensors   \n"}, {"name": "torch.det()", "path": "generated/torch.det#torch.det", "type": "torch", "text": " \ntorch.det(input) \u2192 Tensor  \nCalculates determinant of a square matrix or batches of square matrices.  Note torch.det() is deprecated. Please use torch.linalg.det() instead.   Note Backward through detdet  internally uses SVD results when input is not invertible. In this case, double backward through detdet  will be unstable when input doesn\u2019t have distinct singular values. See torch.svd~torch.svd  for details.   Parameters \ninput (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.   Example: >>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(3.7641)\n\n>>> A = torch.randn(3, 2, 2)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n \n"}, {"name": "torch.diag()", "path": "generated/torch.diag#torch.diag", "type": "torch", "text": " \ntorch.diag(input, diagonal=0, *, out=None) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.  The argument diagonal controls which diagonal to consider:  If diagonal = 0, it is the main diagonal. If diagonal > 0, it is above the main diagonal. If diagonal < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndiagonal (int, optional) \u2013 the diagonal to consider   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    See also torch.diagonal() always returns the diagonal of its input. torch.diagflat() always constructs a tensor with diagonal elements specified by the input.  Examples: Get the square matrix where the input vector is the diagonal: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n Get the k-th diagonal of a given matrix: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n \n"}, {"name": "torch.diagflat()", "path": "generated/torch.diagflat#torch.diagflat", "type": "torch", "text": " \ntorch.diagflat(input, offset=0) \u2192 Tensor  \n If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened input.  The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \noffset (int, optional) \u2013 the diagonal to consider. Default: 0 (main diagonal).    Examples: >>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n \n"}, {"name": "torch.diagonal()", "path": "generated/torch.diagonal#torch.diagonal", "type": "torch", "text": " \ntorch.diagonal(input, offset=0, dim1=0, dim2=1) \u2192 Tensor  \nReturns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 2-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: 0. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: 1.     Note To take a batch diagonal, pass in dim1=-2, dim2=-1.  Examples: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n \n"}, {"name": "torch.diag_embed()", "path": "generated/torch.diag_embed#torch.diag_embed", "type": "torch", "text": " \ntorch.diag_embed(input, offset=0, dim1=-2, dim2=-1) \u2192 Tensor  \nCreates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset > 0, it is above the main diagonal. If offset < 0, it is below the main diagonal.  The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 00 , the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset. Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.  Parameters \n \ninput (Tensor) \u2013 the input tensor. Must be at least 1-dimensional. \noffset (int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). \ndim1 (int, optional) \u2013 first dimension with respect to which to take diagonal. Default: -2. \ndim2 (int, optional) \u2013 second dimension with respect to which to take diagonal. Default: -1.    Example: >>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n \n"}, {"name": "torch.diff()", "path": "generated/torch.diff#torch.diff", "type": "torch", "text": " \ntorch.diff(input, n=1, dim=-1, prepend=None, append=None) \u2192 Tensor  \nComputes the n-th forward difference along the given dimension. The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order differences are calculated by using torch.diff() recursively.  Note Only n = 1 is currently supported   Parameters \n \ninput (Tensor) \u2013 the tensor to compute the differences on \nn (int, optional) \u2013 the number of times to recursively compute the difference \ndim (int, optional) \u2013 the dimension to compute the difference along. Default is the last dimension. \nappend (prepend,) \u2013 values to prepend or append to input along dim before computing the difference. Their dimensions must be equivalent to that of input, and their shapes must match input\u2019s shape except on dim.   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])\n \n"}, {"name": "torch.digamma()", "path": "generated/torch.digamma#torch.digamma", "type": "torch", "text": " \ntorch.digamma(input, *, out=None) \u2192 Tensor  \nComputes the logarithmic derivative of the gamma function on input.  \u03c8(x)=ddxln\u2061(\u0393(x))=\u0393\u2032(x)\u0393(x)\\psi(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)}  \n Parameters \ninput (Tensor) \u2013 the tensor to compute the digamma function on  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Note This function is similar to SciPy\u2019s scipy.special.digamma.   Note From PyTorch 1.8 onwards, the digamma function returns -Inf for 0. Previously it returned NaN for 0.  Example: >>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])\n \n"}, {"name": "torch.dist()", "path": "generated/torch.dist#torch.dist", "type": "torch", "text": " \ntorch.dist(input, other, p=2) \u2192 Tensor  \nReturns the p-norm of (input - other) The shapes of input and other must be broadcastable.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the Right-hand-side input tensor \np (float, optional) \u2013 the norm to be computed    Example: >>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)\n \n"}, {"name": "torch.distributed", "path": "distributed", "type": "torch.distributed", "text": "Distributed communication package - torch.distributed  Note Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.  Backends torch.distributed supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.   \nBackend gloo mpi nccl  \nDevice CPU GPU CPU GPU CPU GPU   \nsend \u2713 \u2718 \u2713 ? \u2718 \u2718  \nrecv \u2713 \u2718 \u2713 ? \u2718 \u2718  \nbroadcast \u2713 \u2713 \u2713 ? \u2718 \u2713  \nall_reduce \u2713 \u2713 \u2713 ? \u2718 \u2713  \nreduce \u2713 \u2718 \u2713 ? \u2718 \u2713  \nall_gather \u2713 \u2718 \u2713 ? \u2718 \u2713  \ngather \u2713 \u2718 \u2713 ? \u2718 \u2718  \nscatter \u2713 \u2718 \u2713 ? \u2718 \u2718  \nreduce_scatter \u2718 \u2718 \u2718 \u2718 \u2718 \u2713  \nall_to_all \u2718 \u2718 \u2713 ? \u2718 \u2718  \nbarrier \u2713 \u2718 \u2713 ? \u2718 \u2713   Backends that come with PyTorch PyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g.building PyTorch on a host that has MPI installed.)  Note As of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the init_method argument of init_process_group() points to a file it must adhere to the following schema:  Local file system, init_method=\"file:///d:/tmp/some_file\"\n Shared file system, init_method=\"file://////{machine_name}/{share_folder_name}/some_file\"\n  Same as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.  Which backend to use? In the past, we were often asked: \u201cwhich backend should I use?\u201d.  \nRule of thumb  Use the NCCL backend for distributed GPU training Use the Gloo backend for distributed CPU training.   \nGPU hosts with InfiniBand interconnect  Use NCCL, since it\u2019s the only backend that currently supports InfiniBand and GPUDirect.   \nGPU hosts with Ethernet interconnect  Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)   \nCPU hosts with InfiniBand interconnect  If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.   \nCPU hosts with Ethernet interconnect  Use Gloo, unless you have specific reasons to use MPI.    Common environment variables Choosing the network interface to use By default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):  \nNCCL_SOCKET_IFNAME, for example export NCCL_SOCKET_IFNAME=eth0\n \nGLOO_SOCKET_IFNAME, for example export GLOO_SOCKET_IFNAME=eth0\n  If you\u2019re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: export GLOO_SOCKET_IFNAME=eth0,eth1,eth2,eth3. The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable. Other NCCL environment variables NCCL has also provided a number of environment variables for fine-tuning purposes. Commonly used ones include the following for debugging purposes:  export NCCL_DEBUG=INFO export NCCL_DEBUG_SUBSYS=ALL  For the full list of NCCL environment variables, please refer to NVIDIA NCCL\u2019s official documentation Basics The torch.distributed package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class torch.nn.parallel.DistributedDataParallel() builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by Multiprocessing package - torch.multiprocessing and torch.nn.DataParallel() in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process. In the single-machine synchronous case, torch.distributed or the torch.nn.parallel.DistributedDataParallel() wrapper may still have advantages over other approaches to data-parallelism, including torch.nn.DataParallel():  Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes. Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and \u201cGIL-thrashing\u201d that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.  Initialization The package needs to be initialized using the torch.distributed.init_process_group() function before calling any other methods. This blocks until all processes have joined.  \ntorch.distributed.is_available() [source]\n \nReturns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS. \n  \ntorch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='') [source]\n \nInitializes the default distributed process group, and this will also initialize the distributed package.  There are 2 main ways to initialize a process group:\n\n Specify store, rank, and world_size explicitly. Specify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.    If neither is specified, init_method is assumed to be \u201cenv://\u201d.  Parameters \n \nbackend (str or Backend) \u2013 The backend to use. Depending on build-time configurations, valid values include mpi, gloo, and nccl. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. \ninit_method (str, optional) \u2013 URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified. Mutually exclusive with store. \nworld_size (int, optional) \u2013 Number of processes participating in the job. Required if store is specified. \nrank (int, optional) \u2013 Rank of the current process (it should be a number between 0 and world_size-1). Required if store is specified. \nstore (Store, optional) \u2013 Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. \ngroup_name (str, optional, deprecated) \u2013 Group name.    To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI. \n  \nclass torch.distributed.Backend [source]\n \nAn enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.  \n  \ntorch.distributed.get_backend(group=None) [source]\n \nReturns the backend of the given process group.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns \nThe backend of the given process group as a lower case string.   \n  \ntorch.distributed.get_rank(group=None) [source]\n \nReturns the rank of current process group Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe rank of the process group -1, if not part of the group   \n  \ntorch.distributed.get_world_size(group=None) [source]\n \nReturns the number of processes in the current process group  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe world size of the process group -1, if not part of the group   \n  \ntorch.distributed.is_initialized() [source]\n \nChecking if the default process group has been initialized \n  \ntorch.distributed.is_mpi_available() [source]\n \nChecks if the MPI backend is available. \n  \ntorch.distributed.is_nccl_available() [source]\n \nChecks if the NCCL backend is available. \n Currently three initialization methods are supported: TCP initialization There are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired world_size. The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks. Note that multicast address is not supported anymore in the latest distributed package. group_name is deprecated as well. import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n Shared file-system initialization Another initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired world_size. The URL should start with file:// and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn\u2019t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next init_process_group() call on the same file path/name. Note that automatic rank assignment is not supported anymore in the latest distributed package and group_name is deprecated as well.  Warning This method assumes that the file system supports locking using fcntl - most local systems and NFS support it.   Warning This method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call init_process_group() multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call init_process_group() again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time init_process_group() is called.  import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n Environment variable initialization This method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:  \nMASTER_PORT - required; has to be a free port on machine with rank 0 \nMASTER_ADDR - required (except for rank 0); address of rank 0 node \nWORLD_SIZE - required; can be set either here, or in a call to init function \nRANK - required; can be set either here, or in a call to init function  The machine with rank 0 will be used to set up all connections. This is the default method, meaning that init_method does not have to be specified (or can be env://). Distributed Key-Value Store The distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed pacakge in torch.distributed.init_process_group() (by explicitly creating the store as an alternative to specifying init_method.) There are 3 choices for Key-Value Stores: TCPStore, FileStore, and HashStore.  \nclass torch.distributed.Store  \nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore). \n  \nclass torch.distributed.TCPStore  \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc.  Parameters \n \nhost_name (str) \u2013 The hostname or IP Address the server store should run on. \nport (int) \u2013 The port on which the server store should listen for incoming requests. \nworld_size (int) \u2013 The total number of store users (number of clients + 1 for the server). \nis_master (bool) \u2013 True when initializing the server store, False for client stores. \ntimeout (timedelta) \u2013 Timeout used by the store during initialization and for methods such as get() and wait().     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n   \n  \nclass torch.distributed.HashStore  \nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.  Example::\n\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n   \n  \nclass torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int) \u2013 The total number of processes using the store     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n   \n  \nclass torch.distributed.PrefixStore  \nA wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.  Parameters \n \nprefix (str) \u2013 The prefix string that is prepended to each key before being inserted into the store. \nstore (torch.distributed.store) \u2013 A store object that forms the underlying key-value store.    \n  \ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) \u2192 None  \nInserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.  Parameters \n \nkey (str) \u2013 The key to be added to the store. \nvalue (str) \u2013 The value associated with key to be added to the store.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bytes  \nRetrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.  Parameters \nkey (str) \u2013 The function will return the value associated with this key.  Returns \nValue associated with key if key is in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) \u2192 int  \nThe first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.  Parameters \n \nkey (str) \u2013 The key in the store whose counter will be incremented. \namount (int) \u2013 The quantity by which the counter will be incremented.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n   \n  \ntorch.distributed.Store.wait(*args, **kwargs)  \nOverloaded function.  wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -> None  Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.  Parameters \nkeys (list) \u2013 List of keys on which to wait until they are set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n    wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -> None  Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.  Parameters \n \nkeys (list) \u2013 List of keys on which to wait until they are set in the store. \ntimeout (timedelta) \u2013 Time to wait for the keys to be added before throwing an exception.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n   \n  \ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) \u2192 int  \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.  Warning When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.   Returns \nThe number of keys present in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n   \n  \ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bool  \nDeletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.  Warning The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.   Parameters \nkey (str) \u2013 The key to be deleted from the store  Returns \nTrue if key was deleted, otherwise False.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n   \n  \ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) \u2192 None  \nSets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().  Parameters \ntimeout (timedelta) \u2013 timeout to be set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n   \n Groups By default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. new_group() function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a group argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).  \ntorch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None) [source]\n \nCreates a new distributed group. This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.  Warning Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.   Parameters \n \nranks (list[int]) \u2013 List of ranks of group members. If None, will be set to all ranks. Default is None. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the gloo backend. \nbackend (str or Backend, optional) \u2013 The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO).   Returns \nA handle of distributed group that can be given to collective calls.   \n Point-to-point communication  \ntorch.distributed.send(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv    \n  \ntorch.distributed.recv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nSender rank -1, if not part of the group   \n isend() and irecv() return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:  \nis_completed() - returns True if the operation has finished \nwait() - will block the process until the operation is finished. is_completed() is guaranteed to return True once it returns.   \ntorch.distributed.isend(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv   Returns \nA distributed request object. None, if not part of the group   \n  \ntorch.distributed.irecv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nA distributed request object. None, if not part of the group   \n Synchronous and asynchronous collective operations Every collective operation function supports the following two kinds of operations, depending on the setting of the async_op flag passed into the collective: Synchronous operation - the default mode, when async_op is set to False. When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see CUDA Semantics. See the below script to see examples of differences in these semantics for CPU and CUDA operations. Asynchronous operation - when async_op is set to True. The collective operation function returns a distributed request object. In general, you don\u2019t need to create it manually and it is guaranteed to support two methods:  \nis_completed() - in the case of CPU collectives, returns True if completed. In the case of CUDA operations, returns True if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization. \nwait() - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block until the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.  Example The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams: # Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n Collective functions  \ntorch.distributed.broadcast(tensor, src, group=None, async_op=False) [source]\n \nBroadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters \n \ntensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.broadcast_object_list(object_list, src=0, group=None) [source]\n \nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.  Parameters \n \nobject_list (List[Any]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. \nsrc (int) \u2013 Source rank from which to broadcast object_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.    Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes. Complex tensors are supported.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n >>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n \n  \ntorch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines. Only the process with rank dst is going to receive the final result.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Complex tensors are supported.  Parameters \n \ntensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. \ntensor (Tensor) \u2013 Tensor to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n \n  \ntorch.distributed.all_gather_object(object_list, obj, group=None) [source]\n \nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobject_list (list[Any]) \u2013 Output list. It should be correctly sized as the size of the group for this collective and will contain the output. \nobject (Any) \u2013 Pickable Python object to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.    Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False) [source]\n \nGathers a list of tensors in a single process.  Parameters \n \ntensor (Tensor) \u2013 Input tensor. \ngather_list (list[Tensor], optional) \u2013 List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank) \ndst (int, optional) \u2013 Destination rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None) [source]\n \nGathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobj (Any) \u2013 Input object. Must be picklable. \nobject_gather_list (list[Any]) \u2013 Output list. On the dst rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be None on non-dst ranks. (default is None) \ndst (int, optional) \u2013 Destination rank. (default is 0) \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. On the dst rank, object_gather_list will contain the output of the collective.    Note Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.   Note Note that this API is not supported when using the NCCL backend.   Warning gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n   \n  \ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False) [source]\n \nScatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the tensor argument.  Parameters \n \ntensor (Tensor) \u2013 Output tensor. \nscatter_list (list[Tensor]) \u2013 List of tensors to scatter (default is None, must be specified on the source rank) \nsrc (int) \u2013 Source rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None) [source]\n \nScatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.  Parameters \n \nscatter_object_output_list (List[Any]) \u2013 Non-empty list whose first element will store the object scattered to this rank. \nscatter_object_input_list (List[Any]) \u2013 List of input objects to scatter. Each object must be picklable. Only objects on the src rank will be scattered, and the argument can be None for non-src ranks. \nsrc (int) \u2013 Source rank from which to scatter scatter_object_input_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.    Note Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.   Warning scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n   \n  \ntorch.distributed.reduce_scatter(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces, then scatters a list of tensors to all processes in a group.  Parameters \n \noutput (Tensor) \u2013 Output tensor. \ninput_list (list[Tensor]) \u2013 List of tensors to reduce and scatter. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n  \ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False) [source]\n \nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.  Parameters \n \noutput_tensor_list (list[Tensor]) \u2013 List of tensors to be gathered one per rank. \ninput_tensor_list (list[Tensor]) \u2013 List of tensors to scatter one per rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n >>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n \n  \ntorch.distributed.barrier(group=None, async_op=False, device_ids=None) [source]\n \nSynchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters \n \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndevice_ids ([int], optional) \u2013 List of device/GPU ids. Valid only for NCCL backend.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \nclass torch.distributed.ReduceOp  \nAn enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR. Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend. Additionally, MAX, MIN and PRODUCT are not supported for complex tensors. The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc. Members: SUM PRODUCT MIN MAX BAND BOR BXOR \n  \nclass torch.distributed.reduce_op  \nDeprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX. ReduceOp is recommended to use instead. \n Autograd-enabled communication primitives If you want to use collective communication functions supporting autograd you can find an implementation of those in the torch.distributed.nn.* module. Functions here are synchronous and will be inserted in the autograd graph, so you need to ensure that all the processes that participated in the collective operation will do the backward pass for the backward communication to effectively happen and don\u2019t cause a deadlock. Please notice that currently the only backend where all the functions are guaranteed to work is gloo. .. autofunction:: torch.distributed.nn.broadcast .. autofunction:: torch.distributed.nn.gather .. autofunction:: torch.distributed.nn.scatter .. autofunction:: torch.distributed.nn.reduce .. autofunction:: torch.distributed.nn.all_gather .. autofunction:: torch.distributed.nn.all_to_all .. autofunction:: torch.distributed.nn.all_reduce Multi-GPU collective functions If you have more than one GPU on each node, when using the NCCL and Gloo backend, broadcast_multigpu() all_reduce_multigpu() reduce_multigpu() all_gather_multigpu() and reduce_scatter_multigpu() support distributed collective operations among multiple GPUs within each node. These functions can potentially improve the overall distributed training performance and be easily used by passing a list of tensors. Each Tensor in the passed tensor list needs to be on a separate GPU device of the host where the function is called. Note that the length of the tensor list needs to be identical among all the distributed processes. Also note that currently the multi-GPU collective functions are only supported by the NCCL backend. For example, if the system we use for distributed training has 2 nodes, each of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would like to all-reduce. The following code can serve as a reference: Code running on Node 0 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n Code running on Node 1 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n After the call, all 16 tensors on the two nodes will have the all-reduced value of 16  \ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0) [source]\n \nBroadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \nsrc_tensor (int, optional) \u2013 Source tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.all_reduce_multigpu(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Complex tensors are supported. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters \n \nlist (tensor) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.reduce_multigpu(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0) [source]\n \nReduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result. Only nccl backend is currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndst_tensor (int, optional) \u2013 Destination tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, otherwise   \n  \ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors Complex tensors are supported.  Parameters \n \noutput_tensor_lists (List[List[Tensor]]) \u2013 \nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ninput_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n  \ntorch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported. Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.  Parameters \n \noutput_tensor_list (List[Tensor]) \u2013 \nOutput tensors (on different GPUs) to receive the result of the operation. Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.  \ninput_tensor_lists (List[List[Tensor]]) \u2013 \nInput lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i]. Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j] Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n Third-party backends Besides the GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to Tutorials - Custom C++ and CUDA Extensions and test/cpp_extensions/cpp_c10d_extension.cpp. The capability of third-party backends are decided by their own implementations. The new backend derives from c10d.ProcessGroup and registers the backend name and the instantiating interface through torch.distributed.Backend.register_backend() when imported. When manually importing this backend and invoking torch.distributed.init_process_group() with the corresponding backend name, the torch.distributed package runs on the new backend.  Warning The support of third-party backend is experimental and subject to change.  Launch utility The torch.distributed package also provides a launch utility in torch.distributed.launch. This helper utility can be used to launch multiple processes per node for distributed training. torch.distributed.launch is a module that spawns up multiple distributed training processes on each of the training nodes. The utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be benefitial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth. In both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (--nproc_per_node). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (nproc_per_node), and each process will be operating on a single GPU from GPU 0 to GPU (nproc_per_node - 1). How to use this module:  Single-Node multi-process distributed training  >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n  Multi-Node multi-process distributed training: (e.g. two nodes)  Node 1: (IP: 192.168.1.1, and has a free port: 1234) >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n Node 2: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n  To look up what optional arguments this module offers:  >>> python -m torch.distributed.launch --help\n Important Notices: 1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training. 2. In your training program, you must parse the command-line argument: --local_rank=LOCAL_PROCESS_RANK, which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by: Parsing the local_rank argument >>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()\n Set your device to local rank using either >>> torch.cuda.set_device(args.local_rank)  # before your code runs\n or >>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n 3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. You need to make sure that the init_method uses env://, which is the only supported init_method by this module. torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')\n 4. In your training program, you can either use regular distributed functions or use torch.nn.parallel.DistributedDataParallel() module. If your training program uses GPUs for training and you would like to use torch.nn.parallel.DistributedDataParallel() module, here is how to configure it. model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)\n Please ensure that device_ids argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the device_ids needs to be [args.local_rank], and output_device needs to be args.local_rank in order to use this utility 5. Another way to pass local_rank to the subprocesses via environment variable LOCAL_RANK. This behavior is enabled when you launch the script with --use_env=True. You must adjust the subprocess example above to replace args.local_rank with os.environ['LOCAL_RANK']; the launcher will not pass --local_rank when you specify this flag.  Warning local_rank is NOT globally unique: it is only unique per process on a machine. Thus, don\u2019t use it to decide if you should, e.g., write to a networked filesystem. See https://github.com/pytorch/pytorch/issues/12042 for an example of how things can go wrong if you don\u2019t do this correctly.  Spawn utility The Multiprocessing package - torch.multiprocessing package also provides a spawn function in torch.multiprocessing.spawn(). This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well. For references on how to use it, please refer to PyTorch example - ImageNet implementation Note that this function requires Python 3.4 or higher.\n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket) [source]\n \nThis DDP communication hook just calls allreduce using GradBucket tensors. Once gradient tensors are aggregated across all workers, its then callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won\u2019t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.  Example::\n\n>>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket) [source]\n \nThis DDP communication hook implements a simple gradient compression approach that converts GradBucket tensors whose type is assumed to be torch.float32 to half-precision floating point format (torch.float16). It allreduces those float16 gradient tensors. Once compressed gradient tensors are allreduced, its then callback called decompress converts the aggregated result back to float32 and takes the mean.  Example::\n\n>>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the paper. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is faster than powerSGD_hook(), but usually results in a much lower accuracy, unless matrix_approximation_rank is 1.  Warning Increasing matrix_approximation_rank here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider powerSGD_hook() first, and only consider this variant when a satisfactory accuracy can be achieved when matrix_approximation_rank is 1.  Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings; Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; Computes P, which is equal to MQ; Allreduces P; Orthogonalizes P; Computes Q, which is approximately equal to M^TP; Allreduces Q; Computes M, which is approximately equal to PQ^T. Truncates the input tensor to the original length.  Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank and start_powerSGD_iter. \nbucket (dist._GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)\n>>> ddp_model.register_comm_hook(state, batched_powerSGD_hook)\n   \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState", "type": "DDP Communication Hooks", "text": " \nclass torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=10, use_error_feedback=True, warm_start=True, random_seed=0) [source]\n \nStores both the algorithm\u2019s hyperparameters and the internal state for all the gradients during the training. Particularly, matrix_approximation_rank and start_powerSGD_iter are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters use_error_feedback and warm_start on.  \nmatrix_approximation_rank controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression. 1.1. If matrix_approximation_rank is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy. 1.2. The increase of matrix_approximation_rank can substantially increase the computation costs of the compression, and the accuracy may not be futher improved beyond a certain matrix_approximation_rank threshold.   To tune matrix_approximation_rank, we suggest to start from 1 and increase by factors of 2 (like an expoential grid search, 1, 2, 4, \u2026), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.  \nstart_powerSGD_iter defers PowerSGD compression util step start_powerSGD_iter, and vanilla allreduce runs prior to step start_powerSGD_iter. This hybrid scheme of vanilla allreduce + PowerSGD can effectively improve the accuracy, even a relatively small matrix_approximation_rank is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.  To tune start_powerSGD_iter, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached.  Warning If error feedback or warm-up is enabled, the minimum value of start_powerSGD_iter allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.  \n"}, {"name": "torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook()", "path": "ddp_comm_hooks#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook", "type": "DDP Communication Hooks", "text": " \ntorch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket) [source]\n \nThis DDP communication hook implements PowerSGD gradient compression algorithm described in the paper. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:  Views the input flattened 1D gradient tensor as two groups of per-parameter tensors: high-rank tensors and vector-like rank-1 tensors (for biases). \nHandles rank-1 tensors by allreducing them without compression: 2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all the rank-1 tensors as a batch, without compression; 2.2. Copies the individual rank-1 tensors from the contiguous memory back to the input tensor.  \nHandles high-rank tensors by PowerSGD compression: 3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized; 3.2. Computes each P in Ps, which is equal to MQ; 3.3. Allreduces Ps as a batch; 3.4. Orthogonalizes each P in Ps; 3.5. Computes each Q in Qs, which is approximately equal to M^TP; 3.6. Allreduces Qs as a batch; 3.7. Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.   Note that this communication hook enforces vanilla allreduce for the first state.start_powerSGD_iter iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.  Parameters \n \nstate (PowerSGDState) \u2013 State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune matrix_approximation_rank` and start_powerSGD_iter. \nbucket (dist._GradBucket) \u2013 Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.   Returns \nFuture handler of the communication, which updates the gradients in place.    Example::\n\n>>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n>>> ddp_model.register_comm_hook(state, powerSGD_hook)\n   \n"}, {"name": "torch.distributed.all_gather()", "path": "distributed#torch.distributed.all_gather", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather(tensor_list, tensor, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Complex tensors are supported.  Parameters \n \ntensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. \ntensor (Tensor) \u2013 Tensor to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n \n"}, {"name": "torch.distributed.all_gather_multigpu()", "path": "distributed#torch.distributed.all_gather_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather_multigpu(output_tensor_lists, input_tensor_list, group=None, async_op=False) [source]\n \nGathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors Complex tensors are supported.  Parameters \n \noutput_tensor_lists (List[List[Tensor]]) \u2013 \nOutput lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ninput_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_gather_object()", "path": "distributed#torch.distributed.all_gather_object", "type": "torch.distributed", "text": " \ntorch.distributed.all_gather_object(object_list, obj, group=None) [source]\n \nGathers picklable objects from the whole group into a list. Similar to all_gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobject_list (list[Any]) \u2013 Output list. It should be correctly sized as the size of the group for this collective and will contain the output. \nobject (Any) \u2013 Pickable Python object to be broadcast from current process. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input object_list. If the calling rank is not part of the group, the passed in object_list will be unmodified.    Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Warning all_gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.all_reduce()", "path": "distributed#torch.distributed.all_reduce", "type": "torch.distributed", "text": " \ntorch.distributed.all_reduce(tensor, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes. Complex tensors are supported.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   Examples >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n >>> # All tensors below are of torch.cfloat type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4.+4.j, 6.+6.j]) # Rank 0\ntensor([4.+4.j, 6.+6.j]) # Rank 1\n \n"}, {"name": "torch.distributed.all_reduce_multigpu()", "path": "distributed#torch.distributed.all_reduce_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.all_reduce_multigpu(tensor_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Complex tensors are supported. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters \n \nlist (tensor) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.all_to_all()", "path": "distributed#torch.distributed.all_to_all", "type": "torch.distributed", "text": " \ntorch.distributed.all_to_all(output_tensor_list, input_tensor_list, group=None, async_op=False) [source]\n \nEach process scatters list of input tensors to all processes in a group and return gathered list of tensors in output list.  Parameters \n \noutput_tensor_list (list[Tensor]) \u2013 List of tensors to be gathered one per rank. \ninput_tensor_list (list[Tensor]) \u2013 List of tensors to scatter one per rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.    Warning all_to_all is experimental and subject to change.  Examples >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n >>> # Essentially, it is similar to following operation:\n>>> scatter_list = input\n>>> gather_list  = output\n>>> for i in range(world_size):\n>>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)\n >>> input\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n>>> input_splits\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n>>> output_splits\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n>>> input = list(input.split(input_splits))\n>>> input\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n>>> output = ...\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n \n"}, {"name": "torch.distributed.autograd.backward()", "path": "rpc#torch.distributed.autograd.backward", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.autograd.backward(context_id: int, roots: List[Tensor], retain_graph = False) \u2192 None  \nKicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass. We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done. We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context to be used is looked up given the context_id that is passed in when torch.distributed.autograd.backward() is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API.  Parameters \n \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients. \nroots (list) \u2013 Tensors which represent the roots of the autograd computation. All the tensors should be scalars. \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     pred = model.forward()\n>>>     loss = loss_func(pred, loss)\n>>>     dist_autograd.backward(context_id, loss)\n   \n"}, {"name": "torch.distributed.autograd.context", "path": "rpc#torch.distributed.autograd.context", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.autograd.context [source]\n \nContext object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass.  Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>   t1 = torch.rand((3, 3), requires_grad=True)\n>>>   t2 = torch.rand((3, 3), requires_grad=True)\n>>>   loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n>>>   dist_autograd.backward(context_id, [loss])\n   \n"}, {"name": "torch.distributed.autograd.get_gradients()", "path": "rpc#torch.distributed.autograd.get_gradients", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.autograd.get_gradients(context_id: int) \u2192 Dict[Tensor, Tensor]  \nRetrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given context_id as part of the distributed autograd backward pass.  Parameters \ncontext_id (int) \u2013 The autograd context id for which we should retrieve the gradients.  Returns \nA map where the key is the Tensor and the value is the associated gradient for that Tensor.    Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     t1 = torch.rand((3, 3), requires_grad=True)\n>>>     t2 = torch.rand((3, 3), requires_grad=True)\n>>>     loss = t1 + t2\n>>>     dist_autograd.backward(context_id, [loss.sum()])\n>>>     grads = dist_autograd.get_gradients(context_id)\n>>>     print(grads[t1])\n>>>     print(grads[t2])\n   \n"}, {"name": "torch.distributed.Backend", "path": "distributed#torch.distributed.Backend", "type": "torch.distributed", "text": " \nclass torch.distributed.Backend [source]\n \nAn enum-like class of available backends: GLOO, NCCL, MPI, and other registered backends. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.  \n"}, {"name": "torch.distributed.barrier()", "path": "distributed#torch.distributed.barrier", "type": "torch.distributed", "text": " \ntorch.distributed.barrier(group=None, async_op=False, device_ids=None) [source]\n \nSynchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters \n \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndevice_ids ([int], optional) \u2013 List of device/GPU ids. Valid only for NCCL backend.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast()", "path": "distributed#torch.distributed.broadcast", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast(tensor, src, group=None, async_op=False) [source]\n \nBroadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters \n \ntensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_multigpu()", "path": "distributed#torch.distributed.broadcast_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast_multigpu(tensor_list, src, group=None, async_op=False, src_tensor=0) [source]\n \nBroadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \nsrc (int) \u2013 Source rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \nsrc_tensor (int, optional) \u2013 Source tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.broadcast_object_list()", "path": "distributed#torch.distributed.broadcast_object_list", "type": "torch.distributed", "text": " \ntorch.distributed.broadcast_object_list(object_list, src=0, group=None) [source]\n \nBroadcasts picklable objects in object_list to the whole group. Similar to broadcast(), but Python objects can be passed in. Note that all objects in object_list must be picklable in order to be broadcasted.  Parameters \n \nobject_list (List[Any]) \u2013 List of input objects to broadcast. Each object must be picklable. Only objects on the src rank will be broadcast, but each rank must provide lists of equal sizes. \nsrc (int) \u2013 Source rank from which to broadcast object_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, object_list will contain the broadcasted objects from src rank.    Note For NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by torch.cuda.current_device() and it is the user\u2019s responsiblity to ensure that this is set so that each rank has an individual GPU, via torch.cuda.set_device().   Note Note that this API differs slightly from the all_gather() collective since it does not provide an async_op handle and thus will be a blocking call.   Warning broadcast_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.FileStore", "path": "distributed#torch.distributed.FileStore", "type": "torch.distributed", "text": " \nclass torch.distributed.FileStore  \nA store implementation that uses a file to store the underlying key-value pairs.  Parameters \n \nfile_name (str) \u2013 path of the file in which to store the key-value pairs \nworld_size (int) \u2013 The total number of processes using the store     Example::\n\n>>> import torch.distributed as dist\n>>> store1 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> store2 = dist.FileStore(\"/tmp/filestore\", 2)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> store1.set(\"first_key\", \"first_value\")\n>>> store2.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.gather()", "path": "distributed#torch.distributed.gather", "type": "torch.distributed", "text": " \ntorch.distributed.gather(tensor, gather_list=None, dst=0, group=None, async_op=False) [source]\n \nGathers a list of tensors in a single process.  Parameters \n \ntensor (Tensor) \u2013 Input tensor. \ngather_list (list[Tensor], optional) \u2013 List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank) \ndst (int, optional) \u2013 Destination rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.gather_object()", "path": "distributed#torch.distributed.gather_object", "type": "torch.distributed", "text": " \ntorch.distributed.gather_object(obj, object_gather_list=None, dst=0, group=None) [source]\n \nGathers picklable objects from the whole group in a single process. Similar to gather(), but Python objects can be passed in. Note that the object must be picklable in order to be gathered.  Parameters \n \nobj (Any) \u2013 Input object. Must be picklable. \nobject_gather_list (list[Any]) \u2013 Output list. On the dst rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be None on non-dst ranks. (default is None) \ndst (int, optional) \u2013 Destination rank. (default is 0) \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. On the dst rank, object_gather_list will contain the output of the collective.    Note Note that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.   Note Note that this API is not supported when using the NCCL backend.   Warning gather_object() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n   \n"}, {"name": "torch.distributed.get_backend()", "path": "distributed#torch.distributed.get_backend", "type": "torch.distributed", "text": " \ntorch.distributed.get_backend(group=None) [source]\n \nReturns the backend of the given process group.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns \nThe backend of the given process group as a lower case string.   \n"}, {"name": "torch.distributed.get_rank()", "path": "distributed#torch.distributed.get_rank", "type": "torch.distributed", "text": " \ntorch.distributed.get_rank(group=None) [source]\n \nReturns the rank of current process group Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe rank of the process group -1, if not part of the group   \n"}, {"name": "torch.distributed.get_world_size()", "path": "distributed#torch.distributed.get_world_size", "type": "torch.distributed", "text": " \ntorch.distributed.get_world_size(group=None) [source]\n \nReturns the number of processes in the current process group  Parameters \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used.  Returns \nThe world size of the process group -1, if not part of the group   \n"}, {"name": "torch.distributed.HashStore", "path": "distributed#torch.distributed.HashStore", "type": "torch.distributed", "text": " \nclass torch.distributed.HashStore  \nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.  Example::\n\n>>> import torch.distributed as dist\n>>> store = dist.HashStore()\n>>> # store can be used from other threads\n>>> # Use any of the store methods after initialization\n>>> store.set(\"first_key\", \"first_value\")\n   \n"}, {"name": "torch.distributed.init_process_group()", "path": "distributed#torch.distributed.init_process_group", "type": "torch.distributed", "text": " \ntorch.distributed.init_process_group(backend, init_method=None, timeout=datetime.timedelta(seconds=1800), world_size=-1, rank=-1, store=None, group_name='') [source]\n \nInitializes the default distributed process group, and this will also initialize the distributed package.  There are 2 main ways to initialize a process group:\n\n Specify store, rank, and world_size explicitly. Specify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.    If neither is specified, init_method is assumed to be \u201cenv://\u201d.  Parameters \n \nbackend (str or Backend) \u2013 The backend to use. Depending on build-time configurations, valid values include mpi, gloo, and nccl. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. \ninit_method (str, optional) \u2013 URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified. Mutually exclusive with store. \nworld_size (int, optional) \u2013 Number of processes participating in the job. Required if store is specified. \nrank (int, optional) \u2013 Rank of the current process (it should be a number between 0 and world_size-1). Required if store is specified. \nstore (Store, optional) \u2013 Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT or NCCL_ASYNC_ERROR_HANDLING is set to 1. When NCCL_BLOCKING_WAIT is set, this is the duration for which the process will block and wait for collectives to complete before throwing an exception. When NCCL_ASYNC_ERROR_HANDLING is set, this is the duration after which collectives will be aborted asynchronously and the process will crash. NCCL_BLOCKING_WAIT will provide errors to the user which can be caught and handled, but due to its blocking nature, it has a performance overhead. On the other hand, NCCL_ASYNC_ERROR_HANDLING has very little performance overhead, but crashes the process on errors. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. Only one of these two environment variables should be set. \ngroup_name (str, optional, deprecated) \u2013 Group name.    To enable backend == Backend.MPI, PyTorch needs to be built from source on a system that supports MPI. \n"}, {"name": "torch.distributed.irecv()", "path": "distributed#torch.distributed.irecv", "type": "torch.distributed", "text": " \ntorch.distributed.irecv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nA distributed request object. None, if not part of the group   \n"}, {"name": "torch.distributed.isend()", "path": "distributed#torch.distributed.isend", "type": "torch.distributed", "text": " \ntorch.distributed.isend(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor asynchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv   Returns \nA distributed request object. None, if not part of the group   \n"}, {"name": "torch.distributed.is_available()", "path": "distributed#torch.distributed.is_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_available() [source]\n \nReturns True if the distributed package is available. Otherwise, torch.distributed does not expose any other APIs. Currently, torch.distributed is available on Linux, MacOS and Windows. Set USE_DISTRIBUTED=1 to enable it when building PyTorch from source. Currently, the default value is USE_DISTRIBUTED=1 for Linux and Windows, USE_DISTRIBUTED=0 for MacOS. \n"}, {"name": "torch.distributed.is_initialized()", "path": "distributed#torch.distributed.is_initialized", "type": "torch.distributed", "text": " \ntorch.distributed.is_initialized() [source]\n \nChecking if the default process group has been initialized \n"}, {"name": "torch.distributed.is_mpi_available()", "path": "distributed#torch.distributed.is_mpi_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_mpi_available() [source]\n \nChecks if the MPI backend is available. \n"}, {"name": "torch.distributed.is_nccl_available()", "path": "distributed#torch.distributed.is_nccl_available", "type": "torch.distributed", "text": " \ntorch.distributed.is_nccl_available() [source]\n \nChecks if the NCCL backend is available. \n"}, {"name": "torch.distributed.new_group()", "path": "distributed#torch.distributed.new_group", "type": "torch.distributed", "text": " \ntorch.distributed.new_group(ranks=None, timeout=datetime.timedelta(seconds=1800), backend=None) [source]\n \nCreates a new distributed group. This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.  Warning Using multiple process groups with the NCCL backend concurrently is not safe and the user should perform explicit synchronization in their application to ensure only one process group is used at a time. This means collectives from one process group should have completed execution on the device (not just enqueued since CUDA execution is async) before collectives from another process group are enqueued. See Using multiple NCCL communicators concurrently for more details.   Parameters \n \nranks (list[int]) \u2013 List of ranks of group members. If None, will be set to all ranks. Default is None. \ntimeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the gloo backend. \nbackend (str or Backend, optional) \u2013 The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO).   Returns \nA handle of distributed group that can be given to collective calls.   \n"}, {"name": "torch.distributed.optim.DistributedOptimizer", "path": "rpc#torch.distributed.optim.DistributedOptimizer", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.optim.DistributedOptimizer(optimizer_class, params_rref, *args, **kwargs) [source]\n \nDistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter. This class uses get_gradients() in order to retrieve the gradients for specific parameters. Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers. DistributedOptimizer creates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) during multithreaded training (e.g. Distributed Model Parallel). This feature is currently in beta stage, enabled for optimizers including Adagrad, Adam, SGD, RMSprop, AdamW and Adadelta. We are increasing the coverage to all optimizers in future releases.  Parameters \n \noptimizer_class (optim.Optimizer) \u2013 the class of optimizer to instantiate on each worker. \nparams_rref (list[RRef]) \u2013 list of RRefs to local or remote parameters to optimize. \nargs \u2013 arguments to pass to the optimizer constructor on each worker. \nkwargs \u2013 arguments to pass to the optimizer constructor on each worker.     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> import torch.distributed.rpc as rpc\n>>> from torch import optim\n>>> from torch.distributed.optim import DistributedOptimizer\n>>>\n>>> with dist_autograd.context() as context_id:\n>>>   # Forward pass.\n>>>   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>>   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>>   loss = rref1.to_here() + rref2.to_here()\n>>>\n>>>   # Backward pass.\n>>>   dist_autograd.backward(context_id, [loss.sum()])\n>>>\n>>>   # Optimizer.\n>>>   dist_optim = DistributedOptimizer(\n>>>      optim.SGD,\n>>>      [rref1, rref2],\n>>>      lr=0.05,\n>>>   )\n>>>   dist_optim.step(context_id)\n    \nstep(context_id) [source]\n \nPerforms a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.  Parameters \ncontext_id \u2013 the autograd context id for which we should run the optimizer step.   \n \n"}, {"name": "torch.distributed.optim.DistributedOptimizer.step()", "path": "rpc#torch.distributed.optim.DistributedOptimizer.step", "type": "Distributed RPC Framework", "text": " \nstep(context_id) [source]\n \nPerforms a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The provided context_id will be used to retrieve the corresponding context that contains the gradients that should be applied to the parameters.  Parameters \ncontext_id \u2013 the autograd context id for which we should run the optimizer step.   \n"}, {"name": "torch.distributed.pipeline.sync.Pipe", "path": "pipeline#torch.distributed.pipeline.sync.Pipe", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.Pipe(module, chunks=1, checkpoint='except_last', deferred_batch_norm=False) [source]\n \nWraps an arbitrary nn.Sequential module to train on using synchronous pipeline parallelism. If the module requires lots of memory and doesn\u2019t fit on a single GPU, pipeline parallelism is a useful technique to employ for training. The implementation is based on the torchgpipe paper. Pipe combines pipeline parallelism with checkpointing to reduce peak memory required to train while minimizing device under-utilization. You should place all the modules on the appropriate devices and wrap them into an nn.Sequential module defining the desired order of execution.  Parameters \n \nmodule (nn.Sequential) \u2013 sequential module to be parallelized using pipelining. Each module in the sequence has to have all of its parameters on a single device. Each module in the sequence has to either be an nn.Module or nn.Sequential (to combine multiple sequential modules on a single device) \nchunks (int) \u2013 number of micro-batches (default: 1) \ncheckpoint (str) \u2013 when to enable checkpointing, one of 'always', 'except_last', or 'never' (default: 'except_last'). 'never' disables checkpointing completely, 'except_last' enables checkpointing for all micro-batches except the last one and 'always' enables checkpointing for all micro-batches. \ndeferred_batch_norm (bool) \u2013 whether to use deferred BatchNorm moving statistics (default: False). If set to True, we track statistics across multiple micro-batches to update the running statistics per mini-batch.   Raises \n \nTypeError \u2013 the module is not a nn.Sequential. \nValueError \u2013 invalid arguments     Example::\n\nPipeline of two FC layers across GPUs 0 and 1. >>> fc1 = nn.Linear(16, 8).cuda(0)\n>>> fc2 = nn.Linear(8, 4).cuda(1)\n>>> model = nn.Sequential(fc1, fc2)\n>>> model = Pipe(model, chunks=8)\n>>> input = torch.rand(16, 16).cuda(0)\n>>> output_rref = model(input)\n    Note You can wrap a Pipe model with torch.nn.parallel.DistributedDataParallel only when the checkpoint parameter of Pipe is 'never'.   Note Pipe only supports intra-node pipelining currently, but will be expanded to support inter-node pipelining in the future. The forward function returns an RRef to allow for inter-node pipelining in the future, where the output might be on a remote host. For intra-node pipelinining you can use local_value() to retrieve the output locally.   Warning Pipe is experimental and subject to change.   \nforward(input) [source]\n \nProcesses a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too. The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.  Parameters \ninput (torch.Tensor or sequence of Tensor) \u2013 input mini-batch  Returns \nRRef to the output of the mini-batch  Raises \nTypeError \u2013 input is not a tensor or sequence of tensors.   \n \n"}, {"name": "torch.distributed.pipeline.sync.Pipe.forward()", "path": "pipeline#torch.distributed.pipeline.sync.Pipe.forward", "type": "Pipeline Parallelism", "text": " \nforward(input) [source]\n \nProcesses a single input mini-batch through the pipe and returns an RRef pointing to the output. Pipe is a fairly transparent module wrapper. It doesn\u2019t modify the input and output signature of the underlying module. But there\u2019s type restriction. Input and output have to be a Tensor or a sequence of tensors. This restriction is applied at partition boundaries too. The input tensor is split into multiple micro-batches based on the chunks parameter used to initialize Pipe. The batch size is assumed to be the first dimension of the tensor and if the batch size is less than chunks, the number of micro-batches is equal to the batch size.  Parameters \ninput (torch.Tensor or sequence of Tensor) \u2013 input mini-batch  Returns \nRRef to the output of the mini-batch  Raises \nTypeError \u2013 input is not a tensor or sequence of tensors.   \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.pop", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.pop", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.skip.skippable.pop(name) [source]\n \nThe command to pop a skip tensor. def forward(self, input):\n    skip = yield pop('name')\n    return f(input) + skip\n  Parameters \nname (str) \u2013 name of skip tensor  Returns \nthe skip tensor previously stashed by another layer under the same name   \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.skippable()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.skippable", "type": "Pipeline Parallelism", "text": " \ntorch.distributed.pipeline.sync.skip.skippable.skippable(stash=(), pop=()) [source]\n \nThe decorator to define a nn.Module with skip connections. Decorated modules are called \u201cskippable\u201d. This functionality works perfectly fine even when the module is not wrapped by Pipe. Each skip tensor is managed by its name. Before manipulating skip tensors, a skippable module must statically declare the names for skip tensors by stash and/or pop parameters. Skip tensors with pre-declared name can be stashed by yield stash(name, tensor) or popped by tensor = yield\npop(name). Here is an example with three layers. A skip tensor named \u201c1to3\u201d is stashed and popped at the first and last layer, respectively: @skippable(stash=['1to3'])\nclass Layer1(nn.Module):\n    def forward(self, input):\n        yield stash('1to3', input)\n        return f1(input)\n\nclass Layer2(nn.Module):\n    def forward(self, input):\n        return f2(input)\n\n@skippable(pop=['1to3'])\nclass Layer3(nn.Module):\n    def forward(self, input):\n        skip_1to3 = yield pop('1to3')\n        return f3(input) + skip_1to3\n\nmodel = nn.Sequential(Layer1(), Layer2(), Layer3())\n One skippable module can stash or pop multiple skip tensors: @skippable(stash=['alice', 'bob'], pop=['carol'])\nclass StashStashPop(nn.Module):\n    def forward(self, input):\n        yield stash('alice', f_alice(input))\n        yield stash('bob', f_bob(input))\n        carol = yield pop('carol')\n        return input + carol\n Every skip tensor must be associated with exactly one pair of stash and pop. Pipe checks this restriction automatically when wrapping a module. You can also check the restriction by verify_skippables() without Pipe. \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.stash", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.stash", "type": "Pipeline Parallelism", "text": " \nclass torch.distributed.pipeline.sync.skip.skippable.stash(name, tensor) [source]\n \nThe command to stash a skip tensor. def forward(self, input):\n    yield stash('name', input)\n    return f(input)\n  Parameters \n \nname (str) \u2013 name of skip tensor \ninput (torch.Tensor or None) \u2013 tensor to pass to the skip connection    \n"}, {"name": "torch.distributed.pipeline.sync.skip.skippable.verify_skippables()", "path": "pipeline#torch.distributed.pipeline.sync.skip.skippable.verify_skippables", "type": "Pipeline Parallelism", "text": " \ntorch.distributed.pipeline.sync.skip.skippable.verify_skippables(module) [source]\n \nVerifies if the underlying skippable modules satisfy integrity. Every skip tensor must have only one pair of stash and pop. If there are one or more unmatched pairs, it will raise TypeError with the detailed messages. Here are a few failure cases. verify_skippables() will report failure for these cases: # Layer1 stashes \"1to3\".\n# Layer3 pops \"1to3\".\n\nnn.Sequential(Layer1(), Layer2())\n#               \u2514\u2500\u2500\u2500\u2500 ?\n\nnn.Sequential(Layer2(), Layer3())\n#                   ? \u2500\u2500\u2500\u2500\u2518\n\nnn.Sequential(Layer1(), Layer2(), Layer3(), Layer3())\n#               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       ^^^^^^\n\nnn.Sequential(Layer1(), Layer1(), Layer2(), Layer3())\n#             ^^^^^^      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n To use the same name for multiple skip tensors, they must be isolated by different namespaces. See isolate().  Raises \nTypeError \u2013 one or more pairs of stash and pop are not matched.   \n"}, {"name": "torch.distributed.PrefixStore", "path": "distributed#torch.distributed.PrefixStore", "type": "torch.distributed", "text": " \nclass torch.distributed.PrefixStore  \nA wrapper around any of the 3 key-value stores (TCPStore, FileStore, and HashStore) that adds a prefix to each key inserted to the store.  Parameters \n \nprefix (str) \u2013 The prefix string that is prepended to each key before being inserted into the store. \nstore (torch.distributed.store) \u2013 A store object that forms the underlying key-value store.    \n"}, {"name": "torch.distributed.recv()", "path": "distributed#torch.distributed.recv", "type": "torch.distributed", "text": " \ntorch.distributed.recv(tensor, src=None, group=None, tag=0) [source]\n \nReceives a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to fill with received data. \nsrc (int, optional) \u2013 Source rank. Will receive from any process if unspecified. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match recv with remote send   Returns \nSender rank -1, if not part of the group   \n"}, {"name": "torch.distributed.reduce()", "path": "distributed#torch.distributed.reduce", "type": "torch.distributed", "text": " \ntorch.distributed.reduce(tensor, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces the tensor data across all machines. Only the process with rank dst is going to receive the final result.  Parameters \n \ntensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.ReduceOp", "path": "distributed#torch.distributed.ReduceOp", "type": "torch.distributed", "text": " \nclass torch.distributed.ReduceOp  \nAn enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR. Note that BAND, BOR, and BXOR reductions are not available when using the NCCL backend. Additionally, MAX, MIN and PRODUCT are not supported for complex tensors. The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc. Members: SUM PRODUCT MIN MAX BAND BOR BXOR \n"}, {"name": "torch.distributed.reduce_multigpu()", "path": "distributed#torch.distributed.reduce_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_multigpu(tensor_list, dst, op=<ReduceOp.SUM: 0>, group=None, async_op=False, dst_tensor=0) [source]\n \nReduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result. Only nccl backend is currently supported tensors should only be GPU tensors  Parameters \n \ntensor_list (List[Tensor]) \u2013 Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. \ndst (int) \u2013 Destination rank \nop (optional) \u2013 One of the values from torch.distributed.ReduceOp enum. Specifies an operation used for element-wise reductions. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op \ndst_tensor (int, optional) \u2013 Destination tensor rank within tensor_list\n   Returns \nAsync work handle, if async_op is set to True. None, otherwise   \n"}, {"name": "torch.distributed.reduce_op", "path": "distributed#torch.distributed.reduce_op", "type": "torch.distributed", "text": " \nclass torch.distributed.reduce_op  \nDeprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX. ReduceOp is recommended to use instead. \n"}, {"name": "torch.distributed.reduce_scatter()", "path": "distributed#torch.distributed.reduce_scatter", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_scatter(output, input_list, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduces, then scatters a list of tensors to all processes in a group.  Parameters \n \noutput (Tensor) \u2013 Output tensor. \ninput_list (list[Tensor]) \u2013 List of tensors to reduce and scatter. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n"}, {"name": "torch.distributed.reduce_scatter_multigpu()", "path": "distributed#torch.distributed.reduce_scatter_multigpu", "type": "torch.distributed", "text": " \ntorch.distributed.reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=<ReduceOp.SUM: 0>, group=None, async_op=False) [source]\n \nReduce and scatter a list of tensors to the whole group. Only nccl backend is currently supported. Each tensor in output_tensor_list should reside on a separate GPU, as should each list of tensors in input_tensor_lists.  Parameters \n \noutput_tensor_list (List[Tensor]) \u2013 \nOutput tensors (on different GPUs) to receive the result of the operation. Note that len(output_tensor_list) needs to be the same for all the distributed processes calling this function.  \ninput_tensor_lists (List[List[Tensor]]) \u2013 \nInput lists. It should contain correctly-sized tensors on each GPU to be used for input of the collective, e.g. input_tensor_lists[i] contains the reduce_scatter input that resides on the GPU of output_tensor_list[i]. Note that each element of input_tensor_lists has the size of world_size * len(output_tensor_list), since the function scatters the result from every single GPU in the group. To interpret each element of input_tensor_lists[i], note that output_tensor_list[j] of rank k receives the reduce-scattered result from input_tensor_lists[i][k * world_size + j] Also note that len(input_tensor_lists), and the size of each element in input_tensor_lists (each element is a list, therefore len(input_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op.   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.   \n"}, {"name": "torch.distributed.rpc.BackendType", "path": "rpc#torch.distributed.rpc.BackendType", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.BackendType  \nAn enum class of available backends. PyTorch ships with two builtin backends: BackendType.TENSORPIPE and BackendType.PROCESS_GROUP. Additional ones can be registered using the register_backend() function. \n"}, {"name": "torch.distributed.rpc.functions.async_execution()", "path": "rpc#torch.distributed.rpc.functions.async_execution", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.functions.async_execution(fn) [source]\n \nA decorator for a function indicating that the return value of the function is guaranteed to be a Future object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the Future returned by the wrapped function and installs subsequent processing steps as a callback to that Future. The installed callback will read the value from the Future when completed and send the value back as the RPC response. That also means the returned Future only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function\u2019s (fn) execution needs to pause and resume due to, e.g., containing rpc_async() or waiting for other signals.  Note To enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a Future object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with @staticmethod or @classmethod, @rpc.functions.async_execution needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by @rpc.functions.async_execution.   Example::\n\nThe returned Future object can come from rpc_async(), then(), or Future constructor. The example below shows directly using the Future returned by then(). >>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @rpc.functions.async_execution\n>>> def async_add_chained(to, x, y, z):\n>>>     # This function runs on \"worker1\" and returns immediately when\n>>>     # the callback is installed through the `then(cb)` API. In the\n>>>     # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n>>>     # When the return value of that `rpc_async` arrives at\n>>>     # \"worker1\", \"worker1\" will run the lambda function accordingly\n>>>     # and set the value for the previously returned `Future`, which\n>>>     # will then trigger RPC to send the result back to \"worker0\".\n>>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>         lambda fut: fut.wait() + z\n>>>     )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add_chained,\n>>>     args=(\"worker2\", torch.ones(2), 1, 1)\n>>> )\n>>> print(ret)  # prints tensor([3., 3.])\n When combined with TorchScript decorators, this decorator must be the outmost one. >>> from torch import Tensor\n>>> from torch.futures import Future\n>>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> @torch.jit.script\n>>> def script_add(x: Tensor, y: Tensor) -> Tensor:\n>>>     return x + y\n>>>\n>>> @rpc.functions.async_execution\n>>> @torch.jit.script\n>>> def async_add(to: str, x: Tensor, y: Tensor) -> Future[Tensor]:\n>>>     return rpc.rpc_async(to, script_add, (x, y))\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1)\n>>> )\n>>> print(ret)  # prints tensor([2., 2.])\n When combined with static or class method, this decorator must be the inner one. >>> from torch.distributed import rpc\n>>>\n>>> # omitting setup and shutdown RPC\n>>>\n>>> # On all workers\n>>> class AsyncExecutionClass:\n>>>\n>>>     @staticmethod\n>>>     @rpc.functions.async_execution\n>>>     def static_async_add(to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>>     @classmethod\n>>>     @rpc.functions.async_execution\n>>>     def class_async_add(cls, to, x, y, z):\n>>>         ret_fut = torch.futures.Future()\n>>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: ret_fut.set_result(fut.wait() + z)\n>>>         )\n>>>         return ret_fut\n>>>\n>>>     @rpc.functions.async_execution\n>>>     def bound_async_add(self, to, x, y, z):\n>>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(\n>>>             lambda fut: fut.wait() + z\n>>>         )\n>>>\n>>> # On worker0\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.static_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> ret = rpc.rpc_sync(\n>>>     \"worker1\",\n>>>     AsyncExecutionClass.class_async_add,\n>>>     args=(\"worker2\", torch.ones(2), 1, 2)\n>>> )\n>>> print(ret)  # prints tensor([4., 4.])\n This decorator also works with RRef helpers, i.e., . torch.distributed.rpc.RRef.rpc_sync(), torch.distributed.rpc.RRef.rpc_async(), and torch.distributed.rpc.RRef.remote(). >>> from torch.distributed import rpc\n>>>\n>>> # reuse the AsyncExecutionClass class above\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_sync().static_async_add(\"worker2\", torch.ones(2), 1, 2)\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.rpc_async().static_async_add(\"worker2\", torch.ones(2), 1, 2).wait()\n>>> print(ret)  # prints tensor([4., 4.])\n>>>\n>>> rref = rpc.remote(\"worker1\", AsyncExecutionClass)\n>>> ret = rref.remote().static_async_add(\"worker2\", torch.ones(2), 1, 2).to_here()\n>>> print(ret)  # prints tensor([4., 4.])\n   \n"}, {"name": "torch.distributed.rpc.get_worker_info()", "path": "rpc#torch.distributed.rpc.get_worker_info", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.get_worker_info(worker_name=None) [source]\n \nGet WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.  Parameters \nworker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)  Returns \nWorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None.   \n"}, {"name": "torch.distributed.rpc.init_rpc()", "path": "rpc#torch.distributed.rpc.init_rpc", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.init_rpc(name, backend=None, rank=-1, world_size=None, rpc_backend_options=None) [source]\n \nInitializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.  Parameters \n \nname (str) \u2013 a globally unique name of this node. (e.g., Trainer3, ParameterServer2, Master, Worker1) Name can only contain number, alphabet, underscore, colon, and/or dash, and must be shorter than 128 characters. \nbackend (BackendType, optional) \u2013 The type of RPC backend implementation. Supported values include BackendType.TENSORPIPE (the default) and BackendType.PROCESS_GROUP. See Backends for more information. \nrank (int) \u2013 a globally unique id/rank of this node. \nworld_size (int) \u2013 The number of workers in the group. \nrpc_backend_options (RpcBackendOptions, optional) \u2013 The options passed to the RpcAgent constructor. It must be an agent-specific subclass of RpcBackendOptions and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using init_method = \"env://\", meaning that environment variables MASTER_ADDR and MASTER_PORT need to be set properly. See Backends for more information and find which options are available.    \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.ProcessGroupRpcBackendOptions  \nThe backend options class for ProcessGroupAgent, which is derived from RpcBackendOptions.  Parameters \n \nnum_send_recv_threads (int, optional) \u2013 The number of threads in the thread-pool used by ProcessGroupAgent (default: 4). \nrpc_timeout (float, optional) \u2013 The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in rpc_sync() and rpc_async() if necessary. \ninit_method (str, optional) \u2013 The URL to initialize ProcessGroupGloo (default: env://).     \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty num_send_recv_threads  \nThe number of threads in the thread-pool used by ProcessGroupAgent. \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.num_send_recv_threads", "type": "Distributed RPC Framework", "text": " \nproperty num_send_recv_threads  \nThe number of threads in the thread-pool used by ProcessGroupAgent. \n"}, {"name": "torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.ProcessGroupRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.remote()", "path": "rpc#torch.distributed.rpc.remote", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.remote(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds for this remote call. If the creation of this RRef on worker to is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as to_here()), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nA user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Warning The remote API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the torch.distributed.rpc.RRef.confirmed_by_owner() API.   Warning Errors such as timeouts for the remote API are handled on a best-effort basis. This means that when remote calls initiated by remote fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as to_here or fork call), then future uses of the RRef will appropriately raise errors. However, it is possible that the user application will use the RRef before the errors are handled. In this case, errors may not be raised as they have not yet been handled.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n>>> x = rref1.to_here() + rref2.to_here()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> rref = rpc.remote(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rref.to_here()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions", "path": "rpc#torch.distributed.rpc.RpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.RpcBackendOptions  \nAn abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to init_rpc() in order to initialize RPC with specific configurations, such as the RPC timeout and init_method to be used.  \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.RpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.RpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.rpc_async()", "path": "rpc#torch.distributed.rpc.rpc_async", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.rpc_async(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a Future that can be awaited on.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nReturns a Future object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the Future object.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Warning The rpc_async API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned Future completes.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n>>> result = fut1.wait() + fut2.wait()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> fut = rpc.rpc_async(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> ret = fut.wait()\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.rpc_sync()", "path": "rpc#torch.distributed.rpc.rpc_sync", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.rpc_sync(to, func, args=None, kwargs=None, timeout=-1.0) [source]\n \nMake a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.  Parameters \n \nto (str or WorkerInfo or int) \u2013 name/rank/WorkerInfo of the destination worker. \nfunc (callable) \u2013 a callable function, such as Python callables, builtin operators (e.g. add()) and annotated TorchScript functions. \nargs (tuple) \u2013 the argument tuple for the func invocation. \nkwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation. \ntimeout (float, optional) \u2013 timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with _set_rpc_timeout is used.   Returns \nReturns the result of running func with args and kwargs.    Warning Using GPU tensors as arguments or return values of func is not supported since we don\u2019t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of func.   Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n Below is an example of running a TorchScript function using RPC. >>> # On both workers:\n>>> @torch.jit.script\n>>> def my_script_add(t1, t2):\n>>>    return torch.add(t1, t2)\n >>> # On worker 0:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> ret = rpc.rpc_sync(\"worker1\", my_script_add, args=(torch.ones(2), 3))\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.RRef", "path": "rpc#torch.distributed.rpc.RRef", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.RRef [source]\n \n \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n  \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n  \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n  \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n  \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n  \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n  \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n  \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.   \n \n"}, {"name": "torch.distributed.rpc.RRef.backward()", "path": "rpc#torch.distributed.rpc.RRef.backward", "type": "Distributed RPC Framework", "text": " \nbackward(self: torch._C._distributed_rpc.PyRRef, dist_autograd_ctx_id: int = -1, retain_graph: bool = False) \u2192 None   Runs the backward pass using the RRef as the root of the backward pass. If dist_autograd_ctx_id is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, get_gradients() should be used to retrieve the gradients. If dist_autograd_ctx_id is None, it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.  Parameters \n \ndist_autograd_ctx_id (int, optional) \u2013 The distributed autograd context id for which we should retrieve the gradients (default: -1). \nretain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times (default: False).     Example::\n\n>>> import torch.distributed.autograd as dist_autograd\n>>> with dist_autograd.context() as context_id:\n>>>     rref.backward(context_id)\n   \n"}, {"name": "torch.distributed.rpc.RRef.confirmed_by_owner()", "path": "rpc#torch.distributed.rpc.RRef.confirmed_by_owner", "type": "Distributed RPC Framework", "text": " \nconfirmed_by_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether this RRef has been confirmed by the owner. OwnerRRef always returns true, while UserRRef only returns true when the owner knowns about this UserRRef. \n"}, {"name": "torch.distributed.rpc.RRef.is_owner()", "path": "rpc#torch.distributed.rpc.RRef.is_owner", "type": "Distributed RPC Framework", "text": " \nis_owner(self: torch._C._distributed_rpc.PyRRef) \u2192 bool  \nReturns whether or not the current node is the owner of this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.local_value()", "path": "rpc#torch.distributed.rpc.RRef.local_value", "type": "Distributed RPC Framework", "text": " \nlocal_value(self: torch._C._distributed_rpc.PyRRef) \u2192 object  \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. \n"}, {"name": "torch.distributed.rpc.RRef.owner()", "path": "rpc#torch.distributed.rpc.RRef.owner", "type": "Distributed RPC Framework", "text": " \nowner(self: torch._C._distributed_rpc.PyRRef) \u2192 torch._C._distributed_rpc.WorkerInfo  \nReturns worker information of the node that owns this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.owner_name()", "path": "rpc#torch.distributed.rpc.RRef.owner_name", "type": "Distributed RPC Framework", "text": " \nowner_name(self: torch._C._distributed_rpc.PyRRef) \u2192 str  \nReturns worker name of the node that owns this RRef. \n"}, {"name": "torch.distributed.rpc.RRef.remote()", "path": "rpc#torch.distributed.rpc.RRef.remote", "type": "Distributed RPC Framework", "text": " \nremote(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch a remote using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.remote().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.remote(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.remote(). If the creation of this RRef is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as to_here), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see rpc.remote() for specific timeout semantics for RRef.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.remote().size().to_here()  # returns torch.Size([2, 2])\n>>> rref.remote().view(1, 4).to_here()  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.rpc_async()", "path": "rpc#torch.distributed.rpc.RRef.rpc_async", "type": "Distributed RPC Framework", "text": " \nrpc_async(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_async using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_async().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_async(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_async(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_async().size().wait()  # returns torch.Size([2, 2])\n>>> rref.rpc_async().view(1, 4).wait()  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.rpc_sync()", "path": "rpc#torch.distributed.rpc.RRef.rpc_sync", "type": "Distributed RPC Framework", "text": " \nrpc_sync(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nCreate a helper proxy to easily launch an rpc_sync using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, rref.rpc_sync().func_name(*args, **kwargs) is the same as the following: >>> def run(rref, func_name, args, kwargs):\n>>>   return getattr(rref.local_value(), func_name)(*args, **kwargs)\n>>>\n>>> rpc.rpc_sync(rref.owner(), run, args=(rref, func_name, args, kwargs))\n  Parameters \ntimeout (float, optional) \u2013 Timeout for rref.rpc_sync(). If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.    Example::\n\n>>> from torch.distributed import rpc\n>>> rref = rpc.remote(\"worker1\", torch.add, args=(torch.zeros(2, 2), 1))\n>>> rref.rpc_sync().size()  # returns torch.Size([2, 2])\n>>> rref.rpc_sync().view(1, 4)  # returns tensor([[1., 1., 1., 1.]])\n   \n"}, {"name": "torch.distributed.rpc.RRef.to_here()", "path": "rpc#torch.distributed.rpc.RRef.to_here", "type": "Distributed RPC Framework", "text": " \nto_here(self: torch._C._distributed_rpc.PyRRef, timeout: float = -1.0) \u2192 object  \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.  Parameters \ntimeout (float, optional) \u2013 Timeout for to_here. If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.   \n"}, {"name": "torch.distributed.rpc.shutdown()", "path": "rpc#torch.distributed.rpc.shutdown", "type": "Distributed RPC Framework", "text": " \ntorch.distributed.rpc.shutdown(graceful=True) [source]\n \nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, this is a local shutdown, and it does not wait for other RPC processes to reach this method.  Warning For Future objects returned by rpc_async(), future.wait() should not be called after shutdown().   Parameters \ngraceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will 1) wait until there is no pending system messages for UserRRefs and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.    Example::\n\nMake sure that MASTER_ADDR and MASTER_PORT are set properly on both workers. Refer to init_process_group() API for more details. For example, >>> export MASTER_ADDR=localhost\n>>> export MASTER_PORT=5678\n Then run the following code in two different processes: >>> # On worker 0:\n>>> import torch\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n>>> # do some work\n>>> result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n>>> # ready to shutdown\n>>> rpc.shutdown()\n >>> # On worker 1:\n>>> import torch.distributed.rpc as rpc\n>>> rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n>>> # wait for worker 0 to finish work, and then shutdown.\n>>> rpc.shutdown()\n   \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.TensorPipeRpcBackendOptions(*, num_worker_threads=16, rpc_timeout=60.0, init_method='env://', device_maps=None, _transports=None, _channels=None) [source]\n \nThe backend options for TensorPipeAgent, derived from RpcBackendOptions.  Parameters \n \nnum_worker_threads (int, optional) \u2013 The number of threads in the thread-pool used by TensorPipeAgent to execute requests (default: 16). \nrpc_timeout (float, optional) \u2013 The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in rpc_sync() and rpc_async() if necessary. \ninit_method (str, optional) \u2013 The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of init_process_group() (default: env://). \ndevice_maps (Dict[str, Dict]) \u2013 Device placement mappings from this worker to the callee. Key is the callee worker name and value the dictionary (Dict of int, str, or torch.device) that maps this worker\u2019s devices to the callee worker\u2019s devices. (default: None)     \nproperty device_maps  \nThe device map locations. \n  \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n  \nproperty num_worker_threads  \nThe number of threads in the thread-pool used by TensorPipeAgent to execute requests. \n  \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n  \nset_device_map(to, device_map) [source]\n \nSet device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.  Parameters \n \nworker_name (str) \u2013 Callee name. \ndevice_map (Dict of python:int, str, or torch.device) \u2013 Device placement mappings from this worker to the callee. This map must be invertible.     Example::\n\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0, 1}}\n>>>     # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1, 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:1')\n   \n \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.device_maps", "type": "Distributed RPC Framework", "text": " \nproperty device_maps  \nThe device map locations. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.init_method", "type": "Distributed RPC Framework", "text": " \nproperty init_method  \nURL specifying how to initialize the process group. Default is env:// \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.num_worker_threads", "type": "Distributed RPC Framework", "text": " \nproperty num_worker_threads  \nThe number of threads in the thread-pool used by TensorPipeAgent to execute requests. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.rpc_timeout", "type": "Distributed RPC Framework", "text": " \nproperty rpc_timeout  \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out. \n"}, {"name": "torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map()", "path": "rpc#torch.distributed.rpc.TensorPipeRpcBackendOptions.set_device_map", "type": "Distributed RPC Framework", "text": " \nset_device_map(to, device_map) [source]\n \nSet device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.  Parameters \n \nworker_name (str) \u2013 Callee name. \ndevice_map (Dict of python:int, str, or torch.device) \u2013 Device placement mappings from this worker to the callee. This map must be invertible.     Example::\n\n>>> # both workers\n>>> def add(x, y):\n>>>     print(x)  # tensor([1., 1.], device='cuda:1')\n>>>     return x + y, (x + y).to(2)\n>>>\n>>> # on worker 0\n>>> options = TensorPipeRpcBackendOptions(\n>>>     num_worker_threads=8,\n>>>     device_maps={\"worker1\": {0, 1}}\n>>>     # maps worker0's cuda:0 to worker1's cuda:1\n>>> )\n>>> options.set_device_map(\"worker1\", {1, 2})\n>>> # maps worker0's cuda:1 to worker1's cuda:2\n>>>\n>>> rpc.init_rpc(\n>>>     \"worker0\",\n>>>     rank=0,\n>>>     world_size=2\n>>>     backend=rpc.BackendType.TENSORPIPE,\n>>>     rpc_backend_options=options\n>>> )\n>>>\n>>> x = torch.ones(2)\n>>> rets = rpc.rpc_sync(\"worker1\", add, args=(x.to(0), 1))\n>>> # The first argument will be moved to cuda:1 on worker1. When\n>>> # sending the return value back, it will follow the invert of\n>>> # the device map, and hence will be moved back to cuda:0 and\n>>> # cuda:1 on worker0\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:0')\n>>> print(rets[0])  # tensor([2., 2.], device='cuda:1')\n   \n"}, {"name": "torch.distributed.rpc.WorkerInfo", "path": "rpc#torch.distributed.rpc.WorkerInfo", "type": "Distributed RPC Framework", "text": " \nclass torch.distributed.rpc.WorkerInfo  \nA structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through get_worker_info() and the result can be passed in to functions such as rpc_sync(), rpc_async(), remote() to avoid copying a string on every invocation.  \nproperty id  \nGlobally unique id to identify the worker. \n  \nproperty name  \nThe name of the worker. \n \n"}, {"name": "torch.distributed.rpc.WorkerInfo.id()", "path": "rpc#torch.distributed.rpc.WorkerInfo.id", "type": "Distributed RPC Framework", "text": " \nproperty id  \nGlobally unique id to identify the worker. \n"}, {"name": "torch.distributed.rpc.WorkerInfo.name()", "path": "rpc#torch.distributed.rpc.WorkerInfo.name", "type": "Distributed RPC Framework", "text": " \nproperty name  \nThe name of the worker. \n"}, {"name": "torch.distributed.scatter()", "path": "distributed#torch.distributed.scatter", "type": "torch.distributed", "text": " \ntorch.distributed.scatter(tensor, scatter_list=None, src=0, group=None, async_op=False) [source]\n \nScatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the tensor argument.  Parameters \n \ntensor (Tensor) \u2013 Output tensor. \nscatter_list (list[Tensor]) \u2013 List of tensors to scatter (default is None, must be specified on the source rank) \nsrc (int) \u2013 Source rank (default is 0) \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \nasync_op (bool, optional) \u2013 Whether this op should be an async op   Returns \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group   \n"}, {"name": "torch.distributed.scatter_object_list()", "path": "distributed#torch.distributed.scatter_object_list", "type": "torch.distributed", "text": " \ntorch.distributed.scatter_object_list(scatter_object_output_list, scatter_object_input_list, src=0, group=None) [source]\n \nScatters picklable objects in scatter_object_input_list to the whole group. Similar to scatter(), but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of scatter_object_output_list. Note that all objects in scatter_object_input_list must be picklable in order to be scattered.  Parameters \n \nscatter_object_output_list (List[Any]) \u2013 Non-empty list whose first element will store the object scattered to this rank. \nscatter_object_input_list (List[Any]) \u2013 List of input objects to scatter. Each object must be picklable. Only objects on the src rank will be scattered, and the argument can be None for non-src ranks. \nsrc (int) \u2013 Source rank from which to scatter scatter_object_input_list. \ngroup \u2013 (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is None.   Returns \nNone. If rank is part of the group, scatter_object_output_list will have its first element set to the scattered object for this rank.    Note Note that this API differs slightly from the scatter collective since it does not provide an async_op handle and thus will be a blocking call.   Warning scatter_object_list() uses pickle module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.   Example::\n\n>>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n   \n"}, {"name": "torch.distributed.send()", "path": "distributed#torch.distributed.send", "type": "torch.distributed", "text": " \ntorch.distributed.send(tensor, dst, group=None, tag=0) [source]\n \nSends a tensor synchronously.  Parameters \n \ntensor (Tensor) \u2013 Tensor to send. \ndst (int) \u2013 Destination rank. \ngroup (ProcessGroup, optional) \u2013 The process group to work on. If None, the default process group will be used. \ntag (int, optional) \u2013 Tag to match send with remote recv    \n"}, {"name": "torch.distributed.Store", "path": "distributed#torch.distributed.Store", "type": "torch.distributed", "text": " \nclass torch.distributed.Store  \nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (TCPStore, FileStore, and HashStore). \n"}, {"name": "torch.distributed.Store.add()", "path": "distributed#torch.distributed.Store.add", "type": "torch.distributed", "text": " \ntorch.distributed.Store.add(self: torch._C._distributed_c10d.Store, arg0: str, arg1: int) \u2192 int  \nThe first call to add for a given key creates a counter associated with key in the store, initialized to amount. Subsequent calls to add with the same key increment the counter by the specified amount. Calling add() with a key that has already been set in the store by set() will result in an exception.  Parameters \n \nkey (str) \u2013 The key in the store whose counter will be incremented. \namount (int) \u2013 The quantity by which the counter will be incremented.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.delete_key()", "path": "distributed#torch.distributed.Store.delete_key", "type": "torch.distributed", "text": " \ntorch.distributed.Store.delete_key(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bool  \nDeletes the key-value pair associated with key from the store. Returns true if the key was successfully deleted, and false if it was not.  Warning The delete_key API is only supported by the TCPStore and HashStore. Using this API with the FileStore will result in an exception.   Parameters \nkey (str) \u2013 The key to be deleted from the store  Returns \nTrue if key was deleted, otherwise False.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n   \n"}, {"name": "torch.distributed.Store.get()", "path": "distributed#torch.distributed.Store.get", "type": "torch.distributed", "text": " \ntorch.distributed.Store.get(self: torch._C._distributed_c10d.Store, arg0: str) \u2192 bytes  \nRetrieves the value associated with the given key in the store. If key is not present in the store, the function will wait for timeout, which is defined when initializing the store, before throwing an exception.  Parameters \nkey (str) \u2013 The function will return the value associated with this key.  Returns \nValue associated with key if key is in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.num_keys()", "path": "distributed#torch.distributed.Store.num_keys", "type": "torch.distributed", "text": " \ntorch.distributed.Store.num_keys(self: torch._C._distributed_c10d.Store) \u2192 int  \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by set() and add() since one key is used to coordinate all the workers using the store.  Warning When used with the TCPStore, num_keys returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.   Returns \nThe number of keys present in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n   \n"}, {"name": "torch.distributed.Store.set()", "path": "distributed#torch.distributed.Store.set", "type": "torch.distributed", "text": " \ntorch.distributed.Store.set(self: torch._C._distributed_c10d.Store, arg0: str, arg1: str) \u2192 None  \nInserts the key-value pair into the store based on the supplied key and value. If key already exists in the store, it will overwrite the old value with the new supplied value.  Parameters \n \nkey (str) \u2013 The key to be added to the store. \nvalue (str) \u2013 The value associated with key to be added to the store.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n   \n"}, {"name": "torch.distributed.Store.set_timeout()", "path": "distributed#torch.distributed.Store.set_timeout", "type": "torch.distributed", "text": " \ntorch.distributed.Store.set_timeout(self: torch._C._distributed_c10d.Store, arg0: datetime.timedelta) \u2192 None  \nSets the store\u2019s default timeout. This timeout is used during initialization and in wait() and get().  Parameters \ntimeout (timedelta) \u2013 timeout to be set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n   \n"}, {"name": "torch.distributed.Store.wait()", "path": "distributed#torch.distributed.Store.wait", "type": "torch.distributed", "text": " \ntorch.distributed.Store.wait(*args, **kwargs)  \nOverloaded function.  wait(self: torch._C._distributed_c10d.Store, arg0: List[str]) -> None  Waits for each key in keys to be added to the store. If not all keys are set before the timeout (set during store initialization), then wait will throw an exception.  Parameters \nkeys (list) \u2013 List of keys on which to wait until they are set in the store.    Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n    wait(self: torch._C._distributed_c10d.Store, arg0: List[str], arg1: datetime.timedelta) -> None  Waits for each key in keys to be added to the store, and throws an exception if the keys have not been set by the supplied timeout.  Parameters \n \nkeys (list) \u2013 List of keys on which to wait until they are set in the store. \ntimeout (timedelta) \u2013 Time to wait for the keys to be added before throwing an exception.     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"], timedelta(seconds=10))\n   \n"}, {"name": "torch.distributed.TCPStore", "path": "distributed#torch.distributed.TCPStore", "type": "torch.distributed", "text": " \nclass torch.distributed.TCPStore  \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as set() to insert a key-value pair, get() to retrieve a key-value pair, etc.  Parameters \n \nhost_name (str) \u2013 The hostname or IP Address the server store should run on. \nport (int) \u2013 The port on which the server store should listen for incoming requests. \nworld_size (int) \u2013 The total number of store users (number of clients + 1 for the server). \nis_master (bool) \u2013 True when initializing the server store, False for client stores. \ntimeout (timedelta) \u2013 Timeout used by the store during initialization and for methods such as get() and wait().     Example::\n\n>>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n   \n"}, {"name": "torch.distributions", "path": "distributions", "type": "torch.distributions", "text": "Probability distributions - torch.distributions The distributions package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the TensorFlow Distributions package. It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples f(x)f(x) , the pathwise derivative requires the derivative f\u2032(x)f'(x) . The next sections discuss these two in a reinforcement learning example. For more details see Gradient Estimation Using Stochastic Computation Graphs . Score function When the probability density function is differentiable with respect to its parameters, we only need sample() and log_prob() to implement REINFORCE:  \u0394\u03b8=\u03b1r\u2202log\u2061p(a\u2223\u03c0\u03b8(s))\u2202\u03b8\\Delta\\theta = \\alpha r \\frac{\\partial\\log p(a|\\pi^\\theta(s))}{\\partial\\theta} \nwhere \u03b8\\theta  are the parameters, \u03b1\\alpha  is the learning rate, rr  is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))  is the probability of taking action aa  in state ss  given policy \u03c0\u03b8\\pi^\\theta . In practice we would sample an action from the output of a network, apply this action in an environment, and then use log_prob to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows: probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n Pathwise derivative The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the rsample() method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows: params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n Distribution  \nclass torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: object Distribution is the abstract base class for probability distributions.  \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n  \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n  \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n  \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n  \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n  \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nproperty mean  \nReturns the mean of the distribution. \n  \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n  \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n  \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n  \nproperty stddev  \nReturns the standard deviation of the distribution. \n  \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n  \nproperty variance  \nReturns the variance of the distribution. \n \n ExponentialFamily  \nclass torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below  pF(x;\u03b8)=exp\u2061(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x)) \nwhere \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.  Note This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).   \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n \n Bernoulli  \nclass torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Bernoulli distribution parameterized by probs or logits (but not both). Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p. Example: >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Boolean() \n  \nproperty variance \n \n Beta  \nclass torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Beta distribution parameterized by concentration1 and concentration0. Example: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nproperty concentration0 \n  \nproperty concentration1 \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n Binomial  \nclass torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits. Example: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n  Parameters \n \ntotal_count (int or Tensor) \u2013 number of Bernoulli trials \nprobs (Tensor) \u2013 Event probabilities \nlogits (Tensor) \u2013 Event log-odds     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Categorical  \nclass torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a categorical distribution parameterized by either probs or logits (but not both).  Note It is equivalent to the distribution that torch.multinomial() samples from.  Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1). If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index. If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.multinomial() Example: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Cauchy  \nclass torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution. Example: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n  Parameters \n \nloc (float or Tensor) \u2013 mode or median of the distribution. \nscale (float or Tensor) \u2013 half width at half maximum.     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n Chi2  \nclass torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]\n\n \n ContinuousBernoulli  \nclass torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both). The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details. Example: >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n  Parameters \n \nprobs (Number, Tensor) \u2013 (0,1) valued parameters \nlogits (Number, Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019    [1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n Dirichlet  \nclass torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Dirichlet distribution parameterized by concentration concentration. Example: >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n  Parameters \nconcentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)    \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Simplex() \n  \nproperty variance \n \n Exponential  \nclass torch.distributions.exponential.Exponential(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Exponential distribution parameterized by rate. Example: >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n  Parameters \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n FisherSnedecor  \nclass torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Fisher-Snedecor distribution parameterized by df1 and df2. Example: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n  Parameters \n \ndf1 (float or Tensor) \u2013 degrees of freedom parameter 1 \ndf2 (float or Tensor) \u2013 degrees of freedom parameter 2     \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Gamma  \nclass torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Gamma distribution parameterized by shape concentration and rate. Example: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n  Parameters \n \nconcentration (float or Tensor) \u2013 shape parameter of the distribution (often referred to as alpha) \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution (often referred to as beta)     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Geometric  \nclass torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success. Samples are non-negative integers [0, inf\u2061\\inf ). Example: >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1. Must be in range (0, 1] \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1.     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n Gumbel  \nclass torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Gumbel Distribution. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n  Parameters \n \nloc (float or Tensor) \u2013 Location parameter of the distribution \nscale (float or Tensor) \u2013 Scale parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n HalfCauchy  \nclass torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n Example: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Cauchy distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n HalfNormal  \nclass torch.distributions.half_normal.HalfNormal(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n Example: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Normal distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n Independent  \nclass torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Reinterprets some of the batch dims of a distribution as event dims. This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n  Parameters \n \nbase_distribution (torch.distributions.distribution.Distribution) \u2013 a base distribution \nreinterpreted_batch_ndims (int) \u2013 the number of batch dims to reinterpret as event dims     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_enumerate_support \n  \nproperty has_rsample \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Kumaraswamy  \nclass torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Kumaraswamy distribution. Example: >>> m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty mean \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n LKJCholesky  \nclass torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3. L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration) Example: >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n  Parameters \n \ndimension (dim) \u2013 dimension of the matrices \nconcentration (float or Tensor) \u2013 concentration/shape parameter of the distribution (often referred to as eta)    References [1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.  \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = CorrCholesky() \n \n Laplace  \nclass torch.distributions.laplace.Laplace(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Laplace distribution parameterized by loc and scale. Example: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n LogNormal  \nclass torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a log-normal distribution parameterized by loc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n Example: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of log of distribution \nscale (float or Tensor) \u2013 standard deviation of log of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty loc \n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n LowRankMultivariateNormal  \nclass torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution with shape batch_shape + event_shape\n \ncov_factor (Tensor) \u2013 factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)\n \ncov_diag (Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape\n     Note The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n   \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nvariance [source]\n\n \n MixtureSameFamily  \nclass torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component. Examples: # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n  Parameters \n \nmixture_distribution \u2013 torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]\n \ncomponent_distribution \u2013 torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(x) [source]\n\n  \nproperty component_distribution \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(x) [source]\n\n  \nproperty mean \n  \nproperty mixture_distribution \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n Multinomial  \nclass torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches. Note that total_count need not be specified if only log_prob() is called (see example below)  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.   \nsample() requires a single shared total_count for all parameters and samples. \nlog_prob() allows different total_count for each parameter and sample.  Example: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n  Parameters \n \ntotal_count (int) \u2013 number of trials \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \ntotal_count: int = None \n  \nproperty variance \n \n MultivariateNormal  \nclass torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix. The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance. Example >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution \ncovariance_matrix (Tensor) \u2013 positive-definite covariance matrix \nprecision_matrix (Tensor) \u2013 positive-definite precision matrix \nscale_tril (Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal     Note Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.   \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nproperty variance \n \n NegativeBinomial  \nclass torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs.  Parameters \n \ntotal_count (float or Tensor) \u2013 non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count \nprobs (Tensor) \u2013 Event probabilities of failure in the half open interval [0, 1) \nlogits (Tensor) \u2013 Event log-odds for probabilities of failure     \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n Normal  \nclass torch.distributions.normal.Normal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a normal (also called Gaussian) distribution parameterized by loc and scale. Example: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution (often referred to as mu) \nscale (float or Tensor) \u2013 standard deviation of the distribution (often referred to as sigma)     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n OneHotCategorical  \nclass torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a one-hot categorical distribution parameterized by probs or logits. Samples are one-hot coded vectors of size probs.size(-1).  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.distributions.Categorical() for specifications of probs and logits. Example: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = OneHot() \n  \nproperty variance \n \n Pareto  \nclass torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Pareto Type 1 distribution. Example: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of the distribution \nalpha (float or Tensor) \u2013 Shape parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nproperty support \n  \nproperty variance \n \n Poisson  \nclass torch.distributions.poisson.Poisson(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Poisson distribution parameterized by rate, the rate parameter. Samples are nonnegative integers, with a pmf given by  rateke\u2212ratek!\\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}  \nExample: >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n  Parameters \nrate (Number, Tensor) \u2013 the rate parameter    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n RelaxedBernoulli  \nclass torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples. Example: >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty temperature \n \n LogitRelaxedBernoulli  \nclass torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution. Samples are logits of values in (0, 1). See [1] for more details.  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017) [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n \n RelaxedOneHotCategorical  \nclass torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable. Example: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 unnormalized log probability for each event     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Simplex() \n  \nproperty temperature \n \n StudentT  \nclass torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n TransformedDistribution  \nclass torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events. An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical  \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_rsample \n  \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n  \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nproperty support \n \n Uniform  \nclass torch.distributions.uniform.Uniform(low, high, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Generates uniformly distributed random samples from the half-open interval [low, high). Example: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n  Parameters \n \nlow (float or Tensor) \u2013 lower range (inclusive). \nhigh (float or Tensor) \u2013 upper range (exclusive).     \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nproperty support \n  \nproperty variance \n \n VonMises  \nclass torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution A circular von Mises distribution. This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.  Example::\n\n>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n    Parameters \n \nloc (torch.Tensor) \u2013 an angle in radians. \nconcentration (torch.Tensor) \u2013 concentration parameter     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n  \nexpand(batch_shape) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(value) [source]\n\n  \nproperty mean  \nThe provided mean is the circular one. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n  \nsupport = Real() \n  \nvariance [source]\n \nThe provided variance is the circular one. \n \n Weibull  \nclass torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a two-parameter Weibull distribution. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of distribution (lambda). \nconcentration (float or Tensor) \u2013 Concentration parameter of distribution (k/shape).     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n KL Divergence  \ntorch.distributions.kl.kl_divergence(p, q) [source]\n \nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.  KL(p\u2225q)=\u222bp(x)log\u2061p(x)q(x)dxKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx \n Parameters \n \np (Distribution) \u2013 A Distribution object. \nq (Distribution) \u2013 A Distribution object.   Returns \nA batch of KL divergences of shape batch_shape.  Return type \nTensor  Raises \nNotImplementedError \u2013 If the distribution types have not been registered via register_kl().   \n  \ntorch.distributions.kl.register_kl(type_p, type_q) [source]\n \nDecorator to register a pairwise function with kl_divergence(). Usage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n  Parameters \n \ntype_p (type) \u2013 A subclass of Distribution. \ntype_q (type) \u2013 A subclass of Distribution.    \n Transforms  \nclass torch.distributions.transforms.Transform(cache_size=0) [source]\n \nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution. Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().  Parameters \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.  Variables \n \n~Transform.domain (Constraint) \u2013 The constraint representing valid inputs to this transform. \n~Transform.codomain (Constraint) \u2013 The constraint representing valid outputs to this transform which are inputs to the inverse transform. \n~Transform.bijective (bool) \u2013 Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). \n~Transform.sign (int or Tensor) \u2013 For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.     \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n  \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n  \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n  \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n  \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n \n  \nclass torch.distributions.transforms.ComposeTransform(parts, cache_size=0) [source]\n \nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.  Parameters \n \nparts (list of Transform) \u2013 A list of transforms to compose. \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.    \n  \nclass torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0) [source]\n \nWrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().  Parameters \n \nbase_transform (Transform) \u2013 A base transform. \nreinterpreted_batch_ndims (int) \u2013 The number of extra rightmost dimensions to treat as dependent.    \n  \nclass torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0) [source]\n \nUnit Jacobian transform to reshape the rightmost part of a tensor. Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().  Parameters \n \nin_shape (torch.Size) \u2013 The input event shape. \nout_shape (torch.Size) \u2013 The output event shape.    \n  \nclass torch.distributions.transforms.ExpTransform(cache_size=0) [source]\n \nTransform via the mapping y=exp\u2061(x)y = \\exp(x) . \n  \nclass torch.distributions.transforms.PowerTransform(exponent, cache_size=0) [source]\n \nTransform via the mapping y=xexponenty = x^{\\text{exponent}} . \n  \nclass torch.distributions.transforms.SigmoidTransform(cache_size=0) [source]\n \nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) . \n  \nclass torch.distributions.transforms.TanhTransform(cache_size=0) [source]\n \nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) . It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead. Note that one should use cache_size=1 when it comes to NaN/Inf values. \n  \nclass torch.distributions.transforms.AbsTransform(cache_size=0) [source]\n \nTransform via the mapping y=\u2223x\u2223y = |x| . \n  \nclass torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) [source]\n \nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x .  Parameters \n \nloc (Tensor or float) \u2013 Location parameter. \nscale (Tensor or float) \u2013 Scale parameter. \nevent_dim (int) \u2013 Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.    \n  \nclass torch.distributions.transforms.CorrCholeskyTransform(cache_size=0) [source]\n \nTransforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:  First we convert x into a lower triangular matrix in row order. For each row XiX_i  of the lower triangular part, we apply a signed version of class StickBreakingTransform to transform XiX_i  into a unit Euclidean length vector using the following steps: - Scales into the interval (\u22121,1)(-1, 1)  domain: ri=tanh\u2061(Xi)r_i = \\tanh(X_i) . - Transforms into an unsigned domain: zi=ri2z_i = r_i^2 . - Applies si=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i) . - Transforms back into signed domain: yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i} .  \n  \nclass torch.distributions.transforms.SoftmaxTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. \n  \nclass torch.distributions.transforms.StickBreakingTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. \n  \nclass torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) [source]\n \nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. \n  \nclass torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0) [source]\n \nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().  Example::\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)   \n Constraints The following constraints are implemented:  constraints.boolean constraints.cat constraints.corr_cholesky constraints.dependent constraints.greater_than(lower_bound) constraints.greater_than_eq(lower_bound) constraints.independent(constraint, reinterpreted_batch_ndims) constraints.integer_interval(lower_bound, upper_bound) constraints.interval(lower_bound, upper_bound) constraints.less_than(upper_bound) constraints.lower_cholesky constraints.lower_triangular constraints.multinomial constraints.nonnegative_integer constraints.one_hot constraints.positive_definite constraints.positive_integer constraints.positive constraints.real_vector constraints.real constraints.simplex constraints.stack constraints.unit_interval   \nclass torch.distributions.constraints.Constraint [source]\n \nAbstract base class for constraints. A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.  Variables \n \n~Constraint.is_discrete (bool) \u2013 Whether constrained space is discrete. Defaults to False. \n~Constraint.event_dim (int) \u2013 Number of rightmost dimensions that together define an event. The check() method will remove this many dimensions when computing validity.     \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n \n  \ntorch.distributions.constraints.dependent_property  \nalias of torch.distributions.constraints._DependentProperty \n  \ntorch.distributions.constraints.independent  \nalias of torch.distributions.constraints._IndependentConstraint \n  \ntorch.distributions.constraints.integer_interval  \nalias of torch.distributions.constraints._IntegerInterval \n  \ntorch.distributions.constraints.greater_than  \nalias of torch.distributions.constraints._GreaterThan \n  \ntorch.distributions.constraints.greater_than_eq  \nalias of torch.distributions.constraints._GreaterThanEq \n  \ntorch.distributions.constraints.less_than  \nalias of torch.distributions.constraints._LessThan \n  \ntorch.distributions.constraints.multinomial  \nalias of torch.distributions.constraints._Multinomial \n  \ntorch.distributions.constraints.interval  \nalias of torch.distributions.constraints._Interval \n  \ntorch.distributions.constraints.half_open_interval  \nalias of torch.distributions.constraints._HalfOpenInterval \n  \ntorch.distributions.constraints.cat  \nalias of torch.distributions.constraints._Cat \n  \ntorch.distributions.constraints.stack  \nalias of torch.distributions.constraints._Stack \n Constraint Registry PyTorch provides two global ConstraintRegistry objects that link Constraint objects to Transform objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.  \nbiject_to(constraint) looks up a bijective Transform from constraints.real to the given constraint. The returned transform is guaranteed to have .bijective = True and should implement .log_abs_det_jacobian(). \ntransform_to(constraint) looks up a not-necessarily bijective Transform from constraints.real to the given constraint. The returned transform is not guaranteed to implement .log_abs_det_jacobian().  The transform_to() registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution\u2019s .arg_constraints dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam: loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n The biject_to() registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained .support are propagated in an unconstrained space, and algorithms are typically rotation invariant.: dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n  Note An example where transform_to and biject_to differ is constraints.simplex: transform_to(constraints.simplex) returns a SoftmaxTransform that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, biject_to(constraints.simplex) returns a StickBreakingTransform that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.  The biject_to and transform_to objects can be extended by user-defined constraints and transforms using their .register() method either as a function on singleton constraints: transform_to.register(my_constraint, my_transform)\n or as a decorator on parameterized constraints: @transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n You can create your own registry by creating a new ConstraintRegistry object.  \nclass torch.distributions.constraint_registry.ConstraintRegistry [source]\n \nRegistry to link constraints to transforms.  \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n \n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli", "path": "distributions#torch.distributions.bernoulli.Bernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Bernoulli distribution parameterized by probs or logits (but not both). Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p. Example: >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Boolean() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "path": "distributions#torch.distributions.bernoulli.Bernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.entropy()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.enumerate_support()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.expand()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.logits", "path": "distributions#torch.distributions.bernoulli.Bernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.log_prob()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.mean()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.param_shape()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.probs", "path": "distributions#torch.distributions.bernoulli.Bernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.sample()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.bernoulli.Bernoulli.support", "path": "distributions#torch.distributions.bernoulli.Bernoulli.support", "type": "torch.distributions", "text": " \nsupport = Boolean() \n"}, {"name": "torch.distributions.bernoulli.Bernoulli.variance()", "path": "distributions#torch.distributions.bernoulli.Bernoulli.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.beta.Beta", "path": "distributions#torch.distributions.beta.Beta", "type": "torch.distributions", "text": " \nclass torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Beta distribution parameterized by concentration1 and concentration0. Example: >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nproperty concentration0 \n  \nproperty concentration1 \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.beta.Beta.arg_constraints", "path": "distributions#torch.distributions.beta.Beta.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.beta.Beta.concentration0()", "path": "distributions#torch.distributions.beta.Beta.concentration0", "type": "torch.distributions", "text": " \nproperty concentration0 \n"}, {"name": "torch.distributions.beta.Beta.concentration1()", "path": "distributions#torch.distributions.beta.Beta.concentration1", "type": "torch.distributions", "text": " \nproperty concentration1 \n"}, {"name": "torch.distributions.beta.Beta.entropy()", "path": "distributions#torch.distributions.beta.Beta.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.beta.Beta.expand()", "path": "distributions#torch.distributions.beta.Beta.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.has_rsample", "path": "distributions#torch.distributions.beta.Beta.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.beta.Beta.log_prob()", "path": "distributions#torch.distributions.beta.Beta.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.mean()", "path": "distributions#torch.distributions.beta.Beta.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.beta.Beta.rsample()", "path": "distributions#torch.distributions.beta.Beta.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=()) [source]\n\n"}, {"name": "torch.distributions.beta.Beta.support", "path": "distributions#torch.distributions.beta.Beta.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.beta.Beta.variance()", "path": "distributions#torch.distributions.beta.Beta.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.binomial.Binomial", "path": "distributions#torch.distributions.binomial.Binomial", "type": "torch.distributions", "text": " \nclass torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits. Example: >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n  Parameters \n \ntotal_count (int or Tensor) \u2013 number of Bernoulli trials \nprobs (Tensor) \u2013 Event probabilities \nlogits (Tensor) \u2013 Event log-odds     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.binomial.Binomial.arg_constraints", "path": "distributions#torch.distributions.binomial.Binomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)} \n"}, {"name": "torch.distributions.binomial.Binomial.enumerate_support()", "path": "distributions#torch.distributions.binomial.Binomial.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.expand()", "path": "distributions#torch.distributions.binomial.Binomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.has_enumerate_support", "path": "distributions#torch.distributions.binomial.Binomial.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.binomial.Binomial.logits", "path": "distributions#torch.distributions.binomial.Binomial.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.log_prob()", "path": "distributions#torch.distributions.binomial.Binomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.mean()", "path": "distributions#torch.distributions.binomial.Binomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.binomial.Binomial.param_shape()", "path": "distributions#torch.distributions.binomial.Binomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.binomial.Binomial.probs", "path": "distributions#torch.distributions.binomial.Binomial.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.sample()", "path": "distributions#torch.distributions.binomial.Binomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.binomial.Binomial.support()", "path": "distributions#torch.distributions.binomial.Binomial.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.binomial.Binomial.variance()", "path": "distributions#torch.distributions.binomial.Binomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.categorical.Categorical", "path": "distributions#torch.distributions.categorical.Categorical", "type": "torch.distributions", "text": " \nclass torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a categorical distribution parameterized by either probs or logits (but not both).  Note It is equivalent to the distribution that torch.multinomial() samples from.  Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}  where K is probs.size(-1). If probs is 1-dimensional with length-K, each element is the relative probability of sampling the class at that index. If probs is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.multinomial() Example: >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.categorical.Categorical.arg_constraints", "path": "distributions#torch.distributions.categorical.Categorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.categorical.Categorical.entropy()", "path": "distributions#torch.distributions.categorical.Categorical.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.enumerate_support()", "path": "distributions#torch.distributions.categorical.Categorical.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.expand()", "path": "distributions#torch.distributions.categorical.Categorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.has_enumerate_support", "path": "distributions#torch.distributions.categorical.Categorical.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.categorical.Categorical.logits", "path": "distributions#torch.distributions.categorical.Categorical.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.log_prob()", "path": "distributions#torch.distributions.categorical.Categorical.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.mean()", "path": "distributions#torch.distributions.categorical.Categorical.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.categorical.Categorical.param_shape()", "path": "distributions#torch.distributions.categorical.Categorical.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.categorical.Categorical.probs", "path": "distributions#torch.distributions.categorical.Categorical.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.sample()", "path": "distributions#torch.distributions.categorical.Categorical.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.categorical.Categorical.support()", "path": "distributions#torch.distributions.categorical.Categorical.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.categorical.Categorical.variance()", "path": "distributions#torch.distributions.categorical.Categorical.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.cauchy.Cauchy", "path": "distributions#torch.distributions.cauchy.Cauchy", "type": "torch.distributions", "text": " \nclass torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution. Example: >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n  Parameters \n \nloc (float or Tensor) \u2013 mode or median of the distribution. \nscale (float or Tensor) \u2013 half width at half maximum.     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.cauchy.Cauchy.arg_constraints", "path": "distributions#torch.distributions.cauchy.Cauchy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.cauchy.Cauchy.cdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.entropy()", "path": "distributions#torch.distributions.cauchy.Cauchy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.expand()", "path": "distributions#torch.distributions.cauchy.Cauchy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.has_rsample", "path": "distributions#torch.distributions.cauchy.Cauchy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.cauchy.Cauchy.icdf()", "path": "distributions#torch.distributions.cauchy.Cauchy.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.log_prob()", "path": "distributions#torch.distributions.cauchy.Cauchy.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.mean()", "path": "distributions#torch.distributions.cauchy.Cauchy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.cauchy.Cauchy.rsample()", "path": "distributions#torch.distributions.cauchy.Cauchy.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.cauchy.Cauchy.support", "path": "distributions#torch.distributions.cauchy.Cauchy.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.cauchy.Cauchy.variance()", "path": "distributions#torch.distributions.cauchy.Cauchy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.chi2.Chi2", "path": "distributions#torch.distributions.chi2.Chi2", "type": "torch.distributions", "text": " \nclass torch.distributions.chi2.Chi2(df, validate_args=None) [source]\n \nBases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n  Parameters \ndf (float or Tensor) \u2013 shape parameter of the distribution    \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n  \nproperty df \n  \nexpand(batch_shape, _instance=None) [source]\n\n \n"}, {"name": "torch.distributions.chi2.Chi2.arg_constraints", "path": "distributions#torch.distributions.chi2.Chi2.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.chi2.Chi2.df()", "path": "distributions#torch.distributions.chi2.Chi2.df", "type": "torch.distributions", "text": " \nproperty df \n"}, {"name": "torch.distributions.chi2.Chi2.expand()", "path": "distributions#torch.distributions.chi2.Chi2.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.constraints.cat", "path": "distributions#torch.distributions.constraints.cat", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.cat  \nalias of torch.distributions.constraints._Cat \n"}, {"name": "torch.distributions.constraints.Constraint", "path": "distributions#torch.distributions.constraints.Constraint", "type": "torch.distributions", "text": " \nclass torch.distributions.constraints.Constraint [source]\n \nAbstract base class for constraints. A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.  Variables \n \n~Constraint.is_discrete (bool) \u2013 Whether constrained space is discrete. Defaults to False. \n~Constraint.event_dim (int) \u2013 Number of rightmost dimensions that together define an event. The check() method will remove this many dimensions when computing validity.     \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n \n"}, {"name": "torch.distributions.constraints.Constraint.check()", "path": "distributions#torch.distributions.constraints.Constraint.check", "type": "torch.distributions", "text": " \ncheck(value) [source]\n \nReturns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. \n"}, {"name": "torch.distributions.constraints.dependent_property", "path": "distributions#torch.distributions.constraints.dependent_property", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.dependent_property  \nalias of torch.distributions.constraints._DependentProperty \n"}, {"name": "torch.distributions.constraints.greater_than", "path": "distributions#torch.distributions.constraints.greater_than", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.greater_than  \nalias of torch.distributions.constraints._GreaterThan \n"}, {"name": "torch.distributions.constraints.greater_than_eq", "path": "distributions#torch.distributions.constraints.greater_than_eq", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.greater_than_eq  \nalias of torch.distributions.constraints._GreaterThanEq \n"}, {"name": "torch.distributions.constraints.half_open_interval", "path": "distributions#torch.distributions.constraints.half_open_interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.half_open_interval  \nalias of torch.distributions.constraints._HalfOpenInterval \n"}, {"name": "torch.distributions.constraints.independent", "path": "distributions#torch.distributions.constraints.independent", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.independent  \nalias of torch.distributions.constraints._IndependentConstraint \n"}, {"name": "torch.distributions.constraints.integer_interval", "path": "distributions#torch.distributions.constraints.integer_interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.integer_interval  \nalias of torch.distributions.constraints._IntegerInterval \n"}, {"name": "torch.distributions.constraints.interval", "path": "distributions#torch.distributions.constraints.interval", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.interval  \nalias of torch.distributions.constraints._Interval \n"}, {"name": "torch.distributions.constraints.less_than", "path": "distributions#torch.distributions.constraints.less_than", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.less_than  \nalias of torch.distributions.constraints._LessThan \n"}, {"name": "torch.distributions.constraints.multinomial", "path": "distributions#torch.distributions.constraints.multinomial", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.multinomial  \nalias of torch.distributions.constraints._Multinomial \n"}, {"name": "torch.distributions.constraints.stack", "path": "distributions#torch.distributions.constraints.stack", "type": "torch.distributions", "text": " \ntorch.distributions.constraints.stack  \nalias of torch.distributions.constraints._Stack \n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry", "type": "torch.distributions", "text": " \nclass torch.distributions.constraint_registry.ConstraintRegistry [source]\n \nRegistry to link constraints to transforms.  \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n \n"}, {"name": "torch.distributions.constraint_registry.ConstraintRegistry.register()", "path": "distributions#torch.distributions.constraint_registry.ConstraintRegistry.register", "type": "torch.distributions", "text": " \nregister(constraint, factory=None) [source]\n \nRegisters a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n  Parameters \n \nconstraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. \nfactory (callable) \u2013 A callable that inputs a constraint object and returns a Transform object.    \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a continuous Bernoulli distribution parameterized by probs or logits (but not both). The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in (0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019 does not correspond to a probability and \u2018logits\u2019 does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details. Example: >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n  Parameters \n \nprobs (Number, Tensor) \u2013 (0,1) valued parameters \nlogits (Number, Tensor) \u2013 real valued parameters whose sigmoid matches \u2018probs\u2019    [1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. https://arxiv.org/abs/1907.06845  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance()", "path": "distributions#torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.dirichlet.Dirichlet", "path": "distributions#torch.distributions.dirichlet.Dirichlet", "type": "torch.distributions", "text": " \nclass torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Dirichlet distribution parameterized by concentration concentration. Example: >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n  Parameters \nconcentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)    \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=()) [source]\n\n  \nsupport = Simplex() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "path": "distributions#torch.distributions.dirichlet.Dirichlet.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)} \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.entropy()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.expand()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.has_rsample", "path": "distributions#torch.distributions.dirichlet.Dirichlet.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.log_prob()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.mean()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.rsample()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=()) [source]\n\n"}, {"name": "torch.distributions.dirichlet.Dirichlet.support", "path": "distributions#torch.distributions.dirichlet.Dirichlet.support", "type": "torch.distributions", "text": " \nsupport = Simplex() \n"}, {"name": "torch.distributions.dirichlet.Dirichlet.variance()", "path": "distributions#torch.distributions.dirichlet.Dirichlet.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.distribution.Distribution", "path": "distributions#torch.distributions.distribution.Distribution", "type": "torch.distributions", "text": " \nclass torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: object Distribution is the abstract base class for probability distributions.  \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n  \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n  \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n  \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n  \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n  \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n  \nproperty mean  \nReturns the mean of the distribution. \n  \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n  \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n  \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n  \nproperty stddev  \nReturns the standard deviation of the distribution. \n  \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n  \nproperty variance  \nReturns the variance of the distribution. \n \n"}, {"name": "torch.distributions.distribution.Distribution.arg_constraints()", "path": "distributions#torch.distributions.distribution.Distribution.arg_constraints", "type": "torch.distributions", "text": " \nproperty arg_constraints  \nReturns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. \n"}, {"name": "torch.distributions.distribution.Distribution.batch_shape()", "path": "distributions#torch.distributions.distribution.Distribution.batch_shape", "type": "torch.distributions", "text": " \nproperty batch_shape  \nReturns the shape over which parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.cdf()", "path": "distributions#torch.distributions.distribution.Distribution.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n \nReturns the cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.entropy()", "path": "distributions#torch.distributions.distribution.Distribution.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n \nReturns entropy of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n"}, {"name": "torch.distributions.distribution.Distribution.enumerate_support()", "path": "distributions#torch.distributions.distribution.Distribution.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters \nexpand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns \nTensor iterating over dimension 0.   \n"}, {"name": "torch.distributions.distribution.Distribution.event_shape()", "path": "distributions#torch.distributions.distribution.Distribution.event_shape", "type": "torch.distributions", "text": " \nproperty event_shape  \nReturns the shape of a single sample (without batching). \n"}, {"name": "torch.distributions.distribution.Distribution.expand()", "path": "distributions#torch.distributions.distribution.Distribution.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters \n \nbatch_shape (torch.Size) \u2013 the desired expanded size. \n_instance \u2013 new instance provided by subclasses that need to override .expand.   Returns \nNew distribution instance with batch dimensions expanded to batch_size.   \n"}, {"name": "torch.distributions.distribution.Distribution.icdf()", "path": "distributions#torch.distributions.distribution.Distribution.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n \nReturns the inverse cumulative density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.log_prob()", "path": "distributions#torch.distributions.distribution.Distribution.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n \nReturns the log of the probability density/mass function evaluated at value.  Parameters \nvalue (Tensor) \u2013    \n"}, {"name": "torch.distributions.distribution.Distribution.mean()", "path": "distributions#torch.distributions.distribution.Distribution.mean", "type": "torch.distributions", "text": " \nproperty mean  \nReturns the mean of the distribution. \n"}, {"name": "torch.distributions.distribution.Distribution.perplexity()", "path": "distributions#torch.distributions.distribution.Distribution.perplexity", "type": "torch.distributions", "text": " \nperplexity() [source]\n \nReturns perplexity of distribution, batched over batch_shape.  Returns \nTensor of shape batch_shape.   \n"}, {"name": "torch.distributions.distribution.Distribution.rsample()", "path": "distributions#torch.distributions.distribution.Distribution.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.sample()", "path": "distributions#torch.distributions.distribution.Distribution.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.sample_n()", "path": "distributions#torch.distributions.distribution.Distribution.sample_n", "type": "torch.distributions", "text": " \nsample_n(n) [source]\n \nGenerates n samples or n batches of samples if the distribution parameters are batched. \n"}, {"name": "torch.distributions.distribution.Distribution.set_default_validate_args()", "path": "distributions#torch.distributions.distribution.Distribution.set_default_validate_args", "type": "torch.distributions", "text": " \nstatic set_default_validate_args(value) [source]\n \nSets whether validation is enabled or disabled. The default behavior mimics Python\u2019s assert statement: validation is on by default, but is disabled if Python is run in optimized mode (via python -O). Validation may be expensive, so you may want to disable it once a model is working.  Parameters \nvalue (bool) \u2013 Whether to enable validation.   \n"}, {"name": "torch.distributions.distribution.Distribution.stddev()", "path": "distributions#torch.distributions.distribution.Distribution.stddev", "type": "torch.distributions", "text": " \nproperty stddev  \nReturns the standard deviation of the distribution. \n"}, {"name": "torch.distributions.distribution.Distribution.support()", "path": "distributions#torch.distributions.distribution.Distribution.support", "type": "torch.distributions", "text": " \nproperty support  \nReturns a Constraint object representing this distribution\u2019s support. \n"}, {"name": "torch.distributions.distribution.Distribution.variance()", "path": "distributions#torch.distributions.distribution.Distribution.variance", "type": "torch.distributions", "text": " \nproperty variance  \nReturns the variance of the distribution. \n"}, {"name": "torch.distributions.exponential.Exponential", "path": "distributions#torch.distributions.exponential.Exponential", "type": "torch.distributions", "text": " \nclass torch.distributions.exponential.Exponential(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Exponential distribution parameterized by rate. Example: >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n  Parameters \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.exponential.Exponential.arg_constraints", "path": "distributions#torch.distributions.exponential.Exponential.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.exponential.Exponential.cdf()", "path": "distributions#torch.distributions.exponential.Exponential.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.entropy()", "path": "distributions#torch.distributions.exponential.Exponential.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.expand()", "path": "distributions#torch.distributions.exponential.Exponential.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.has_rsample", "path": "distributions#torch.distributions.exponential.Exponential.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.exponential.Exponential.icdf()", "path": "distributions#torch.distributions.exponential.Exponential.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.log_prob()", "path": "distributions#torch.distributions.exponential.Exponential.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.mean()", "path": "distributions#torch.distributions.exponential.Exponential.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.exponential.Exponential.rsample()", "path": "distributions#torch.distributions.exponential.Exponential.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.exponential.Exponential.stddev()", "path": "distributions#torch.distributions.exponential.Exponential.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.exponential.Exponential.support", "path": "distributions#torch.distributions.exponential.Exponential.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.exponential.Exponential.variance()", "path": "distributions#torch.distributions.exponential.Exponential.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.exp_family.ExponentialFamily", "path": "distributions#torch.distributions.exp_family.ExponentialFamily", "type": "torch.distributions", "text": " \nclass torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below  pF(x;\u03b8)=exp\u2061(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x)) \nwhere \u03b8\\theta  denotes the natural parameters, t(x)t(x)  denotes the sufficient statistic, F(\u03b8)F(\\theta)  is the log normalizer function for a given family and k(x)k(x)  is the carrier measure.  Note This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).   \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n \n"}, {"name": "torch.distributions.exp_family.ExponentialFamily.entropy()", "path": "distributions#torch.distributions.exp_family.ExponentialFamily.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n \nMethod to compute the entropy using Bregman divergence of the log normalizer. \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor", "type": "torch.distributions", "text": " \nclass torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Fisher-Snedecor distribution parameterized by df1 and df2. Example: >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n  Parameters \n \ndf1 (float or Tensor) \u2013 degrees of freedom parameter 1 \ndf2 (float or Tensor) \u2013 degrees of freedom parameter 2     \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.expand()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.mean()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.rsample()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.support", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.fishersnedecor.FisherSnedecor.variance()", "path": "distributions#torch.distributions.fishersnedecor.FisherSnedecor.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.gamma.Gamma", "path": "distributions#torch.distributions.gamma.Gamma", "type": "torch.distributions", "text": " \nclass torch.distributions.gamma.Gamma(concentration, rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Gamma distribution parameterized by shape concentration and rate. Example: >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n  Parameters \n \nconcentration (float or Tensor) \u2013 shape parameter of the distribution (often referred to as alpha) \nrate (float or Tensor) \u2013 rate = 1 / scale of the distribution (often referred to as beta)     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.gamma.Gamma.arg_constraints", "path": "distributions#torch.distributions.gamma.Gamma.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.gamma.Gamma.entropy()", "path": "distributions#torch.distributions.gamma.Gamma.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.expand()", "path": "distributions#torch.distributions.gamma.Gamma.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.has_rsample", "path": "distributions#torch.distributions.gamma.Gamma.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.gamma.Gamma.log_prob()", "path": "distributions#torch.distributions.gamma.Gamma.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.mean()", "path": "distributions#torch.distributions.gamma.Gamma.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.gamma.Gamma.rsample()", "path": "distributions#torch.distributions.gamma.Gamma.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.gamma.Gamma.support", "path": "distributions#torch.distributions.gamma.Gamma.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.gamma.Gamma.variance()", "path": "distributions#torch.distributions.gamma.Gamma.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.geometric.Geometric", "path": "distributions#torch.distributions.geometric.Geometric", "type": "torch.distributions", "text": " \nclass torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1  Bernoulli trials, the first kk  trials failed, before seeing a success. Samples are non-negative integers [0, inf\u2061\\inf ). Example: >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n  Parameters \n \nprobs (Number, Tensor) \u2013 the probability of sampling 1. Must be in range (0, 1] \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1.     \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.geometric.Geometric.arg_constraints", "path": "distributions#torch.distributions.geometric.Geometric.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.geometric.Geometric.entropy()", "path": "distributions#torch.distributions.geometric.Geometric.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.expand()", "path": "distributions#torch.distributions.geometric.Geometric.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.logits", "path": "distributions#torch.distributions.geometric.Geometric.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.log_prob()", "path": "distributions#torch.distributions.geometric.Geometric.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.mean()", "path": "distributions#torch.distributions.geometric.Geometric.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.geometric.Geometric.probs", "path": "distributions#torch.distributions.geometric.Geometric.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.sample()", "path": "distributions#torch.distributions.geometric.Geometric.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.geometric.Geometric.support", "path": "distributions#torch.distributions.geometric.Geometric.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.geometric.Geometric.variance()", "path": "distributions#torch.distributions.geometric.Geometric.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.gumbel.Gumbel", "path": "distributions#torch.distributions.gumbel.Gumbel", "type": "torch.distributions", "text": " \nclass torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Gumbel Distribution. Examples: >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n  Parameters \n \nloc (float or Tensor) \u2013 Location parameter of the distribution \nscale (float or Tensor) \u2013 Scale parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.gumbel.Gumbel.arg_constraints", "path": "distributions#torch.distributions.gumbel.Gumbel.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.gumbel.Gumbel.entropy()", "path": "distributions#torch.distributions.gumbel.Gumbel.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.expand()", "path": "distributions#torch.distributions.gumbel.Gumbel.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.log_prob()", "path": "distributions#torch.distributions.gumbel.Gumbel.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.gumbel.Gumbel.mean()", "path": "distributions#torch.distributions.gumbel.Gumbel.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.gumbel.Gumbel.stddev()", "path": "distributions#torch.distributions.gumbel.Gumbel.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.gumbel.Gumbel.support", "path": "distributions#torch.distributions.gumbel.Gumbel.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.gumbel.Gumbel.variance()", "path": "distributions#torch.distributions.gumbel.Gumbel.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy", "type": "torch.distributions", "text": " \nclass torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n Example: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Cauchy distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.cdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.entropy()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.expand()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.icdf()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.icdf", "type": "torch.distributions", "text": " \nicdf(prob) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.log_prob()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.mean()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.scale()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.support", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.half_cauchy.HalfCauchy.variance()", "path": "distributions#torch.distributions.half_cauchy.HalfCauchy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.half_normal.HalfNormal", "path": "distributions#torch.distributions.half_normal.HalfNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.half_normal.HalfNormal(scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n Example: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n  Parameters \nscale (float or Tensor) \u2013 scale of the full Normal distribution    \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(prob) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.half_normal.HalfNormal.arg_constraints", "path": "distributions#torch.distributions.half_normal.HalfNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.half_normal.HalfNormal.cdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.entropy()", "path": "distributions#torch.distributions.half_normal.HalfNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.expand()", "path": "distributions#torch.distributions.half_normal.HalfNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.has_rsample", "path": "distributions#torch.distributions.half_normal.HalfNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.half_normal.HalfNormal.icdf()", "path": "distributions#torch.distributions.half_normal.HalfNormal.icdf", "type": "torch.distributions", "text": " \nicdf(prob) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.log_prob()", "path": "distributions#torch.distributions.half_normal.HalfNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.half_normal.HalfNormal.mean()", "path": "distributions#torch.distributions.half_normal.HalfNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.half_normal.HalfNormal.scale()", "path": "distributions#torch.distributions.half_normal.HalfNormal.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.half_normal.HalfNormal.support", "path": "distributions#torch.distributions.half_normal.HalfNormal.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.half_normal.HalfNormal.variance()", "path": "distributions#torch.distributions.half_normal.HalfNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.independent.Independent", "path": "distributions#torch.distributions.independent.Independent", "type": "torch.distributions", "text": " \nclass torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Reinterprets some of the batch dims of a distribution as event dims. This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n  Parameters \n \nbase_distribution (torch.distributions.distribution.Distribution) \u2013 a base distribution \nreinterpreted_batch_ndims (int) \u2013 the number of batch dims to reinterpret as event dims     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_enumerate_support \n  \nproperty has_rsample \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.independent.Independent.arg_constraints", "path": "distributions#torch.distributions.independent.Independent.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.independent.Independent.entropy()", "path": "distributions#torch.distributions.independent.Independent.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.independent.Independent.enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.expand()", "path": "distributions#torch.distributions.independent.Independent.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.has_enumerate_support()", "path": "distributions#torch.distributions.independent.Independent.has_enumerate_support", "type": "torch.distributions", "text": " \nproperty has_enumerate_support \n"}, {"name": "torch.distributions.independent.Independent.has_rsample()", "path": "distributions#torch.distributions.independent.Independent.has_rsample", "type": "torch.distributions", "text": " \nproperty has_rsample \n"}, {"name": "torch.distributions.independent.Independent.log_prob()", "path": "distributions#torch.distributions.independent.Independent.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.mean()", "path": "distributions#torch.distributions.independent.Independent.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.independent.Independent.rsample()", "path": "distributions#torch.distributions.independent.Independent.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.sample()", "path": "distributions#torch.distributions.independent.Independent.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.independent.Independent.support()", "path": "distributions#torch.distributions.independent.Independent.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.independent.Independent.variance()", "path": "distributions#torch.distributions.independent.Independent.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.kl.kl_divergence()", "path": "distributions#torch.distributions.kl.kl_divergence", "type": "torch.distributions", "text": " \ntorch.distributions.kl.kl_divergence(p, q) [source]\n \nCompute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)  between two distributions.  KL(p\u2225q)=\u222bp(x)log\u2061p(x)q(x)dxKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx \n Parameters \n \np (Distribution) \u2013 A Distribution object. \nq (Distribution) \u2013 A Distribution object.   Returns \nA batch of KL divergences of shape batch_shape.  Return type \nTensor  Raises \nNotImplementedError \u2013 If the distribution types have not been registered via register_kl().   \n"}, {"name": "torch.distributions.kl.register_kl()", "path": "distributions#torch.distributions.kl.register_kl", "type": "torch.distributions", "text": " \ntorch.distributions.kl.register_kl(type_p, type_q) [source]\n \nDecorator to register a pairwise function with kl_divergence(). Usage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n  Parameters \n \ntype_p (type) \u2013 A subclass of Distribution. \ntype_q (type) \u2013 A subclass of Distribution.    \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy", "type": "torch.distributions", "text": " \nclass torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Kumaraswamy distribution. Example: >>> m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n  Parameters \n \nconcentration1 (float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) \nconcentration0 (float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty mean \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.entropy()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.expand()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.mean()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.support", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.kumaraswamy.Kumaraswamy.variance()", "path": "distributions#torch.distributions.kumaraswamy.Kumaraswamy.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.laplace.Laplace", "path": "distributions#torch.distributions.laplace.Laplace", "type": "torch.distributions", "text": " \nclass torch.distributions.laplace.Laplace(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Laplace distribution parameterized by loc and scale. Example: >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.laplace.Laplace.arg_constraints", "path": "distributions#torch.distributions.laplace.Laplace.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.laplace.Laplace.cdf()", "path": "distributions#torch.distributions.laplace.Laplace.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.entropy()", "path": "distributions#torch.distributions.laplace.Laplace.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.expand()", "path": "distributions#torch.distributions.laplace.Laplace.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.has_rsample", "path": "distributions#torch.distributions.laplace.Laplace.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.laplace.Laplace.icdf()", "path": "distributions#torch.distributions.laplace.Laplace.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.log_prob()", "path": "distributions#torch.distributions.laplace.Laplace.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.mean()", "path": "distributions#torch.distributions.laplace.Laplace.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.laplace.Laplace.rsample()", "path": "distributions#torch.distributions.laplace.Laplace.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.laplace.Laplace.stddev()", "path": "distributions#torch.distributions.laplace.Laplace.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.laplace.Laplace.support", "path": "distributions#torch.distributions.laplace.Laplace.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.laplace.Laplace.variance()", "path": "distributions#torch.distributions.laplace.Laplace.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky", "type": "torch.distributions", "text": " \nclass torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by concentration parameter \u03b7\\eta  to make the probability of the correlation matrix MM  generated from a Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1} . Because of that, when concentration == 1, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the LKJCorr distribution. For sampling, this uses the Onion method from [1] Section 3. L ~ LKJCholesky(dim, concentration) X = L @ L\u2019 ~ LKJCorr(dim, concentration) Example: >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n  Parameters \n \ndimension (dim) \u2013 dimension of the matrices \nconcentration (float or Tensor) \u2013 concentration/shape parameter of the distribution (often referred to as eta)    References [1] Generating random correlation matrices based on vines and extended onion method, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.  \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = CorrCholesky() \n \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.expand()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.log_prob()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.sample()", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.lkj_cholesky.LKJCholesky.support", "path": "distributions#torch.distributions.lkj_cholesky.LKJCholesky.support", "type": "torch.distributions", "text": " \nsupport = CorrCholesky() \n"}, {"name": "torch.distributions.log_normal.LogNormal", "path": "distributions#torch.distributions.log_normal.LogNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a log-normal distribution parameterized by loc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n Example: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of log of distribution \nscale (float or Tensor) \u2013 standard deviation of log of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty loc \n  \nproperty mean \n  \nproperty scale \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.log_normal.LogNormal.arg_constraints", "path": "distributions#torch.distributions.log_normal.LogNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.log_normal.LogNormal.entropy()", "path": "distributions#torch.distributions.log_normal.LogNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.expand()", "path": "distributions#torch.distributions.log_normal.LogNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.log_normal.LogNormal.has_rsample", "path": "distributions#torch.distributions.log_normal.LogNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.log_normal.LogNormal.loc()", "path": "distributions#torch.distributions.log_normal.LogNormal.loc", "type": "torch.distributions", "text": " \nproperty loc \n"}, {"name": "torch.distributions.log_normal.LogNormal.mean()", "path": "distributions#torch.distributions.log_normal.LogNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.log_normal.LogNormal.scale()", "path": "distributions#torch.distributions.log_normal.LogNormal.scale", "type": "torch.distributions", "text": " \nproperty scale \n"}, {"name": "torch.distributions.log_normal.LogNormal.support", "path": "distributions#torch.distributions.log_normal.LogNormal.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.log_normal.LogNormal.variance()", "path": "distributions#torch.distributions.log_normal.LogNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n Example >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution with shape batch_shape + event_shape\n \ncov_factor (Tensor) \u2013 factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,)\n \ncov_diag (Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape\n     Note The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n   \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nvariance [source]\n\n \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)} \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": " \ncovariance_matrix [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "type": "torch.distributions", "text": " \nprecision_matrix [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample()", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "type": "torch.distributions", "text": " \nscale_tril [source]\n\n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "type": "torch.distributions", "text": " \nsupport = IndependentConstraint(Real(), 1) \n"}, {"name": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "path": "distributions#torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "type": "torch.distributions", "text": " \nvariance [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily", "type": "torch.distributions", "text": " \nclass torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution The MixtureSameFamily distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a Categorical \u201cselecting distribution\u201d (over k component) and a component distribution, i.e., a Distribution with a rightmost batch shape (equal to [k]) which indexes each (batch of) component. Examples: # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n  Parameters \n \nmixture_distribution \u2013 torch.distributions.Categorical-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the component_distribution. Must have either scalar batch_shape or batch_shape matching component_distribution.batch_shape[:-1]\n \ncomponent_distribution \u2013 torch.distributions.Distribution-like instance. Right-most batch dimension indexes component.     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(x) [source]\n\n  \nproperty component_distribution \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(x) [source]\n\n  \nproperty mean \n  \nproperty mixture_distribution \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.cdf()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.cdf", "type": "torch.distributions", "text": " \ncdf(x) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution", "type": "torch.distributions", "text": " \nproperty component_distribution \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.expand()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = False \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.log_prob()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.log_prob", "type": "torch.distributions", "text": " \nlog_prob(x) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mean()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution", "type": "torch.distributions", "text": " \nproperty mixture_distribution \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.sample()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.support()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.mixture_same_family.MixtureSameFamily.variance()", "path": "distributions#torch.distributions.mixture_same_family.MixtureSameFamily.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.multinomial.Multinomial", "path": "distributions#torch.distributions.multinomial.Multinomial", "type": "torch.distributions", "text": " \nclass torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches. Note that total_count need not be specified if only log_prob() is called (see example below)  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.   \nsample() requires a single shared total_count for all parameters and samples. \nlog_prob() allows different total_count for each parameter and sample.  Example: >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n  Parameters \n \ntotal_count (int) \u2013 number of trials \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty support \n  \ntotal_count: int = None \n  \nproperty variance \n \n"}, {"name": "torch.distributions.multinomial.Multinomial.arg_constraints", "path": "distributions#torch.distributions.multinomial.Multinomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.multinomial.Multinomial.expand()", "path": "distributions#torch.distributions.multinomial.Multinomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.logits()", "path": "distributions#torch.distributions.multinomial.Multinomial.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.multinomial.Multinomial.log_prob()", "path": "distributions#torch.distributions.multinomial.Multinomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.mean()", "path": "distributions#torch.distributions.multinomial.Multinomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.multinomial.Multinomial.param_shape()", "path": "distributions#torch.distributions.multinomial.Multinomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.multinomial.Multinomial.probs()", "path": "distributions#torch.distributions.multinomial.Multinomial.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.multinomial.Multinomial.sample()", "path": "distributions#torch.distributions.multinomial.Multinomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.multinomial.Multinomial.support()", "path": "distributions#torch.distributions.multinomial.Multinomial.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.multinomial.Multinomial.total_count", "path": "distributions#torch.distributions.multinomial.Multinomial.total_count", "type": "torch.distributions", "text": " \ntotal_count: int = None \n"}, {"name": "torch.distributions.multinomial.Multinomial.variance()", "path": "distributions#torch.distributions.multinomial.Multinomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal", "type": "torch.distributions", "text": " \nclass torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix. The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}  or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}  or a lower-triangular matrix L\\mathbf{L}  with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance. Example >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n  Parameters \n \nloc (Tensor) \u2013 mean of the distribution \ncovariance_matrix (Tensor) \u2013 positive-definite covariance matrix \nprecision_matrix (Tensor) \u2013 positive-definite precision matrix \nscale_tril (Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal     Note Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.   \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n  \ncovariance_matrix [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nprecision_matrix [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nscale_tril [source]\n\n  \nsupport = IndependentConstraint(Real(), 1) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()} \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "type": "torch.distributions", "text": " \ncovariance_matrix [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.entropy()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.expand()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.mean()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "type": "torch.distributions", "text": " \nprecision_matrix [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.rsample()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "type": "torch.distributions", "text": " \nscale_tril [source]\n\n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.support", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.support", "type": "torch.distributions", "text": " \nsupport = IndependentConstraint(Real(), 1) \n"}, {"name": "torch.distributions.multivariate_normal.MultivariateNormal.variance()", "path": "distributions#torch.distributions.multivariate_normal.MultivariateNormal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial", "type": "torch.distributions", "text": " \nclass torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of failure of each Bernoulli trial is probs.  Parameters \n \ntotal_count (float or Tensor) \u2013 non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count \nprobs (Tensor) \u2013 Event probabilities of failure in the half open interval [0, 1) \nlogits (Tensor) \u2013 Event log-odds for probabilities of failure     \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty mean \n  \nproperty param_shape \n  \nprobs [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)} \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.expand()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.logits", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.log_prob()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.mean()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.param_shape()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.probs", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.sample()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.support", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.negative_binomial.NegativeBinomial.variance()", "path": "distributions#torch.distributions.negative_binomial.NegativeBinomial.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.normal.Normal", "path": "distributions#torch.distributions.normal.Normal", "type": "torch.distributions", "text": " \nclass torch.distributions.normal.Normal(loc, scale, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a normal (also called Gaussian) distribution parameterized by loc and scale. Example: >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n  Parameters \n \nloc (float or Tensor) \u2013 mean of the distribution (often referred to as mu) \nscale (float or Tensor) \u2013 standard deviation of the distribution (often referred to as sigma)     \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.normal.Normal.arg_constraints", "path": "distributions#torch.distributions.normal.Normal.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.normal.Normal.cdf()", "path": "distributions#torch.distributions.normal.Normal.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.entropy()", "path": "distributions#torch.distributions.normal.Normal.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.normal.Normal.expand()", "path": "distributions#torch.distributions.normal.Normal.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.has_rsample", "path": "distributions#torch.distributions.normal.Normal.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.normal.Normal.icdf()", "path": "distributions#torch.distributions.normal.Normal.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.log_prob()", "path": "distributions#torch.distributions.normal.Normal.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.mean()", "path": "distributions#torch.distributions.normal.Normal.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.normal.Normal.rsample()", "path": "distributions#torch.distributions.normal.Normal.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.sample()", "path": "distributions#torch.distributions.normal.Normal.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.normal.Normal.stddev()", "path": "distributions#torch.distributions.normal.Normal.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.normal.Normal.support", "path": "distributions#torch.distributions.normal.Normal.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.normal.Normal.variance()", "path": "distributions#torch.distributions.normal.Normal.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical", "type": "torch.distributions", "text": " \nclass torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a one-hot categorical distribution parameterized by probs or logits. Samples are one-hot coded vectors of size probs.size(-1).  Note The probs argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:probs will return this normalized value. The logits argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:logits will return this normalized value.  See also: torch.distributions.Categorical() for specifications of probs and logits. Example: >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n  Parameters \n \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 event log probabilities (unnormalized)     \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nentropy() [source]\n\n  \nenumerate_support(expand=True) [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_enumerate_support = True \n  \nlog_prob(value) [source]\n\n  \nproperty logits \n  \nproperty mean \n  \nproperty param_shape \n  \nproperty probs \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = OneHot() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "type": "torch.distributions", "text": " \nenumerate_support(expand=True) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.expand()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "type": "torch.distributions", "text": " \nhas_enumerate_support = True \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.logits()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.mean()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.probs()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.sample()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.support", "type": "torch.distributions", "text": " \nsupport = OneHot() \n"}, {"name": "torch.distributions.one_hot_categorical.OneHotCategorical.variance()", "path": "distributions#torch.distributions.one_hot_categorical.OneHotCategorical.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.pareto.Pareto", "path": "distributions#torch.distributions.pareto.Pareto", "type": "torch.distributions", "text": " \nclass torch.distributions.pareto.Pareto(scale, alpha, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Pareto Type 1 distribution. Example: >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of the distribution \nalpha (float or Tensor) \u2013 Shape parameter of the distribution     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.pareto.Pareto.arg_constraints", "path": "distributions#torch.distributions.pareto.Pareto.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.pareto.Pareto.entropy()", "path": "distributions#torch.distributions.pareto.Pareto.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.pareto.Pareto.expand()", "path": "distributions#torch.distributions.pareto.Pareto.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.pareto.Pareto.mean()", "path": "distributions#torch.distributions.pareto.Pareto.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.pareto.Pareto.support()", "path": "distributions#torch.distributions.pareto.Pareto.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.pareto.Pareto.variance()", "path": "distributions#torch.distributions.pareto.Pareto.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.poisson.Poisson", "path": "distributions#torch.distributions.poisson.Poisson", "type": "torch.distributions", "text": " \nclass torch.distributions.poisson.Poisson(rate, validate_args=None) [source]\n \nBases: torch.distributions.exp_family.ExponentialFamily Creates a Poisson distribution parameterized by rate, the rate parameter. Samples are nonnegative integers, with a pmf given by  rateke\u2212ratek!\\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}  \nExample: >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n  Parameters \nrate (Number, Tensor) \u2013 the rate parameter    \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = IntegerGreaterThan(lower_bound=0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.poisson.Poisson.arg_constraints", "path": "distributions#torch.distributions.poisson.Poisson.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'rate': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.poisson.Poisson.expand()", "path": "distributions#torch.distributions.poisson.Poisson.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.log_prob()", "path": "distributions#torch.distributions.poisson.Poisson.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.mean()", "path": "distributions#torch.distributions.poisson.Poisson.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.poisson.Poisson.sample()", "path": "distributions#torch.distributions.poisson.Poisson.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.poisson.Poisson.support", "path": "distributions#torch.distributions.poisson.Poisson.support", "type": "torch.distributions", "text": " \nsupport = IntegerGreaterThan(lower_bound=0) \n"}, {"name": "torch.distributions.poisson.Poisson.variance()", "path": "distributions#torch.distributions.poisson.Poisson.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution. Samples are logits of values in (0, 1). See [1] for more details.  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017) [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)  \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nlog_prob(value) [source]\n\n  \nlogits [source]\n\n  \nproperty param_shape \n  \nprobs [source]\n\n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "type": "torch.distributions", "text": " \nlogits [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "type": "torch.distributions", "text": " \nproperty param_shape \n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "type": "torch.distributions", "text": " \nprobs [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample()", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples. Example: >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Number, Tensor) \u2013 the probability of sampling 1\n \nlogits (Number, Tensor) \u2013 the log-odds of sampling 1\n     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n  \nproperty temperature \n \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)} \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "type": "torch.distributions", "text": " \nsupport = Interval(lower_bound=0.0, upper_bound=1.0) \n"}, {"name": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature()", "path": "distributions#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "type": "torch.distributions", "text": " \nproperty temperature \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "type": "torch.distributions", "text": " \nclass torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable. Example: >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n  Parameters \n \ntemperature (Tensor) \u2013 relaxation temperature \nprobs (Tensor) \u2013 event probabilities \nlogits (Tensor) \u2013 unnormalized log probability for each event     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nproperty logits \n  \nproperty probs \n  \nsupport = Simplex() \n  \nproperty temperature \n \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()} \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "type": "torch.distributions", "text": " \nproperty logits \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "type": "torch.distributions", "text": " \nproperty probs \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "type": "torch.distributions", "text": " \nsupport = Simplex() \n"}, {"name": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature()", "path": "distributions#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "type": "torch.distributions", "text": " \nproperty temperature \n"}, {"name": "torch.distributions.studentT.StudentT", "path": "distributions#torch.distributions.studentT.StudentT", "type": "torch.distributions", "text": " \nclass torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n  Parameters \n \ndf (float or Tensor) \u2013 degrees of freedom \nloc (float or Tensor) \u2013 mean of the distribution \nscale (float or Tensor) \u2013 scale of the distribution     \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nsupport = Real() \n  \nproperty variance \n \n"}, {"name": "torch.distributions.studentT.StudentT.arg_constraints", "path": "distributions#torch.distributions.studentT.StudentT.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.studentT.StudentT.entropy()", "path": "distributions#torch.distributions.studentT.StudentT.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.expand()", "path": "distributions#torch.distributions.studentT.StudentT.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.has_rsample", "path": "distributions#torch.distributions.studentT.StudentT.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.studentT.StudentT.log_prob()", "path": "distributions#torch.distributions.studentT.StudentT.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.mean()", "path": "distributions#torch.distributions.studentT.StudentT.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.studentT.StudentT.rsample()", "path": "distributions#torch.distributions.studentT.StudentT.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.studentT.StudentT.support", "path": "distributions#torch.distributions.studentT.StudentT.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.studentT.StudentT.variance()", "path": "distributions#torch.distributions.studentT.StudentT.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution", "type": "torch.distributions", "text": " \nclass torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events. An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical  \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n  \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty has_rsample \n  \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n  \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n  \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n  \nproperty support \n \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {} \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.cdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.expand()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "type": "torch.distributions", "text": " \nproperty has_rsample \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.icdf()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.rsample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.sample()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. \n"}, {"name": "torch.distributions.transformed_distribution.TransformedDistribution.support()", "path": "distributions#torch.distributions.transformed_distribution.TransformedDistribution.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.transforms.AbsTransform", "path": "distributions#torch.distributions.transforms.AbsTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.AbsTransform(cache_size=0) [source]\n \nTransform via the mapping y=\u2223x\u2223y = |x| . \n"}, {"name": "torch.distributions.transforms.AffineTransform", "path": "distributions#torch.distributions.transforms.AffineTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0) [source]\n \nTransform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times x .  Parameters \n \nloc (Tensor or float) \u2013 Location parameter. \nscale (Tensor or float) \u2013 Scale parameter. \nevent_dim (int) \u2013 Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.    \n"}, {"name": "torch.distributions.transforms.ComposeTransform", "path": "distributions#torch.distributions.transforms.ComposeTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ComposeTransform(parts, cache_size=0) [source]\n \nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.  Parameters \n \nparts (list of Transform) \u2013 A list of transforms to compose. \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.    \n"}, {"name": "torch.distributions.transforms.CorrCholeskyTransform", "path": "distributions#torch.distributions.transforms.CorrCholeskyTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.CorrCholeskyTransform(cache_size=0) [source]\n \nTransforms an uncontrained real vector xx  with length D\u2217(D\u22121)/2D*(D-1)/2  into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:  First we convert x into a lower triangular matrix in row order. For each row XiX_i  of the lower triangular part, we apply a signed version of class StickBreakingTransform to transform XiX_i  into a unit Euclidean length vector using the following steps: - Scales into the interval (\u22121,1)(-1, 1)  domain: ri=tanh\u2061(Xi)r_i = \\tanh(X_i) . - Transforms into an unsigned domain: zi=ri2z_i = r_i^2 . - Applies si=StickBreakingTransform(zi)s_i = StickBreakingTransform(z_i) . - Transforms back into signed domain: yi=sign(ri)\u2217siy_i = sign(r_i) * \\sqrt{s_i} .  \n"}, {"name": "torch.distributions.transforms.ExpTransform", "path": "distributions#torch.distributions.transforms.ExpTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ExpTransform(cache_size=0) [source]\n \nTransform via the mapping y=exp\u2061(x)y = \\exp(x) . \n"}, {"name": "torch.distributions.transforms.IndependentTransform", "path": "distributions#torch.distributions.transforms.IndependentTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0) [source]\n \nWrapper around another transform to treat reinterpreted_batch_ndims-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out reinterpreted_batch_ndims-many of the rightmost dimensions in log_abs_det_jacobian().  Parameters \n \nbase_transform (Transform) \u2013 A base transform. \nreinterpreted_batch_ndims (int) \u2013 The number of extra rightmost dimensions to treat as dependent.    \n"}, {"name": "torch.distributions.transforms.LowerCholeskyTransform", "path": "distributions#torch.distributions.transforms.LowerCholeskyTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.LowerCholeskyTransform(cache_size=0) [source]\n \nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. \n"}, {"name": "torch.distributions.transforms.PowerTransform", "path": "distributions#torch.distributions.transforms.PowerTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.PowerTransform(exponent, cache_size=0) [source]\n \nTransform via the mapping y=xexponenty = x^{\\text{exponent}} . \n"}, {"name": "torch.distributions.transforms.ReshapeTransform", "path": "distributions#torch.distributions.transforms.ReshapeTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0) [source]\n \nUnit Jacobian transform to reshape the rightmost part of a tensor. Note that in_shape and out_shape must have the same number of elements, just as for torch.Tensor.reshape().  Parameters \n \nin_shape (torch.Size) \u2013 The input event shape. \nout_shape (torch.Size) \u2013 The output event shape.    \n"}, {"name": "torch.distributions.transforms.SigmoidTransform", "path": "distributions#torch.distributions.transforms.SigmoidTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.SigmoidTransform(cache_size=0) [source]\n \nTransform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}  and x=logit(y)x = \\text{logit}(y) . \n"}, {"name": "torch.distributions.transforms.SoftmaxTransform", "path": "distributions#torch.distributions.transforms.SoftmaxTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.SoftmaxTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)  then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. \n"}, {"name": "torch.distributions.transforms.StackTransform", "path": "distributions#torch.distributions.transforms.StackTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0) [source]\n \nTransform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().  Example::\n\nx = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)   \n"}, {"name": "torch.distributions.transforms.StickBreakingTransform", "path": "distributions#torch.distributions.transforms.StickBreakingTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.StickBreakingTransform(cache_size=0) [source]\n \nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. \n"}, {"name": "torch.distributions.transforms.TanhTransform", "path": "distributions#torch.distributions.transforms.TanhTransform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.TanhTransform(cache_size=0) [source]\n \nTransform via the mapping y=tanh\u2061(x)y = \\tanh(x) . It is equivalent to `\nComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])\n` However this might not be numerically stable, thus it is recommended to use TanhTransform instead. Note that one should use cache_size=1 when it comes to NaN/Inf values. \n"}, {"name": "torch.distributions.transforms.Transform", "path": "distributions#torch.distributions.transforms.Transform", "type": "torch.distributions", "text": " \nclass torch.distributions.transforms.Transform(cache_size=0) [source]\n \nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution. Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().  Parameters \ncache_size (int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.  Variables \n \n~Transform.domain (Constraint) \u2013 The constraint representing valid inputs to this transform. \n~Transform.codomain (Constraint) \u2013 The constraint representing valid outputs to this transform which are inputs to the inverse transform. \n~Transform.bijective (bool) \u2013 Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). \n~Transform.sign (int or Tensor) \u2013 For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.     \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n  \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n  \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n  \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n  \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n \n"}, {"name": "torch.distributions.transforms.Transform.forward_shape()", "path": "distributions#torch.distributions.transforms.Transform.forward_shape", "type": "torch.distributions", "text": " \nforward_shape(shape) [source]\n \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape. \n"}, {"name": "torch.distributions.transforms.Transform.inv()", "path": "distributions#torch.distributions.transforms.Transform.inv", "type": "torch.distributions", "text": " \nproperty inv  \nReturns the inverse Transform of this transform. This should satisfy t.inv.inv is t. \n"}, {"name": "torch.distributions.transforms.Transform.inverse_shape()", "path": "distributions#torch.distributions.transforms.Transform.inverse_shape", "type": "torch.distributions", "text": " \ninverse_shape(shape) [source]\n \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape. \n"}, {"name": "torch.distributions.transforms.Transform.log_abs_det_jacobian()", "path": "distributions#torch.distributions.transforms.Transform.log_abs_det_jacobian", "type": "torch.distributions", "text": " \nlog_abs_det_jacobian(x, y) [source]\n \nComputes the log det jacobian log |dy/dx| given input and output. \n"}, {"name": "torch.distributions.transforms.Transform.sign()", "path": "distributions#torch.distributions.transforms.Transform.sign", "type": "torch.distributions", "text": " \nproperty sign  \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. \n"}, {"name": "torch.distributions.uniform.Uniform", "path": "distributions#torch.distributions.uniform.Uniform", "type": "torch.distributions", "text": " \nclass torch.distributions.uniform.Uniform(low, high, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution Generates uniformly distributed random samples from the half-open interval [low, high). Example: >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n  Parameters \n \nlow (float or Tensor) \u2013 lower range (inclusive). \nhigh (float or Tensor) \u2013 upper range (exclusive).     \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n  \ncdf(value) [source]\n\n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nhas_rsample = True \n  \nicdf(value) [source]\n\n  \nlog_prob(value) [source]\n\n  \nproperty mean \n  \nrsample(sample_shape=torch.Size([])) [source]\n\n  \nproperty stddev \n  \nproperty support \n  \nproperty variance \n \n"}, {"name": "torch.distributions.uniform.Uniform.arg_constraints", "path": "distributions#torch.distributions.uniform.Uniform.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'high': Dependent(), 'low': Dependent()} \n"}, {"name": "torch.distributions.uniform.Uniform.cdf()", "path": "distributions#torch.distributions.uniform.Uniform.cdf", "type": "torch.distributions", "text": " \ncdf(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.entropy()", "path": "distributions#torch.distributions.uniform.Uniform.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.expand()", "path": "distributions#torch.distributions.uniform.Uniform.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.has_rsample", "path": "distributions#torch.distributions.uniform.Uniform.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = True \n"}, {"name": "torch.distributions.uniform.Uniform.icdf()", "path": "distributions#torch.distributions.uniform.Uniform.icdf", "type": "torch.distributions", "text": " \nicdf(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.log_prob()", "path": "distributions#torch.distributions.uniform.Uniform.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.mean()", "path": "distributions#torch.distributions.uniform.Uniform.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.uniform.Uniform.rsample()", "path": "distributions#torch.distributions.uniform.Uniform.rsample", "type": "torch.distributions", "text": " \nrsample(sample_shape=torch.Size([])) [source]\n\n"}, {"name": "torch.distributions.uniform.Uniform.stddev()", "path": "distributions#torch.distributions.uniform.Uniform.stddev", "type": "torch.distributions", "text": " \nproperty stddev \n"}, {"name": "torch.distributions.uniform.Uniform.support()", "path": "distributions#torch.distributions.uniform.Uniform.support", "type": "torch.distributions", "text": " \nproperty support \n"}, {"name": "torch.distributions.uniform.Uniform.variance()", "path": "distributions#torch.distributions.uniform.Uniform.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.distributions.von_mises.VonMises", "path": "distributions#torch.distributions.von_mises.VonMises", "type": "torch.distributions", "text": " \nclass torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None) [source]\n \nBases: torch.distributions.distribution.Distribution A circular von Mises distribution. This implementation uses polar coordinates. The loc and value args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.  Example::\n\n>>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n    Parameters \n \nloc (torch.Tensor) \u2013 an angle in radians. \nconcentration (torch.Tensor) \u2013 concentration parameter     \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n  \nexpand(batch_shape) [source]\n\n  \nhas_rsample = False \n  \nlog_prob(value) [source]\n\n  \nproperty mean  \nThe provided mean is the circular one. \n  \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n  \nsupport = Real() \n  \nvariance [source]\n \nThe provided variance is the circular one. \n \n"}, {"name": "torch.distributions.von_mises.VonMises.arg_constraints", "path": "distributions#torch.distributions.von_mises.VonMises.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()} \n"}, {"name": "torch.distributions.von_mises.VonMises.expand()", "path": "distributions#torch.distributions.von_mises.VonMises.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape) [source]\n\n"}, {"name": "torch.distributions.von_mises.VonMises.has_rsample", "path": "distributions#torch.distributions.von_mises.VonMises.has_rsample", "type": "torch.distributions", "text": " \nhas_rsample = False \n"}, {"name": "torch.distributions.von_mises.VonMises.log_prob()", "path": "distributions#torch.distributions.von_mises.VonMises.log_prob", "type": "torch.distributions", "text": " \nlog_prob(value) [source]\n\n"}, {"name": "torch.distributions.von_mises.VonMises.mean()", "path": "distributions#torch.distributions.von_mises.VonMises.mean", "type": "torch.distributions", "text": " \nproperty mean  \nThe provided mean is the circular one. \n"}, {"name": "torch.distributions.von_mises.VonMises.sample()", "path": "distributions#torch.distributions.von_mises.VonMises.sample", "type": "torch.distributions", "text": " \nsample(sample_shape=torch.Size([])) [source]\n \nThe sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. \u201cEfficient simulation of the von Mises distribution.\u201d Applied Statistics (1979): 152-157. \n"}, {"name": "torch.distributions.von_mises.VonMises.support", "path": "distributions#torch.distributions.von_mises.VonMises.support", "type": "torch.distributions", "text": " \nsupport = Real() \n"}, {"name": "torch.distributions.von_mises.VonMises.variance", "path": "distributions#torch.distributions.von_mises.VonMises.variance", "type": "torch.distributions", "text": " \nvariance [source]\n \nThe provided variance is the circular one. \n"}, {"name": "torch.distributions.weibull.Weibull", "path": "distributions#torch.distributions.weibull.Weibull", "type": "torch.distributions", "text": " \nclass torch.distributions.weibull.Weibull(scale, concentration, validate_args=None) [source]\n \nBases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a two-parameter Weibull distribution. Example >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n  Parameters \n \nscale (float or Tensor) \u2013 Scale parameter of distribution (lambda). \nconcentration (float or Tensor) \u2013 Concentration parameter of distribution (k/shape).     \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n  \nentropy() [source]\n\n  \nexpand(batch_shape, _instance=None) [source]\n\n  \nproperty mean \n  \nsupport = GreaterThan(lower_bound=0.0) \n  \nproperty variance \n \n"}, {"name": "torch.distributions.weibull.Weibull.arg_constraints", "path": "distributions#torch.distributions.weibull.Weibull.arg_constraints", "type": "torch.distributions", "text": " \narg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)} \n"}, {"name": "torch.distributions.weibull.Weibull.entropy()", "path": "distributions#torch.distributions.weibull.Weibull.entropy", "type": "torch.distributions", "text": " \nentropy() [source]\n\n"}, {"name": "torch.distributions.weibull.Weibull.expand()", "path": "distributions#torch.distributions.weibull.Weibull.expand", "type": "torch.distributions", "text": " \nexpand(batch_shape, _instance=None) [source]\n\n"}, {"name": "torch.distributions.weibull.Weibull.mean()", "path": "distributions#torch.distributions.weibull.Weibull.mean", "type": "torch.distributions", "text": " \nproperty mean \n"}, {"name": "torch.distributions.weibull.Weibull.support", "path": "distributions#torch.distributions.weibull.Weibull.support", "type": "torch.distributions", "text": " \nsupport = GreaterThan(lower_bound=0.0) \n"}, {"name": "torch.distributions.weibull.Weibull.variance()", "path": "distributions#torch.distributions.weibull.Weibull.variance", "type": "torch.distributions", "text": " \nproperty variance \n"}, {"name": "torch.div()", "path": "generated/torch.div#torch.div", "type": "torch", "text": " \ntorch.div(input, other, *, rounding_mode=None, out=None) \u2192 Tensor  \nDivides each element of the input input by the corresponding element of other.  outi=inputiotheri\\text{out}_i = \\frac{\\text{input}_i}{\\text{other}_i}  \n Note By default, this performs a \u201ctrue\u201d division like Python 3. See the rounding_mode argument for floor division.  Supports broadcasting to a common shape, type promotion, and integer, float, and complex inputs. Always promotes integer types to the default scalar type.  Parameters \n \ninput (Tensor) \u2013 the dividend \nother (Tensor or Number) \u2013 the divisor   Keyword Arguments \n \nrounding_mode (str, optional) \u2013 \nType of rounding applied to the result:  None - default behavior. Performs no rounding and, if both input and other are integer types, promotes the inputs to the default scalar type. Equivalent to true division in Python (the / operator) and NumPy\u2019s np.true_divide. \n\"trunc\" - rounds the results of the division towards zero. Equivalent to C-style integer division. \n\"floor\" - rounds the results of the division down. Equivalent to floor division in Python (the // operator) and NumPy\u2019s np.floor_divide.   \nout (Tensor, optional) \u2013 the output tensor.    Examples: >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])\n \n"}, {"name": "torch.divide()", "path": "generated/torch.divide#torch.divide", "type": "torch", "text": " \ntorch.divide(input, other, *, rounding_mode=None, out=None) \u2192 Tensor  \nAlias for torch.div(). \n"}, {"name": "torch.dot()", "path": "generated/torch.dot#torch.dot", "type": "torch", "text": " \ntorch.dot(input, other, *, out=None) \u2192 Tensor  \nComputes the dot product of two 1D tensors.  Note Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.   Parameters \n \ninput (Tensor) \u2013 first tensor in the dot product, must be 1D. \nother (Tensor) \u2013 second tensor in the dot product, must be 1D.   Keyword Arguments \n{out} \u2013    Example: >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n \n"}, {"name": "torch.dstack()", "path": "generated/torch.dstack#torch.dstack", "type": "torch", "text": " \ntorch.dstack(tensors, *, out=None) \u2192 Tensor  \nStack tensors in sequence depthwise (along third axis). This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d().  Parameters \ntensors (sequence of Tensors) \u2013 sequence of tensors to concatenate  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.    Example::\n\n>>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])\n   \n"}, {"name": "torch.eig()", "path": "generated/torch.eig#torch.eig", "type": "torch", "text": " \ntorch.eig(input, eigenvectors=False, *, out=None) -> (Tensor, Tensor)  \nComputes the eigenvalues and eigenvectors of a real square matrix.  Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only if eigenvalues and eigenvectors are all real valued. When input is on CUDA, torch.eig() causes host-device synchronization.   Parameters \n \ninput (Tensor) \u2013 the square matrix of shape (n\u00d7n)(n \\times n)  for which the eigenvalues and eigenvectors will be computed \neigenvectors (bool) \u2013 True to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed   Keyword Arguments \nout (tuple, optional) \u2013 the output tensors  Returns \nA namedtuple (eigenvalues, eigenvectors) containing  \neigenvalues (Tensor): Shape (n\u00d72)(n \\times 2) . Each row is an eigenvalue of input, where the first element is the real part and the second element is the imaginary part. The eigenvalues are not necessarily ordered. \neigenvectors (Tensor): If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor of shape (n\u00d7n)(n \\times n)  can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding eigenvalues[j] is a real number, column eigenvectors[:, j] is the eigenvector corresponding to eigenvalues[j]. If the corresponding eigenvalues[j] and eigenvalues[j + 1] form a complex conjugate pair, then the true eigenvectors can be computed as true eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1] , true eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1] .   Return type \n(Tensor, Tensor)   Example: Trivial example with a diagonal matrix. By default, only eigenvalues are computed:\n\n>>> a = torch.diag(torch.tensor([1, 2, 3], dtype=torch.double))\n>>> e, v = torch.eig(a)\n>>> e\ntensor([[1., 0.],\n        [2., 0.],\n        [3., 0.]], dtype=torch.float64)\n>>> v\ntensor([], dtype=torch.float64)\n\nCompute also the eigenvectors:\n\n>>> e, v = torch.eig(a, eigenvectors=True)\n>>> e\ntensor([[1., 0.],\n        [2., 0.],\n        [3., 0.]], dtype=torch.float64)\n>>> v\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]], dtype=torch.float64)\n \n"}, {"name": "torch.einsum()", "path": "generated/torch.einsum#torch.einsum", "type": "torch", "text": " \ntorch.einsum(equation, *operands) \u2192 Tensor [source]\n \nSums the product of the elements of the input operands along dimensions specified using a notation based on the Einstein summation convention. Einsum allows computing many common multi-dimensional linear algebraic array operations by representing them in a short-hand format based on the Einstein summation convention, given by equation. The details of this format are described below, but the general idea is to label every dimension of the input operands with some subscript and define which subscripts are part of the output. The output is then computed by summing the product of the elements of the operands along the dimensions whose subscripts are not part of the output. For example, matrix multiplication can be computed using einsum as torch.einsum(\u201cij,jk->ik\u201d, A, B). Here, j is the summation subscript and i and k the output subscripts (see section below for more details on why). Equation: The equation string specifies the subscripts (lower case letters [\u2018a\u2019, \u2018z\u2019]) for each dimension of the input operands in the same order as the dimensions, separating subcripts for each operand by a comma (\u2018,\u2019), e.g. \u2018ij,jk\u2019 specify subscripts for two 2D operands. The dimensions labeled with the same subscript must be broadcastable, that is, their size must either match or be 1. The exception is if a subscript is repeated for the same input operand, in which case the dimensions labeled with this subscript for this operand must match in size and the operand will be replaced by its diagonal along these dimensions. The subscripts that appear exactly once in the equation will be part of the output, sorted in increasing alphabetical order. The output is computed by multiplying the input operands element-wise, with their dimensions aligned based on the subscripts, and then summing out the dimensions whose subscripts are not part of the output. Optionally, the output subscripts can be explicitly defined by adding an arrow (\u2018->\u2019) at the end of the equation followed by the subscripts for the output. For instance, the following equation computes the transpose of a matrix multiplication: \u2018ij,jk->ki\u2019. The output subscripts must appear at least once for some input operand and at most once for the output. Ellipsis (\u2018\u2026\u2019) can be used in place of subscripts to broadcast the dimensions covered by the ellipsis. Each input operand may contain at most one ellipsis which will cover the dimensions not covered by subscripts, e.g. for an input operand with 5 dimensions, the ellipsis in the equation \u2018ab\u2026c\u2019 cover the third and fourth dimensions. The ellipsis does not need to cover the same number of dimensions across the operands but the \u2018shape\u2019 of the ellipsis (the size of the dimensions covered by them) must broadcast together. If the output is not explicitly defined with the arrow (\u2018->\u2019) notation, the ellipsis will come first in the output (left-most dimensions), before the subscript labels that appear exactly once for the input operands. e.g. the following equation implements batch matrix multiplication \u2018\u2026ij,\u2026jk\u2019. A few final notes: the equation may contain whitespaces between the different elements (subscripts, ellipsis, arrow and comma) but something like \u2018\u2026\u2019 is not valid. An empty string \u2018\u2019 is valid for scalar operands.  Note torch.einsum handles ellipsis (\u2018\u2026\u2019) differently from NumPy in that it allows dimensions covered by the ellipsis to be summed over, that is, ellipsis are not required to be part of the output.   Note This function does not optimize the given expression, so a different formula for the same computation may run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/) can optimize the formula for you.   Parameters \n \nequation (string) \u2013 The subscripts for the Einstein summation. \noperands (Tensor) \u2013 The operands to compute the Einstein sum of.    Examples: # trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])\n \n"}, {"name": "torch.empty()", "path": "generated/torch.empty#torch.empty", "type": "torch", "text": " \ntorch.empty(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nReturns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.  Parameters \nsize (int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.  Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.contiguous_format.    Example: >>> torch.empty(2, 3)\ntensor(1.00000e-08 *\n       [[ 6.3984,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n \n"}, {"name": "torch.empty_like()", "path": "generated/torch.empty_like#torch.empty_like", "type": "torch", "text": " \ntorch.empty_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \ninput (Tensor) \u2013 the size of input will determine size of the output tensor.  Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    Example: >>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n \n"}, {"name": "torch.empty_strided()", "path": "generated/torch.empty_strided#torch.empty_strided", "type": "torch", "text": " \ntorch.empty_strided(size, stride, *, dtype=None, layout=None, device=None, requires_grad=False, pin_memory=False) \u2192 Tensor  \nReturns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument size and stride respectively. torch.empty_strided(size, stride) is equivalent to torch.empty(size).as_strided(size, stride).  Warning More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.   Parameters \n \nsize (tuple of python:ints) \u2013 the shape of the output tensor \nstride (tuple of python:ints) \u2013 the strides of the output tensor   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \npin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: >>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])\n \n"}, {"name": "torch.enable_grad", "path": "generated/torch.enable_grad#torch.enable_grad", "type": "torch", "text": " \nclass torch.enable_grad [source]\n \nContext-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Example: >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n \n"}, {"name": "torch.eq()", "path": "generated/torch.eq#torch.eq", "type": "torch", "text": " \ntorch.eq(input, other, *, out=None) \u2192 Tensor  \nComputes element-wise equality The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters \n \ninput (Tensor) \u2013 the tensor to compare \nother (Tensor or float) \u2013 the tensor or value to compare   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.  Returns \nA boolean tensor that is True where input is equal to other and False elsewhere   Example: >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])\n \n"}, {"name": "torch.equal()", "path": "generated/torch.equal#torch.equal", "type": "torch", "text": " \ntorch.equal(input, other) \u2192 bool  \nTrue if two tensors have the same size and elements, False otherwise. Example: >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n \n"}, {"name": "torch.erf()", "path": "generated/torch.erf#torch.erf", "type": "torch", "text": " \ntorch.erf(input, *, out=None) \u2192 Tensor  \nComputes the error function of each element. The error function is defined as follows:  erf(x)=2\u03c0\u222b0xe\u2212t2dt\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n \n"}, {"name": "torch.erfc()", "path": "generated/torch.erfc#torch.erfc", "type": "torch", "text": " \ntorch.erfc(input, *, out=None) \u2192 Tensor  \nComputes the complementary error function of each element of input. The complementary error function is defined as follows:  erfc(x)=1\u22122\u03c0\u222b0xe\u2212t2dt\\mathrm{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n \n"}, {"name": "torch.erfinv()", "path": "generated/torch.erfinv#torch.erfinv", "type": "torch", "text": " \ntorch.erfinv(input, *, out=None) \u2192 Tensor  \nComputes the inverse error function of each element of input. The inverse error function is defined in the range (\u22121,1)(-1, 1)  as:  erfinv(erf(x))=x\\mathrm{erfinv}(\\mathrm{erf}(x)) = x  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n \n"}, {"name": "torch.exp()", "path": "generated/torch.exp#torch.exp", "type": "torch", "text": " \ntorch.exp(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the exponential of the elements of the input tensor input.  yi=exiy_{i} = e^{x_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n \n"}, {"name": "torch.exp2()", "path": "generated/torch.exp2#torch.exp2", "type": "torch", "text": " \ntorch.exp2(input, *, out=None) \u2192 Tensor  \nComputes the base two exponential function of input.  yi=2xiy_{i} = 2^{x_{i}}  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n \n"}, {"name": "torch.expm1()", "path": "generated/torch.expm1#torch.expm1", "type": "torch", "text": " \ntorch.expm1(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the exponential of the elements minus 1 of input.  yi=exi\u22121y_{i} = e^{x_{i}} - 1  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n \n"}, {"name": "torch.eye()", "path": "generated/torch.eye#torch.eye", "type": "torch", "text": " \ntorch.eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.  Parameters \n \nn (int) \u2013 the number of rows \nm (int, optional) \u2013 the number of columns with default being n\n   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns \nA 2-D tensor with ones on the diagonal and zeros elsewhere  Return type \nTensor   Example: >>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])\n \n"}, {"name": "torch.fake_quantize_per_channel_affine()", "path": "generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine", "type": "torch", "text": " \ntorch.fake_quantize_per_channel_affine(input, scale, zero_point, quant_min, quant_max) \u2192 Tensor  \nReturns a new tensor with the data in input fake quantized per channel using scale, zero_point, quant_min and quant_max, across the channel specified by axis.  output=min(quant_max,max(quant_min,std::nearby_int(input/scale)+zero_point))\\text{output} = min( \\text{quant\\_max}, max( \\text{quant\\_min}, \\text{std::nearby\\_int}(\\text{input} / \\text{scale}) + \\text{zero\\_point} ) )  \n Parameters \n \ninput (Tensor) \u2013 the input value(s), in torch.float32. \nscale (Tensor) \u2013 quantization scale, per channel \nzero_point (Tensor) \u2013 quantization zero_point, per channel \naxis (int32) \u2013 channel axis \nquant_min (int64) \u2013 lower bound of the quantized domain \nquant_max (int64) \u2013 upper bound of the quantized domain   Returns \nA newly fake_quantized per channel tensor  Return type \nTensor   Example: >>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])\n \n"}, {"name": "torch.fake_quantize_per_tensor_affine()", "path": "generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine", "type": "torch", "text": " \ntorch.fake_quantize_per_tensor_affine(input, scale, zero_point, quant_min, quant_max) \u2192 Tensor  \nReturns a new tensor with the data in input fake quantized using scale, zero_point, quant_min and quant_max.  output=min(quant_max,max(quant_min,std::nearby_int(input/scale)+zero_point))\\text{output} = min( \\text{quant\\_max}, max( \\text{quant\\_min}, \\text{std::nearby\\_int}(\\text{input} / \\text{scale}) + \\text{zero\\_point} ) )  \n Parameters \n \ninput (Tensor) \u2013 the input value(s), in torch.float32. \nscale (double) \u2013 quantization scale \nzero_point (int64) \u2013 quantization zero_point \nquant_min (int64) \u2013 lower bound of the quantized domain \nquant_max (int64) \u2013 upper bound of the quantized domain   Returns \nA newly fake_quantized tensor  Return type \nTensor   Example: >>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])\n \n"}, {"name": "torch.fft", "path": "fft", "type": "torch.fft", "text": "torch.fft Discrete Fourier transforms and related functions. Fast Fourier Transforms  \ntorch.fft.fft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n >>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\n>>> torch.fft.fft(t)\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\n \n  \ntorch.fft.ifft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n>>> torch.fft.ifft(t)\ntensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])\n \n  \ntorch.fft.fft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fft2 = torch.fft.fft2(t)\n The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fft2, two_ffts)\n \n  \ntorch.fft.ifft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifft2 = torch.fft.ifft2(t)\n The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifft2, two_iffts)\n \n  \ntorch.fft.fftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fftn = torch.fft.fftn(t)\n The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fftn, two_ffts)\n \n  \ntorch.fft.ifftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifftn = torch.fft.ifftn(t)\n The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifftn, two_iffts)\n \n  \ntorch.fft.rfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional Fourier transform of real-valued input. The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the real FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.rfft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j])\n Compare against the full output from fft(): >>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued. \n  \ntorch.fft.irfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of rfft(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the output signal. If given, the input will either be zero-padded or trimmed to this length before computing the real IFFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.rfft(t)\n>>> T\ntensor([10.0000+0.0000j, -2.5000+3.4410j, -2.5000+0.8123j])\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length: >>> torch.fft.irfft(T)\ntensor([0.6250, 1.4045, 3.1250, 4.8455])\n So, it is recommended to always pass the signal length n: >>> torch.fft.irfft(T, t.numel())\ntensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])\n \n  \ntorch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default. The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfft2 = torch.fft.rfft2(t)\n>>> rfft2.size()\ntorch.Size([10, 6])\n Compared against the full output from fft2(), we have all elements up to the Nyquist frequency. >>> fft2 = torch.fft.fft2(t)\n>>> torch.allclose(fft2[..., :6], rfft2)\nTrue\n The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfft2, two_ffts)\n \n  \ntorch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default. input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfft2(t)\n Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfft2(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfft2(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n  \ntorch.fft.rfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N-dimensional discrete Fourier transform of real input. The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfftn = torch.fft.rfftn(t)\n>>> rfftn.size()\ntorch.Size([10, 6])\n Compared against the full output from fftn(), we have all elements up to the Nyquist frequency. >>> fftn = torch.fft.fftn(t)\n>>> torch.allclose(fftn[..., :6], rfftn)\nTrue\n The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfftn, two_ffts)\n \n  \ntorch.fft.irfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the inverse of rfftn(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfftn(t)\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfftn(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfftn(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n  \ntorch.fft.hfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.  Note hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().   Note Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the real output. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian FFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (hfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)  Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (no normalization).     Example Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output: >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.ifft(t)\n>>> T\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n        -0.5000+0.6882j])\n Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies: >>> torch.fft.hfft(T[:3], n=5)\ntensor([0., 1., 2., 3., 4.])\n Like with irfft(), the output length must be given in order to recover an even length output: >>> torch.fft.hfft(T[:3])\ntensor([0.5000, 1.1236, 2.5000, 3.8764])\n \n  \ntorch.fft.ihfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of hfft(). input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ihfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> torch.fft.ihfft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j])\n Compare against the full output from ifft(): >>> torch.fft.ifft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n    -0.5000+0.6882j])\n \n Helper Functions  \ntorch.fft.fftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the discrete Fourier Transform sample frequencies for a signal of size n.  Note By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are: f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.   Parameters \n \nn (int) \u2013 the FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.fftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n For even input, we can see the Nyquist frequency at f[2] is given as negative: >>> torch.fft.fftfreq(4)\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n \n  \ntorch.fft.rfftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the sample frequencies for rfft() with a signal of size n.  Note rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are: f = torch.arange((n + 1) // 2) / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.   Parameters \n \nn (int) \u2013 the real FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.rfftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000])\n >>> torch.fft.rfftfreq(4)\ntensor([ 0.0000,  0.2500, 0.5000])\n Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500]) \n  \ntorch.fft.fftshift(input, dim=None) \u2192 Tensor  \nReorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first. This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.  Note By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().   Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(4)\n>>> f\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n >>> torch.fft.fftshift(f)\ntensor([-0.5000, -0.2500,  0.0000,  0.2500])\n Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor. This also works for multi-dimensional transforms: >>> x = torch.fft.fftfreq(5, d=1/5) + 0.1 * torch.fft.fftfreq(5, d=1/5).unsqueeze(1)\n>>> x\ntensor([[ 0.0000,  1.0000,  2.0000, -2.0000, -1.0000],\n        [ 0.1000,  1.1000,  2.1000, -1.9000, -0.9000],\n        [ 0.2000,  1.2000,  2.2000, -1.8000, -0.8000],\n        [-0.2000,  0.8000,  1.8000, -2.2000, -1.2000],\n        [-0.1000,  0.9000,  1.9000, -2.1000, -1.1000]])\n >>> torch.fft.fftshift(x)\ntensor([[-2.2000, -1.2000, -0.2000,  0.8000,  1.8000],\n        [-2.1000, -1.1000, -0.1000,  0.9000,  1.9000],\n        [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n        [-1.9000, -0.9000,  0.1000,  1.1000,  2.1000],\n        [-1.8000, -0.8000,  0.2000,  1.2000,  2.2000]])\n fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift(). >>> x_centered = torch.arange(-5, 5)\n>>> x_uncentered = torch.fft.ifftshift(x_centered)\n>>> fft_uncentered = torch.fft.fft(x_uncentered)\n Similarly, we can convert the frequency domain components to centered convention by applying fftshift(). >>> fft_centered = torch.fft.fftshift(fft_uncentered)\n The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order: >>> x_centered_2 = torch.fft.fftshift(torch.fft.ifft(torch.fft.ifftshift(fft_centered)))\n>>> torch.allclose(x_centered.to(torch.complex64), x_centered_2)\nTrue\n \n  \ntorch.fft.ifftshift(input, dim=None) \u2192 Tensor  \nInverse of fftshift().  Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(5)\n>>> f\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n A round-trip through fftshift() and ifftshift() gives the same result: >>> shifted = torch.fftshift(f)\n>>> torch.ifftshift(shifted)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n \n\n"}, {"name": "torch.fft.fft()", "path": "fft#torch.fft.fft", "type": "torch.fft", "text": " \ntorch.fft.fft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i] = conj(X[-i]). This function always returns both the positive and negative frequency terms even though, for real inputs, the negative frequencies are redundant. rfft() returns the more compact one-sided representation where only the positive frequencies are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (ifft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n >>> t = tensor([0.+1.j, 2.+3.j, 4.+5.j, 6.+7.j])\n>>> torch.fft.fft(t)\ntensor([12.+16.j, -8.+0.j, -4.-4.j,  0.-8.j])\n \n"}, {"name": "torch.fft.fft2()", "path": "fft#torch.fft.fft2", "type": "torch.fft", "text": " \ntorch.fft.fft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional discrete Fourier transform of input. Equivalent to fftn() but FFTs only the last two dimensions by default.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i, j] = conj(X[-i, -j]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfft2() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fft2 = torch.fft.fft2(t)\n The discrete Fourier transform is separable, so fft2() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fft2, two_ffts)\n \n"}, {"name": "torch.fft.fftfreq()", "path": "fft#torch.fft.fftfreq", "type": "torch.fft", "text": " \ntorch.fft.fftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the discrete Fourier Transform sample frequencies for a signal of size n.  Note By convention, fft() returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. For an FFT of length n and with inputs spaced in length unit d, the frequencies are: f = [0, 1, ..., (n - 1) // 2, -(n // 2), ..., -1] / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftfreq() follows NumPy\u2019s convention of taking it to be negative.   Parameters \n \nn (int) \u2013 the FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.fftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n For even input, we can see the Nyquist frequency at f[2] is given as negative: >>> torch.fft.fftfreq(4)\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n \n"}, {"name": "torch.fft.fftn()", "path": "fft#torch.fft.fftn", "type": "torch.fft", "text": " \ntorch.fft.fftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional discrete Fourier transform of input.  Note The Fourier domain representation of any real signal satisfies the Hermitian property: X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]). This function always returns all positive and negative frequency terms even though, for real inputs, half of these values are redundant. rfftn() returns the more compact one-sided representation where only the positive frequencies of the last dimension are returned.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (fftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (ifftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> fftn = torch.fft.fftn(t)\n The discrete Fourier transform is separable, so fftn() here is equivalent to two one-dimensional fft() calls: >>> two_ffts = torch.fft.fft(torch.fft.fft(x, dim=0), dim=1)\n>>> torch.allclose(fftn, two_ffts)\n \n"}, {"name": "torch.fft.fftshift()", "path": "fft#torch.fft.fftshift", "type": "torch.fft", "text": " \ntorch.fft.fftshift(input, dim=None) \u2192 Tensor  \nReorders n-dimensional FFT data, as provided by fftn(), to have negative frequency terms first. This performs a periodic shift of n-dimensional data such that the origin (0, ..., 0) is moved to the center of the tensor. Specifically, to input.shape[dim] // 2 in each selected dimension.  Note By convention, the FFT returns positive frequency terms first, followed by the negative frequencies in reverse order, so that f[-i] for all 0<i\u2264n/20 < i \\leq n/2  in Python gives the negative frequency terms. fftshift() rearranges all frequencies into ascending order from negative to positive with the zero-frequency term in the center.   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. fftshift() always puts the Nyquist term at the 0-index. This is the same convention used by fftfreq().   Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(4)\n>>> f\ntensor([ 0.0000,  0.2500, -0.5000, -0.2500])\n >>> torch.fft.fftshift(f)\ntensor([-0.5000, -0.2500,  0.0000,  0.2500])\n Also notice that the Nyquist frequency term at f[2] was moved to the beginning of the tensor. This also works for multi-dimensional transforms: >>> x = torch.fft.fftfreq(5, d=1/5) + 0.1 * torch.fft.fftfreq(5, d=1/5).unsqueeze(1)\n>>> x\ntensor([[ 0.0000,  1.0000,  2.0000, -2.0000, -1.0000],\n        [ 0.1000,  1.1000,  2.1000, -1.9000, -0.9000],\n        [ 0.2000,  1.2000,  2.2000, -1.8000, -0.8000],\n        [-0.2000,  0.8000,  1.8000, -2.2000, -1.2000],\n        [-0.1000,  0.9000,  1.9000, -2.1000, -1.1000]])\n >>> torch.fft.fftshift(x)\ntensor([[-2.2000, -1.2000, -0.2000,  0.8000,  1.8000],\n        [-2.1000, -1.1000, -0.1000,  0.9000,  1.9000],\n        [-2.0000, -1.0000,  0.0000,  1.0000,  2.0000],\n        [-1.9000, -0.9000,  0.1000,  1.1000,  2.1000],\n        [-1.8000, -0.8000,  0.2000,  1.2000,  2.2000]])\n fftshift() can also be useful for spatial data. If our data is defined on a centered grid ([-(N//2), (N-1)//2]) then we can use the standard FFT defined on an uncentered grid ([0, N)) by first applying an ifftshift(). >>> x_centered = torch.arange(-5, 5)\n>>> x_uncentered = torch.fft.ifftshift(x_centered)\n>>> fft_uncentered = torch.fft.fft(x_uncentered)\n Similarly, we can convert the frequency domain components to centered convention by applying fftshift(). >>> fft_centered = torch.fft.fftshift(fft_uncentered)\n The inverse transform, from centered Fourier space back to centered spatial data, can be performed by applying the inverse shifts in reverse order: >>> x_centered_2 = torch.fft.fftshift(torch.fft.ifft(torch.fft.ifftshift(fft_centered)))\n>>> torch.allclose(x_centered.to(torch.complex64), x_centered_2)\nTrue\n \n"}, {"name": "torch.fft.hfft()", "path": "fft#torch.fft.hfft", "type": "torch.fft", "text": " \ntorch.fft.hfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric input signal.  Note hfft()/ihfft() are analogous to rfft()/irfft(). The real FFT expects a real signal in the time-domain and gives a Hermitian symmetry in the frequency-domain. The Hermitian FFT is the opposite; Hermitian symmetric in the time-domain and real-valued in the frequency-domain. For this reason, special care needs to be taken with the length argument n, in the same way as with irfft().   Note Because the signal is Hermitian in the time-domain, the result will be real in the frequency domain. Note that some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in input[0] would result in one or more complex frequency terms which cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the real output. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian FFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (hfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the Hermitian FFT orthonormal)  Calling the backward transform (ihfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (no normalization).     Example Taking a real-valued frequency signal and bringing it into the time domain gives Hermitian symmetric output: >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.ifft(t)\n>>> T\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n        -0.5000+0.6882j])\n Note that T[1] == T[-1].conj() and T[2] == T[-2].conj() is redundant. We can thus compute the forward transform without considering negative frequencies: >>> torch.fft.hfft(T[:3], n=5)\ntensor([0., 1., 2., 3., 4.])\n Like with irfft(), the output length must be given in order to recover an even length output: >>> torch.fft.hfft(T[:3])\ntensor([0.5000, 1.1236, 2.5000, 3.8764])\n \n"}, {"name": "torch.fft.ifft()", "path": "fft#torch.fft.ifft", "type": "torch.fft", "text": " \ntorch.fft.ifft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (fft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.tensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n>>> torch.fft.ifft(t)\ntensor([0.+0.j, 1.+0.j, 2.+0.j, 3.+0.j])\n \n"}, {"name": "torch.fft.ifft2()", "path": "fft#torch.fft.ifft2", "type": "torch.fft", "text": " \ntorch.fft.ifft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2 dimensional inverse discrete Fourier transform of input. Equivalent to ifftn() but IFFTs only the last two dimensions by default.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifft2 = torch.fft.ifft2(t)\n The discrete Fourier transform is separable, so ifft2() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifft2, two_iffts)\n \n"}, {"name": "torch.fft.ifftn()", "path": "fft#torch.fft.ifftn", "type": "torch.fft", "text": " \ntorch.fft.ifftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N dimensional inverse discrete Fourier transform of input.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the IFFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ifftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (fftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ifftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> x = torch.rand(10, 10, dtype=torch.complex64)\n>>> ifftn = torch.fft.ifftn(t)\n The discrete Fourier transform is separable, so ifftn() here is equivalent to two one-dimensional ifft() calls: >>> two_iffts = torch.fft.ifft(torch.fft.ifft(x, dim=0), dim=1)\n>>> torch.allclose(ifftn, two_iffts)\n \n"}, {"name": "torch.fft.ifftshift()", "path": "fft#torch.fft.ifftshift", "type": "torch.fft", "text": " \ntorch.fft.ifftshift(input, dim=None) \u2192 Tensor  \nInverse of fftshift().  Parameters \n \ninput (Tensor) \u2013 the tensor in FFT order \ndim (int, Tuple[int], optional) \u2013 The dimensions to rearrange. Only dimensions specified here will be rearranged, any other dimensions will be left in their original order. Default: All dimensions of input.    Example >>> f = torch.fft.fftfreq(5)\n>>> f\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n A round-trip through fftshift() and ifftshift() gives the same result: >>> shifted = torch.fftshift(f)\n>>> torch.ifftshift(shifted)\ntensor([ 0.0000,  0.2000,  0.4000, -0.4000, -0.2000])\n \n"}, {"name": "torch.fft.ihfft()", "path": "fft#torch.fft.ihfft", "type": "torch.fft", "text": " \ntorch.fft.ihfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of hfft(). input must be a real-valued signal, interpreted in the Fourier domain. The IFFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]). ihfft() represents this in the one-sided form where only the positive frequencies below the Nyquist frequency are included. To compute the full output, use ifft().  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the Hermitian IFFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional Hermitian IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (ihfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the IFFT orthonormal)  Calling the forward transform (hfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make ihfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> torch.fft.ihfft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j])\n Compare against the full output from ifft(): >>> torch.fft.ifft(t)\ntensor([ 2.0000+-0.0000j, -0.5000-0.6882j, -0.5000-0.1625j, -0.5000+0.1625j,\n    -0.5000+0.6882j])\n \n"}, {"name": "torch.fft.irfft()", "path": "fft#torch.fft.irfft", "type": "torch.fft", "text": " \ntorch.fft.irfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the inverse of rfft(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by n. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal length n.   Parameters \n \ninput (Tensor) \u2013 the input tensor representing a half-Hermitian signal \nn (int, optional) \u2013 Output signal length. This determines the length of the output signal. If given, the input will either be zero-padded or trimmed to this length before computing the real IFFT. Defaults to even output: n=2*(input.size(dim) - 1). \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real IFFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Calling the forward transform (rfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.arange(5)\n>>> t\ntensor([0, 1, 2, 3, 4])\n>>> T = torch.fft.rfft(t)\n>>> T\ntensor([10.0000+0.0000j, -2.5000+3.4410j, -2.5000+0.8123j])\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length: >>> torch.fft.irfft(T)\ntensor([0.6250, 1.4045, 3.1250, 4.8455])\n So, it is recommended to always pass the signal length n: >>> torch.fft.irfft(T, t.numel())\ntensor([0.0000, 1.0000, 2.0000, 3.0000, 4.0000])\n \n"}, {"name": "torch.fft.irfft2()", "path": "fft#torch.fft.irfft2", "type": "torch.fft", "text": " \ntorch.fft.irfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the inverse of rfft2(). Equivalent to irfftn() but IFFTs only the last two dimensions by default. input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfft2(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfft2()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfft2(t)\n Without specifying the output length to irfft2(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfft2(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfft2(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n"}, {"name": "torch.fft.irfftn()", "path": "fft#torch.fft.irfftn", "type": "torch.fft", "text": " \ntorch.fft.irfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the inverse of rfftn(). input is interpreted as a one-sided Hermitian signal in the Fourier domain, as produced by rfftn(). By the Hermitian property, the output will be real-valued.  Note Some input frequencies must be real-valued to satisfy the Hermitian property. In these cases the imaginary component will be ignored. For example, any imaginary component in the zero-frequency term cannot be represented in a real output and so will always be ignored.   Note The correct interpretation of the Hermitian input depends on the length of the original data, as given by s. This is because each input shape could correspond to either an odd or even length signal. By default, the signal is assumed to be even length and odd signals will not round-trip properly. So, it is recommended to always pass the signal shape s.   Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Defaults to even output in the last dimension: s[-1] = 2*(input.size(dim[-1]) - 1). \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. The last dimension must be the half-Hermitian compressed dimension. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the backward transform (irfftn()), these correspond to:  \n\"forward\" - no normalization \n\"backward\" - normalize by 1/n\n \n\"ortho\" - normalize by 1/sqrt(n) (making the real IFFT orthonormal)  Where n = prod(s) is the logical IFFT size. Calling the forward transform (rfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (normalize by 1/n).     Example >>> t = torch.rand(10, 9)\n>>> T = torch.fft.rfftn(t)\n Without specifying the output length to irfft(), the output will not round-trip properly because the input is odd-length in the last dimension: >>> torch.fft.irfftn(T).size()\ntorch.Size([10, 10])\n So, it is recommended to always pass the signal shape s. >>> roundtrip = torch.fft.irfftn(T, t.size())\n>>> roundtrip.size()\ntorch.Size([10, 9])\n>>> torch.allclose(roundtrip, t)\nTrue\n \n"}, {"name": "torch.fft.rfft()", "path": "fft#torch.fft.rfft", "type": "torch.fft", "text": " \ntorch.fft.rfft(input, n=None, dim=-1, norm=None) \u2192 Tensor  \nComputes the one dimensional Fourier transform of real-valued input. The FFT of a real signal is Hermitian-symmetric, X[i] = conj(X[-i]) so the output contains only the positive frequencies below the Nyquist frequency. To compute the full output, use fft()  Parameters \n \ninput (Tensor) \u2013 the real input tensor \nn (int, optional) \u2013 Signal length. If given, the input will either be zero-padded or trimmed to this length before computing the real FFT. \ndim (int, optional) \u2013 The dimension along which to take the one dimensional real FFT. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the FFT orthonormal)  Calling the backward transform (irfft()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.arange(4)\n>>> t\ntensor([0, 1, 2, 3])\n>>> torch.fft.rfft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j])\n Compare against the full output from fft(): >>> torch.fft.fft(t)\ntensor([ 6.+0.j, -2.+2.j, -2.+0.j, -2.-2.j])\n Notice that the symmetric element T[-1] == T[1].conj() is omitted. At the Nyquist frequency T[-2] == T[2] is it\u2019s own symmetric pair, and therefore must always be real-valued. \n"}, {"name": "torch.fft.rfft2()", "path": "fft#torch.fft.rfft2", "type": "torch.fft", "text": " \ntorch.fft.rfft2(input, s=None, dim=(-2, -1), norm=None) \u2192 Tensor  \nComputes the 2-dimensional discrete Fourier transform of real input. Equivalent to rfftn() but FFTs only the last two dimensions by default. The FFT of a real signal is Hermitian-symmetric, X[i, j] = conj(X[-i, -j]), so the full fft2() output contains redundant information. rfft2() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: last two dimensions. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfft2()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfft2()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfft2() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfft2 = torch.fft.rfft2(t)\n>>> rfft2.size()\ntorch.Size([10, 6])\n Compared against the full output from fft2(), we have all elements up to the Nyquist frequency. >>> fft2 = torch.fft.fft2(t)\n>>> torch.allclose(fft2[..., :6], rfft2)\nTrue\n The discrete Fourier transform is separable, so rfft2() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfft2, two_ffts)\n \n"}, {"name": "torch.fft.rfftfreq()", "path": "fft#torch.fft.rfftfreq", "type": "torch.fft", "text": " \ntorch.fft.rfftfreq(n, d=1.0, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nComputes the sample frequencies for rfft() with a signal of size n.  Note rfft() returns Hermitian one-sided output, so only the positive frequency terms are returned. For a real FFT of length n and with inputs spaced in length unit d, the frequencies are: f = torch.arange((n + 1) // 2) / (d * n)\n   Note For even lengths, the Nyquist frequency at f[n/2] can be thought of as either negative or positive. Unlike fftfreq(), rfftfreq() always returns it as positive.   Parameters \n \nn (int) \u2013 the real FFT length \nd (float, optional) \u2013 The sampling length scale. The spacing between individual samples of the FFT input. The default assumes unit spacing, dividing that result by the actual spacing gives the result in physical frequency units.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example >>> torch.fft.rfftfreq(5)\ntensor([ 0.0000,  0.2000,  0.4000])\n >>> torch.fft.rfftfreq(4)\ntensor([ 0.0000,  0.2500, 0.5000])\n Compared to the output from fftfreq(), we see that the Nyquist frequency at f[2] has changed sign: >>> torch.fft.fftfreq(4) tensor([ 0.0000, 0.2500, -0.5000, -0.2500]) \n"}, {"name": "torch.fft.rfftn()", "path": "fft#torch.fft.rfftn", "type": "torch.fft", "text": " \ntorch.fft.rfftn(input, s=None, dim=None, norm=None) \u2192 Tensor  \nComputes the N-dimensional discrete Fourier transform of real input. The FFT of a real signal is Hermitian-symmetric, X[i_1, ..., i_n] = conj(X[-i_1, ..., -i_n]) so the full fftn() output contains redundant information. rfftn() instead omits the negative frequencies in the last dimension.  Parameters \n \ninput (Tensor) \u2013 the input tensor \ns (Tuple[int], optional) \u2013 Signal size in the transformed dimensions. If given, each dimension dim[i] will either be zero-padded or trimmed to the length s[i] before computing the real FFT. If a length -1 is specified, no padding is done in that dimension. Default: s = [input.size(d) for d in dim]\n \ndim (Tuple[int], optional) \u2013 Dimensions to be transformed. Default: all dimensions, or the last len(s) dimensions if s is given. \nnorm (str, optional) \u2013 \nNormalization mode. For the forward transform (rfftn()), these correspond to:  \n\"forward\" - normalize by 1/n\n \n\"backward\" - no normalization \n\"ortho\" - normalize by 1/sqrt(n) (making the real FFT orthonormal)  Where n = prod(s) is the logical FFT size. Calling the backward transform (irfftn()) with the same normalization mode will apply an overall normalization of 1/n between the two transforms. This is required to make irfftn() the exact inverse. Default is \"backward\" (no normalization).     Example >>> t = torch.rand(10, 10)\n>>> rfftn = torch.fft.rfftn(t)\n>>> rfftn.size()\ntorch.Size([10, 6])\n Compared against the full output from fftn(), we have all elements up to the Nyquist frequency. >>> fftn = torch.fft.fftn(t)\n>>> torch.allclose(fftn[..., :6], rfftn)\nTrue\n The discrete Fourier transform is separable, so rfftn() here is equivalent to a combination of fft() and rfft(): >>> two_ffts = torch.fft.fft(torch.fft.rfft(x, dim=1), dim=0)\n>>> torch.allclose(rfftn, two_ffts)\n \n"}, {"name": "torch.fix()", "path": "generated/torch.fix#torch.fix", "type": "torch", "text": " \ntorch.fix(input, *, out=None) \u2192 Tensor  \nAlias for torch.trunc() \n"}, {"name": "torch.flatten()", "path": "generated/torch.flatten#torch.flatten", "type": "torch", "text": " \ntorch.flatten(input, start_dim=0, end_dim=-1) \u2192 Tensor  \nFlattens input by reshaping it into a one-dimensional tensor. If start_dim or end_dim are passed, only dimensions starting with start_dim and ending with end_dim are flattened. The order of elements in input is unchanged. Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view, or copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can be viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the flattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned.  Note Flattening a zero-dimensional tensor will return a one-dimensional view.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \nstart_dim (int) \u2013 the first dim to flatten \nend_dim (int) \u2013 the last dim to flatten    Example: >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n \n"}, {"name": "torch.flip()", "path": "generated/torch.flip#torch.flip", "type": "torch", "text": " \ntorch.flip(input, dims) \u2192 Tensor  \nReverse the order of a n-D tensor along given axis in dims.  Note torch.flip makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flip, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flip is expected to be slower than np.flip.   Parameters \n \ninput (Tensor) \u2013 the input tensor. \ndims (a list or tuple) \u2013 axis to flip on    Example: >>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])\n \n"}, {"name": "torch.fliplr()", "path": "generated/torch.fliplr#torch.fliplr", "type": "torch", "text": " \ntorch.fliplr(input) \u2192 Tensor  \nFlip tensor in the left/right direction, returning a new tensor. Flip the entries in each row in the left/right direction. Columns are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 2-D.   Note torch.fliplr makes a copy of input\u2019s data. This is different from NumPy\u2019s np.fliplr, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.fliplr is expected to be slower than np.fliplr.   Parameters \ninput (Tensor) \u2013 Must be at least 2-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])\n \n"}, {"name": "torch.flipud()", "path": "generated/torch.flipud#torch.flipud", "type": "torch", "text": " \ntorch.flipud(input) \u2192 Tensor  \nFlip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction. Rows are preserved, but appear in a different order than before.  Note Requires the tensor to be at least 1-D.   Note torch.flipud makes a copy of input\u2019s data. This is different from NumPy\u2019s np.flipud, which returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data, torch.flipud is expected to be slower than np.flipud.   Parameters \ninput (Tensor) \u2013 Must be at least 1-dimensional.   Example: >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])\n \n"}, {"name": "torch.FloatStorage", "path": "storage#torch.FloatStorage", "type": "torch.Storage", "text": " \nclass torch.FloatStorage(*args, **kwargs) [source]\n \n \nbfloat16()  \nCasts this storage to bfloat16 type \n  \nbool()  \nCasts this storage to bool type \n  \nbyte()  \nCasts this storage to byte type \n  \nchar()  \nCasts this storage to char type \n  \nclone()  \nReturns a copy of this storage \n  \ncomplex_double()  \nCasts this storage to complex double type \n  \ncomplex_float()  \nCasts this storage to complex float type \n  \ncopy_() \n  \ncpu()  \nReturns a CPU copy of this storage if it\u2019s not already on the CPU \n  \ncuda(device=None, non_blocking=False, **kwargs)  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (int) \u2013 The destination GPU id. Defaults to the current device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    \n  \ndata_ptr() \n  \ndevice \n  \ndouble()  \nCasts this storage to double type \n  \ndtype \n  \nelement_size() \n  \nfill_() \n  \nfloat()  \nCasts this storage to float type \n  \nstatic from_buffer() \n  \nstatic from_file(filename, shared=False, size=0) \u2192 Storage  \nIf shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters \n \nfilename (str) \u2013 file name to map \nshared (bool) \u2013 whether to share memory \nsize (int) \u2013 number of elements in the storage    \n  \nget_device() \n  \nhalf()  \nCasts this storage to half type \n  \nint()  \nCasts this storage to int type \n  \nis_cuda: bool = False \n  \nis_pinned() \n  \nis_shared() \n  \nis_sparse: bool = False \n  \nlong()  \nCasts this storage to long type \n  \nnew() \n  \npin_memory()  \nCopies the storage to pinned memory, if it\u2019s not already pinned. \n  \nresize_() \n  \nshare_memory_()  \nMoves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self \n  \nshort()  \nCasts this storage to short type \n  \nsize() \n  \ntolist()  \nReturns a list containing the elements of this storage \n  \ntype(dtype=None, non_blocking=False, **kwargs)  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n \n"}, {"name": "torch.FloatStorage.bfloat16()", "path": "storage#torch.FloatStorage.bfloat16", "type": "torch.Storage", "text": " \nbfloat16()  \nCasts this storage to bfloat16 type \n"}, {"name": "torch.FloatStorage.bool()", "path": "storage#torch.FloatStorage.bool", "type": "torch.Storage", "text": " \nbool()  \nCasts this storage to bool type \n"}, {"name": "torch.FloatStorage.byte()", "path": "storage#torch.FloatStorage.byte", "type": "torch.Storage", "text": " \nbyte()  \nCasts this storage to byte type \n"}, {"name": "torch.FloatStorage.char()", "path": "storage#torch.FloatStorage.char", "type": "torch.Storage", "text": " \nchar()  \nCasts this storage to char type \n"}, {"name": "torch.FloatStorage.clone()", "path": "storage#torch.FloatStorage.clone", "type": "torch.Storage", "text": " \nclone()  \nReturns a copy of this storage \n"}, {"name": "torch.FloatStorage.complex_double()", "path": "storage#torch.FloatStorage.complex_double", "type": "torch.Storage", "text": " \ncomplex_double()  \nCasts this storage to complex double type \n"}, {"name": "torch.FloatStorage.complex_float()", "path": "storage#torch.FloatStorage.complex_float", "type": "torch.Storage", "text": " \ncomplex_float()  \nCasts this storage to complex float type \n"}, {"name": "torch.FloatStorage.copy_()", "path": "storage#torch.FloatStorage.copy_", "type": "torch.Storage", "text": " \ncopy_() \n"}, {"name": "torch.FloatStorage.cpu()", "path": "storage#torch.FloatStorage.cpu", "type": "torch.Storage", "text": " \ncpu()  \nReturns a CPU copy of this storage if it\u2019s not already on the CPU \n"}, {"name": "torch.FloatStorage.cuda()", "path": "storage#torch.FloatStorage.cuda", "type": "torch.Storage", "text": " \ncuda(device=None, non_blocking=False, **kwargs)  \nReturns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters \n \ndevice (int) \u2013 The destination GPU id. Defaults to the current device. \nnon_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    \n"}, {"name": "torch.FloatStorage.data_ptr()", "path": "storage#torch.FloatStorage.data_ptr", "type": "torch.Storage", "text": " \ndata_ptr() \n"}, {"name": "torch.FloatStorage.device", "path": "storage#torch.FloatStorage.device", "type": "torch.Storage", "text": " \ndevice \n"}, {"name": "torch.FloatStorage.double()", "path": "storage#torch.FloatStorage.double", "type": "torch.Storage", "text": " \ndouble()  \nCasts this storage to double type \n"}, {"name": "torch.FloatStorage.dtype", "path": "storage#torch.FloatStorage.dtype", "type": "torch.Storage", "text": " \ndtype \n"}, {"name": "torch.FloatStorage.element_size()", "path": "storage#torch.FloatStorage.element_size", "type": "torch.Storage", "text": " \nelement_size() \n"}, {"name": "torch.FloatStorage.fill_()", "path": "storage#torch.FloatStorage.fill_", "type": "torch.Storage", "text": " \nfill_() \n"}, {"name": "torch.FloatStorage.float()", "path": "storage#torch.FloatStorage.float", "type": "torch.Storage", "text": " \nfloat()  \nCasts this storage to float type \n"}, {"name": "torch.FloatStorage.from_buffer()", "path": "storage#torch.FloatStorage.from_buffer", "type": "torch.Storage", "text": " \nstatic from_buffer() \n"}, {"name": "torch.FloatStorage.from_file()", "path": "storage#torch.FloatStorage.from_file", "type": "torch.Storage", "text": " \nstatic from_file(filename, shared=False, size=0) \u2192 Storage  \nIf shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters \n \nfilename (str) \u2013 file name to map \nshared (bool) \u2013 whether to share memory \nsize (int) \u2013 number of elements in the storage    \n"}, {"name": "torch.FloatStorage.get_device()", "path": "storage#torch.FloatStorage.get_device", "type": "torch.Storage", "text": " \nget_device() \n"}, {"name": "torch.FloatStorage.half()", "path": "storage#torch.FloatStorage.half", "type": "torch.Storage", "text": " \nhalf()  \nCasts this storage to half type \n"}, {"name": "torch.FloatStorage.int()", "path": "storage#torch.FloatStorage.int", "type": "torch.Storage", "text": " \nint()  \nCasts this storage to int type \n"}, {"name": "torch.FloatStorage.is_cuda", "path": "storage#torch.FloatStorage.is_cuda", "type": "torch.Storage", "text": " \nis_cuda: bool = False \n"}, {"name": "torch.FloatStorage.is_pinned()", "path": "storage#torch.FloatStorage.is_pinned", "type": "torch.Storage", "text": " \nis_pinned() \n"}, {"name": "torch.FloatStorage.is_shared()", "path": "storage#torch.FloatStorage.is_shared", "type": "torch.Storage", "text": " \nis_shared() \n"}, {"name": "torch.FloatStorage.is_sparse", "path": "storage#torch.FloatStorage.is_sparse", "type": "torch.Storage", "text": " \nis_sparse: bool = False \n"}, {"name": "torch.FloatStorage.long()", "path": "storage#torch.FloatStorage.long", "type": "torch.Storage", "text": " \nlong()  \nCasts this storage to long type \n"}, {"name": "torch.FloatStorage.new()", "path": "storage#torch.FloatStorage.new", "type": "torch.Storage", "text": " \nnew() \n"}, {"name": "torch.FloatStorage.pin_memory()", "path": "storage#torch.FloatStorage.pin_memory", "type": "torch.Storage", "text": " \npin_memory()  \nCopies the storage to pinned memory, if it\u2019s not already pinned. \n"}, {"name": "torch.FloatStorage.resize_()", "path": "storage#torch.FloatStorage.resize_", "type": "torch.Storage", "text": " \nresize_() \n"}, {"name": "torch.FloatStorage.share_memory_()", "path": "storage#torch.FloatStorage.share_memory_", "type": "torch.Storage", "text": " \nshare_memory_()  \nMoves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self \n"}, {"name": "torch.FloatStorage.short()", "path": "storage#torch.FloatStorage.short", "type": "torch.Storage", "text": " \nshort()  \nCasts this storage to short type \n"}, {"name": "torch.FloatStorage.size()", "path": "storage#torch.FloatStorage.size", "type": "torch.Storage", "text": " \nsize() \n"}, {"name": "torch.FloatStorage.tolist()", "path": "storage#torch.FloatStorage.tolist", "type": "torch.Storage", "text": " \ntolist()  \nReturns a list containing the elements of this storage \n"}, {"name": "torch.FloatStorage.type()", "path": "storage#torch.FloatStorage.type", "type": "torch.Storage", "text": " \ntype(dtype=None, non_blocking=False, **kwargs)  \nReturns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters \n \ndtype (type or string) \u2013 The desired type \nnon_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. \n**kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    \n"}, {"name": "torch.float_power()", "path": "generated/torch.float_power#torch.float_power", "type": "torch", "text": " \ntorch.float_power(input, exponent, *, out=None) \u2192 Tensor  \nRaises input to the power of exponent, elementwise, in double precision. If neither input is complex returns a torch.float64 tensor, and if one or more inputs is complex returns a torch.complex128 tensor.  Note This function always computes in double precision, unlike torch.pow(), which implements more typical type promotion. This is useful when the computation needs to be performed in a wider or more precise dtype, or the results of the computation may contain fractional values not representable in the input dtypes, like when an integer base is raised to a negative integer exponent.   Parameters \n \ninput (Tensor or Number) \u2013 the base value(s) \nexponent (Tensor or Number) \u2013 the exponent value(s)   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)\n \n"}, {"name": "torch.floor()", "path": "generated/torch.floor#torch.floor", "type": "torch", "text": " \ntorch.floor(input, *, out=None) \u2192 Tensor  \nReturns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.  outi=\u230ainputi\u230b\\text{out}_{i} = \\left\\lfloor \\text{input}_{i} \\right\\rfloor  \n Parameters \ninput (Tensor) \u2013 the input tensor.  Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n \n"}, {"name": "torch.floor_divide()", "path": "generated/torch.floor_divide#torch.floor_divide", "type": "torch", "text": " \ntorch.floor_divide(input, other, *, out=None) \u2192 Tensor  \n Warning This function\u2019s name is a misnomer. It actually rounds the quotient towards zero instead of taking its floor. This behavior will be deprecated in a future PyTorch release.  Computes input divided by other, elementwise, and rounds each quotient towards zero. Equivalently, it truncates the quotient(s):  outi=trunc(inputiotheri)\\text{{out}}_i = \\text{trunc} \\left( \\frac{{\\text{{input}}_i}}{{\\text{{other}}_i}} \\right)  \nSupports broadcasting to a common shape, type promotion, and integer and float inputs.  Parameters \n \ninput (Tensor or Number) \u2013 the dividend \nother (Tensor or Number) \u2013 the divisor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])\n \n"}, {"name": "torch.fmax()", "path": "generated/torch.fmax#torch.fmax", "type": "torch", "text": " \ntorch.fmax(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise maximum of input and other. This is like torch.maximum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the maximum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmax and is similar to NumPy\u2019s fmax function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])\n \n"}, {"name": "torch.fmin()", "path": "generated/torch.fmin#torch.fmin", "type": "torch", "text": " \ntorch.fmin(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise minimum of input and other. This is like torch.minimum() except it handles NaNs differently: if exactly one of the two elements being compared is a NaN then the non-NaN element is taken as the minimum. Only if both elements are NaN is NaN propagated. This function is a wrapper around C++\u2019s std::fmin and is similar to NumPy\u2019s fmin function. Supports broadcasting to a common shape, type promotion, and integer and floating-point inputs.  Parameters \n \ninput (Tensor) \u2013 the input tensor. \nother (Tensor) \u2013 the second input tensor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])\n \n"}, {"name": "torch.fmod()", "path": "generated/torch.fmod#torch.fmod", "type": "torch", "text": " \ntorch.fmod(input, other, *, out=None) \u2192 Tensor  \nComputes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape, type promotion, and integer and float inputs.  Note When the divisor is zero, returns NaN for floating point dtypes on both CPU and GPU; raises RuntimeError for integer division by zero on CPU; Integer division by zero on GPU may return any value.   Parameters \n \ninput (Tensor) \u2013 the dividend \nother (Tensor or Scalar) \u2013 the divisor   Keyword Arguments \nout (Tensor, optional) \u2013 the output tensor.   Example: >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])\n \n"}, {"name": "torch.frac()", "path": "generated/torch.frac#torch.frac", "type": "torch", "text": " \ntorch.frac(input, *, out=None) \u2192 Tensor  \nComputes the fractional portion of each element in input.  outi=inputi\u2212\u230a\u2223inputi\u2223\u230b\u2217sgn\u2061(inputi)\\text{out}_{i} = \\text{input}_{i} - \\left\\lfloor |\\text{input}_{i}| \\right\\rfloor * \\operatorname{sgn}(\\text{input}_{i})  \nExample: >>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n \n"}, {"name": "torch.from_numpy()", "path": "generated/torch.from_numpy#torch.from_numpy", "type": "torch", "text": " \ntorch.from_numpy(ndarray) \u2192 Tensor  \nCreates a Tensor from a numpy.ndarray. The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable. It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.complex64, numpy.complex128, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool. Example: >>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n \n"}, {"name": "torch.full()", "path": "generated/torch.full#torch.full", "type": "torch", "text": " \ntorch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) \u2192 Tensor  \nCreates a tensor of size size filled with fill_value. The tensor\u2019s dtype is inferred from fill_value.  Parameters \n \nsize (int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. \nfill_value (Scalar) \u2013 the value to fill the output tensor with.   Keyword Arguments \n \nout (Tensor, optional) \u2013 the output tensor. \ndtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). \nlayout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: >>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])\n \n"}, {"name": "torch.full_like()", "path": "generated/torch.full_like#torch.full_like", "type": "torch", "text": " \ntorch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) \u2192 Tensor  \nReturns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).  Parameters \n \ninput (Tensor) \u2013 the size of input will determine size of the output tensor. \nfill_value \u2013 the number to fill the output tensor with.   Keyword Arguments \n \ndtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. \nlayout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. \ndevice (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. \nrequires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. \nmemory_format (torch.memory_format, optional) \u2013 the desired memory format of returned Tensor. Default: torch.preserve_format.    \n"}, {"name": "torch.futures", "path": "futures", "type": "torch.futures", "text": "torch.futures  Warning The torch.futures package is experimental and subject to change.  This package provides a Future type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects. Currently, the Future type is primarily used by the Distributed RPC Framework.  \nclass torch.futures.Future  \nWrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.  \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n  \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n  \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n  \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n  \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n  \nvalue(self: torch._C.Future) \u2192 object \n  \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n \n  \ntorch.futures.collect_all(futures) [source]\n \nCollects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.  Parameters \nfutures (list) \u2013 a list of Future objects.  Returns \nReturns a Future object to a list of the passed in Futures.    Example::\n\n>>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n   \n  \ntorch.futures.wait_all(futures) [source]\n \nWaits for all provided futures to be complete, and returns the list of completed values.  Parameters \nfutures (list) \u2013 a list of Future object.  Returns \nA list of the completed Future results. This method will throw an error if wait on any Future throws.   \n\n"}, {"name": "torch.futures.collect_all()", "path": "futures#torch.futures.collect_all", "type": "torch.futures", "text": " \ntorch.futures.collect_all(futures) [source]\n \nCollects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed.  Parameters \nfutures (list) \u2013 a list of Future objects.  Returns \nReturns a Future object to a list of the passed in Futures.    Example::\n\n>>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n   \n"}, {"name": "torch.futures.Future", "path": "futures#torch.futures.Future", "type": "torch.futures", "text": " \nclass torch.futures.Future  \nWrapper around a torch._C.Future which encapsulates an asynchronous execution of a callable, e.g. rpc_async(). It also exposes a set of APIs to add callback functions and set results.  \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n  \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n  \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n  \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n  \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n  \nvalue(self: torch._C.Future) \u2192 object \n  \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n \n"}, {"name": "torch.futures.Future.add_done_callback()", "path": "futures#torch.futures.Future.add_done_callback", "type": "torch.futures", "text": " \nadd_done_callback(self: torch._C.Future, arg0: function) \u2192 None \n"}, {"name": "torch.futures.Future.done()", "path": "futures#torch.futures.Future.done", "type": "torch.futures", "text": " \ndone() [source]\n \nReturn True if this Future is done. A Future is done if it has a result or an exception. \n"}, {"name": "torch.futures.Future.set_exception()", "path": "futures#torch.futures.Future.set_exception", "type": "torch.futures", "text": " \nset_exception(result) [source]\n \nSet an exception for this Future, which will mark this Future as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this Future, the exception set here will be raised inline.  Parameters \nresult (BaseException) \u2013 the exception for this Future.    Example::\n\n>>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n   \n"}, {"name": "torch.futures.Future.set_result()", "path": "futures#torch.futures.Future.set_result", "type": "torch.futures", "text": " \nset_result(result) [source]\n \nSet the result for this Future, which will mark this Future as completed and trigger all attached callbacks. Note that a Future cannot be marked completed twice.  Parameters \nresult (object) \u2013 the result object of this Future.    Example::\n\n>>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n   \n"}, {"name": "torch.futures.Future.then()", "path": "futures#torch.futures.Future.then", "type": "torch.futures", "text": " \nthen(callback) [source]\n \nAppend the given callback function to this Future, which will be run when the Future is completed. Multiple callbacks can be added to the same Future, and will be invoked in the same order as they were added. The callback must take one argument, which is the reference to this Future. The callback function can use the Future.wait() API to get the value. Note that if this Future is already completed, the given callback will be run immediately inline.  Parameters \ncallback (Callable) \u2013 a Callable that takes this Future as the only argument.  Returns \nA new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes.    Example::\n\n>>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n   \n"}, {"name": "torch.futures.Future.value()", "path": "futures#torch.futures.Future.value", "type": "torch.futures", "text": " \nvalue(self: torch._C.Future) \u2192 object \n"}, {"name": "torch.futures.Future.wait()", "path": "futures#torch.futures.Future.wait", "type": "torch.futures", "text": " \nwait() [source]\n \nBlock until the value of this Future is ready.  Returns \nThe value held by this Future. If the function (callback or RPC) creating the value has thrown an error, this wait method will also throw an error.   \n"}, {"name": "torch.futures.wait_all()", "path": "futures#torch.futures.wait_all", "type": "torch.futures", "text": " \ntorch.futures.wait_all(futures) [source]\n \nWaits for all provided futures to be complete, and returns the list of completed values.  Parameters \nfutures (list) \u2013 a list of Future object.  Returns \nA list of the completed Future results. This method will throw an error if wait on any Future throws.   \n"}, {"name": "torch.fx", "path": "fx", "type": "torch.fx", "text": "torch.fx Overview This feature is under a Beta release and its API may change. FX is a toolkit for developers to use to transform nn.Module instances. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation. A demonstration of these components in action: import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"\n The symbolic tracer performs \u201csymbolic execution\u201d of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the symbolic_trace() and Tracer documentation. The intermediate representation is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or torch.nn.Module instances), and return values. More information about the IR can be found in the documentation for Graph. The IR is the format on which transformations are applied. Python code generation is what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph\u2019s semantics. This functionality is wrapped up in GraphModule, which is a torch.nn.Module instance that holds a Graph as well as a forward method generated from the Graph. Taken together, this pipeline of components (symbolic tracing \u2192 intermediate representation \u2192 transforms \u2192 Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX! Several example transformations can be found at the examples repository. Writing Transformations What is an FX transform? Essentially, it\u2019s a function that looks like this. import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n Your transform will take in an torch.nn.Module, acquire a Graph from it, do some modifications, and return a new torch.nn.Module. You should think of the torch.nn.Module that your FX transform returns as identical to a regular torch.nn.Module \u2013 you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a torch.nn.Module will allow for composability.  Note It is also possible to modify an existing GraphModule instead of creating a new one, like so: import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module):\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n Note that you MUST call GraphModule.recompile() to bring the generated forward() method on the GraphModule in sync with the modified Graph.  Given that you\u2019ve passed in a torch.nn.Module that has been traced into a Graph, there are now two primary approaches you can take to building a new Graph. A Quick Primer on Graphs Full treatment of the semantics of graphs can be found in the Graph documentation, but we are going to cover the basics here. A Graph is a data structure that represents a method on a GraphModule. The information that this requires is:  What are the inputs to the method? What are the operations that run inside the method? What is the output (i.e. return) value from the method?  All three of these concepts are represented with Node instances. Let\u2019s see what we mean by that with a short example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n Here we define a module MyModule for demonstration purposes, instantiate it, symbolically trace it, then call the Graph.print_tabular() method to print out a table showing the nodes of this Graph:   \nopcode name target args kwargs   \nplaceholder x x () {}  \nget_attr linear_weight linear.weight () {}  \ncall_function add_1 <built-in function add> (x, linear_weight) {}  \ncall_module linear_1 linear (add_1,) {}  \ncall_method relu_1 relu (linear_1,) {}  \ncall_function sum_1 <built-in method sum \u2026> (relu_1,) {\u2018dim\u2019: -1}  \ncall_function topk_1 <built-in method topk \u2026> (sum_1, 3) {}  \noutput output output (topk_1,) {}   We can use this information to answer the questions we posed above.  What are the inputs to the method? In FX, method inputs are specified via special placeholder nodes. In this case, we have a single placeholder node with a target of x, meaning we have a single (non-self) argument named x. What are the operations within the method? The get_attr, call_function, call_module, and call_method nodes represent the operations in the method. A full treatment of the semantics of all of these can be found in the Node documentation. What is the return value of the method? The return value in a Graph is specified by a special output node.  Given that we now know the basics of how code is represented in FX, we can now explore how we would edit a Graph. Graph Manipulation Direct Graph Manipulation One approach to building this new Graph is to directly manipulate your old one. To aid in this, we can simply take the Graph we obtain from symbolic tracing and modify it. For example, let\u2019s say we desire to replace torch.add() calls with torch.mul() calls. import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n We can also do more involved Graph rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the Graph documentation. An example of using these APIs to append a torch.relu() call can be found below. # Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n For simple transformations that only consist of substitutions, you can also make use of the subgraph rewriter. Subgraph Rewriting With replace_pattern() FX also provides another level of automation on top of direct graph manipulation. The replace_pattern() API is essentially a \u201cfind/replace\u201d tool for editing Graphs. It allows you to specify a pattern and replacement function and it will trace through those functions, find instances of the group of operations in the pattern graph, and replace those instances with copies of the replacement graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex. Graph Manipulation Examples  Replace one op Conv/Batch Norm fusion replace_pattern: Basic usage Quantization Invert Transformation  Proxy/Retracing Another way of manipulating Graphs is by reusing the Proxy machinery used in symbolic tracing. For example, let\u2019s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every F.relu(x) call into (x > 0) * x. One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the F.relu, and then clean up the original F.relu. However, we can automate this process by using Proxy objects to automatically record operations into the Graph. To use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with Proxy objects as arugments. These Proxy objects will capture the operations that are performed on them and append them to the Graph. # Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n In addition to avoiding explicit graph manipulation, using Proxys also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. A worked example of using Proxys for Graph manipulation can be found here. The Interpreter Pattern A useful code organizational pattern in FX is to loop over all the Nodes in a Graph and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with Proxys. For example, suppose we want to run a GraphModule and record the torch.Tensor shape and dtype properties on the nodes as we see them at runtime. That might look like: import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n As you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the Interpreter class, which encompasses the above logic in a way that certain aspects of the interpreter\u2019s execution can be overridden via method overrides. In addition to executing operations, we can also generate a new Graph by feeding Proxy values through an interpreter. Similarly, we provide the Transformer class to encompass this pattern. Transformer behaves similarly to Interpreter, but instead of calling the run method to get a concrete output value from the Module, you would call the Transformer.transform() method to return a new GraphModule which was subject to any transformation rules you installed as overridden methods. Examples of the Interpreter Pattern  Shape Propagation Performance Profiler  Debugging Introduction Often in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code. If you\u2019re not familiar with debuggers, please see the auxiliary section Available Debuggers. Checking Correctness of Modules Because the output of most deep learning modules consists of floating point torch.Tensor instances, checking for equivalence between the results of two torch.nn.Module is not as straightforward as doing a simple equality check. To motivate this, let\u2019s use an example: import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n Here, we\u2019ve tried to check equality of the values of two deep learning models with the == equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see here for more details). We can use torch.allclose() instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold: assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n This is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation. Debugging the Generated Code Because FX generates the forward() function on GraphModules, using traditional debugging techniques like print statements or pdb is not as straightfoward. Luckily, we have several techniques we can use for debugging the generated code. Use pdb\n Invoke pdb to step into the running program. Although the code that represents the Graph is not in any source file, we can still step into it manually using pdb when the forward pass is invoked. import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n Print the Generated Code If you\u2019d like to run the same code multiple times, then it can be a bit tedious to step to the right code with pdb. In that case, one approach is to simply copy-paste the generated forward pass into your code and examine it from there. # Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n Use the to_folder Function From GraphModule\n GraphModule.to_folder() is a method in GraphModule that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in Print the Generated Code, it may be easier to examine modules and parameters using to_folder. m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n After running the above example, we can then look at the code within foo/module.py and modify it as desired (e.g. adding print statements or using pdb) to debug the generated code. Debugging the Transformation Now that we\u2019ve identified that a transformation is creating incorrect code, it\u2019s time to debug the transformation itself. First, we\u2019ll check the Limitations of Symbolic Tracing section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our GraphModule transformation. There may be a quick answer in Writing Transformations, but, if not, there are several ways to examine our traced module: # Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"\n Using the utility functions above, we can compare our traced Module before and after we\u2019ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it\u2019s still not clear what\u2019s going wrong, a debugger like pdb can be a good next step. Going off of the example above, consider the following code: # Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n Using the above example, let\u2019s say that the call to print(traced) showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a pdb session. We can see what\u2019s happening during the transform by breaking on transform_graph(traced), then pressing s to \u201cstep into\u201d the call to transform_graph(traced). We may also have good luck by editing the print_tabular method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node\u2019s input_nodes and users.) Available Debuggers The most common Python debugger is pdb. You can start your program in \u201cdebug mode\u201d with pdb by typing python -m pdb FILENAME.py into the command line, where FILENAME is the name of the file you want to debug. After that, you can use the pdb debugger commands to move through your running program stepwise. It\u2019s common to set a breakpoint (b LINE-NUMBER) when you start pdb, then call c to run the program until that point. This prevents you from having to step through each line of execution (using s or n) to get to the part of the code you want to examine. Alternatively, you can write import pdb; pdb.set_trace() before the line you want to break at. If you add pdb.set_trace(), your program will automatically start in debug mode when you run it. (In other words, you can just type python FILENAME.py into the command line instead of python -m pdb FILENAME.py.) Once you\u2019re running your file in debug mode, you can step through the code and examine your program\u2019s internal state using certain commands. There are many excellent tutorials on pdb online, including RealPython\u2019s \u201cPython Debugging With Pdb\u201d. IDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use pdb by pulling up a terminal window in your IDE (e.g. View \u2192 Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around pdb). Limitations of Symbolic Tracing FX uses a system of symbolic tracing (a.k.a symbolic execution) to capture the semantics of programs in a transformable/analyzable form. The system is tracing in that it executes the program (really a torch.nn.Module or function) to record operations. It is symbolic in that the data flowing through the program during this execution is not real data, but rather symbols (Proxy in FX parlance). Although symbolic tracing works for most neural net code, it has some limitations. Dynamic Control Flow The main limitation of symbolic tracing is it does not currently support dynamic control flow. That is, loops or if statements where the condition may depend on the input values of the program. For example, let\u2019s examine the following program: def func_to_trace(x):\n    dim0 = x.size[0]\n    if dim0 == 3:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if dim0 == 3:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n The condition to the if statement relies on the value of dim0, which eventually relies on the value of x, a function input. Since x can change (i.e. if you pass a new input tensor to the traced function), this is dynamic control flow. The traceback walks back up through your code to show you where this situation happens. Static Control Flow On the other hand, so-called static control flow is supported. Static control flow is loops or if statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model\u2019s architecture based on hyper-parameters. As a concrete example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n The if-statement if self.do_activation does not depend on any function inputs, thus it is static. do_activation can be considered to be a hyper-parameter, and the traces of different instances of MyModule with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing. Many instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to Module attributes or by passing constant values during symbolic tracing: def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\ndef wrapper(flag):\n    return lambda x: f(x, flag)\n\nnew_f = wrapper(flag=True)\nfx.symbolic_trace(new_f)\n In the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see Customizing Tracing with the Tracer class) or function (see wrap()) rather than tracing through them. Non-torch Functions FX uses __torch_function__ as the mechanism by which it intercepts calls (see the technical overview for more information about this). Some functions, such as builtin Python functions or those in the math module, are things that are not covered by __torch_function__, but we would still like to capture them in symbolic tracing. For example: import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n The error tells us that the built-in function len is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the wrap() API: torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n Customizing Tracing with the Tracer class The Tracer class is the class that underlies the implementation of symbolic_trace. The behavior of tracing can be customized by subclassing Tracer, like so: class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n Leaf Modules Leaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard torch.nn module instances. For example: class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n The set of leaf modules can be customized by overriding Tracer.is_leaf_module(). Miscellanea  \nTensor constructors (e.g. torch.zeros, torch.ones, torch.rand, torch.randn, torch.sparse_coo_tensor) are currently not traceable.  The deterministic constructors (zeros, ones) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, ones_like or zeros_like may be a viable substitute. Nondeterministic constructors (rand, randn) will have a single random value embedded in the trace. This is likely not the intended behavior. This behavior may be fixed in a future release.   \nType annotations  Python 3-style type annotations (e.g. func(x : torch.Tensor, y : int) -> torch.Tensor) are supported and will be preserved by symbolic tracing. Python 2-style comment type annotations # type: (torch.Tensor, int) -> torch.Tensor are not currently supported. Annotations on local names within a function are not currently supported.    API Reference  \ntorch.fx.symbolic_trace(root, concrete_args=None) [source]\n \nSymbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.  Parameters \n \nroot (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted into a Graph representation. \nconcrete_args (Optional[Dict[str, any]]) \u2013 Concrete arguments that should not be treated as Proxies.   Returns \na Module created from the recorded operations from root.  Return type \nGraphModule   \n  \ntorch.fx.wrap(fn_or_name) [source]\n \nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.  Parameters \nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called   \n  \nclass torch.fx.GraphModule(root, graph, class_name='GraphModule') [source]\n \nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.  Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.   \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n  \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n  \nproperty graph  \nReturn the Graph underlying this GraphModule \n  \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n  \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n \n  \nclass torch.fx.Graph [source]\n \nGraph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n Will produce the following Graph: print(gm.graph)\n graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n For the semantics of operations represented in the Graph, please see Node.  \n__init__() [source]\n \nConstruct an empty Graph. \n  \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n  \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n  \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n  \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n  \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n  \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n  \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n  \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n \n  \nclass torch.fx.Node(graph, name, op, target, args, kwargs, type=None) [source]\n \nNode is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:  \nplaceholder represents a function input. The name attribute specifies the name this value will take on. target is similarly the name of the argument. args holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. kwargs is don\u2019t-care. Placeholders correspond to the function parameters (e.g. x) in the graph printout. \nget_attr retrieves a parameter from the module hierarchy. name is similarly the name the result of the fetch is assigned to. target is the fully-qualified name of the parameter\u2019s position in the module hierarchy. args and kwargs are don\u2019t-care \ncall_function applies a free function to some values. name is similarly the name of the value to assign to. target is the function to be applied. args and kwargs represent the arguments to the function, following the Python calling convention \ncall_module applies a module in the module hierarchy\u2019s forward() method to given arguments. name is as previous. target is the fully-qualified name of the module in the module hierarchy to call. args and kwargs represent the arguments to invoke the module on, including the self argument. \ncall_method calls a method on a value. name is as similar. target is the string name of the method to apply to the self argument. args and kwargs represent the arguments to invoke the module on, including the self argument\n \noutput contains the output of the traced function in its args[0] attribute. This corresponds to the \u201creturn\u201d statement in the Graph printout.   \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n  \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n  \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n  \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n  \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n  \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n \n  \nclass torch.fx.Tracer(autowrap_modules=(<module 'math' from '/home/matti/miniconda3/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so'>, )) [source]\n \nTracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m). Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.  \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n  \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n  \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n  \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n  \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n  \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n \n  \nclass torch.fx.Proxy(node, tracer=None) [source]\n \nProxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph. If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph. \n  \nclass torch.fx.Interpreter(module) [source]\n \nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes. Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The module to be executed    \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n  \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n  \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n  \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n  \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n  \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n  \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n  \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n  \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n  \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n \n  \nclass torch.fx.Transformer(module) [source]\n \nTransformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically. Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The Module to be transformed.    \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n \n  \ntorch.fx.replace_pattern(gm, pattern, replacement) [source]\n \nMatches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).  Parameters \n \ngm \u2013 The GraphModule that wraps the Graph to operate on \npattern \u2013 The subgraph to match in gm for replacement \nreplacement \u2013 The subgraph to replace pattern with   Returns \nA list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n  Return type \nList[Match]   Examples: import torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m). The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph. When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.) One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n with def replacement(x, y):\n    return torch.relu(x)\n In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement. After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n \n\n"}, {"name": "torch.fx.Graph", "path": "fx#torch.fx.Graph", "type": "torch.fx", "text": " \nclass torch.fx.Graph [source]\n \nGraph is the main data structure used in the FX Intermediate Representation. It consists of a series of Node s, each representing callsites (or other syntactic constructs). The list of Node s, taken together, constitute a valid Python function. For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n Will produce the following Graph: print(gm.graph)\n graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n For the semantics of operations represented in the Graph, please see Node.  \n__init__() [source]\n \nConstruct an empty Graph. \n  \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n  \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n  \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n  \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n  \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n  \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n  \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n  \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n  \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n  \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n  \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n \n"}, {"name": "torch.fx.Graph.call_function()", "path": "fx#torch.fx.Graph.call_function", "type": "torch.fx", "text": " \ncall_function(the_function, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_function Node into the Graph. A call_function node represents a call to a Python callable, specified by the_function. the_function can be  Parameters \n \nthe_function (Callable[.., Any]) \u2013 The function to be called. Can be any PyTorch operator, Python function, or member of the builtins or operator namespaces. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called function. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called function \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.    Returns The newly created and inserted call_function node.  Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.call_method()", "path": "fx#torch.fx.Graph.call_method", "type": "torch.fx", "text": " \ncall_method(method_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_method Node into the Graph. A call_method node represents a call to a given method on the 0th element of args.  Parameters \n \nmethod_name (str) \u2013 The name of the method to apply to the self argument. For example, if args[0] is a Node representing a Tensor, then to call relu() on that Tensor, pass relu to method_name. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly created and inserted call_method node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.call_module()", "path": "fx#torch.fx.Graph.call_module", "type": "torch.fx", "text": " \ncall_module(module_name, args=None, kwargs=None, type_expr=None) [source]\n \nInsert a call_module Node into the Graph. A call_module node represents a call to the forward() function of a Module in the Module hierarchy.  Parameters \n \nmodule_name (str) \u2013 The qualified name of the Module in the Module hierarchy to be called. For example, if the traced Module has a submodule named foo, which has a submodule named bar, the qualified name foo.bar should be passed as module_name to call that module. \nargs (Optional[Tuple[Argument, ..]]) \u2013 The positional arguments to be passed to the called method. Note that this should not include a self argument. \nkwargs (Optional[Dict[str, Argument]]) \u2013 The keyword arguments to be passed to the called method \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted call_module node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node().  \n"}, {"name": "torch.fx.Graph.create_node()", "path": "fx#torch.fx.Graph.create_node", "type": "torch.fx", "text": " \ncreate_node(op, target, args=None, kwargs=None, name=None, type_expr=None) [source]\n \nCreate a Node and add it to the Graph at the current insert-point. Note that the current insert-point can be set via Graph.inserting_before() and Graph.inserting_after().  Parameters \n \nop (str) \u2013 the opcode for this Node. One of \u2018call_function\u2019, \u2018call_method\u2019, \u2018get_attr\u2019, \u2018call_module\u2019, \u2018placeholder\u2019, or \u2018output\u2019. The semantics of these opcodes are described in the Graph docstring. \nargs (Optional[Tuple[Argument, ..]]) \u2013 is a tuple of arguments to this node. \nkwargs (Optional[Dict[str, Argument]]) \u2013 the kwargs of this Node \nname (Optional[str]) \u2013 an optional string name for the Node. This will influence the name of the value assigned to in the Python generated code. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted node.   \n"}, {"name": "torch.fx.Graph.erase_node()", "path": "fx#torch.fx.Graph.erase_node", "type": "torch.fx", "text": " \nerase_node(to_erase) [source]\n \nErases a Node from the Graph. Throws an exception if there are still users of that node in the Graph.  Parameters \nto_erase (Node) \u2013 The Node to erase from the Graph.   \n"}, {"name": "torch.fx.Graph.get_attr()", "path": "fx#torch.fx.Graph.get_attr", "type": "torch.fx", "text": " \nget_attr(qualified_name, type_expr=None) [source]\n \nInsert a get_attr node into the Graph. A get_attr Node represents the fetch of an attribute from the Module hierarchy.  Parameters \n \nqualified_name (str) \u2013 the fully-qualified name of the attribute to be retrieved. For example, if the traced Module has a submodule named foo, which has a submodule named bar, which has an attribute named baz, the qualified name foo.bar.baz should be passed as qualified_name. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.   Returns \nThe newly-created and inserted get_attr node.    Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.graph_copy()", "path": "fx#torch.fx.Graph.graph_copy", "type": "torch.fx", "text": " \ngraph_copy(g, val_map) [source]\n \nCopy all nodes from a given graph into self.  Parameters \n \ng (Graph) \u2013 The source graph from which to copy Nodes. \nval_map (Dict[Node, Node]) \u2013 a dictionary that will be populated with a mapping from nodes in g to nodes in self. Note that val_map can be passed in with values in it already to override copying of certain values.   Returns \nThe value in self that is now equivalent to the output value in g, if g had an output node. None otherwise.   \n"}, {"name": "torch.fx.Graph.inserting_after()", "path": "fx#torch.fx.Graph.inserting_after", "type": "torch.fx", "text": " \ninserting_after(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert after the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n"}, {"name": "torch.fx.Graph.inserting_before()", "path": "fx#torch.fx.Graph.inserting_before", "type": "torch.fx", "text": " \ninserting_before(n=None) [source]\n \nSet the point at which create_node and companion methods will insert into the graph. When used within a \u2018with\u2019 statement, this will temporary set the insert point and then restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n  Parameters \nn (Optional[Node]) \u2013 The node before which to insert. If None this will insert before the beginning of the entire graph.  Returns \nA resource manager that will restore the insert point on __exit__.   \n"}, {"name": "torch.fx.Graph.lint()", "path": "fx#torch.fx.Graph.lint", "type": "torch.fx", "text": " \nlint(root=None) [source]\n \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If root is provided, checks that targets exist in root  Parameters \nroot (Optional[torch.nn.Module]) \u2013 The root module with which to check for targets. This is equivalent to the root argument that is passed when constructing a GraphModule.   \n"}, {"name": "torch.fx.Graph.nodes()", "path": "fx#torch.fx.Graph.nodes", "type": "torch.fx", "text": " \nproperty nodes  \nGet the list of Nodes that constitute this Graph. Note that this Node list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.  Returns \nA doubly-linked list of Nodes. Note that reversed can be called on this list to switch iteration order.   \n"}, {"name": "torch.fx.Graph.node_copy()", "path": "fx#torch.fx.Graph.node_copy", "type": "torch.fx", "text": " \nnode_copy(node, arg_transform=<function Graph.<lambda>>) [source]\n \nCopy a node from one graph into another. arg_transform needs to transform arguments from the graph of node to the graph of self. Example: # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n  Parameters \n \nnode (Node) \u2013 The node to copy into self. \narg_transform (Callable[[Node], Argument]) \u2013 A function that transforms Node arguments in node\u2019s args and kwargs into the equivalent argument in self. In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to self.    \n"}, {"name": "torch.fx.Graph.output()", "path": "fx#torch.fx.Graph.output", "type": "torch.fx", "text": " \noutput(result, type_expr=None) [source]\n \nInsert an output Node into the Graph. An output node represents a return statement in Python code. result is the value that should be returned.  Parameters \n \nresult (Argument) \u2013 The value to be returned. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have.     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.placeholder()", "path": "fx#torch.fx.Graph.placeholder", "type": "torch.fx", "text": " \nplaceholder(name, type_expr=None) [source]\n \nInsert a placeholder node into the Graph. A placeholder represents a function input.  Parameters \n \nname (str) \u2013 A name for the input value. This corresponds to the name of the positional argument to the function this Graph represents. \ntype_expr (Optional[Any]) \u2013 an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).     Note The same insertion point and type expression rules apply for this method as Graph.create_node.  \n"}, {"name": "torch.fx.Graph.print_tabular()", "path": "fx#torch.fx.Graph.print_tabular", "type": "torch.fx", "text": " \nprint_tabular() [source]\n \nPrints the intermediate representation of the graph in tabular format. \n"}, {"name": "torch.fx.Graph.python_code()", "path": "fx#torch.fx.Graph.python_code", "type": "torch.fx", "text": " \npython_code(root_module) [source]\n \nTurn this Graph into valid Python code.  Parameters \nroot_module (str) \u2013 The name of the root module on which to look-up qualified name targets. This is usually \u2018self\u2019.  Returns \nThe string source code generated from this Graph.   \n"}, {"name": "torch.fx.Graph.__init__()", "path": "fx#torch.fx.Graph.__init__", "type": "torch.fx", "text": " \n__init__() [source]\n \nConstruct an empty Graph. \n"}, {"name": "torch.fx.GraphModule", "path": "fx#torch.fx.GraphModule", "type": "torch.fx", "text": " \nclass torch.fx.GraphModule(root, graph, class_name='GraphModule') [source]\n \nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a graph attribute, as well as code and forward attributes generated from that graph.  Warning When graph is reassigned, code and forward will be automatically regenerated. However, if you edit the contents of the graph without reassigning the graph attribute itself, you must call recompile() to update the generated code.   \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n  \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n  \nproperty graph  \nReturn the Graph underlying this GraphModule \n  \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n  \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n \n"}, {"name": "torch.fx.GraphModule.code()", "path": "fx#torch.fx.GraphModule.code", "type": "torch.fx", "text": " \nproperty code  \nReturn the Python code generated from the Graph underlying this GraphModule. \n"}, {"name": "torch.fx.GraphModule.graph()", "path": "fx#torch.fx.GraphModule.graph", "type": "torch.fx", "text": " \nproperty graph  \nReturn the Graph underlying this GraphModule \n"}, {"name": "torch.fx.GraphModule.recompile()", "path": "fx#torch.fx.GraphModule.recompile", "type": "torch.fx", "text": " \nrecompile() [source]\n \nRecompile this GraphModule from its graph attribute. This should be called after editing the contained graph, otherwise the generated code of this GraphModule will be out of date. \n"}, {"name": "torch.fx.GraphModule.to_folder()", "path": "fx#torch.fx.GraphModule.to_folder", "type": "torch.fx", "text": " \nto_folder(folder, module_name='FxModule') [source]\n \nDumps out module to folder with module_name so that it can be imported with from <folder> import <module_name>  Parameters \n \nfolder (Union[str, os.PathLike]) \u2013 The folder to write the code out to \nmodule_name (str) \u2013 Top-level name to use for the Module while writing out the code    \n"}, {"name": "torch.fx.GraphModule.__init__()", "path": "fx#torch.fx.GraphModule.__init__", "type": "torch.fx", "text": " \n__init__(root, graph, class_name='GraphModule') [source]\n \nConstruct a GraphModule.  Parameters \n \nroot (Union[torch.nn.Module, Dict[str, Any]) \u2013 root can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that root is a Module, any references to Module-based objects (via qualified name) in the Graph\u2019s Nodes\u2019 target field will be copied over from the respective place within root\u2019s Module hierarchy into the GraphModule\u2019s module hierarchy. In the case that root is a dict, the qualified name found in a Node\u2019s target will be looked up directly in the dict\u2019s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule\u2019s module hierarchy. \ngraph (Graph) \u2013 graph contains the nodes this GraphModule should use for code generation \nname (str) \u2013 name denotes the name of this GraphModule for debugging purposes. If it\u2019s unset, all error messages will report as originating from GraphModule. It may be helpful to set this to root\u2019s original name or a name that makes sense within the context of your transform.    \n"}, {"name": "torch.fx.Interpreter", "path": "fx#torch.fx.Interpreter", "type": "torch.fx", "text": " \nclass torch.fx.Interpreter(module) [source]\n \nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes. Methods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The module to be executed    \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n  \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n  \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n  \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n  \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n  \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n  \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n  \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n  \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n  \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n \n"}, {"name": "torch.fx.Interpreter.call_function()", "path": "fx#torch.fx.Interpreter.call_function", "type": "torch.fx", "text": " \ncall_function(target, args, kwargs) [source]\n \nExecute a call_function node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the function invocation   \n"}, {"name": "torch.fx.Interpreter.call_method()", "path": "fx#torch.fx.Interpreter.call_method", "type": "torch.fx", "text": " \ncall_method(target, args, kwargs) [source]\n \nExecute a call_method node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the method invocation   \n"}, {"name": "torch.fx.Interpreter.call_module()", "path": "fx#torch.fx.Interpreter.call_module", "type": "torch.fx", "text": " \ncall_module(target, args, kwargs) [source]\n \nExecute a call_module node and return the result.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation     Return\n\nAny: The value returned by the module invocation   \n"}, {"name": "torch.fx.Interpreter.fetch_args_kwargs_from_env()", "path": "fx#torch.fx.Interpreter.fetch_args_kwargs_from_env", "type": "torch.fx", "text": " \nfetch_args_kwargs_from_env(n) [source]\n \nFetch the concrete values of args and kwargs of node n from the current execution environment.  Parameters \nn (Node) \u2013 The node for which args and kwargs should be fetched.  Returns \nargs and kwargs with concrete values for n.  Return type \nTuple[Tuple, Dict]   \n"}, {"name": "torch.fx.Interpreter.fetch_attr()", "path": "fx#torch.fx.Interpreter.fetch_attr", "type": "torch.fx", "text": " \nfetch_attr(target) [source]\n \nFetch an attribute from the Module hierarchy of self.module.  Parameters \ntarget (str) \u2013 The fully-qualfiied name of the attribute to fetch  Returns \nThe value of the attribute.  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.get_attr()", "path": "fx#torch.fx.Interpreter.get_attr", "type": "torch.fx", "text": " \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. Will retrieve an attribute value from the Module hierarchy of self.module.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe value of the attribute that was retrieved  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.map_nodes_to_values()", "path": "fx#torch.fx.Interpreter.map_nodes_to_values", "type": "torch.fx", "text": " \nmap_nodes_to_values(args, n) [source]\n \nRecursively descend through args and look up the concrete value for each Node in the current execution environment.  Parameters \n \nargs (Argument) \u2013 Data structure within which to look up concrete values \nn (Node) \u2013 Node to which args belongs. This is only used for error reporting.    \n"}, {"name": "torch.fx.Interpreter.output()", "path": "fx#torch.fx.Interpreter.output", "type": "torch.fx", "text": " \noutput(target, args, kwargs) [source]\n \nExecute an output node. This really just retrieves the value referenced by the output node and returns it.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe return value referenced by the output node  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.placeholder()", "path": "fx#torch.fx.Interpreter.placeholder", "type": "torch.fx", "text": " \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. Note that this is stateful: Interpreter maintains an internal iterator over arguments passed to run and this method returns next() on that iterator.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation   Returns \nThe argument value that was retrieved.  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.run()", "path": "fx#torch.fx.Interpreter.run", "type": "torch.fx", "text": " \nrun(*args, initial_env=None) [source]\n \nRun module via interpretation and return the result.  Parameters \n \n*args \u2013 The arguments to the Module to run, in positional order \ninitial_env (Optional[Dict[Node, Any]]) \u2013 An optional starting environment for execution. This is a dict mapping Node to any value. This can be used, for example, to pre-populate results for certain Nodes so as to do only partial evaluation within the interpreter.   Returns \nThe value returned from executing the Module  Return type \nAny   \n"}, {"name": "torch.fx.Interpreter.run_node()", "path": "fx#torch.fx.Interpreter.run_node", "type": "torch.fx", "text": " \nrun_node(n) [source]\n \nRun a specific node n and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on node.op  Parameters \nn (Node) \u2013 The Node to execute  Returns \nThe result of executing n  Return type \nAny   \n"}, {"name": "torch.fx.Node", "path": "fx#torch.fx.Node", "type": "torch.fx", "text": " \nclass torch.fx.Node(graph, name, op, target, args, kwargs, type=None) [source]\n \nNode is the data structure that represents individual operations within a Graph. For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each Node has a function specified by its op property. The Node semantics for each value of op are as follows:  \nplaceholder represents a function input. The name attribute specifies the name this value will take on. target is similarly the name of the argument. args holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. kwargs is don\u2019t-care. Placeholders correspond to the function parameters (e.g. x) in the graph printout. \nget_attr retrieves a parameter from the module hierarchy. name is similarly the name the result of the fetch is assigned to. target is the fully-qualified name of the parameter\u2019s position in the module hierarchy. args and kwargs are don\u2019t-care \ncall_function applies a free function to some values. name is similarly the name of the value to assign to. target is the function to be applied. args and kwargs represent the arguments to the function, following the Python calling convention \ncall_module applies a module in the module hierarchy\u2019s forward() method to given arguments. name is as previous. target is the fully-qualified name of the module in the module hierarchy to call. args and kwargs represent the arguments to invoke the module on, including the self argument. \ncall_method calls a method on a value. name is as similar. target is the string name of the method to apply to the self argument. args and kwargs represent the arguments to invoke the module on, including the self argument\n \noutput contains the output of the traced function in its args[0] attribute. This corresponds to the \u201creturn\u201d statement in the Graph printout.   \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n  \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n  \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n  \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n  \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n  \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n  \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n \n"}, {"name": "torch.fx.Node.all_input_nodes()", "path": "fx#torch.fx.Node.all_input_nodes", "type": "torch.fx", "text": " \nproperty all_input_nodes  \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over args and kwargs and only collecting the values that are Nodes.  Returns \nList of Nodes that appear in the args and kwargs of this Node, in that order.   \n"}, {"name": "torch.fx.Node.append()", "path": "fx#torch.fx.Node.append", "type": "torch.fx", "text": " \nappend(x) [source]\n \nInsert x after this node in the list of nodes in the graph. Equvalent to self.next.prepend(x)  Parameters \nx (Node) \u2013 The node to put after this node. Must be a member of the same graph.   \n"}, {"name": "torch.fx.Node.args()", "path": "fx#torch.fx.Node.args", "type": "torch.fx", "text": " \nproperty args  \nThe tuple of arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n"}, {"name": "torch.fx.Node.kwargs()", "path": "fx#torch.fx.Node.kwargs", "type": "torch.fx", "text": " \nproperty kwargs  \nThe dict of keyword arguments to this Node. The interpretation of arguments depends on the node\u2019s opcode. See the Node docstring for more information. Assignment to this property is allowed. All accounting of uses and users is updated automatically on assignment. \n"}, {"name": "torch.fx.Node.next()", "path": "fx#torch.fx.Node.next", "type": "torch.fx", "text": " \nproperty next  \nReturns the next Node in the linked list of Nodes.  Returns \nThe next Node in the linked list of Nodes.   \n"}, {"name": "torch.fx.Node.prepend()", "path": "fx#torch.fx.Node.prepend", "type": "torch.fx", "text": " \nprepend(x) [source]\n \nInsert x before this node in the list of nodes in the graph. Example: Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n  Parameters \nx (Node) \u2013 The node to put before this node. Must be a member of the same graph.   \n"}, {"name": "torch.fx.Node.prev()", "path": "fx#torch.fx.Node.prev", "type": "torch.fx", "text": " \nproperty prev  \nReturns the previous Node in the linked list of Nodes.  Returns \nThe previous Node in the linked list of Nodes.   \n"}, {"name": "torch.fx.Node.replace_all_uses_with()", "path": "fx#torch.fx.Node.replace_all_uses_with", "type": "torch.fx", "text": " \nreplace_all_uses_with(replace_with) [source]\n \nReplace all uses of self in the Graph with the Node replace_with.  Parameters \nreplace_with (Node) \u2013 The node to replace all uses of self with.  Returns \nThe list of Nodes on which this change was made.   \n"}, {"name": "torch.fx.Proxy", "path": "fx#torch.fx.Proxy", "type": "torch.fx", "text": " \nclass torch.fx.Proxy(node, tracer=None) [source]\n \nProxy objects are Node wrappers that flow through the program during symbolic tracing and record all the operations (torch function calls, method calls, operators) that they touch into the growing FX Graph. If you\u2019re doing graph transforms, you can wrap your own Proxy method around a raw Node so that you can use the overloaded operators to add additional things to a Graph. \n"}, {"name": "torch.fx.replace_pattern()", "path": "fx#torch.fx.replace_pattern", "type": "torch.fx", "text": " \ntorch.fx.replace_pattern(gm, pattern, replacement) [source]\n \nMatches all possible non-overlapping sets of operators and their data dependencies (pattern) in the Graph of a GraphModule (gm), then replaces each of these matched subgraphs with another subgraph (replacement).  Parameters \n \ngm \u2013 The GraphModule that wraps the Graph to operate on \npattern \u2013 The subgraph to match in gm for replacement \nreplacement \u2013 The subgraph to replace pattern with   Returns \nA list of Match objects representing the places in the original graph that pattern was matched to. The list is empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n  Return type \nList[Match]   Examples: import torch\nfrom torch.fx import symbolic_trace, subgraph_rewriter\n\nclass M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, w1, w2):\n        m1 = torch.cat([w1, w2]).sum()\n        m2 = torch.cat([w1, w2]).sum()\n        return x + torch.max(m1) + torch.max(m2)\n\ndef pattern(w1, w2):\n    return torch.cat([w1, w2]).sum()\n\ndef replacement(w1, w2):\n    return torch.stack([w1, w2])\n\ntraced_module = symbolic_trace(M())\n\nsubgraph_rewriter.replace_pattern(traced_module, pattern, replacement)\n The above code will first match pattern in the forward method of traced_module. Pattern-matching is done based on use-def relationships, not node names. For example, if you had p = torch.cat([a, b]) in pattern, you could match m = torch.cat([a, b]) in the original forward function, despite the variable names being different (p vs m). The return statement in pattern is matched based on its value only; it may or may not match to the return statement in the larger graph. In other words, the pattern doesn\u2019t have to extend to the end of the larger graph. When the pattern is matched, it will be removed from the larger function and replaced by replacement. If there are multiple matches for pattern in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (\u201cFirst\u201d here being defined as the first in a topological ordering of the Nodes\u2019 use-def relationships. In most cases, the first Node is the parameter that appears directly after self, while the last Node is whatever the function returns.) One important thing to note is that the parameters of the pattern Callable must be used in the Callable itself, and the parameters of the replacement Callable must match the pattern. The first rule is why, in the above code block, the forward function has parameters x, w1, w2, but the pattern function only has parameters w1, w2. pattern doesn\u2019t use x, so it shouldn\u2019t specify x as a parameter. As an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n with def replacement(x, y):\n    return torch.relu(x)\n In this case, replacement needs the same number of parameters as pattern (both x and y), even though the parameter y isn\u2019t used in replacement. After calling subgraph_rewriter.replace_pattern, the generated Python code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n \n"}, {"name": "torch.fx.symbolic_trace()", "path": "fx#torch.fx.symbolic_trace", "type": "torch.fx", "text": " \ntorch.fx.symbolic_trace(root, concrete_args=None) [source]\n \nSymbolic tracing API Given an nn.Module or function instance root, this function will return a GraphModule constructed by recording operations seen while tracing through root.  Parameters \n \nroot (Union[torch.nn.Module, Callable]) \u2013 Module or function to be traced and converted into a Graph representation. \nconcrete_args (Optional[Dict[str, any]]) \u2013 Concrete arguments that should not be treated as Proxies.   Returns \na Module created from the recorded operations from root.  Return type \nGraphModule   \n"}, {"name": "torch.fx.Tracer", "path": "fx#torch.fx.Tracer", "type": "torch.fx", "text": " \nclass torch.fx.Tracer(autowrap_modules=(<module 'math' from '/home/matti/miniconda3/lib/python3.7/lib-dynload/math.cpython-37m-x86_64-linux-gnu.so'>, )) [source]\n \nTracer is the class that implements the symbolic tracing functionality of torch.fx.symbolic_trace. A call to symbolic_trace(m) is equivalent to Tracer().trace(m). Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.  \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n  \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n  \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n  \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n  \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n  \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n \n"}, {"name": "torch.fx.Tracer.call_module()", "path": "fx#torch.fx.Tracer.call_module", "type": "torch.fx", "text": " \ncall_module(m, forward, args, kwargs) [source]\n \nMethod that specifies the behavior of this Tracer when it encounters a call to an nn.Module instance. By default, the behavior is to check if the called module is a leaf module via is_leaf_module. If it is, emit a call_module node referring to m in the Graph. Otherwise, call the Module normally, tracing through the operations in its forward function. This method can be overridden to\u2013for example\u2013create nested traced GraphModules, or any other behavior you would want while tracing across Module boundaries. Module boundaries.  Parameters \n \nm (Module) \u2013 The module for which a call is being emitted \nforward (Callable) \u2013 The forward() method of the Module to be invoked \nargs (Tuple) \u2013 args of the module callsite \nkwargs (Dict) \u2013 kwargs of the module callsite   Returns \nThe return value from the Module call. In the case that a call_module node was emitted, this is a Proxy value. Otherwise, it is whatever value was returned from the Module invocation.   \n"}, {"name": "torch.fx.Tracer.create_arg()", "path": "fx#torch.fx.Tracer.create_arg", "type": "torch.fx", "text": " \ncreate_arg(a) [source]\n \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the Graph. By default, the behavior includes:  Iterate through collection types (e.g. tuple, list, dict) and recursively call create_args on the elements. Given a Proxy object, return a reference to the underlying IR Node\n \nGiven a non-Proxy Tensor object, emit IR for various cases:  For a Parameter, emit a get_attr node referring to that Parameter For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.    This method can be overridden to support more types.  Parameters \na (Any) \u2013 The value to be emitted as an Argument in the Graph.  Returns \nThe value a converted into the appropriate Argument   \n"}, {"name": "torch.fx.Tracer.create_args_for_root()", "path": "fx#torch.fx.Tracer.create_args_for_root", "type": "torch.fx", "text": " \ncreate_args_for_root(root_fn, is_module, concrete_args=None) [source]\n \nCreate placeholder nodes corresponding to the signature of the root Module. This method introspects root\u2019s signature and emits those nodes accordingly, also supporting *args and **kwargs. \n"}, {"name": "torch.fx.Tracer.is_leaf_module()", "path": "fx#torch.fx.Tracer.is_leaf_module", "type": "torch.fx", "text": " \nis_leaf_module(m, module_qualified_name) [source]\n \nA method to specify whether a given nn.Module is a \u201cleaf\u201d module. Leaf modules are the atomic units that appear in the IR, referenced by call_module calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.  Parameters \n \nm (Module) \u2013 The module being queried about \nmodule_qualified_name (str) \u2013 The path to root of this module. For example, if you have a module hierarchy where submodule foo contains submodule bar, which contains submodule baz, that module will appear with the qualified name foo.bar.baz here.    \n"}, {"name": "torch.fx.Tracer.path_of_module()", "path": "fx#torch.fx.Tracer.path_of_module", "type": "torch.fx", "text": " \npath_of_module(mod) [source]\n \nHelper method to find the qualified name of mod in the Module hierarchy of root. For example, if root has a submodule named foo, which has a submodule named bar, passing bar into this function will return the string \u201cfoo.bar\u201d.  Parameters \nmod (str) \u2013 The Module to retrieve the qualified name for.   \n"}, {"name": "torch.fx.Tracer.trace()", "path": "fx#torch.fx.Tracer.trace", "type": "torch.fx", "text": " \ntrace(root, concrete_args=None) [source]\n \nTrace root and return the corresponding FX Graph representation. root can either be an nn.Module instance or a Python callable. Note that after this call, self.root may be different from the root passed in here. For example, when a free function is passed to trace(), we will create an nn.Module instance to use as the root and add embedded constants to.  Parameters \nroot (Union[Module, Callable]) \u2013 Either a Module or a function to be traced through.  Returns \nA Graph representing the semantics of the passed-in root.   \n"}, {"name": "torch.fx.Transformer", "path": "fx#torch.fx.Transformer", "type": "torch.fx", "text": " \nclass torch.fx.Transformer(module) [source]\n \nTransformer is a special type of interpreter that produces a new Module. It exposes a transform() method that returns the transformed Module. Transformer does not require arguments to run, as Interpreter does. Transformer works entirely symbolically. Example Suppose we want to swap all instances of torch.neg with torch.sigmoid and vice versa (including their Tensor method equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n  Parameters \nmodule (GraphModule) \u2013 The Module to be transformed.    \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n  \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n \n"}, {"name": "torch.fx.Transformer.get_attr()", "path": "fx#torch.fx.Transformer.get_attr", "type": "torch.fx", "text": " \nget_attr(target, args, kwargs) [source]\n \nExecute a get_attr node. In Transformer, this is overridden to insert a new get_attr node into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n"}, {"name": "torch.fx.Transformer.placeholder()", "path": "fx#torch.fx.Transformer.placeholder", "type": "torch.fx", "text": " \nplaceholder(target, args, kwargs) [source]\n \nExecute a placeholder node. In Transformer, this is overridden to insert a new placeholder into the output graph.  Parameters \n \ntarget (Target) \u2013 The call target for this node. See Node for details on semantics \nargs (Tuple) \u2013 Tuple of positional args for this invocation \nkwargs (Dict) \u2013 Dict of keyword arguments for this invocation    \n"}, {"name": "torch.fx.Transformer.transform()", "path": "fx#torch.fx.Transformer.transform", "type": "torch.fx", "text": " \ntransform() [source]\n \nTransform self.module and return the transformed GraphModule. \n"}, {"name": "torch.fx.wrap()", "path": "fx#torch.fx.wrap", "type": "torch.fx", "text": " \ntorch.fx.wrap(fn_or_name) [source]\n \nThis function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d. A \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being traced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n A wrapped function can be thought of a \u201cleaf function\u201d, analogous to the concept of \u201cleaf modules\u201d, that is, they are functions that are left as calls in the FX trace rather than traced through.  Parameters \nfn_or_name (Union[str, Callable]) \u2013 The function or name of the global function to insert into the graph when it\u2019s called   \n"}, {"name": "torch.gather()", "path": "generated/torch.gather#torch.gather", "type": "torch", "text": " \ntorch.gather(input, dim, index, *, sparse_grad=False, out=None) \u2192 Tensor  \nGathers values along an axis specified by dim. For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n input and index must have the same number of dimensions. It is also required that index.size(d) <= input.size(d) for all dimensions d != dim. out will have the same shape as index. Note that input and index do not bro